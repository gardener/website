<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener – Gardener</title><link>https://gardener.cloud/docs/gardener/</link><description>Recent content on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 19 Nov 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://gardener.cloud/docs/gardener/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: API Reference</title><link>https://gardener.cloud/docs/gardener/api-reference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/api-reference/</guid><description>
&lt;h1 id="gardener-api-reference">Gardener API Reference&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/authentication/">&lt;code>authentication.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/">&lt;code>core.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/extensions/">&lt;code>extensions.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/operations/">&lt;code>operations.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/resources/">&lt;code>resources.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/seedmanagement/">&lt;code>seedmanagement.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/settings/">&lt;code>settings.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Extensions</title><link>https://gardener.cloud/docs/gardener/extensions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/extensions/</guid><description/></item><item><title>Docs: Monitoring</title><link>https://gardener.cloud/docs/gardener/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/monitoring/</guid><description>
&lt;h1 id="monitoring">Monitoring&lt;/h1>
&lt;h2 id="roles-of-the-different-prometheus-instances">Roles of the different Prometheus instances&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/monitoring_06c124.png" alt="monitoring">&lt;/p>
&lt;h3 id="prometheus">Prometheus&lt;/h3>
&lt;p>Deployed in the &lt;code>garden&lt;/code> namespace. Important scrape targets:&lt;/p>
&lt;ul>
&lt;li>cadvisor&lt;/li>
&lt;li>node-exporter&lt;/li>
&lt;li>kube-state-metrics&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Purpose&lt;/strong>: Acts as a cache for other Prometheus instances. The metrics are kept for a short amount of time (~2 hours) due to the high cardinality. For example if another Prometheus needs access to cadvisor metrics it will query this Prometheus instead of the cadvisor. This also reduces load on the kubelets and API Server.&lt;/p>
&lt;p>Some of the high cardinality metrics are aggregated with recording rules. These &lt;em>pre-aggregated&lt;/em> metrics are scraped by the &lt;a href="#aggregate-prometheus">Aggregate Prometheus&lt;/a>.&lt;/p>
&lt;p>This Prometheus is not used for alerting.&lt;/p>
&lt;h3 id="aggregate-prometheus">Aggregate Prometheus&lt;/h3>
&lt;p>Deployed in the &lt;code>garden&lt;/code> namespace. Important scrape targets:&lt;/p>
&lt;ul>
&lt;li>other prometheus instances&lt;/li>
&lt;li>logging components&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Purpose&lt;/strong>: Store pre-aggregated data from &lt;a href="#prometheus">prometheus&lt;/a> and &lt;a href="#shoot-prometheus">shoot prometheus&lt;/a>. An ingress exposes this Prometheus allowing it to be scraped from another cluster.&lt;/p>
&lt;h3 id="seed-prometheus">Seed Prometheus&lt;/h3>
&lt;p>Deployed in the &lt;code>garden&lt;/code> namespace. Important scrape targets:&lt;/p>
&lt;ul>
&lt;li>pods in extension namespaces annotated with:&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>prometheus.io/scrape=true
prometheus.io/port=&amp;lt;port&amp;gt;
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>cadvisor metrics from pods in the garden and extension namespaces&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Purpose&lt;/strong>: Entrypoint for operators when debugging issues with extensions or other garden components.&lt;/p>
&lt;h3 id="shoot-prometheus">Shoot Prometheus&lt;/h3>
&lt;p>Deployed in the shoot control plane namespace. Important scrape targets:&lt;/p>
&lt;ul>
&lt;li>control plane components&lt;/li>
&lt;li>shoot nodes (node-exporter)&lt;/li>
&lt;li>blackbox-exporter used to measure &lt;a href="https://gardener.cloud/docs/gardener/monitoring/connectivity/">connectivity&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Purpose&lt;/strong>: Monitor all relevant components belonging to a shoot cluster managed by Gardener. Shoot owners can view the metrics in Grafana dashboards and receive &lt;a href="https://gardener.cloud/docs/gardener/monitoring/user_alerts/">alerts&lt;/a> based on these metrics. Gardener operators will receive a different set of &lt;a href="https://gardener.cloud/docs/gardener/monitoring/operator_alerts/">alerts&lt;/a>. For alerting internals refer to &lt;a href="https://gardener.cloud/docs/gardener/monitoring/alerting/">this&lt;/a> document.&lt;/p>
&lt;h2 id="collect-all-shoot-prometheus-with-remote-write">Collect all Shoot Prometheus with remote write&lt;/h2>
&lt;p>An optional collection of all Shoot Prometheus metrics to a central prometheus (or cortex) instance is possible with the &lt;code>monitoring.shoot&lt;/code> setting in &lt;code>GardenletConfiguration&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>monitoring:
shoot:
remoteWrite:
url: https://remoteWriteUrl # remote write URL
keep:# metrics that should be forwarded to the external write endpoint. If empty all metrics get forwarded
- kube_pod_container_info
queueConfig: | # queue_config of prometheus remote write as multiline string
max_shards: 100
batch_send_deadline: 20s
min_backoff: 500ms
max_backoff: 60s
externalLabels: # add additional labels to metrics to identify it on the central instance
additional: label
&lt;/code>&lt;/pre>&lt;p>If basic auth is needed it can be set via secret in garden namespace (Gardener API Server). &lt;a href="https://github.com/gardener/gardener/blob/master/example/10-secret-remote-write.yaml">Example secret&lt;/a>&lt;/p></description></item><item><title>Docs: Usage</title><link>https://gardener.cloud/docs/gardener/usage/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/usage/</guid><description/></item><item><title>Docs: Admission Controller</title><link>https://gardener.cloud/docs/gardener/concepts/admission-controller/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/admission-controller/</guid><description>
&lt;h1 id="gardener-admission-controller">Gardener Admission Controller&lt;/h1>
&lt;p>While the Gardener API server works with &lt;a href="https://gardener.cloud/docs/gardener/concepts/apiserver_admission_plugins/">admission plugins&lt;/a> to validate and mutate resources belonging to Gardener related API groups, e.g. &lt;code>core.gardener.cloud&lt;/code>, the same is needed for resources belonging to non-Gardener API groups as well, e.g. &lt;code>Secret&lt;/code>s in the &lt;code>core&lt;/code> API group.
Therefore, the Gardener Admission Controller runs a http(s) server with the following handlers which serve as validating/mutating endpoints for &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">admission webhooks&lt;/a>.
It is also used to serve http(s) handlers for authorization webhooks.&lt;/p>
&lt;h2 id="admission-webhook-handlers">Admission Webhook Handlers&lt;/h2>
&lt;p>This section describes the admission webhook handlers that are currently served.&lt;/p>
&lt;h3 id="kubeconfig-secret-validator">Kubeconfig Secret Validator&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/kubectl/issues/697">Malicious Kubeconfigs&lt;/a> applied by end users may cause a leakage of sensitive data.
This handler checks if the incoming request contains a Kubernetes secret with a &lt;code>.data.kubeconfig&lt;/code> field and denies the request if the Kubeconfig structure violates Gardener&amp;rsquo;s security standards.&lt;/p>
&lt;h3 id="namespace-validator">Namespace Validator&lt;/h3>
&lt;p>Namespaces are the backing entities of Gardener projects in which shoot clusters objects reside.
This validation handler protects active namespaces against premature deletion requests.
Therefore, it denies deletion requests if a namespace still contains shoot clusters or if it belongs to a non-deleting Gardener project (w/o &lt;code>.metadata.deletionTimestamp&lt;/code>).&lt;/p>
&lt;h3 id="resource-size-validator">Resource Size Validator&lt;/h3>
&lt;p>Since users directly apply Kubernetes native objects to the Garden cluster, it also involves the risk of being vulnerable to DoS attacks because these resources are read continuously watched and read by controllers.
One example is the creation of &lt;code>Shoot&lt;/code> resources with large annotation values (up to 256 kB per value) which can cause severe out-of-memory issues for the Gardenlet component.
&lt;a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">Vertical autoscaling&lt;/a> can help to mitigate such situations, but we cannot expect to scale infinitely, and thus need means to block the attack itself.&lt;/p>
&lt;p>The Resource Size Validator checks arbitrary incoming admission requests against a configured maximum size for the resource&amp;rsquo;s group-version-kind combination and denies the request if the contained object exceeds the quota.&lt;/p>
&lt;p>Example for Gardener Admission Controller configuration:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>server:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resourceAdmissionConfiguration:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> limits:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - apiGroups: [&lt;span style="color:#a31515">&amp;#34;core.gardener.cloud&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersions: [&lt;span style="color:#a31515">&amp;#34;*&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources: [&lt;span style="color:#a31515">&amp;#34;shoots&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;plants&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> size: 100k
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - apiGroups: [&lt;span style="color:#a31515">&amp;#34;&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersions: [&lt;span style="color:#a31515">&amp;#34;v1&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources: [&lt;span style="color:#a31515">&amp;#34;secrets&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> size: 100k
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> unrestrictedSubjects:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - kind: Group
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardener.cloud:system:seeds
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - kind: User&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># name: admin&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># apiGroup: rbac.authorization.k8s.io&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - kind: ServiceAccount&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># name: &amp;#34;*&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># namespace: garden&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># apiGroup: &amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operationMode: block &lt;span style="color:#008000">#log&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>With the configuration above, the Resource Size Validator denies requests for shoots and plants with Gardener&amp;rsquo;s core API group which exceed a size of 100 kB. The same is done for Kubernetes secrets.&lt;/p>
&lt;p>As this feature is meant to protect the system from malicious requests sent by users, it is recommended to exclude trusted groups, users or service accounts from the size restriction via &lt;code>resourceAdmissionConfiguration.unrestrictedSubjects&lt;/code>.
For example, the backing user for the Gardenlet should always be capable of changing the shoot resource instead of being blocked due to size restrictions.
This is because the Gardenlet itself occasionally changes the shoot specification, labels or annotations, and might violate the quota if the existing resource is already close to the quota boundary.
Also, operators are supposed to be trusted users and subjecting them to a size limitation can inhibit important operational tasks.
Wildcard (&amp;quot;*&amp;quot;) in subject &lt;code>name&lt;/code> is supported.&lt;/p>
&lt;p>Size limitations depend on the individual Gardener setup and choosing the wrong values can affect the availability of your Gardener service.
&lt;code>resourceAdmissionConfiguration.operationMode&lt;/code> allows to control if a violating request is actually denied (default) or only logged.
It&amp;rsquo;s recommended to start with &lt;code>log&lt;/code>, check the logs for exceeding requests, adjust the limits if necessary and finally switch to &lt;code>block&lt;/code>.&lt;/p>
&lt;h3 id="seedrestriction">SeedRestriction&lt;/h3>
&lt;p>Please refer to &lt;a href="https://gardener.cloud/docs/gardener/deployment/gardenlet_api_access/">this document&lt;/a> for more information.&lt;/p>
&lt;h2 id="authorization-webhook-handlers">Authorization Webhook Handlers&lt;/h2>
&lt;p>This section describes the authorization webhook handlers that are currently served.&lt;/p>
&lt;h3 id="seedauthorization">SeedAuthorization&lt;/h3>
&lt;p>Please refer to &lt;a href="https://gardener.cloud/docs/gardener/deployment/gardenlet_api_access/">this document&lt;/a> for more information.&lt;/p></description></item><item><title>Docs: Apiserver</title><link>https://gardener.cloud/docs/gardener/concepts/apiserver/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/apiserver/</guid><description>
&lt;h1 id="gardener-api-server">Gardener API server&lt;/h1>
&lt;p>The Gardener API server is a Kubernetes-native extension based on its &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">aggregation layer&lt;/a>.
It is registered via an &lt;code>APIService&lt;/code> object and designed to run inside a Kubernetes cluster whose API it wants to extend.&lt;/p>
&lt;p>After registration, it exposes the following resources:&lt;/p>
&lt;h2 id="cloudprofiles">&lt;code>CloudProfile&lt;/code>s&lt;/h2>
&lt;p>&lt;code>CloudProfile&lt;/code>s are resources that describe a specific environment of an underlying infrastructure provider, e.g. AWS, Azure, etc.
Each shoot has to reference a &lt;code>CloudProfile&lt;/code> to declare the environment it should be created in.
In a &lt;code>CloudProfile&lt;/code> the gardener operator specifies certain constraints like available machine types, regions, which Kubernetes versions they want to offer, etc.
End-users can read &lt;code>CloudProfile&lt;/code>s to see these values, but only operators can change the content or create/delete them.
When a shoot is created or updated then an admission plugin checks that only values are used that are allowed via the referenced &lt;code>CloudProfile&lt;/code>.&lt;/p>
&lt;p>Additionally, a &lt;code>CloudProfile&lt;/code> may contain a &lt;code>providerConfig&lt;/code> which is a special configuration dedicated for the infrastructure provider.
Gardener does not evaluate or understand this config, but extension controllers might need for declaration of provider-specific constraints, or global settings.&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml">this&lt;/a> example manifest and consult the documentation of your provider extension controller to get information about its &lt;code>providerConfig&lt;/code>.&lt;/p>
&lt;h2 id="seeds">&lt;code>Seed&lt;/code>s&lt;/h2>
&lt;p>&lt;code>Seed&lt;/code>s are resources that represent seed clusters.
Gardener does not care about how a seed cluster got created - the only requirement is that it is of at least Kubernetes v1.17 and passes the Kubernetes conformance tests.
The Gardener operator has to either deploy the Gardenlet into the cluster they want to use as seed (recommended, then the Gardenlet will create the &lt;code>Seed&lt;/code> object itself after bootstrapping), or they provide the kubeconfig to the cluster inside a secret (that is referenced by the &lt;code>Seed&lt;/code> resource) and create the &lt;code>Seed&lt;/code> resource themselves.&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/45-secret-seed-backup.yaml">this&lt;/a>, &lt;a href="https://github.com/gardener/gardener/blob/master/example/50-seed.yaml">this&lt;/a>(, and optionally &lt;a href="https://github.com/gardener/gardener/blob/master/example/40-secret-seed.yaml">this&lt;/a>) example manifests.&lt;/p>
&lt;h2 id="shootquotas">Shoot&lt;code>Quota&lt;/code>s&lt;/h2>
&lt;p>In order to allow end-users not having their own dedicated infrastructure account to try out Gardener the operator can register an account owned by them that they allow to be used for trial clusters.
Trial clusters can be put under quota such that they don&amp;rsquo;t consume too many resources (resulting in costs), and so that one user cannot consume all resources on their own.
These clusters are automatically terminated after a specified time, but end-users may extend the lifetime manually if needed.&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/60-quota.yaml">this&lt;/a> example manifest.&lt;/p>
&lt;h2 id="projects">&lt;code>Project&lt;/code>s&lt;/h2>
&lt;p>The first thing before creating a shoot cluster is to create a &lt;code>Project&lt;/code>.
A project is used to group multiple shoot clusters together.
End-users can invite colleagues to the project to enable collaboration, and they can either make them &lt;code>admin&lt;/code> or &lt;code>viewer&lt;/code>.
After an end-user has created a project they will get a dedicated namespace in the garden cluster for all their shoots.&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/05-project-dev.yaml">this&lt;/a> example manifest.&lt;/p>
&lt;h2 id="secretbindings">&lt;code>SecretBinding&lt;/code>s&lt;/h2>
&lt;p>Now that the end-user has a namespace the next step is registering their infrastructure provider account.&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/70-secret-provider.yaml">this&lt;/a> example manifest and consult the documentation of the extension controller for the respective infrastructure provider to get information about which keys are required in this secret.&lt;/p>
&lt;p>After the secret has been created the end-user has to create a special &lt;code>SecretBinding&lt;/code> resource that binds this secret.
Later when creating shoot clusters they will reference such a binding.&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/80-secretbinding.yaml">this&lt;/a> example manifest.&lt;/p>
&lt;h2 id="shoots">&lt;code>Shoot&lt;/code>s&lt;/h2>
&lt;p>Shoot cluster contain various settings that influence how end-user Kubernetes clusters will look like in the end.
As Gardener heavily relies on extension controllers for operating system configuration, networking, and infrastructure specifics, the end-user has the possibility (and responsibility) to provide these provider-specific configurations as well.
Such configurations are not evaluated by Gardener (because it doesn&amp;rsquo;t know/understand them), but they are only transported to the respective extension controller.&lt;/p>
&lt;p>⚠️ This means that any configuration issues/mistake on the end-user side that relates to a provider-specific flag or setting cannot be caught during the update request itself but only later during the reconciliation (unless a validator webhook has been registered in the garden cluster by an operator).&lt;/p>
&lt;p>Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml">this&lt;/a> example manifest and consult the documentation of the provider extension controller to get information about its &lt;code>spec.provider.controlPlaneConfig&lt;/code>, &lt;code>.spec.provider.infrastructureConfig&lt;/code>, and &lt;code>.spec.provider.workers[].providerConfig&lt;/code>.&lt;/p>
&lt;h2 id="clusteropenidconnectpresets">&lt;code>(Cluster)OpenIDConnectPreset&lt;/code>s&lt;/h2>
&lt;p>Please see &lt;a href="https://gardener.cloud/docs/gardener/usage/openidconnect-presets/">this&lt;/a> separate documentation file.&lt;/p>
&lt;h2 id="overview-data-model">Overview Data Model&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardener-data-model-overview_c5b559.png" alt="Gardener Overview Data Model">&lt;/p></description></item><item><title>Docs: Apiserver Admission Plugins</title><link>https://gardener.cloud/docs/gardener/concepts/apiserver_admission_plugins/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/apiserver_admission_plugins/</guid><description>
&lt;h1 id="admission-plugins">Admission Plugins&lt;/h1>
&lt;p>Similar to the kube-apiserver, the gardener-apiserver comes with a few in-tree managed admission plugins.
If you want to get an overview of the what and why of admission plugins then &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">this document&lt;/a> might be a good start.&lt;/p>
&lt;p>This document lists all existing admission plugins with a short explanation of what it is responsible for.&lt;/p>
&lt;h2 id="clusteropenidconnectpreset-openidconnectpreset">&lt;code>ClusterOpenIDConnectPreset&lt;/code>, &lt;code>OpenIDConnectPreset&lt;/code>&lt;/h2>
&lt;p>&lt;em>(both enabled by default)&lt;/em>&lt;/p>
&lt;p>These admission controllers react on &lt;code>CREATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
If the &lt;code>Shoot&lt;/code> does not specify any OIDC configuration (&lt;code>.spec.kubernetes.kubeAPIServer.oidcConfig=nil&lt;/code>) then it tries to find a matching &lt;code>ClusterOpenIDConnectPreset&lt;/code> or &lt;code>OpenIDConnectPreset&lt;/code>, respectively.
If there are multiples that match then the one with the highest weight &amp;ldquo;wins&amp;rdquo;.
In this case, the admission controller will default the OIDC configuration in the &lt;code>Shoot&lt;/code>.&lt;/p>
&lt;h2 id="controllerregistrationresources">&lt;code>ControllerRegistrationResources&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>ControllerRegistration&lt;/code>s.
It validates that there exists only one &lt;code>ControllerRegistration&lt;/code> in the system that is primarily responsible for a given kind/type resource combination.
This prevents misconfiguration by the Gardener administrator/operator.&lt;/p>
&lt;h2 id="customverbauthorizer">&lt;code>CustomVerbAuthorizer&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Project&lt;/code>s.
It validates whether the user is bound to a RBAC role with the &lt;code>modify-spec-tolerations-whitelist&lt;/code> verb in case the user tries to change the &lt;code>.spec.tolerations.whitelist&lt;/code> field of the respective &lt;code>Project&lt;/code> resource.
Usually, regular project members are not bound to this custom verb, allowing the Gardener administrator to manage certain toleration whitelists on &lt;code>Project&lt;/code> basis.&lt;/p>
&lt;h2 id="deletionconfirmation">&lt;code>DeletionConfirmation&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>DELETE&lt;/code> operations for &lt;code>Project&lt;/code>s and &lt;code>Shoot&lt;/code>s and &lt;code>ShootState&lt;/code>s.
It validates that the respective resource is annotated with a deletion confirmation annotation, namely &lt;code>confirmation.gardener.cloud/deletion=true&lt;/code>.
Only if this annotation is present it allows the &lt;code>DELETE&lt;/code> operation to pass.
This prevents users from accidental/undesired deletions.&lt;/p>
&lt;h2 id="exposureclass">&lt;code>ExposureClass&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>Create&lt;/code> operations for &lt;code>Shoots&lt;/code>s.
It mutates &lt;code>Shoot&lt;/code> resources which has an &lt;code>ExposureClass&lt;/code> referenced by merging their both &lt;code>shootSelectors&lt;/code> and/or &lt;code>tolerations&lt;/code> into the &lt;code>Shoot&lt;/code> resource.&lt;/p>
&lt;h2 id="extensionvalidator">&lt;code>ExtensionValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>BackupEntry&lt;/code>s, &lt;code>BackupBucket&lt;/code>s, &lt;code>Seed&lt;/code>s, and &lt;code>Shoot&lt;/code>s.
For all the various extension types in the specifications of these objects, it validates whether there exists a &lt;code>ControllerRegistration&lt;/code> in the system that is primarily responsible for the stated extension type(s).
This prevents misconfigurations that would otherwise allow users to create such resources with extension types that don&amp;rsquo;t exist in the cluster, effectively leading to failing reconciliation loops.&lt;/p>
&lt;h2 id="extensionlabels">&lt;code>ExtensionLabels&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>BackupBucket&lt;/code>s, &lt;code>BackupEntry&lt;/code>s, &lt;code>CloudProfile&lt;/code>s, &lt;code>Seed&lt;/code>s, &lt;code>SecretBinding&lt;/code>s and &lt;code>Shoot&lt;/code>s. For all the various extension types in the specifications of these objects, it adds a corresponding label in the resource. This would allow extension admission webhooks to filter out the resources they are responsible for and ignore all others. This label is of the form &lt;code>&amp;lt;extension-type&amp;gt;.extensions.gardener.cloud/&amp;lt;extension-name&amp;gt; : &amp;quot;true&amp;quot;&lt;/code>. For example, an extension label for provider extension type &lt;code>aws&lt;/code>, looks like &lt;code>provider.extensions.gardener.cloud/aws : &amp;quot;true&amp;quot;&lt;/code>.&lt;/p>
&lt;h2 id="plantvalidator">&lt;code>PlantValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Plant&lt;/code>s.
It sets the &lt;code>gardener.cloud/created-by&lt;/code> annotation for newly created &lt;code>Plant&lt;/code> resources.
Also, it prevents creating new &lt;code>Plant&lt;/code> resources in &lt;code>Project&lt;/code>s that are already have a deletion timestamp.&lt;/p>
&lt;h2 id="projectvalidator">&lt;code>ProjectValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> operations for &lt;code>Project&lt;/code>s.
It prevents creating &lt;code>Project&lt;/code>s with a non-empty &lt;code>.spec.namespace&lt;/code> if the value in &lt;code>.spec.namespace&lt;/code> does not start with &lt;code>garden-&lt;/code>.&lt;/p>
&lt;p>⚠️ This admission plugin will be removed in a future release and its business logic will be incorporated into the static validation of the &lt;code>gardener-apiserver&lt;/code>.&lt;/p>
&lt;h2 id="resourcequota">&lt;code>ResourceQuota&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller enables &lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/#object-count-quota">object count ResourceQuotas&lt;/a> for Gardener resources, e.g. &lt;code>Shoots&lt;/code>, &lt;code>SecretBindings&lt;/code>, &lt;code>Projects&lt;/code>, etc..&lt;/p>
&lt;blockquote>
&lt;p>⚠️ In addition to this admission plugin, the &lt;a href="https://github.com/kubernetes/kubernetes/blob/release-1.2/docs/design/admission_control_resource_quota.md#resource-quota-controller">ResourceQuota controller&lt;/a> must be enabled for the Kube-Controller-Manager of your Garden cluster.&lt;/p>
&lt;/blockquote>
&lt;h2 id="resourcereferencemanager">&lt;code>ResourceReferenceManager&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>CloudProfile&lt;/code>s, &lt;code>Project&lt;/code>s, &lt;code>SecretBinding&lt;/code>s, &lt;code>Seed&lt;/code>s, and &lt;code>Shoot&lt;/code>s.
Generally, it checks whether referred resources stated in the specifications of these objects exist in the system (e.g., if a referenced &lt;code>Secret&lt;/code> exists).
However, it also has some special behaviours for certain resources:&lt;/p>
&lt;ul>
&lt;li>&lt;code>CloudProfile&lt;/code>s: It rejects removing Kubernetes or machine image versions if there is at least one &lt;code>Shoot&lt;/code> that refers to them.&lt;/li>
&lt;li>&lt;code>Project&lt;/code>s: It sets the &lt;code>.spec.createdBy&lt;/code> field for newly created &lt;code>Project&lt;/code> resources, and defaults the &lt;code>.spec.owner&lt;/code> field in case it is empty (to the same value of &lt;code>.spec.createdBy&lt;/code>).&lt;/li>
&lt;li>&lt;code>Seed&lt;/code>s: It rejects changing the &lt;code>.spec.settings.shootDNS.enabled&lt;/code> value if there is at least one &lt;code>Shoot&lt;/code> that refers to this seed.&lt;/li>
&lt;li>&lt;code>Shoot&lt;/code>s: It sets the &lt;code>gardener.cloud/created-by=&amp;lt;username&amp;gt;&lt;/code> annotation for newly created &lt;code>Shoot&lt;/code> resources.&lt;/li>
&lt;/ul>
&lt;h2 id="seedvalidator">&lt;code>SeedValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>DELETE&lt;/code> operations for &lt;code>Seed&lt;/code>s.
Rejects the deletion if &lt;code>Shoot&lt;/code>(s) reference the seed cluster.&lt;/p>
&lt;h2 id="shootdns">&lt;code>ShootDNS&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It tries to assign a default domain to the &lt;code>Shoot&lt;/code> if it gets scheduled to a seed that enables DNS for shoots (&lt;code>.spec.settings.shootDNS.enabled=true&lt;/code>).
It also validates that the DNS configuration (&lt;code>.spec.dns&lt;/code>) is not set if the seed disables DNS for shoots.&lt;/p>
&lt;h2 id="shootquotavalidator">&lt;code>ShootQuotaValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It validates the resource consumption declared in the specification against applicable &lt;code>Quota&lt;/code> resources.
Only if the applicable &lt;code>Quota&lt;/code> resources admit the configured resources in the &lt;code>Shoot&lt;/code> then it allows the request.
Applicable &lt;code>Quota&lt;/code>s are referred in the &lt;code>SecretBinding&lt;/code> that is used by the &lt;code>Shoot&lt;/code>.&lt;/p>
&lt;h2 id="shootvpaenabledbydefault">&lt;code>ShootVPAEnabledByDefault&lt;/code>&lt;/h2>
&lt;p>&lt;em>(disabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
If enabled, it will enable the managed &lt;code>VerticalPodAutoscaler&lt;/code> components (see &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_autoscaling/#vertical-pod-auto-scaling">this doc&lt;/a>)
by setting &lt;code>spec.kubernetes.verticalPodAutoscaler.enabled=true&lt;/code> for newly created Shoots.
Already existing Shoots and new Shoots that explicitly disable VPA (&lt;code>spec.kubernetes.verticalPodAutoscaler.enabled=false&lt;/code>)
will not be affected by this admission plugin.&lt;/p>
&lt;h2 id="shoottolerationrestriction">&lt;code>ShootTolerationRestriction&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It validates the &lt;code>.spec.tolerations&lt;/code> used in &lt;code>Shoot&lt;/code>s against the whitelist of its &lt;code>Project&lt;/code>, or against the whitelist configured in the admission controller&amp;rsquo;s configuration, respectively.
Additionally, it defaults the &lt;code>.spec.tolerations&lt;/code> in &lt;code>Shoot&lt;/code>s with those configured in its &lt;code>Project&lt;/code>, and those configured in the admission controller&amp;rsquo;s configuration, respectively.&lt;/p>
&lt;h2 id="shootvalidator">&lt;code>ShootValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It validates certain configurations in the specification against the referred &lt;code>CloudProfile&lt;/code> (e.g., machine images, machine types, used Kubernetes version, &amp;hellip;).
Generally, it performs validations that cannot be handled by the static API validation due to their dynamic nature (e.g., when something needs to be checked against referred resources).
Additionally, it takes over certain defaulting tasks (e.g., default machine image for worker pools).&lt;/p>
&lt;h2 id="shootmanagedseed">&lt;code>ShootManagedSeed&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>DELETE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It rejects the deletion if the &lt;code>Shoot&lt;/code> is referred to by a &lt;code>ManagedSeed&lt;/code>.&lt;/p>
&lt;h2 id="managedseedvalidator">&lt;code>ManagedSeedValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>ManagedSeeds&lt;/code>s.
It validates certain configuration values in the specification against the referred &lt;code>Shoot&lt;/code>, for example Seed provider, network ranges, DNS domain, etc.
Similarly to &lt;code>ShootValidator&lt;/code>, it performs validations that cannot be handled by the static API validation due to their dynamic nature.
Additionally, it performs certain defaulting tasks, making sure that configuration values that are not specified are defaulted to the values of the referred &lt;code>Shoot&lt;/code>, for example Seed provider, network ranges, DNS domain, etc.&lt;/p>
&lt;h2 id="managedseedshoot">&lt;code>ManagedSeedShoot&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>DELETE&lt;/code> operations for &lt;code>ManagedSeed&lt;/code>s.
It rejects the deletion if there are &lt;code>Shoot&lt;/code>s that are scheduled onto the &lt;code>Seed&lt;/code> that is registered by the &lt;code>ManagedSeed&lt;/code>.&lt;/p></description></item><item><title>Docs: Architecture</title><link>https://gardener.cloud/docs/gardener/concepts/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/architecture/</guid><description>
&lt;h4 id="official-definition---what-is-kubernetes">Official Definition - What is Kubernetes?&lt;/h4>
&lt;blockquote>
&lt;p>&amp;ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.&amp;rdquo;&lt;/p>
&lt;/blockquote>
&lt;h4 id="introduction---basic-principle">Introduction - Basic Principle&lt;/h4>
&lt;p>The foundation of the Gardener (providing &lt;em>&lt;strong>Kubernetes Clusters as a Service&lt;/strong>&lt;/em>) is Kubernetes itself, because Kubernetes is the go-to solution to manage software in the Cloud, even when it&amp;rsquo;s Kubernetes itself (see also OpenStack which is provisioned more and more on top of Kubernetes as well).&lt;/p>
&lt;p>While self-hosting, meaning to run Kubernetes components inside Kubernetes, is a popular topic in the community, we apply a special pattern catering to the needs of our cloud platform to provision hundreds or even thousands of clusters. We take a so-called &amp;ldquo;seed&amp;rdquo; cluster and seed the control plane (such as the API server, scheduler, controllers, etcd persistence and others) of an end-user cluster, which we call &amp;ldquo;shoot&amp;rdquo; cluster, as pods into the &amp;ldquo;seed&amp;rdquo; cluster. That means one &amp;ldquo;seed&amp;rdquo; cluster, of which we will have one per IaaS and region, hosts the control planes of multiple &amp;ldquo;shoot&amp;rdquo; clusters. That allows us to avoid dedicated hardware/virtual machines for the &amp;ldquo;shoot&amp;rdquo; cluster control planes. We simply put the control plane into pods/containers and since the &amp;ldquo;seed&amp;rdquo; cluster watches them, they can be deployed with a replica count of 1 and only need to be scaled out when the control plane gets under pressure, but no longer for HA reasons. At the same time, the deployments get simpler (standard Kubernetes deployment) and easier to update (standard Kubernetes rolling update). The actual &amp;ldquo;shoot&amp;rdquo; cluster consists only out of the worker nodes (no control plane) and therefore the users may get full administrative access to their clusters.&lt;/p>
&lt;h4 id="setting-the-scene---components-and-procedure">Setting The Scene - Components and Procedure&lt;/h4>
&lt;p>We provide a central operator UI, which we call the &amp;ldquo;Gardener Dashboard&amp;rdquo;. It talks to a dedicated cluster, which we call the &amp;ldquo;Garden&amp;rdquo; cluster and uses custom resources managed by an &lt;a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#api-server-aggregation">aggregated API server&lt;/a>, one of the general extension concepts of Kubernetes) to represent &amp;ldquo;shoot&amp;rdquo; clusters. In this &amp;ldquo;Garden&amp;rdquo; cluster runs the &amp;ldquo;Gardener&amp;rdquo;, which is basically a Kubernetes controller that watches the custom resources and acts upon them, i.e. creates, updates/modifies, or deletes &amp;ldquo;shoot&amp;rdquo; clusters. The creation follows basically these steps:&lt;/p>
&lt;ul>
&lt;li>Create a namespace in the &amp;ldquo;seed&amp;rdquo; cluster for the &amp;ldquo;shoot&amp;rdquo; cluster which will host the &amp;ldquo;shoot&amp;rdquo; cluster control plane&lt;/li>
&lt;li>Generate secrets and credentials which the worker nodes will need to talk to the control plane&lt;/li>
&lt;li>Create the infrastructure (using &lt;a href="https://www.terraform.io/">Terraform&lt;/a>), which basically consists out of the network setup)&lt;/li>
&lt;li>Deploy the &amp;ldquo;shoot&amp;rdquo; cluster control plane into the &amp;ldquo;shoot&amp;rdquo; namespace in the &amp;ldquo;seed&amp;rdquo; cluster, containing the &amp;ldquo;machine-controller-manager&amp;rdquo; pod&lt;/li>
&lt;li>Create machine CRDs in the &amp;ldquo;seed&amp;rdquo; cluster, describing the configuration and the number of worker machines for the &amp;ldquo;shoot&amp;rdquo; (the machine-controller-manager watches the CRDs and creates virtual machines out of it)&lt;/li>
&lt;li>Wait for the &amp;ldquo;shoot&amp;rdquo; cluster API server to become responsive (pods will be scheduled, persistent volumes and load balancers are created by Kubernetes via the respective cloud provider)&lt;/li>
&lt;li>Finally we deploy &lt;code>kube-system&lt;/code> daemons like &lt;code>kube-proxy&lt;/code> and further add-ons like the &lt;code>dashboard&lt;/code> into the &amp;ldquo;shoot&amp;rdquo; cluster and the cluster becomes active&lt;/li>
&lt;/ul>
&lt;h4 id="overview-architecture-diagram">Overview Architecture Diagram&lt;/h4>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardener-architecture-overview_2bd462.png" alt="Gardener Overview Architecture Diagram">&lt;/p>
&lt;h4 id="detailed-architecture-diagram">Detailed Architecture Diagram&lt;/h4>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardener-architecture-detailed_945c90.png" alt="Gardener Detailed Architecture Diagram">&lt;/p>
&lt;p>Note: The &lt;code>kubelet&lt;/code> as well as the pods inside the &amp;ldquo;shoot&amp;rdquo; cluster talk through the front-door (load balancer IP; public Internet) to its &amp;ldquo;shoot&amp;rdquo; cluster API server running in the &amp;ldquo;seed&amp;rdquo; cluster. The reverse communication from the API server to the pod, service, and node networks happens through a VPN connection that we deploy into &amp;ldquo;seed&amp;rdquo; and &amp;ldquo;shoot&amp;rdquo; clusters.&lt;/p></description></item><item><title>Docs: Authentication Gardener Control Plane</title><link>https://gardener.cloud/docs/gardener/deployment/authentication_gardener_control_plane/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/authentication_gardener_control_plane/</guid><description>
&lt;h1 id="authentication-of-gardener-control-plane-components-against-the-garden-cluster">Authentication of Gardener control plane components against the Garden cluster&lt;/h1>
&lt;p>&lt;strong>Note:&lt;/strong> This document refers to Gardener&amp;rsquo;s API server, admission controller, controller manager and scheduler components. Any reference to the term &lt;strong>Gardener control plane component&lt;/strong> can be replaced with any of the mentioned above.&lt;/p>
&lt;p>There are several authentication possibilities depending on whether or not &lt;a href="https://github.com/gardener/garden-setup#concept-the-virtual-cluster">the concept of Virtual Garden&lt;/a> is used.&lt;/p>
&lt;h2 id="virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster">Virtual Garden is not used, i.e., the &lt;code>runtime&lt;/code> Garden cluster is also the &lt;code>target&lt;/code> Garden cluster.&lt;/h2>
&lt;h4 id="automounted-service-account-token">Automounted Service Account Token&lt;/h4>
&lt;p>The easiest way to deploy a &lt;strong>Gardener control plane component&lt;/strong> will be to not provide &lt;code>kubeconfig&lt;/code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.&lt;/p>
&lt;h4 id="service-account-token-volume-projection">Service Account Token Volume Projection&lt;/h4>
&lt;p>Another solution will be to use &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection">Service Account Token Volume Projection&lt;/a> combined with a &lt;code>kubeconfig&lt;/code> referencing a token file (see example below).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusters:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- cluster:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> certificate-authority-data: &amp;lt;CA-DATA&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: https://default.kubernetes.svc.cluster.local
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>current-context: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>users:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tokenFile: /var/run/secrets/projected/serviceaccount/token
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will allow for automatic rotation of the service account token by the &lt;code>kubelet&lt;/code>. The configuration can be achieved by setting both &lt;code>.Values.global.&amp;lt;GardenerControlPlaneComponent&amp;gt;.serviceAccountTokenVolumeProjection.enabled: true&lt;/code> and &lt;code>.Values.global.&amp;lt;GardenerControlPlaneComponent&amp;gt;.kubeconfig&lt;/code> in the respective chart&amp;rsquo;s &lt;code>values.yaml&lt;/code> file.&lt;/p>
&lt;h2 id="virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster">Virtual Garden is used, i.e., the &lt;code>runtime&lt;/code> Garden cluster is different from the &lt;code>target&lt;/code> Garden cluster.&lt;/h2>
&lt;h4 id="service-account">Service Account&lt;/h4>
&lt;p>The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the &lt;code>target&lt;/code> cluster. Then use the generated service account token and craft a &lt;code>kubeconfig&lt;/code> which will be used by the workload in the &lt;code>runtime&lt;/code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting &lt;code>.Values.global.deployment.virtualGarden.enabled: true&lt;/code> and following these steps:&lt;/p>
&lt;ol>
&lt;li>Deploy the &lt;code>application&lt;/code> part of the charts in the &lt;code>target&lt;/code> cluster.&lt;/li>
&lt;li>Get the service account token and craft the &lt;code>kubeconfig&lt;/code>.&lt;/li>
&lt;li>Set the crafted &lt;code>kubeconfig&lt;/code> and deploy the &lt;code>runtime&lt;/code> part of the charts in the &lt;code>runtime&lt;/code> cluster.&lt;/li>
&lt;/ol>
&lt;h4 id="client-certificate">Client Certificate&lt;/h4>
&lt;p>Another solution will be to bind the roles in the &lt;code>target&lt;/code> cluster to a &lt;code>User&lt;/code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both &lt;code>.Values.global.deployment.virtualGarden.enabled: true&lt;/code> and &lt;code>.Values.global.deployment.virtualGarden.&amp;lt;GardenerControlPlaneComponent&amp;gt;.user.name&lt;/code>, then following these steps:&lt;/p>
&lt;ol>
&lt;li>Generate a client certificate for the &lt;code>target&lt;/code> cluster for the respective user.&lt;/li>
&lt;li>Deploy the &lt;code>application&lt;/code> part of the charts in the &lt;code>target&lt;/code> cluster.&lt;/li>
&lt;li>Craft a &lt;code>kubeconfig&lt;/code> using the already generated client certificate.&lt;/li>
&lt;li>Set the crafted &lt;code>kubeconfig&lt;/code> and deploy the &lt;code>runtime&lt;/code> part of the charts in the &lt;code>runtime&lt;/code> cluster.&lt;/li>
&lt;/ol>
&lt;h4 id="projected-service-account-token">Projected Service Account Token&lt;/h4>
&lt;p>This approach requires an already deployed and configured &lt;a href="https://github.com/gardener/oidc-webhook-authenticator">oidc-webhook-authenticator&lt;/a> for the &lt;code>target&lt;/code> cluster. Also the &lt;code>runtime&lt;/code> cluster should be registered as a trusted identity provider in the &lt;code>target&lt;/code> cluster. Then projected service accounts tokens from the &lt;code>runtime&lt;/code> cluster can be used to authenticate against the &lt;code>target&lt;/code> cluster. The needed steps are as follows:&lt;/p>
&lt;ol>
&lt;li>Deploy &lt;a href="https://github.com/gardener/oidc-webhook-authenticator">OWA&lt;/a> and establish the needed trust.&lt;/li>
&lt;li>Set &lt;code>.Values.global.deployment.virtualGarden.enabled: true&lt;/code> and &lt;code>.Values.global.deployment.virtualGarden.&amp;lt;GardenerControlPlaneComponent&amp;gt;.user.name&lt;/code>. &lt;strong>Note:&lt;/strong> username value will depend on the trust configuration, e.g., &lt;code>&amp;lt;prefix&amp;gt;:system:serviceaccount:&amp;lt;namespace&amp;gt;:&amp;lt;serviceaccount&amp;gt;&lt;/code>&lt;/li>
&lt;li>Set &lt;code>.Values.global.&amp;lt;GardenerControlPlaneComponent&amp;gt;.serviceAccountTokenVolumeProjection.enabled: true&lt;/code> and &lt;code>.Values.global.&amp;lt;GardenerControlPlaneComponent&amp;gt;.serviceAccountTokenVolumeProjection.audience&lt;/code>. &lt;strong>Note:&lt;/strong> audience value will depend on the trust configuration, e.g., &lt;code>&amp;lt;cliend-id-from-trust-config&amp;gt;&lt;/code>.&lt;/li>
&lt;li>Craft a kubeconfig (see example below).&lt;/li>
&lt;li>Deploy the &lt;code>application&lt;/code> part of the charts in the &lt;code>target&lt;/code> cluster.&lt;/li>
&lt;li>Deploy the &lt;code>runtime&lt;/code> part of the charts in the &lt;code>runtime&lt;/code> cluster.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusters:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- cluster:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> certificate-authority-data: &amp;lt;CA-DATA&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: https://virtual-garden.api
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: virtual-garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: virtual-garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: virtual-garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: virtual-garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>current-context: virtual-garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>users:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: virtual-garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tokenFile: /var/run/secrets/projected/serviceaccount/token
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Backup Restore</title><link>https://gardener.cloud/docs/gardener/concepts/backup-restore/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/backup-restore/</guid><description>
&lt;h1 id="backup-and-restore">Backup and restore&lt;/h1>
&lt;p>Kubernetes uses Etcd as the key-value store for its resource definitions. Gardener supports the backup and restore of etcd. It is the responsibility of the shoot owners to backup the workload data.&lt;/p>
&lt;p>Gardener uses &lt;a href="https://github.com/gardener/etcd-backup-restore">etcd-backup-restore&lt;/a> component to backup the etcd backing the Shoot cluster regularly and restore in case of disaster. It is deployed as sidecar via &lt;a href="https://github.com/gardener/etcd-druid">etcd-druid&lt;/a>. This doc mainly focuses on the backup and restore configuration used by Gardener when deploying these components. For more details on the design and internal implementation details, please refer &lt;a href="https://gardener.cloud/docs/gardener/proposals/06-etcd-druid/">GEP-06&lt;/a> and documentation on individual repository.&lt;/p>
&lt;h2 id="bucket-provisioning">Bucket provisioning&lt;/h2>
&lt;p>Refer the &lt;a href="https://gardener.cloud/docs/gardener/extensions/backupbucket/">backup bucket extension document&lt;/a> to know details about configuring backup bucket.&lt;/p>
&lt;h2 id="backup-policy">Backup Policy&lt;/h2>
&lt;p>etcd-backup-restore supports full snapshot and delta snapshots over full snapshot. In Gardener, this configuration is currently hard-coded to following parameters:&lt;/p>
&lt;ul>
&lt;li>Full Snapshot Schedule:
&lt;ul>
&lt;li>Daily, &lt;code>24hr&lt;/code> interval.&lt;/li>
&lt;li>For each Shoot, the schedule time in a day is randomized based on the configured Shoot maintenance window.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Delta Snapshot schedule:
&lt;ul>
&lt;li>At &lt;code>5min&lt;/code> interval.&lt;/li>
&lt;li>If aggregated events size since last snapshot goes beyond &lt;code>100Mib&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup History / Garbage backup deletion policy:
&lt;ul>
&lt;li>Gardener configure backup restore to have &lt;code>Exponential&lt;/code> garbage collection policy.&lt;/li>
&lt;li>As per policy, following backups are retained.&lt;/li>
&lt;li>All full backups and delta backups for the previous hour.&lt;/li>
&lt;li>Latest full snapshot of each previous hour for the day.&lt;/li>
&lt;li>Latest full snapshot of each previous day for 7 days.&lt;/li>
&lt;li>Latest full snapshot of the previous 4 weeks.&lt;/li>
&lt;li>Garbage Collection is configured at &lt;code>12hr&lt;/code> interval.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Listing:
&lt;ul>
&lt;li>Gardener don&amp;rsquo;t have any API to list out the backups.&lt;/li>
&lt;li>To find the backup list, admin can checkout the &lt;code>BackupEntry&lt;/code> resource associated with Shoot which holds the bucket and prefix details on object store.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="restoration">Restoration&lt;/h2>
&lt;p>Restoration process of etcd is automated through the etcd-backup-restore component from latest snapshot. Gardener dosen&amp;rsquo;t support Point-In-Time-Recovery (PITR) of etcd. In case of etcd disaster, the etcd is recovered from latest backup automatically. For further details, please refer the &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/master/doc/proposals/restoration.md">doc&lt;/a>. Post restoration of etcd, the Shoot reconciliation loop brings back the cluster to same state.&lt;/p>
&lt;p>Again, Shoot owner is responsible for maintaining the backup/restore of his workload. Gardener does only take care of the cluster&amp;rsquo;s etcd.&lt;/p></description></item><item><title>Docs: Changing the APIs</title><link>https://gardener.cloud/docs/gardener/development/changing-the-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/changing-the-api/</guid><description>
&lt;h1 id="extending-the-api">Extending the API&lt;/h1>
&lt;p>This document describes the steps that need to be performed when changing the API.
It provides guidance for API changes to both (Gardener system in general or component configurations).&lt;/p>
&lt;p>Generally, as Gardener is a Kubernetes-native extension, it follows the same API conventions and guidelines like Kubernetes itself.
&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md">This document&lt;/a> as well as &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md">this document&lt;/a> already provide a good overview and general explanation of the basic concepts behind it.
We are following the same approaches.&lt;/p>
&lt;h2 id="gardener-api">Gardener API&lt;/h2>
&lt;p>The Gardener API is defined in &lt;code>pkg/apis/{core,extensions,settings}&lt;/code> directories and is the main point of interaction with the system.
It must be ensured that the API is always backwards-compatible.
If fields shall be removed permanently from the API then a proper deprecation period must be adhered to so that end-users have enough time adapt their clients.&lt;/p>
&lt;p>&lt;strong>Checklist&lt;/strong> when changing the API:&lt;/p>
&lt;ol>
&lt;li>Modify the field(s) in the respective Golang files of all external and the internal version.
&lt;ol>
&lt;li>Make sure new fields are being added as &amp;ldquo;optional&amp;rdquo; fields, i.e., they are of pointer types, they have the &lt;code>// +optional&lt;/code> comment, and they have the &lt;code>omitempty&lt;/code> JSON tag.&lt;/li>
&lt;li>Make sure that the existing field numbers in the protobuf tags are not changed.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>If necessary then implement/adapt the conversion logic defined in the versioned APIs (e.g., &lt;code>pkg/apis/core/v1beta1/conversions*.go&lt;/code>).&lt;/li>
&lt;li>If necessary then implement/adapt defaulting logic defined in the versioned APIs (e.g., &lt;code>pkg/apis/core/v1beta1/defaults*.go&lt;/code>).&lt;/li>
&lt;li>Run the code generation: &lt;code>make generate&lt;/code>&lt;/li>
&lt;li>If necessary then implement/adapt validation logic defined in the internal API (e.g., &lt;code>pkg/apis/core/validation/validation*.go&lt;/code>).&lt;/li>
&lt;li>If necessary then adapt the exemplary YAML manifests of the Gardener resources defined in &lt;code>example/*.yaml&lt;/code>.&lt;/li>
&lt;li>In most cases it makes sense to add/adapt the documentation for administrators/operators and/or end-users in the &lt;code>docs&lt;/code> folder to provide information on purpose and usage of the added/changed fields.&lt;/li>
&lt;li>When opening the pull request then always add a release note so that end-users are becoming aware of the changes.&lt;/li>
&lt;/ol>
&lt;h2 id="component-configuration-apis">Component configuration APIs&lt;/h2>
&lt;p>Most Gardener components have a component configuration that follows similar principles to the Gardener API.
Those component configurations are defined in &lt;code>pkg/{controllermanager,gardenlet,scheduler},pkg/apis/config&lt;/code>.
Hence, the above checklist also applies for changes to those APIs.
However, since these APIs are only used internally and only during the deployment of Gardener the guidelines with respect to changes and backwards-compatibility are slightly relaxed.
If necessary then it is allowed to remove fields without a proper deprecation period if the release note uses the &lt;code>breaking operator&lt;/code> keywords.&lt;/p>
&lt;p>In addition to the above checklist:&lt;/p>
&lt;ol>
&lt;li>If necessary then adapt the Helm chart of Gardener defined in &lt;code>charts/gardener&lt;/code>. Adapt the &lt;code>values.yaml&lt;/code> file as well as the manifest templates.&lt;/li>
&lt;/ol></description></item><item><title>Docs: Cluster API</title><link>https://gardener.cloud/docs/gardener/concepts/cluster-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/cluster-api/</guid><description>
&lt;h1 id="relation-between-gardener-api-and-cluster-api-sig-cluster-lifecycle">Relation between Gardener API and Cluster API (SIG Cluster Lifecycle)&lt;/h1>
&lt;p>In essence, the Cluster API harmonizes how to get to clusters, while Gardener goes one step further and also harmonizes the clusters themselves. The Cluster API delegates the specifics to so-called providers for infrastructures or control planes via specific CR(D)s while Gardener only has one cluster CR(D). Different Cluster API providers, e.g. for AWS, Azure, GCP, etc. give you vastly different Kubernetes clusters. In contrast, Gardener gives you the exact same clusters with the exact same K8s version, operating system, control plane configuration like for API server or kubelet, add-ons like overlay network, HPA/VPA, DNS and certificate controllers, ingress and network policy controllers, control plane monitoring and logging stacks, down to the behavior of update procedures, auto-scaling, self-healing, etc. on all supported infrastructures. These homogeneous clusters are an essential goal for Gardener as its main purpose is to simplify operations for teams that need to develop and ship software on Kubernetes clusters on a plethora of infrastructures (a.k.a. multi-cloud).&lt;/p>
&lt;p>Incidentally, Gardener influenced the Machine API in the Cluster API with its &lt;a href="https://github.com/gardener/machine-controller-manager">Machine Controller Manager&lt;/a> and was the &lt;a href="https://github.com/kubernetes-sigs/cluster-api/commit/00b1ead264aea6f88585559056c180771cce3815">first to adopt it&lt;/a>, see also &lt;a href="https://www.youtube.com/watch?v=Mtg8jygK3Hs">joint SIG Cluster Lifecycle KubeCon talk&lt;/a> where @hardikdr from our Gardener team in India spoke.&lt;/p>
&lt;p>That means, we follow the &lt;a href="https://github.com/kubernetes-sigs/cluster-api#cluster-api">Cluster API&lt;/a> with great interest and are active members. It was completely overhauled from &lt;code>v1alpha1&lt;/code> to &lt;code>v1alpha2&lt;/code>. But because &lt;code>v1alpha2&lt;/code> made too many assumptions about the bring-up of masters and was enforcing master machine operations (see &lt;a href="https://cluster-api.sigs.k8s.io/user/concepts.html#control-plane">here&lt;/a>: “As of &lt;code>v1alpha2&lt;/code>, Machine-Based is the only control plane type that Cluster API supports”), services that managed their control planes differently like GKE or Gardener couldn&amp;rsquo;t adopt it (e.g. &lt;a href="https://cloud.google.com/anthos/gke/docs/on-prem/concepts/cluster-api">Google only supports &lt;code>v1alpha1&lt;/code>&lt;/a>). In 2020 &lt;a href="https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience/">&lt;code>v1alpha3&lt;/code>&lt;/a> was introduced and made it possible (again) to integrate managed services like GKE or Gardener. The mapping from the Gardener API to the Cluster API is mostly syntactic.&lt;/p>
&lt;p>To wrap it up, while the Cluster API knows about clusters, it doesn&amp;rsquo;t know about their make-up. With Gardener, we wanted to go beyond that and harmonize the make-up of the clusters themselves and make them homogeneous across all supported infrastructures. Gardener can therefore deliver homogeneous clusters with exactly the same configuration and behavior on all infrastructures (see also &lt;a href="https://k8s-testgrid.appspot.com/conformance-all">Gardener&amp;rsquo;s coverage in the official conformance test grid&lt;/a>).&lt;/p>
&lt;p>With &lt;a href="https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience">Cluster API &lt;code>v1alpha3&lt;/code>&lt;/a> and the support for declarative control plane management, it became now possible (again) to enable Kubernetes managed services like GKE or Gardener. We would be more than happy, if the community would be interested, to contribute a Gardener control plane provider.&lt;/p></description></item><item><title>Docs: Configuring Logging</title><link>https://gardener.cloud/docs/gardener/deployment/configuring_logging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/configuring_logging/</guid><description>
&lt;h1 id="configuring-the-logging-stack-via-gardenlet-configurations">Configuring the Logging stack via Gardenlet configurations&lt;/h1>
&lt;h1 id="enable-the-logging">Enable the Logging&lt;/h1>
&lt;p>In order to install the Gardener logging stack the &lt;code>logging.enabled&lt;/code> configuration option has to be enabled in the Gardenlet configuration:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>logging:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>From now on each Seed is going to have a logging stack which will collect logs from all pods and some systemd services. Logs related to Shoots with &lt;code>testing&lt;/code> purpose are dropped in the &lt;code>fluent-bit&lt;/code> output plugin. Shoots with a purpose different than &lt;code>testing&lt;/code> have the same type of log aggregator (but different instance) as the Seed. The logs can be viewed in the Grafana in the &lt;code>garden&lt;/code> namespace for the Seed components and in the respective shoot control plane namespaces.&lt;/p>
&lt;h1 id="enable-logs-from-the-shoots-node-systemd-services">Enable logs from the Shoot&amp;rsquo;s node systemd services.&lt;/h1>
&lt;p>The logs from the systemd services on each node can be retrieved by enabling the &lt;code>logging.shootNodeLogging&lt;/code> option in the Gardenlet configuration:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>logging:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shootNodeLogging:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shootPurposes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#a31515">&amp;#34;evaluation&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#a31515">&amp;#34;deployment&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Under the &lt;code>shootPurpose&lt;/code> section just list all the shoot purposes for which the Shoot node logging feature will be enabled. Specifying the &lt;code>testing&lt;/code> purpose has no effect because this purpose prevents the logging stack installation.
Logs can be viewed in the operator Grafana!
The dedicated labels are &lt;code>unit&lt;/code>, &lt;code>syslog_identifier&lt;/code> and &lt;code>nodename&lt;/code> in the &lt;code>Explore&lt;/code> menu.&lt;/p>
&lt;h1 id="configuring-the-log-processor">Configuring the log processor&lt;/h1>
&lt;p>Under &lt;code>logging.fluentBit&lt;/code> there is three optional sections.&lt;/p>
&lt;ul>
&lt;li>&lt;code>input&lt;/code>: This overwrite the input configuration of the fluent-bit log processor.&lt;/li>
&lt;li>&lt;code>output&lt;/code>: This overwrite the output configuration of the fluent-bit log processor.&lt;/li>
&lt;li>&lt;code>service&lt;/code>: This overwrite the service configuration of the fluent-bit log processor.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>logging:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fluentBit:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output: |-&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> [Output]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> input: |-&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> [Input]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service: |-&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> [Service]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> ...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="configuring-the-loki-priorityclass">Configuring the Loki PriorityClass&lt;/h1>
&lt;p>The central Loki, which is in the &lt;code>garden&lt;/code> namespace, contains all the logs from the most important seed components. When the central Loki &lt;code>PriorityClass&lt;/code> is with low value then its pods can be preempted and often moved from one node to another while Kubernetes tries to free space for more important pods. The persistent volume will be detached/attached again as well. Based on the performance of the underlying infrastructure, this leads to great central Loki downtime. To give greater priority of the seed Loki you can use the &lt;code>logging.loki.garden.priority&lt;/code> option.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>logging:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> loki:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> garden:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> priority: 100
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="configure-central-logging">Configure central logging&lt;/h1>
&lt;p>For central logging, the output configuration of the fluent-bit log processor can be overwritten (&lt;code>logging.fluentBit.output&lt;/code>) and the Loki instances deployments in Garden and Shoot namespace can be enabled/disabled (&lt;code>logging.loki.enabled&lt;/code>), by default Loki is enabled.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>logging:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fluentBit:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output: |-&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> [Output]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> loki:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="configuring-central-loki-storage-capacity">Configuring central Loki storage capacity&lt;/h1>
&lt;p>By default, the central Loki has &lt;code>100Gi&lt;/code> of storage capacity.
To overwrite the current central Loki storage capacity, the &lt;code>logging.loki.garden.storage&lt;/code> setting in the gardenlet&amp;rsquo;s component configuration should be altered.
If you need to increase it you can do so without losing the current data by specifying higher capacity. Doing so, the Loki&amp;rsquo;s &lt;code>PersistentVolume&lt;/code> capacity will be increased instead of deleting the current PV.
However, if you specify less capacity then the &lt;code>PersistentVolume&lt;/code> will be deleted and with it the logs, too.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>logging:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fluentBit:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> output: |-&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> [Output]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> loki:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> garden:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> storage: &lt;span style="color:#a31515">&amp;#34;200Gi&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Controller Manager</title><link>https://gardener.cloud/docs/gardener/concepts/controller-manager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/controller-manager/</guid><description>
&lt;h1 id="gardener-controller-manager">Gardener Controller Manager&lt;/h1>
&lt;p>The Gardener Controller Manager (often refered to as &amp;ldquo;GCM&amp;rdquo;) is a component that runs next to the Gardener API server, similar to the Kubernetes Controller Manager.
It runs several control loops that do not require talking to any seed or shoot cluster.
Also, as of today it exposes a HTTPS server that is serving several endpoints for webhooks for certain resources.&lt;/p>
&lt;p>This document explains the various functionalities of the Gardener Controller Manager and their purpose.&lt;/p>
&lt;h2 id="control-loops">Control Loops&lt;/h2>
&lt;h3 id="project-controller">&lt;code>Project&lt;/code> Controller&lt;/h3>
&lt;p>This controller consists out of three reconciliation loops:
The main loop is reconciling &lt;code>Project&lt;/code> resources while the second loop is controlling the necessary actions for stale projects.&lt;/p>
&lt;h4 id="main-reconciler">&amp;ldquo;Main&amp;rdquo; Reconciler&lt;/h4>
&lt;p>This reconciler will create a dedicated &lt;code>Namespace&lt;/code> prefixed with &lt;code>garden-&lt;/code> for each &lt;code>Project&lt;/code> resource.
The name of the namespace can either be stated in the &lt;code>.spec.namespace&lt;/code>, or it will be auto-generated by the reconciler.
If &lt;code>.spec.namespace&lt;/code> is set then it creates it if it does not exist yet.
Otherwise, it tries to adopt it.
This will only succeed if the &lt;code>Namespace&lt;/code> was previously labeled with &lt;code>gardener.cloud/role=project&lt;/code> and &lt;code>project.gardener.cloud/name=&amp;lt;project-name&amp;gt;&lt;/code>.
This is to prevent that end-users can adopt arbitrary namespaces and escalate their privileges, e.g. the &lt;code>kube-system&lt;/code> namespace.&lt;/p>
&lt;p>After the namespace was created/adopted the reconciler creates several &lt;code>ClusterRole&lt;/code>s and &lt;code>ClusterRoleBinding&lt;/code>s that allow the project members to access related resources based on their roles.
These RBAC resources are prefixed with &lt;code>gardener.cloud:system:project{-member,-viewer}:&amp;lt;project-name&amp;gt;&lt;/code>.
Gardener administrators and extension developers can define their own roles, see &lt;a href="https://gardener.cloud/docs/gardener/extensions/project-roles/">this document&lt;/a> for more information.&lt;/p>
&lt;p>In addition, operators can configure the Project controller to maintain a default &lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">ResourceQuota&lt;/a> for project namespaces.
Quotas can especially limit the creation of user facing resources, e.g. &lt;code>Shoots&lt;/code>, &lt;code>SecretBindings&lt;/code>, &lt;code>Secrets&lt;/code> and thus protect the Garden cluster from massive resource exhaustion but also enable operators to align quotas with respective enterprise policies.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ &lt;strong>Gardener itself is not exempted from configured quotas&lt;/strong>. For example, Gardener creates &lt;code>Secrets&lt;/code> for every shoot cluster in the project namespace and at the same time increases the available quota count. Please mind this additional resource consumption.&lt;/p>
&lt;/blockquote>
&lt;p>The GCM configuration provides a template section &lt;code>controllers.project.quotas&lt;/code> where such a ResourceQuota (see example below) can be deposited.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>controllers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> project:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> quotas:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - config:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: ResourceQuota
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hard:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> count/shoots.core.gardener.cloud: &lt;span style="color:#a31515">&amp;#34;100&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> count/secretbindings.core.gardener.cloud: &lt;span style="color:#a31515">&amp;#34;10&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> count/secrets: &lt;span style="color:#a31515">&amp;#34;800&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> projectSelector: {}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The Project controller takes the shown &lt;code>config&lt;/code> and creates a &lt;code>ResourceQuota&lt;/code> with the name &lt;code>gardener&lt;/code> in the project namespace.
If a &lt;code>ResourceQuota&lt;/code> resource with the name &lt;code>gardener&lt;/code> already exists, the controller will only update fields in &lt;code>spec.hard&lt;/code> which are &lt;strong>unavailable&lt;/strong> at that time.
Labels and annotations on the &lt;code>ResourceQuota&lt;/code> &lt;code>config&lt;/code> get merged with the respective fields on existing &lt;code>ResourceQuota&lt;/code>s.
An optional &lt;code>projectSelector&lt;/code> narrows down the amount of projects that are equipped with the given &lt;code>config&lt;/code>.
If multiple configs match for a project, then only the first match in the list is applied to the project namespace.&lt;/p>
&lt;p>The &lt;code>.status.phase&lt;/code> of the &lt;code>Project&lt;/code> resources will be set to &lt;code>Ready&lt;/code> or &lt;code>Failed&lt;/code> by the reconciler to indicate whether the reconciliation loop was performed successfully.
Also, it will generate &lt;code>Event&lt;/code>s to provide further information about its operations.&lt;/p>
&lt;h4 id="stale-projects-reconciler">&amp;ldquo;Stale Projects&amp;rdquo; Reconciler&lt;/h4>
&lt;p>As Gardener is a large-scale Kubernetes as a Service it is designed for being used by a large amount of end-users.
Over time, it is likely to happen that some of the hundreds or thousands of &lt;code>Project&lt;/code> resources are no longer actively used.&lt;/p>
&lt;p>Gardener offers the &amp;ldquo;stale projects&amp;rdquo; reconciler which will take care of identifying such stale projects, marking them with a &amp;ldquo;warning&amp;rdquo;, and eventually deleting them after a certain time period.
This reconciler is enabled by default and works as following:&lt;/p>
&lt;ol>
&lt;li>Projects are considered as &amp;ldquo;stale&amp;rdquo;/not actively used when all of the following conditions apply: The namespace associated with the &lt;code>Project&lt;/code> does not have any&amp;hellip;
&lt;ol>
&lt;li>&lt;code>Shoot&lt;/code> resources.&lt;/li>
&lt;li>&lt;code>Plant&lt;/code> resources.&lt;/li>
&lt;li>&lt;code>BackupEntry&lt;/code> resources.&lt;/li>
&lt;li>&lt;code>Secret&lt;/code> resources that are referenced by a &lt;code>SecretBinding&lt;/code> that is in use by a &lt;code>Shoot&lt;/code> (not necessarily in the same namespace).&lt;/li>
&lt;li>&lt;code>Quota&lt;/code> resources that are referenced by a &lt;code>SecretBinding&lt;/code> that is in use by a &lt;code>Shoot&lt;/code> (not necessarily in the same namespace).&lt;/li>
&lt;li>The time period when the projet was used for the last time (&lt;code>status.lastActivityTimestamp&lt;/code>) is longer than the configured &lt;code>minimumLifetimeDays&lt;/code>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>If a project is considered &amp;ldquo;stale&amp;rdquo; then its &lt;code>.status.staleSinceTimestamp&lt;/code> will be set to the time when it was first detected to be stale.
If it gets actively used again this timestamp will be removed.
After some time the &lt;code>.status.staleAutoDeleteTimestamp&lt;/code> will be set to a timestamp after which Gardener will auto-delete the &lt;code>Project&lt;/code> resource if it still is not actively used.&lt;/p>
&lt;p>The component configuration of the Gardener Controller Manager offers to configure the following options:&lt;/p>
&lt;ul>
&lt;li>&lt;code>minimumLifetimeDays&lt;/code>: Don&amp;rsquo;t consider newly created &lt;code>Project&lt;/code>s as &amp;ldquo;stale&amp;rdquo; too early to give people/end-users some time to onboard and get familiar with the system. The &amp;ldquo;stale project&amp;rdquo; reconciler won&amp;rsquo;t set any timestamp for &lt;code>Project&lt;/code>s younger than &lt;code>minimumLifetimeDays&lt;/code>. When you change this value then projects marked as &amp;ldquo;stale&amp;rdquo; may be no longer marked as &amp;ldquo;stale&amp;rdquo; in case they are young enough, or vice versa.&lt;/li>
&lt;li>&lt;code>staleGracePeriodDays&lt;/code>: Don&amp;rsquo;t compute auto-delete timestamps for stale &lt;code>Project&lt;/code>s that are unused for only less than &lt;code>staleGracePeriodDays&lt;/code>. This is to not unnecessarily make people/end-users nervous &amp;ldquo;just because&amp;rdquo; they haven&amp;rsquo;t actively used their &lt;code>Project&lt;/code> for a given amount of time. When you change this value then already assigned auto-delete timestamps may be removed again if the new grace period is not yet exceeded.&lt;/li>
&lt;li>&lt;code>staleExpirationTimeDays&lt;/code>: Expiration time after which stale &lt;code>Project&lt;/code>s are finally auto-deleted (after &lt;code>.status.staleSinceTimestamp&lt;/code>). If this value is changed and an auto-delete timestamp got already assigned to the projects then the new value will only take effect if it&amp;rsquo;s increased. Hence, decreasing the &lt;code>staleExpirationTimeDays&lt;/code> will not decrease already assigned auto-delete timestamps.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Gardener administrators/operators can exclude specific &lt;code>Project&lt;/code>s from the stale check by annotating the related &lt;code>Namespace&lt;/code> resource with &lt;code>project.gardener.cloud/skip-stale-check=true&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;h4 id="activity-reconciler">&amp;ldquo;Activity&amp;rdquo; Reconciler&lt;/h4>
&lt;p>Since the other two reconcilers are unable to actively monitor the relevant objects that are used in a &lt;code>Project&lt;/code> (&lt;code>Shoot&lt;/code>, &lt;code>Plant&lt;/code>, etc.), there could be a situation where the user creates and deletes objects in a short period of time. In that case the &lt;code>Stale Project Reconciler&lt;/code> could not see that there was any activity on that project and it will still mark it as a &lt;code>Stale&lt;/code>, even though it is actively used.&lt;/p>
&lt;p>The &lt;code>Project Activity Reconciler&lt;/code> is implemented to take care of such cases. An event handler will notify the reconciler for any acitivity (Currently only for &lt;code>Shoots&lt;/code>) and then it will update the &lt;code>status.lastActivityTimestamp&lt;/code>. This update will also trigger the &lt;code>Stale Project Reconciler&lt;/code>.&lt;/p>
&lt;h3 id="event-controller">Event Controller&lt;/h3>
&lt;p>With the Gardener Event Controller you can prolong the lifespan of events related to Shoot clusters.
This is an optional controller which will become active once you provide the below mentioned configuration.&lt;/p>
&lt;p>All events in K8s are deleted after a configurable time-to-live (controlled via a &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver argument&lt;/a> called &lt;code>--event-ttl&lt;/code> (defaulting to 1 hour)).
The need to prolong the time-to-live for Shoot cluster events frequently arises when debugging customer issues on live systems.
This controller leaves events involving Shoots untouched while deleting all other events after a configured time.
In order to activate it, provide the following configuration:&lt;/p>
&lt;ul>
&lt;li>&lt;code>concurrentSyncs&lt;/code>: The amount of goroutines scheduled for reconciling events.&lt;/li>
&lt;li>&lt;code>ttlNonShootEvents&lt;/code>: When an event reaches this time-to-live it gets deleted unless it is a Shoot-related event (defaults to &lt;code>1h&lt;/code>, equivalent to the &lt;code>event-ttl&lt;/code> default).&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>⚠️ In addition, you should also configure the &lt;code>--event-ttl&lt;/code> for the kube-apiserver to define an upper-limit of how long Shoot-related events should be stored.
The &lt;code>--event-ttl&lt;/code> should be larger than the &lt;code>ttlNonShootEvents&lt;/code> or this controller will have no effect.&lt;/p>
&lt;/blockquote>
&lt;h3 id="shoot-reference-controller">Shoot Reference Controller&lt;/h3>
&lt;p>Shoot objects may specify references to further objects in the Garden cluster which are required for certain features.
For example, users can configure various DNS providers via &lt;code>.spec.dns.providers&lt;/code> and usually need to refer to a corresponding &lt;code>secret&lt;/code> with valid DNS provider credentials inside.
Such objects need a special protection against deletion requests as long as they are still being referenced by one or multiple shoots.&lt;/p>
&lt;p>Therefore, the Shoot Reference Controller scans shoot clusters for referenced objects and adds the finalizer &lt;code>gardener.cloud/reference-protection&lt;/code> to their &lt;code>.metadata.finalizers&lt;/code> list.
The scanned shoot also gets this finalizer to enable a proper garbage collection in case the Gardener-Controller-Manager is offline at the moment of an incoming deletion request.
When an object is not actively referenced anymore because the shoot specification has changed or all related shoots were deleted (are in deletion), the controller will remove the added finalizer again, so that the object can safely be deleted or garbage collected.&lt;/p>
&lt;p>The Shoot Reference Controller inspects the following references:&lt;/p>
&lt;ul>
&lt;li>DNS provider secrets (&lt;code>.spec.dns.provider&lt;/code>)&lt;/li>
&lt;li>Audit policy configmaps (&lt;code>.spec.kubernetes.kubeAPIServer.auditConfig.auditPolicy.configMapRef&lt;/code>)&lt;/li>
&lt;/ul>
&lt;p>Further checks might be added in the future.&lt;/p>
&lt;h3 id="shoot-retry-controller">Shoot Retry Controller&lt;/h3>
&lt;p>The Shoot Retry Controller is responsible for retrying certain failed Shoots. Currently the controller retries only failed Shoots with error code &lt;code>ERR_INFRA_RATE_LIMITS_EXCEEDED&lt;/code>.&lt;/p>
&lt;h3 id="seed-controller">Seed Controller&lt;/h3>
&lt;p>The Seed controller in the Gardener Controller Manager reconciles &lt;code>Seed&lt;/code> objects with the help of the following reconcilers.&lt;/p>
&lt;h4 id="main-reconciler-1">&amp;ldquo;Main&amp;rdquo; Reconciler&lt;/h4>
&lt;p>This reconciliation loop takes care about seed related operations in the Garden cluster. When a new &lt;code>Seed&lt;/code> object is created
the reconciler creates a new &lt;code>Namespace&lt;/code> in the garden cluster &lt;code>seed-&amp;lt;seed-name&amp;gt;&lt;/code>. &lt;code>Namespaces&lt;/code> dedicated to single
seed clusters allow us to segregate access permissions i.e., a Gardenlet must not have permissions to access objects in
all &lt;code>Namespaces&lt;/code> in the Garden cluster.
There are objects in a Garden environment which are created once by the operator e.g., default domain secret,
alerting credentials, and required for operations happening in the Gardenlet. Therefore, we not only need a seed specific
&lt;code>Namespace&lt;/code> but also a copy of these &amp;ldquo;shared&amp;rdquo; objects.&lt;/p>
&lt;p>The &amp;ldquo;main&amp;rdquo; reconciler takes care about this replication:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">Kind&lt;/th>
&lt;th style="text-align:center">Namespace&lt;/th>
&lt;th style="text-align:center">Label Selector&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">Secret&lt;/td>
&lt;td style="text-align:center">garden&lt;/td>
&lt;td style="text-align:center">gardener.cloud/role&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="backup-bucket-reconciler">&amp;ldquo;Backup Bucket&amp;rdquo; Reconciler&lt;/h4>
&lt;p>Every time a &lt;code>BackupBucket&lt;/code> object is created or updated, the referenced &lt;code>Seed&lt;/code> object is enqueued for reconciliation.
It&amp;rsquo;s the reconciler&amp;rsquo;s task to check the &lt;code>status&lt;/code> subresource of all existing &lt;code>BackupBuckets&lt;/code> that belong to this seed.
If at least one &lt;code>BackupBucket&lt;/code> has &lt;code>.status.lastError&lt;/code>, the seed condition &lt;code>BackupBucketsReady&lt;/code> will turn &lt;code>false&lt;/code> and
consequently the seed is considered as &lt;code>NotReady&lt;/code>. Once the &lt;code>BackupBucket&lt;/code> is healthy again, the seed will be re-queued
and the condition will turn &lt;code>true&lt;/code>.&lt;/p>
&lt;h4 id="lifecycle-reconciler">&amp;ldquo;Lifecycle&amp;rdquo; Reconciler&lt;/h4>
&lt;p>The &amp;ldquo;Lifecycle&amp;rdquo; reconciler processes &lt;code>Seed&lt;/code> objects which are enqueued every 10 seconds in order to check if the responsible
Gardenlet is still responding and operable. Therefore, it checks renewals via &lt;code>Lease&lt;/code> objects of the seed in the garden cluster
which are renewed regularly by the Gardenlet.&lt;/p>
&lt;p>In case a &lt;code>Lease&lt;/code> is not renewed for the configured amount in &lt;code>config.controllers.seed.monitorPeriod.duration&lt;/code>:&lt;/p>
&lt;ol>
&lt;li>The reconciler assumes that the Gardenlet stopped operating and updates the &lt;code>GardenletReady&lt;/code> condition to &lt;code>Unknown&lt;/code>.&lt;/li>
&lt;li>Additionally, conditions and constraints of all &lt;code>Shoot&lt;/code> resources scheduled on the affected seed are set to &lt;code>Unknown&lt;/code> as well
because a striking Gardenlet won&amp;rsquo;t be able to maintain these conditions any more.&lt;/li>
&lt;li>If the gardenlet&amp;rsquo;s client certificate has expired (identified based on the &lt;code>.status.clientCertificateExpirationTimestamp&lt;/code> field in the &lt;code>Seed&lt;/code> resource) and if it is managed by a &lt;code>ManagedSeed&lt;/code> then this will be triggered for a reconciliation. This will trigger the bootstrapping process again and allows gardenlets to obtain a fresh client certificate.&lt;/li>
&lt;/ol>
&lt;h3 id="controllerregistration-controller">ControllerRegistration Controller&lt;/h3>
&lt;p>The &lt;code>ControllerRegistration&lt;/code> controller makes sure that the required &lt;a href="https://github.com/gardener/gardener/blob/master/docs/README.md#extensions">Gardener extensions&lt;/a> specified by the &lt;a href="https://gardener.cloud/docs/gardener/extensions/controllerregistration/">&lt;code>ControllerRegistration&lt;/code>&lt;/a> resources are present in the seed clusters. It also takes care of the creation and deletion of &lt;code>ControllerInstallation&lt;/code> objects for a given seed cluster.
The controller has three reconciliation loops.&lt;/p>
&lt;h4 id="main-reconciler-2">&amp;ldquo;Main&amp;rdquo; Reconciler&lt;/h4>
&lt;p>This reconciliation loop watches the &lt;code>Seed&lt;/code> objects and determines which &lt;code>ControllerRegistrations&lt;/code> are required for them and creates/deletes the corresponding extension controller to reach the determined state. To begin with, it computes the kind/type combinations of extensions required for the seed. For this, the controller examines a live list of &lt;code>ControllerRegistration&lt;/code>s, &lt;code>ControllerInstallation&lt;/code>s, &lt;code>BackupBucket&lt;/code>s, &lt;code>BackupEntry&lt;/code>s, &lt;code>Shoot&lt;/code>s, and &lt;code>Secret&lt;/code>s from the garden cluster. For example, it examines the shoots running on the seed and deducts kind/type like &lt;code>Infrastructure/gcp&lt;/code>. It also decides whether they should always be deployed based on the &lt;code>.spec.deployment.policy&lt;/code>.
For the configuration options, please see this &lt;a href="https://gardener.cloud/docs/gardener/extensions/controllerregistration/#deployment-configuration-options">section&lt;/a>.&lt;/p>
&lt;p>Based on these required combinations, each of them are mapped to &lt;code>ControllerRegistration&lt;/code> objects and then to their corresponding &lt;code>ControllerInstallation&lt;/code> objects (if existing). The controller then creates or updates the required &lt;code>ControllerInstallation&lt;/code> objects for the given seed. It also deletes every existing &lt;code>ControllerInstallation&lt;/code> whose referenced &lt;code>ControllerRegistration&lt;/code> is not part of the required list. For example, if the shoots in the seed are no longer using the DNS provider &lt;code>aws-route53&lt;/code>, then the controller proceeds to delete the respective &lt;code>ControllerInstallation&lt;/code> object.&lt;/p>
&lt;h4 id="controllerregistration-reconciler">&amp;ldquo;ControllerRegistration&amp;rdquo; Reconciler&lt;/h4>
&lt;p>This reconciliation loop watches the &lt;code>ControllerRegistration&lt;/code> resource and adds finalizers to it when they are created. In case a deletion request comes in for the resource, i.e., if a &lt;code>.metadata.deletionTimestamp&lt;/code> is set, it actively scans for a &lt;code>ControllerInstallation&lt;/code> resource using this &lt;code>ControllerRegistration&lt;/code>, and decides whether the deletion can be allowed. In case no related &lt;code>ControllerInstallation&lt;/code> is present, it removes the finalizer and marks it for deletion.&lt;/p>
&lt;h4 id="seed-reconciler">&amp;ldquo;Seed&amp;rdquo; Reconciler&lt;/h4>
&lt;p>This loop also watches the &lt;code>Seed&lt;/code> object and adds finalizers to it at creation. If a &lt;code>.metadata.deletionTimestamp&lt;/code> is set for the seed then the controller checks for existing &lt;code>ControllerInstallation&lt;/code> objects which reference this seed. If no such objects exist then it removes the finalizer and allows the deletion.&lt;/p>
&lt;h3 id="certificatesigningrequest-controller">&amp;ldquo;CertificateSigningRequest&amp;rdquo; controller&lt;/h3>
&lt;p>After the &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/">gardenlet&lt;/a> gets deployed on the Seed cluster it needs to establish itself as a trusted party to communicate with the Gardener API server. It runs through a bootstrap flow similar to the &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/">kubelet bootstrap&lt;/a> process.&lt;/p>
&lt;p>On startup the gardenlet uses a &lt;code>kubeconfig&lt;/code> with a &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/">bootstrap token&lt;/a> which authenticates it as being part of the &lt;code>system:bootstrappers&lt;/code> group. This kubeconfig is used to create a &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">&lt;code>CertificateSigningRequest&lt;/code>&lt;/a> (CSR) against the Gardener API server.&lt;/p>
&lt;p>The controller in &lt;code>gardener-controller-manager&lt;/code> checks whether the &lt;code>CertificateSigningRequest&lt;/code> has the expected organisation, common name and usages which the gardenlet would request.&lt;/p>
&lt;p>It only auto-approves the CSR if the client making the request is allowed to &amp;ldquo;create&amp;rdquo; the
&lt;code>certificatesigningrequests/seedclient&lt;/code> subresource. Clients with the &lt;code>system:bootstrappers&lt;/code> group are bound to the &lt;code>gardener.cloud:system:seed-bootstrapper&lt;/code> &lt;code>ClusterRole&lt;/code>, hence, they have such privileges. As the bootstrap kubeconfig for the gardenlet contains a bootstrap token which is authenticated as being part of the &lt;a href="https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/clusterrolebinding-seed-bootstrapper.yaml">&lt;code>systems:bootstrappers&lt;/code> group&lt;/a>, its created CSR gets auto-approved.&lt;/p>
&lt;h3 id="bastion-controller">&amp;ldquo;Bastion&amp;rdquo; Controller&lt;/h3>
&lt;p>&lt;code>Bastion&lt;/code> resources have a limited lifetime, which can be extended up to a certain amount by performing a heartbeat on
them. The &lt;code>Bastion&lt;/code> controller is responsible for deleting expired or rotten &lt;code>Bastion&lt;/code>s.&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;expired&amp;rdquo; means a &lt;code>Bastion&lt;/code> has exceeded its &lt;code>status.ExpirationTimestamp&lt;/code>.&lt;/li>
&lt;li>&amp;ldquo;rotten&amp;rdquo; means a &lt;code>Bastion&lt;/code> is older than the configured &lt;code>maxLifetime&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>The &lt;code>maxLifetime&lt;/code> is an option on the &lt;code>Bastion&lt;/code> controller and defaults to 24 hours.&lt;/p>
&lt;p>The deletion triggers the gardenlet to perform the necessary cleanups in the Seed cluster, so some time can pass between
deletion and the &lt;code>Bastion&lt;/code> actually disappearing. Clients like &lt;code>gardenctl&lt;/code> are advised to not re-use &lt;code>Bastion&lt;/code>s whose
deletion timestamp has been set already.&lt;/p>
&lt;p>Refer to &lt;a href="https://gardener.cloud/docs/gardener/proposals/15-manage-bastions-and-ssh-key-pair-rotation/">GEP-15&lt;/a> for more information on the lifecycle of
&lt;code>Bastion&lt;/code> resources.&lt;/p>
&lt;h3 id="plant-controller">&amp;ldquo;Plant&amp;rdquo; Controller&lt;/h3>
&lt;p>Using the &lt;code>Plant&lt;/code> resource, an external Kubernetes cluster (not managed by Gardener) can be registered to Gardener. Gardener Controller Manager is the component that is responsible for the &lt;code>Plant&lt;/code> resource reconciliation. As part of the reconciliation loop, the Gardener Controller Manager performs health checks on the external Kubernetes cluster and gathers more information about it - all of this information serves for monitoring purposes of the external Kubernetes cluster.&lt;/p>
&lt;p>The component configuration of the Gardener Controller Manager offers to configure the following options for the plant controller:&lt;/p>
&lt;ul>
&lt;li>&lt;code>syncPeriod&lt;/code>: The duration of how often the Plant resource is reconciled, i.e., how often health checks are performed. The default value is &lt;code>30s&lt;/code>.&lt;/li>
&lt;li>&lt;code>concurrentSyncs&lt;/code>: The number of goroutines scheduled for reconciling events, i.e., the number of possible parallel reconciliations. The default value is &lt;code>5&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>The &lt;code>Plant&lt;/code> resource reports the following information for the external Kubernetes cluster:&lt;/p>
&lt;ul>
&lt;li>Cluster information
&lt;ul>
&lt;li>Cloud provider information - the cloud provider type and region are maintained in the &lt;code>Plant&lt;/code> status (&lt;code>.status.clusterInfo.cloud&lt;/code>).&lt;/li>
&lt;li>Kubernetes version - the Kubernetes version is maintained in the &lt;code>Plant&lt;/code> status (&lt;code>.status.clusterInfo.kubernetes.version&lt;/code>).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Cluster status
&lt;ul>
&lt;li>API Server availability - maintained as condition with type &lt;code>APIServerAvailable&lt;/code>.&lt;/li>
&lt;li>Cluster &lt;code>Node&lt;/code>s healthiness - maintained as condition with type &lt;code>EveryNodeReady&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: Dependencies</title><link>https://gardener.cloud/docs/gardener/development/dependencies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/dependencies/</guid><description>
&lt;h1 id="dependency-management">Dependency Management&lt;/h1>
&lt;p>We are using &lt;a href="https://github.com/golang/go/wiki/Modules">go modules&lt;/a> for depedency management.
In order to add a new package dependency to the project, you can perform &lt;code>go get &amp;lt;PACKAGE&amp;gt;@&amp;lt;VERSION&amp;gt;&lt;/code> or edit the &lt;code>go.mod&lt;/code> file and append the package along with the version you want to use.&lt;/p>
&lt;h2 id="updating-dependencies">Updating Dependencies&lt;/h2>
&lt;p>The &lt;code>Makefile&lt;/code> contains a rule called &lt;code>revendor&lt;/code> which performs &lt;code>go mod tidy&lt;/code> and &lt;code>go mod vendor&lt;/code>.
&lt;code>go mod tidy&lt;/code> makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module&amp;rsquo;s packages and dependencies, and it removes unused modules that don&amp;rsquo;t provide any relevant packages.
&lt;code>go mod vendor&lt;/code> resets the main module&amp;rsquo;s vendor directory to include all packages needed to build and test all the main module&amp;rsquo;s packages. It does not include test code for vendored packages.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make revendor
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The dependencies are installed into the &lt;code>vendor&lt;/code> folder which &lt;strong>should be added&lt;/strong> to the VCS.&lt;/p>
&lt;p>⚠️ Make sure that you test the code after you have updated the dependencies!&lt;/p>
&lt;h2 id="exported-packages">Exported Packages&lt;/h2>
&lt;p>This repository contains several packages that could be considered &amp;ldquo;exported packages&amp;rdquo;, in a sense that they are supposed to be reused in other Go projects.
For example:&lt;/p>
&lt;ul>
&lt;li>Gardener&amp;rsquo;s API packages: &lt;code>pkg/apis&lt;/code>&lt;/li>
&lt;li>Library for building Gardener extensions: &lt;code>extensions&lt;/code>&lt;/li>
&lt;li>Gardener&amp;rsquo;s Test Framework: &lt;code>test/framework&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>There are a few more folders in this repository (non-Go sources) that are reused across projects in the gardener organization:&lt;/p>
&lt;ul>
&lt;li>GitHub templates: &lt;code>.github&lt;/code>&lt;/li>
&lt;li>Concourse / cc-utils related helpers: &lt;code>hack/.ci&lt;/code>&lt;/li>
&lt;li>Development, build and testing helpers: &lt;code>hack&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>These packages feature a dummy &lt;code>doc.go&lt;/code> file to allow other Go projects to pull them in as go mod dependencies.&lt;/p>
&lt;p>These packages are explicitly &lt;em>not&lt;/em> supposed to be used in other projects (consider them as &amp;ldquo;non-exported&amp;rdquo;):&lt;/p>
&lt;ul>
&lt;li>API validation packages: &lt;code>pkg/apis/*/*/validation&lt;/code>&lt;/li>
&lt;li>Operation package (main Gardener business logic regarding &lt;code>Seed&lt;/code> and &lt;code>Shoot&lt;/code> clusters): &lt;code>pkg/operation&lt;/code>&lt;/li>
&lt;li>Third party code: &lt;code>third_party&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Currently, we don&amp;rsquo;t have a mechanism yet for selectively syncing out these exported packages into dedicated repositories like kube&amp;rsquo;s &lt;a href="https://github.com/kubernetes/kubernetes/tree/master/staging">staging mechanism&lt;/a> (&lt;a href="https://github.com/kubernetes/publishing-bot">publishing-bot&lt;/a>).&lt;/p>
&lt;h2 id="import-restrictions">Import Restrictions&lt;/h2>
&lt;p>We want to make sure, that other projects can depend on this repository&amp;rsquo;s &amp;ldquo;exported&amp;rdquo; packages without pulling in the entire repository (including &amp;ldquo;non-exported&amp;rdquo; packages) or a high number of other unwanted dependencies.
Hence, we have to be careful when adding new imports or references between our packages.&lt;/p>
&lt;blockquote>
&lt;p>ℹ️ General rule of thumb: the mentioned &amp;ldquo;exported&amp;rdquo; packages should be as self-contained as possible and depend on as few other packages in the repository and other projects as possible.&lt;/p>
&lt;/blockquote>
&lt;p>In order to support that rule and automatically check compliance with that goal, we leverage &lt;a href="https://github.com/kubernetes/code-generator/tree/master/cmd/import-boss">import-boss&lt;/a>.
The tool checks all imports of the given packages (including transitive imports) against rules defined in &lt;code>.import-restrictions&lt;/code> files in each directory.
An import is allowed if it matches at least one allowed prefix and does not match any forbidden prefixes.
Note: &lt;code>''&lt;/code> (the empty string) is a prefix of everything.
For more details, see: &lt;a href="https://github.com/kubernetes/code-generator/tree/master/cmd/import-boss">https://github.com/kubernetes/code-generator/tree/master/cmd/import-boss&lt;/a>&lt;/p>
&lt;p>&lt;code>import-boss&lt;/code> is executed on every pull request and blocks the PR if it doesn&amp;rsquo;t comply with the defined import restrictions.
You can also run it locally using &lt;code>make check&lt;/code>.&lt;/p>
&lt;p>Import restrictions should be changed in the following situations:&lt;/p>
&lt;ul>
&lt;li>We spot a new pattern of imports across our packages that was not restricted before but makes it more difficult for other projects to depend on our &amp;ldquo;exported&amp;rdquo; packages.
In that case, the imports should be further restricted to disallow such problematic imports, and the code/package structure should be reworked to comply with the newly given restrictions.&lt;/li>
&lt;li>We want to share code between packages, but existing import restrictions prevent us from doing so.
In that case, please consider what additional dependencies it will pull in, when loosening existing restrictions.
Also consider possible alternatives, like code restructurings or extracting shared code into dedicated packages for minimal impact on dependent projects.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Deploy Gardenlet</title><link>https://gardener.cloud/docs/gardener/deployment/deploy_gardenlet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/deploy_gardenlet/</guid><description>
&lt;h1 id="deploying-gardenlets">Deploying Gardenlets&lt;/h1>
&lt;p>Gardenlets act as decentral &amp;ldquo;agents&amp;rdquo; to manage shoot clusters of a seed cluster.&lt;/p>
&lt;p>To support scaleability in an automated way, gardenlets are deployed automatically. However, you can still deploy gardenlets manually to be more flexible, for example, when shoot clusters that need to be managed by Gardener are behind a firewall. The gardenlet only requires network connectivity from the gardenlet to the Garden cluster (not the other way round), so it can be used to register Kubernetes clusters with no public endpoint.&lt;/p>
&lt;h2 id="procedure">Procedure&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>First, an initial gardenlet needs to be deployed:&lt;/p>
&lt;ul>
&lt;li>Deploy it manually if you have special requirements. More information: &lt;a href="https://gardener.cloud/docs/gardener/deployment/deploy_gardenlet_manually/">Deploy a Gardenlet Manually&lt;/a>&lt;/li>
&lt;li>Let the Gardener installer deploy it automatically otherwise. More information: &lt;a href="https://gardener.cloud/docs/gardener/deployment/deploy_gardenlet_automatically/">Automatic Deployment of Gardenlets&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>To add additional seed clusters, it is recommended to use regular shoot clusters. You can do this by creating a &lt;code>ManagedSeed&lt;/code> resource with a &lt;code>gardenlet&lt;/code> section as described in &lt;a href="https://gardener.cloud/docs/gardener/usage/managed_seed/">Register Shoot as Seed&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Docs: Deploy Gardenlet Automatically</title><link>https://gardener.cloud/docs/gardener/deployment/deploy_gardenlet_automatically/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/deploy_gardenlet_automatically/</guid><description>
&lt;h1 id="automatic-deployment-of-gardenlets">Automatic Deployment of Gardenlets&lt;/h1>
&lt;p>The gardenlet can automatically deploy itself into shoot clusters, and register this cluster as a seed cluster.
These clusters are called &amp;ldquo;managed seeds&amp;rdquo; (aka &amp;ldquo;shooted seeds&amp;rdquo;).
This procedure is the preferred way to add additional seed clusters, because shoot clusters already come with production-grade qualities that are also demanded for seed clusters.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>The only prerequisite is to register an initial cluster as a seed cluster that has already a gardenlet deployed:&lt;/p>
&lt;ul>
&lt;li>This gardenlet was either deployed as part of a Gardener installation using a setup tool (for example, &lt;code>gardener/garden-setup&lt;/code>) or&lt;/li>
&lt;li>the gardenlet was deployed manually
&lt;ul>
&lt;li>for a step-by-step manual installation Guide see: &lt;a href="https://gardener.cloud/docs/gardener/deployment/deploy_gardenlet_manually/">Deploy a Gardenlet Manually&lt;/a>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>The initial cluster can be the garden cluster itself.&lt;/p>
&lt;/blockquote>
&lt;h2 id="self-deployment-of-gardenlets-in-additional-managed-seed-clusters">Self-Deployment of Gardenlets in Additional Managed Seed Clusters&lt;/h2>
&lt;p>For a better scalability, you usually need more seed clusters that you can create as follows:&lt;/p>
&lt;ol>
&lt;li>Use the initial cluster as the seed cluster for other managed seed clusters. It hosts the control planes of the other seed clusters.&lt;/li>
&lt;li>The gardenlet deployed in the initial cluster deploys itself automatically into the managed seed clusters.&lt;/li>
&lt;/ol>
&lt;p>The advantage of this approach is that there’s only one initial gardenlet installation required. Every other managed seed cluster has a gardenlet deployed automatically.&lt;/p>
&lt;h2 id="related-links">Related Links&lt;/h2>
&lt;p>&lt;a href="https://gardener.cloud/docs/gardener/usage/managed_seed/">Register Shoot as Seed&lt;/a>&lt;/p>
&lt;p>&lt;a href="http://github.com/gardener/garden-setup">garden-setup&lt;/a>&lt;/p></description></item><item><title>Docs: Deploy Gardenlet Manually</title><link>https://gardener.cloud/docs/gardener/deployment/deploy_gardenlet_manually/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/deploy_gardenlet_manually/</guid><description>
&lt;h1 id="deploy-a-gardenlet-manually">Deploy a Gardenlet Manually&lt;/h1>
&lt;p>Manually deploying a gardenlet is required in the following cases:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The Kubernetes cluster to be registered as a seed cluster has no public endpoint,
because it is behind a firewall.
The gardenlet must then be deployed into the cluster itself.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The Kubernetes cluster to be registered as a seed cluster is managed externally
(the Kubernetes cluster is not a shoot cluster, so &lt;a href="https://gardener.cloud/docs/gardener/deployment/deploy_gardenlet_automatically/">Automatic Deployment of Gardenlets&lt;/a> cannot be used).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The gardenlet runs outside of the Kubernetes cluster
that should be registered as a seed cluster.
(The gardenlet is not restricted to run in the seed cluster or
to be deployed into a Kubernetes cluster at all).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Once you’ve deployed a gardenlet manually, for example, behind a firewall, you can deploy new gardenlets automatically. The manually deployed gardenlet is then used as a template for the new gardenlets. More information: &lt;a href="https://gardener.cloud/docs/gardener/deployment/deploy_gardenlet_automatically/">Automatic Deployment of Gardenlets&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;h3 id="kubernetes-cluster-that-should-be-registered-as-a-seed-cluster">Kubernetes cluster that should be registered as a seed cluster&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Verify that the cluster has a &lt;a href="https://gardener.cloud/docs/gardener/usage/supported_k8s_versions/">supported Kubernetes version&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Determine the nodes, pods, and services CIDR of the cluster.
You need to configure this information in the &lt;code>Seed&lt;/code> configuration.
Gardener uses this information to check that the shoot cluster isn’t created with overlapping CIDR ranges.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Every Seed cluster needs an Ingress controller which distributes external requests to internal components like grafana and prometheus. Gardener supports two approaches to achieve this:&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>a. Gardener managed Ingress controller and DNS records. For this configure the following lines in your &lt;a href="https://github.com/gardener/gardener/blob/master/example/50-seed.yaml">Seed resource&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: aws-route53
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: ingress-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ingress:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> domain: ingress.my-seed.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> controller:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;some-optional-provider-specific-config-for-the-ingressController&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>⚠ Please note that if you set &lt;code>.spec.ingress&lt;/code> then &lt;code>.spec.dns.ingressDomain&lt;/code> must be &lt;code>nil&lt;/code>.&lt;/p>
&lt;p>b. Self-managed DNS record and Ingress controller:&lt;/p>
&lt;p>⚠️
There should exist a DNS record &lt;code>*.ingress.&amp;lt;SEED-CLUSTER-DOMAIN&amp;gt;&lt;/code> where &lt;code>&amp;lt;SEED-CLUSTER-DOMAIN&amp;gt;&lt;/code> is the value of the &lt;code>.dns.ingressDomain&lt;/code> field of &lt;a href="https://github.com/gardener/gardener/blob/master/example/50-seed.yaml">a Seed cluster resource&lt;/a> (or the &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml#L84-L85">respective Gardenlet configuration&lt;/a>).&lt;/p>
&lt;p>&lt;em>This is how it could be done for the Nginx ingress controller&lt;/em>&lt;/p>
&lt;p>Deploy nginx into the &lt;code>kube-system&lt;/code> namespace in the Kubernetes cluster that should be registered as a &lt;code>Seed&lt;/code>.&lt;/p>
&lt;p>Nginx will on most cloud providers create the service with type &lt;code>LoadBalancer&lt;/code> with an external ip.&lt;/p>
&lt;pre tabindex="0">&lt;code>NAME TYPE CLUSTER-IP EXTERNAL-IP
nginx-ingress-controller LoadBalancer 10.0.15.46 34.200.30.30
&lt;/code>&lt;/pre>&lt;p>Create a wildcard &lt;code>A&lt;/code> record (e.g *.ingress.sweet-seed.&lt;my-domain>. IN A 34.200.30.30) with your DNS provider and point it to the external ip of the ingress service. This ingress domain is later required to register the &lt;code>Seed&lt;/code> cluster.&lt;/p>
&lt;p>Please configure the ingress domain in the &lt;code>Seed&lt;/code> specification as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ingressDomain: ingress.sweet-seed.&amp;lt;my-domain&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>⚠ Please note that if you set &lt;code>.spec.dns.ingressDomain&lt;/code> then &lt;code>.spec.ingress&lt;/code> must be &lt;code>nil&lt;/code>.&lt;/p>
&lt;h3 id="kubeconfig-for-the-seed-cluster">&lt;code>kubeconfig&lt;/code> for the Seed Cluster&lt;/h3>
&lt;p>The &lt;code>kubeconfig&lt;/code> is required to deploy the gardenlet Helm chart to the seed cluster.
The gardenlet requires certain privileges to be able to operate.
These privileges are described in RBAC resources in the gardenlet Helm chart (see &lt;a href="https://github.com/gardener/gardener/tree/master/charts/gardener/gardenlet/charts/runtime/templates">charts/gardener/gardenlet/charts/runtime/templates&lt;/a>).
The Helm chart contains a service account &lt;code>gardenlet&lt;/code>
that the gardenlet deployment uses by default to talk to the Seed API server.&lt;/p>
&lt;blockquote>
&lt;p>If the gardenlet isn’t deployed in the seed cluster,
the gardenlet can be configured to use a &lt;code>kubeconfig&lt;/code>,
which also requires the above-mentioned privileges, from a mounted directory.
The &lt;code>kubeconfig&lt;/code> is specified in section &lt;code>seedClientConnection.kubeconfig&lt;/code>
of the &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml">Gardenlet configuration&lt;/a>.
This configuration option isn’t used in the following,
as this procedure only describes the recommended setup option
where the gardenlet is running in the seed cluster itself.&lt;/p>
&lt;/blockquote>
&lt;h2 id="procedure-overview">Procedure Overview&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Prepare the garden cluster:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="#create-a-bootstrap-token-secret-in-the-kube-system-namespace-of-the-garden-cluster">Create a bootstrap token secret in the &lt;code>kube-system&lt;/code> namespace of the garden cluster&lt;/a>&lt;/li>
&lt;li>&lt;a href="#create-rbac-roles-for-the-gardenlet-to-allow-bootstrapping-in-the-garden-cluster">Create RBAC roles for the gardenlet to allow bootstrapping in the garden cluster&lt;/a>&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="#prepare-the-gardenlet-helm-chart">Prepare the gardenlet Helm chart&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="#automatically-register-shoot-cluster-as-a-seed-cluster">Automatically register shoot cluster as a seed cluster&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="#deploy-the-gardenlet">Deploy the gardenlet&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="#check-that-the-gardenlet-is-successfully-deployed">Check that the gardenlet is successfully deployed&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="create-a-bootstrap-token-secret-in-the-kube-system-namespace-of-the-garden-cluster">Create a bootstrap token secret in the &lt;code>kube-system&lt;/code> namespace of the garden cluster&lt;/h2>
&lt;p>The gardenlet needs to talk to the &lt;a href="https://gardener.cloud/docs/gardener/concepts/apiserver/">Gardener API server&lt;/a> residing in the garden cluster.&lt;/p>
&lt;p>The gardenlet can be configured with an already existing garden cluster &lt;code>kubeconfig&lt;/code> in one of the following ways:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Either by specifying &lt;code>gardenClientConnection.kubeconfig&lt;/code>
in the &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml">Gardenlet configuration&lt;/a> or&lt;/p>
&lt;/li>
&lt;li>
&lt;p>by supplying the environment variable &lt;code>GARDEN_KUBECONFIG&lt;/code> pointing to
a mounted &lt;code>kubeconfig&lt;/code> file).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>The preferred way however, is to use the gardenlets ability to request
a signed certificate for the garden cluster by leveraging
&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">Kubernetes Certificate Signing Requests&lt;/a>.
The gardenlet performs a TLS bootstrapping process that is similar to the
&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/">Kubelet TLS Bootstrapping&lt;/a>.
Make sure that the API server of the garden cluster has
&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/#enabling-bootstrap-token-authentication">bootstrap token authentication&lt;/a>
enabled.&lt;/p>
&lt;p>The client credentials required for the gardenlets TLS bootstrapping process,
need to be either &lt;code>token&lt;/code> or &lt;code>certificate&lt;/code> (OIDC isn’t supported) and have permissions
to create a Certificate Signing Request (&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">CSR&lt;/a>).
It’s recommended to use &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/">bootstrap tokens&lt;/a>
due to their desirable security properties (such as a limited token lifetime).&lt;/p>
&lt;p>Therefore, first create a bootstrap token secret for the garden cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Name MUST be of form &amp;#34;bootstrap-token-&amp;lt;token id&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: bootstrap-token-07401b
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Type MUST be &amp;#39;bootstrap.kubernetes.io/token&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type: bootstrap.kubernetes.io/token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>stringData:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Human readable description. Optional.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> description: &lt;span style="color:#a31515">&amp;#34;Token to be used by the gardenlet for Seed `sweet-seed`.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Token ID and secret. Required.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> token-id: 07401b &lt;span style="color:#008000"># 6 characters&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> token-secret: f395accd246ae52d &lt;span style="color:#008000"># 16 characters&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Expiration. Optional.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># expiration: 2017-03-10T03:22:11Z&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Allowed usages.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usage-bootstrap-authentication: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usage-bootstrap-signing: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When you later prepare the gardenlet Helm chart,
a &lt;code>kubeconfig&lt;/code> based on this token is shared with the gardenlet upon deployment.&lt;/p>
&lt;h2 id="create-rbac-roles-for-the-gardenlet-to-allow-bootstrapping-in-the-garden-cluster">Create RBAC roles for the gardenlet to allow bootstrapping in the garden cluster&lt;/h2>
&lt;p>This step is only required if the gardenlet you deploy is the first gardenlet
in the Gardener installation.
Additionally, when using the &lt;a href="https://github.com/gardener/gardener/tree/master/charts/gardener/controlplane">control plane chart&lt;/a>,
the following resources are already contained in the Helm chart,
that is, if you use it you can skip these steps as the needed RBAC roles already exist.&lt;/p>
&lt;p>The gardenlet uses the configured bootstrap &lt;code>kubeconfig&lt;/code> in &lt;code>gardenClientConnection.bootstrapKubeconfig&lt;/code> to request a signed certificate for the user &lt;code>gardener.cloud:system:seed:&amp;lt;seed-name&amp;gt;&lt;/code> in the group &lt;code>gardener.cloud:system:seeds&lt;/code>.&lt;/p>
&lt;p>Create a &lt;code>ClusterRole&lt;/code> and &lt;code>ClusterRoleBinding&lt;/code> that grant full admin permissions to authenticated gardenlets.&lt;/p>
&lt;p>Create the following resources in the garden cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: rbac.authorization.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterRole
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardener.cloud:system:seeds
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - apiGroups:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#a31515">&amp;#39;*&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#a31515">&amp;#39;*&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> verbs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#a31515">&amp;#39;*&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: rbac.authorization.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterRoleBinding
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardener.cloud:system:seeds
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>roleRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: ClusterRole
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardener.cloud:system:seeds
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>subjects:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - kind: Group
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardener.cloud:system:seeds
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: rbac.authorization.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterRole
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardener.cloud:system:seed-bootstrapper
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - apiGroups:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - certificates.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - certificatesigningrequests
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> verbs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - create
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - get
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - apiGroups:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - certificates.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - certificatesigningrequests/seedclient
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> verbs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - create
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># A kubelet/gardenlet authenticating using bootstrap tokens is authenticated as a user in the group system:bootstrappers&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Allows the Gardenlet to create a CSR&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: rbac.authorization.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterRoleBinding
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardener.cloud:system:seed-bootstrapper
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>roleRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: ClusterRole
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardener.cloud:system:seed-bootstrapper
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>subjects:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - kind: Group
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: system:bootstrappers
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ℹ️ After bootstrapping, the gardenlet has full administrative access to the garden cluster.
You might be interested to harden this and limit its permissions to only resources related to the seed cluster it is responsible for.
Please take a look into &lt;a href="https://gardener.cloud/docs/gardener/deployment/gardenlet_api_access/">this document&lt;/a>.&lt;/p>
&lt;h2 id="prepare-the-gardenlet-helm-chart">Prepare the gardenlet Helm chart&lt;/h2>
&lt;p>This section only describes the minimal configuration,
using the global configuration values of the gardenlet Helm chart.
For an overview over all values, see the &lt;a href="https://github.com/gardener/gardener/blob/master/charts/gardener/gardenlet/values.yaml">configuration values&lt;/a>.
We refer to the global configuration values as &lt;em>gardenlet configuration&lt;/em> in the remaining procedure.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Create a gardenlet configuration &lt;code>gardenlet-values.yaml&lt;/code> based on &lt;a href="https://github.com/gardener/gardener/blob/master/charts/gardener/gardenlet/values.yaml">this template&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create a bootstrap &lt;code>kubeconfig&lt;/code> based on the bootstrap token created in the garden cluster.&lt;/p>
&lt;p>Replace the &lt;code>&amp;lt;bootstrap-token&amp;gt;&lt;/code> with &lt;code>token-id.token-secret&lt;/code> (from our previous example: &lt;code>07401b.f395accd246ae52d&lt;/code>) from the bootstrap token secret.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>current-context: gardenlet-bootstrap@default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusters:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- cluster:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> certificate-authority-data: &amp;lt;ca-of-garden-cluster&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: https://&amp;lt;endpoint-of-garden-cluster&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: gardenlet-bootstrap
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardenlet-bootstrap@default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>users:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: gardenlet-bootstrap
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> token: &amp;lt;bootstrap-token&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>In section &lt;code>gardenClientConnection.bootstrapKubeconfig&lt;/code> of your gardenlet configuration, provide the bootstrap &lt;code>kubeconfig&lt;/code> together with a name and namespace to the gardenlet Helm chart.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>gardenClientConnection:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> bootstrapKubeconfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardenlet-kubeconfig-bootstrap
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeconfig: |&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &lt;/span> &amp;lt;bootstrap-kubeconfig&amp;gt; &lt;span style="color:#008000"># will be base64 encoded by helm&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The bootstrap &lt;code>kubeconfig&lt;/code> is stored in the specified secret.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In section &lt;code>gardenClientConnection.kubeconfigSecret&lt;/code> of your gardenlet configuration,
define a name and a namespace where the gardenlet stores
the real &lt;code>kubeconfig&lt;/code> that it creates during the bootstrap process. If the secret doesn&amp;rsquo;t exist,
the gardenlet creates it for you.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>gardenClientConnection:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeconfigSecret:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardenlet-kubeconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h2 id="automatically-register-shoot-cluster-as-a-seed-cluster">Automatically register shoot cluster as a seed cluster&lt;/h2>
&lt;p>A seed cluster can either be registered by manually creating
the &lt;a href="https://github.com/gardener/gardener/blob/master/example/50-seed.yaml">&lt;code>Seed&lt;/code> resource&lt;/a>
or automatically by the gardenlet.
This functionality is useful for managed seed clusters,
as the gardenlet in the garden cluster deploys a copy of itself
into the cluster with automatic registration of the &lt;code>Seed&lt;/code> configured.
However, it can also be used to have a streamlined seed cluster registration process when manually deploying the gardenlet.&lt;/p>
&lt;blockquote>
&lt;p>This procedure doesn’t describe all the possible configurations
for the &lt;code>Seed&lt;/code> resource. For more information, see:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/gardener/gardener/blob/master/example/50-seed.yaml">Example Seed resource&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/usage/seed_settings/">Configurable Seed settings&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h3 id="adjust-the-gardenlet-component-configuration">Adjust the gardenlet component configuration&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Supply the &lt;code>Seed&lt;/code> resource in section &lt;code>seedConfig&lt;/code> of your gardenlet configuration &lt;code>gardenlet-values.yaml&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Add the &lt;code>seedConfig&lt;/code> to your gardenlet configuration &lt;code>gardenlet-values.yaml&lt;/code>.
The field &lt;code>seedConfig.spec.provider.type&lt;/code> specifies the infrastructure provider type (for example, &lt;code>aws&lt;/code>) of the seed cluster.
For all supported infrastructure providers, see &lt;a href="https://github.com/gardener/gardener/blob/master/extensions/README.md#known-extension-implementations">Known Extension Implementations&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>....
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>seedConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: sweet-seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ingressDomain: ingress.sweet-seed.&amp;lt;my-domain&amp;gt; &lt;span style="color:#008000"># see prerequisites&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networks: &lt;span style="color:#008000"># see prerequisites&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodes: 10.240.0.0/16
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pods: 100.244.0.0/16
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> services: 100.32.0.0/13
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shootDefaults: # optional: non-overlapping default CIDRs for shoot clusters of that Seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pods: 100.96.0.0/11
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> services: 100.64.0.0/13
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> region: eu-west-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: &amp;lt;provider&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h3 id="optional-enable-backup-and-restore">Optional: Enable backup and restore&lt;/h3>
&lt;p>The seed cluster can be set up with backup and restore
for the main &lt;code>etcds&lt;/code> of shoot clusters.&lt;/p>
&lt;p>Gardener uses &lt;a href="https://github.com/gardener/etcd-backup-restore">etcd-backup-restore&lt;/a>
that &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/master/doc/usage/getting_started.md#usage">integrates with different storage providers&lt;/a>
to store the shoot cluster&amp;rsquo;s main &lt;code>etcd&lt;/code> backups.
Make sure to obtain client credentials that have sufficient permissions with the chosen storage provider.&lt;/p>
&lt;p>Create a secret in the garden cluster with client credentials for the storage provider.
The format of the secret is cloud provider specific and can be found
in the repository of the respective Gardener extension.
For example, the secret for AWS S3 can be found in the AWS provider extension
(&lt;a href="https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/30-etcd-backup-secret.yaml">30-etcd-backup-secret.yaml&lt;/a>).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: sweet-seed-backup
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type: Opaque
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># client credentials format is provider specific&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Configure the &lt;code>Seed&lt;/code> resource in section &lt;code>seedConfig&lt;/code> of your gardenlet configuration to use backup and restore:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>seedConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: sweet-seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backup:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider: &amp;lt;provider&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: sweet-seed-backup
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="deploy-the-gardenlet">Deploy the gardenlet&lt;/h2>
&lt;blockquote>
&lt;p>The gardenlet doesn’t have to run in the same Kubernetes cluster as the seed cluster
it’s registering and reconciling, but it is in most cases advantageous
to use in-cluster communication to talk to the Seed API server.
Running a gardenlet outside of the cluster is mostly used for local development.&lt;/p>
&lt;/blockquote>
&lt;p>The &lt;code>gardenlet-values.yaml&lt;/code> looks something like this
(with automatic Seed registration and backup for shoot clusters enabled):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>global:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Gardenlet configuration values&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> gardenlet:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;default config&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> gardenClientConnection:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> bootstrapKubeconfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardenlet-bootstrap-kubeconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeconfig: |&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> apiVersion: v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> clusters:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> - cluster:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> certificate-authority-data: &amp;lt;dummy&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> server: &amp;lt;my-garden-cluster-endpoint&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> name: my-kubernetes-cluster
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> ....&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeconfigSecret:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardenlet-kubeconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;default config&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seedConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: sweet-seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ingressDomain: ingress.sweet-seed.&amp;lt;my-domain&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networks:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodes: 10.240.0.0/16
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pods: 100.244.0.0/16
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> services: 100.32.0.0/13
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shootDefaults:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pods: 100.96.0.0/11
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> services: 100.64.0.0/13
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> region: eu-west-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: &amp;lt;provider&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backup:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider: &amp;lt;provider&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: sweet-seed-backup
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Deploy the gardenlet Helm chart to the Kubernetes cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>helm install gardenlet charts/gardener/gardenlet &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --namespace garden &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> -f gardenlet-values.yaml &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --wait
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This helm chart creates:&lt;/p>
&lt;ul>
&lt;li>A service account &lt;code>gardenlet&lt;/code> that the gardenlet can use to talk to the Seed API server.&lt;/li>
&lt;li>RBAC roles for the service account (full admin rights at the moment).&lt;/li>
&lt;li>The secret (&lt;code>garden&lt;/code>/&lt;code>gardenlet-bootstrap-kubeconfig&lt;/code>) containing the bootstrap &lt;code>kubeconfig&lt;/code>.&lt;/li>
&lt;li>The gardenlet deployment in the &lt;code>garden&lt;/code> namespace.&lt;/li>
&lt;/ul>
&lt;h2 id="check-that-the-gardenlet-is-successfully-deployed">Check that the gardenlet is successfully deployed&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Check that the gardenlets certificate bootstrap was successful.&lt;/p>
&lt;p>Check if the secret &lt;code>gardenlet-kubeconfig&lt;/code> in the namespace &lt;code>garden&lt;/code> in the seed cluster
is created and contains a &lt;code>kubeconfig&lt;/code> with a valid certificate.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Get the &lt;code>kubeconfig&lt;/code> from the created secret.&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl -n garden get secret gardenlet-kubeconfig -o json | jq -r .data.kubeconfig | base64 -d
&lt;/code>&lt;/pre>&lt;/li>
&lt;li>
&lt;p>Test against the garden cluster and verify it’s working.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Extract the &lt;code>client-certificate-data&lt;/code> from the user &lt;code>gardenlet&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>View the certificate:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ openssl x509 -in ./gardenlet-cert -noout -text
&lt;/code>&lt;/pre>&lt;p>Check that the certificate is valid for a year (that is the lifetime of new certificates).&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Check that the bootstrap secret &lt;code>gardenlet-bootstrap-kubeconfig&lt;/code> has been deleted from the seed cluster in namespace &lt;code>garden&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check that the seed cluster is registered and &lt;code>READY&lt;/code> in the garden cluster.&lt;/p>
&lt;p>Check that the seed cluster &lt;code>sweet-seed&lt;/code> exists and all conditions indicate that it’s available.
If so, the &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/#heartbeats">Gardenlet is sending regular heartbeats&lt;/a> and the &lt;a href="https://gardener.cloud/docs/gardener/usage/seed_bootstrapping/">seed bootstrapping&lt;/a> was successful.&lt;/p>
&lt;p>Check that the conditions on the &lt;code>Seed&lt;/code> resource look similar to the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get seed sweet-seed -o json | jq .status.conditions
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;lastTransitionTime&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;2020-07-17T09:17:29Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;lastUpdateTime&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;2020-07-17T09:17:29Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;message&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;Gardenlet is posting ready status.&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;reason&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;GardenletReady&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;True&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;GardenletReady&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;lastTransitionTime&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;2020-07-17T09:17:49Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;lastUpdateTime&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;2020-07-17T09:53:17Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;message&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;Seed cluster has been bootstrapped successfully.&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;reason&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;BootstrappingSucceeded&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;True&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;Bootstrapped&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;lastTransitionTime&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;2020-07-17T09:17:49Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;lastUpdateTime&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;2020-07-17T09:53:17Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;message&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;Backup Buckets are available.&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;reason&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;BackupBucketsAvailable&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;True&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;type&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;BackupBucketsReady&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h2 id="related-links">Related Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/gardener/gardener/issues/1724">Issue #1724: Harden Gardenlet RBAC privileges&lt;/a>.&lt;/p>
&lt;p>&lt;a href="https://gardener.cloud/docs/gardener/concepts/backup-restore/">Backup and Restore&lt;/a>.&lt;/p></description></item><item><title>Docs: Etcd</title><link>https://gardener.cloud/docs/gardener/concepts/etcd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/etcd/</guid><description>
&lt;h1 id="etcd---key-value-store-for-kubernetes">etcd - Key-Value Store for Kubernetes&lt;/h1>
&lt;p>&lt;a href="https://etcd.io/">etcd&lt;/a> is a strongly consistent key-value store and the most prevalent choice for the Kubernetes
persistence layer. All API cluster objects like &lt;code>Pod&lt;/code>s, &lt;code>Deployment&lt;/code>s, &lt;code>Secret&lt;/code>s, etc. are stored in &lt;code>etcd&lt;/code> which
makes it an essential part of a &lt;a href="https://kubernetes.io/docs/concepts/overview/components/#control-plane-components">Kubernetes control plane&lt;/a>.&lt;/p>
&lt;h2 id="shoot-cluster-persistence">Shoot cluster persistence&lt;/h2>
&lt;p>Each shoot cluster gets its very own persistence for the control plane. It runs in the shoot namespace on the respective
seed cluster. Concretely, there are two etcd instances per shoot cluster which the &lt;code>Kube-Apiserver&lt;/code> is configured
to use in the following way:&lt;/p>
&lt;ul>
&lt;li>etcd-main&lt;/li>
&lt;/ul>
&lt;p>A store that contains all &amp;ldquo;cluster critical&amp;rdquo; or &amp;ldquo;long-term&amp;rdquo; objects. These object kinds are typically considered
for a backup to prevent any data loss.&lt;/p>
&lt;ul>
&lt;li>etcd-events&lt;/li>
&lt;/ul>
&lt;p>A store that contains all &lt;code>Event&lt;/code> objects (&lt;code>events.k8s.io&lt;/code>) of a cluster. &lt;code>Events&lt;/code> have usually a short retention
period, occur frequently but are not essential for a disaster recovery.&lt;/p>
&lt;p>The setup above prevents both, the critical &lt;code>etcd-main&lt;/code> is not flooded by Kubernetes &lt;code>Events&lt;/code> as well as backup space is
not occupied by non-critical data. This segmentation saves time and resources.&lt;/p>
&lt;h2 id="etcd-operator">etcd Operator&lt;/h2>
&lt;p>Configuring, maintaining and health-checking &lt;code>etcd&lt;/code> is outsourced to a dedicated operator called &lt;a href="https://github.com/gardener/etcd-druid/">ETCD Druid&lt;/a>.
When &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/">Gardenlet&lt;/a> reconciles a &lt;code>Shoot&lt;/code> resource, it creates or updates an &lt;a href="https://github.com/gardener/etcd-druid/blob/1d427e9167adac1476d1847c0e265c2c09d6bc62/config/samples/druid_v1alpha1_etcd.yaml">Etcd&lt;/a>
resources in the seed cluster, containing necessary information (backup information, defragmentation schedule, resources, etc.) &lt;code>etcd-druid&lt;/code>
needs to manage the lifecycle of the desired etcd instance (today &lt;code>main&lt;/code> or &lt;code>events&lt;/code>). Likewise, when the shoot is deleted,
Gardenlet deletes the &lt;code>Etcd&lt;/code> resource and &lt;a href="https://github.com/gardener/etcd-druid/">ETCD Druid&lt;/a> takes care about cleaning up
all related objects, e.g. the backing &lt;code>StatefulSet&lt;/code>.&lt;/p>
&lt;h2 id="autoscaling">Autoscaling&lt;/h2>
&lt;p>Gardenlet maintains &lt;a href="https://github.com/gardener/hvpa-controller/blob/master/config/samples/autoscaling_v1alpha1_hvpa.yaml">HVPA&lt;/a>
objects for etcd &lt;code>StatefulSet&lt;/code>s if the corresponding &lt;a href="https://gardener.cloud/docs/gardener/deployment/feature_gates/">feature gate&lt;/a> is enabled. This enables
a vertical scaling for &lt;code>etcd&lt;/code>. Downscaling is handled more pessimistic to prevent many subsequent &lt;code>etcd&lt;/code> restarts. Thus,
for &lt;code>production&lt;/code> and &lt;code>infrastructure&lt;/code> clusters downscaling is deactivated and for all other clusters lower advertised requests/limits are only
applied during a shoot&amp;rsquo;s maintenance time window.&lt;/p>
&lt;h2 id="backup">Backup&lt;/h2>
&lt;p>If &lt;code>Seed&lt;/code>s specify backups for etcd (&lt;a href="https://github.com/gardener/gardener/blob/e9bf88a7a091a8cf8c495bef298bdada17a03c7f/example/50-seed.yaml#L19">example&lt;/a>),
then Gardener and the respective &lt;a href="https://gardener.cloud/docs/gardener/extensions/overview/">provider extensions&lt;/a> are responsible for creating a bucket
on the cloud provider&amp;rsquo;s side (modelled through &lt;a href="https://gardener.cloud/docs/gardener/extensions/backupbucket/">BackupBucket resource&lt;/a>). The bucket stores
backups of shoots scheduled on that seed. Furthermore, Gardener creates a &lt;a href="https://gardener.cloud/docs/gardener/extensions/backupentry/">BackupEntry&lt;/a>
which subdivides the bucket and thus makes it possible to store backups of multiple shoot clusters.&lt;/p>
&lt;p>The &lt;code>etcd-main&lt;/code> instance itself is configured to run with a special backup-restore &lt;em>sidecar&lt;/em>. It takes care about regularly
backing up etcd data and restoring it in case of data loss. More information can be found on the component&amp;rsquo;s GitHub
page &lt;a href="https://github.com/gardener/etcd-backup-restore">https://github.com/gardener/etcd-backup-restore&lt;/a>.&lt;/p>
&lt;p>How long backups are stored in the bucket after a shoot has been deleted, depends on the configured &lt;em>retention period&lt;/em> in the
&lt;code>Seed&lt;/code> resource. Please see this &lt;a href="https://github.com/gardener/gardener/blob/849cd857d0d20e5dde26b9740ca2814603a56dfd/example/20-componentconfig-gardenlet.yaml#L20">example configuration&lt;/a> for more information.&lt;/p>
&lt;h2 id="housekeeping">Housekeeping&lt;/h2>
&lt;p>&lt;a href="https://etcd.io/docs/v3.3/op-guide/maintenance/">etcd maintenance tasks&lt;/a> must be performed from time to time in order
to re-gain database storage and to ensure the system&amp;rsquo;s reliability. The &lt;a href="https://github.com/gardener/etcd-backup-restore">backup-restore&lt;/a>
&lt;em>sidecar&lt;/em> takes care about this job as well. Gardener chooses a random time &lt;strong>within the shoot&amp;rsquo;s maintenance time&lt;/strong> to
schedule these tasks.&lt;/p></description></item><item><title>Docs: Feature Gates</title><link>https://gardener.cloud/docs/gardener/deployment/feature_gates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/feature_gates/</guid><description>
&lt;h1 id="feature-gates-in-gardener">Feature Gates in Gardener&lt;/h1>
&lt;p>This page contains an overview of the various feature gates an administrator can specify on different Gardener components.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Feature gates are a set of key=value pairs that describe Gardener features. You can turn these features on or off using the a component configuration file for a specific component.&lt;/p>
&lt;p>Each Gardener component lets you enable or disable a set of feature gates that are relevant to that component. For example this is the configuration of the &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml">gardenlet&lt;/a> component.&lt;/p>
&lt;p>The following tables are a summary of the feature gates that you can set on different Gardener components.&lt;/p>
&lt;ul>
&lt;li>The “Since” column contains the Gardener release when a feature is introduced or its release stage is changed.&lt;/li>
&lt;li>The “Until” column, if not empty, contains the last Gardener release in which you can still use a feature gate.&lt;/li>
&lt;li>If a feature is in the Alpha or Beta state, you can find the feature listed in the Alpha/Beta feature gate table.&lt;/li>
&lt;li>If a feature is stable you can find all stages for that feature listed in the Graduated/Deprecated feature gate table.&lt;/li>
&lt;li>The Graduated/Deprecated feature gate table also lists deprecated and withdrawn features.&lt;/li>
&lt;/ul>
&lt;h2 id="feature-gates-for-alpha-or-beta-features">Feature gates for Alpha or Beta features&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Feature&lt;/th>
&lt;th>Default&lt;/th>
&lt;th>Stage&lt;/th>
&lt;th>Since&lt;/th>
&lt;th>Until&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>HVPA&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>0.31&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HVPAForShootedSeed&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>0.32&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ManagedIstio&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.5&lt;/code>&lt;/td>
&lt;td>&lt;code>1.18&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ManagedIstio&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>Beta&lt;/code>&lt;/td>
&lt;td>&lt;code>1.19&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>APIServerSNI&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.7&lt;/code>&lt;/td>
&lt;td>&lt;code>1.18&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>APIServerSNI&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>Beta&lt;/code>&lt;/td>
&lt;td>&lt;code>1.19&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SeedChange&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.12&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SeedKubeScheduler&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.15&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ReversedVPN&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.22&lt;/code>&lt;/td>
&lt;td>&lt;code>1.41&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ReversedVPN&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>Beta&lt;/code>&lt;/td>
&lt;td>&lt;code>1.42&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RotateSSHKeypairOnMaintenance&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.28&lt;/code>&lt;/td>
&lt;td>&lt;code>1.44&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RotateSSHKeypairOnMaintenance&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>Beta&lt;/code>&lt;/td>
&lt;td>&lt;code>1.45&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WorkerPoolKubernetesVersion&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.35&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CopyEtcdBackupsDuringControlPlaneMigration&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.37&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SecretBindingProviderValidation&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.38&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ForceRestore&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.39&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DisableDNSProviderManagement&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.41&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ShootCARotation&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.42&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ShootMaxTokenExpirationOverwrite&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.43&lt;/code>&lt;/td>
&lt;td>&lt;code>1.44&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ShootMaxTokenExpirationOverwrite&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>Beta&lt;/code>&lt;/td>
&lt;td>&lt;code>1.45&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ShootMaxTokenExpirationValidation&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.43&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="feature-gates-for-graduated-or-deprecated-features">Feature gates for graduated or deprecated features&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Feature&lt;/th>
&lt;th>Default&lt;/th>
&lt;th>Stage&lt;/th>
&lt;th>Since&lt;/th>
&lt;th>Until&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>NodeLocalDNS&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.7&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>NodeLocalDNS&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;code>Removed&lt;/code>&lt;/td>
&lt;td>&lt;code>1.26&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>KonnectivityTunnel&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.6&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>KonnectivityTunnel&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;code>Removed&lt;/code>&lt;/td>
&lt;td>&lt;code>1.27&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MountHostCADirectories&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.11&lt;/code>&lt;/td>
&lt;td>&lt;code>1.25&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MountHostCADirectories&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>Beta&lt;/code>&lt;/td>
&lt;td>&lt;code>1.26&lt;/code>&lt;/td>
&lt;td>&lt;code>1.27&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MountHostCADirectories&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>GA&lt;/code>&lt;/td>
&lt;td>&lt;code>1.27&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MountHostCADirectories&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;code>Removed&lt;/code>&lt;/td>
&lt;td>&lt;code>1.30&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DisallowKubeconfigRotationForShootInDeletion&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.28&lt;/code>&lt;/td>
&lt;td>&lt;code>1.31&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DisallowKubeconfigRotationForShootInDeletion&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>Beta&lt;/code>&lt;/td>
&lt;td>&lt;code>1.32&lt;/code>&lt;/td>
&lt;td>&lt;code>1.35&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DisallowKubeconfigRotationForShootInDeletion&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>GA&lt;/code>&lt;/td>
&lt;td>&lt;code>1.36&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DisallowKubeconfigRotationForShootInDeletion&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;code>Removed&lt;/code>&lt;/td>
&lt;td>&lt;code>1.38&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Logging&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>0.13&lt;/code>&lt;/td>
&lt;td>&lt;code>1.40&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Logging&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Removed&lt;/code>&lt;/td>
&lt;td>&lt;code>1.41&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AdminKubeconfigRequest&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.24&lt;/code>&lt;/td>
&lt;td>&lt;code>1.38&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AdminKubeconfigRequest&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>Beta&lt;/code>&lt;/td>
&lt;td>&lt;code>1.39&lt;/code>&lt;/td>
&lt;td>&lt;code>1.41&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AdminKubeconfigRequest&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>GA&lt;/code>&lt;/td>
&lt;td>&lt;code>1.42&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UseDNSRecords&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.27&lt;/code>&lt;/td>
&lt;td>&lt;code>1.38&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UseDNSRecords&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>Beta&lt;/code>&lt;/td>
&lt;td>&lt;code>1.39&lt;/code>&lt;/td>
&lt;td>&lt;code>1.43&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UseDNSRecords&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>GA&lt;/code>&lt;/td>
&lt;td>&lt;code>1.44&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CachedRuntimeClients&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.7&lt;/code>&lt;/td>
&lt;td>&lt;code>1.33&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CachedRuntimeClients&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>Beta&lt;/code>&lt;/td>
&lt;td>&lt;code>1.34&lt;/code>&lt;/td>
&lt;td>&lt;code>1.44&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CachedRuntimeClients&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>GA&lt;/code>&lt;/td>
&lt;td>&lt;code>1.45&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DenyInvalidExtensionResources&lt;/td>
&lt;td>&lt;code>false&lt;/code>&lt;/td>
&lt;td>&lt;code>Alpha&lt;/code>&lt;/td>
&lt;td>&lt;code>1.31&lt;/code>&lt;/td>
&lt;td>&lt;code>1.41&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DenyInvalidExtensionResources&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>Beta&lt;/code>&lt;/td>
&lt;td>&lt;code>1.42&lt;/code>&lt;/td>
&lt;td>&lt;code>1.44&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DenyInvalidExtensionResources&lt;/td>
&lt;td>&lt;code>true&lt;/code>&lt;/td>
&lt;td>&lt;code>GA&lt;/code>&lt;/td>
&lt;td>&lt;code>1.45&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="using-a-feature">Using a feature&lt;/h2>
&lt;p>A feature can be in &lt;em>Alpha&lt;/em>, &lt;em>Beta&lt;/em> or &lt;em>GA&lt;/em> stage.
An &lt;em>Alpha&lt;/em> feature means:&lt;/p>
&lt;ul>
&lt;li>Disabled by default.&lt;/li>
&lt;li>Might be buggy. Enabling the feature may expose bugs.&lt;/li>
&lt;li>Support for feature may be dropped at any time without notice.&lt;/li>
&lt;li>The API may change in incompatible ways in a later software release without notice.&lt;/li>
&lt;li>Recommended for use only in short-lived testing clusters, due to increased
risk of bugs and lack of long-term support.&lt;/li>
&lt;/ul>
&lt;p>A &lt;em>Beta&lt;/em> feature means:&lt;/p>
&lt;ul>
&lt;li>Enabled by default.&lt;/li>
&lt;li>The feature is well tested. Enabling the feature is considered safe.&lt;/li>
&lt;li>Support for the overall feature will not be dropped, though details may change.&lt;/li>
&lt;li>The schema and/or semantics of objects may change in incompatible ways in a
subsequent beta or stable release. When this happens, we will provide instructions
for migrating to the next version. This may require deleting, editing, and
re-creating API objects. The editing process may require some thought.
This may require downtime for applications that rely on the feature.&lt;/li>
&lt;li>Recommended for only non-critical uses because of potential for
incompatible changes in subsequent releases.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Please do try &lt;em>Beta&lt;/em> features and give feedback on them!
After they exit beta, it may not be practical for us to make more changes.&lt;/p>
&lt;/blockquote>
&lt;p>A &lt;em>General Availability&lt;/em> (GA) feature is also referred to as a &lt;em>stable&lt;/em> feature. It means:&lt;/p>
&lt;ul>
&lt;li>The feature is always enabled; you cannot disable it.&lt;/li>
&lt;li>The corresponding feature gate is no longer needed.&lt;/li>
&lt;li>Stable versions of features will appear in released software for many subsequent versions.&lt;/li>
&lt;/ul>
&lt;h2 id="list-of-feature-gates">List of Feature Gates&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Feature&lt;/th>
&lt;th>Relevant Components&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>HVPA&lt;/td>
&lt;td>&lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td>Enables simultaneous horizontal and vertical scaling in Seed Clusters.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HVPAForShootedSeed&lt;/td>
&lt;td>&lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td>Enables simultaneous horizontal and vertical scaling in managed seed (aka &amp;ldquo;shooted seed&amp;rdquo;) clusters.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ManagedIstio&lt;/td>
&lt;td>&lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td>Enables a Gardener-tailored &lt;a href="https://istio.io">Istio&lt;/a> in each Seed cluster. Disable this feature if Istio is already installed in the cluster. Istio is not automatically removed if this feature is disabled. See the &lt;a href="https://gardener.cloud/docs/gardener/usage/istio/">detailed documentation&lt;/a> for more information.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>APIServerSNI&lt;/td>
&lt;td>&lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td>Enables only one LoadBalancer to be used for every Shoot cluster API server in a Seed. Enable this feature when &lt;code>ManagedIstio&lt;/code> is enabled or Istio is manually deployed in Seed cluster. See &lt;a href="https://gardener.cloud/docs/gardener/proposals/08-shoot-apiserver-via-sni/">GEP-8&lt;/a> for more details.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CachedRuntimeClients&lt;/td>
&lt;td>&lt;code>gardener-controller-manager&lt;/code>, &lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td>Enables a cache in the controller-runtime clients, that Gardener components use. The feature gate can be specified for gardenlet and gardener-controller-manager (and gardener-scheduler for the versions &lt;code>&amp;lt; 1.29&lt;/code>).&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SeedChange&lt;/td>
&lt;td>&lt;code>gardener-apiserver&lt;/code>&lt;/td>
&lt;td>Enables updating the &lt;code>spec.seedName&lt;/code> field during shoot validation from a non-empty value in order to trigger shoot control plane migration.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SeedKubeScheduler&lt;/td>
&lt;td>&lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td>Adds custom &lt;code>kube-scheduler&lt;/code> in &lt;code>gardener-kube-scheduler&lt;/code> namespace. It schedules &lt;a href="https://gardener.cloud/docs/gardener/concepts/seed-admission-controller/#mutating-webhooks">pods with scheduler name&lt;/a> &lt;code>gardener-kube-scheduler&lt;/code> on Nodes with higher resource utilization. It requires Seed cluster with kubernetes version &lt;code>1.18&lt;/code> or higher.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ReversedVPN&lt;/td>
&lt;td>&lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td>Reverses the connection setup of the vpn tunnel between the Seed and the Shoot cluster(s). It allows Seed and Shoot clusters to be in different networks with only direct access in one direction (Shoot -&amp;gt; Seed). In addition to that, it reduces the amount of load balancers required, i.e. no load balancers are required for the vpn tunnel anymore. It requires &lt;code>APIServerSNI&lt;/code> and kubernetes version &lt;code>1.18&lt;/code> or higher to work. Details can be found in &lt;a href="https://gardener.cloud/docs/gardener/proposals/14-reversed-cluster-vpn/">GEP-14&lt;/a>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AdminKubeconfigRequest&lt;/td>
&lt;td>&lt;code>gardener-apiserver&lt;/code>&lt;/td>
&lt;td>Enables the &lt;code>AdminKubeconfigRequest&lt;/code> endpoint on Shoot resources. See &lt;a href="https://gardener.cloud/docs/gardener/proposals/16-adminkubeconfig-subresource/">GEP-16&lt;/a> for more details.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UseDNSRecords&lt;/td>
&lt;td>&lt;code>gardener-apiserver&lt;/code>, &lt;code>gardener-controller-manager&lt;/code>, &lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td>Enables using &lt;code>DNSRecord&lt;/code> resources for Gardener DNS records instead of &lt;code>DNSProvider&lt;/code>, &lt;code>DNSEntry&lt;/code>, and &lt;code>DNSOwner&lt;/code> resources. See &lt;a href="https://gardener.cloud/docs/gardener/extensions/dnsrecord/">Contract: &lt;code>DNSRecord&lt;/code> resources&lt;/a> for more details.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RotateSSHKeypairOnMaintenance&lt;/td>
&lt;td>&lt;code>gardener-controller-manager&lt;/code>&lt;/td>
&lt;td>Enables SSH keypair rotation in the maintenance controller of the gardener-controller-manager. Details can be found in &lt;a href="https://gardener.cloud/docs/gardener/proposals/15-manage-bastions-and-ssh-key-pair-rotation/">GEP-15&lt;/a>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DenyInvalidExtensionResources&lt;/td>
&lt;td>&lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td>Causes the &lt;code>seed-admission-controller&lt;/code> to deny invalid extension resources, instead of just logging validation errors.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>WorkerPoolKubernetesVersion&lt;/td>
&lt;td>&lt;code>gardener-apiserver&lt;/code>&lt;/td>
&lt;td>Allows to overwrite the Kubernetes version used for shoot clusters per worker pool (see &lt;a href="https://gardener.cloud/docs/gardener/usage/worker_pool_k8s_versions/">this document&lt;/a>)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CopyEtcdBackupsDuringControlPlaneMigration&lt;/td>
&lt;td>&lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td>Enables the copy of etcd backups from the object store of the source seed to the object store of the destination seed during control plane migration.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SecretBindingProviderValidation&lt;/td>
&lt;td>&lt;code>gardener-apiserver&lt;/code>&lt;/td>
&lt;td>Enables validations on Gardener API server that:&lt;br>- requires the provider type of a SecretBinding to be set (on SecretBinding creation)&lt;br>- requires the SecretBinding provider type to match the Shoot provider type (on Shoot creation)&lt;br>- enforces immutability on the provider type of a SecretBinding&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ForceRestore&lt;/td>
&lt;td>&lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td>Enables forcing the shoot&amp;rsquo;s restoration to the destination seed during control plane migration if the preparation for migration in the source seed is not finished after a certain grace period and is considered unlikely to succeed (falling back to the &lt;a href="https://gardener.cloud/docs/gardener/proposals/17-shoot-control-plane-migration-bad-case/">control plane migration &amp;ldquo;bad case&amp;rdquo; scenario&lt;/a>). If you enable this feature gate, make sure to also enable &lt;code>UseDNSRecords&lt;/code> and &lt;code>CopyEtcdBackupsDuringControlPlaneMigration&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DisableDNSProviderManagement&lt;/td>
&lt;td>&lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td>Disables management of &lt;code>dns.gardener.cloud/v1alpha1.DNSProvider&lt;/code> resources. In this case, the &lt;code>shoot-dns-service&lt;/code> extension will take this over if it is installed. This feature is only effective if the feature &lt;code>UseDNSRecords&lt;/code> is &lt;code>true&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ShootCARotation&lt;/td>
&lt;td>&lt;code>gardener-apiserver&lt;/code>, &lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td>Enables the feature to trigger automated CA rotation for shoot clusters.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ShootMaxTokenExpirationOverwrite&lt;/td>
&lt;td>&lt;code>gardener-apiserver&lt;/code>&lt;/td>
&lt;td>Makes the Gardener API server overwriting values in the &lt;code>.spec.kubernetes.kubeAPIServer.serviceAccountConfig.maxTokenExpiration&lt;/code> field of Shoot specifications to&lt;br>- be at least 720h (30d) when the current value is lower&lt;br>- be at most 2160h (90d) when the current value is higher&lt;br>before persisting the object to etcd.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ShootMaxTokenExpirationValidation&lt;/td>
&lt;td>&lt;code>gardener-apiserver&lt;/code>&lt;/td>
&lt;td>Enables validations on Gardener API server that enforce that the value of the &lt;code>.spec.kubernetes.kubeAPIServer.serviceAccountConfig.maxTokenExpiration&lt;/code> field&lt;br>- is at least 720h (30d).&lt;br>- is at most 2160h (90d).&lt;br>Only enable this after &lt;code>ShootMaxTokenExpirationOverwrite&lt;/code> is enabled and all shoots got updated accordingly.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Docs: Gardenlet</title><link>https://gardener.cloud/docs/gardener/concepts/gardenlet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/gardenlet/</guid><description>
&lt;h1 id="gardenlet">Gardenlet&lt;/h1>
&lt;p>Gardener is implemented using the &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">operator pattern&lt;/a>:
It uses custom controllers that act on our own custom resources,
and apply Kubernetes principles to manage clusters instead of containers.
Following this analogy, you can recognize components of the Gardener architecture
as well-known Kubernetes components, for example, shoot clusters can be compared with pods,
and seed clusters can be seen as worker nodes.&lt;/p>
&lt;p>The following Gardener components play a similar role as the corresponding components
in the Kubernetes architecture:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Gardener Component&lt;/th>
&lt;th style="text-align:left">Kubernetes Component&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">&lt;code>gardener-apiserver&lt;/code>&lt;/td>
&lt;td style="text-align:left">&lt;code>kube-apiserver&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">&lt;code>gardener-controller-manager&lt;/code>&lt;/td>
&lt;td style="text-align:left">&lt;code>kube-controller-manager&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">&lt;code>gardener-scheduler&lt;/code>&lt;/td>
&lt;td style="text-align:left">&lt;code>kube-scheduler&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">&lt;code>gardenlet&lt;/code>&lt;/td>
&lt;td style="text-align:left">&lt;code>kubelet&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Similar to how the &lt;code>kube-scheduler&lt;/code> of Kubernetes finds an appropriate node
for newly created pods, the &lt;code>gardener-scheduler&lt;/code> of Gardener finds an appropriate seed cluster
to host the control plane for newly ordered clusters.
By providing multiple seed clusters for a region or provider, and distributing the workload,
Gardener also reduces the blast radius of potential issues.&lt;/p>
&lt;p>Kubernetes runs a primary &amp;ldquo;agent&amp;rdquo; on every node, the kubelet,
which is responsible for managing pods and containers on its particular node.
Decentralizing the responsibility to the kubelet has the advantage that the overall system
is scalable. Gardener achieves the same for cluster management by using a &lt;strong>gardenlet&lt;/strong>
as primary &amp;ldquo;agent&amp;rdquo; on every seed cluster, and is only responsible for shoot clusters
located in its particular seed cluster:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardenlet-architecture-similarities_ba8a1c.png" alt="Counterparts in the Gardener Architecture and the Kubernetes Architecture">&lt;/p>
&lt;p>The &lt;code>gardener-controller-manager&lt;/code> has control loops to manage resources of the Gardener API. However, instead of letting the &lt;code>gardener-controller-manager&lt;/code> talk directly to seed clusters or shoot clusters, the responsibility isn’t only delegated to the gardenlet, but also managed using a reversed control flow: It&amp;rsquo;s up to the gardenlet to contact the Gardener API server, for example, to share a status for its managed seed clusters.&lt;/p>
&lt;p>Reversing the control flow allows placing seed clusters or shoot clusters behind firewalls without the necessity of direct access via VPN tunnels anymore.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardenlet-architecture-detailed_6f3172.png" alt="Reversed Control Flow Using a Gardenlet">&lt;/p>
&lt;h2 id="tls-bootstrapping">TLS Bootstrapping&lt;/h2>
&lt;p>Kubernetes doesn’t manage worker nodes itself, and it’s also not
responsible for the lifecycle of the kubelet running on the workers.
Similarly, Gardener doesn’t manage seed clusters itself,
so Gardener is also not responsible for the lifecycle of the gardenlet running on the seeds.
As a consequence, both the gardenlet and the kubelet need to prepare
a trusted connection to the Gardener API server
and the Kubernetes API server correspondingly.&lt;/p>
&lt;p>To prepare a trusted connection between the gardenlet
and the Gardener API server, the gardenlet initializes
a bootstrapping process after you deployed it into your seed clusters:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The gardenlet starts up with a bootstrap &lt;code>kubeconfig&lt;/code>
having a bootstrap token that allows to create &lt;code>CertificateSigningRequest&lt;/code> (CSR) resources.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>After the CSR is signed, the gardenlet downloads
the created client certificate, creates a new &lt;code>kubeconfig&lt;/code> with it,
and stores it inside a &lt;code>Secret&lt;/code> in the seed cluster.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The gardenlet deletes the bootstrap &lt;code>kubeconfig&lt;/code> secret,
and starts up with its new &lt;code>kubeconfig&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The gardenlet starts normal operation.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>The &lt;code>gardener-controller-manager&lt;/code> runs a control loop
that automatically signs CSRs created by gardenlets.&lt;/p>
&lt;blockquote>
&lt;p>The gardenlet bootstrapping process is based on the
kubelet bootstrapping process. More information:
&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/">Kubelet&amp;rsquo;s TLS bootstrapping&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>If you don&amp;rsquo;t want to run this bootstrap process you can create
a &lt;code>kubeconfig&lt;/code> pointing to the garden cluster for the gardenlet yourself,
and use field &lt;code>gardenClientConnection.kubeconfig&lt;/code> in the
gardenlet configuration to share it with the gardenlet.&lt;/p>
&lt;h2 id="gardenlet-certificate-rotation">Gardenlet Certificate Rotation&lt;/h2>
&lt;p>The certificate used to authenticate the gardenlet against the API server
has a certain validity based on the configuration of the garden cluster
(&lt;code>--cluster-signing-duration&lt;/code> flag of the &lt;code>kube-controller-manager&lt;/code> (default &lt;code>1y&lt;/code>)).
After about 80% of the validity expired, the gardenlet tries to automatically replace
the current certificate with a new one (certificate rotation).&lt;/p>
&lt;p>To use certificate rotation, you need to specify the secret to store
the &lt;code>kubeconfig&lt;/code> with the rotated certificate in field
&lt;code>.gardenClientConnection.kubeconfigSecret&lt;/code> of the
gardenlet &lt;a href="#component-configuration">component configuration&lt;/a>.&lt;/p>
&lt;h3 id="rotate-certificates-using-bootstrap-kubeconfig">Rotate certificates using bootstrap &lt;code>kubeconfig&lt;/code>&lt;/h3>
&lt;p>If the gardenlet created the certificate during the initial TLS Bootstrapping
using the Bootstrap &lt;code>kubeconfig&lt;/code>, certificates can be rotated automatically.
The same control loop in the &lt;code>gardener-controller-manager&lt;/code> that signs
the CSRs during the initial TLS Bootstrapping also automatically signs
the CSR during a certificate rotation.&lt;/p>
&lt;p>ℹ️ You can trigger an immediate renewal by annotating the &lt;code>Secret&lt;/code> in the seed
cluster stated in the &lt;code>.gardenClientConnection.kubeconfigSecret&lt;/code> field with
&lt;code>gardener.cloud/operation=renew&lt;/code> and restarting the gardenlet. After it booted
up again, gardenlet will issue a new certificate independent of the remaining
validity of the existing one.&lt;/p>
&lt;h3 id="rotate-certificate-using-custom-kubeconfig">Rotate Certificate Using Custom &lt;code>kubeconfig&lt;/code>&lt;/h3>
&lt;p>When trying to rotate a custom certificate that wasn’t created by gardenlet
as part of the TLS Bootstrap, the x509 certificate&amp;rsquo;s &lt;code>Subject&lt;/code> field
needs to conform to the following:&lt;/p>
&lt;ul>
&lt;li>the Common Name (CN) is prefixed with &lt;code>gardener.cloud:system:seed:&lt;/code>&lt;/li>
&lt;li>the Organization (O) equals &lt;code>gardener.cloud:system:seeds&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Otherwise, the &lt;code>gardener-controller-manager&lt;/code> doesn’t automatically
sign the CSR.
In this case, an external component or user needs to approve the CSR manually,
for example, using command &lt;code>kubectl certificate approve seed-csr-&amp;lt;...&amp;gt;&lt;/code>).
If that doesn’t happen within 15 minutes,
the gardenlet repeats the process and creates another CSR.&lt;/p>
&lt;h2 id="configuring-the-seed-to-work-with">Configuring the Seed to work with&lt;/h2>
&lt;p>The Gardenlet works with a single seed, which must be configured in the
&lt;code>GardenletConfiguration&lt;/code> under &lt;code>.seedConfig&lt;/code>. This must be a copy of the
&lt;code>Seed&lt;/code> resource, for example (see &lt;code>example/20-componentconfig-gardenlet.yaml&lt;/code>
for a more complete example):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: gardenlet.config.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: GardenletConfiguration
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>seedConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: aws
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-seed-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When using &lt;code>make start-gardenlet&lt;/code>, the corresponding script will automatically
fetch the seed cluster&amp;rsquo;s &lt;code>kubeconfig&lt;/code> based on the &lt;code>seedConfig.spec.secretRef&lt;/code>
and set the environment accordingly.&lt;/p>
&lt;p>On startup, gardenlet registers a &lt;code>Seed&lt;/code> resource using the given template
in &lt;code>seedConfig&lt;/code> if it&amp;rsquo;s not present already.&lt;/p>
&lt;h2 id="component-configuration">Component Configuration&lt;/h2>
&lt;p>In the component configuration for the gardenlet, it’s possible to define:&lt;/p>
&lt;ul>
&lt;li>settings for the Kubernetes clients interacting with the various clusters&lt;/li>
&lt;li>settings for the control loops inside the gardenlet&lt;/li>
&lt;li>settings for leader election and log levels, feature gates, and seed selection or seed configuration.&lt;/li>
&lt;/ul>
&lt;p>More information: &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml">Example Gardenlet Component Configuration&lt;/a>.&lt;/p>
&lt;h2 id="heartbeats">Heartbeats&lt;/h2>
&lt;p>Similar to how Kubernetes uses &lt;code>Lease&lt;/code> objects for node heart beats
(see &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/589-efficient-node-heartbeats/README.md">KEP&lt;/a>),
the gardenlet is using &lt;code>Lease&lt;/code> objects for heart beats of the seed cluster.
Every two seconds, the gardenlet checks that the seed cluster&amp;rsquo;s &lt;code>/healthz&lt;/code>
endpoint returns HTTP status code 200.
If that is the case, the gardenlet renews the lease in the Garden cluster in the &lt;code>gardener-system-seed-lease&lt;/code> namespace and updates
the &lt;code>GardenletReady&lt;/code> condition in the &lt;code>status.conditions&lt;/code> field of the &lt;code>Seed&lt;/code> resource(s).&lt;/p>
&lt;p>Similarly to the &lt;code>node-lifecycle-controller&lt;/code> inside the &lt;code>kube-controller-manager&lt;/code>,
the &lt;code>gardener-controller-manager&lt;/code> features a &lt;code>seed-lifecycle-controller&lt;/code> that sets
the &lt;code>GardenletReady&lt;/code> condition to &lt;code>Unknown&lt;/code> in case the gardenlet fails to renew the lease.
As a consequence, the &lt;code>gardener-scheduler&lt;/code> doesn’t consider this seed cluster for newly created shoot clusters anymore.&lt;/p>
&lt;h3 id="healthz-endpoint">&lt;code>/healthz&lt;/code> Endpoint&lt;/h3>
&lt;p>The gardenlet includes an HTTPS server that serves a &lt;code>/healthz&lt;/code> endpoint.
It’s used as a liveness probe in the &lt;code>Deployment&lt;/code> of the gardenlet.
If the gardenlet fails to renew its lease
then the endpoint returns &lt;code>500 Internal Server Error&lt;/code>, otherwise it returns &lt;code>200 OK&lt;/code>.&lt;/p>
&lt;p>Please note that the &lt;code>/healthz&lt;/code> only indicates whether the gardenlet
could successfully probe the Seed&amp;rsquo;s API server and renew the lease with
the Garden cluster.
It does &lt;em>not&lt;/em> show that the Gardener extension API server (with the Gardener resource groups)
is available.
However, the Gardenlet is designed to withstand such connection outages and
retries until the connection is reestablished.&lt;/p>
&lt;h2 id="control-loops">Control Loops&lt;/h2>
&lt;p>The gardenlet consists out of several controllers which are now described in more detail.&lt;/p>
&lt;p>⚠️ This section is not necessarily complete and might be under construction.&lt;/p>
&lt;h3 id="backupentry-controller">&lt;code>BackupEntry&lt;/code> Controller&lt;/h3>
&lt;p>The &lt;code>BackupEntry&lt;/code> controller reconciles those &lt;code>core.gardener.cloud/v1beta1.BackupEntry&lt;/code> resources whose &lt;code>.spec.seedName&lt;/code> value is equal to the name of a &lt;code>Seed&lt;/code> the respective gardenlet is responsible for.
Those resources are created by the &lt;code>Shoot&lt;/code> controller (only if backup is enabled for the respective &lt;code>Seed&lt;/code>) and there is exactly one &lt;code>BackupEntry&lt;/code> per &lt;code>Shoot&lt;/code>.&lt;/p>
&lt;p>The controller creates an &lt;code>extensions.gardener.cloud/v1alpha1.BackupEntry&lt;/code> resource (non-namespaced) in the seed cluster and waits until the responsible extension controller reconciled it (see &lt;a href="https://gardener.cloud/docs/gardener/extensions/backupentry/">this&lt;/a> for more details).
The status is populated in the &lt;code>.status.lastOperation&lt;/code> field.&lt;/p>
&lt;p>The &lt;code>core.gardener.cloud/v1beta1.BackupEntry&lt;/code> resource has an owner reference pointing to the corresponding &lt;code>Shoot&lt;/code>.
Hence, if the &lt;code>Shoot&lt;/code> is deleted, also the &lt;code>BackupEntry&lt;/code> resource gets deleted.
In this case, the controller deletes the &lt;code>extensions.gardener.cloud/v1alpha1.BackupEntry&lt;/code> resource in the seed cluster and waits until the responsible extension controller has deleted it.
Afterwards, the finalizer of the &lt;code>core.gardener.cloud/v1beta1.BackupEntry&lt;/code> resource is released so that it finally disappears from the system.&lt;/p>
&lt;h4 id="keep-backup-for-deleted-shoots">Keep Backup for Deleted Shoots&lt;/h4>
&lt;p>In some scenarios it might be beneficial to not immediately delete the &lt;code>BackupEntry&lt;/code>s (and with them, the etcd backup) for deleted &lt;code>Shoot&lt;/code>s.&lt;/p>
&lt;p>In this case you can configure the &lt;code>.controllers.backupEntry.deletionGracePeriodHours&lt;/code> field in the component configuration of the gardenlet.
For example, if you set it to &lt;code>48&lt;/code>, then the &lt;code>BackupEntry&lt;/code>s for deleted &lt;code>Shoot&lt;/code>s will only be deleted &lt;code>48&lt;/code> hours after the &lt;code>Shoot&lt;/code> was deleted.&lt;/p>
&lt;p>Additionally, you can limit the &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_purposes/">shoot purposes&lt;/a> for which this applies by setting &lt;code>.controllers.backupEntry.deletionGracePeriodShootPurposes[]&lt;/code>.
For example, if you set it to &lt;code>[production]&lt;/code> then only the &lt;code>BackupEntry&lt;/code>s for &lt;code>Shoot&lt;/code>s with &lt;code>.spec.purpose=production&lt;/code> will be deleted after the configured grace period. All others will be deleted immediately after the &lt;code>Shoot&lt;/code> deletion.&lt;/p>
&lt;h2 id="managed-seeds">Managed Seeds&lt;/h2>
&lt;p>Gardener users can use shoot clusters as seed clusters, so-called &amp;ldquo;managed seeds&amp;rdquo; (aka &amp;ldquo;shooted seeds&amp;rdquo;),
by creating &lt;code>ManagedSeed&lt;/code> resources.
By default, the gardenlet that manages this shoot cluster then automatically
creates a clone of itself with the same version and the same configuration
that it currently has.
Then it deploys the gardenlet clone into the managed seed cluster.&lt;/p>
&lt;p>If you want to prevent the automatic gardenlet deployment,
specify the &lt;code>seedTemplate&lt;/code> section in the &lt;code>ManagedSeed&lt;/code> resource, and don&amp;rsquo;t specify
the &lt;code>gardenlet&lt;/code> section.
In this case, you have to deploy the gardenlet on your own into the seed cluster.&lt;/p>
&lt;p>More information: &lt;a href="https://gardener.cloud/docs/gardener/usage/managed_seed/">Register Shoot as Seed&lt;/a>&lt;/p>
&lt;h2 id="migrating-from-previous-gardener-versions">Migrating from Previous Gardener Versions&lt;/h2>
&lt;p>If your Gardener version doesn’t support gardenlets yet,
no special migration is required, but the following prerequisites must be met:&lt;/p>
&lt;ul>
&lt;li>Your Gardener version is at least 0.31 before upgrading to v1.&lt;/li>
&lt;li>You have to make sure that your garden cluster is exposed in a way
that it’s reachable from all your seed clusters.&lt;/li>
&lt;/ul>
&lt;p>With previous Gardener versions, you had deployed the Gardener Helm chart
(incorporating the API server, &lt;code>controller-manager&lt;/code>, and scheduler).
With v1, this stays the same, but you now have to deploy the gardenlet Helm chart as well
into all of your seeds (if they aren’t managed, as mentioned earlier).&lt;/p>
&lt;p>More information: &lt;a href="https://gardener.cloud/docs/gardener/deployment/deploy_gardenlet/">Deploy a Gardenlet&lt;/a> for all instructions.&lt;/p>
&lt;h2 id="related-links">Related Links&lt;/h2>
&lt;p>&lt;a href="https://github.com/gardener/documentation/wiki/Architecture">Gardener Architecture&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/gardener/gardener/issues/356">Issue #356: Implement Gardener Scheduler&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/gardener/gardener/pull/2309">PR #2309: Add /healthz endpoint for Gardenlet&lt;/a>&lt;/p></description></item><item><title>Docs: Gardenlet API Access</title><link>https://gardener.cloud/docs/gardener/deployment/gardenlet_api_access/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/gardenlet_api_access/</guid><description>
&lt;h1 id="scoped-api-access-for-gardenlets">Scoped API Access for Gardenlets&lt;/h1>
&lt;p>By default, &lt;code>gardenlet&lt;/code>s have administrative access in the garden cluster.
They are able to execute any API request on any object independent of whether the object is related to the seed cluster the &lt;code>gardenlet&lt;/code> is responsible for.
As RBAC is not powerful enough for fine-grained checks and for the sake of security, Gardener provides two optional but recommended configurations for your environments that scope the API access for &lt;code>gardenlet&lt;/code>s.&lt;/p>
&lt;p>Similar to the &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">&lt;code>Node&lt;/code> authorization mode in Kubernetes&lt;/a>, Gardener features a &lt;code>SeedAuthorizer&lt;/code> plugin.
It is a special-purpose authorization plugin that specifically authorizes API requests made by the &lt;code>gardenlet&lt;/code>s.&lt;/p>
&lt;p>Likewise, similar to the &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction">&lt;code>NodeRestriction&lt;/code> admission plugin in Kubernetes&lt;/a>, Gardener features a &lt;code>SeedRestriction&lt;/code> plugin.
It is a special-purpose admission plugin that specifically limits the Kubernetes objects &lt;code>gardenlet&lt;/code>s can modify.&lt;/p>
&lt;p>📚 You might be interested to look into the &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/kubelet-authorizer.md">design proposal for scoped Kubelet API access&lt;/a> from the Kubernetes community.
It can be translated to Gardener and Gardenlets with their &lt;code>Seed&lt;/code> and &lt;code>Shoot&lt;/code> resources.&lt;/p>
&lt;h2 id="flow-diagram">Flow Diagram&lt;/h2>
&lt;p>The following diagram shows how the two plugins are included in the request flow of a &lt;code>gardenlet&lt;/code>.
When they are not enabled then the &lt;code>kube-apiserver&lt;/code> is internally authorizing the request via RBAC before forwarding the request directly to the &lt;code>gardener-apiserver&lt;/code>, i.e., the &lt;code>gardener-admission-controller&lt;/code> would not be consulted (this is not entirely correct because it also serves other admission webhook handlers, but for simplicity reasons this document focuses on the API access scope only).&lt;/p>
&lt;p>When enabling the plugins, there is one additional step for each before the &lt;code>gardener-apiserver&lt;/code> responds to the request.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardenlet_api_access_flow_6dfa06.png" alt="Flow Diagram">&lt;/p>
&lt;p>Please note that the example shows a request to an object (&lt;code>Shoot&lt;/code>) residing in one of the API groups served by &lt;code>gardener-apiserver&lt;/code>.
However, the &lt;code>gardenlet&lt;/code> is also interacting with objects in API groups served by the &lt;code>kube-apiserver&lt;/code> (e.g., &lt;code>Secret&lt;/code>,&lt;code>ConfigMap&lt;/code>, etc.).
In this case, the consultation of the &lt;code>SeedRestriction&lt;/code> admission plugin is performed by the &lt;code>kube-apiserver&lt;/code> itself before it forwards the request to the &lt;code>gardener-apiserver&lt;/code>.&lt;/p>
&lt;p>Today, the following rules are implemented:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Resource&lt;/th>
&lt;th>Verbs&lt;/th>
&lt;th>Path(s)&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>BackupBucket&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code>, &lt;code>create&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code>, &lt;code>delete&lt;/code>&lt;/td>
&lt;td>&lt;code>BackupBucket&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code> requests for all &lt;code>BackupBucket&lt;/code>s. Allow only &lt;code>create&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code>, &lt;code>delete&lt;/code> requests for &lt;code>BackupBucket&lt;/code>s assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>BackupEntry&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code>, &lt;code>create&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code>&lt;/td>
&lt;td>&lt;code>BackupEntry&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code> requests for all &lt;code>BackupEntry&lt;/code>s. Allow only &lt;code>create&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code> requests for &lt;code>BackupEntry&lt;/code>s assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code> and referencing &lt;code>BackupBucket&lt;/code>s assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>Bastion&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code>, &lt;code>create&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code>&lt;/td>
&lt;td>&lt;code>Bastion&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code> requests for all &lt;code>Bastion&lt;/code>s. Allow only &lt;code>create&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code> requests for &lt;code>Bastion&lt;/code>s assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>CertificateSigningRequest&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>, &lt;code>create&lt;/code>&lt;/td>
&lt;td>&lt;code>CertificateSigningRequest&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow only &lt;code>get&lt;/code>, &lt;code>create&lt;/code> requests for &lt;code>CertificateSigningRequest&lt;/code>s related to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>CloudProfile&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>&lt;/td>
&lt;td>&lt;code>CloudProfile&lt;/code> -&amp;gt; &lt;code>Shoot&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow only &lt;code>get&lt;/code> requests for &lt;code>CloudProfile&lt;/code>s referenced by &lt;code>Shoot&lt;/code>s that are assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>ClusterRoleBinding&lt;/code>&lt;/td>
&lt;td>&lt;code>create&lt;/code>, &lt;code>get&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code>, &lt;code>delete&lt;/code>&lt;/td>
&lt;td>&lt;code>ClusterRoleBinding&lt;/code> -&amp;gt; &lt;code>ManagedSeed&lt;/code> -&amp;gt; &lt;code>Shoot&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>create&lt;/code>, &lt;code>get&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code> requests for &lt;code>ManagedSeed&lt;/code>s in the bootstrapping phase assigned to the gardenlet&amp;rsquo;s &lt;code>Seed&lt;/code>s. Allow &lt;code>delete&lt;/code> requests from gardenlets bootstrapped via &lt;code>ManagedSeed&lt;/code>s.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>ConfigMap&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>&lt;/td>
&lt;td>&lt;code>ConfigMap&lt;/code> -&amp;gt; &lt;code>Shoot&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow only &lt;code>get&lt;/code> requests for &lt;code>ConfigMap&lt;/code>s referenced by &lt;code>Shoot&lt;/code>s that are assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>. Allows reading the &lt;code>kube-system/cluster-identity&lt;/code> &lt;code>ConfigMap&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>ControllerRegistration&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code>&lt;/td>
&lt;td>&lt;code>ControllerRegistration&lt;/code> -&amp;gt; &lt;code>ControllerInstallation&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code> requests for all &lt;code>ControllerRegistration&lt;/code>s.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>ControllerDeployment&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>&lt;/td>
&lt;td>&lt;code>ControllerDeployment&lt;/code> -&amp;gt; &lt;code>ControllerInstallation&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>get&lt;/code> requests for &lt;code>ControllerDeployments&lt;/code>s referenced by &lt;code>ControllerInstallation&lt;/code>s assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>ControllerInstallation&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code>&lt;/td>
&lt;td>&lt;code>ControllerInstallation&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code> requests for all &lt;code>ControllerInstallation&lt;/code>s. Allow only &lt;code>update&lt;/code>, &lt;code>patch&lt;/code> requests for &lt;code>ControllerInstallation&lt;/code>s assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>Event&lt;/code>&lt;/td>
&lt;td>&lt;code>create&lt;/code>, &lt;code>patch&lt;/code>&lt;/td>
&lt;td>none&lt;/td>
&lt;td>Allow to &lt;code>create&lt;/code> or &lt;code>patch&lt;/code> all kinds of &lt;code>Event&lt;/code>s.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>ExposureClass&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>&lt;/td>
&lt;td>&lt;code>ExposureClass&lt;/code> -&amp;gt; &lt;code>Shoot&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>get&lt;/code> requests for &lt;code>ExposureClass&lt;/code>es referenced by &lt;code>Shoot&lt;/code>s that are assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>. Deny &lt;code>get&lt;/code> requests to other &lt;code>ExposureClass&lt;/code>es.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>Lease&lt;/code>&lt;/td>
&lt;td>&lt;code>create&lt;/code>, &lt;code>get&lt;/code>, &lt;code>watch&lt;/code>, &lt;code>update&lt;/code>&lt;/td>
&lt;td>&lt;code>Lease&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>create&lt;/code>, &lt;code>get&lt;/code>, &lt;code>update&lt;/code>, and &lt;code>delete&lt;/code> requests for &lt;code>Lease&lt;/code>s of the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>ManagedSeed&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code>&lt;/td>
&lt;td>&lt;code>ManagedSeed&lt;/code> -&amp;gt; &lt;code>Shoot&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code> requests for all &lt;code>ManagedSeed&lt;/code>s. Allow only &lt;code>update&lt;/code>, &lt;code>patch&lt;/code> requests for &lt;code>ManagedSeed&lt;/code>s referencing a &lt;code>Shoot&lt;/code> assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>Namespace&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>&lt;/td>
&lt;td>&lt;code>Namespace&lt;/code> -&amp;gt; &lt;code>Shoot&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>get&lt;/code> requests for &lt;code>Namespace&lt;/code>s of &lt;code>Shoot&lt;/code>s that are assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>. Always allow &lt;code>get&lt;/code> requests for the &lt;code>garden&lt;/code> &lt;code>Namespace&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>Project&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>&lt;/td>
&lt;td>&lt;code>Project&lt;/code> -&amp;gt; &lt;code>Namespace&lt;/code> -&amp;gt; &lt;code>Shoot&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>get&lt;/code> requests for &lt;code>Project&lt;/code>s referenced by the &lt;code>Namespace&lt;/code> of &lt;code>Shoot&lt;/code>s that are assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>SecretBinding&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>&lt;/td>
&lt;td>&lt;code>SecretBinding&lt;/code> -&amp;gt; &lt;code>Shoot&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow only &lt;code>get&lt;/code> requests for &lt;code>SecretBinding&lt;/code>s referenced by &lt;code>Shoot&lt;/code>s that are assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>Secret&lt;/code>&lt;/td>
&lt;td>&lt;code>create&lt;/code>, &lt;code>get&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code>, &lt;code>delete&lt;/code>(, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code>)&lt;/td>
&lt;td>&lt;code>Secret&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>, &lt;code>Secret&lt;/code> -&amp;gt; &lt;code>Shoot&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>, &lt;code>Secret&lt;/code> -&amp;gt; &lt;code>SecretBinding&lt;/code> -&amp;gt; &lt;code>Shoot&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>, &lt;code>BackupBucket&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code> requests for all &lt;code>Secret&lt;/code>s in the &lt;code>seed-&amp;lt;name&amp;gt;&lt;/code> namespace. Allow only &lt;code>create&lt;/code>, &lt;code>get&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code>, &lt;code>delete&lt;/code> requests for the &lt;code>Secret&lt;/code>s related to resources assigned to the gardenlet&lt;code>'s &lt;/code>Seed`s.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>Seed&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code>, &lt;code>create&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code>, &lt;code>delete&lt;/code>&lt;/td>
&lt;td>&lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code> requests for all &lt;code>Seed&lt;/code>s. Allow only &lt;code>create&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code>, &lt;code>delete&lt;/code> requests for the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>s. [1]&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>ServiceAccount&lt;/code>&lt;/td>
&lt;td>&lt;code>create&lt;/code>, &lt;code>get&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code>, &lt;code>delete&lt;/code>&lt;/td>
&lt;td>&lt;code>ServiceAccount&lt;/code> -&amp;gt; &lt;code>ManagedSeed&lt;/code> -&amp;gt; &lt;code>Shoot&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>create&lt;/code>, &lt;code>get&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code> requests for &lt;code>ManagedSeed&lt;/code>s in the bootstrapping phase assigned to the gardenlet&amp;rsquo;s &lt;code>Seed&lt;/code>s. Allow &lt;code>delete&lt;/code> requests from gardenlets bootstrapped via &lt;code>ManagedSeed&lt;/code>s.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>Shoot&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code>&lt;/td>
&lt;td>&lt;code>Shoot&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow &lt;code>get&lt;/code>, &lt;code>list&lt;/code>, &lt;code>watch&lt;/code> requests for all &lt;code>Shoot&lt;/code>s. Allow only &lt;code>update&lt;/code>, &lt;code>patch&lt;/code> requests for &lt;code>Shoot&lt;/code>s assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>ShootState&lt;/code>&lt;/td>
&lt;td>&lt;code>get&lt;/code>, &lt;code>create&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code>&lt;/td>
&lt;td>&lt;code>ShootState&lt;/code> -&amp;gt; &lt;code>Shoot&lt;/code> -&amp;gt; &lt;code>Seed&lt;/code>&lt;/td>
&lt;td>Allow only &lt;code>get&lt;/code>, &lt;code>create&lt;/code>, &lt;code>update&lt;/code>, &lt;code>patch&lt;/code> requests for &lt;code>ShootState&lt;/code>s belonging by &lt;code>Shoot&lt;/code>s that are assigned to the &lt;code>gardenlet&lt;/code>&amp;rsquo;s &lt;code>Seed&lt;/code>.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>[1] If you use &lt;code>ManagedSeed&lt;/code> resources then the gardenlet reconciling them (&amp;ldquo;parent gardenlet&amp;rdquo;) may be allowed to submit certain requests for the &lt;code>Seed&lt;/code> resources resulting out of such &lt;code>ManagedSeed&lt;/code> reconciliations (even if the &amp;ldquo;parent gardenlet&amp;rdquo; is not responsible for them):&lt;/p>
&lt;ul>
&lt;li>ℹ️ It is allowed to delete the &lt;code>Seed&lt;/code> resources if the corresponding &lt;code>ManagedSeed&lt;/code> objects already have a &lt;code>deletionTimestamp&lt;/code> (this is secure as gardenlets themselves don&amp;rsquo;t have permissions for deleting &lt;code>ManagedSeed&lt;/code>s).&lt;/li>
&lt;li>⚠ It is allowed to create or update &lt;code>Seed&lt;/code> resources if the corresponding &lt;code>ManagedSeed&lt;/code> objects use a seed template, i.e., &lt;code>.spec.seedTemplate != nil&lt;/code>. In this case, there is at least one gardenlet in your system which is responsible for two or more &lt;code>Seed&lt;/code>s. Please keep in mind that this use case is not recommended for production scenarios (you should only have one dedicated gardenlet per seed cluster), hence, the security improvements discussed in this document might be limited.&lt;/li>
&lt;/ul>
&lt;h2 id="seedauthorizer-authorization-webhook-enablement">&lt;code>SeedAuthorizer&lt;/code> Authorization Webhook Enablement&lt;/h2>
&lt;p>The &lt;code>SeedAuthorizer&lt;/code> is implemented as &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/webhook/">Kubernetes authorization webhook&lt;/a> and part of the &lt;a href="https://gardener.cloud/docs/gardener/concepts/admission-controller/">&lt;code>gardener-admission-controller&lt;/code>&lt;/a> component running in the garden cluster.&lt;/p>
&lt;p>🎛 In order to activate it, you have to follow these steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Set the following flags for the &lt;code>kube-apiserver&lt;/code> of the garden cluster (i.e., the &lt;code>kube-apiserver&lt;/code> whose API is extended by Gardener):&lt;/p>
&lt;ul>
&lt;li>&lt;code>--authorization-mode=RBAC,Node,Webhook&lt;/code> (please note that &lt;code>Webhook&lt;/code> should appear after &lt;code>RBAC&lt;/code> in the list [1]; &lt;code>Node&lt;/code> might not be needed if you use a virtual garden cluster)&lt;/li>
&lt;li>&lt;code>--authorization-webhook-config-file=&amp;lt;path-to-the-webhook-config-file&amp;gt;&lt;/code>&lt;/li>
&lt;li>&lt;code>--authorization-webhook-cache-authorized-ttl=0&lt;/code>&lt;/li>
&lt;li>&lt;code>--authorization-webhook-cache-unauthorized-ttl=0&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>The webhook config file (stored at &lt;code>&amp;lt;path-to-the-webhook-config-file&amp;gt;&lt;/code>) should look as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusters:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> certificate-authority-data: base64(CA-CERT-OF-GARDENER-ADMISSION-CONTROLLER)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: https://gardener-admission-controller.garden/webhooks/auth/seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>users:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: auth-webhook
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>current-context: auth-webhook
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>When deploying the &lt;a href="https://github.com/gardener/gardener/tree/master/charts/gardener/controlplane">Gardener &lt;code>controlplane&lt;/code> Helm chart&lt;/a>, set &lt;code>.global.rbac.seedAuthorizer.enabled=true&lt;/code>. This will prevent that the RBAC resources granting global access for all gardenlets will be deployed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Delete the existing RBAC resources granting global access for all gardenlets by running:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl delete &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> clusterrole.rbac.authorization.k8s.io/gardener.cloud:system:seeds &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> clusterrolebinding.rbac.authorization.k8s.io/gardener.cloud:system:seeds &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --ignore-not-found
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;p>Please note that you should activate the &lt;a href="#seedrestriction-admission-webhook-enablement">&lt;code>SeedRestriction&lt;/code>&lt;/a> admission handler as well.&lt;/p>
&lt;blockquote>
&lt;p>[1] The reason for the fact that &lt;code>Webhook&lt;/code> authorization plugin should appear after &lt;code>RBAC&lt;/code> is that the &lt;code>kube-apiserver&lt;/code> will be depending on the &lt;code>gardener-admission-controller&lt;/code> (serving the webhook). However, the &lt;code>gardener-admission-controller&lt;/code> can only start when &lt;code>gardener-apiserver&lt;/code> runs, but &lt;code>gardener-apiserver&lt;/code> itself can only start when &lt;code>kube-apiserver&lt;/code> runs. If &lt;code>Webhook&lt;/code> is before &lt;code>RBAC&lt;/code> then &lt;code>gardener-apiserver&lt;/code> might not be able to start, leading to a deadlock.&lt;/p>
&lt;/blockquote>
&lt;h3 id="authorizer-decisions">Authorizer Decisions&lt;/h3>
&lt;p>As mentioned earlier, it&amp;rsquo;s the authorizer&amp;rsquo;s job to evaluate API requests and return one of the following decisions:&lt;/p>
&lt;ul>
&lt;li>&lt;code>DecisionAllow&lt;/code>: The request is allowed, further configured authorizers won&amp;rsquo;t be consulted.&lt;/li>
&lt;li>&lt;code>DecisionDeny&lt;/code>: The request is denied, further configured authorizers won&amp;rsquo;t be consulted.&lt;/li>
&lt;li>&lt;code>DecisionNoOpinion&lt;/code>: A decision cannot be made, further configured authorizers will be consulted.&lt;/li>
&lt;/ul>
&lt;p>For backwards compatibility, no requests are denied at the moment, so that they are still deferred to a subsequent authorizer like RBAC.
Though, this might change in the future.&lt;/p>
&lt;p>First, the &lt;code>SeedAuthorizer&lt;/code> extracts the &lt;code>Seed&lt;/code> name from the API request. This requires a proper TLS certificate the &lt;code>gardenlet&lt;/code> uses to contact the API server and is automatically given if &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/#TLS-Bootstrapping">TLS bootstrapping&lt;/a> is used.
Concretely, the authorizer checks the certificate for name &lt;code>gardener.cloud:system:seed:&amp;lt;seed-name&amp;gt;&lt;/code> and group &lt;code>gardener.cloud:system:seeds&lt;/code>.
In cases where this information is missing e.g., when a custom Kubeconfig is used, the authorizer cannot make any decision. Thus, RBAC is still a considerable option to restrict the &lt;code>gardenlet&lt;/code>&amp;rsquo;s access permission if the above explained preconditions are not given.&lt;/p>
&lt;p>With the &lt;code>Seed&lt;/code> name at hand, the authorizer checks for an &lt;strong>existing path&lt;/strong> from the resource that a request is being made for to the &lt;code>Seed&lt;/code> belonging to the &lt;code>gardenlet&lt;/code>. Take a look at the &lt;a href="#implementation-details">Implementation Details&lt;/a> section for more information.&lt;/p>
&lt;h3 id="implementation-details">Implementation Details&lt;/h3>
&lt;p>Internally, the &lt;code>SeedAuthorizer&lt;/code> uses a directed, acyclic graph data structure in order to efficiently respond to authorization requests for gardenlets:&lt;/p>
&lt;ul>
&lt;li>A vertex in this graph represents a Kubernetes resource with its kind, namespace, and name (e.g., &lt;code>Shoot:garden-my-project/my-shoot&lt;/code>).&lt;/li>
&lt;li>An edge from vertex &lt;code>u&lt;/code> to vertex &lt;code>v&lt;/code> in this graph exists when
&lt;ul>
&lt;li>(1) &lt;code>v&lt;/code> is referred by &lt;code>u&lt;/code> and &lt;code>v&lt;/code> is a &lt;code>Seed&lt;/code>, or when&lt;/li>
&lt;li>(2) &lt;code>u&lt;/code> is referred by &lt;code>v&lt;/code>, or when&lt;/li>
&lt;li>(3) &lt;code>u&lt;/code> is strictly associated with &lt;code>v&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>For example, a &lt;code>Shoot&lt;/code> refers to a &lt;code>Seed&lt;/code>, a &lt;code>CloudProfile&lt;/code>, a &lt;code>SecretBinding&lt;/code>, etc., so it has an outgoing edge to the &lt;code>Seed&lt;/code> (1) and incoming edges from the &lt;code>CloudProfile&lt;/code> and &lt;code>SecretBinding&lt;/code> vertices (2).
However, there might also be a &lt;code>ShootState&lt;/code> or a &lt;code>BackupEntry&lt;/code> resource strictly associated with this &lt;code>Shoot&lt;/code>, hence, it has incoming edges from these vertices (3).&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardenlet_api_access_graph_5486ba.png" alt="Resource Dependency Graph">&lt;/p>
&lt;p>In above picture the resources that are actively watched have are shaded.
Gardener resources are green while Kubernetes resources are blue.
It shows the dependencies between the resources and how the graph is built based on above rules.&lt;/p>
&lt;p>ℹ️ Above picture shows all resources that may be accessed by &lt;code>gardenlet&lt;/code>s, except for the &lt;code>Quota&lt;/code> resource which is only included for completeness.&lt;/p>
&lt;p>Now, when a &lt;code>gardenlet&lt;/code> wants to access certain resources then the &lt;code>SeedAuthorizer&lt;/code> uses a Depth-First traversal starting from the vertex representing the resource in question, e.g., from a &lt;code>Project&lt;/code> vertex.
If there is a path from the &lt;code>Project&lt;/code> vertex to the vertex representing the &lt;code>Seed&lt;/code> the gardenlet is responsible for then it allows the request.&lt;/p>
&lt;h4 id="metrics">Metrics&lt;/h4>
&lt;p>The &lt;code>SeedAuthorizer&lt;/code> registers the following metrics related to the mentioned graph implementation:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Metric&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>gardener_admission_controller_seed_authorizer_graph_update_duration_seconds&lt;/code>&lt;/td>
&lt;td>Histogram of duration of resource dependency graph updates in seed authorizer, i.e., how long does it take to update the graph&amp;rsquo;s vertices/edges when a resource is created, changed, or deleted.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener_admission_controller_seed_authorizer_graph_path_check_duration_seconds&lt;/code>&lt;/td>
&lt;td>Histogram of duration of checks whether a path exists in the resource dependency graph in seed authorizer.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="debug-handler">Debug Handler&lt;/h4>
&lt;p>When the &lt;code>.server.enableDebugHandlers&lt;/code> field in the &lt;code>gardener-admission-controller&lt;/code>&amp;rsquo;s component configuration is set to &lt;code>true&lt;/code> then it serves a handler that can be used for debugging the resource dependency graph under &lt;code>/debug/resource-dependency-graph&lt;/code>.&lt;/p>
&lt;p>🚨 Only use this setting for development purposes as it enables unauthenticated users to view all data if they have access to the &lt;code>gardener-admission-controller&lt;/code> component.&lt;/p>
&lt;p>The handler renders an HTML page displaying the current graph with a list of vertices and its associated incoming and outgoing edges to other vertices.
Depending on the size of the Gardener landscape (and consequently, the size of the graph), it might not be possible to render it in its entirety.
If there are more than 2000 vertices then the default filtering will selected for &lt;code>kind=Seed&lt;/code> to prevent overloading the output.&lt;/p>
&lt;p>&lt;em>Example output&lt;/em>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>-------------------------------------------------------------------------------
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>|
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| # Seed:my-seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| &amp;lt;- (11)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| BackupBucket:73972fe2-3d7e-4f61-a406-b8f9e670e6b7
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| BackupEntry:garden-my-project/shoot--dev--my-shoot--4656a460-1a69-4f00-9372-7452cbd38ee3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| ControllerInstallation:dns-external-mxt8m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| ControllerInstallation:extension-shoot-cert-service-4qw5j
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| ControllerInstallation:networking-calico-bgrb2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| ControllerInstallation:os-gardenlinux-qvb5z
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| ControllerInstallation:provider-gcp-w4mvf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| Secret:garden/backup
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| Shoot:garden-my-project/my-shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>|
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-------------------------------------------------------------------------------
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>|
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| # Shoot:garden-my-project/my-shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| &amp;lt;- (5)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| CloudProfile:gcp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| Namespace:garden-my-project
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| Secret:garden-my-project/my-dns-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| SecretBinding:garden-my-project/my-credentials
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| ShootState:garden-my-project/my-shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| -&amp;gt; (1)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| Seed:my-seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>|
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-------------------------------------------------------------------------------
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>|
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| # ShootState:garden-my-project/my-shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| -&amp;gt; (1)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| Shoot:garden-my-project/my-shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>|
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>-------------------------------------------------------------------------------
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>... (etc., similarly for the other resources)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There are anchor links to easily jump from one resource to another, and the page provides means for filtering the results based on the &lt;code>kind&lt;/code>, &lt;code>namespace&lt;/code>, and/or &lt;code>name&lt;/code>.&lt;/p>
&lt;h4 id="pitfalls">Pitfalls&lt;/h4>
&lt;p>When there is a relevant update to an existing resource, i.e., when a reference to another resource is changed, then the corresponding vertex (along with all associated edges) is first deleted from the graph before it gets added again with the up-to-date edges.
However, this does only work for vertices belonging to resources that are only created in exactly one &amp;ldquo;watch handler&amp;rdquo;.
For example, the vertex for a &lt;code>SecretBinding&lt;/code> can either be created in the &lt;code>SecretBinding&lt;/code> handler itself or in the &lt;code>Shoot&lt;/code> handler.
In such cases, deleting the vertex before (re-)computing the edges might lead to race conditions and potentially renders the graph invalid.
Consequently, instead of deleting the vertex, only the edges the respective handler is responsible for are deleted.
If the vertex ends up with no remaining edges then it also gets deleted automatically.
Afterwards, the vertex can either be added again or the updated edges can be created.&lt;/p>
&lt;h2 id="seedrestriction-admission-webhook-enablement">&lt;code>SeedRestriction&lt;/code> Admission Webhook Enablement&lt;/h2>
&lt;p>The &lt;code>SeedRestriction&lt;/code> is implemented as &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">Kubernetes admission webhook&lt;/a> and part of the &lt;a href="https://gardener.cloud/docs/gardener/concepts/admission-controller/">&lt;code>gardener-admission-controller&lt;/code>&lt;/a> component running in the garden cluster.&lt;/p>
&lt;p>🎛 In order to activate it, you have to set &lt;code>.global.admission.seedRestriction.enabled=true&lt;/code> when using the &lt;a href="https://github.com/gardener/gardener/tree/master/charts/gardener/controlplane">Gardener &lt;code>controlplane&lt;/code> Helm chart&lt;/a>.
This will add an additional webhook in the existing &lt;code>ValidatingWebhookConfiguration&lt;/code> of the &lt;code>gardener-admission-controller&lt;/code> which contains the configuration for the &lt;code>SeedRestriction&lt;/code> handler.
Please note that it should only be activated when the &lt;code>SeedAuthorizer&lt;/code> is active as well.&lt;/p>
&lt;h3 id="admission-decisions">Admission Decisions&lt;/h3>
&lt;p>The admission&amp;rsquo;s purpose is to perform extended validation on requests which require the body of the object in question.
Additionally, it handles &lt;code>CREATE&lt;/code> requests of gardenlets (above discussed resource dependency graph cannot be used in such cases because there won&amp;rsquo;t be any vertex/edge for non-existing resources).&lt;/p>
&lt;p>Gardenlets are restricted to only create new resources which are somehow related to the seed clusters they are responsible for.&lt;/p></description></item><item><title>Docs: Getting Started Locally</title><link>https://gardener.cloud/docs/gardener/deployment/getting_started_locally/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/getting_started_locally/</guid><description>
&lt;h1 id="deploying-gardener-locally">Deploying Gardener locally&lt;/h1>
&lt;p>This document will walk you through deploying Gardener on your local machine.
If you encounter difficulties, please open an issue so that we can make this process easier.&lt;/p>
&lt;p>Gardener runs in any Kubernetes cluster.
In this guide, we will start a &lt;a href="https://kind.sigs.k8s.io/">KinD&lt;/a> cluster which is used as both garden and seed cluster (please refer to the &lt;a href="https://gardener.cloud/docs/gardener/concepts/architecture/">architecture overview&lt;/a>) for simplicity.&lt;/p>
&lt;p>Based on &lt;a href="https://skaffold.dev/">Skaffold&lt;/a>, the container images for all required components will be built and deployed into the cluster (via their &lt;a href="https://helm.sh/">Helm charts&lt;/a>).&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/getting_started_locally_f66391.png" alt="Architecture Diagram">&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Make sure your Docker daemon is up-to-date, up and running and has enough resources (at least &lt;code>8&lt;/code> CPUs and &lt;code>8Gi&lt;/code> memory; see &lt;a href="https://docs.docker.com/desktop/mac/#resources">here&lt;/a> how to configure the resources for Docker for Mac).&lt;/p>
&lt;blockquote>
&lt;p>Please note that 8 CPU / 8Gi memory might not be enough for more than two &lt;code>Shoot&lt;/code> clusters, i.e., you might need to increase these values if you want to run additional &lt;code>Shoot&lt;/code>s.&lt;/p>
&lt;/blockquote>
&lt;p>Additionally, please configure at least &lt;code>120Gi&lt;/code> of disk size for the Docker daemon.&lt;/p>
&lt;blockquote>
&lt;p>Tip: With &lt;code>docker system df&lt;/code> and &lt;code>docker system prune -a&lt;/code> you can cleanup unused data.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;h2 id="setting-up-the-kind-cluster-garden-and-seed">Setting up the KinD cluster (garden and seed)&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make kind-up
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This command sets up a new KinD cluster named &lt;code>gardener-local&lt;/code> and stores the kubeconfig in the &lt;code>./example/gardener-local/kind/kubeconfig&lt;/code> file.&lt;/p>
&lt;blockquote>
&lt;p>It might be helpful to copy this file to &lt;code>$HOME/.kube/config&lt;/code> since you will need to target this KinD cluster multiple times.
Alternatively, make sure to set your &lt;code>KUBECONFIG&lt;/code> environment variable to &lt;code>./example/gardener-local/kind/kubeconfig&lt;/code> for all future steps via &lt;code>export KUBECONFIG=example/gardener-local/kind/kubeconfig&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>All following steps assume that your are using this kubeconfig.&lt;/p>
&lt;h2 id="setting-up-gardener">Setting up Gardener&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make gardener-up
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will first build the images based (which might take a bit if you do it for the first time).
Afterwards, the Gardener resources will be deployed into the cluster.&lt;/p>
&lt;h2 id="creating-a-shoot-cluster">Creating a &lt;code>Shoot&lt;/code> cluster&lt;/h2>
&lt;p>You can wait for the &lt;code>Seed&lt;/code> to be ready by running&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl wait --for=condition=gardenletready --for=condition=extensionsready --for=condition=bootstrapped seed local --timeout=5m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Alternatively, you can run &lt;code>kubectl get seed local&lt;/code> and wait for the &lt;code>STATUS&lt;/code> to indicate readiness:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>NAME STATUS PROVIDER REGION AGE VERSION K8S VERSION
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>local Ready local local 4m42s vX.Y.Z-dev v1.21.1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In order to create a first shoot cluster, just run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f example/provider-local/shoot.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can wait for the &lt;code>Shoot&lt;/code> to be ready by running&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl wait --for=condition=apiserveravailable --for=condition=controlplanehealthy --for=condition=everynodeready --for=condition=systemcomponentshealthy shoot local -n garden-local --timeout=10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Alternatively, you can run &lt;code>kubectl -n garden-local get shoot local&lt;/code> and wait for the &lt;code>LAST OPERATION&lt;/code> to reach &lt;code>100%&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>NAME CLOUDPROFILE PROVIDER REGION K8S VERSION HIBERNATION LAST OPERATION STATUS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>local local local local 1.21.0 Awake Create Processing (43%) healthy 94s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>(Optional): You could also execute a simple e2e test (creating and deleting a shoot) by running&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>make test-e2e-local-fast KUBECONFIG=&lt;span style="color:#a31515">&amp;#34;&lt;/span>$PWD&lt;span style="color:#a31515">/example/gardener-local/kind/kubeconfig&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>⚠️ Please note that in this setup shoot clusters are not accessible by default when you download the kubeconfig and try to communicate with them.
The reason is that your host most probably cannot resolve the DNS names of the clusters since &lt;code>provider-local&lt;/code> extension runs inside the KinD cluster (see &lt;a href="https://gardener.cloud/docs/gardener/extensions/provider-local/#dnsrecord">this&lt;/a> for more details).
Hence, if you want to access the shoot cluster, you have to run the following command which will extend your &lt;code>/etc/hosts&lt;/code> file with the required information to make the DNS names resolvable:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cat &lt;span style="color:#a31515">&amp;lt;&amp;lt;EOF | sudo tee -a /etc/hosts
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"># Manually created to access local Gardener shoot clusters with names &amp;#39;local&amp;#39; or &amp;#39;e2e-local&amp;#39; in the &amp;#39;garden-local&amp;#39; namespace.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"># TODO: Remove this again when the shoot cluster access is no longer required.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">127.0.0.1 api.local.local.external.local.gardener.cloud
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">127.0.0.1 api.local.local.internal.local.gardener.cloud
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">127.0.0.1 api.e2e-local.local.external.local.gardener.cloud
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">127.0.0.1 api.e2e-local.local.internal.local.gardener.cloud
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now you can access it by running&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n garden-local get secret local.kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d &amp;gt; /tmp/kubeconfig-shoot-local.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl --kubeconfig=/tmp/kubeconfig-shoot-local.yaml get nodes
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="deleting-the-shoot-cluster">Deleting the &lt;code>Shoot&lt;/code> cluster&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>./hack/usage/delete shoot local garden-local
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="tear-down-the-gardener-environment">Tear down the Gardener environment&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>make kind-down
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="further-reading">Further reading&lt;/h2>
&lt;p>This setup makes use of the local provider extension. You can read more about it in &lt;a href="https://gardener.cloud/docs/gardener/extensions/provider-local/">this document&lt;/a>.&lt;/p></description></item><item><title>Docs: Getting Started Locally</title><link>https://gardener.cloud/docs/gardener/development/getting_started_locally/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/getting_started_locally/</guid><description>
&lt;h1 id="running-gardener-locally">Running Gardener locally&lt;/h1>
&lt;p>This document will walk you through running Gardener on your local machine for development purposes.
If you encounter difficulties, please open an issue so that we can make this process easier.&lt;/p>
&lt;p>Gardener runs in any Kubernetes cluster.
In this guide, we will start a &lt;a href="https://kind.sigs.k8s.io/">KinD&lt;/a> cluster which is used as both garden and seed cluster (please refer to the &lt;a href="https://gardener.cloud/docs/gardener/concepts/architecture/">architecture overview&lt;/a>) for simplicity.&lt;/p>
&lt;p>The Gardener components, however, will be run as regular processes on your machine (hence, no container images are being built).&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/getting_started_locally_39a5b3.png" alt="Architecture Diagram">&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Make sure your Docker daemon is up-to-date, up and running and has enough resources (at least &lt;code>4&lt;/code> CPUs and &lt;code>4Gi&lt;/code> memory; see &lt;a href="https://docs.docker.com/desktop/mac/#resources">here&lt;/a> how to configure the resources for Docker for Mac).&lt;/p>
&lt;blockquote>
&lt;p>Please note that 4 CPU / 4Gi memory might not be enough for more than one &lt;code>Shoot&lt;/code> cluster, i.e., you might need to increase these values if you want to run additional &lt;code>Shoot&lt;/code>s.&lt;/p>
&lt;/blockquote>
&lt;p>Additionally, please configure at least &lt;code>120Gi&lt;/code> of disk size for the Docker daemon.&lt;/p>
&lt;blockquote>
&lt;p>Tip: With &lt;code>docker system df&lt;/code> and &lt;code>docker system prune -a&lt;/code> you can cleanup unused data.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>Make sure that you increase the maximum number of open files on your host:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>On Mac, run &lt;code>sudo launchctl limit maxfiles 65536 200000&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>On Linux, extend the &lt;code>/etc/security/limits.conf&lt;/code> file with&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>* hard nofile 97816
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* soft nofile 97816
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>and reload the terminal.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="setting-up-the-kind-cluster-garden-and-seed">Setting up the KinD cluster (garden and seed)&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make kind-up KIND_ENV=local
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This command sets up a new KinD cluster named &lt;code>gardener-local&lt;/code> and stores the kubeconfig in the &lt;code>./example/gardener-local/kind/kubeconfig&lt;/code> file.&lt;/p>
&lt;blockquote>
&lt;p>It might be helpful to copy this file to &lt;code>$HOME/.kube/config&lt;/code> since you will need to target this KinD cluster multiple times.
Alternatively, make sure to set your &lt;code>KUBECONFIG&lt;/code> environment variable to &lt;code>./example/gardener-local/kind/kubeconfig&lt;/code> for all future steps via &lt;code>export KUBECONFIG=example/gardener-local/kind/kubeconfig&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>All following steps assume that your are using this kubeconfig.&lt;/p>
&lt;h2 id="setting-up-gardener">Setting up Gardener&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make dev-setup &lt;span style="color:#008000"># preparing the environment (without webhooks for now)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl wait --for=condition=ready pod -l run=etcd -n garden --timeout 2m &lt;span style="color:#008000"># wait for etcd to be ready&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>make start-apiserver &lt;span style="color:#008000"># starting gardener-apiserver&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In a new terminal pane, run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl wait --for=condition=available apiservice v1beta1.core.gardener.cloud &lt;span style="color:#008000"># wait for gardener-apiserver to be ready&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>make start-admission-controller &lt;span style="color:#008000"># starting gardener-admission-controller&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In a new terminal pane, run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make dev-setup DEV_SETUP_WITH_WEBHOOKS=true &lt;span style="color:#008000"># preparing the environment with webhooks&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>make start-controller-manager &lt;span style="color:#008000"># starting gardener-controller-manager&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>(Optional): In a new terminal pane, run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make start-scheduler &lt;span style="color:#008000"># starting gardener-scheduler&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In a new terminal pane, run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make register-local-env &lt;span style="color:#008000"># registering the local environment (CloudProfile, Seed, etc.)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>make start-gardenlet SEED_NAME=local &lt;span style="color:#008000"># starting gardenlet&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In a new terminal pane, run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make start-extension-provider-local &lt;span style="color:#008000"># starting gardener-extension-provider-local&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ℹ️ The &lt;a href="https://gardener.cloud/docs/gardener/extensions/provider-local/">&lt;code>provider-local&lt;/code>&lt;/a> is started with elevated privileges since it needs to manipulate your &lt;code>/etc/hosts&lt;/code> file to enable you accessing the created shoot clusters from your local machine, see &lt;a href="https://gardener.cloud/docs/gardener/extensions/provider-local/#dnsrecord">this&lt;/a> for more details.&lt;/p>
&lt;h2 id="creating-a-shoot-cluster">Creating a &lt;code>Shoot&lt;/code> cluster&lt;/h2>
&lt;p>You can wait for the &lt;code>Seed&lt;/code> to be ready by running&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl wait --for=condition=gardenletready --for=condition=extensionsready --for=condition=bootstrapped seed local --timeout=5m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Alternatively, you can run &lt;code>kubectl get seed local&lt;/code> and wait for the &lt;code>STATUS&lt;/code> to indicate readiness:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>NAME STATUS PROVIDER REGION AGE VERSION K8S VERSION
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>local Ready local local 4m42s vX.Y.Z-dev v1.21.1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In order to create a first shoot cluster, just run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f example/provider-local/shoot.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can wait for the &lt;code>Shoot&lt;/code> to be ready by running&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl wait --for=condition=apiserveravailable --for=condition=controlplanehealthy --for=condition=everynodeready --for=condition=systemcomponentshealthy shoot local -n garden-local --timeout=10m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Alternatively, you can run &lt;code>kubectl -n garden-local get shoot local&lt;/code> and wait for the &lt;code>LAST OPERATION&lt;/code> to reach &lt;code>100%&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>NAME CLOUDPROFILE PROVIDER REGION K8S VERSION HIBERNATION LAST OPERATION STATUS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>local local local local 1.21.0 Awake Create Processing (43%) healthy 94s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>(Optional): You could also execute a simple e2e test (creating and deleting a shoot) by running&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>make test-e2e-local-fast KUBECONFIG=&lt;span style="color:#a31515">&amp;#34;&lt;/span>$PWD&lt;span style="color:#a31515">/example/gardener-local/kind/kubeconfig&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When the shoot got successfully created you can access it as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n garden-local get secret local.kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d &amp;gt; /tmp/kubeconfig-shoot-local.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl --kubeconfig=/tmp/kubeconfig-shoot-local.yaml get nodes
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="deleting-the-shoot-cluster">Deleting the &lt;code>Shoot&lt;/code> cluster&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>./hack/usage/delete shoot local garden-local
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="tear-down-the-gardener-environment">Tear down the Gardener environment&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>make tear-down-local-env
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>make kind-down
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="further-reading">Further reading&lt;/h2>
&lt;p>This setup makes use of the local provider extension. You can read more about it in &lt;a href="https://gardener.cloud/docs/gardener/extensions/provider-local/">this document&lt;/a>.&lt;/p></description></item><item><title>Docs: Image Vector</title><link>https://gardener.cloud/docs/gardener/deployment/image_vector/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/image_vector/</guid><description>
&lt;h1 id="image-vector">Image Vector&lt;/h1>
&lt;p>The Gardenlet is deploying several different container images into the seed and the shoot clusters.
The image repositories and tags are defined in a &lt;a href="https://github.com/gardener/gardener/blob/master/charts/images.yaml">central image vector file&lt;/a>.
Obviously, the image versions defined there must fit together with the deployment manifests (e.g., some command-line flags do only exist in certain versions).&lt;/p>
&lt;h2 id="example">Example&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>images:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: pause-container
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> repository: gcr.io/google_containers/pause-amd64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tag: &lt;span style="color:#a31515">&amp;#34;3.0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.17.x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: pause-container
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> repository: gcr.io/google_containers/pause-amd64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tag: &lt;span style="color:#a31515">&amp;#34;3.1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: &lt;span style="color:#a31515">&amp;#34;&amp;gt;= 1.18&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>That means that the Gardenlet will use the &lt;code>pause-container&lt;/code> in with tag &lt;code>3.0&lt;/code> for all seed/shoot clusters with Kubernetes version &lt;code>1.17.x&lt;/code>, and tag &lt;code>3.1&lt;/code> for all clusters with Kubernetes &lt;code>&amp;gt;= 1.18&lt;/code>.&lt;/p>
&lt;h2 id="overwrite-image-vector">Overwrite image vector&lt;/h2>
&lt;p>In some environment it is not possible to use these &amp;ldquo;pre-defined&amp;rdquo; images that come with a Gardener release.
A prominent example for that is Alicloud in China which does not allow access to Google&amp;rsquo;s GCR.
In these cases you might want to overwrite certain images, e.g., point the &lt;code>pause-container&lt;/code> to a different registry.&lt;/p>
&lt;p>⚠️ If you specify an image that does not fit to the resource manifest then the seed/shoot reconciliation might fail.&lt;/p>
&lt;p>In order to overwrite the images you must provide a similar file to Gardenlet:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>images:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: pause-container
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> repository: my-custom-image-registry/pause-amd64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tag: &lt;span style="color:#a31515">&amp;#34;3.0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.17.x
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: pause-container
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sourceRepository: github.com/kubernetes/kubernetes/blob/master/build/pause/Dockerfile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> repository: my-custom-image-registry/pause-amd64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tag: &lt;span style="color:#a31515">&amp;#34;3.1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: &lt;span style="color:#a31515">&amp;#34;&amp;gt;= 1.18&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>During deployment of the gardenlet create a &lt;code>ConfigMap&lt;/code> containing the above content and mount it as a volume into the gardenlet pod.
Next, specify the environment variable &lt;code>IMAGEVECTOR_OVERWRITE&lt;/code> whose value must be the path to the file you just mounted:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ConfigMap
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardenlet-images-overwrite
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> images_overwrite.yaml: |&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> images:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> - ...&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Deployment
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardenlet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: gardenlet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> env:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: IMAGEVECTOR_OVERWRITE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: /charts-overwrite/images_overwrite.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumeMounts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: gardenlet-images-overwrite
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /charts-overwrite
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: gardenlet-images-overwrite
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> configMap:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardenlet-images-overwrite
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="image-vectors-for-dependent-components">Image vectors for dependent components&lt;/h2>
&lt;p>The gardenlet is deploying a lot of different components that might deploy other images themselves.
These components might use an image vector as well.
Operators might want to customize the image locations for these transitive images as well, hence, they might need to specify an image vector overwrite for the components directly deployed by Gardener.&lt;/p>
&lt;p>It is possible to specify the &lt;code>IMAGEVECTOR_OVERWRITE_COMPONENTS&lt;/code> environment variable to the gardenlet that points to a file with the following content:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>components:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: etcd-druid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageVectorOverwrite: |&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> images:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> - name: etcd
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> tag: v1.2.3
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> repository: etcd/etcd&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The gardenlet will, if supported by the directly deployed component (&lt;code>etcd-druid&lt;/code> in this example), inject the given &lt;code>imageVectorOverwrite&lt;/code> into the &lt;code>Deployment&lt;/code> manifest.
The respective component is responsible for using the overwritten images instead of its defaults.&lt;/p></description></item><item><title>Docs: Kubernetes Clients</title><link>https://gardener.cloud/docs/gardener/development/kubernetes-clients/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/kubernetes-clients/</guid><description>
&lt;h1 id="kubernetes-clients-in-gardener">Kubernetes Clients in Gardener&lt;/h1>
&lt;p>This document aims at providing a general developer guideline on different aspects of using Kubernetes clients in a large-scale distributed system and project like Gardener.
The points included here are not meant to be consulted as absolute rules, but rather as general rules of thumb, that allow developers to get a better feeling about certain gotchas and caveats.
It should be updated with lessons learned from maintaining the project and running Gardener in production.&lt;/p>
&lt;p>&lt;strong>Prerequisites&lt;/strong>:&lt;/p>
&lt;p>Please familiarize yourself with the following basic Kubernetes API concepts first, if you&amp;rsquo;re new to Kubernetes. A good understanding of these basics will help you better comprehend the following document.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/using-api/api-concepts/">Kubernetes API Concepts&lt;/a> (including terminology, watch basics, etc.)&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/">Extending the Kubernetes API&lt;/a> (including Custom Resources and aggregation layer / extension API servers)&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">Extend the Kubernetes API with CustomResourceDefinitions&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/">Working with Kubernetes Objects&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md">Sample Controller&lt;/a> (the diagram helps to build an understanding of an controller&amp;rsquo;s basic structure)&lt;/li>
&lt;/ul>
&lt;h2 id="client-types-client-go-generated-controller-runtime">Client Types: Client-Go, Generated, Controller-Runtime&lt;/h2>
&lt;p>For historical reasons, you will find different kinds of Kubernetes clients in Gardener:&lt;/p>
&lt;h3 id="client-go-clients">Client-Go Clients&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/client-go">client-go&lt;/a> is the default/official client for talking to the Kubernetes API in Golang.
It features so called &lt;a href="https://github.com/kubernetes/client-go/blob/release-1.21/kubernetes/clientset.go#L72">&amp;ldquo;client sets&amp;rdquo;&lt;/a> for all built-in Kubernetes API groups and versions (e.g. &lt;code>v1&lt;/code> (aka &lt;code>core/v1&lt;/code>), &lt;code>apps/v1&lt;/code>, etc.).
client-go clients are generated from the built-in API types using &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/generating-clientset.md">client-gen&lt;/a> and are composed of interfaces for every known API GroupVersionKind.
A typical client-go usage looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx context.Context
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c kubernetes.Interface &lt;span style="color:#008000">// &amp;#34;k8s.io/client-go/kubernetes&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> deployment *appsv1.Deployment &lt;span style="color:#008000">// &amp;#34;k8s.io/api/apps/v1&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>updatedDeployment, err := c.AppsV1().Deployments(&lt;span style="color:#a31515">&amp;#34;default&amp;#34;&lt;/span>).Update(ctx, deployment, metav1.UpdateOptions{})
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;em>Important characteristics of client-go clients:&lt;/em>&lt;/p>
&lt;ul>
&lt;li>clients are specific to a given API GroupVersionKind, i.e., clients are hard-coded to corresponding API-paths (don&amp;rsquo;t need to use the discovery API to map GVK to a REST endpoint path).&lt;/li>
&lt;li>client&amp;rsquo;s don&amp;rsquo;t modify the passed in-memory object (e.g. &lt;code>deployment&lt;/code> in the above example). Instead, they return a new in-memory object.&lt;br>
This means, controllers have to continue working with the new in-memory object or overwrite the shared object to not lose any state updates.&lt;/li>
&lt;/ul>
&lt;h3 id="generated-client-sets-for-gardener-apis">Generated Client Sets for Gardener APIs&lt;/h3>
&lt;p>Gardener&amp;rsquo;s APIs extend the Kubernetes API by registering an extension API server (in the garden cluster) and &lt;code>CustomResourceDefinition&lt;/code>s (on Seed clusters), meaning that the Kubernetes API will expose additional REST endpoints to manage Gardener resources in addition to the built-in API resources.
In order to talk to these extended APIs in our controllers and components, client-gen is used to generate client-go-style clients to &lt;a href="https://github.com/gardener/gardener/tree/master/pkg/client">&lt;code>pkg/client/{core,extensions,seedmanagement,...}&lt;/code>&lt;/a>.&lt;/p>
&lt;p>Usage of these clients is equivalent to &lt;code>client-go&lt;/code> clients, and the same characteristics apply. For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx context.Context
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c gardencoreclientset.Interface &lt;span style="color:#008000">// &amp;#34;github.com/gardener/gardener/pkg/client/core/clientset/versioned&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> shoot *gardencorev1beta1.Shoot &lt;span style="color:#008000">// &amp;#34;github.com/gardener/gardener/pkg/apis/core/v1beta1&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>updatedShoot, err := c.CoreV1beta1().Shoots(&lt;span style="color:#a31515">&amp;#34;garden-my-project&amp;#34;&lt;/span>).Update(ctx, shoot, metav1.UpdateOptions{})
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="controller-runtime-clients">Controller-Runtime Clients&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes-sigs/controller-runtime">controller-runtime&lt;/a> is a Kubernetes community project (&lt;a href="https://github.com/kubernetes-sigs/kubebuilder">kubebuilder&lt;/a> subproject) for building controllers and operators for custom resources.
Therefore, it features a generic client, that follows a different approach and does not rely on generated client sets. Instead, the client can be used for managing any Kubernetes resources (built-in or custom) homogeneously.
For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx context.Context
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c client.Client &lt;span style="color:#008000">// &amp;#34;sigs.k8s.io/controller-runtime/pkg/client&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> deployment *appsv1.Deployment &lt;span style="color:#008000">// &amp;#34;k8s.io/api/apps/v1&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> shoot *gardencorev1beta1.Shoot &lt;span style="color:#008000">// &amp;#34;github.com/gardener/gardener/pkg/apis/core/v1beta1&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>err := c.Update(ctx, deployment)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// or
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>err = c.Update(ctx, shoot)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A brief introduction to controller-runtime and its basic constructs can be found &lt;a href="https://pkg.go.dev/sigs.k8s.io/controller-runtime">here&lt;/a>.&lt;/p>
&lt;p>&lt;em>Important characteristics of controller-runtime clients:&lt;/em>&lt;/p>
&lt;ul>
&lt;li>The client functions take a generic &lt;code>client.Object&lt;/code> or &lt;code>client.ObjectList&lt;/code> value. These interfaces are implemented by all Golang types, that represent Kubernetes API objects or lists respectively which can be interacted with via usual API requests. [1]&lt;/li>
&lt;li>The client first consults a &lt;code>runtime.Scheme&lt;/code> (configured during client creation) for recognizing the object&amp;rsquo;s &lt;code>GroupVersionKind&lt;/code> (this happens on the client-side only).&lt;br>
A &lt;code>runtime.Scheme&lt;/code> is basically a registry for Golang API types, defaulting and conversion functions. Schemes are usually provided per &lt;code>GroupVersion&lt;/code> (see &lt;a href="https://github.com/kubernetes/api/blob/release-1.21/apps/v1/register.go">this example&lt;/a> for &lt;code>apps/v1&lt;/code>) and can be combined to one single scheme for further usage (&lt;a href="https://github.com/gardener/gardener/blob/v1.29.0/pkg/client/kubernetes/types.go#L96">example&lt;/a>). In controller-runtime clients, schemes are used only for mapping a typed API object to its &lt;code>GroupVersionKind&lt;/code>.&lt;/li>
&lt;li>It then consults a &lt;code>meta.RESTMapper&lt;/code> (also configured during client creation) for mapping the &lt;code>GroupVersionKind&lt;/code> to a &lt;code>RESTMapping&lt;/code>, which contains the &lt;code>GroupVersionResource&lt;/code> and &lt;code>Scope&lt;/code> (namespaced or cluster-scoped). From these values, the client can unambiguously determine the REST endpoint path of the corresponding API resource. For instance: &lt;code>appsv1.DeploymentList&lt;/code> is available at &lt;code>/apis/apps/v1/deployments&lt;/code> or &lt;code>/apis/apps/v1/namespaces/&amp;lt;namespace&amp;gt;/deployments&lt;/code> respectively.
&lt;ul>
&lt;li>There are different &lt;code>RESTMapper&lt;/code> implementations, but generally they are talking to the API server&amp;rsquo;s discovery API for retrieving &lt;code>RESTMappings&lt;/code> for all API resources known to the API server (either built-in, registered via API extension or &lt;code>CustomResourceDefinition&lt;/code>s).&lt;/li>
&lt;li>The default implementation of controller-runtime (which Gardener uses as well), is the &lt;a href="https://github.com/kubernetes-sigs/controller-runtime/blob/v0.9.0/pkg/client/apiutil/dynamicrestmapper.go#L77">dynamic &lt;code>RESTMapper&lt;/code>&lt;/a>. It caches discovery results (i.e. &lt;code>RESTMappings&lt;/code>) in-memory and only re-discovers resources from the API server, when a client tries to use an unknown &lt;code>GroupVersionKind&lt;/code>, i.e., when it encounters a &lt;code>No{Kind,Resource}MatchError&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The client writes back results from the API server into the passed in-memory object.
&lt;ul>
&lt;li>This means, that controllers don&amp;rsquo;t have to worry about copying back the results and should just continue to work on the given in-memory object.&lt;/li>
&lt;li>This is a nice and flexible pattern and helper functions should try to follow it wherever applicable. Meaning, if possible accept an object param, pass it down to clients and keep working on the same in-memory object instead of creating a new one in your helper function.&lt;/li>
&lt;li>The benefit is, that you don&amp;rsquo;t lose updates to the API object and always have the last-known state in memory. Therefore, you don&amp;rsquo;t have to read it again, e.g., for getting the current &lt;code>resourceVersion&lt;/code> when working with &lt;a href="#conflicts-concurrency-control-and-optimistic-locking">optimistic locking&lt;/a>, and thus minimize the chances for running into conflicts.&lt;/li>
&lt;li>However, controllers &lt;em>must not&lt;/em> use the same in-memory object concurrently in multiple goroutines. For example, decoding results from the API server in multiple goroutines into the same maps (e.g., labels, annotations) will cause panics because of &amp;ldquo;concurrent map writes&amp;rdquo;. Also, reading from an in-memory API object in one goroutine while decoding into it in another goroutine will yield non-atomic reads, meaning data might be corrupt and represent a non-valid/non-existing API object.&lt;/li>
&lt;li>Therefore, if you need to use the same in-memory object in multiple goroutines concurrently (e.g., shared state), remember to leverage proper synchronization techniques like channels, mutexes, &lt;code>atomic.Value&lt;/code> and/or copy the object prior to use. The average controller however, will not need to share in-memory API objects between goroutines, and it&amp;rsquo;s typically an indicator that the controller&amp;rsquo;s design should be improved.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The client decoder erases the object&amp;rsquo;s &lt;code>TypeMeta&lt;/code> (&lt;code>apiVersion&lt;/code> and &lt;code>kind&lt;/code> fields) after retrieval from the API server, see &lt;a href="https://github.com/kubernetes/kubernetes/issues/80609">kubernetes/kubernetes#80609&lt;/a>, &lt;a href="https://github.com/kubernetes-sigs/controller-runtime/issues/1517">kubernetes-sigs/controller-runtime#1517&lt;/a>.
Unstructured and metadata-only requests objects are an exception to this because the contained &lt;code>TypeMeta&lt;/code> is the only way to identify the object&amp;rsquo;s type.
Because of this behavior, &lt;code>obj.GetObjectKind().GroupVersionKind()&lt;/code> is likely to return an empty &lt;code>GroupVersionKind&lt;/code>.
I.e., you must not rely on &lt;code>TypeMeta&lt;/code> being set or &lt;code>GetObjectKind()&lt;/code> to return something usable.&lt;br>
If you need to identify an object&amp;rsquo;s &lt;code>GroupVersionKind&lt;/code>, use a scheme and its &lt;code>ObjectKinds&lt;/code> function instead (or the helper function &lt;code>apiutil.GVKForObject&lt;/code>).
This is not specific to controller-runtime clients and applies to client-go clients as well.&lt;/li>
&lt;/ul>
&lt;p>[1] Other lower level, config or internal API types (e.g., such as &lt;a href="https://github.com/kubernetes/api/blob/release-1.21/admission/v1/types.go#L29">&lt;code>AdmissionReview&lt;/code>&lt;/a>) don&amp;rsquo;t implement &lt;code>client.Object&lt;/code>. However, you also can&amp;rsquo;t interact with such objects via the Kubernetes API and thus also not via a client, so this can be disregarded at this point.&lt;/p>
&lt;h3 id="metadata-only-clients">Metadata-Only Clients&lt;/h3>
&lt;p>Additionally, controller-runtime clients can be used to easily retrieve metadata-only objects or lists.
This is useful for efficiently checking if at least one object of a given kind exists, or retrieving metadata of an object, if one is not interested in the rest (e.g., spec/status).&lt;br>
The &lt;code>Accept&lt;/code> header sent to the API server then contains &lt;code>application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1&lt;/code>, which makes the API server only return metadata of the retrieved object(s).
This saves network traffic and cpu/memory load on the API server and client side.
If the client fully lists all objects of a given kind including their spec/status, the resulting list can be quite large and easily exceed the controllers available memory.
That&amp;rsquo;s why it&amp;rsquo;s important to carefully check, if a full list is actually needed or if metadata-only list can be used instead.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx context.Context
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c client.Client &lt;span style="color:#008000">// &amp;#34;sigs.k8s.io/controller-runtime/pkg/client&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> shootList = &amp;amp;metav1.PartialObjectMetadataList{} &lt;span style="color:#008000">// &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>shootList.SetGroupVersionKind(gardencorev1beta1.SchemeGroupVersion.WithKind(&lt;span style="color:#a31515">&amp;#34;ShootList&amp;#34;&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">if&lt;/span> err := c.List(ctx, shootList, client.InNamespace(&lt;span style="color:#a31515">&amp;#34;garden-my-project&amp;#34;&lt;/span>), client.Limit(1)); err != &lt;span style="color:#00f">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00f">return&lt;/span> err
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">if&lt;/span> len(shootList.Items) &amp;gt; 0 {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">// project has at least one shoot
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>} &lt;span style="color:#00f">else&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">// project doesn&amp;#39;t have any shoots
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="gardeners-client-collection-clientmaps">Gardener&amp;rsquo;s Client Collection, ClientMaps&lt;/h3>
&lt;p>The Gardener codebase has a collection of clients (&lt;a href="https://github.com/gardener/gardener/blob/v1.29.0/pkg/client/kubernetes/types.go#L149">&lt;code>kubernetes.Interface&lt;/code>&lt;/a>), which can return all the above mentioned client types.
Additionally, it contains helpers for rendering and applying helm charts (&lt;code>ChartRender&lt;/code>, &lt;code>ChartApplier&lt;/code>) and retrieving the API server&amp;rsquo;s version (&lt;code>Version&lt;/code>).&lt;br>
Client sets are managed by so called &lt;code>ClientMap&lt;/code>s, which are a form of registry for all client set for a given type of cluster, i.e., Garden, Seed, Shoot and Plant.
ClientMaps manage the whole lifecycle of clients: they take care of creating them if they don&amp;rsquo;t exist already, running their caches, refreshing their cached server version and invalidating them when they are no longer needed.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx context.Context
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cm clientmap.ClientMap &lt;span style="color:#008000">// &amp;#34;github.com/gardener/gardener/pkg/client/kubernetes/clientmap&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> shoot *gardencorev1beta1.Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cs, err := cm.GetClient(ctx, keys.ForShoot(shoot)) &lt;span style="color:#008000">// kubernetes.Interface
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>&lt;span style="color:#00f">if&lt;/span> err != &lt;span style="color:#00f">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00f">return&lt;/span> err
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>c := cs.Client() &lt;span style="color:#008000">// client.Client
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The client collection mainly exist for historical reasons (there used to be a lot of code using the client-go style clients).
However, Gardener is in the process of moving more towards controller-runtime and only using their clients, as they provide many benefits and are much easier to use.
Also, &lt;a href="https://github.com/gardener/gardener/issues/4251">gardener/gardener#4251&lt;/a> aims at refactoring our controller and admission components to native controller-runtime components.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ Please always prefer controller-runtime clients over other clients when writing new code or refactoring existing code.&lt;/p>
&lt;/blockquote>
&lt;h2 id="cache-types-informers-listers-controller-runtime-caches">Cache Types: Informers, Listers, Controller-Runtime Caches&lt;/h2>
&lt;p>Similar to the different types of client(set)s, there are also different kinds of Kubernetes client caches.
However, all of them are based on the same concept: &lt;code>Informer&lt;/code>s.
An &lt;code>Informer&lt;/code> is a watch-based cache implementation, meaning it opens &lt;a href="https://kubernetes.io/docs/reference/using-api/api-concepts/#efficient-detection-of-changes">watch connections&lt;/a> to the API server and continuously updates cached objects based on the received watch events (&lt;code>ADDED&lt;/code>, &lt;code>MODIFIED&lt;/code>, &lt;code>DELETED&lt;/code>).
&lt;code>Informer&lt;/code>s offer to add indices to the cache for efficient object lookup (e.g., by name or labels) and to add &lt;code>EventHandler&lt;/code>s for the watch events.
The latter is used by controllers to fill queues with objects that should be reconciled on watch events.&lt;/p>
&lt;p>Informers are used in and created via several higher-level constructs:&lt;/p>
&lt;h3 id="sharedinformerfactories-listers">SharedInformerFactories, Listers&lt;/h3>
&lt;p>The generated clients (built-in as well as extended) feature a &lt;code>SharedInformerFactory&lt;/code> for every API group, which can be used to create and retrieve &lt;code>Informers&lt;/code> for all GroupVersionKinds.
Similarly, it can be used to retrieve &lt;code>Listers&lt;/code>, that allow getting and listing objects from the &lt;code>Informer&lt;/code>&amp;rsquo;s cache.
However, both of these constructs are only used for historical reasons, and we are in the process of migrating away from them in favor of cached controller-runtime clients (see &lt;a href="https://github.com/gardener/gardener/issues/2414">gardener/gardener#2414&lt;/a>, &lt;a href="https://github.com/gardener/gardener/issues/2822">gardener/gardener#2822&lt;/a>). Thus, they are described only briefly here.&lt;/p>
&lt;p>&lt;em>Important characteristics of Listers:&lt;/em>&lt;/p>
&lt;ul>
&lt;li>Objects read from Informers and Listers can always be slightly out-out-date (i.e., stale) because the client has to first observe changes to API objects via watch events (which can intermittently lag behind by a second or even more).&lt;/li>
&lt;li>Thus, don&amp;rsquo;t make any decisions based on data read from Listers if the consequences of deciding wrongfully based on stale state might be catastrophic (e.g. leaking infrastructure resources). In such cases, read directly from the API server via a client instead.&lt;/li>
&lt;li>Objects retrieved from Informers or Listers are pointers to the cached objects, so they must not be modified without copying them first, otherwise the objects in the cache are also modified.&lt;/li>
&lt;/ul>
&lt;h3 id="controller-runtime-caches">Controller-Runtime Caches&lt;/h3>
&lt;p>controller-runtime features a cache implementation that can be used equivalently as their clients. In fact, it implements a subset of the &lt;code>client.Client&lt;/code> interface containing the &lt;code>Get&lt;/code> and &lt;code>List&lt;/code> functions.
Under the hood, a &lt;code>cache.Cache&lt;/code> dynamically creates &lt;code>Informers&lt;/code> (i.e., opens watches) for every object GroupVersionKind that is being retrieved from it.&lt;/p>
&lt;p>Note, that the underlying Informers of a controller-runtime cache (&lt;code>cache.Cache&lt;/code>) and the ones of a &lt;code>SharedInformerFactory&lt;/code> (client-go) are not related in any way.
Both create &lt;code>Informers&lt;/code> and watch objects on the API server individually.
This means, that if you read the same object from different cache implementations, you may receive different versions of the object because the watch connections of the individual Informers are not synced.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ Because of this, controllers/reconcilers should get the object from the same cache in the reconcile loop, where the &lt;code>EventHandler&lt;/code> was also added to set up the controller. For example, if a &lt;code>SharedInformerFactory&lt;/code> is used for setting up the controller then read the object in the reconciler from the &lt;code>Lister&lt;/code> instead of from a cached controller-runtime client.&lt;/p>
&lt;/blockquote>
&lt;p>By default, the &lt;code>client.Client&lt;/code> created by a controller-runtime &lt;code>Manager&lt;/code> is a &lt;code>DelegatingClient&lt;/code>. It delegates &lt;code>Get&lt;/code> and &lt;code>List&lt;/code> calls to a &lt;code>Cache&lt;/code> and all other calls to a client, that talks directly to the API server. Exceptions are requests with &lt;code>*unstructured.Unstructured&lt;/code> objects and object kinds that were configured to be excluded from the cache in the &lt;code>DelegatingClient&lt;/code>.&lt;/p>
&lt;blockquote>
&lt;p>ℹ️
&lt;code>kubernetes.Interface.Client()&lt;/code> returns a &lt;code>DelegatingClient&lt;/code> that uses the cache returned from &lt;code>kubernetes.Interface.Cache()&lt;/code> under the hood. This means, all &lt;code>Client()&lt;/code> usages need to be ready for cached clients and should be able to cater with stale cache reads.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;em>Important characteristics of cached controller-runtime clients:&lt;/em>&lt;/p>
&lt;ul>
&lt;li>Like for Listers, objects read from a controller-runtime cache can always be slightly out of date. Hence, don&amp;rsquo;t base any important decisions on data read from the cache (see above).&lt;/li>
&lt;li>In contrast to Listers, controller-runtime caches fill the passed in-memory object with the state of the object in the cache (i.e., they perform something like a &amp;ldquo;deep copy into&amp;rdquo;). This means that objects read from a controller-runtime cache can safely be modified without unintended side effects.&lt;/li>
&lt;li>Reading from a controller-runtime cache or a cached controller-runtime client implicitly starts a watch for the given object kind under the hood. This has important consequences:
&lt;ul>
&lt;li>Reading a given object kind from the cache for the first time can take up to a few seconds depending on size and amount of objects as well as API server latency. This is because the cache has to do a full list operation and wait for an initial watch sync before returning results.&lt;/li>
&lt;li>⚠️ Controllers need appropriate RBAC permissions for the object kinds they retrieve via cached clients (i.e., &lt;code>list&lt;/code> and &lt;code>watch&lt;/code>).&lt;/li>
&lt;li>⚠️ By default, watches started by a controller-runtime cache are cluster-scoped, meaning it watches and caches objects across all namespaces. Thus, be careful which objects to read from the cache as it might significantly increase the controller&amp;rsquo;s memory footprint.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>There is no interaction with the cache on writing calls (&lt;code>Create&lt;/code>, &lt;code>Update&lt;/code>, &lt;code>Patch&lt;/code> and &lt;code>Delete&lt;/code>), see below.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Uncached objects, filtered caches, &lt;code>APIReader&lt;/code>s:&lt;/strong>&lt;/p>
&lt;p>In order to allow more granular control over which object kinds should be cached and which calls should bypass the cache, controller-runtime offers a few mechanisms to further tweak the client/cache behavior:&lt;/p>
&lt;ul>
&lt;li>When creating a &lt;code>DelegatingClient&lt;/code>, certain object kinds can be configured to always be read directly from the API instead of from the cache. Note that this does not prevent starting a new Informer when retrieving them directly from the cache.&lt;/li>
&lt;li>Watches can be restricted to a given (set of) namespace(s) by using &lt;code>cache.MultiNamespacedCacheBuilder&lt;/code> or setting &lt;code>cache.Options.Namespace&lt;/code>.&lt;/li>
&lt;li>Watches can be filtered (e.g., by label) per object kind by configuring &lt;code>cache.Options.SelectorsByObject&lt;/code> on creation of the cache.&lt;/li>
&lt;li>Retrieving metadata-only objects or lists from a cache results in a metadata-only watch/cache for that object kind.&lt;/li>
&lt;li>The &lt;code>APIReader&lt;/code> can be used to always talk directly to the API server for a given &lt;code>Get&lt;/code> or &lt;code>List&lt;/code> call (use with care and only as a last resort!).&lt;/li>
&lt;/ul>
&lt;h3 id="to-cache-or-not-to-cache">To Cache or Not to Cache&lt;/h3>
&lt;p>Although watch-based caches are an important factor for the immense scalability of Kubernetes, it definitely comes at a price (mainly in terms of memory consumption).
Thus, developers need to be careful when introducing new API calls and caching new object kinds.
Here are some general guidelines on choosing whether to read from a cache or not:&lt;/p>
&lt;ul>
&lt;li>Always try to use the cache wherever possible and make your controller able to tolerate stale reads.
&lt;ul>
&lt;li>Leverage optimistic locking: use deterministic naming for objects you create (this is what the &lt;code>Deployment&lt;/code> controller does [2]).&lt;/li>
&lt;li>Leverage optimistic locking / concurrency control of the API server: send updates/patches with the last-known &lt;code>resourceVersion&lt;/code> from the cache (see below). This will make the request fail, if there were concurrent updates to the object (conflict error), which indicates that we have operated on stale data and might have made wrong decisions. In this case, let the controller handle the error with exponential backoff. This will make the controller eventually consistent.&lt;/li>
&lt;li>Track the actions you took, e.g., when creating objects with &lt;code>generateName&lt;/code> (this is what the &lt;code>ReplicaSet&lt;/code> controller does [3]). The actions can be tracked in memory and repeated if the expected watch events don&amp;rsquo;t occur after a given amount of time.&lt;/li>
&lt;li>Always try to write controllers with the assumption that data will only be eventually correct and can be slightly out of date (even if read directly from the API server!).&lt;/li>
&lt;li>If there is already some other code that needs a cache (e.g., a controller watch), reuse it instead of doing extra direct reads.&lt;/li>
&lt;li>Don&amp;rsquo;t read an object again if you just sent a write request. Write requests (&lt;code>Create&lt;/code>, &lt;code>Update&lt;/code>, &lt;code>Patch&lt;/code> and &lt;code>Delete&lt;/code>) don&amp;rsquo;t interact with the cache. Hence, use the current state that the API server returned (filled into the passed in-memory object), which is basically a &amp;ldquo;free direct read&amp;rdquo;, instead of reading the object again from a cache, because this will probably set back the object to an older &lt;code>resourceVersion&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If you are concerned about the impact of the resulting cache, try to minimize that by using filtered or metadata-only watches.&lt;/li>
&lt;li>If watching and caching an object type is not feasible, for example because there will be a lot of updates, and you are only interested in the object every ~5m, or because it will blow up the controllers memory footprint, fallback to a direct read. This can either be done by disabling caching the object type generally or doing a single request via an &lt;code>APIReader&lt;/code>. In any case, please bear in mind that every direct API call results in a &lt;a href="https://kubernetes.io/docs/reference/using-api/api-concepts/#the-resourceversion-parameter">quorum read from etcd&lt;/a>, which can be costly in a heavily-utilized cluster and impose significant scalability limits. Thus, always try to minimize the impact of direct calls by filtering results by namespace or labels, limiting the number of results and/or using metadata-only calls.&lt;/li>
&lt;/ul>
&lt;p>[2] The &lt;code>Deployment&lt;/code> controller uses the pattern &lt;code>&amp;lt;deployment-name&amp;gt;-&amp;lt;podtemplate-hash&amp;gt;&lt;/code> for naming &lt;code>ReplicaSets&lt;/code>. This means, the name of a &lt;code>ReplicaSet&lt;/code> it tries to create/update/delete at any given time is deterministically calculated based on the &lt;code>Deployment&lt;/code> object. By this, it is insusceptible to stale reads from its &lt;code>ReplicaSets&lt;/code> cache.&lt;/p>
&lt;p>[3] In simple terms, the &lt;code>ReplicaSet&lt;/code> controller tracks its &lt;code>CREATE pod&lt;/code> actions as follows: when creating new &lt;code>Pods&lt;/code>, it increases a counter of expected &lt;code>ADDED&lt;/code> watch events for the corresponding &lt;code>ReplicaSet&lt;/code>. As soon as such events arrive, it decreases the counter accordingly. It only creates new &lt;code>Pods&lt;/code> for a given &lt;code>ReplicaSet&lt;/code>, once all expected events occurred (counter is back to zero) or a timeout occurred. This way, it prevents creating more &lt;code>Pods&lt;/code> than desired because of stale cache reads and makes the controller eventually consistent.&lt;/p>
&lt;h2 id="conflicts-concurrency-control-and-optimistic-locking">Conflicts, Concurrency Control and Optimistic Locking&lt;/h2>
&lt;p>Every Kubernetes API object contains the &lt;code>metadata.resourceVersion&lt;/code> field, which identifies an object&amp;rsquo;s version in the backing data store, i.e., etcd. Every write to an object in etcd results in a newer &lt;code>resourceVersion&lt;/code>.
This field is mainly used for concurrency control on the API server in an optimistic locking fashion, but also for efficient resumption of interrupted watch connections.&lt;/p>
&lt;p>Optimistic locking in the Kubernetes API sense means that when a client wants to update an API object then it includes the object&amp;rsquo;s &lt;code>resourceVersion&lt;/code> in the request to indicate the object&amp;rsquo;s version the modifications are based on.
If the &lt;code>resourceVersion&lt;/code> in etcd has not changed in the meantime, the update request is accepted by the API server and the updated object is written to etcd.
If the &lt;code>resourceVersion&lt;/code> sent by the client does not match the one of the object stored in etcd, there were concurrent modifications to the object. Consequently, the request is rejected with a conflict error (status code &lt;code>409&lt;/code>, API reason &lt;code>Conflict&lt;/code>), for example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;kind&amp;#34;: &lt;span style="color:#a31515">&amp;#34;Status&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;apiVersion&amp;#34;: &lt;span style="color:#a31515">&amp;#34;v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;metadata&amp;#34;: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;status&amp;#34;: &lt;span style="color:#a31515">&amp;#34;Failure&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;message&amp;#34;: &lt;span style="color:#a31515">&amp;#34;Operation cannot be fulfilled on configmaps \&amp;#34;foo\&amp;#34;: the object has been modified; please apply your changes to the latest version and try again&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;reason&amp;#34;: &lt;span style="color:#a31515">&amp;#34;Conflict&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;details&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;name&amp;#34;: &lt;span style="color:#a31515">&amp;#34;foo&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;kind&amp;#34;: &lt;span style="color:#a31515">&amp;#34;configmaps&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;code&amp;#34;: 409
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This concurrency control is an important mechanism in Kubernetes as there are typically multiple clients acting on API objects at the same time (humans, different controllers, etc.). If a client receives a conflict error, it should read the object&amp;rsquo;s latest version from the API server, make the modifications based on the newest changes and retry the update.
The reasoning behind this is that a client might choose to make different decisions based on the concurrent changes made by other actors compared to the outdated version that it operated on.&lt;/p>
&lt;p>&lt;em>Important points about concurrency control and conflicts:&lt;/em>&lt;/p>
&lt;ul>
&lt;li>The &lt;code>resourceVersion&lt;/code> field carries a string value and clients must not assume numeric values (the type and structure of versions depend on the backing data store). This means clients may compare &lt;code>resourceVersion&lt;/code> values to detect whether objects were changed. But they must not compare &lt;code>resourceVersion&lt;/code>s to figure out which one is newer/older, i.e., no greater/less-than comparisons are allowed.&lt;/li>
&lt;li>By default, update calls (e.g. via client-go and controller-runtime clients) use optimistic locking as the passed in-memory usually object contains the latest &lt;code>resourceVersion&lt;/code> known to the controller which is then also sent to the API server.&lt;/li>
&lt;li>API servers can also choose to accept update calls without optimistic locking (i.e., without a &lt;code>resourceVersion&lt;/code> in the object&amp;rsquo;s metadata) for any given resource. However, sending update requests without optimistic locking is strongly discouraged as doing so overwrites the entire object discarding any concurrent changes made to it.&lt;/li>
&lt;li>On the other side, patch requests can always be executed either with or without optimistic locking, by (not) including the &lt;code>resourceVersion&lt;/code> in the patched object&amp;rsquo;s metadata. Sending patch requests without optimistic locking might be safe and even desirable as a patch typically updates only a specific section of the object. However, there are also situations where patching without optimistic locking is not safe (see below).&lt;/li>
&lt;/ul>
&lt;h3 id="dont-retry-on-conflict">Don’t Retry on Conflict&lt;/h3>
&lt;p>Similar to how a human would typically handle a conflict error, there are helper functions implementing &lt;code>RetryOnConflict&lt;/code>-semantics, i.e., try an update call, then re-read the object if a conflict occurs, apply the modification again and retry the update.
However, controllers should generally &lt;em>not&lt;/em> use &lt;code>RetryOnConflict&lt;/code>-semantics. Instead, controllers should abort their current reconciliation run and let the queue handle the conflict error with exponential backoff.
The reasoning behind this is, that a conflict error indicates that the controller has operated on stale data and might have made wrong decisions earlier on in the reconciliation.
When using a helper function that implements &lt;code>RetryOnConflict&lt;/code>-semantics, the controller doesn&amp;rsquo;t check which fields were changed and doesn&amp;rsquo;t revise its previous decisions accordingly.
Instead, retrying on conflict basically just ignores any conflict error and blindly applies the modification.&lt;/p>
&lt;p>To properly solve the conflict situation, controllers should immediately return with the error from the update call. This will cause retries with exponential backoff so that the cache has a chance to observe the latest changes to the object.
In a later run, the controller will then make correct decisions based on the newest version of the object, not run into conflict errors and will then be able to successfully reconcile the object. This way, the controller becomes eventually consistent.&lt;/p>
&lt;p>The other way to solve the situation is to modify objects without optimistic locking in order to avoid running into a conflict in the first place (only if this is safe).
This can be a preferable solution for controllers with long-running reconciliations (which is actually an anti-pattern but quite unavoidable in some of Gardener&amp;rsquo;s controllers).
Aborting the entire reconciliation run is rather undesirable in such cases as it will add a lot of unnecessary waiting time for end users and overhead in terms of compute and network usage.&lt;/p>
&lt;p>However, in any case retrying on conflict is probably not the right option to solve the situation (there are some correct use cases for it, though, they are very rare). Hence, don&amp;rsquo;t retry on conflict.&lt;/p>
&lt;h3 id="to-lock-or-not-to-lock">To Lock or Not to Lock&lt;/h3>
&lt;p>As explained before, conflicts are actually important and prevent clients from doing wrongful concurrent updates. This means, conflicts are not something we generally want to avoid or ignore.
However, in many cases controllers are exclusive owners of the fields they want to update and thus it might be safe to run without optimistic locking.&lt;/p>
&lt;p>For example, the gardenlet is the exclusive owner of the &lt;code>spec&lt;/code> section of the Extension resources it creates on behalf of a Shoot (e.g., the &lt;code>Infrastructure&lt;/code> resource for creating VPC, etc.). Meaning, it knows the exact desired state and no other actor is supposed to update the Infrastructure&amp;rsquo;s &lt;code>spec&lt;/code> fields.
When the gardenlet now updates the Infrastructures &lt;code>spec&lt;/code> section as part of the Shoot reconciliation, it can simply issue a &lt;code>PATCH&lt;/code> request that only updates the &lt;code>spec&lt;/code> and runs without optimistic locking.
If another controller concurrently updated the object in the meantime (e.g., the &lt;code>status&lt;/code> section), the &lt;code>resourceVersion&lt;/code> got changed which would cause a conflict error if running with optimistic locking.
However, concurrent &lt;code>status&lt;/code> updates would not change the gardenlet&amp;rsquo;s mind on the desired &lt;code>spec&lt;/code> of the Infrastructure resource as it is determined only by looking at the Shoot&amp;rsquo;s specification.
If the &lt;code>spec&lt;/code> section was changed concurrently, it&amp;rsquo;s still fine to overwrite it because the gardenlet should reconcile the &lt;code>spec&lt;/code> back to its desired state.&lt;/p>
&lt;p>Generally speaking, if a controller is the exclusive owner of a given set of fields and they are independent of concurrent changes to other fields in that object, it can patch these fields without optimistic locking.
This might ignore concurrent changes to other fields or blindly overwrite changes to the same fields, but this is fine if the mentioned conditions apply.
Obviously, this applies only to patch requests that modify only a specific set of fields but not to update requests that replace the entire object.&lt;/p>
&lt;p>In such cases, it&amp;rsquo;s even desirable to run without optimistic locking as it will be more performant and save retries.
If certain requests are made with high frequency and have a good chance of causing conflicts, retries because of optimistic locking can cause a lot of additional network traffic in a large-scale Gardener installation.&lt;/p>
&lt;h2 id="updates-patches-server-side-apply">Updates, Patches, Server-side Apply&lt;/h2>
&lt;p>There are different ways of modifying Kubernetes API objects.
The following snippet demonstrates how to do a given modification with the most frequently used options using a controller-runtime client:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx context.Context
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c client.Client
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shoot *gardencorev1beta1.Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// update
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>shoot.Spec.Kubernetes.Version = &lt;span style="color:#a31515">&amp;#34;1.22&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>err := c.Update(ctx, shoot)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// json merge patch
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>patch := client.MergeFrom(shoot.DeepCopy())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>shoot.Spec.Kubernetes.Version = &lt;span style="color:#a31515">&amp;#34;1.22&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>err = c.Patch(ctx, shoot, patch)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// strategic merge patch
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>patch = client.StrategicMergeFrom(shoot.DeepCopy())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>shoot.Spec.Kubernetes.Version = &lt;span style="color:#a31515">&amp;#34;1.22&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>err = c.Patch(ctx, shoot, patch)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;em>Important characteristics of the shown request types:&lt;/em>&lt;/p>
&lt;ul>
&lt;li>Update requests always send the entire object to the API server and update all fields accordingly. By default, optimistic locking is used (&lt;code>resourceVersion&lt;/code> is included).&lt;/li>
&lt;li>Both patch types run without optimistic locking by default. However, it can be enabled explicitly if needed:
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// json merge patch + optimistic locking
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>patch := client.MergeFromWithOptions(shoot.DeepCopy(), client.MergeFromWithOptimisticLock{})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// strategic merge patch + optimistic locking
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>patch = client.StrategicMergeFrom(shoot.DeepCopy(), client.MergeFromWithOptimisticLock{})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Patch requests only contain the changes made to the in-memory object between the copy passed to &lt;code>client.*MergeFrom&lt;/code> and the object passed to &lt;code>Client.Patch()&lt;/code>. The diff is calculated on the client-side based on the in-memory objects only. This means, if in the meantime some fields were changed on the API server to a different value than the one on the client-side, the fields will not be changed back as long as they are not changed on the client-side as well (there will be no diff in memory).&lt;/li>
&lt;li>Thus, if you want to ensure a given state using patch requests, always read the object first before patching it, as there will be no diff otherwise, meaning the patch will be empty. Also see &lt;a href="https://github.com/gardener/gardener/pull/4057">gardener/gardener#4057&lt;/a> and comments in &lt;a href="https://github.com/gardener/gardener/pull/4027">gardener/gardener#4027&lt;/a>.&lt;/li>
&lt;li>Also, always send updates and patch requests even if your controller hasn&amp;rsquo;t made any changes to the current state on the API server. I.e., don&amp;rsquo;t make any optimization for preventing empty patches or no-op updates. There might be mutating webhooks in the system that will modify the object and that rely on update/patch requests being sent (even if they are no-op). Gardener&amp;rsquo;s extension concept makes heavy use of mutating webhooks, so it&amp;rsquo;s important to keep this in mind.&lt;/li>
&lt;li>JSON merge patches always replace lists as a whole and don&amp;rsquo;t merge them. Keep this in mind when operating on lists with merge patch requests. If the controller is the exclusive owner of the entire list, it&amp;rsquo;s safe to run without optimistic locking. Though, if you want to prevent overwriting concurrent changes to the list or its items made by other actors (e.g., additions/removals to the &lt;code>metadata.finalizers&lt;/code> list), enable optimistic locking.&lt;/li>
&lt;li>Strategic merge patches are able to make more granular modifications to lists and their elements without replacing the entire list. It uses Golang struct tags of the API types to determine which and how lists should be merged. See &lt;a href="https://kubernetes.io/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/">this document&lt;/a> or the &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/strategic-merge-patch.md">strategic merge patch documentation&lt;/a> for more in-depth explanations and comparison with JSON merge patches.
With this, controllers &lt;em>might&lt;/em> be able to issue patch requests for individual list items without optimistic locking, even if they are not exclusive owners of the entire list. Remember to check the &lt;code>patchStrategy&lt;/code> and &lt;code>patchMergeKey&lt;/code> struct tags of the fields you want to modify before blindly adding patch requests without optimistic locking.&lt;/li>
&lt;li>Strategic merge patches are only supported by built-in Kubernetes resources and custom resources served by Extension API servers. Strategic merge patches are not supported by custom resources defined by &lt;code>CustomResourceDefinition&lt;/code>s (see &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#advanced-features-and-flexibility">this comparison&lt;/a>). In that case, fallback to JSON merge patches.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">Server-side Apply&lt;/a> is yet another mechanism to modify API objects, which is supported by all API resources (in newer Kubernetes versions). However, it has a few problems and more caveats preventing us from using it in Gardener at the time of writing. See &lt;a href="https://github.com/gardener/gardener/issues/4122">gardener/gardener#4122&lt;/a> for more details.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Generally speaking, patches are often the better option compared to update requests because they can save network traffic, encoding/decoding effort and avoid conflicts under the presented conditions.
If choosing a patch type, consider which type is supported by the resource you&amp;rsquo;re modifying and what will happen in case of a conflict. Consider whether your modification is safe to run without optimistic locking.
However, there is no simple rule of thumb on which patch type to choose.&lt;/p>
&lt;/blockquote>
&lt;h2 id="on-helper-functions">On Helper Functions&lt;/h2>
&lt;p>Here is a note on some helper functions, that should be avoided and why:&lt;/p>
&lt;p>&lt;code>controllerutil.CreateOrUpdate&lt;/code> does a basic get, mutate and create or update call chain, which is often used in controllers. We should avoid using this helper function in Gardener, because it is likely to cause conflicts for cached clients and doesn&amp;rsquo;t send no-op requests if nothing was changed, which can cause problems because of the heavy use of webhooks in Gardener extensions (see above).
That&amp;rsquo;s why usage of this function was completely replaced in &lt;a href="https://github.com/gardener/gardener/pull/4227">gardener/gardener#4227&lt;/a> and similar PRs.&lt;/p>
&lt;p>&lt;code>controllerutil.CreateOrPatch&lt;/code> is similar to &lt;code>CreateOrUpdate&lt;/code> but does a patch request instead of an update request. It has the same drawback as &lt;code>CreateOrUpdate&lt;/code> regarding no-op updates.
Also, controllers can&amp;rsquo;t use optimistic locking or strategic merge patches when using &lt;code>CreateOrPatch&lt;/code>.
Another reason for avoiding use of this function is, that it also implicitly patches the status section if it was changed, which is confusing for others reading the code. To accomplish this, the func does some back and forth conversion, comparison and checks, which are unnecessary in most of our cases and simply wasted CPU cycles and complexity we want to avoid.&lt;/p>
&lt;p>There were some &lt;code>Try{Update,UpdateStatus,Patch,PatchStatus}&lt;/code> helper functions in Gardener that were already removed by &lt;a href="https://github.com/gardener/gardener/pull/4378">gardener/gardener#4378&lt;/a> but are still used in some extension code at the time of writing.
The reason for eliminating these functions is that they implement &lt;code>RetryOnConflict&lt;/code>-semantics. Meaning, they first get the object, mutate it, then try to update and retry if a conflict error occurs.
As explained above, retrying on conflict is a controller anti-pattern and should be avoided in almost every situation.
The other problem with these functions is that they read the object first from the API server (always do a direct call), although in most cases we already have a recent version of the object at hand. So, using this function generally does unnecessary API calls and therefore causes unwanted compute and network load.&lt;/p>
&lt;p>For the reasons explained above, there are similar helper functions that accomplish similar things but address the mentioned drawbacks: &lt;code>controllerutils.{GetAndCreateOrMergePatch,GetAndCreateOrStrategicMergePatch}&lt;/code>.
These can be safely used as replacements for the aforementioned helper funcs.
If they are not fitting for your use case, for example because you need to use optimistic locking, just do the appropriate calls in the controller directly.&lt;/p>
&lt;h2 id="further-resources">Further Resources&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=RPsUo925PUA&amp;amp;t=40s">Kubernetes Client usage in Gardener&lt;/a> (Community Meeting talk, 2020-06-26)&lt;/li>
&lt;/ul>
&lt;p>These resources are only partially related to the topics covered in this doc, but might still be interesting for developer seeking a deeper understanding of Kubernetes API machinery, architecture and foundational concepts.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md">API Conventions&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/resource-management.md">The Kubernetes Resource Model&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Local Setup</title><link>https://gardener.cloud/docs/gardener/development/local_setup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/local_setup/</guid><description>
&lt;h1 id="overview">Overview&lt;/h1>
&lt;p>Conceptually, all Gardener components are designed to run as a Pod inside a Kubernetes cluster.
The Gardener API server extends the Kubernetes API via the user-aggregated API server concepts.
However, if you want to develop it, you may want to work locally with the Gardener without building a Docker image and deploying it to a cluster each and every time.
That means that the Gardener runs outside a Kubernetes cluster which requires providing a &lt;a href="https://kubernetes.io/docs/tasks/access-application-cluster/authenticate-across-clusters-kubeconfig/">Kubeconfig&lt;/a> in your local filesystem and point the Gardener to it when starting it (see below).&lt;/p>
&lt;p>Further details can be found in&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/">Principles of Kubernetes&lt;/a>, and its &lt;a href="https://kubernetes.io/docs/concepts/overview/components/">components&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/tree/master/contributors/devel">Kubernetes Development Guide&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/documentation/wiki/Architecture">Architecture of Gardener&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>This guide is split into three main parts:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#preparing-the-setup">Preparing your setup by installing all dependencies and tools&lt;/a>&lt;/li>
&lt;li>&lt;a href="#start-gardener-locally">Building and starting Gardener components locally&lt;/a>&lt;/li>
&lt;li>&lt;a href="#create-a-shoot">Using your local Gardener setup to create a Shoot&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="limitations-of-the-local-development-setup">Limitations of the local development setup&lt;/h2>
&lt;p>You can run Gardener (API server, controller manager, scheduler, gardenlet) against any local Kubernetes cluster, however, your seed and shoot clusters must be deployed to a cloud provider.
Currently, it is not possible to run Gardener entirely isolated from any cloud provider. This means that to be able create Shoot clusters you need to register an external Seed cluster (e.g., one created in AWS).&lt;/p>
&lt;h1 id="preparing-the-setup">Preparing the Setup&lt;/h1>
&lt;h2 id="macos-only-installing-homebrew">[macOS only] Installing homebrew&lt;/h2>
&lt;p>The copy-paste instructions in this guide are designed for macOS and use the package manager &lt;a href="https://brew.sh/">Homebrew&lt;/a>.&lt;/p>
&lt;p>On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>/bin/bash -c &lt;span style="color:#a31515">&amp;#34;&lt;/span>&lt;span style="color:#00f">$(&lt;/span>curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh&lt;span style="color:#00f">)&lt;/span>&lt;span style="color:#a31515">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="installing-git">Installing git&lt;/h2>
&lt;p>We use &lt;code>git&lt;/code> as VCS which you need to install. On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install git
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For other OS, please check the &lt;a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">Git installation documentation&lt;/a>.&lt;/p>
&lt;h2 id="installing-go">Installing Go&lt;/h2>
&lt;p>Install the latest version of Go. On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install go
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For other OS, please check &lt;a href="https://golang.org/doc/install">Go installation documentation&lt;/a>.&lt;/p>
&lt;h2 id="installing-kubectl">Installing kubectl&lt;/h2>
&lt;p>Install &lt;code>kubectl&lt;/code>. Please make sure that the version of &lt;code>kubectl&lt;/code> is at least &lt;code>v1.11.x&lt;/code>. On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install kubernetes-cli
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For other OS, please check the &lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl installation documentation&lt;/a>.&lt;/p>
&lt;h2 id="installing-helm">Installing helm&lt;/h2>
&lt;p>You also need the &lt;a href="https://github.com/kubernetes/helm">Helm&lt;/a> CLI. On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install helm
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For other OS please check the &lt;a href="https://helm.sh/docs/intro/install/">Helm installation documentation&lt;/a>.&lt;/p>
&lt;h2 id="installing-openvpn">Installing openvpn&lt;/h2>
&lt;p>We use &lt;code>OpenVPN&lt;/code> to establish network connectivity from the control plane running in the Seed cluster to the Shoot&amp;rsquo;s worker nodes running in private networks.
To harden the security we need to generate another secret to encrypt the network traffic (&lt;a href="https://openvpn.net/index.php/open-source/documentation/howto.html#security">details&lt;/a>).
Please install the &lt;code>openvpn&lt;/code> binary. On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install openvpn
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export PATH=&lt;span style="color:#00f">$(&lt;/span>brew --prefix openvpn&lt;span style="color:#00f">)&lt;/span>/sbin:$PATH
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For other OS, please check the &lt;a href="https://openvpn.net/index.php/open-source/downloads.html">OpenVPN downloads page&lt;/a>.&lt;/p>
&lt;h2 id="installing-docker">Installing Docker&lt;/h2>
&lt;p>You need to have docker installed and running. On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install --cask docker
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For other OS please check the &lt;a href="https://docs.docker.com/get-docker/">docker installation documentation&lt;/a>.&lt;/p>
&lt;h2 id="installing-iproute2">Installing iproute2&lt;/h2>
&lt;p>&lt;code>iproute2&lt;/code> provides a collection of utilities for network administration and configuration. On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install iproute2mac
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="installing-jq">Installing jq&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install jq
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="installing-gnu-parallel">Installing GNU Parallel&lt;/h2>
&lt;p>&lt;a href="https://www.gnu.org/software/parallel/">GNU Parallel&lt;/a> is a shell tool for executing jobs in parallel, used by the code generation scripts (&lt;code>make generate&lt;/code>). On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install parallel
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="macos-only-install-gnu-core-utilities">[macOS only] Install GNU core utilities&lt;/h2>
&lt;p>When running on macOS, install the GNU core utilities and friends:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install coreutils gnu-sed gnu-tar grep
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will create symbolic links for the GNU utilities with &lt;code>g&lt;/code> prefix in &lt;code>/usr/local/bin&lt;/code>, e.g., &lt;code>gsed&lt;/code> or &lt;code>gbase64&lt;/code>. To allow using them without the &lt;code>g&lt;/code> prefix please put &lt;code>/usr/local/opt/coreutils/libexec/gnubin&lt;/code> etc. at the beginning of your &lt;code>PATH&lt;/code> environment variable, e.g., &lt;code>export PATH=/usr/local/opt/coreutils/libexec/gnubin:$PATH&lt;/code> (&lt;code>brew&lt;/code> will print out instructions for each installed formula).&lt;/p>
&lt;h2 id="windows-only-wsl2">[Windows only] WSL2&lt;/h2>
&lt;p>Apart from Linux distributions and macOS, the local gardener setup can also run on the Windows Subsystem for Linux 2.&lt;/p>
&lt;p>While WSL1, plain docker for windows and various Linux distributions and local Kubernetes environments may be supported, this setup was verified with:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.microsoft.com/en-us/windows/wsl/wsl2-index">WSL2&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.docker.com/docker-for-windows/wsl/">Docker Desktop WSL2 Engine&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://ubuntu.com/blog/ubuntu-on-wsl-2-is-generally-available">Ubuntu 18.04 LTS on WSL2&lt;/a>&lt;/li>
&lt;li>Nodeless local garden (see below)&lt;/li>
&lt;/ul>
&lt;p>The Gardener repository and all the above-mentioned tools (git, golang, kubectl, &amp;hellip;) should be installed in your WSL2 distro, according to the distribution-specific Linux installation instructions.&lt;/p>
&lt;h1 id="start-gardener-locally">Start Gardener locally&lt;/h1>
&lt;h2 id="get-the-sources">Get the sources&lt;/h2>
&lt;p>Clone the repository from GitHub into your &lt;code>$GOPATH&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>mkdir -p $GOPATH/src/github.com/gardener
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cd $GOPATH/src/github.com/gardener
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>git clone git@github.com:gardener/gardener.git
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cd gardener
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>Note: Gardener is using Go modules and cloning the repository into &lt;code>$GOPATH&lt;/code> is not a hard requirement. However it is still recommended to clone into &lt;code>$GOPATH&lt;/code> because &lt;code>k8s.io/code-generator&lt;/code> does not work yet outside of &lt;code>$GOPATH&lt;/code> - &lt;a href="https://github.com/kubernetes/kubernetes/issues/86753">kubernetes/kubernetes#86753&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;h2 id="start-the-gardener">Start the Gardener&lt;/h2>
&lt;p>ℹ️ In the following guide, you have to define the configuration (&lt;code>CloudProfile&lt;/code>s, &lt;code>SecretBinding&lt;/code>s, &lt;code>Seed&lt;/code>s, etc.) manually for the infrastructure environment you want to develop against.
Additionally, you have to register the respective Gardener extensions manually.
If you are rather looking for a quick start guide to develop entirely locally on your machine (no real cloud provider or infrastructure involved) then you should rather follow &lt;a href="https://gardener.cloud/docs/gardener/development/getting_started_locally/">this guide&lt;/a>.&lt;/p>
&lt;h3 id="start-a-local-kubernetes-cluster">Start a local kubernetes cluster&lt;/h3>
&lt;p>For the development of Gardener you need a Kubernetes API server on which you can register Gardener&amp;rsquo;s own Extension API Server as &lt;code>APIService&lt;/code>. This cluster doesn&amp;rsquo;t need any worker nodes to run pods, though, therefore, you can use the &amp;ldquo;nodeless Garden cluster setup&amp;rdquo; residing in &lt;code>hack/local-garden&lt;/code>. This will start all minimally required components of a Kubernetes cluster (&lt;code>etcd&lt;/code>, &lt;code>kube-apiserver&lt;/code>, &lt;code>kube-controller-manager&lt;/code>)
and an &lt;code>etcd&lt;/code> Instance for the &lt;code>gardener-apiserver&lt;/code> as Docker containers. This is the easiest way to get your
Gardener development setup up and running.&lt;/p>
&lt;p>&lt;strong>Using the nodeless cluster setup&lt;/strong>&lt;/p>
&lt;p>Use the provided Makefile rules to start your local Garden:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make local-garden-up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[...]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Starting gardener-dev kube-etcd cluster..!
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Starting gardener-dev kube-apiserver..!
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Starting gardener-dev kube-controller-manager..!
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Starting gardener-dev gardener-etcd cluster..!
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>namespace/garden created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusterrole.rbac.authorization.k8s.io/gardener.cloud:admin created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusterrolebinding.rbac.authorization.k8s.io/front-proxy-client created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[...]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ℹ️ [Optional] If you want to develop the &lt;code>SeedAuthorization&lt;/code> feature then you have to run &lt;code>make ACTIVATE_SEEDAUTHORIZER=true local-garden-up&lt;/code>. However, please note that this forces you to start the &lt;code>gardener-admission-controller&lt;/code> via &lt;code>make start-admission-controller&lt;/code>.&lt;/p>
&lt;p>To tear down the local Garden cluster and remove the Docker containers, simply run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make local-garden-down
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;details>
&lt;summary>&lt;b>Alternative: Using a local kubernetes cluster&lt;/b>&lt;/summary>
&lt;p>Instead of starting a kubernetes API server and etcd as docker containers, you can also opt for running a local kubernetes cluster, provided by e.g. &lt;a href="https://minikube.sigs.k8s.io/docs/start/">minikube&lt;/a>, &lt;a href="https://kind.sigs.k8s.io/docs/user/quick-start/">kind&lt;/a> or docker desktop.&lt;/p>
&lt;blockquote>
&lt;p>Note: Gardener requires self-contained kubeconfig files because of a &lt;a href="https://banzaicloud.com/blog/kubeconfig-security/">security issue&lt;/a>. You can configure your minikube to create self-contained kubeconfig files via:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>minikube config set embed-certs true
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>or when starting the local cluster&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>minikube start --embed-certs
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/blockquote>
&lt;/details>
&lt;details>
&lt;summary>&lt;b>Alternative: Using a remote kubernetes cluster&lt;/b>&lt;/summary>
&lt;p>For some testing scenarios, you may want to use a remote cluster instead of a local one as your Garden cluster.
To do this, you can use the &amp;ldquo;remote Garden cluster setup&amp;rdquo; residing in &lt;code>hack/remote-garden&lt;/code>. This will start an &lt;code>etcd&lt;/code> instance for the &lt;code>gardener-apiserver&lt;/code> as a Docker container, and open tunnels for accessing local gardener components from the remote cluster.&lt;/p>
&lt;p>To avoid mistakes, the remote cluster must have a &lt;code>garden&lt;/code> namespace labeled with &lt;code>gardener.cloud/purpose=remote-garden&lt;/code>.
You must create the &lt;code>garden&lt;/code> namespace and label it manually before running &lt;code>make remote-garden-up&lt;/code> as described below.&lt;/p>
&lt;p>Use the provided &lt;code>Makefile&lt;/code> rules to bootstrap your remote Garden:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>export KUBECONFIG=&amp;lt;path to kubeconfig&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>make remote-garden-up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[...]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Start gardener etcd used to store gardener resources (e.g., seeds, shoots)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Starting gardener-dev-remote gardener-etcd cluster!
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[...]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Open tunnels for accessing local gardener components from the remote cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[...]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To close the tunnels and remove the locally-running Docker containers, run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make remote-garden-down
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>Note: The minimum K8S version of the remote cluster that can be used as Garden cluster is &lt;code>1.19.x&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>ℹ️ [Optional] If you want to use the remote Garden cluster setup with the &lt;code>SeedAuthorization&lt;/code> feature you have to adapt the &lt;code>kube-apiserver&lt;/code> process of your remote Garden cluster. To do this, perform the following steps after running &lt;code>make remote-garden-up&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Create an &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/webhook/#configuration-file-format">authorization webhook configuration file&lt;/a> using the IP of the &lt;code>garden/quic-server&lt;/code> pod running in your remote Garden cluster and port 10444 that tunnels to your locally running &lt;code>gardener-admission-controller&lt;/code> process.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>current-context: seedauthorizer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusters:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: gardener-admission-controller
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> insecure-skip-tls-verify: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: https://&amp;lt;quic-server-pod-ip&amp;gt;:10444/webhooks/auth/seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>users:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: seedauthorizer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: gardener-admission-controller
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: kube-apiserver
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Change or add the following command line parameters to your &lt;code>kube-apiserver&lt;/code> process:&lt;/p>
&lt;ul>
&lt;li>&lt;code>--authorization-mode=&amp;lt;...&amp;gt;,Webhook&lt;/code>&lt;/li>
&lt;li>&lt;code>--authorization-webhook-config-file=&amp;lt;path to config file&amp;gt;&lt;/code>&lt;/li>
&lt;li>&lt;code>--authorization-webhook-cache-authorized-ttl=0&lt;/code>&lt;/li>
&lt;li>&lt;code>--authorization-webhook-cache-unauthorized-ttl=0&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Delete the cluster role and rolebinding &lt;code>gardener.cloud:system:seeds&lt;/code> from your remote Garden cluster.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>If your remote Garden cluster is a Gardener shoot, and you can access the seed on which this shoot is scheduled, you can automate the above steps by running the &lt;a href="https://github.com/gardener/gardener/blob/master/hack/local-development/remote-garden/enable-seed-authorizer">&lt;code>enable-seed-authorizer&lt;/code> script&lt;/a> and passing the kubeconfig of the seed cluster and the shoot namespace as parameters:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>hack/local-development/remote-garden/enable-seed-authorizer &amp;lt;seed kubeconfig&amp;gt; &amp;lt;namespace&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>Note: The configuration changes introduced by this script result in a working &lt;code>SeedAuthorization&lt;/code> feature only on shoots for which the &lt;code>ReversedVPN&lt;/code> feature is not enabled. If the corresponding feature gate is enabled in &lt;code>gardenlet&lt;/code>, add the annotation &lt;code>alpha.featuregates.shoot.gardener.cloud/reversed-vpn: 'false'&lt;/code> to the remote Garden shoot to disable it for that particular shoot.&lt;/p>
&lt;/blockquote>
&lt;p>To prevent Gardener from reconciling the shoot and overwriting your changes, add the annotation &lt;code>shoot.gardener.cloud/ignore: 'true'&lt;/code> to the remote Garden shoot. Note that this annotation takes effect only if it is enabled via the &lt;code>constollers.shoot.respectSyncPeriodOverwrite: true&lt;/code> option in the &lt;code>gardenlet&lt;/code> configuration.&lt;/p>
&lt;p>To disable the seed authorizer again, run the same script with &lt;code>-d&lt;/code> as a third parameter:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>hack/local-development/remote-garden/enable-seed-authorizer &amp;lt;seed kubeconfig&amp;gt; &amp;lt;namespace&amp;gt; -d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If the seed authorizer is enabled, you also have to start the &lt;code>gardener-admission-controller&lt;/code> via &lt;code>make start-admission-controller&lt;/code>.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ In the remote garden setup all Gardener components run with administrative permissions, i.e., there is no fine-grained access control via RBAC (as opposed to productive installations of Gardener).&lt;/p>
&lt;/blockquote>
&lt;/details>
&lt;h3 id="prepare-the-gardener">Prepare the Gardener&lt;/h3>
&lt;p>Now, that you have started your local cluster, we can go ahead and register the Gardener API Server.
Just point your &lt;code>KUBECONFIG&lt;/code> environment variable to the cluster you created in the previous step and run:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make dev-setup
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[...]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>namespace/garden created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>namespace/garden-dev created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>deployment.apps/etcd created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>service/etcd created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>service/gardener-apiserver created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>service/gardener-admission-controller created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>endpoints/gardener-apiserver created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>endpoints/gardener-admission-controller created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiservice.apiregistration.k8s.io/v1alpha1.core.gardener.cloud created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiservice.apiregistration.k8s.io/v1beta1.core.gardener.cloud created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiservice.apiregistration.k8s.io/v1alpha1.seedmanagement.gardener.cloud created
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiservice.apiregistration.k8s.io/v1alpha1.settings.gardener.cloud created
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ℹ️ [Optional] If you want to enable logging, in the Gardenlet configuration add:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>logging:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The Gardener exposes the API servers of Shoot clusters via Kubernetes services of type &lt;code>LoadBalancer&lt;/code>.
In order to establish stable endpoints (robust against changes of the load balancer address), it creates DNS records pointing to these load balancer addresses. They are used internally and by all cluster components to communicate.
You need to have control over a domain (or subdomain) for which these records will be created.
Please provide an &lt;em>internal domain secret&lt;/em> (see &lt;a href="https://github.com/gardener/gardener/blob/master/example/10-secret-internal-domain.yaml">this&lt;/a> for an example) which contains credentials with the proper privileges. Further information can be found &lt;a href="https://gardener.cloud/docs/gardener/usage/configuration/">here&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f example/10-secret-internal-domain-unmanaged.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>secret/internal-domain-unmanaged created
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="run-the-gardener">Run the Gardener&lt;/h3>
&lt;p>Next, run the Gardener API Server, the Gardener Controller Manager (optionally), the Gardener Scheduler (optionally), and the Gardenlet in different terminal windows/panes using rules in the &lt;code>Makefile&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make start-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[...]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0306 15:23:51.044421 74536 plugins.go:84] Registered admission plugin &lt;span style="color:#a31515">&amp;#34;ResourceReferenceManager&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0306 15:23:51.044523 74536 plugins.go:84] Registered admission plugin &lt;span style="color:#a31515">&amp;#34;DeletionConfirmation&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[...]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>I0306 15:23:51.626836 74536 secure_serving.go:116] Serving securely on [::]:8443
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[...]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>(Optional) Now you are ready to launch the Gardener Controller Manager.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make start-controller-manager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-03-06T15:24:17+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Starting Gardener controller manager...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-03-06T15:24:17+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Feature Gates: &amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-03-06T15:24:17+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Starting HTTP server on 0.0.0.0:2718&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-03-06T15:24:17+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Acquired leadership, starting controllers.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-03-06T15:24:18+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Starting HTTPS server on 0.0.0.0:2719&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-03-06T15:24:18+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Found internal domain secret internal-domain-unmanaged for domain nip.io.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-03-06T15:24:18+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Successfully bootstrapped the Garden cluster.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-03-06T15:24:18+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Gardener controller manager (version 1.0.0-dev) initialized.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-03-06T15:24:18+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;ControllerRegistration controller initialized.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-03-06T15:24:18+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;SecretBinding controller initialized.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-03-06T15:24:18+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Project controller initialized.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-03-06T15:24:18+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Quota controller initialized.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-03-06T15:24:18+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;CloudProfile controller initialized.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[...]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>(Optional) Now you are ready to launch the Gardener Scheduler.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make start-scheduler
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-05-02T16:31:50+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Starting Gardener scheduler ...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-05-02T16:31:50+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Starting HTTP server on 0.0.0.0:10251&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-05-02T16:31:50+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Acquired leadership, starting scheduler.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-05-02T16:31:50+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Gardener scheduler initialized (with Strategy: SameRegion)&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-05-02T16:31:50+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Scheduler controller initialized.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[...]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The Gardener should now be ready to operate on Shoot resources. You can use&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl get shoots
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>No resources found.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>to operate against your local running Gardener API Server.&lt;/p>
&lt;blockquote>
&lt;p>Note: It may take several seconds until the Gardener API server has been started and is available. &lt;code>No resources found&lt;/code> is the expected result of our initial development setup.&lt;/p>
&lt;/blockquote>
&lt;h1 id="create-a-shoot">Create a Shoot&lt;/h1>
&lt;p>The steps below describe the general process of creating a Shoot. Have in mind that the steps do not provide full example manifests. The reader needs to check the provider documentation and adapt the manifests accordingly.&lt;/p>
&lt;h4 id="1-copy-the-example-manifests">1. Copy the example manifests&lt;/h4>
&lt;p>The next steps require modifications of the example manifests. These modifications are part of local setup and should not be &lt;code>git push&lt;/code>-ed. To do not interfere with git, let&amp;rsquo;s copy the example manifests to &lt;code>dev/&lt;/code> which is ignored by git.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cp example/*.yaml dev/
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="2-create-a-project">2. Create a Project&lt;/h4>
&lt;p>Every Shoot is associated with a Project. Check the corresponding example manifests &lt;code>dev/00-namespace-garden-dev.yaml&lt;/code> and &lt;code>dev/05-project-dev.yaml&lt;/code>. Adapt them and create them.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/00-namespace-garden-dev.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/05-project-dev.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Make sure that the Project is successfully reconciled:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get project dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME NAMESPACE STATUS OWNER CREATOR AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dev garden-dev Ready john.doe@example.com kubernetes-admin 6s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="3-create-a-cloudprofile">3. Create a CloudProfile&lt;/h4>
&lt;p>The &lt;code>CloudProfile&lt;/code> resource is provider specific and describes the underlying cloud provider (available machine types, regions, machine images, etc.). Check the corresponding example manifest &lt;code>dev/30-cloudprofile.yaml&lt;/code>. Check also the documentation and example manifests of the provider extension. Adapt &lt;code>dev/30-cloudprofile.yaml&lt;/code> and apply it.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/30-cloudprofile.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="4-install-necessary-gardener-extensions">4. Install necessary Gardener Extensions&lt;/h4>
&lt;p>The &lt;a href="https://github.com/gardener/gardener/blob/master/extensions/README.md#known-extension-implementations">Known Extension Implementations&lt;/a> section contains a list of available extension implementations. You need to create a ControllerRegistration and ControllerDeployment for&lt;/p>
&lt;ul>
&lt;li>at least one infrastructure provider&lt;/li>
&lt;li>a dns provider (if the DNS for the Seed is not disabled)&lt;/li>
&lt;li>at least one operating system extension&lt;/li>
&lt;li>at least one network plugin extension&lt;/li>
&lt;/ul>
&lt;p>As a convention, the example ControllerRegistration manifest (containing also the necessary ControllerDeployment) for an extension is located under &lt;code>example/controller-registration.yaml&lt;/code> in the corresponding repository (for example for AWS the ControllerRegistration can be found &lt;a href="https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/controller-registration.yaml">here&lt;/a>). An example creation for provider-aws (make sure to replace &lt;code>&amp;lt;version&amp;gt;&lt;/code> with the newest released version tag):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/gardener/gardener-extension-provider-aws/&amp;lt;version&amp;gt;/example/controller-registration.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Instead of updating extensions manually you can use &lt;a href="https://github.com/gardener/gem">Gardener Extensions Manager&lt;/a> to install and update extension controllers. This is especially useful if you want to keep and maintain your development setup for a longer time.
Also, please refer to &lt;a href="https://gardener.cloud/docs/gardener/extensions/controllerregistration/">this document&lt;/a> for further information about how extensions are registered in case you want to use other versions than the latest releases.&lt;/p>
&lt;h4 id="5-register-a-seed">5. Register a Seed&lt;/h4>
&lt;p>Shoot controlplanes run in seed clusters, so we need to create our first Seed now.&lt;/p>
&lt;p>Check the corresponding example manifest &lt;code>dev/40-secret-seed.yaml&lt;/code> and &lt;code>dev/50-seed.yaml&lt;/code>. Update &lt;code>dev/40-secret-seed.yaml&lt;/code> with base64 encoded kubeconfig of the cluster that will be used as Seed (the scope of the permissions should be identical to the kubeconfig that the Gardenlet creates during bootstrapping - for now, &lt;code>cluster-admin&lt;/code> privileges are recommended).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/40-secret-seed.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Adapt &lt;code>dev/50-seed.yaml&lt;/code> - adjust &lt;code>.spec.secretRef&lt;/code> to refer the newly created Secret, adjust &lt;code>.spec.provider&lt;/code> with the Seed cluster provider and revise the other fields.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/50-seed.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="6-start-gardenlet">6. Start Gardenlet&lt;/h4>
&lt;p>Once the Seed is created, start the Gardenlet to reconcile it. The &lt;code>make start-gardenlet&lt;/code> command will automatically configure the local Gardenlet process to use the Seed and its kubeconfig. If you have multiple Seeds, you have to specify which to use by setting the &lt;code>SEED_NAME&lt;/code> environment variable like in &lt;code>make start-gardenlet SEED_NAME=my-first-seed&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make start-gardenlet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-11-06T15:24:17+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Starting Gardenlet...&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-11-06T15:24:17+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Feature Gates: HVPA=true, Logging=true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-11-06T15:24:17+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Acquired leadership, starting controllers.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-11-06T15:24:18+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Found internal domain secret internal-domain-unmanaged for domain nip.io.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-11-06T15:24:18+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Gardenlet (version 1.0.0-dev) initialized.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-11-06T15:24:18+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;ControllerInstallation controller initialized.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-11-06T15:24:18+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Shoot controller initialized.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>time=&lt;span style="color:#a31515">&amp;#34;2019-11-06T15:24:18+02:00&amp;#34;&lt;/span> level=info msg=&lt;span style="color:#a31515">&amp;#34;Seed controller initialized.&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[...]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The Gardenlet will now reconcile the Seed. Check the progess from time to time until it&amp;rsquo;s &lt;code>Ready&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl get seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STATUS PROVIDER REGION AGE VERSION K8S VERSION
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>seed-aws Ready aws eu-west-1 4m v1.11.0-dev v1.18.12
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="7-create-a-shoot">7. Create a Shoot&lt;/h4>
&lt;p>A Shoot requires a SecretBinding. The SecretBinding refers to a Secret that contains the cloud provider credentials. The Secret data keys are provider specific and you need to check the documentation of the provider to find out which data keys are expected (for example for AWS the related documentation can be found &lt;a href="https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-end-user/#provider-secret-data">here&lt;/a>). Adapt &lt;code>dev/70-secret-provider.yaml&lt;/code> and &lt;code>dev/80-secretbinding.yaml&lt;/code> and apply them.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/70-secret-provider.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/80-secretbinding.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After the SecretBinding creation, you are ready to proceed with the Shoot creation. You need to check the documentation of the provider to find out the expected configuration (for example for AWS the related documentation and example Shoot manifest can be found &lt;a href="https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/docs/usage-as-end-user/">here&lt;/a>). Adapt &lt;code>dev/90-shoot.yaml&lt;/code> and apply it.&lt;/p>
&lt;p>To make sure that a specific Seed cluster will be chosen or to skip the scheduling (the sheduling requires Gardener Scheduler to be running), specify the &lt;code>.spec.seedName&lt;/code> field (see &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L317-L318">here&lt;/a>).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f dev/90-shoot.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Watch the progress of the operation and make sure that the Shoot will be successfully created.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>watch kubectl get shoot --all-namespaces
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Log Parsers</title><link>https://gardener.cloud/docs/gardener/development/log_parsers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/log_parsers/</guid><description>
&lt;h1 id="how-to-create-log-parser-for-container-into-fluent-bit">How to create log parser for container into fluent-bit&lt;/h1>
&lt;p>If our log message is parsed correctly, it has to be showed in Grafana like this:&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-jsonc" data-lang="jsonc"> {&amp;#34;log&amp;#34;:&amp;#34;OpenAPI AggregationController: Processing item v1beta1.metrics.k8s.io&amp;#34;,&amp;#34;pid&amp;#34;:&amp;#34;1&amp;#34;,&amp;#34;severity&amp;#34;:&amp;#34;INFO&amp;#34;,&amp;#34;source&amp;#34;:&amp;#34;controller.go:107&amp;#34;}
&lt;/code>&lt;/pre>&lt;p>Otherwise it will looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-jsonc" data-lang="jsonc">{
&amp;#34;log&amp;#34;:&amp;#34;{
\&amp;#34;level\&amp;#34;:\&amp;#34;info\&amp;#34;,\&amp;#34;ts\&amp;#34;:\&amp;#34;2020-06-01T11:23:26.679Z\&amp;#34;,\&amp;#34;logger\&amp;#34;:\&amp;#34;gardener-resource-manager.health-reconciler\&amp;#34;,\&amp;#34;msg\&amp;#34;:\&amp;#34;Finished ManagedResource health checks\&amp;#34;,\&amp;#34;object\&amp;#34;:\&amp;#34;garden/provider-aws-dsm9r\&amp;#34;
}\n&amp;#34;
}
}
&lt;/code>&lt;/pre>&lt;h2 id="lets-make-a-custom-parser-now">Lets make a custom parser now&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>First of all we need to know how does the log for the specific container look like (for example lets take a log from the &lt;code>alertmanager&lt;/code> :
&lt;code>level=info ts=2019-01-28T12:33:49.362015626Z caller=main.go:175 build_context=&amp;quot;(go=go1.11.2, user=root@4ecc17c53d26, date=20181109-15:40:48)&lt;/code>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We can see that this log contains 4 subfields(severity=info, timestamp=2019-01-28T12:33:49.362015626Z, source=main.go:175 and the actual message).
So we have to write a regex which matches this log in 4 groups(We can use &lt;a href="https://regex101.com/">https://regex101.com/&lt;/a> like helping tool). So for this purpose our regex
looks like this:&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>^level=(?&amp;lt;severity&amp;gt;\w+)\s+ts=(?&amp;lt;time&amp;gt;\d{4}-\d{2}-\d{2}[Tt].*[zZ])\s+caller=(?&amp;lt;source&amp;gt;[^\s]*+)\s+(?&amp;lt;log&amp;gt;.*)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Now we have to create correct time format for the timestamp(We can use this site for this purpose: &lt;a href="http://ruby-doc.org/stdlib-2.4.1/libdoc/time/rdoc/Time.html#method-c-strptime)">http://ruby-doc.org/stdlib-2.4.1/libdoc/time/rdoc/Time.html#method-c-strptime)&lt;/a>.
So our timestamp matches correctly the following format:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>%Y-%m-%dT%H:%M:%S.%L
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>It&amp;rsquo;s a time to apply our new regex into fluent-bit configuration. Go to fluent-bit-configmap.yaml and create new filter using the following template:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>[FILTER]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Name parser
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Match kubernetes.&amp;lt;&amp;lt; pod-name &amp;gt;&amp;gt;*&amp;lt;&amp;lt; container-name &amp;gt;&amp;gt;*
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Key_Name log
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Parser &amp;lt;&amp;lt; parser-name &amp;gt;&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Reserve_Data True
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>EXAMPLE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[FILTER]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Name parser
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Match kubernetes.alertmanager*alertmanager*
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Key_Name log
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Parser alermanagerParser
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Reserve_Data True
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Now lets check if there is already exists parser with such a regex and time format that we need. if not, let`s create one:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>[PARSER]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Name &amp;lt;&amp;lt; parser-name &amp;gt;&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Format regex
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Regex &amp;lt;&amp;lt; regex &amp;gt;&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Time_Key time
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Time_Format &amp;lt;&amp;lt; time-format &amp;gt;&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>EXAMPLE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[PARSER]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Name alermanagerParser
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Format regex
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Regex ^level=(?&amp;lt;severity&amp;gt;\w+)\s+ts=(?&amp;lt;time&amp;gt;\d{4}-\d{2}-\d{2}[Tt].*[zZ])\s+caller=(?&amp;lt;source&amp;gt;[^\s]*+)\s+(?&amp;lt;log&amp;gt;.*)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Time_Key time
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Time_Format %Y-%m-%dT%H:%M:%S.%L
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>Follow your development setup to validate that parsers are working correctly.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Logging</title><link>https://gardener.cloud/docs/gardener/development/logging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/logging/</guid><description>
&lt;h1 id="logging-in-gardener-components">Logging in Gardener Components&lt;/h1>
&lt;p>This document aims at providing a general developer guideline on different aspects of logging practices and conventions used in the Gardener codebase.
It contains mostly Gardener-specific points and references other existing and commonly accepted logging guidelines for general advice.
Developers and reviewers should consult this guide when writing, refactoring and reviewing Gardener code.
If parts are unclear or new learnings arise, this guide should be adapted accordingly.&lt;/p>
&lt;h2 id="logging-libraries--implementations">Logging Libraries / Implementations&lt;/h2>
&lt;p>Historically, Gardener components have been using &lt;a href="https://github.com/sirupsen/logrus">logrus&lt;/a>.
There is a global logrus logger (&lt;a href="https://github.com/gardener/gardener/blob/626ba7c10e1150819b3905116d3988512c18c9ee/pkg/logger/logrus.go#L28">&lt;code>logger.Logger&lt;/code>&lt;/a>) that is initialized by components on startup and used across the codebase.
In most places, it is used as a &lt;code>printf&lt;/code>-style logger and only in some instances we make use of logrus&amp;rsquo; structured logging functionality.&lt;/p>
&lt;p>In the process of migrating our components to native controller-runtime components (see &lt;a href="https://github.com/gardener/gardener/issues/4251">gardener/gardener#4251&lt;/a>), we also want to make use of controller-runtime&amp;rsquo;s built-in mechanisms for streamlined logging.
controller-runtime uses &lt;a href="https://github.com/go-logr/logr">logr&lt;/a>, a simple structured logging interface, for library-internal logging and logging in controllers.&lt;/p>
&lt;p>logr itself is only an interface and doesn&amp;rsquo;t provide an implementation out of the box.
Instead, it needs to be backed by a logging implementation like &lt;a href="https://github.com/go-logr/zapr">zapr&lt;/a>. Code that uses the logr interface is thereby not tied to a specific logging implementation and makes the implementation easily exchangeable.
controller-runtime already provides a &lt;a href="https://github.com/kubernetes-sigs/controller-runtime/tree/v0.11.0/pkg/log/zap">set of helpers&lt;/a> for constructing zapr loggers, i.e., logr loggers backed by &lt;a href="https://github.com/uber-go/zap">zap&lt;/a>, which is a popular logging library in the go community.
Hence, we are migrating our component logging from logrus to logr (backed by zap) as part of &lt;a href="https://github.com/gardener/gardener/issues/4251">gardener/gardener#4251&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ &lt;code>logger.Logger&lt;/code> (logrus logger) is deprecated in Gardener and shall not be used in new code – use logr loggers when writing new code! (also see &lt;a href="#migration-from-logrus-to-logr">Migration from logrus to logr&lt;/a>)&lt;/p>
&lt;p>ℹ️ Don&amp;rsquo;t use zap loggers directly, always use the logr interface in order to avoid tight coupling to a specific logging implementation.&lt;/p>
&lt;/blockquote>
&lt;p>gardener-apiserver differs from the other components as it is based on the &lt;a href="https://github.com/kubernetes/apiserver">apiserver library&lt;/a> and therefore uses &lt;a href="https://github.com/kubernetes/klog">klog&lt;/a> – just like kube-apiserver.
As gardener-apiserver writes (almost) no logs in our coding (outside the apiserver library), there is currently no plan for switching the logging implementation.
Hence, the following sections focus on logging in the controller and admission components only.&lt;/p>
&lt;h2 id="logcheck-tool">&lt;code>logcheck&lt;/code> Tool&lt;/h2>
&lt;p>To ensure a smooth migration to logr and make logging in Gardener components more consistent, the &lt;a href="https://github.com/gardener/gardener/tree/master/hack/tools/logcheck">&lt;code>logcheck&lt;/code> tool&lt;/a> was added.
It enforces (parts of) this guideline and detects programmer-level errors early on in order to prevent bugs.
Please check out the &lt;a href="https://github.com/gardener/gardener/tree/master/hack/tools/logcheck">tool&amp;rsquo;s documentation&lt;/a> for a detailed description.&lt;/p>
&lt;h2 id="structured-logging">Structured Logging&lt;/h2>
&lt;p>Similar to &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md">efforts in the Kubernetes project&lt;/a>, we want to migrate our component logs to structured logging.
As motivated above, we will use the logr interface instead of klog though.&lt;/p>
&lt;p>You can read more about the motivation behind structured logging in &lt;a href="https://github.com/go-logr/logr#background">logr&amp;rsquo;s background and FAQ&lt;/a> (also see &lt;a href="http://dave.cheney.net/2015/11/05/lets-talk-about-logging">this blog post by Dave Cheney&lt;/a>).
Also, make sure to check out controller-runtime&amp;rsquo;s &lt;a href="https://github.com/kubernetes-sigs/controller-runtime/blob/v0.11.0/TMP-LOGGING.md">logging guideline&lt;/a> with specifics for projects using the library.
The following sections will focus on the most important takeaways from those guidelines and give general instructions on how to apply them to Gardener and its controller-runtime components.
Note: some parts in this guideline differ slightly from controller-runtime&amp;rsquo;s document.&lt;/p>
&lt;h3 id="tldr-of-structured-logging">TL;DR of Structured Logging&lt;/h3>
&lt;p>❌ stop using &lt;code>printf&lt;/code>-style logging:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> logger *logrus.Logger
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>logger.Infof(&lt;span style="color:#a31515">&amp;#34;Scaling deployment %s/%s to %d replicas&amp;#34;&lt;/span>, deployment.Namespace, deployment.Name, replicaCount)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>✅ instead, write static log messages and enrich them with additional structured information in form of key-value pairs:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> logger logr.Logger
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>logger.Info(&lt;span style="color:#a31515">&amp;#34;Scaling deployment&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;deployment&amp;#34;&lt;/span>, client.ObjectKeyFromObject(deployment), &lt;span style="color:#a31515">&amp;#34;replicas&amp;#34;&lt;/span>, replicaCount)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="log-configuration">Log Configuration&lt;/h2>
&lt;p>Gardener components can be configured to either log in &lt;code>json&lt;/code> (default) or &lt;code>text&lt;/code> format:
&lt;code>json&lt;/code> format is supposed to be used in production, while &lt;code>text&lt;/code> format might be nicer for development.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span># json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:&amp;#34;2021-12-16T08:32:21.059+0100&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Hello botanist&amp;#34;,&amp;#34;garden&amp;#34;:&amp;#34;eden&amp;#34;}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span># text
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2021-12-16T08:32:21.059+0100 INFO Hello botanist {&amp;#34;garden&amp;#34;: &amp;#34;eden&amp;#34;}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Components can be set to one of the following log levels (with increasing verbosity): &lt;code>error&lt;/code>, &lt;code>info&lt;/code> (default), &lt;code>debug&lt;/code>.&lt;/p>
&lt;blockquote>
&lt;p>ℹ️ Note: some Gardener components don&amp;rsquo;t feature a configurable log level and format yet.
In this case, they log at &lt;code>info&lt;/code> in &lt;code>json&lt;/code> format.
We might add configuration options via command line flags that can be used in all components in the future though (see &lt;a href="https://github.com/gardener/gardener/issues/5191">gardener/gardener#5191&lt;/a>).&lt;/p>
&lt;/blockquote>
&lt;h2 id="log-levels">Log Levels&lt;/h2>
&lt;p>logr uses &lt;a href="https://github.com/go-logr/logr#why-v-levels">V-levels&lt;/a> (numbered log levels), higher V-level means higher verbosity.
V-levels are relative (in contrast to &lt;code>klog&lt;/code>&amp;rsquo;s absolute V-levels), i.e., &lt;code>V(1)&lt;/code> creates a logger, that is one level more verbose than its parent logger.&lt;/p>
&lt;p>In Gardener components, the mentioned log levels in the component config (&lt;code>error&lt;/code>, &lt;code>info&lt;/code>, &lt;code>debug&lt;/code>) map to the zap levels with the same names (see &lt;a href="https://github.com/gardener/gardener/blob/770fc01a34b70f6cb77b8cfe929d9daef0026d1c/pkg/logger/zap.go#L43-L55">here&lt;/a>).
Hence, our loggers follow the same mapping from numerical logr levels to named zap levels like described in &lt;a href="https://github.com/go-logr/zapr/tree/v1.1.0#increasing-verbosity">zapr&lt;/a>, i.e.:&lt;/p>
&lt;ul>
&lt;li>component config specifies &lt;code>debug&lt;/code> ➡️ both &lt;code>V(0)&lt;/code> and &lt;code>V(1)&lt;/code> are enabled&lt;/li>
&lt;li>component config specifies &lt;code>info&lt;/code> ➡️ &lt;code>V(0)&lt;/code> is enabled, &lt;code>V(1)&lt;/code> will not be shown&lt;/li>
&lt;li>component config specifies &lt;code>error&lt;/code> ➡️ neither &lt;code>V(0)&lt;/code> nor &lt;code>V(1)&lt;/code> will be shown&lt;/li>
&lt;li>&lt;code>Error()&lt;/code> logs will always be shown&lt;/li>
&lt;/ul>
&lt;p>This mapping applies to the components&amp;rsquo; root loggers (the ones that are not &amp;ldquo;derived&amp;rdquo; from any other logger; constructed on component startup).
If you derive a new logger with e.g. &lt;code>V(1)&lt;/code>, the mapping will shift by one. For example, &lt;code>V(0)&lt;/code> will then log at zap&amp;rsquo;s &lt;code>debug&lt;/code> level.&lt;/p>
&lt;p>There is no &lt;code>warning&lt;/code> level (see &lt;a href="https://dave.cheney.net/2015/11/05/lets-talk-about-logging">Dave Cheney&amp;rsquo;s post&lt;/a>).
If there is an error condition (e.g., unexpected error received from a called function), the error should either be handled or logged at &lt;code>error&lt;/code> if it is neither handled nor returned.
If you have an &lt;code>error&lt;/code> value at hand that doesn&amp;rsquo;t represent an actual error condition, but you still want to log it as an informational message, log it at &lt;code>info&lt;/code> level with key &lt;code>err&lt;/code>.&lt;/p>
&lt;p>We might consider to make use of a broader range of log levels in the future when introducing more logs and common command line flags for our components (comparable to &lt;code>--v&lt;/code> of Kubernetes components).
For now, we stick to the mentioned two log levels like controller-runtime: info (&lt;code>V(0)&lt;/code>) and debug (&lt;code>V(1)&lt;/code>).&lt;/p>
&lt;h2 id="logging-in-controllers">Logging in Controllers&lt;/h2>
&lt;h3 id="named-loggers">Named Loggers&lt;/h3>
&lt;p>Controllers should use named loggers that include their name, e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>controllerLogger := rootLogger.WithName(&lt;span style="color:#a31515">&amp;#34;controller&amp;#34;&lt;/span>).WithName(&lt;span style="color:#a31515">&amp;#34;shoot&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>controllerLogger.Info(&lt;span style="color:#a31515">&amp;#34;Deploying kube-apiserver&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>results in&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>2021-12-16T09:27:56.550+0100 INFO controller.shoot Deploying kube-apiserver
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Logger names are hierarchical. You can make use of it, where controllers are composed of multiple &amp;ldquo;subcontrollers&amp;rdquo;, e.g., &lt;code>controller.shoot.hibernation&lt;/code> or &lt;code>controller.shoot.maintenance&lt;/code>.&lt;/p>
&lt;p>Using the global logger &lt;code>logf.Log&lt;/code> directly is discouraged and should be rather exceptional because it makes correlating logs with code harder.
Preferably, all parts of the code should use some named logger.&lt;/p>
&lt;h3 id="reconciler-loggers">Reconciler Loggers&lt;/h3>
&lt;p>In your &lt;code>Reconcile&lt;/code> function, retrieve a logger from the given &lt;code>context.Context&lt;/code>.
It inherits from the controller&amp;rsquo;s logger (i.e., is already named) and is preconfigured with &lt;code>name&lt;/code> and &lt;code>namespace&lt;/code> values for the reconciliation request:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">func&lt;/span> (r *reconciler) Reconcile(ctx context.Context, request reconcile.Request) (reconcile.Result, &lt;span style="color:#2b91af">error&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> log := logf.FromContext(ctx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> log.Info(&lt;span style="color:#a31515">&amp;#34;Reconciling Shoot&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> &lt;span style="color:#00f">return&lt;/span> reconcile.Result{}, &lt;span style="color:#00f">nil&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>results in&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>2021-12-16T09:35:59.099+0100 INFO controller.shoot Reconciling Shoot {&amp;#34;name&amp;#34;: &amp;#34;sunflower&amp;#34;, &amp;#34;namespace&amp;#34;: &amp;#34;garden-greenhouse&amp;#34;}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The logger is injected by controller-runtime&amp;rsquo;s &lt;code>Controller&lt;/code> implementation and our &lt;code>controllerutils.CreateWorker&lt;/code> alike (if a logger is passed using &lt;code>controllerutils.WithLogger&lt;/code>). The logger returned by &lt;code>logf.FromContext&lt;/code> is never &lt;code>nil&lt;/code>. If the context doesn&amp;rsquo;t carry a logger, it falls back to the global logger (&lt;code>logf.Log&lt;/code>), which might discard logs if not configured, but is also never &lt;code>nil&lt;/code>.&lt;/p>
&lt;p>The controller implementation (controller-runtime / &lt;code>CreateWorker&lt;/code>) itself takes care of logging the error returned by reconcilers.
Hence, don&amp;rsquo;t log an error that you are returning.
Generally, functions should not return an error, if they already logged it, because that means the error is already handled and not an error anymore.
See &lt;a href="https://dave.cheney.net/2015/11/05/lets-talk-about-logging">Dave Cheney&amp;rsquo;s post&lt;/a> for more on this.&lt;/p>
&lt;h3 id="messages">Messages&lt;/h3>
&lt;ul>
&lt;li>Log messages should be static. Don&amp;rsquo;t put variable content in there, i.e., no &lt;code>fmt.Sprintf&lt;/code> or string concatenation (&lt;code>+&lt;/code>). Use key-value pairs instead.&lt;/li>
&lt;li>Log messages should be capitalized. Note: this contrasts with error messages, that should not be capitalized. However, both should not end with a punctuation mark.&lt;/li>
&lt;/ul>
&lt;h3 id="keys-and-values">Keys and Values&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Use &lt;code>WithValues&lt;/code> instead of repeatedly adding key-value pairs for multiple log statements. &lt;code>WithValues&lt;/code> creates a new logger from the parent, that carries the given key-value pairs. E.g., use it when acting on one object in multiple steps and logging something for each step:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>log := parentLog.WithValues(&lt;span style="color:#a31515">&amp;#34;infrastructure&amp;#34;&lt;/span>, client.ObjectKeyFromObject(infrastrucutre))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>log.Info(&lt;span style="color:#a31515">&amp;#34;Creating Infrastructure&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>log.Info(&lt;span style="color:#a31515">&amp;#34;Waiting for Infrastructure to be reconciled&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note: &lt;code>WithValues&lt;/code> bypasses controller-runtime&amp;rsquo;s special zap encoder that nicely encodes &lt;code>ObjectKey&lt;/code>/&lt;code>NamespacedName&lt;/code> and &lt;code>runtime.Object&lt;/code> values, see &lt;a href="https://github.com/kubernetes-sigs/controller-runtime/issues/1290">kubernetes-sigs/controller-runtime#1290&lt;/a>.
Thus, the end result might look different depending on the value and its &lt;code>Stringer&lt;/code> implementation.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Use &lt;a href="https://en.wiktionary.org/wiki/lowerCamelCase">lowerCamelCase&lt;/a> for keys. Don&amp;rsquo;t put spaces in keys, as it will make log processing with simple tools like &lt;code>jq&lt;/code> harder.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Keys should be constant, human-readable, consistent across the codebase and naturally match parts of the log message, see &lt;a href="https://github.com/go-logr/logr#how-do-i-choose-my-keys">logr guideline&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When logging object keys (name and namespace), use the object&amp;rsquo;s type as the log key and a &lt;code>client.ObjectKey&lt;/code>/&lt;code>types.NamespacedName&lt;/code> value as value, e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> deployment *appsv1.Deployment
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>log.Info(&lt;span style="color:#a31515">&amp;#34;Creating Deployment&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;deployment&amp;#34;&lt;/span>, client.ObjectKeyFromObject(deployment))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>which results in&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:&amp;#34;2021-12-16T08:32:21.059+0100&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Creating Deployment&amp;#34;,&amp;#34;deployment&amp;#34;:{&amp;#34;name&amp;#34;: &amp;#34;bar&amp;#34;, &amp;#34;namespace&amp;#34;: &amp;#34;foo&amp;#34;}}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Earlier, we often used &lt;code>kutil.ObjectName()&lt;/code> for logging object keys, which encodes them into a flat string like &lt;code>foo/bar&lt;/code>. However, this flat string cannot be processed so easily by logging stacks (or &lt;code>jq&lt;/code>) like a structured log. Hence, the use of &lt;code>kutil.ObjectName()&lt;/code> for logging object keys is discouraged. Existing usages should be refactored to use &lt;code>client.ObjectKeyFromObject()&lt;/code> instead.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>There are cases where you don&amp;rsquo;t have the full object key or the object itself at hand, e.g., if an object references another object (in the same namespace) by name (think &lt;code>secretRef&lt;/code> or similar).
In such a cases, either construct the full object key including the implied namespace or log the object name under a key ending in &lt;code>Name&lt;/code>, e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">// object to reconcile
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> shoot *gardencorev1beta1.Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">// retrieved via logf.FromContext, preconfigured by controller with namespace and name of reconciliation request
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> log logr.Logger
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// option a: full object key, manually constructed
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>log.Info(&lt;span style="color:#a31515">&amp;#34;Shoot uses SecretBinding&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;secretBinding&amp;#34;&lt;/span>, client.ObjectKey{Namespace: shoot.Namespace, Name: shoot.Spec.SecretBindingName})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// option b: only name under respective *Name log key
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>log.Info(&lt;span style="color:#a31515">&amp;#34;Shoot uses SecretBinding&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;secretBindingName&amp;#34;&lt;/span>, shoot.Spec.SecretBindingName)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Both options result in well-structured logs, that are easy to interpret and process:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:&amp;#34;2022-01-18T18:00:56.672+0100&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Shoot uses SecretBinding&amp;#34;,&amp;#34;name&amp;#34;:&amp;#34;my-shoot&amp;#34;,&amp;#34;namespace&amp;#34;:&amp;#34;garden-project&amp;#34;,&amp;#34;secretBinding&amp;#34;:{&amp;#34;namespace&amp;#34;:&amp;#34;garden-project&amp;#34;,&amp;#34;name&amp;#34;:&amp;#34;aws&amp;#34;}}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:&amp;#34;2022-01-18T18:00:56.673+0100&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Shoot uses SecretBinding&amp;#34;,&amp;#34;name&amp;#34;:&amp;#34;my-shoot&amp;#34;,&amp;#34;namespace&amp;#34;:&amp;#34;garden-project&amp;#34;,&amp;#34;secretBindingName&amp;#34;:&amp;#34;aws&amp;#34;}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>When handling generic &lt;code>client.Object&lt;/code> values (e.g. in helper funcs), use &lt;code>object&lt;/code> as key.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When adding timestamps to key-value pairs, use &lt;code>time.Time&lt;/code> values. By this, they will be encoded in the same format as the log entry&amp;rsquo;s timestamp.&lt;br>
Don&amp;rsquo;t use &lt;code>metav1.Time&lt;/code> values, as they will be encoded in a different format by their &lt;code>Stringer&lt;/code> implementation. Pass &lt;code>&amp;lt;someTimestamp&amp;gt;.Time&lt;/code> to loggers in case you have a &lt;code>metav1.Time&lt;/code> value at hand.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Same applies to durations. Use &lt;code>time.Duration&lt;/code> values instead of &lt;code>*metav1.Duration&lt;/code>. Durations can be handled specially by zap just like timestamps.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Event recorders not only create &lt;code>Event&lt;/code> objects but also log them.
However, both Gardener&amp;rsquo;s manually instantiated event recorders and the ones that controller-runtime provides log to &lt;code>debug&lt;/code> level and use generic formats, that are not very easy to interpret or process (no structured logs).
Hence, don&amp;rsquo;t use event recorders as replacements for well-structured logs.
If a controller records an event for a completed action or important information, it should probably log it as well, e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>log.Info(&lt;span style="color:#a31515">&amp;#34;Creating ManagedSeed&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;replica&amp;#34;&lt;/span>, r.GetObjectKey())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>a.recorder.Eventf(managedSeedSet, corev1.EventTypeNormal, EventCreatingManagedSeed, &lt;span style="color:#a31515">&amp;#34;Creating ManagedSeed %s&amp;#34;&lt;/span>, r.GetFullName())
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;h2 id="logging-in-test-code">Logging in Test Code&lt;/h2>
&lt;ul>
&lt;li>If the tested production code requires a logger, you can pass &lt;code>logr.Discard()&lt;/code> or &lt;code>logf.NullLogger{}&lt;/code> in your test, which simply discards all logs.&lt;/li>
&lt;li>Pass &lt;code>logzap.New(logzap.WriteTo(GinkgoWriter))&lt;/code> in tests where you want to see the logs on test failure but not on success.&lt;/li>
&lt;li>&lt;code>logf.Log&lt;/code> is safe to use in tests and will not cause a nil pointer deref, even if it&amp;rsquo;s not initialized via &lt;code>logf.SetLogger&lt;/code>.
It is initially set to a &lt;code>NullLogger&lt;/code> by default, which means all logs are discarded, unless &lt;code>logf.SetLogger&lt;/code> is called in the first 30 seconds of execution.&lt;/li>
&lt;/ul>
&lt;h2 id="migration-from-logrus-to-logr">Migration from logrus to logr&lt;/h2>
&lt;p>These points might be helpful when refactoring existing code during the migration period:&lt;/p>
&lt;ul>
&lt;li>For migrating an existing controller to logr:
&lt;ul>
&lt;li>Create a named logger (&lt;a href="https://github.com/gardener/gardener/blob/ce9d741798eac2df8c470190ab483aa4c5818ebf/pkg/controllermanager/controller/cloudprofile/cloudprofile.go#L63">example&lt;/a>).&lt;/li>
&lt;li>Pass &lt;code>controllerutils.WithLogger&lt;/code> to &lt;code>CreateWorker&lt;/code> (&lt;a href="https://github.com/gardener/gardener/blob/ce9d741798eac2df8c470190ab483aa4c5818ebf/pkg/controllermanager/controller/cloudprofile/cloudprofile.go#L113">example&lt;/a>). This allows &lt;code>logf.FromContext&lt;/code> to be used in reconcilers.&lt;/li>
&lt;li>Use &lt;code>logf.FromContext&lt;/code> in &lt;code>Reconcile&lt;/code> to retrieve the logr logger and use it from there on (&lt;a href="https://github.com/gardener/gardener/blob/ce9d741798eac2df8c470190ab483aa4c5818ebf/pkg/controllermanager/controller/cloudprofile/cloudprofile_control.go#L72">example&lt;/a>).&lt;/li>
&lt;li>Make sure to follow the other guidelines mentioned above as well (see &lt;a href="#logging-in-controllers">Logging in Controllers&lt;/a>).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Libraries might expect a different logging implementation than the component which uses it. E.g., a controller that already uses logr might want to use the &lt;code>flow&lt;/code> package which still uses logrus. In such cases:
&lt;ul>
&lt;li>You can consider refactoring the library along with the component itself, if feasible.&lt;/li>
&lt;li>It is acceptable for the migration period to use a logger derived from the respective global logger (&lt;code>logger.Logger&lt;/code> or &lt;code>logf.Log&lt;/code>) and pass it to the library.
However, please add a &lt;code>TODO&lt;/code> for cleaning it up later on, once the migration is completed. E.g.:
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// TODO: switch to logr once flow package is migrated
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>err := shootFlow.Run(flow.Opts{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Logger: logger.Logger.WithFields(logrus.Fields{&lt;span style="color:#a31515">&amp;#34;logger&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;controller.&amp;#34;&lt;/span> + ControllerName, &lt;span style="color:#a31515">&amp;#34;name&amp;#34;&lt;/span>: shoot.Name, &lt;span style="color:#a31515">&amp;#34;namespace&amp;#34;&lt;/span>: shoot.Namespace})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>})
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: Migration V0 To V1</title><link>https://gardener.cloud/docs/gardener/deployment/migration_v0_to_v1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/migration_v0_to_v1/</guid><description>
&lt;h1 id="migration-from-gardener-v0-to-v1">Migration from Gardener &lt;code>v0&lt;/code> to &lt;code>v1&lt;/code>&lt;/h1>
&lt;p>Please refer to the &lt;a href="https://github.com/gardener/gardener/blob/v1.10.1/docs/deployment/migration_v0_to_v1.md">document for older Gardener versions&lt;/a>.&lt;/p></description></item><item><title>Docs: Monitoring Stack</title><link>https://gardener.cloud/docs/gardener/development/monitoring-stack/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/monitoring-stack/</guid><description>
&lt;h1 id="extending-the-monitoring-stack">Extending the Monitoring Stack&lt;/h1>
&lt;p>This document provides instructions to extend the Shoot cluster monitoring stack by integrating new scrape targets, alerts and dashboards.&lt;/p>
&lt;p>Please ensure that you have understood the basic principles of &lt;a href="https://prometheus.io/docs/introduction/overview/">Prometheus&lt;/a> and its ecosystem before you continue.&lt;/p>
&lt;p>‼️ &lt;strong>The purpose of the monitoring stack is to observe the behaviour of the control plane and the system components deployed by Gardener onto the worker nodes. Monitoring of custom workloads running in the cluster is out of scope.&lt;/strong>&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/monitoring-architecture_cd945d.png" alt="Monitoring Architecture">&lt;/p>
&lt;p>Each Shoot cluster comes with its own monitoring stack. The following components are deployed into the seed and shoot:&lt;/p>
&lt;ul>
&lt;li>Seed
&lt;ul>
&lt;li>&lt;a href="https://github.com/prometheus/prometheus">Prometheus&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/grafana/grafana">Grafana&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/prometheus/blackbox_exporter">blackbox-exporter&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics&lt;/a> (Seed metrics)&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics&lt;/a> (Shoot metrics)&lt;/li>
&lt;li>&lt;a href="https://github.com/prometheus/alertmanager">Alertmanager&lt;/a> (Optional)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Shoot
&lt;ul>
&lt;li>&lt;a href="https://github.com/prometheus/node_exporter">node-exporter(s)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/prometheus/blackbox_exporter">blackbox-exporter&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>In each Seed cluster there is a Prometheus in the &lt;code>garden&lt;/code> namespace responsible for collecting metrics from the Seed kubelets and cAdvisors. These metrics are provided to each Shoot Prometheus via federation.&lt;/p>
&lt;p>The alerts for all Shoot clusters hosted on a Seed are routed to a central Alertmanger running in the &lt;code>garden&lt;/code> namespace of the Seed. The purpose of this central alertmanager is to forward all important alerts to the operators of the Gardener setup.&lt;/p>
&lt;p>The Alertmanager in the Shoot namespace on the Seed is only responsible for forwarding alerts from its Shoot cluster to a cluster owner/cluster alert receiver via email. The Alertmanager is optional and the conditions for a deployment are already described &lt;a href="https://gardener.cloud/docs/gardener/monitoring/alerting/">here&lt;/a>.&lt;/p>
&lt;h2 id="adding-new-monitoring-targets">Adding New Monitoring Targets&lt;/h2>
&lt;p>After exploring the metrics which your component provides or adding new metrics, you should be aware which metrics are required to write the needed alerts and dashboards.&lt;/p>
&lt;p>Prometheus prefers a pull based metrics collection approach and therefore the targets to observe need to be defined upfront. The targets are defined in &lt;code>charts/seed-monitoring/charts/prometheus/templates/config.yaml&lt;/code>.
New scrape jobs can be added in the section &lt;code>scrape_configs&lt;/code>. Detailed information how to configure scrape jobs and how to use the kubernetes service discovery are available in the &lt;a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config">Prometheus documentation&lt;/a>.&lt;/p>
&lt;p>The &lt;code>job_name&lt;/code> of a scrape job should be the name of the component e.g. &lt;code>kube-apiserver&lt;/code> or &lt;code>vpn&lt;/code>. The collection interval should be the default of &lt;code>30s&lt;/code>. You do not need to specify this in the configuration.&lt;/p>
&lt;p>Please do not ingest all metrics which are provided by a component. Rather collect only those metrics which are needed to define the alerts and dashboards (i.e. whitelist). This can be achieved by adding the following &lt;code>metric_relabel_configs&lt;/code> statement to your scrape jobs (replace &lt;code>exampleComponent&lt;/code> with component name).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span> - job_name: example-component
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metric_relabel_configs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{{ include &amp;#34;prometheus.keep-metrics.metric-relabel-config&amp;#34; .Values.allowedMetrics.exampleComponent | indent 6 }}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The whitelist for the metrics of your job can be maintained in &lt;code>charts/seed-monitoring/charts/prometheus/values.yaml&lt;/code> in section &lt;code>allowedMetrics.exampleComponent&lt;/code> (replace &lt;code>exampleComponent&lt;/code> with component name). Check the following example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>allowedMetrics:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exampleComponent:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> * metrics_name_1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> * metrics_name_2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="adding-alerts">Adding Alerts&lt;/h2>
&lt;p>The alert definitons are located in &lt;code>charts/seed-monitoring/charts/prometheus/rules&lt;/code>. There are two approaches for adding new alerts.&lt;/p>
&lt;ol>
&lt;li>Adding additional alerts for a component which already has a set of alerts. In this case you have to extend the existing rule file for the component.&lt;/li>
&lt;li>Adding alerts for a new component. In this case a new rule file with name scheme &lt;code>example-component.rules.yaml&lt;/code> needs to be added.&lt;/li>
&lt;li>Add the new alert to &lt;code>alertInhibitionGraph.dot&lt;/code>, add any required inhibition flows and render the new graph. To render the graph run:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>dot -Tpng ./content/alertInhibitionGraph.dot -o ./content/alertInhibitionGraph.png
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>Create a test for the new alert. See &lt;code>Alert Tests&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>Example alert:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>groups:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* name: example.rules
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> * alert: ExampleAlert
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expr: absent(up{job=&amp;#34;exampleJob&amp;#34;} == 1)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> for: 20m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service: example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> severity: critical &lt;span style="color:#008000"># How severe is the alert? (blocker|critical|info|warning)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: shoot &lt;span style="color:#008000"># For which topology is the alert relevant? (seed|shoot)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> visibility: all &lt;span style="color:#008000"># Who should receive the alerts? (all|operator|owner)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> description: A longer description of the example alert that should also explain the impact of the alert.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> summary: Short summary of an example alert.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If the deployment of component is optional then the alert definitions needs to be added to &lt;code>charts/seed-monitoring/charts/prometheus/optional-rules&lt;/code> instead. Furthermore the alerts for component need to be activatable in &lt;code>charts/seed-monitoring/charts/prometheus/values.yaml&lt;/code> via &lt;code>rules.optional.example-component.enabled&lt;/code>. The default should be &lt;code>true&lt;/code>.&lt;/p>
&lt;p>Basic instruction how to define alert rules can be found in the &lt;a href="https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules">Prometheus documentation&lt;/a>.&lt;/p>
&lt;h3 id="routing-tree">Routing tree&lt;/h3>
&lt;p>The Alertmanager is grouping incoming alerts based on labels into buckets. Each bucket has its own configuration like alert receivers, initial delaying duration or resending frequency etc. You can find more information about Alertmanager routing in the &lt;a href="https://prometheus.io/docs/alerting/configuration/#route">Prometheus/Alertmanager documentation&lt;/a>. The routing trees for the Alertmanagers deployed by Gardener are depicted below.&lt;/p>
&lt;p>Central Seed Alertmanager&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>∟ main route (all alerts for all shoots on the seed will enter)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by project and shoot name
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by visibility &amp;#34;all&amp;#34; and &amp;#34;operator&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by severity &amp;#34;blocker&amp;#34;, &amp;#34;critical&amp;#34;, and &amp;#34;info&amp;#34; → route to Garden operators
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by severity &amp;#34;warning&amp;#34; (dropped)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by visibility &amp;#34;owner&amp;#34; (dropped)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Shoot Alertmanager&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>∟ main route (only alerts for one Shoot will enter)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by visibility &amp;#34;all&amp;#34; and &amp;#34;owner&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by severity &amp;#34;blocker&amp;#34;, &amp;#34;critical&amp;#34;, and &amp;#34;info&amp;#34; → route to cluster alert receiver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by severity &amp;#34;warning&amp;#34; (dropped, will change soon → route to cluster alert receiver)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by visibility &amp;#34;operator&amp;#34; (dropped)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="alert-inhibition">Alert Inhibition&lt;/h3>
&lt;p>All alerts related to components running on the Shoot workers are inhibited in case of an issue with the vpn connection, because those components can&amp;rsquo;t be scraped anymore and Prometheus will fire alerts in consequence. The components running on the workers are probably healthy and the alerts are presumably false positives. The inhibition flow is shown in the figure below. If you add a new alert make sure to add it to the diagram.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/alertInhibitionGraph_ceaef0.png" alt="alertDiagram">&lt;/p>
&lt;h3 id="alert-attributes">Alert Attributes&lt;/h3>
&lt;p>Each alert rule definition has to contain the following annotations:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>summary&lt;/strong>: A short description of the issue.&lt;/li>
&lt;li>&lt;strong>description&lt;/strong>: A detailed explanation of the issue with hints to the possible root causes and the impact assessment of the issue.&lt;/li>
&lt;/ul>
&lt;p>In addtion each alert must contain the following labels:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>type&lt;/strong>
&lt;ul>
&lt;li>&lt;code>shoot&lt;/code>: Components running on the Shoot worker nodes in the &lt;code>kube-system&lt;/code> namespace.&lt;/li>
&lt;li>&lt;code>seed&lt;/code>: Components running on the Seed in the Shoot namespace as part of/next to the control plane.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>service&lt;/strong>
&lt;ul>
&lt;li>Name of the component (in lowercase) e.g. &lt;code>kube-apiserver&lt;/code>, &lt;code>alertmanager&lt;/code> or &lt;code>vpn&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>severity&lt;/strong>
&lt;ul>
&lt;li>&lt;code>blocker&lt;/code>: All issues which make the cluster entirely unusable e.g. &lt;code>KubeAPIServerDown&lt;/code> or &lt;code>KubeSchedulerDown&lt;/code>&lt;/li>
&lt;li>&lt;code>critical&lt;/code>: All issues which affect single functionalities/components but not affect the cluster in its core functionality e.g. &lt;code>VPNDown&lt;/code> or &lt;code>KubeletDown&lt;/code>.&lt;/li>
&lt;li>&lt;code>info&lt;/code>: All issues that do not affect the cluster or its core functionality, but if this component is down we cannot determine if a blocker alert is firing. (i.e. A component with an info level severity is a dependency for a component with a blocker severity)&lt;/li>
&lt;li>&lt;code>warning&lt;/code>: No current existing issue, rather a hint for situations which could lead to real issue in the close future e.g. &lt;code>HighLatencyApiServerToWorkers&lt;/code> or &lt;code>ApiServerResponseSlow&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="alert-tests">Alert Tests&lt;/h3>
&lt;p>To test the Prometheus alerts:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make test-prometheus
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you want to add alert tests:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Create a new file in &lt;code>rules-tests&lt;/code> in the form &lt;code>&amp;lt;alert-group-name&amp;gt;.rules.test.yaml&lt;/code> or if the alerts are for an existing component with existing tests, simply add the tests to the appropriate files.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Make sure that newly added tests succeed. See above.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="adding-grafana-dashboards">Adding Grafana Dashboards&lt;/h2>
&lt;p>The dashboard definition files are located in &lt;code>charts/seed-monitoring/charts/grafana/dashboards&lt;/code>. Every dashboard needs its own file.&lt;/p>
&lt;p>If you are adding a new component dashboard please also update the overview dashboard by adding a chart for its current up/down status and with a drill down option to the component dashboard.&lt;/p>
&lt;h3 id="dashboard-structure">Dashboard Structure&lt;/h3>
&lt;p>The dashboards should be structured in the following way. The assignment of the component dashboards to the categories should be handled via dashboard tags.&lt;/p>
&lt;ul>
&lt;li>Kubernetes control plane components (Tag: &lt;code>control-plane&lt;/code>)
&lt;ul>
&lt;li>All components which are part of the Kubernetes control plane e. g. Kube API Server, Kube Controller Manager, Kube Scheduler and Cloud Controller Manager&lt;/li>
&lt;li>ETCD + Backup/Restore&lt;/li>
&lt;li>Kubernetes Addon Manager&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Node/Machine components (Tag: &lt;code>node/machine&lt;/code>)
&lt;ul>
&lt;li>All metrics which are related to the behaviour/control of the Kubernetes nodes and kubelets&lt;/li>
&lt;li>Machine-Controller-Manager + Cluster Autoscaler&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Networking components (Tag: &lt;code>network&lt;/code>)
&lt;ul>
&lt;li>CoreDNS, KubeProxy, Calico, VPN, Nginx Ingress&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Addon components (Tag: &lt;code>addon&lt;/code>)
&lt;ul>
&lt;li>Cert Broker&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Monitoring components (Tag: &lt;code>monitoring&lt;/code>)&lt;/li>
&lt;li>Logging components (Tag: &lt;code>logging&lt;/code>)&lt;/li>
&lt;/ul>
&lt;h4 id="mandatory-charts-for-component-dashboards">Mandatory Charts for Component Dashboards&lt;/h4>
&lt;p>For each new component, its corresponding dashboard should contain the following charts in the first row, before adding custom charts for the component in the subsequent rows.&lt;/p>
&lt;ol>
&lt;li>Pod up/down status &lt;code>up{job=&amp;quot;example-component&amp;quot;}&lt;/code>&lt;/li>
&lt;li>Pod/containers cpu utilization&lt;/li>
&lt;li>Pod/containers memorty consumption&lt;/li>
&lt;li>Pod/containers network i/o&lt;/li>
&lt;/ol>
&lt;p>These information is provided by the cAdvisor metrics. These metrics are already integrated. Please check the other dashboards for detailed information on how to query.&lt;/p>
&lt;h5 id="chart-requirements">Chart Requirements&lt;/h5>
&lt;p>Each chart needs to contain:&lt;/p>
&lt;ul>
&lt;li>a meaningful name&lt;/li>
&lt;li>a detailed description (for non trivial charts)&lt;/li>
&lt;li>appropriate x/y axis descriptions&lt;/li>
&lt;li>appropriate scaling levels for the x/y axis&lt;/li>
&lt;li>proper units for the x/y axis&lt;/li>
&lt;/ul>
&lt;h5 id="dashboard-parameters">Dashboard Parameters&lt;/h5>
&lt;p>The following parameters should be added to all dashboards to ensure a homogeneous experience across all dashboards.&lt;/p>
&lt;p>Dashboards have to &amp;hellip;&lt;/p>
&lt;ul>
&lt;li>contain a title which refers to the component name(s)&lt;/li>
&lt;li>contain a timezone statement which should be the browser time&lt;/li>
&lt;li>contain tags which express where the component is running (&lt;code>seed&lt;/code> or &lt;code>shoot&lt;/code>) and to which category the component belong (see dashboard structure)&lt;/li>
&lt;li>contain a version statement with a value of 1&lt;/li>
&lt;li>be immutable&lt;/li>
&lt;/ul>
&lt;p>Example dashboard configuration&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;title&amp;#34;: &lt;span style="color:#a31515">&amp;#34;example-component&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;timezone&amp;#34;: &lt;span style="color:#a31515">&amp;#34;utc&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;tags&amp;#34;: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;seed&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;control-plane&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;version&amp;#34;: 1,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;editable&amp;#34;: &lt;span style="color:#a31515">&amp;#34;false&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Furthermore all dashboards should contain the following time options:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;time&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;from&amp;#34;: &lt;span style="color:#a31515">&amp;#34;now-1h&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;to&amp;#34;: &lt;span style="color:#a31515">&amp;#34;now&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;timepicker&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;refresh_intervals&amp;#34;: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;30s&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;1m&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;5m&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;time_options&amp;#34;: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;5m&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;15m&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;1h&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;6h&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;12h&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;24h&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;2d&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;10d&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Network Policies</title><link>https://gardener.cloud/docs/gardener/concepts/network_policies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/network_policies/</guid><description>
&lt;h1 id="network-policies-in-gardener">Network Policies in Gardener&lt;/h1>
&lt;p>As &lt;code>Seed&lt;/code> clusters can host the &lt;a href="https://kubernetes.io/docs/concepts/#kubernetes-control-plane">Kubernetes control planes&lt;/a> of many &lt;code>Shoot&lt;/code> clusters, it is necessary to isolate the control planes from each other for security reasons.
Besides deploying each control plane in its own namespace, Gardener creates &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">network policies&lt;/a> to also isolate the networks.
Essentially, network policies make sure that pods can only talk to other pods over the network they are supposed to.
As such, network policies are an important part of Gardener&amp;rsquo;s tenant isolation.&lt;/p>
&lt;p>Gardener deploys network policies into&lt;/p>
&lt;ul>
&lt;li>each namespace hosting the Kubernetes control plane of the Shoot cluster.&lt;/li>
&lt;li>the namespace dedicated to Gardener seed-wide global controllers. This namespace is often called &lt;code>garden&lt;/code> and contains e.g. the &lt;a href="https://github.com/gardener/gardener/blob/15cae57db802cbe460ff4cb3f80c26b2fc15e26f/docs/concepts/gardenlet.md">Gardenlet&lt;/a>.&lt;/li>
&lt;li>the &lt;code>kube-system&lt;/code> namespace in the Shoot.&lt;/li>
&lt;/ul>
&lt;p>The aforementioned namespaces in the Seed contain a &lt;code>deny-all&lt;/code> network policy that &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-and-all-egress-traffic">denies all ingress and egress traffic&lt;/a>.
This &lt;a href="https://en.wikipedia.org/wiki/Secure_by_default">secure by default&lt;/a> setting requires pods to allow network traffic.
This is done by pods having &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#networkpolicy-resource">labels matching to the selectors of the network policies&lt;/a> deployed by Gardener.&lt;/p>
&lt;p>More details on the deployed network policies can be found in the &lt;a href="https://github.com/gardener/gardener/tree/master/docs/development/seed_network_policies.md">development&lt;/a> and &lt;a href="https://github.com/gardener/gardener/tree/master/docs/usage/shoot_network_policies.md">usage&lt;/a> sections.&lt;/p></description></item><item><title>Docs: New Cloud Provider</title><link>https://gardener.cloud/docs/gardener/development/new-cloud-provider/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/new-cloud-provider/</guid><description>
&lt;h1 id="adding-cloud-providers">Adding Cloud Providers&lt;/h1>
&lt;p>This document provides an overview of how to integrate a new cloud provider into Gardener. Each component that requires integration has a detailed description of how to integrate it and the steps required.&lt;/p>
&lt;h2 id="cloud-components">Cloud Components&lt;/h2>
&lt;p>Gardener is composed of 2 or more Kubernetes clusters:&lt;/p>
&lt;ul>
&lt;li>Shoot: These are the end-user clusters, the regular Kubernetes clusters you have seen. They provide places for your workloads to run.&lt;/li>
&lt;li>Seed: This is the &amp;ldquo;management&amp;rdquo; cluster. It manages the control planes of shoots by running them as native Kubernetes workloads.&lt;/li>
&lt;/ul>
&lt;p>These two clusters can run in the same cloud provider, but they do not need to. For example, you could run your Seed in AWS, while having one shoot in Azure, two in Google, two in Alicloud, and three in Equinix Metal.&lt;/p>
&lt;p>The Seed cluster deploys and manages the Shoot clusters. Importantly, for this discussion, the &lt;code>etcd&lt;/code> data store backing each Shoot runs as workloads inside the Seed. Thus, to use the above example, the clusters in Azure, Google, Alicloud and Equinix Metal will have their worker nodes and master nodes running in those clouds, but the &lt;code>etcd&lt;/code> clusters backing them will run as separate &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">deployments&lt;/a> in the Seed Kubernetes cluster on AWS.&lt;/p>
&lt;p>This distinction becomes important when preparing the integration to a new cloud provider.&lt;/p>
&lt;h2 id="gardener-cloud-integration">Gardener Cloud Integration&lt;/h2>
&lt;p>Gardener and its related components integrate with cloud providers at the following key lifecycle elements:&lt;/p>
&lt;ul>
&lt;li>Create/destroy/get/list machines for the Shoot&lt;/li>
&lt;li>Create/destroy/get/list infrastructure components for the Shoot, e.g. VPCs, subnets, routes, etc.&lt;/li>
&lt;li>Backup/restore etcd for the Seed via writing files to and reading them from object storage&lt;/li>
&lt;/ul>
&lt;p>Thus, the integrations you need for your cloud provider depend on whether you want to deploy Shoot clusters to the provider, Seed or both.&lt;/p>
&lt;ul>
&lt;li>Shoot Only: machine lifecycle management, infrastructure.&lt;/li>
&lt;li>Seed: etcd backup/restore&lt;/li>
&lt;/ul>
&lt;h2 id="gardener-api">Gardener API&lt;/h2>
&lt;p>In addition to the requirements to integrate with the cloud provider, you also need to enable the core Gardener app to receive, validate and process requests to use that cloud provider.&lt;/p>
&lt;ul>
&lt;li>Expose the cloud provider to the consumers of the Gardener API, so it can be told to use that cloud provider as an option&lt;/li>
&lt;li>Validate that API as requests come in&lt;/li>
&lt;li>Write cloud provider specific implementation (called &amp;ldquo;provider extension&amp;rdquo;)&lt;/li>
&lt;/ul>
&lt;h2 id="cloud-provider-api-requirements">Cloud Provider API Requirements&lt;/h2>
&lt;p>In order for a cloud provider to integrate with Gardener, the provider must have an API to perform machine lifecycle events, specifically:&lt;/p>
&lt;ul>
&lt;li>Create a machine&lt;/li>
&lt;li>Destroy a machine&lt;/li>
&lt;li>Get information about a machine and its state&lt;/li>
&lt;li>List machines&lt;/li>
&lt;/ul>
&lt;p>In addition, if the Seed is to run on the given provider, it also must have an API to save files to block storage and retrieve them, for etcd backup/restore.&lt;/p>
&lt;p>The current integration with cloud providers is to add their API calls to Gardener and the Machine Controller Manager. As both Gardener and the Machine Controller Manager are written in &lt;a href="https://golang.org">go&lt;/a>, the cloud provider should have a go SDK. However, if it has an API that is wrappable in go, e.g. a REST API, then you can use that to integrate.&lt;/p>
&lt;p>The Gardener team is working on bringing cloud provider integrations out-of-tree, making them pluggable, which should simplify the process and make it possible to use other SDKs.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>To add a new cloud provider, you need some or all of the following. Each repository contains instructions on how to extend it to a new cloud provider.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Type&lt;/th>
&lt;th>Purpose&lt;/th>
&lt;th>Location&lt;/th>
&lt;th>Documentation&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Seed or Shoot&lt;/td>
&lt;td>Machine Lifecycle&lt;/td>
&lt;td>&lt;a href="https://github.com/gardener/machine-controller-manager">machine-controller-manager&lt;/a>&lt;/td>
&lt;td>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/docs/development/cp_support_new/">MCM new cloud provider&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Seed only&lt;/td>
&lt;td>etcd backup/restore&lt;/td>
&lt;td>&lt;a href="https://github.com/gardener/etcd-backup-restore/">etcd-backup-restore&lt;/a>&lt;/td>
&lt;td>In process&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>All&lt;/td>
&lt;td>Extension implementation&lt;/td>
&lt;td>&lt;a href="https://github.com/gardener/gardener">gardener&lt;/a>&lt;/td>
&lt;td>&lt;a href="https://gardener.cloud/docs/gardener/extensions/overview/">Extension controller&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Docs: New Kubernetes Version</title><link>https://gardener.cloud/docs/gardener/development/new-kubernetes-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/new-kubernetes-version/</guid><description>
&lt;h1 id="adding-support-for-a-new-kubernetes-version">Adding Support For A New Kubernetes Version&lt;/h1>
&lt;p>This document describes the steps needed to perform in order to confidently add support for a new Kubernetes &lt;strong>minor&lt;/strong> version.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ Typically, once a minor Kubernetes version &lt;code>vX.Y&lt;/code> is supported by Gardener then all patch versions &lt;code>vX.Y.Z&lt;/code> are also automatically supported without any required action.
This is because patch versions do not introduce any new feature or API changes, so there is nothing that needs to be adapted in &lt;code>gardener/gardener&lt;/code> code.&lt;/p>
&lt;/blockquote>
&lt;p>The Kubernetes community release a new minor version roughly every 4 months.
Please refer to the &lt;a href="https://kubernetes.io/releases/release/">official documentation&lt;/a> about their release cycles for any additional information.&lt;/p>
&lt;p>Shortly before a new release, an &amp;ldquo;umbrella&amp;rdquo; issue should be opened which is used to collect the required adaptations and to track the work items.
For example, &lt;a href="https://github.com/gardener/gardener/issues/5102">#5102&lt;/a> can be used as a template for the issue description.&lt;br>
As you can see, the task of supporting a new Kubernetes version also includes the provider extensions maintained in the &lt;code>gardener&lt;/code> GitHub organization and is not restricted to &lt;code>gardener/gardener&lt;/code> only.&lt;/p>
&lt;p>Generally, the work items can be split into two groups:
The first group contains Kubernetes release-independent tasks, the second group contains tasks specific to the changes in the given Kubernetes release.&lt;/p>
&lt;blockquote>
&lt;p>ℹ️ Upgrading the &lt;code>k8s.io/*&lt;/code> and &lt;code>sigs.k8s.io/controller-runtime&lt;/code> Golang dependencies is typically tracked and worked on separately (see e.g. &lt;a href="https://github.com/gardener/gardener/issues/4772">#4772&lt;/a> or &lt;a href="https://github.com/gardener/gardener/issues/5282">#5282&lt;/a>).&lt;/p>
&lt;/blockquote>
&lt;h2 id="deriving-release-specific-tasks">Deriving Release-Specific Tasks&lt;/h2>
&lt;p>Most new minor Kubernetes releases incorporate API changes, deprecations or new features.
The community announces them via their &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/">change logs&lt;/a>.
In order to derive the release-specific tasks, the respective change log for the new version &lt;code>vX.Y&lt;/code> has to be read and understood (for example, &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md">this document&lt;/a> for &lt;code>v1.24&lt;/code>).&lt;/p>
&lt;p>As already mentioned, typical changes to watch out for are:&lt;/p>
&lt;ul>
&lt;li>API version promotions or deprecations&lt;/li>
&lt;li>Feature gate promotions or deprecations&lt;/li>
&lt;li>CLI flag changes for Kubernetes components&lt;/li>
&lt;li>New default values in resources&lt;/li>
&lt;li>New available fields in resources&lt;/li>
&lt;li>New features potentially relevant for the Gardener system&lt;/li>
&lt;li>Changes of labels or annotations Gardener relies on&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>Obviously, this requires a certain experience and understanding of the Gardener project so that all &amp;ldquo;relevant changes&amp;rdquo; can be identified.
While reading the change log, add the tasks (along with the respective PR in &lt;code>kubernetes/kubernetes&lt;/code> to the umbrella issue).&lt;/p>
&lt;blockquote>
&lt;p>ℹ️ Some of the changes might be specific to certain cloud providers. Pay attention to those as well and add related tasks to the issue.&lt;/p>
&lt;/blockquote>
&lt;h2 id="list-of-release-independent-tasks">List Of Release-Independent Tasks&lt;/h2>
&lt;p>The following paragraphs describe recurring tasks that need to be performed for each new release.&lt;/p>
&lt;h3 id="releasing-a-new-hyperkube-image">Releasing A New &lt;code>hyperkube&lt;/code> Image&lt;/h3>
&lt;p>The &lt;a href="https://github.com/gardener/hyperkube">&lt;code>gardener/hyperkube&lt;/code>&lt;/a> repository is used to release container images consisting of the &lt;code>kubectl&lt;/code> and &lt;code>kubelet&lt;/code> binaries.&lt;/p>
&lt;p>Run the &lt;a href="https://github.com/gardener/hyperkube/blob/master/.ci/check-and-release">&lt;code>.ci/check-and-release&lt;/code>&lt;/a> script to automatically build the image (make sure Docker is running!), push the images to the GCR (make sure &lt;code>gcloud&lt;/code> is configured properly!) and publish the release on GitHub (make sure &lt;code>git&lt;/code> is configured properly!).&lt;/p>
&lt;h3 id="adapting-gardener">Adapting Gardener&lt;/h3>
&lt;ul>
&lt;li>Allow instantiation of a Kubernetes client for the new minor version and update the &lt;code>README.md&lt;/code>:
&lt;ul>
&lt;li>See &lt;a href="https://github.com/gardener/gardener/pull/5255/commits/63bdae022f1cb1c9cbd1cd49b557545dca2ec32a">this&lt;/a> example commit.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Maintain the Kubernetes feature gates used for validation of &lt;code>Shoot&lt;/code> resources:
&lt;ul>
&lt;li>The feature gates are maintained in &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/utils/validation/features/featuregates.go">this&lt;/a> file.&lt;/li>
&lt;li>To maintain this list for new Kubernetes versions, run &lt;code>hack/compare-k8s-feature-gates.sh &amp;lt;old-version&amp;gt; &amp;lt;new-version&amp;gt;&lt;/code> (e.g. &lt;code>hack/compare-k8s-feature-gates.sh v1.22 v1.23&lt;/code>).&lt;/li>
&lt;li>It will present 3 lists of feature gates: those added and those removed in &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> compared to &lt;code>&amp;lt;old-version&amp;gt;&lt;/code> and feature gates that got locked to default in &lt;code>&amp;lt;new-version&amp;gt;&lt;/code>.&lt;/li>
&lt;li>Add all added feature gates to the map with &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> as &lt;code>AddedInVersion&lt;/code> and no &lt;code>RemovedInVersion&lt;/code>.&lt;/li>
&lt;li>For any removed feature gates, add &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> as RemovedInVersion to the already existing feature gate in the map.&lt;/li>
&lt;li>For feature gates locked to default, add &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> as &lt;code>LockedToDefaultInVersion&lt;/code> to the already existing feature gate in the map.&lt;/li>
&lt;li>See &lt;a href="https://github.com/gardener/gardener/pull/5255/commits/97923b0604300ff805def8eae981ed388d5e4a83">this&lt;/a> example commit.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Maintain the &lt;code>ServiceAccount&lt;/code> names for the controllers part of &lt;code>kube-controller-manager&lt;/code>:
&lt;ul>
&lt;li>The names are maintained in &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/operation/botanist/component/shootsystem/shootsystem.go">this&lt;/a> file.&lt;/li>
&lt;li>To maintain this list for new Kubernetes versions, run &lt;code>hack/compare-k8s-controllers.sh &amp;lt;old-version&amp;gt; &amp;lt;new-version&amp;gt;&lt;/code> (e.g. &lt;code>hack/compare-k8s-controllers.sh 1.22 1.23&lt;/code>).&lt;/li>
&lt;li>It will present 2 lists of controllers: those added and those removed in &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> compared to &lt;code>&amp;lt;old-version&amp;gt;&lt;/code>.&lt;/li>
&lt;li>Double check whether such &lt;code>ServiceAccount&lt;/code> indeed appears in the &lt;code>kube-system&lt;/code> namespace when creating a cluster with &lt;code>&amp;lt;new-version&amp;gt;&lt;/code>. Note that it sometimes might be hidden behind a default-off feature gate. You can create a local cluster with the new version using the &lt;a href="https://gardener.cloud/docs/gardener/development/getting_started_locally/">local provider&lt;/a>.&lt;/li>
&lt;li>If it appears, add all added controllers to the list based on the Kubernetes version (&lt;a href="https://github.com/gardener/gardener/blob/5f87b18b951e104c2c25a7145548c8a2d08adefc/pkg/operation/botanist/component/shootsystem/shootsystem.go#L170-L174">example&lt;/a>).&lt;/li>
&lt;li>For any removed controllers, add them only to the Kubernetes version if it is low enough.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Bump the used Kubernetes version for local &lt;code>Shoot&lt;/code> and local e2e test.
&lt;ul>
&lt;li>See &lt;a href="https://github.com/gardener/gardener/pull/5255/commits/5707c4c7a4fd265b176387178b755cabeea89ffe">this&lt;/a> example commit.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="filing-the-pull-request">Filing The Pull Request&lt;/h4>
&lt;p>Work on all the tasks you have collected and validate them using the &lt;a href="https://gardener.cloud/docs/gardener/development/getting_started_locally/">local provider&lt;/a>.
Execute the e2e tests and if everything looks good, then go ahead and file the PR (&lt;a href="https://github.com/gardener/gardener/pull/5255">example PR&lt;/a>).
Generally, it is great if you add the PRs also to the umbrella issue so that they can be tracked more easily.&lt;/p>
&lt;h3 id="adapting-provider-extensions">Adapting Provider Extensions&lt;/h3>
&lt;p>After the PR in &lt;code>gardener/gardener&lt;/code> for the support of the new version has been merged, you can go ahead and work on the provider extensions.&lt;/p>
&lt;blockquote>
&lt;p>Actually, you can already start even if the PR is not yet merged and use the branch of your fork.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Revendor the &lt;code>github.com/gardener/gardener&lt;/code> dependency in the extension and update the &lt;code>README.md&lt;/code>.&lt;/li>
&lt;li>Work on release-specific tasks related to this provider.&lt;/li>
&lt;/ul>
&lt;h4 id="maintaining-the-cloud-controller-manager-images">Maintaining The &lt;code>cloud-controller-manager&lt;/code> Images&lt;/h4>
&lt;p>Some of the cloud providers are not yet using upstream &lt;code>cloud-controller-manager&lt;/code> images.
Instead, we build and maintain them ourselves:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/gardener/cloud-provider-aws">https://github.com/gardener/cloud-provider-aws&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/cloud-provider-azure">https://github.com/gardener/cloud-provider-azure&lt;/a> (since &lt;code>v1.23&lt;/code>, we use the upstream image)&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/cloud-provider-gcp">https://github.com/gardener/cloud-provider-gcp&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Until we switch to upstream images, you need to revendor the Kubernetes dependencies and release a new image.
The required steps are as follows:&lt;/p>
&lt;ul>
&lt;li>Checkout the &lt;code>legacy-cloud-provider&lt;/code> branch of the respective repository&lt;/li>
&lt;li>Bump the versions in the &lt;code>Dockerfile&lt;/code> (&lt;a href="https://github.com/gardener/cloud-provider-gcp/commit/b7eb3f56b252aaf29adc78406672574b1bc17495">example commit&lt;/a>).&lt;/li>
&lt;li>Update the &lt;code>VERSION&lt;/code> to &lt;code>vX.Y.Z-dev&lt;/code> where &lt;code>Z&lt;/code> is the latest available Kubernetes patch version for the &lt;code>vX.Y&lt;/code> minor version.&lt;/li>
&lt;li>Update the &lt;code>k8s.io/*&lt;/code> dependencies in the &lt;code>go.mod&lt;/code> file to &lt;code>vX.Y.Z&lt;/code> and run &lt;code>go mod vendor&lt;/code> and &lt;code>go mod tidy&lt;/code> (&lt;a href="https://github.com/gardener/cloud-provider-gcp/commit/d41cc9f035bcc4893b40d90a4f617c4d436c5d62">example commit&lt;/a>).&lt;/li>
&lt;li>Checkout a new &lt;code>release-vX.Y&lt;/code> branch and release it (&lt;a href="https://github.com/gardener/cloud-provider-gcp/commits/release-v1.23">example&lt;/a>)&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>As you are already on it, it is great if you also bump the &lt;code>k8s.io/*&lt;/code> dependencies for the last three minor releases as well.
In this case, you need to checkout the &lt;code>release-vX.{Y-{1,2,3}}&lt;/code> branches and only perform the last three steps (&lt;a href="https://github.com/gardener/cloud-provider-gcp/commits/release-v1.20">example branch&lt;/a>, &lt;a href="https://github.com/gardener/cloud-provider-gcp/commit/372aa43fbacdeb76b3da9f6fad6cfd924d916227">example commit&lt;/a>).&lt;/p>
&lt;/blockquote>
&lt;p>Now you need to update the new releases in the &lt;code>charts/images.yaml&lt;/code> of the respective provider extension so that they are used (see this &lt;a href="https://github.com/gardener/gardener-extension-provider-aws/pull/480/commits/76256de933d5a508aba26a8f589dd1a39026142e">example commit&lt;/a> for reference).&lt;/p>
&lt;h4 id="filing-the-pull-request-1">Filing The Pull Request&lt;/h4>
&lt;p>Again, work on all the tasks you have collected.
This time, you cannot use the local provider for validation but should create real clusters on the various infrastructures.
Typically, the following validations should be performed:&lt;/p>
&lt;ul>
&lt;li>Create new clusters with versions &amp;lt; &lt;code>vX.Y&lt;/code>&lt;/li>
&lt;li>Create new clusters with version = &lt;code>vX.Y&lt;/code>&lt;/li>
&lt;li>Upgrade old clusters from version &lt;code>vX.{Y-1}&lt;/code> to version &lt;code>vX.Y&lt;/code>&lt;/li>
&lt;li>Delete clusters with versions &amp;lt; &lt;code>vX.Y&lt;/code>&lt;/li>
&lt;li>Delete clusters with version = &lt;code>vX.Y&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>If everything looks good, then go ahead and file the PR (&lt;a href="https://github.com/gardener/gardener-extension-provider-aws/pull/480">example PR&lt;/a>).
Generally, it is again great if you add the PRs also to the umbrella issue so that they can be tracked more easily.&lt;/p></description></item><item><title>Docs: Process</title><link>https://gardener.cloud/docs/gardener/development/process/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/process/</guid><description>
&lt;h1 id="releases-features-hotfixes">Releases, Features, Hotfixes&lt;/h1>
&lt;p>This document describes how to contribute features or hotfixes, and how new Gardener releases are usually scheduled, validated, etc.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#releases">Releases&lt;/a>&lt;/li>
&lt;li>&lt;a href="#contributing-new-features-or-fixes">Contributing new Features or Fixes&lt;/a>&lt;/li>
&lt;li>&lt;a href="#cherry-picks">Cherry Picks&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="releases">Releases&lt;/h2>
&lt;p>The &lt;a href="https://github.com/orgs/gardener/teams/gardener-maintainers">@gardener-maintainers&lt;/a> are trying to provide a new release roughly every other week (depending on their capacity and the stability/robustness of the &lt;code>master&lt;/code> branch).&lt;/p>
&lt;p>Hotfixes are usually maintained for the latest three minor releases, though, there are no fixed release dates.&lt;/p>
&lt;h3 id="release-responsible-plan">Release Responsible Plan&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Version&lt;/th>
&lt;th>Week No&lt;/th>
&lt;th>Begin Validation Phase&lt;/th>
&lt;th>Due Date&lt;/th>
&lt;th>Release Responsible&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>v1.45&lt;/td>
&lt;td>Week 15-16&lt;/td>
&lt;td>April 11, 2022&lt;/td>
&lt;td>April 24, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/acumino">@acumino&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.46&lt;/td>
&lt;td>Week 17-18&lt;/td>
&lt;td>April 25, 2022&lt;/td>
&lt;td>May 8, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.47&lt;/td>
&lt;td>Week 19-20&lt;/td>
&lt;td>May 9, 2022&lt;/td>
&lt;td>May 22, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/shafeeqes">@shafeeqes&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.48&lt;/td>
&lt;td>Week 21-22&lt;/td>
&lt;td>May 23, 2022&lt;/td>
&lt;td>June 5, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/ary1992">@ary1992&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.49&lt;/td>
&lt;td>Week 23-24&lt;/td>
&lt;td>June 6, 2022&lt;/td>
&lt;td>June 19, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/plkokanov">@plkokanov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.50&lt;/td>
&lt;td>Week 25-26&lt;/td>
&lt;td>June 20, 2022&lt;/td>
&lt;td>July 3, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.51&lt;/td>
&lt;td>Week 27-28&lt;/td>
&lt;td>July 4, 2022&lt;/td>
&lt;td>July 17, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/timebertt">@timebertt&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.52&lt;/td>
&lt;td>Week 29-30&lt;/td>
&lt;td>July 18, 2022&lt;/td>
&lt;td>July 31, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/kris94">@kris94&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.53&lt;/td>
&lt;td>Week 31-32&lt;/td>
&lt;td>August 1, 2022&lt;/td>
&lt;td>August 14, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/acumino">@acumino&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.54&lt;/td>
&lt;td>Week 33-34&lt;/td>
&lt;td>August 15, 2022&lt;/td>
&lt;td>August 28, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.55&lt;/td>
&lt;td>Week 35-36&lt;/td>
&lt;td>August 29, 2022&lt;/td>
&lt;td>September 11, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/oliver-goetz">@oliver-goetz&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.56&lt;/td>
&lt;td>Week 37-38&lt;/td>
&lt;td>September 12, 2022&lt;/td>
&lt;td>September 25, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/shafeeqes">@shafeeqes&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Apart from the release of the next version, the release responsible is also taking care of potential hotfix releases of the last three minor versions.
The release responsible is the main contact person for coordinating new feature PRs for the next minor versions or cherry-pick PRs for the last three minor versions.&lt;/p>
&lt;details>
&lt;summary>Click to expand the archived release responsible associations!&lt;/summary>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Version&lt;/th>
&lt;th>Week No&lt;/th>
&lt;th>Begin Validation Phase&lt;/th>
&lt;th>Due Date&lt;/th>
&lt;th>Release Responsible&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>v1.17&lt;/td>
&lt;td>Week 07-08&lt;/td>
&lt;td>February 15, 2021&lt;/td>
&lt;td>February 28, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.18&lt;/td>
&lt;td>Week 09-10&lt;/td>
&lt;td>March 1, 2021&lt;/td>
&lt;td>March 14, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/danielfoehrKn">@danielfoehrKn&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.19&lt;/td>
&lt;td>Week 11-12&lt;/td>
&lt;td>March 15, 2021&lt;/td>
&lt;td>March 28, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/timebertt">@timebertt&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.20&lt;/td>
&lt;td>Week 13-14&lt;/td>
&lt;td>March 29, 2021&lt;/td>
&lt;td>April 11, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/vpnachev">@vpnachev&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.21&lt;/td>
&lt;td>Week 15-16&lt;/td>
&lt;td>April 12, 2021&lt;/td>
&lt;td>April 25, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/timuthy">@timuthy&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.22&lt;/td>
&lt;td>Week 17-18&lt;/td>
&lt;td>April 26, 2021&lt;/td>
&lt;td>May 9, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/BeckerMax">@BeckerMax&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.23&lt;/td>
&lt;td>Week 19-20&lt;/td>
&lt;td>May 10, 2021&lt;/td>
&lt;td>May 23, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.24&lt;/td>
&lt;td>Week 21-22&lt;/td>
&lt;td>May 24, 2021&lt;/td>
&lt;td>June 5, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/stoyanr">@stoyanr&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.25&lt;/td>
&lt;td>Week 23-24&lt;/td>
&lt;td>June 7, 2021&lt;/td>
&lt;td>June 20, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.26&lt;/td>
&lt;td>Week 25-26&lt;/td>
&lt;td>June 21, 2021&lt;/td>
&lt;td>July 4, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/danielfoehrKn">@danielfoehrKn&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.27&lt;/td>
&lt;td>Week 27-28&lt;/td>
&lt;td>July 5, 2021&lt;/td>
&lt;td>July 18, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/timebertt">@timebertt&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.28&lt;/td>
&lt;td>Week 29-30&lt;/td>
&lt;td>July 19, 2021&lt;/td>
&lt;td>August 1, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.29&lt;/td>
&lt;td>Week 31-32&lt;/td>
&lt;td>August 2, 2021&lt;/td>
&lt;td>August 15, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/timuthy">@timuthy&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.30&lt;/td>
&lt;td>Week 33-34&lt;/td>
&lt;td>August 16, 2021&lt;/td>
&lt;td>August 29, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/BeckerMax">@BeckerMax&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.31&lt;/td>
&lt;td>Week 35-36&lt;/td>
&lt;td>August 30, 2021&lt;/td>
&lt;td>September 12, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/stoyanr">@stoyanr&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.32&lt;/td>
&lt;td>Week 37-38&lt;/td>
&lt;td>September 13, 2021&lt;/td>
&lt;td>September 26, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/vpnachev">@vpnachev&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.33&lt;/td>
&lt;td>Week 39-40&lt;/td>
&lt;td>September 27, 2021&lt;/td>
&lt;td>October 10, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/voelzmo">@voelzmo&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.34&lt;/td>
&lt;td>Week 41-42&lt;/td>
&lt;td>October 11, 2021&lt;/td>
&lt;td>October 24, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/plkokanov">@plkokanov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.35&lt;/td>
&lt;td>Week 43-44&lt;/td>
&lt;td>October 25, 2021&lt;/td>
&lt;td>November 7, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/kris94">@kris94&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.36&lt;/td>
&lt;td>Week 45-46&lt;/td>
&lt;td>November 8, 2021&lt;/td>
&lt;td>November 21, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/timebertt">@timebertt&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.37&lt;/td>
&lt;td>Week 47-48&lt;/td>
&lt;td>November 22, 2021&lt;/td>
&lt;td>December 5, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/danielfoehrKn">@danielfoehrKn&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.38&lt;/td>
&lt;td>Week 49-50&lt;/td>
&lt;td>December 6, 2021&lt;/td>
&lt;td>December 19, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.39&lt;/td>
&lt;td>Week 01-04&lt;/td>
&lt;td>January 3, 2022&lt;/td>
&lt;td>January 30, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>, &lt;a href="https://github.com/timuthy">@timuthy&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.40&lt;/td>
&lt;td>Week 05-06&lt;/td>
&lt;td>January 31, 2022&lt;/td>
&lt;td>February 13, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/BeckerMax">@BeckerMax&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.41&lt;/td>
&lt;td>Week 07-08&lt;/td>
&lt;td>February 14, 2022&lt;/td>
&lt;td>February 27, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/plkokanov">@plkokanov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.42&lt;/td>
&lt;td>Week 09-10&lt;/td>
&lt;td>February 28, 2022&lt;/td>
&lt;td>March 13, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/kris94">@kris94&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.43&lt;/td>
&lt;td>Week 11-12&lt;/td>
&lt;td>March 14, 2022&lt;/td>
&lt;td>March 27, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.44&lt;/td>
&lt;td>Week 13-14&lt;/td>
&lt;td>March 28, 2022&lt;/td>
&lt;td>April 10, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/timebertt">@timebertt&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/details>
&lt;h3 id="release-validation">Release Validation&lt;/h3>
&lt;p>The release phase for a new minor version lasts two weeks.
Typically, the first week is used for the validation of the release.
This phase includes the following steps:&lt;/p>
&lt;ol>
&lt;li>&lt;code>master&lt;/code> (or latest &lt;code>release-*&lt;/code> branch) is deployed to a development landscape that already hosts some existing seed and shoot clusters.&lt;/li>
&lt;li>An extended test suite is triggered by the &amp;ldquo;release responsible&amp;rdquo; which
&lt;ol>
&lt;li>executes the Gardener integration tests for different Kubernetes versions, infrastructures, and &lt;code>Shoot&lt;/code> settings.&lt;/li>
&lt;li>executes the Kubernetes conformance tests.&lt;/li>
&lt;li>executes further tests like Kubernetes/OS patch/minor version upgrades.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Additionally, every four hours (or on demand) more tests (e.g., including the Kubernetes e2e test suite) are executed for different infrastructures.&lt;/li>
&lt;li>The &amp;ldquo;release responsible&amp;rdquo; is verifying new features or other notable changes (derived of the draft release notes) in this development system.&lt;/li>
&lt;/ol>
&lt;p>Usually, the new release is triggered in the beginning of the second week if all tests are green, all checks were successful, and if all of the planned verifications were performed by the release responsible.&lt;/p>
&lt;h2 id="contributing-new-features-or-fixes">Contributing new Features or Fixes&lt;/h2>
&lt;p>Please refer to the &lt;a href="https://gardener.cloud/docs/contribute/">Gardener contributor guide&lt;/a>.
Besides a lot of a general information, it also provides a checklist for newly created pull requests that may help you to prepare your changes for an efficient review process.
If you are contributing a fix or major improvement, please take care to open cherry-pick PRs to all affected and still supported versions once the change is approved and merged in the &lt;code>master&lt;/code> branch.&lt;/p>
&lt;p>⚠️ Please ensure that your modifications pass the verification checks (linting, formatting, static code checks, tests, etc.) by executing&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make verify
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>before filing your pull request.&lt;/p>
&lt;p>The guide applies for both changes to the &lt;code>master&lt;/code> and to any &lt;code>release-*&lt;/code> branch.
All changes must be submitted via a pull request and be reviewed and approved by at least one code owner.&lt;/p>
&lt;h2 id="cherry-picks">Cherry Picks&lt;/h2>
&lt;p>This section explains how to initiate cherry picks on release branches within the &lt;code>gardener/gardener&lt;/code> repository.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#prerequisites">Prerequisites&lt;/a>&lt;/li>
&lt;li>&lt;a href="#initiate-a-cherry-pick">Initiate a Cherry Pick&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="prerequisites">Prerequisites&lt;/h3>
&lt;p>Before you initiate a cherry pick, make sure that the following prerequisites are accomplished.&lt;/p>
&lt;ul>
&lt;li>A pull request merged against the &lt;code>master&lt;/code> branch.&lt;/li>
&lt;li>The release branch exists (check in the &lt;a href="https://github.com/gardener/gardener/branches">branches section&lt;/a>)&lt;/li>
&lt;li>Have the &lt;code>gardener/gardener&lt;/code> repository cloned as follows:
&lt;ul>
&lt;li>the &lt;code>origin&lt;/code> remote should point to your fork (alternatively this can be overwritten by passing &lt;code>FORK_REMOTE=&amp;lt;fork-remote&amp;gt;&lt;/code>)&lt;/li>
&lt;li>the &lt;code>upstream&lt;/code> remote should point to the Gardener github org (alternatively this can be overwritten by passing &lt;code>UPSTREAM_REMOTE=&amp;lt;upstream-remote&amp;gt;&lt;/code>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Have &lt;code>hub&lt;/code> installed, which is most easily installed via
&lt;code>go get github.com/github/hub&lt;/code> assuming you have a standard golang
development environment.&lt;/li>
&lt;li>A github token which has permissions to create a PR in an upstream branch.&lt;/li>
&lt;/ul>
&lt;h3 id="initiate-a-cherry-pick">Initiate a Cherry Pick&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Run the &lt;a href="https://github.com/gardener/gardener/blob/master/hack/cherry-pick-pull.sh">cherry pick script&lt;/a>&lt;/p>
&lt;p>This example applies a master branch PR #3632 to the remote branch
&lt;code>upstream/release-v3.14&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>GITHUB_USER=&amp;lt;your-user&amp;gt; hack/cherry-pick-pull.sh upstream/release-v3.14 3632
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;p>Be aware the cherry pick script assumes you have a git remote called
&lt;code>upstream&lt;/code> that points at the Gardener github org.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You will need to run the cherry pick script separately for each patch
release you want to cherry pick to. Cherry picks should be applied to all
active release branches where the fix is applicable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When asked for your github password, provide the created github token
rather than your actual github password.
Refer &lt;a href="https://github.com/github/hub/issues/2655#issuecomment-735836048">https://github.com/github/hub/issues/2655#issuecomment-735836048&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: Proposals</title><link>https://gardener.cloud/docs/gardener/proposals/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/proposals/</guid><description>
&lt;h1 id="gardener-enhancement-proposal-gep">Gardener Enhancement Proposal (GEP)&lt;/h1>
&lt;p>Changes to the Gardener code base are often incorporated directly via pull requests which either themselves contain a description about the motivation and scope of a change or a linked GitHub issue does.&lt;/p>
&lt;p>If a perspective feature has a bigger extent, requires the involvement of several parties or more discussion is needed before the actual implementation can be started, you may consider filing a pull request with a Gardener Enhancement Proposal (GEP) first.&lt;/p>
&lt;p>GEPs are a measure to propose a change or to add a feature to Gardener, help you to describe the change(s) conceptionally, and to list the steps that are necessary to reach this goal. It helps the Gardener maintainers as well as the community to understand the motivation and scope around your proposed change(s) and encourages their contribution to discussions and future pull requests. If you are familiar with the Kubernetes community, GEPs are analogue to Kubernetes Enhancement Proposals (&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps">KEPs&lt;/a>).&lt;/p>
&lt;h2 id="reasons-for-a-gep">Reasons for a GEP&lt;/h2>
&lt;p>You may consider filing a GEP for the following reasons:&lt;/p>
&lt;ul>
&lt;li>A Gardener architectural change is intended / necessary&lt;/li>
&lt;li>Major changes to the Gardener code base&lt;/li>
&lt;li>A phased implementation approach is expected because of the widespread scope of the change&lt;/li>
&lt;li>Your proposed changes may be controversial&lt;/li>
&lt;/ul>
&lt;p>We encourage you to take a look at already merged &lt;a href="https://github.com/gardener/gardener/tree/master/docs/proposals">GEPs&lt;/a> since they give you a sense of what a typical GEP comprises.&lt;/p>
&lt;h2 id="before-creating-a-gep">Before creating a GEP&lt;/h2>
&lt;p>Before starting your work and creating a GEP, please take some time to familiarize yourself with our
general &lt;a href="https://gardener.cloud/docs/contribute/">Gardener Contribution Guidelines&lt;/a>.&lt;/p>
&lt;p>It is recommended to discuss and outline the motivation of your prospective GEP as a draft with the community before you take the investment of creating the actual GEP. This early briefing supports the understanding for the broad community and leads to a fast feedback for your proposal from the respective experts in the community.
An appropriate format for this may be the regular &lt;a href="https://gardener.cloud/docs/contribute/#bi-weekly-meetings">Gardener community meetings&lt;/a>.&lt;/p>
&lt;h2 id="how-to-file-a-gep">How to file a GEP&lt;/h2>
&lt;p>GEPs should be created as Markdown &lt;code>.md&lt;/code> files and are submitted through a GitHub pull request to their current home in &lt;a href="https://github.com/gardener/gardener/tree/master/docs/proposals">docs/proposals&lt;/a>. Please use the provided &lt;a href="https://gardener.cloud/docs/gardener/proposals/00-template/">template&lt;/a> or follow the structure of existing &lt;a href="https://github.com/gardener/gardener/tree/master/docs/proposals">GEPs&lt;/a> which makes reviewing easier and faster. Additionally, please link the new GEP in our documentation &lt;a href="https://github.com/gardener/gardener/blob/master/docs/README.md#Proposals">index&lt;/a>.&lt;/p>
&lt;p>If not already done, please present your GEP in the &lt;a href="https://gardener.cloud/docs/contribute/#bi-weekly-meetings">regular community meetings&lt;/a> to brief the community about your proposal (we strive for personal communication :) ). Also consider that this may be an important step to raise awareness and understanding for everyone involved.&lt;/p>
&lt;p>The GEP template contains a small set of metadata, which is helpful for keeping track of the enhancement
in general and especially of who is responsible for implementing and reviewing PRs that are part of
the enhancement.&lt;/p>
&lt;h3 id="main-reviewers">Main Reviewers&lt;/h3>
&lt;p>Apart from general metadata, the GEP should name at least one &amp;ldquo;main reviewer&amp;rdquo;.
You can find a main reviewer for your GEP either when discussing the proposal in the community meeting, by asking in our
&lt;a href="https://gardener.cloud/docs/contribute/#slack-channel">Slack Channel&lt;/a> or at latest during the GEP PR review.
New GEPs should only be accepted once at least one main reviewer is nominated/assigned.&lt;/p>
&lt;p>The main reviewers are charged with the following tasks:&lt;/p>
&lt;ul>
&lt;li>familiarizing themselves with the details of the proposal&lt;/li>
&lt;li>reviewing the GEP PR itself and any further updates to the document&lt;/li>
&lt;li>discussing design details and clarifying implementation questions with the author before and after
the proposal was accepted&lt;/li>
&lt;li>reviewing PRs related to the GEP in-depth&lt;/li>
&lt;/ul>
&lt;p>Other community members are of course also welcome to help the GEP author, review his work and raise
general concerns with the enhancement. Nevertheless, the main reviewers are supposed to focus on more
in-depth reviews and accompaning the whole GEP process end-to-end, which helps with getting more
high-quality reviews and faster feedback cycles instead of having more people looking at the process
with lower priority and less focus.&lt;/p>
&lt;h2 id="gep-process">GEP Process&lt;/h2>
&lt;ol>
&lt;li>Pre-discussions about GEP (if necessary)&lt;/li>
&lt;li>Find a main reviewer for your enhancement&lt;/li>
&lt;li>GEP is filed through GitHub PR&lt;/li>
&lt;li>Presentation in Gardener community meeting (if possible)&lt;/li>
&lt;li>Review of GEP from maintainers/community&lt;/li>
&lt;li>GEP is merged if accepted&lt;/li>
&lt;li>Implementation of GEP&lt;/li>
&lt;li>Consider keeping GEP up-to-date in case implementation differs essentially&lt;/li>
&lt;/ol></description></item><item><title>Docs: Resource Manager</title><link>https://gardener.cloud/docs/gardener/concepts/resource-manager/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/resource-manager/</guid><description>
&lt;h1 id="gardener-resource-manager">Gardener Resource Manager&lt;/h1>
&lt;p>Initially, the gardener-resource-manager was a project similar to the &lt;a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager">kube-addon-manager&lt;/a>.
It manages Kubernetes resources in a target cluster which means that it creates, updates, and deletes them.
Also, it makes sure that manual modifications to these resources are reconciled back to the desired state.&lt;/p>
&lt;p>In the Gardener project we were using the kube-addon-manager since more than two years.
While we have progressed with our &lt;a href="https://gardener.cloud/docs/gardener/proposals/01-extensibility/">extensibility story&lt;/a> (moving cloud providers out-of-tree) we had decided that the kube-addon-manager is no longer suitable for this use-case.
The problem with it is that it needs to have its managed resources on its file system.
This requires storing the resources in &lt;code>ConfigMap&lt;/code>s or &lt;code>Secret&lt;/code>s and mounting them to the kube-addon-manager pod during deployment time.
The gardener-resource-manager uses &lt;code>CustomResourceDefinition&lt;/code>s which allows to dynamically add, change, and remove resources with immediate action and without the need to reconfigure the volume mounts/restarting the pod.&lt;/p>
&lt;p>Meanwhile, the &lt;code>gardener-resource-manager&lt;/code> has evolved to a more generic component comprising several controllers and webhook handlers.
It is deployed by gardenlet once per seed (in the &lt;code>garden&lt;/code> namespace) and once per shoot (in the respective shoot namespaces in the seed).&lt;/p>
&lt;h2 id="controllers">Controllers&lt;/h2>
&lt;h3 id="managedresource-controller">&lt;code>ManagedResource&lt;/code> controller&lt;/h3>
&lt;p>This controller watches custom objects called &lt;code>ManagedResource&lt;/code>s in the &lt;code>resources.gardener.cloud/v1alpha1&lt;/code> API group.
These objects contain references to secrets which itself contain the resources to be managed.
The reason why a &lt;code>Secret&lt;/code> is used to store the resources is that they could contain confidential information like credentials.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: managedresource-example1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type: Opaque
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> objects.yaml: YXBpVmVyc2lvbjogdjEKa2luZDogQ29uZmlnTWFwCm1ldGFkYXRhOgogIG5hbWU6IHRlc3QtMTIzNAogIG5hbWVzcGFjZTogZGVmYXVsdAotLS0KYXBpVmVyc2lvbjogdjEKa2luZDogQ29uZmlnTWFwCm1ldGFkYXRhOgogIG5hbWU6IHRlc3QtNTY3OAogIG5hbWVzcGFjZTogZGVmYXVsdAo=
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># apiVersion: v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># kind: ConfigMap&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># metadata:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># name: test-1234&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># namespace: default&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># ---&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># apiVersion: v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># kind: ConfigMap&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># metadata:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># name: test-5678&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># namespace: default&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: resources.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ManagedResource
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretRefs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: managedresource-example1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In the above example, the controller creates two &lt;code>ConfigMap&lt;/code>s in the &lt;code>default&lt;/code> namespace.
When a user is manually modifying them they will be reconciled back to the desired state stored in the &lt;code>managedresource-example&lt;/code> secret.&lt;/p>
&lt;p>It is also possible to inject labels into all the resources:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: managedresource-example2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type: Opaque
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> other-objects.yaml: YXBpVmVyc2lvbjogYXBwcy92MSAjIGZvciB2ZXJzaW9ucyBiZWZvcmUgMS45LjAgdXNlIGFwcHMvdjFiZXRhMgpraW5kOiBEZXBsb3ltZW50Cm1ldGFkYXRhOgogIG5hbWU6IG5naW54LWRlcGxveW1lbnQKc3BlYzoKICBzZWxlY3RvcjoKICAgIG1hdGNoTGFiZWxzOgogICAgICBhcHA6IG5naW54CiAgcmVwbGljYXM6IDIgIyB0ZWxscyBkZXBsb3ltZW50IHRvIHJ1biAyIHBvZHMgbWF0Y2hpbmcgdGhlIHRlbXBsYXRlCiAgdGVtcGxhdGU6CiAgICBtZXRhZGF0YToKICAgICAgbGFiZWxzOgogICAgICAgIGFwcDogbmdpbngKICAgIHNwZWM6CiAgICAgIGNvbnRhaW5lcnM6CiAgICAgIC0gbmFtZTogbmdpbngKICAgICAgICBpbWFnZTogbmdpbng6MS43LjkKICAgICAgICBwb3J0czoKICAgICAgICAtIGNvbnRhaW5lclBvcnQ6IDgwCg==
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># apiVersion: apps/v1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># kind: Deployment&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># metadata:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># name: nginx-deployment&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># spec:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># selector:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># matchLabels:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># app: nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># replicas: 2 # tells deployment to run 2 pods matching the template&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># template:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># metadata:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># labels:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># app: nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># spec:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># containers:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - name: nginx&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># image: nginx:1.7.9&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># ports:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - containerPort: 80&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: resources.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ManagedResource
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretRefs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: managedresource-example2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> injectLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> foo: bar
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In this example the label &lt;code>foo=bar&lt;/code> will be injected into the &lt;code>Deployment&lt;/code> as well as into all created &lt;code>ReplicaSet&lt;/code>s and &lt;code>Pod&lt;/code>s.&lt;/p>
&lt;h4 id="preventing-reconciliations">Preventing Reconciliations&lt;/h4>
&lt;p>If a ManagedResource is annotated with &lt;code>resources.gardener.cloud/ignore=true&lt;/code> then it will be skipped entirely by the controller (no reconciliations or deletions of managed resources at all).
However, when the ManagedResource itself is deleted (for example when a shoot is deleted) then the annotation is not respected and all resources will be deleted as usual.
This feature can be helpful to temporarily patch/change resources managed as part of such ManagedResource.&lt;/p>
&lt;h4 id="modes">Modes&lt;/h4>
&lt;p>The gardener-resource-manager can manage a resource in different modes. The supported modes are:&lt;/p>
&lt;ul>
&lt;li>&lt;code>Ignore&lt;/code>
&lt;ul>
&lt;li>The corresponding resource is removed from the ManagedResource status (&lt;code>.status.resources&lt;/code>). No action is performed on the cluster - the resource is no longer &amp;ldquo;managed&amp;rdquo; (updated or deleted).&lt;/li>
&lt;li>The primary use case is a migration of a resource from one ManagedResource to another one.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>The mode for a resource can be specified with the &lt;code>resources.gardener.cloud/mode&lt;/code> annotation. The annotation should be specified in the encoded resource manifest in the Secret that is referenced by the ManagedResource.&lt;/p>
&lt;h4 id="resource-class">Resource Class&lt;/h4>
&lt;p>By default, gardener-resource-manager controller watches for ManagedResources in all namespaces. &lt;code>--namespace&lt;/code> flag can be specified to gardener-resource-manager binary to restrict the watch to ManagedResources in a single namespace.
A ManagedResource has an optional &lt;code>.spec.class&lt;/code> field that allows to indicate that it belongs to given class of resources. &lt;code>--resource-class&lt;/code> flag can be specified to gardener-resource-manager binary to restrict the watch to ManagedResources with the given &lt;code>.spec.class&lt;/code>. A default class is assumed if no class is specified.&lt;/p>
&lt;h4 id="conditions">Conditions&lt;/h4>
&lt;p>A ManagedResource has a ManagedResourceStatus, which has an array of Conditions. Conditions currently include:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Condition&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>ResourcesApplied&lt;/code>&lt;/td>
&lt;td>&lt;code>True&lt;/code> if all resources are applied to the target cluster&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>ResourcesHealthy&lt;/code>&lt;/td>
&lt;td>&lt;code>True&lt;/code> if all resources are present and healthy&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;code>ResourcesApplied&lt;/code> may be &lt;code>False&lt;/code> when:&lt;/p>
&lt;ul>
&lt;li>the resource &lt;code>apiVersion&lt;/code> is not known to the target cluster&lt;/li>
&lt;li>the resource spec is invalid (for example the label value does not match the required regex for it)&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>&lt;code>ResourcesHealthy&lt;/code> may be &lt;code>False&lt;/code> when:&lt;/p>
&lt;ul>
&lt;li>the resource is not found&lt;/li>
&lt;li>the resource is a Deployment and the Deployment does not have the minimum availability.&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>Each Kubernetes resources has different notion for being healthy. For example, a Deployment is considered healthy if the controller observed its current revision and if the number of updated replicas is equal to the number of replicas.&lt;/p>
&lt;p>The following section describes a healthy ManagedResource:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&amp;#34;conditions&amp;#34;&lt;/span>&lt;span style="">:&lt;/span> [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;ResourcesApplied&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;status&amp;#34;: &lt;span style="color:#a31515">&amp;#34;True&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;reason&amp;#34;: &lt;span style="color:#a31515">&amp;#34;ApplySucceeded&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;message&amp;#34;: &lt;span style="color:#a31515">&amp;#34;All resources are applied.&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;lastUpdateTime&amp;#34;: &lt;span style="color:#a31515">&amp;#34;2019-09-09T11:31:21Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;lastTransitionTime&amp;#34;: &lt;span style="color:#a31515">&amp;#34;2019-09-08T19:53:23Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;ResourcesHealthy&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;status&amp;#34;: &lt;span style="color:#a31515">&amp;#34;True&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;reason&amp;#34;: &lt;span style="color:#a31515">&amp;#34;ResourcesHealthy&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;message&amp;#34;: &lt;span style="color:#a31515">&amp;#34;All resources are healthy.&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;lastUpdateTime&amp;#34;: &lt;span style="color:#a31515">&amp;#34;2019-09-09T11:31:21Z&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;lastTransitionTime&amp;#34;: &lt;span style="color:#a31515">&amp;#34;2019-09-09T11:31:21Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="ignoring-updates">Ignoring Updates&lt;/h4>
&lt;p>In some cases it is not desirable to update or re-apply some of the cluster components (for example, if customization is required or needs to be applied by the end-user).
For these resources, the annotation &amp;ldquo;resources.gardener.cloud/ignore&amp;rdquo; needs to be set to &amp;ldquo;true&amp;rdquo; or a truthy value (Truthy values are &amp;ldquo;1&amp;rdquo;, &amp;ldquo;t&amp;rdquo;, &amp;ldquo;T&amp;rdquo;, &amp;ldquo;true&amp;rdquo;, &amp;ldquo;TRUE&amp;rdquo;, &amp;ldquo;True&amp;rdquo;) in the corresponding managed resource secrets,
this can be done from the components that create the managed resource secrets, for example Gardener extensions or Gardener. Once this is done, the resource will be initially created and later ignored during reconciliation.&lt;/p>
&lt;h4 id="preserving-replicas-or-resources-in-workload-resources">Preserving &lt;code>replicas&lt;/code> or &lt;code>resources&lt;/code> in Workload Resources&lt;/h4>
&lt;p>The objects which are part of the &lt;code>ManagedResource&lt;/code> can be annotated with&lt;/p>
&lt;ul>
&lt;li>&lt;code>resources.gardener.cloud/preserve-replicas=true&lt;/code> in case the &lt;code>.spec.replicas&lt;/code> field of workload resources like &lt;code>Deployment&lt;/code>s, &lt;code>StatefulSet&lt;/code>s, etc. shall be preserved during updates.&lt;/li>
&lt;li>&lt;code>resources.gardener.cloud/preserve-resources=true&lt;/code> in case the &lt;code>.spec.containers[*].resources&lt;/code> fields of all containers of workload resources like &lt;code>Deployment&lt;/code>s, &lt;code>StatefulSet&lt;/code>s, etc. shall be preserved during updates.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>This can be useful if there are non-standard horizontal/vertical auto-scaling mechanisms in place.
Standard mechanisms like &lt;code>HorizontalPodAutoscaler&lt;/code> or &lt;code>VerticalPodAutoscaler&lt;/code> will be auto-recognized by &lt;code>gardener-resource-manager&lt;/code>, i.e., in such cases the annotations are not needed.&lt;/p>
&lt;/blockquote>
&lt;h4 id="origin">Origin&lt;/h4>
&lt;p>All the objects managed by the resource manager get a dedicated annotation
&lt;code>resources.gardener.cloud/origin&lt;/code> describing the &lt;code>ManagedResource&lt;/code> object that describes
this object.&lt;/p>
&lt;p>By default this is in this format &amp;lt;namespace&amp;gt;/&amp;lt;objectname&amp;gt;.
In multi-cluster scenarios (the &lt;code>ManagedResource&lt;/code> objects are maintained in a
cluster different from the one the described objects are managed), it might
be useful to include the cluster identity, as well.&lt;/p>
&lt;p>This can be enforced by setting the &lt;code>--cluster-id&lt;/code> option. Here, several
possibilities are supported:&lt;/p>
&lt;ul>
&lt;li>given a direct value: use this as id for the source cluster&lt;/li>
&lt;li>&lt;code>&amp;lt;cluster&amp;gt;&lt;/code>: read the cluster identity from a &lt;code>cluster-identity&lt;/code> config map
in the &lt;code>kube-system&lt;/code> namespace (attribute &lt;code>cluster-identity&lt;/code>). This is
automatically maintained in all clusters managed or involved in a gardener landscape.&lt;/li>
&lt;li>&lt;code>&amp;lt;default&amp;gt;&lt;/code>: try to read the cluster identity from the config map. If not found,
no identity is used&lt;/li>
&lt;li>empty string: no cluster identity is used (completely cluster local scenarios)&lt;/li>
&lt;/ul>
&lt;p>The format of the origin annotation with a cluster id is &amp;lt;cluster id&amp;gt;:&amp;lt;namespace&amp;gt;/&amp;lt;objectname&amp;gt;.&lt;/p>
&lt;p>The default for the cluster id is the empty value (do not use cluster id).&lt;/p>
&lt;h3 id="garbage-collector-for-immutable-configmapssecrets">Garbage Collector For Immutable &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s&lt;/h3>
&lt;p>In Kubernetes, workload resources (e.g., &lt;code>Pod&lt;/code>s) can mount &lt;code>ConfigMap&lt;/code>s or &lt;code>Secret&lt;/code>s or reference them via environment variables in containers.
Typically, when the content of such &lt;code>ConfigMap&lt;/code>/&lt;code>Secret&lt;/code> gets changed then the respective workload is usually not dynamically reloading the configuration, i.e., a restart is required.
The most commonly used approach is probably having so-called &lt;a href="https://helm.sh/docs/howto/charts_tips_and_tricks/#automatically-roll-deployments">checksum annotations in the pod template&lt;/a> which makes Kubernetes to recreate the pod if the checksum changes.
However, it has the downside that old, still running versions of the workload might not be able to properly work with the already updated content in the &lt;code>ConfigMap&lt;/code>/&lt;code>Secret&lt;/code>, potentially causing application outages.&lt;/p>
&lt;p>In order to protect users from such outages (and to also improve the performance of the cluster), the Kubernetes community provides the &lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/#configmap-immutable">&amp;ldquo;immutable &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s feature&amp;rdquo;&lt;/a>.
Enabling immutability requires &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s to have unique names.
Having unique names requires the client to delete &lt;code>ConfigMap&lt;/code>s&lt;code>/&lt;/code>Secret`s no longer in use.&lt;/p>
&lt;p>In order to provide a similarly lightweight experience for clients (compared to the well-established checksum annotation approach), the Gardener Resource Manager features an optional garbage collector controller (disabled by default).
The purpose of this controller is cleaning up such immutable &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s if they are no longer in use.&lt;/p>
&lt;h4 id="how-does-the-garbage-collector-work">How does the garbage collector work?&lt;/h4>
&lt;p>The following algorithm is implemented in the GC controller:&lt;/p>
&lt;ol>
&lt;li>List all &lt;code>ConfigMap&lt;/code>s and &lt;code>Secret&lt;/code>s labeled with &lt;code>resources.gardener.cloud/garbage-collectable-reference=true&lt;/code>.&lt;/li>
&lt;li>List all &lt;code>Deployment&lt;/code>s, &lt;code>StatefulSet&lt;/code>s, &lt;code>DaemonSet&lt;/code>s, &lt;code>Job&lt;/code>s, &lt;code>CronJob&lt;/code>s, &lt;code>Pod&lt;/code>s and for each of them
&lt;ol>
&lt;li>iterate over the &lt;code>.metadata.annotations&lt;/code> and for each of them
&lt;ol>
&lt;li>If the annotation key follows the &lt;code>reference.resources.gardener.cloud/{configmap,secret}-&amp;lt;hash&amp;gt;&lt;/code> scheme and the value equals &lt;code>&amp;lt;name&amp;gt;&lt;/code> then consider it as &amp;ldquo;in-use&amp;rdquo;.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Delete all &lt;code>ConfigMap&lt;/code>s and &lt;code>Secret&lt;/code>s not considered as &amp;ldquo;in-use&amp;rdquo;.&lt;/li>
&lt;/ol>
&lt;p>Consequently, clients need to&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Create immutable &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s with unique names (e.g., a checksum suffix based on the &lt;code>.data&lt;/code>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Label such &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s with &lt;code>resources.gardener.cloud/garbage-collectable-reference=true&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Annotate their workload resources with &lt;code>reference.resources.gardener.cloud/{configmap,secret}-&amp;lt;hash&amp;gt;=&amp;lt;name&amp;gt;&lt;/code> for all &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s used by the containers of the respective &lt;code>Pod&lt;/code>s.&lt;/p>
&lt;p>⚠️ Add such annotations to &lt;code>.metadata.annotations&lt;/code> as well as to all templates of other resources (e.g., &lt;code>.spec.template.metadata.annotations&lt;/code> in &lt;code>Deployment&lt;/code>s or &lt;code>.spec.jobTemplate.metadata.annotations&lt;/code> and &lt;code>.spec.jobTemplate.spec.template.metadata.annotations&lt;/code> for &lt;code>CronJob&lt;/code>s.
This ensures that the GC controller does not unintentionally consider &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s as &amp;ldquo;not in use&amp;rdquo; just because there isn&amp;rsquo;t a &lt;code>Pod&lt;/code> referencing them anymore (e.g., they could still be used by a &lt;code>Deployment&lt;/code> scaled down to &lt;code>0&lt;/code>).&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>ℹ️ For the last step, there is a helper function &lt;code>InjectAnnotations&lt;/code> in the &lt;code>pkg/controller/garbagecollector/references&lt;/code> which you can use for your convenience.&lt;/p>
&lt;p>&lt;strong>Example:&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ConfigMap
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: test-1234
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources.gardener.cloud/garbage-collectable-reference: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ConfigMap
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: test-5678
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources.gardener.cloud/garbage-collectable-reference: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reference.resources.gardener.cloud/configmap-82a3537f: test-5678
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> image: nginx:1.14.2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> terminationGracePeriodSeconds: 2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The GC controller would delete the &lt;code>ConfigMap/test-1234&lt;/code> because it is considered as not &amp;ldquo;in-use&amp;rdquo;.&lt;/p>
&lt;p>ℹ️ If the GC controller is activated then the &lt;code>ManagedResource&lt;/code> controller will no longer delete &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s having the above label.&lt;/p>
&lt;h4 id="how-to-activate-the-garbage-collector">How to activate the garbage collector?&lt;/h4>
&lt;p>The GC controller can be activated by providing the &lt;code>--garbage-collector-sync-period&lt;/code> flag with a value larger than &lt;code>0&lt;/code> (e.g., &lt;code>1h&lt;/code>) to the Gardener Resource Manager.&lt;/p>
&lt;h3 id="tokeninvalidator">TokenInvalidator&lt;/h3>
&lt;p>The Kubernetes community is slowly transitioning from static &lt;code>ServiceAccount&lt;/code> token &lt;code>Secret&lt;/code>s to &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection">&lt;code>ServiceAccount&lt;/code> Token Volume Projection&lt;/a>.
Typically, when you create a &lt;code>ServiceAccount&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ServiceAccount
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: default
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>then the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/serviceaccount/tokens_controller.go">&lt;code>serviceaccount-token&lt;/code>&lt;/a> controller (part of &lt;code>kube-controller-manager&lt;/code>) auto-generates a &lt;code>Secret&lt;/code> with a static token:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes.io/service-account.name: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes.io/service-account.uid: 86e98645-2e05-11e9-863a-b2d4d086dd5a)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: default-token-ntxs9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type: kubernetes.io/service-account-token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ca.crt: base64(cluster-ca-cert)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: base64(namespace)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> token: base64(static-jwt-token)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Unfortunately, when using &lt;code>ServiceAccount&lt;/code> Token Volume Projection in a &lt;code>Pod&lt;/code>, this static token is actually not used at all:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> serviceAccountName: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - image: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumeMounts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - mountPath: /var/run/secrets/tokens
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> projected:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - serviceAccountToken:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expirationSeconds: 7200
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>While the &lt;code>Pod&lt;/code> is now using an expiring and auto-rotated token, the static token is still generated and valid.&lt;/p>
&lt;p>As of Kubernetes v1.22, there is neither a way of preventing &lt;code>kube-controller-manager&lt;/code> to generate such static tokens, nor a way to proactively remove or invalidate them:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/77599">https://github.com/kubernetes/kubernetes/issues/77599&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/77600">https://github.com/kubernetes/kubernetes/issues/77600&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Disabling the &lt;code>serviceaccount-token&lt;/code> controller is an option, however, especially in the Gardener context it may either break end-users or it may not even be possible to control such settings.
Also, even if a future Kubernetes version supports native configuration of above behaviour, Gardener still supports older versions which won&amp;rsquo;t get such features but need a solution as well.&lt;/p>
&lt;p>This is where the &lt;em>TokenInvalidator&lt;/em> comes into play:
Since it is not possible to prevent &lt;code>kube-controller-manager&lt;/code> from generating static &lt;code>ServiceAccount&lt;/code> &lt;code>Secret&lt;/code>s, the &lt;em>TokenInvalidator&lt;/em> is - as its name suggests - just invalidating these tokens.
It considers all such &lt;code>Secret&lt;/code>s belonging to &lt;code>ServiceAccount&lt;/code>s with &lt;code>.automountServiceAccountToken=false&lt;/code>.
By default, all namespaces in the target cluster are watched, however, this can be configured by specifying the &lt;code>--target-namespace&lt;/code> flag.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ServiceAccount
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-serviceaccount
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>automountServiceAccountToken: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will result in a static &lt;code>ServiceAccount&lt;/code> token secret whose &lt;code>token&lt;/code> value is invalid:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes.io/service-account.name: my-serviceaccount
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes.io/service-account.uid: 86e98645-2e05-11e9-863a-b2d4d086dd5a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-serviceaccount-token-ntxs9
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type: kubernetes.io/service-account-token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ca.crt: base64(cluster-ca-cert)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: base64(namespace)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> token: AAAA
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Any attempt to regenerate the token or creating a new such secret will again make the component invalidating it.&lt;/p>
&lt;blockquote>
&lt;p>You can opt-out of this behaviour for &lt;code>ServiceAccount&lt;/code>s setting &lt;code>.automountServiceAccountToken=false&lt;/code> by labeling them with &lt;code>token-invalidator.resources.gardener.cloud/skip=true&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>In order to enable the &lt;em>TokenInvalidator&lt;/em> you have to set &lt;code>--token-invalidator-max-concurrent-workers&lt;/code> to a value larger than &lt;code>0&lt;/code>.&lt;/p>
&lt;p>Below graphic shows an overview of the Token Invalidator for Service account secrets in the Shoot cluster.
&lt;img src="https://gardener.cloud/__resources/resource-manager-token-invalidator_b43fa2.jpg" alt="image">&lt;/p>
&lt;h3 id="tokenrequestor">TokenRequestor&lt;/h3>
&lt;p>This controller provides the service to create and auto-renew tokens via the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-request-v1/">&lt;code>TokenRequest&lt;/code> API&lt;/a>.&lt;/p>
&lt;p>It provides a functionality similar to the kubelet&amp;rsquo;s &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection">Service Account Token Volume Projection&lt;/a>.
It was created to handle the special case of issuing tokens to pods that run in a different cluster than the API server they communicate with (hence, using the native token volume projection feature is not possible).&lt;/p>
&lt;p>The controller differentiates between &lt;code>source cluster&lt;/code> and &lt;code>target cluster&lt;/code>.
The &lt;code>source cluster&lt;/code> hosts the gardener-resource-manager pod. Secrets in this cluster are watched and modified by the controller.
The &lt;code>target cluster&lt;/code> &lt;em>can&lt;/em> be configured to point to another cluster. The existence of ServiceAccounts are ensured and token requests are issued against the target.
When the gardener-resource-manager is deployed next to the Shoot&amp;rsquo;s controlplane in the Seed the &lt;code>source cluster&lt;/code> is the Seed while the &lt;code>target cluster&lt;/code> points to the Shoot.&lt;/p>
&lt;h4 id="reconciliation-loop">Reconciliation Loop&lt;/h4>
&lt;p>This controller reconciles secrets in all namespaces in the source cluster with the label: &lt;code>resources.gardener.cloud/purpose: token-requestor&lt;/code>.
See &lt;a href="https://github.com/gardener/gardener/blob/master/example/resource-manager/30-secret-tokenrequestor.yaml">here&lt;/a> for an example of the secret.&lt;/p>
&lt;p>The controller ensures a &lt;code>ServiceAccount&lt;/code> exists in the target cluster as specified in the annotations of the &lt;code>Secret&lt;/code> in the source cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>serviceaccount.resources.gardener.cloud/name: &amp;lt;sa-name&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>serviceaccount.resources.gardener.cloud/namespace: &amp;lt;sa-namespace&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The requested tokens will act with the privileges which are assigned to this &lt;code>ServiceAccount&lt;/code>.&lt;/p>
&lt;p>The controller will then request a token via the &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-request-v1/">&lt;code>TokenRequest&lt;/code> API&lt;/a> and populate it into the &lt;code>.data.token&lt;/code> field to the &lt;code>Secret&lt;/code> in the source cluster.&lt;/p>
&lt;p>Alternatively, the client can provide a raw kubeconfig (in YAML or JSON format) via the &lt;code>Secret&lt;/code>&amp;rsquo;s &lt;code>.data.kubeconfig&lt;/code> field.
The controller will then populate the requested token in the kubeconfig for the user used in the &lt;code>.current-context&lt;/code>.
For example, if &lt;code>.data.kubeconfig&lt;/code> is&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusters:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- cluster:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> certificate-authority-data: AAAA
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: some-server-url
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: shoot--foo--bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: shoot--foo--bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: shoot--foo--bar-token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: shoot--foo--bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>current-context: shoot--foo--bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>preferences: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>users:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: shoot--foo--bar-token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> token: &lt;span style="color:#a31515">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>then the &lt;code>.users[0].user.token&lt;/code> field of the kubeconfig will be updated accordingly.&lt;/p>
&lt;p>The controller also adds an annotation to the &lt;code>Secret&lt;/code> to keep track when to renew the token before it expires.
By default, the tokens are issued to expire after 12 hours. The expiration time can be set with the following annotation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>serviceaccount.resources.gardener.cloud/token-expiration-duration: 6h
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It automatically renews once 80% of the lifetime is reached or after &lt;code>24h&lt;/code>.&lt;/p>
&lt;p>Optionally, the controller can also populate the token into a &lt;code>Secret&lt;/code> in the target cluster. This can be requested by annotating the &lt;code>Secret&lt;/code> in the source cluster with&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>token-requestor.resources.gardener.cloud/target-secret-name: &lt;span style="color:#a31515">&amp;#34;foo&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>token-requestor.resources.gardener.cloud/target-secret-namespace: &lt;span style="color:#a31515">&amp;#34;bar&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Overall, the TokenRequestor controller provides credentials with limited lifetime (JWT tokens) used by Shoot control plane components running in the Seed
to talk to the Shoot API Server.
Please see the graphic below:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/resource-manager-projected-token-controlplane-to-shoot-apiserver_da4cda.jpg" alt="image">&lt;/p>
&lt;h2 id="webhooks">Webhooks&lt;/h2>
&lt;h3 id="auto-mounting-projected-serviceaccount-tokens">Auto-Mounting Projected &lt;code>ServiceAccount&lt;/code> Tokens&lt;/h3>
&lt;p>When this webhook is activated then it automatically injects projected &lt;code>ServiceAccount&lt;/code> token volumes into &lt;code>Pod&lt;/code>s and all its containers if all of the following preconditions are fulfilled:&lt;/p>
&lt;ol>
&lt;li>The &lt;code>Pod&lt;/code> is NOT labeled with &lt;code>projected-token-mount.resources.gardener.cloud/skip=true&lt;/code>.&lt;/li>
&lt;li>The &lt;code>Pod&lt;/code>&amp;rsquo;s &lt;code>.spec.serviceAccountName&lt;/code> field is NOT empty and NOT set to &lt;code>default&lt;/code>.&lt;/li>
&lt;li>The &lt;code>ServiceAccount&lt;/code> specified in the &lt;code>Pod&lt;/code>&amp;rsquo;s &lt;code>.spec.serviceAccountName&lt;/code> sets &lt;code>.automountServiceAccountToken=false&lt;/code>.&lt;/li>
&lt;li>The &lt;code>Pod&lt;/code>&amp;rsquo;s &lt;code>.spec.volumes[]&lt;/code> DO NOT already contain a volume with a name prefixed with &lt;code>kube-api-access-&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>The projected volume will look as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: kube-api-access-gardener
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> projected:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> defaultMode: 420
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - serviceAccountToken:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expirationSeconds: 43200
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - configMap:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> items:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: ca.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: ca.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: kube-root-ca.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - downwardAPI:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> items:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - fieldRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fieldPath: metadata.namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: namespace
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>The &lt;code>expirationSeconds&lt;/code> are defaulted to &lt;code>12h&lt;/code> and can be overwritten with the &lt;code>--projected-token-mount-expiration-seconds&lt;/code> flag, or with the &lt;code>projected-token-mount.resources.gardener.cloud/expiration-seconds&lt;/code> annotation on a &lt;code>Pod&lt;/code> resource.&lt;/p>
&lt;/blockquote>
&lt;p>The volume will be mounted into all containers specified in the &lt;code>Pod&lt;/code> to the path &lt;code>/var/run/secrets/kubernetes.io/serviceaccount&lt;/code>.
This is the default location where client libraries expect to find the tokens and mimics the &lt;a href="https://github.com/kubernetes/kubernetes/tree/v1.22.2/plugin/pkg/admission/serviceaccount">upstream &lt;code>ServiceAccount&lt;/code> admission plugin&lt;/a>, see &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#serviceaccount-admission-controller">this document&lt;/a> for more information.&lt;/p>
&lt;p>Overall, this webhook is used to inject projected service account tokens into pods running in the Shoot and the Seed cluster.
Hence, it is served from the Seed GRM and each Shoot GRM.
Please find an overview below for pods deployed in the Shoot cluster:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/resource-manager-projected-token-shoot-to-shoot-apiserver_4fdaf3.jpg" alt="image">&lt;/p></description></item><item><title>Docs: Scheduler</title><link>https://gardener.cloud/docs/gardener/concepts/scheduler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/scheduler/</guid><description>
&lt;h1 id="gardener-scheduler">Gardener Scheduler&lt;/h1>
&lt;p>The Gardener Scheduler is in essence a controller that watches newly created shoots and assigns a seed cluster to them.
Conceptually, the task of the Gardener Scheduler is very similar to the task of the Kubernetes Scheduler: finding a seed for a shoot instead of a node for a pod.&lt;/p>
&lt;p>Either the scheduling strategy or the shoot cluster purpose hereby determines how the scheduler is operating.
The following sections explain the configuration and flow in greater detail.&lt;/p>
&lt;h2 id="why-is-the-gardener-scheduler-needed">Why is the Gardener Scheduler needed?&lt;/h2>
&lt;h3 id="1-decoupling">1. Decoupling&lt;/h3>
&lt;p>Previously, an admission plugin in the Gardener API server conducted the scheduling decisions.
This implies changes to the API server whenever adjustments of the scheduling are needed.
Decoupling the API server and the scheduler comes with greater flexibility to develop these components independently from each other.&lt;/p>
&lt;h3 id="2-extensibility">2. Extensibility&lt;/h3>
&lt;p>It should be possible to easily extend and tweak the scheduler in the future.
Possibly, similar to the Kubernetes scheduler, hooks could be provided which influence the scheduling decisions.
It should be also possible to completely replace the standard Gardener Scheduler with a custom implementation.&lt;/p>
&lt;h2 id="algorithm-overview">Algorithm overview&lt;/h2>
&lt;p>The following &lt;strong>sequence&lt;/strong> describes the steps involved to determine a seed candidate:&lt;/p>
&lt;ol>
&lt;li>Determine usable seeds with &amp;ldquo;usable&amp;rdquo; defined as follows:
&lt;ul>
&lt;li>no &lt;code>.metadata.deletionTimestamp&lt;/code>&lt;/li>
&lt;li>&lt;code>.spec.settings.scheduling.visible&lt;/code> is &lt;code>true&lt;/code>&lt;/li>
&lt;li>conditions &lt;code>Bootstrapped&lt;/code>, &lt;code>GardenletReady&lt;/code>, &lt;code>BackupBucketsReady&lt;/code> (if available) are &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Filter seeds:
&lt;ul>
&lt;li>matching &lt;code>.spec.seedSelector&lt;/code> in &lt;code>CloudProfile&lt;/code> used by the &lt;code>Shoot&lt;/code>&lt;/li>
&lt;li>matching &lt;code>.spec.seedSelector&lt;/code> in &lt;code>Shoot&lt;/code>&lt;/li>
&lt;li>having no network intersection with the &lt;code>Shoot&lt;/code>&amp;rsquo;s networks (due to the VPN connectivity between seeds and shoots their networks must be disjoint)&lt;/li>
&lt;li>having &lt;code>.spec.settings.shootDNS.enabled=false&lt;/code> (only if the shoot specifies a DNS domain or does not use the &lt;code>unmanaged&lt;/code> DNS provider)&lt;/li>
&lt;li>whose taints (&lt;code>.spec.taints&lt;/code>) are tolerated by the &lt;code>Shoot&lt;/code> (&lt;code>.spec.tolerations&lt;/code>)&lt;/li>
&lt;li>whose capacity for shoots would not be exceeded if the shoot is scheduled onto the seed, see &lt;a href="#ensuring-seeds-capacity-for-shoots-is-not-exceeded">Ensuring seeds capacity for shoots is not exceeded&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Apply active &lt;a href="#strategies">strategy&lt;/a> e.g., &lt;em>Minimal Distance strategy&lt;/em>&lt;/li>
&lt;li>Choose least utilized seed, i.e., the one with the least number of shoot control planes, will be the winner and written to the &lt;code>.spec.seedName&lt;/code> field of the &lt;code>Shoot&lt;/code>.&lt;/li>
&lt;/ol>
&lt;h2 id="configuration">Configuration&lt;/h2>
&lt;p>The Gardener Scheduler configuration has to be supplied on startup. It is a mandatory and also the only available flag.
&lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-scheduler.yaml">Here&lt;/a> is an example scheduler configuration.&lt;/p>
&lt;p>Most of the configuration options are the same as in the Gardener Controller Manager (leader election, client connection, &amp;hellip;).
However, the Gardener Scheduler on the other hand does not need a TLS configuration, because there are currently no webhooks configurable.&lt;/p>
&lt;h2 id="strategies">Strategies&lt;/h2>
&lt;p>The scheduling strategy is defined in the &lt;em>&lt;strong>candidateDeterminationStrategy&lt;/strong>&lt;/em> of the scheduler&amp;rsquo;s configuration and can have the possible values &lt;code>SameRegion&lt;/code> and &lt;code>MinimalDistance&lt;/code>.
The &lt;code>SameRegion&lt;/code> strategy is the default strategy.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;em>Same Region strategy&lt;/em>&lt;/p>
&lt;p>The Gardener Scheduler reads the &lt;code>spec.provider.type&lt;/code> and &lt;code>.spec.region&lt;/code> fields from the &lt;code>Shoot&lt;/code> resource.
It tries to find a seed that has the identical &lt;code>.spec.provider.type&lt;/code> and &lt;code>.spec.provider.region&lt;/code> fields set.
If it cannot find a suitable seed, it adds an event to the shoot stating, that it is unschedulable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Minimal Distance strategy&lt;/em>&lt;/p>
&lt;p>The Gardener Scheduler tries to find a valid seed with minimal distance to the shoot&amp;rsquo;s intended region.
The distance is calculated based on the Levenshtein distance of the region. Therefore the region name
is split into a base name and an orientation. Possible orientations are &lt;code>north&lt;/code>, &lt;code>south&lt;/code>, &lt;code>east&lt;/code>, &lt;code>west&lt;/code> and &lt;code>central&lt;/code>.
The distance then is twice the Levenshtein distance of the region&amp;rsquo;s base name plus a correction value based on the
orientation and the provider.&lt;/p>
&lt;p>If the orientations of shoot and seed candidate match, the correction value is 0, if they differ it is 2 and if
either the seed&amp;rsquo;s or the shoot&amp;rsquo;s region does not have an orientation it is 1.
If the provider differs the correction value is additionally incremented by 2.&lt;/p>
&lt;p>Because of this a matching region with a matching provider is always prefered.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>In order to put the scheduling decision into effect, the scheduler sends an update request for the &lt;code>Shoot&lt;/code> resource to
the API server. After validation, the Gardener Aggregated API server updates the shoot to have the &lt;code>spec.seedName&lt;/code> field set.
Subsequently, the Gardenlet picks up and starts to create the cluster on the specified seed.&lt;/p>
&lt;ol start="3">
&lt;li>&lt;em>Special handling based on shoot cluster purpose&lt;/em>&lt;/li>
&lt;/ol>
&lt;p>Every shoot cluster can have a purpose that describes what the cluster is used for, and also influences how the cluster is setup (see &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_purposes/">this document&lt;/a> for more information).&lt;/p>
&lt;p>In case the shoot has the &lt;code>testing&lt;/code> purpose then the scheduler only reads the &lt;code>.spec.provider.type&lt;/code> from the &lt;code>Shoot&lt;/code> resource and tries to find a &lt;code>Seed&lt;/code> that has the identical &lt;code>.spec.provider.type&lt;/code>.
The region does not matter, i.e., &lt;code>testing&lt;/code> shoots may also be scheduled on a seed in a complete different region if it is better for balancing the whole Gardener system.&lt;/p>
&lt;h2 id="seedselector-field-in-the-shoot-specification">&lt;code>seedSelector&lt;/code> field in the &lt;code>Shoot&lt;/code> specification&lt;/h2>
&lt;p>Similar to the &lt;code>.spec.nodeSelector&lt;/code> field in &lt;code>Pod&lt;/code>s, the &lt;code>Shoot&lt;/code> specification has an optional &lt;code>.spec.seedSelector&lt;/code> field.
It allows the user to provide a label selector that must match the labels of &lt;code>Seed&lt;/code>s in order to be scheduled to one of them.
The labels on &lt;code>Seed&lt;/code>s are usually controlled by Gardener administrators/operators - end users cannot add arbitrary labels themselves.
If provided, the Gardener Scheduler will only consider those seeds as &amp;ldquo;suitable&amp;rdquo; whose labels match those provided in the &lt;code>.spec.seedSelector&lt;/code> of the &lt;code>Shoot&lt;/code>.&lt;/p>
&lt;p>By default only seeds with the same provider than the shoot are selected. By adding a &lt;code>providerTypes&lt;/code> field to the &lt;code>seedSelector&lt;/code>
a dedicated set of possible providers (&lt;code>*&lt;/code> means all provider types) can be selected.&lt;/p>
&lt;h2 id="ensuring-seeds-capacity-for-shoots-is-not-exceeded">Ensuring seeds capacity for shoots is not exceeded&lt;/h2>
&lt;p>Seeds have a practical limit of how many shoots they can accommodate. Exceeding this limit is undesirable as the system performance will be noticeably impacted. Therefore, the scheduler ensures that a seed&amp;rsquo;s capacity for shoots is not exceeded by taking into account a maximum number of shoots that can be scheduled onto a seed.&lt;/p>
&lt;p>This mechanism works as follows:&lt;/p>
&lt;ul>
&lt;li>The &lt;code>gardenlet&lt;/code> is configured with certain &lt;em>resources&lt;/em> and their total &lt;em>capacity&lt;/em> (and, for certain resources, the amount &lt;em>reserved&lt;/em> for Gardener), see &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml">/example/20-componentconfig-gardenlet.yaml&lt;/a>. Currently, the only such resource is the maximum number of shoots that can be scheduled onto a seed.&lt;/li>
&lt;li>The &lt;code>gardenlet&lt;/code> seed controller updates the &lt;code>capacity&lt;/code> and &lt;code>allocatable&lt;/code> fields in Seed status with the capacity of each resource and how much of it is actually available to be consumed by shoots. The &lt;code>allocatable&lt;/code> value of a resource is equal to &lt;code>capacity&lt;/code> minus &lt;code>reserved&lt;/code>.&lt;/li>
&lt;li>When scheduling shoots, the scheduler filters out all candidate seeds whose allocatable capacity for shoots would be exceeded if the shoot is scheduled onto the seed.&lt;/li>
&lt;/ul>
&lt;h2 id="failure-to-determine-a-suitable-seed">Failure to determine a suitable seed&lt;/h2>
&lt;p>In case the scheduler fails to find a suitable seed, the operation is being retried with exponential backoff.&lt;/p>
&lt;h2 id="current-limitation--future-plans">Current Limitation / Future Plans&lt;/h2>
&lt;ul>
&lt;li>Azure has unfortunately a geographically non-hierarchical naming pattern and does not start with the continent. This is the reason why we will exchange the implementation of the &lt;code>MinimalDistance&lt;/code> strategy with a more suitable one in the future.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Secret Binding Provider Controller</title><link>https://gardener.cloud/docs/gardener/deployment/secret_binding_provider_controller/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/secret_binding_provider_controller/</guid><description>
&lt;h1 id="secretbinding-provider-controller">SecretBinding Provider Controller&lt;/h1>
&lt;p>This page describes the process on how to enable the SecretBinding provider controller.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>With Gardener v1.38.0 the SecretBinding resource does now contain a new optional field &lt;code>.provider.type&lt;/code> (details about the motivation can be found in &lt;a href="https://github.com/gardener/gardener/issues/4888)">https://github.com/gardener/gardener/issues/4888)&lt;/a>. To make the process of setting the new field automated and afterwards to enforce validation on the new field in backwards compatible manner, Gardener features the SecretBinding provider controller and a feature gate - &lt;code>SecretBindingProviderValidation&lt;/code>.&lt;/p>
&lt;h2 id="process">Process&lt;/h2>
&lt;p>A Gardener landscape operator can follow the following steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Enable the SecretBinding provider controller of Gardener Controller Manager.&lt;/p>
&lt;p>The SecretBinding provider controller is responsible to populate the &lt;code>.provider.type&lt;/code> field of a SecretBinding based on its current usage by Shoot resources. For example if a Shoot &lt;code>crazy-botany&lt;/code> with &lt;code>.provider.type=aws&lt;/code> is using a SecretBinding &lt;code>my-secret-binding&lt;/code>, then the SecretBinding provider controller will take care to set the &lt;code>.provider.type&lt;/code> field of the SecretBinding to the same provider type (&lt;code>aws&lt;/code>).
To enable the SecretBinding provider controller, in the ControllerManagerConfiguration set the &lt;code>controller.secretBindingProvider.concurentSyncs&lt;/code> field (e.g set it to &lt;code>5&lt;/code>).
Although that it is not recommended, the API allows Shoots from different provider types to reference the same SecretBinding (assuming that backing Secret contains data for both of the provider types). To preserve the backwards compatibility for such SecretBindings, the provider controller will maintain the multiple provider types in the field (it will join them with separator &lt;code>,&lt;/code> - for example &lt;code>aws,gcp&lt;/code>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Disable the SecretBinding provider controller and enable &lt;code>SecretBindingProviderValidation&lt;/code> feature gate of Gardener API server.&lt;/p>
&lt;p>The &lt;code>SecretBindingProviderValidation&lt;/code> feature gate of Gardener API server enables set of validations for the SecretBinding provider field. It forbids creating a Shoot that has a different provider type from the referenced SecretBinding&amp;rsquo;s one. It also enforces immutability on the field.
After making sure that SecretBinding provider controller is enabled and it populated the &lt;code>.provider.type&lt;/code> field of a majority of the SecretBindings on a Gardener landscape (the SecretBindings that are unused will have their provider type unset), a Gardener landscape operator has to disable the SecretBinding provider controller and to enable the &lt;code>SecretBindingProviderValidation&lt;/code> feature gate of Gardener API server. To disable the SecretBinding provider controller, in the ControllerManagerConfiguration set the &lt;code>controller.secretBindingProvider.concurentSyncs&lt;/code> field to &lt;code>0&lt;/code>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="implementation-history">Implementation History&lt;/h2>
&lt;ul>
&lt;li>Gardener v1.38: SecretBinding resource has a new optional field &lt;code>.provider.type&lt;/code>. SecretBinding provider controller is disabled by default. &lt;code>SecretBindingProviderValidation&lt;/code> feature gate of Gardener API server is disabled by default.&lt;/li>
&lt;li>Gardener v1.42: SecretBinding provider controller is enabled by default.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Secrets Management</title><link>https://gardener.cloud/docs/gardener/development/secrets_management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/secrets_management/</guid><description>
&lt;h1 id="secrets-management-for-seed-and-shoot-cluster">Secrets Management for Seed and Shoot Cluster&lt;/h1>
&lt;blockquote>
&lt;p>🚧️ Please note that the work in the new secrets management is ongoing and hence not yet completed.
Accordingly, expect adaptations to this document and implementation details.&lt;/p>
&lt;/blockquote>
&lt;p>The gardenlet needs to create quite some amount of credentials (certificates, private keys, passwords, etc.) for seed and shoot clusters in order to ensure secure deployments.
Such credentials typically should be renewed automatically when their validity expires, rotated regularly, and they potentially need to be persisted such that they don&amp;rsquo;t get lost in case of a control plane migration or a lost seed cluster.&lt;/p>
&lt;h2 id="secretsmanager-introduction">SecretsManager Introduction&lt;/h2>
&lt;p>These requirements can be covered by using the &lt;code>SecretsManager&lt;/code> package maintained in &lt;a href="https://github.com/gardener/gardener/tree/master/docs/development/pkg/utils/secrets/manager">&lt;code>pkg/utils/secrets/manager&lt;/code>&lt;/a>.
It is built on top of the &lt;code>ConfigInterface&lt;/code> and &lt;code>DataInterface&lt;/code> interfaces part of &lt;a href="https://github.com/gardener/gardener/tree/master/docs/development/pkg/utils/secrets">&lt;code>pkg/utils/secrets&lt;/code>&lt;/a> and provides the following functions:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>Generate(context.Context, secrets.ConfigInterface, ...GenerateOption) (*corev1.Secret, error)&lt;/code>&lt;/p>
&lt;p>This method either retrieves the current secret for the given configuration or it (re)generates it in case the configuration changed, the signing CA changed (for certificate secrets), or when proactive rotation was triggered.
If the configuration describes a certificate authority secret then this method automatically generates a bundle secret containing the current and potentially the old certificate.&lt;br>
Available &lt;code>GenerateOption&lt;/code>s:&lt;/p>
&lt;ul>
&lt;li>&lt;code>SignedByCA(string, ...SignedByCAOption)&lt;/code>: This is only valid for certificate secrets and automatically retrieves the correct certificate authority in order to sign the provided server or client certificate.
&lt;ul>
&lt;li>There is only one &lt;code>SignedByCAOption&lt;/code>: &lt;code>UseCurrentCA&lt;/code>. This option will sign server certificates with the new/current CA in case of a CA rotation. For more information, please refer to the &lt;a href="#certificate-signing">&amp;ldquo;Certificate Signing&amp;rdquo;&lt;/a> section below).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Persist()&lt;/code>: This marks the secret such that it gets persisted in the &lt;code>ShootState&lt;/code> resource in the garden cluster. Consequently, it should only be used for secrets related to a shoot cluster.&lt;/li>
&lt;li>&lt;code>Rotate(rotationStrategy)&lt;/code>: This specifies the strategy in case this secret is to be rotated or regenerated (either &lt;code>InPlace&lt;/code> which immediately forgets about the old secret, or &lt;code>KeepOld&lt;/code> which keeps the old secret in the system).&lt;/li>
&lt;li>&lt;code>IgnoreOldSecrets()&lt;/code>: This specifies whether old secrets should be considered and loaded (which is done by default). It should be used when old secrets are no longer important and can be &amp;ldquo;forgotten&amp;rdquo; (e.g. in &lt;a href="https://gardener.cloud/docs/gardener/proposals/18-shoot-ca-rotation/#rotation-sequence-for-cluster-and-client-ca">&amp;ldquo;phase 2&amp;rdquo; (&lt;code>t2&lt;/code>) of the CA certificate rotation&lt;/a>).&lt;/li>
&lt;li>&lt;code>Validity(time.Duration)&lt;/code>: This specifies how long the secret should be valid. For certificate secret configurations, the manager will automatically deduce this information from the generated certificate.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Get(string, ...GetOption) (*corev1.Secret, bool)&lt;/code>&lt;/p>
&lt;p>This method retrieves the current secret for the given name.
In case the secret in question is a certificate authority secret then it retrieves the bundle secret by default.
It is important that this method only knows about secrets for which there were prior &lt;code>Generate&lt;/code> calls.&lt;br>
Available &lt;code>GetOption&lt;/code>s:&lt;/p>
&lt;ul>
&lt;li>&lt;code>Bundle&lt;/code> (default): This retrieves the bundle secret.&lt;/li>
&lt;li>&lt;code>Current&lt;/code>: This retrieves the current secret.&lt;/li>
&lt;li>&lt;code>Old&lt;/code>: This retrieves the old secret.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Cleanup(context.Context) error&lt;/code>&lt;/p>
&lt;p>This method deletes secrets which are no longer required.
No longer required secrets are those still existing in the system which weren&amp;rsquo;t detected by prior &lt;code>Generate&lt;/code> calls.
Consequently, only call &lt;code>Cleanup&lt;/code> after you have executed &lt;code>Generate&lt;/code> calls for all desired secrets.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Some exemplary usages would look as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>secret, err := k.secretsManager.Generate(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;amp;secrets.CertificateSecretConfig{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Name: &lt;span style="color:#a31515">&amp;#34;my-server-secret&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> CommonName: &lt;span style="color:#a31515">&amp;#34;server-abc&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> DNSNames: []&lt;span style="color:#2b91af">string&lt;/span>{&lt;span style="color:#a31515">&amp;#34;first-name&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;second-name&amp;#34;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> CertType: secrets.ServerCert,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> SkipPublishingCACertificate: &lt;span style="color:#00f">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretsmanager.SignedByCA(&lt;span style="color:#a31515">&amp;#34;my-ca&amp;#34;&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretsmanager.Persist(),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretsmanager.Rotate(secretsmanager.InPlace),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">if&lt;/span> err != &lt;span style="color:#00f">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00f">return&lt;/span> err
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As explained above, the caller does not need to care about the renewal, rotation or the persistence of this secret - all of these concerns are handled by the secrets manager.
Automatic renewal of secrets happens when their validity approaches 80% or less than &lt;code>10d&lt;/code> are left until expiration.&lt;/p>
&lt;p>In case a CA certificate is needed by some component then it can be retrieved as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>caSecret, found := k.secretsManager.Get(&lt;span style="color:#a31515">&amp;#34;my-ca&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">if&lt;/span> !found {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00f">return&lt;/span> fmt.Errorf(&lt;span style="color:#a31515">&amp;#34;secret my-ca not found&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As explained above, this returns the bundle secret for the CA &lt;code>my-ca&lt;/code> which might potentially contain both the current and the old CA (in case of rotation/regeneration).&lt;/p>
&lt;h3 id="certificate-signing">Certificate Signing&lt;/h3>
&lt;p>By default, client certificates are always signed by the current CA while server certificate are signed by the old CA (if it exists).
This is to ensure a smooth exchange of certificate during a CA rotation (typically has two phases, ref &lt;a href="https://gardener.cloud/docs/gardener/proposals/18-shoot-ca-rotation/#rotation-sequence-for-cluster-and-client-ca">GEP-18&lt;/a>):&lt;/p>
&lt;ul>
&lt;li>Client certificates:
&lt;ul>
&lt;li>In phase 1, clients get new certificates as soon as possible to ensure that all clients have been adapted before phase 2.&lt;/li>
&lt;li>In phase 2, the respective server drops accepting certificates signed by the old CA.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Server certificates:
&lt;ul>
&lt;li>In phase 1, servers still use their old/existing certificates to allow clients to update their CA bundle used for verification of the servers&amp;rsquo; certificates.&lt;/li>
&lt;li>In phase 2, the old CA is dropped, hence servers need to get a certificate signed by the new/current CA. At this point in time, clients have already adapted their CA bundles.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>In case you control all clients and update them at the same time as the server, it is possible to make the secrets manager generate even server certificates with the new/current CA.
This can help preventing certificate mismatches when the CA bundle is already exchanged while the server still serves with a certificate signed by a CA no longer part of the bundle.&lt;/p>
&lt;p>Let&amp;rsquo;s consider the two following examples:&lt;/p>
&lt;ol>
&lt;li>&lt;code>gardenlet&lt;/code> deploys a webhook server (&lt;code>gardener-resource-manager&lt;/code>) and a corresponding &lt;code>MutatingWebhookConfiguration&lt;/code> at the same time. In this case, the server certificate should be generated with the new/current CA to avoid above mentioned certificate mismatches during a CA rotation.&lt;/li>
&lt;li>&lt;code>gardenlet&lt;/code> deploys a server (&lt;code>etcd&lt;/code>) in one step, and a client (&lt;code>kube-apiserver&lt;/code>) in a subsequent step. In this case, the default behaviour should apply (server certificate should be signed by old/existing CA).&lt;/li>
&lt;/ol>
&lt;h2 id="reusing-the-secretsmanager-in-other-components">Reusing the SecretsManager in Other Components&lt;/h2>
&lt;p>While the &lt;code>SecretsManager&lt;/code> is primarily used by gardenlet, it can be reused by other components (e.g. extensions) as well for managing secrets that are specific to the component or extension. For example, provider extensions might use their own &lt;code>SecretsManager&lt;/code> instance for managing the serving certificate of &lt;code>cloud-controller-manager&lt;/code>.&lt;/p>
&lt;p>External components that want to reuse the &lt;code>SecretsManager&lt;/code> should consider the following aspects:&lt;/p>
&lt;ul>
&lt;li>On initialization of a &lt;code>SecretsManager&lt;/code>, pass an &lt;code>identity&lt;/code> specific to the component, controller and purpose. For example, gardenlet&amp;rsquo;s shoot controller uses &lt;code>gardenlet&lt;/code> as the &lt;code>SecretsManager&lt;/code>&amp;rsquo;s identity, the &lt;code>Worker&lt;/code> controller in &lt;code>provider-foo&lt;/code> should use &lt;code>provider-foo-worker&lt;/code> and the &lt;code>ControlPlane&lt;/code> controller should use &lt;code>provider-foo-controlplane-exposure&lt;/code> for &lt;code>ControlPlane&lt;/code> objects of purpose &lt;code>exposure&lt;/code>.&lt;br>
The given identity is added as a value for the &lt;code>manager-identity&lt;/code> label on managed &lt;code>Secret&lt;/code>s.
This label is used by the &lt;code>Cleanup&lt;/code> function to select only those &lt;code>Secret&lt;/code>s that are actually managed by the particular &lt;code>SecretManager&lt;/code> instance. This is done to prevent removing still needed &lt;code>Secret&lt;/code>s that are managed by other instances.&lt;/li>
&lt;li>Generate dedicated CAs for signing certificates instead of depending on CAs managed by gardenlet.&lt;/li>
&lt;li>Names of &lt;code>Secret&lt;/code>s managed by external &lt;code>SecretsManager&lt;/code> instances must not conflict with &lt;code>Secret&lt;/code> names from other instances (e.g. gardenlet).&lt;/li>
&lt;li>For CAs that should be rotated in lock-step with the Shoot CAs managed by gardenlet, components need to pass information about the last rotation initiation time and the current rotation phase to the &lt;code>SecretsManager&lt;/code> upon initialization.
The relevant information can be retrieved from the &lt;code>Cluster&lt;/code> resource under &lt;code>.spec.shoot.status.credentials.rotation.certificateAuthorities&lt;/code>.&lt;/li>
&lt;li>Independent of the specific identity, secrets marked with the &lt;code>Persist&lt;/code> option are automatically saved in the &lt;code>ShootState&lt;/code> resource by gardenlet and are also restored by gardenlet on Control Plane Migration to the new Seed.&lt;/li>
&lt;/ul>
&lt;h2 id="implementation-details">Implementation Details&lt;/h2>
&lt;p>The source of truth for the secrets manager is the list of &lt;code>Secret&lt;/code>s in the Kubernetes cluster it acts upon (typically, the seed cluster).
The persisted secrets in the &lt;code>ShootState&lt;/code> are only used if and only if the shoot is in the &lt;code>Restore&lt;/code> phase - in this case all secrets are just synced to the seed cluster so that they can be picked up by the secrets manager.&lt;/p>
&lt;p>In order to prevent kubelets from unneeded watches (thus, causing some significant traffic against the &lt;code>kube-apiserver&lt;/code>), the &lt;code>Secret&lt;/code>s are marked as immutable.
Consequently, they have a unique, deterministic name which is computed as follows:&lt;/p>
&lt;ul>
&lt;li>For CA secrets, the name is just exactly the name specified in the configuration (e.g., &lt;code>ca&lt;/code>). This is for backwards-compatibility and will be dropped in a future release once all components depending on the static name have been adapted.&lt;/li>
&lt;li>For all other secrets, the name specified in the configuration is used as prefix followed by an 8-digit hash. This hash is computed out of the checksum of the secret configuration and the checksum of the certificate of the signing CA (only for certificate configurations).&lt;/li>
&lt;/ul>
&lt;p>In all cases, the name of the secrets is suffixed with a 5-digit hash computed out of the time when the rotation for this secret was last started.&lt;/p></description></item><item><title>Docs: Seed Admission Controller</title><link>https://gardener.cloud/docs/gardener/concepts/seed-admission-controller/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/seed-admission-controller/</guid><description>
&lt;h1 id="gardener-seed-admission-controller">Gardener Seed Admission Controller&lt;/h1>
&lt;p>The Gardener Seed admission controller is deployed by the Gardenlet as part of its seed bootstrapping phase and, consequently, running in every seed cluster.
It&amp;rsquo;s main purpose is to serve webhooks (validating or mutating) in order to admit or deny certain requests to the seed&amp;rsquo;s API server.&lt;/p>
&lt;h2 id="what-is-it-doing-concretely">What is it doing concretely?&lt;/h2>
&lt;h3 id="validating-webhooks">Validating Webhooks&lt;/h3>
&lt;h4 id="unconfirmed-deletion-prevention">Unconfirmed Deletion Prevention&lt;/h4>
&lt;p>As part of Gardener&amp;rsquo;s &lt;a href="https://gardener.cloud/docs/gardener/extensions/overview/">extensibility concepts&lt;/a> a lot of &lt;code>CustomResourceDefinition&lt;/code>s are deployed to the seed clusters that serve as extension points for provider-specific controllers.
For example, the &lt;a href="https://gardener.cloud/docs/gardener/extensions/infrastructure/">&lt;code>Infrastructure&lt;/code> CRD&lt;/a> triggers the provider extension to prepare the IaaS infrastructure of the underlying cloud provider for a to-be-created shoot cluster.
Consequently, these extension CRDs have a lot of power and control large portions of the end-user&amp;rsquo;s shoot cluster.
Accidental or undesired deletions of those resource can cause tremendous and hard-to-recover-from outages and should be prevented.&lt;/p>
&lt;p>Together with the deployment of the Gardener seed admission controller a &lt;code>ValidatingWebhookConfiguration&lt;/code> for &lt;code>CustomResourceDefinitions&lt;/code> and most (custom) resources in the &lt;code>extensions.gardener.cloud/v1alpha1&lt;/code> API group is registered.
It prevents &lt;code>DELETE&lt;/code> requests for those &lt;code>CustomResourceDefinitions&lt;/code> labeled with &lt;code>gardener.cloud/deletion-protected=true&lt;/code>, and for all mentioned custom resources if they were not previously annotated with the &lt;code>confirmation.gardener.cloud/deletion=true&lt;/code>.
This prevents that undesired &lt;code>kubectl delete &amp;lt;...&amp;gt;&lt;/code> requests are accepted.&lt;/p>
&lt;h3 id="mutating-webhooks">Mutating Webhooks&lt;/h3>
&lt;p>The admission controller endpoint &lt;code>/webhooks/default-pod-scheduler-name/gardener-kube-scheduler&lt;/code> mutates &lt;code>pods&lt;/code> and adds &lt;code>gardener-kube-scheduler&lt;/code> to &lt;code>.spec.scheduleName&lt;/code>.&lt;/p>
&lt;p>When &lt;code>SeedKubeScheduler&lt;/code> feature gate is enabled, all control plane components are mutated. The scheduler scores &lt;code>Nodes&lt;/code> with most resource usage higher than the rest, resulting in greater resource utilization.&lt;/p></description></item><item><title>Docs: Seed Network Policies</title><link>https://gardener.cloud/docs/gardener/development/seed_network_policies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/seed_network_policies/</guid><description>
&lt;h1 id="network-policies-in-the-seed-cluster">Network Policies in the Seed Cluster&lt;/h1>
&lt;p>This document describes the &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Kubernetes network policies&lt;/a> deployed by Gardener into the Seed cluster.
For network policies deployed into the Shoot &lt;code>kube-system&lt;/code> namespace, please see the &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_network_policies/">usage section&lt;/a>.&lt;/p>
&lt;p>Network policies deployed by Gardener have names and annotations describing their purpose, so this document does only highlight a subset of the policies in detail.&lt;/p>
&lt;h2 id="network-policies-in-the-shoot-namespace-in-the-seed">Network policies in the Shoot namespace in the Seed&lt;/h2>
&lt;p>The network policies in the Shoot namespace in the Seed can roughly be grouped into policies required for the control plane components and for logging &amp;amp; monitoring.&lt;/p>
&lt;p>The network policy &lt;code>deny-all&lt;/code> plays a special role. This policy &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-and-all-egress-traffic">denies all ingress and egress traffic&lt;/a> from each pod in the Shoot namespace.
So per default, a pod running in the control plane cannot talk to any other pod in the whole Seed cluster.
This means the pod needs to have labels matching to appropriate network policies allowing it to talk to exactly the components required to execute its desired functionality.
&lt;a href="#implications-for-gardener-extensions">This has also implications for Gardener extensions&lt;/a> that need to deploy additional components into the &lt;code>Shoot's&lt;/code> control plane.&lt;/p>
&lt;h3 id="network-policies-for-control-plane-components">Network Policies for Control Plane Components&lt;/h3>
&lt;p>This section highlights a selection of network policies that exist in the Shoot namespace in the Seed cluster.
In general, the control plane components serve different purposes and thus need access to different pods and network ranges.&lt;/p>
&lt;p>In contrast to other network policies, the policy &lt;code>allow-to-shoot-networks&lt;/code> is tailored to the individual Shoot cluster,
because it is based on the network configuration in the Shoot manifest.
It allows pods with the label &lt;code>networking.gardener.cloud/to-shoot-networks=allowed&lt;/code> to access pods in the Shoot pod,
service and node CIDR range. This is used by the Shoot API Server and the prometheus pods to communicate over VPN/proxy with pods in the Shoot cluster.&lt;/p>
&lt;p>The policy &lt;code>allow-to-blocked-cidrs&lt;/code> allows pods with the label &lt;code>networking.gardener.cloud/to-blocked-cidrs=allowed&lt;/code> to access IPs that are explicitly blocked for all control planes in a Seed cluster (configurable via &lt;code>spec.networks.blockCIDRS&lt;/code>).
This is used for instance to block the cloud provider&amp;rsquo;s metadata service.&lt;/p>
&lt;p>Another network policy to be highlighted is &lt;code>allow-to-seed-apiserver&lt;/code>.
Some components need access to the Seed API Server. This can be allowed by labeling the pod with &lt;code>networking.gardener.cloud/to-seed-apiserver=allowed&lt;/code>.
This policy allows exactly the IPs of the &lt;code>kube-apiserver&lt;/code> of the Seed.
While all other policies have a static set of permissions (do not change during the lifecycle of the Shoot), the policy &lt;code>allow-to-seed-apiserver&lt;/code> is reconciled to reflect the endpoints in the &lt;code>default&lt;/code> namespace.
This is required because endpoint IPs are not necessarily stable (think of scaling the Seed API Server pods or hibernating the Seed cluster (acting as a managed seed) in a local development environment).&lt;/p>
&lt;p>Furthermore, the following network policies exist in the Shoot namespace.
These policies are the same for every Shoot control plane.&lt;/p>
&lt;pre tabindex="0">&lt;code>NAME POD-SELECTOR
# Pods that need to access the Shoot API server. Used by all Kubernetes control plane components.
allow-to-shoot-apiserver networking.gardener.cloud/to-shoot-apiserver=allowed
# allows access to kube-dns/core-dns pods for DNS queries
allow-to-dns networking.gardener.cloud/to-dns=allowed
# allows access to private IP address ranges
allow-to-private-networks networking.gardener.cloud/to-private-networks=allowed
# allows access to all but private IP address ranges
allow-to-public-networks networking.gardener.cloud/to-public-networks=allowed
# allows Ingress to etcd pods from the Shoot&amp;#39;s Kubernetes API Server
allow-etcd app=etcd-statefulset,garden.sapcloud.io/role=controlplane
# used by the Shoot API server to allows ingress from pods labeled
# with&amp;#39;networking.gardener.cloud/to-shoot-apiserver=allowed&amp;#39;, from Prometheus, and allows Egress to etcd pods
allow-kube-apiserver app=kubernetes,gardener.cloud/role=controlplane,role=apiserver
&lt;/code>&lt;/pre>&lt;h3 id="network-policies-for-logging--monitoring">Network policies for Logging &amp;amp; Monitoring&lt;/h3>
&lt;p>Gardener currently introduces a logging stack based on &lt;a href="https://github.com/grafana/loki">Loki&lt;/a>. So this section is subject to change.
Please checkout &lt;a href="https://www.youtube.com/watch?v=345b8xCcB-U&amp;amp;t=1166s">the Community Meeting for more information&lt;/a>.&lt;/p>
&lt;p>These are the logging and monitoring related network policies:&lt;/p>
&lt;pre tabindex="0">&lt;code>NAME POD-SELECTOR
allow-from-prometheus networking.gardener.cloud/from-prometheus=allowed
allow-grafana component=grafana,gardener.cloud/role=monitoring
allow-prometheus app=prometheus,gardener.cloud/role=monitoring,role=monitoring
allow-to-aggregate-prometheus networking.gardener.cloud/to-aggregate-prometheus=allowed
allow-to-loki networking.gardener.cloud/to-loki=allowed
&lt;/code>&lt;/pre>&lt;p>Let&amp;rsquo;s take for instance a look at the network policy &lt;code>from-prometheus&lt;/code>.
As part of the shoot reconciliation flow, Gardener deploys a shoot-specific Prometheus into the shoot namespace.
Each pod that should be scraped for metrics must be labeled with &lt;code>networking.gardener.cloud/from-prometheus=allowed&lt;/code> to allow incoming network requests by the prometheus pod.
Most components of the Shoot cluster&amp;rsquo;s control plane expose metrics and are therefore labeled appropriately.&lt;/p>
&lt;h3 id="implications-for-gardener-extensions">Implications for Gardener Extensions&lt;/h3>
&lt;p>Gardener extensions sometimes need to deploy additional components into the Shoot namespace in the Seed hosting the control plane.
For example the Gardener extension &lt;a href="https://github.com/gardener/gardener-extension-provider-aws">provider-aws&lt;/a> deploys the &lt;code>MachineControllerManager&lt;/code> into the Shoot namespace, that is ultimately responsible to create the VMs with the cloud provider AWS.&lt;/p>
&lt;p>Every Shoot namespace in the Seed contains the network policy &lt;code>deny-all&lt;/code>.
This requires a pod deployed by a Gardener extension to have labels from network policies, that exist in the Shoot namespace, that allow the required network ranges.&lt;/p>
&lt;p>Additionally, extensions could also deploy their own network policies. This is used e.g by the Gardener extension &lt;a href="https://github.com/gardener/gardener-extension-provider-aws">provider-aws&lt;/a>
to serve &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">Admission Webhooks&lt;/a> for the Shoot API server that need to be reachable from within the Shoot namespace.&lt;/p>
&lt;p>The pod can use an arbitrary combination of network policies.&lt;/p>
&lt;h2 id="network-policies-in-the-garden-namespace">Network policies in the &lt;code>garden&lt;/code> namespace&lt;/h2>
&lt;p>The network policies in the &lt;code>garden&lt;/code> namespace are, with a few exceptions (e.g Kubernetes control plane specific policies), the same as in the Shoot namespaces.
For your reference, these are all the deployed network policies.&lt;/p>
&lt;pre tabindex="0">&lt;code>NAME POD-SELECTOR
allow-fluentbit app=fluent-bit,gardener.cloud/role=logging,role=logging
allow-from-aggregate-prometheus networking.gardener.cloud/from-aggregate-prometheus=allowed
allow-to-aggregate-prometheus networking.gardener.cloud/to-aggregate-prometheus=allowed
allow-to-all-shoot-apiservers networking.gardener.cloud/to-all-shoot-apiservers=allowed
allow-to-blocked-cidrs networking.gardener.cloud/to-blocked-cidrs=allowed
allow-to-dns networking.gardener.cloud/to-dns=allowed
allow-to-loki networking.gardener.cloud/to-loki=allowed
allow-to-private-networks networking.gardener.cloud/to-private-networks=allowed
allow-to-public-networks networking.gardener.cloud/to-public-networks=allowed
allow-to-seed-apiserver networking.gardener.cloud/to-seed-apiserver=allowed
deny-all networking.gardener.cloud/to-all=disallowed
&lt;/code>&lt;/pre>&lt;p>This section describes the network policies that are unique to the &lt;code>garden&lt;/code> namespace.&lt;/p>
&lt;p>The network policy &lt;code>allow-to-all-shoot-apiservers&lt;/code> allows pods to access every &lt;code>Shoot&lt;/code> API server in the &lt;code>Seed&lt;/code>.
This is for instance used by the &lt;a href="https://github.com/gardener/dependency-watchdog">dependency watchdog&lt;/a> to regularly check
the health of all the Shoot API servers.&lt;/p>
&lt;p>&lt;a href="https://gardener.cloud/docs/gardener/extensions/logging-and-monitoring/#monitoring">Gardener deploys a central Prometheus instance&lt;/a> in the &lt;code>garden&lt;/code> namespace that fetches metrics and data from all seed cluster nodes and all seed cluster pods.
The network policies &lt;code>allow-to-aggregate-prometheus&lt;/code> and &lt;code>allow-from-aggregate-prometheus&lt;/code> allow traffic from and to this prometheus instance.&lt;/p>
&lt;p>Worth mentioning is, that the network policy &lt;code>allow-to-shoot-networks&lt;/code> does not exist in the &lt;code>garden&lt;/code> namespace. This is to forbid Gardener system components to talk to workload deployed in the Shoot VPC.&lt;/p></description></item><item><title>Docs: Setup Gardener</title><link>https://gardener.cloud/docs/gardener/deployment/setup_gardener/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/setup_gardener/</guid><description>
&lt;h1 id="deploying-the-gardener-into-a-kubernetes-cluster">Deploying the Gardener into a Kubernetes cluster&lt;/h1>
&lt;p>Similar to Kubernetes, Gardener consists out of control plane components (Gardener API server, Gardener controller manager, Gardener scheduler), and an agent component (Gardenlet).
The control plane is deployed in the so-called garden cluster while the agent is installed into every seed cluster.
Please note that it is possible to use the garden cluster as seed cluster by simply deploying the Gardenlet into it.&lt;/p>
&lt;p>We are providing &lt;a href="https://github.com/gardener/gardener/tree/master/charts/gardener">Helm charts&lt;/a> in order to manage the various resources of the components.
Please always make sure that you use the Helm chart version that matches the Gardener version you want to deploy.&lt;/p>
&lt;h2 id="deploying-the-gardener-control-plane-api-server-admission-controller-controller-manager-scheduler">Deploying the Gardener control plane (API server, admission controller, controller manager, scheduler)&lt;/h2>
&lt;p>The &lt;a href="https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/values.yaml">configuration values&lt;/a> depict the various options to configure the different components.
Please consult &lt;a href="https://gardener.cloud/docs/gardener/usage/configuration/">this document&lt;/a> for component specific configurations and &lt;a href="https://gardener.cloud/docs/gardener/deployment/authentication_gardener_control_plane/">this document&lt;/a> for authentication related specifics.&lt;/p>
&lt;p>Also note that all resources and deployments need to be created in the &lt;code>garden&lt;/code> namespace (not overrideable).
If you enable the Gardener admission controller as part of you setup, please make sure the &lt;code>garden&lt;/code> namespace is labelled with &lt;code>app: gardener&lt;/code>.
Otherwise, the backing service account for the admission controller Pod might not be created successfully.
No action is necessary, if you deploy the &lt;code>garden&lt;/code> namespace with the Gardener control plane Helm chart.&lt;/p>
&lt;p>After preparing your values in a separate &lt;code>controlplane-values.yaml&lt;/code> file (&lt;a href="https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/values.yaml">values.yaml&lt;/a> can be used as starting point), you can run the following command against your garden cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>helm install charts/gardener/controlplane &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --namespace garden &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --name gardener-controlplane &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> -f controlplane-values.yaml &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --wait
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="deploying-gardener-extensions">Deploying Gardener extensions&lt;/h2>
&lt;p>Gardener is an extensible system that does not contain the logic for provider-specific things like DNS management, cloud infrastructures, network plugins, operating system configs, and many more.&lt;/p>
&lt;p>You have to install extension controllers for these parts.
Please consult &lt;a href="https://gardener.cloud/docs/gardener/extensions/overview/">the documentation regarding extensions&lt;/a> to get more information.&lt;/p>
&lt;h2 id="deploying-the-gardener-agent-gardenlet">Deploying the Gardener agent (Gardenlet)&lt;/h2>
&lt;p>Please refer to &lt;a href="https://gardener.cloud/docs/gardener/deployment/deploy_gardenlet/">this document&lt;/a> on how to deploy a Gardenlet.&lt;/p></description></item><item><title>Docs: Testing</title><link>https://gardener.cloud/docs/gardener/development/testing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/testing/</guid><description>
&lt;h1 id="testing">Testing&lt;/h1>
&lt;h2 id="unit-tests">Unit Tests&lt;/h2>
&lt;p>We follow the BDD-style testing principles and are leveraging the &lt;a href="https://onsi.github.io/ginkgo/">Ginkgo&lt;/a> framework along with &lt;a href="http://onsi.github.io/gomega/">Gomega&lt;/a> as matcher library. In order to execute the existing tests, you can use&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make test &lt;span style="color:#008000"># runs tests&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>make verify &lt;span style="color:#008000"># runs static code checks and tests (unit and integration)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There is an additional command for analyzing the code coverage of the tests. Ginkgo will generate standard Go cover profiles which will be translated into an HTML file by the &lt;a href="https://blog.golang.org/cover">Go Cover Tool&lt;/a>. Another command helps you to clean up the filesystem from the temporary cover profile files and the HTML report:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make test-cov
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>open gardener.coverage.html
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>make test-cov-clean
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="integration-tests-envtests">Integration Tests (envtests)&lt;/h2>
&lt;p>Integration tests in Gardener use the &lt;code>sigs.k8s.io/controller-runtime/pkg/envtest&lt;/code> package.
It sets up a temporary control plane (etcd + kube-apiserver) and runs the test against it.
The &lt;code>test-integration&lt;/code> make rule prepares the environment automatically by downloading the respective binaries (if not yet present) and sets the necessary environment variables.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make test-integration
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you want to run a specific set of integration tests, you can also execute them using &lt;code>./hack/test-integration.sh&lt;/code> directly instead of using the &lt;code>test-integration&lt;/code> rule. For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./hack/test-integration.sh ./test/integration/resourcemanager/tokenrequestor
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The script takes care of preparing the environment for you.
If you want to execute the test suites directly via &lt;code>go test&lt;/code> or &lt;code>ginkgo&lt;/code>, you have to point the &lt;code>KUBEBUILDER_ASSETS&lt;/code> environment variable to the path that contains the etcd and kube-apiserver binaries. Alternatively, you can install the binaries to &lt;code>/usr/local/kubebuilder/bin&lt;/code>.&lt;/p>
&lt;h3 id="debugging-integration-tests">Debugging Integration Tests&lt;/h3>
&lt;p>You can configure envtest to use an existing cluster instead of starting a temporary control plane for your test.
This can be helpful for debugging integration tests, because you can easily inspect what is going on in your test cluster.
For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make kind-up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export KUBECONFIG=$PWD/example/gardener-local/kind/kubeconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export USE_EXISTING_CLUSTER=true
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># run test with verbose output&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>./hack/test-integration.sh -v ./test/integration/resourcemanager/health -ginkgo.v
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="end-to-end-tests-using-provider-local">End-to-end Tests (using provider-local)&lt;/h2>
&lt;p>We run a suite of e2e tests on every pull request and periodically on the &lt;code>master&lt;/code> branch.
It uses a &lt;a href="https://kind.sigs.k8s.io/">KinD cluster&lt;/a> and &lt;a href="https://skaffold.dev/">skaffold&lt;/a> to boostrap a full installation of Gardener based on the current revision, including &lt;a href="https://gardener.cloud/docs/gardener/extensions/provider-local/">provider-local&lt;/a>.
This allows us to run e2e tests in an isolated test environment and fully locally without any infrastructure interaction.
The tests perform a set of operations on Shoot clusters, e.g. creating, deleting, hibernating and waking up.&lt;/p>
&lt;p>These tests are executed in our prow instance at &lt;a href="https://prow.gardener.cloud/">prow.gardener.cloud&lt;/a>, see &lt;a href="https://github.com/gardener/ci-infra/blob/e324cb79c39c013d7f253c33690b7fcc92c001d8/config/jobs/gardener/gardener-e2e-kind.yaml">job definition&lt;/a> and &lt;a href="https://prow.gardener.cloud/?repo=gardener%2Fgardener&amp;amp;job=*gardener-e2e-kind">job history&lt;/a>.&lt;/p>
&lt;p>You can also run these tests on your development machine, using the following commands:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make kind-up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export KUBECONFIG=$PWD/example/gardener-local/kind/kubeconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>make gardener-up
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>make test-e2e-local &lt;span style="color:#008000"># alternatively: make test-e2e-local-fast&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you want to run a specific set of e2e test cases, you can also execute them using &lt;code>./hack/test-e2e-local.sh&lt;/code> directly in combination with &lt;a href="https://onsi.github.io/ginkgo/#spec-labels">ginkgo label filters&lt;/a>. For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>./hack/test-e2e-local.sh --label-filter &lt;span style="color:#a31515">&amp;#34;Shoot &amp;amp;&amp;amp; ca-rotation&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Also see: &lt;a href="https://gardener.cloud/docs/gardener/development/getting_started_locally/">developing Gardener locally&lt;/a> and &lt;a href="https://gardener.cloud/docs/gardener/deployment/getting_started_locally/">deploying Gardener locally&lt;/a>.&lt;/p>
&lt;h2 id="test-machinery-tests">Test Machinery Tests&lt;/h2>
&lt;p>Please see &lt;a href="https://gardener.cloud/docs/gardener/development/testmachinery_tests/">Test Machinery Tests&lt;/a>.&lt;/p></description></item><item><title>Docs: Testmachinery Tests</title><link>https://gardener.cloud/docs/gardener/development/testmachinery_tests/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/development/testmachinery_tests/</guid><description>
&lt;h1 id="test-machinery-tests">Test Machinery Tests&lt;/h1>
&lt;p>In order to automatically qualify Gardener releases, we execute a set of end-to-end tests using &lt;a href="https://github.com/gardener/test-infra">Test Machinery&lt;/a>.
This requires a full Gardener installation including infrastructure extensions as well as a setup of Test Machinery itself.
These tests operate on Shoot clusters across different Cloud Providers, using different supported Kubernetes versions and various configuration options (huge test matrix).&lt;/p>
&lt;p>This manual gives an overview about test machinery tests in Gardener.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#structure">Structure&lt;/a>&lt;/li>
&lt;li>&lt;a href="#add-a-new-test">Add a new test&lt;/a>&lt;/li>
&lt;li>&lt;a href="#test-labels">Test Labels&lt;/a>&lt;/li>
&lt;li>&lt;a href="#framework">Framework&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="structure">Structure&lt;/h2>
&lt;p>Gardener test machinery tests are split into two test suites that can be found under &lt;a href="https://github.com/gardener/gardener/tree/master/test/testmachinery/suites">&lt;code>test/testmachinery/suites&lt;/code>&lt;/a>:&lt;/p>
&lt;ul>
&lt;li>The &lt;strong>Gardener Test Suite&lt;/strong> contains all tests that only require a running gardener instance.&lt;/li>
&lt;li>The &lt;strong>Shoot Test Suite&lt;/strong> contains all tests that require a predefined running shoot cluster.&lt;/li>
&lt;/ul>
&lt;p>The corresponding tests of a test suite are defined in the import statement of the suite definition see &lt;a href="https://github.com/gardener/gardener/blob/master/test/testmachinery/suites/shoot/run_suite_test.go">&lt;code>shoot/run_suite_test.go&lt;/code>&lt;/a>
and their source code can be found under &lt;a href="https://github.com/gardener/gardener/tree/master/test/testmachinery">&lt;code>test/testmachinery&lt;/code>&lt;/a>&lt;/p>
&lt;p>The &lt;code>test&lt;/code> directory is structured as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>test
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── e2e # end-to-end tests (using provider-local)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ └── shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── framework # helper code shared across integration, e2e and testmachinery tests
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── integration # integration tests (envtests)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├── controllermanager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├── envtest
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├── resourcemanager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├── scheduler
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├── seedadmissioncontroller
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ ├── shootmaintenance
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│ └── ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>└── testmachinery # test machinery tests
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── gardener # actual test cases imported by suites/gardener
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ └── security
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── plants
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── shoots # actual test cases imported by suites/shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ ├── applications
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ ├── care
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ ├── logging
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ ├── operatingsystem
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ ├── operations
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ └── vpntunnel
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── suites # suites that run agains a running garden or shoot cluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ ├── gardener
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> │ └── shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── system # suites that are used for building a full test flow
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── complete_reconcile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── managed_seed_creation
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── managed_seed_deletion
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── shoot_cp_migration
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── shoot_creation
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── shoot_deletion
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── shoot_hibernation
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── shoot_hibernation_wakeup
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── shoot_update
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A suite can be executed by running the suite definition with ginkgo&amp;rsquo;s &lt;code>focus&lt;/code> and &lt;code>skip&lt;/code> flags
to control the execution of specific labeled test. See example below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>go test -timeout=0 -mod=vendor ./test/testmachinery/suites/shoot \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --v -ginkgo.v -ginkgo.progress -ginkgo.no-color \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --report-file=/tmp/report.json \ # write elasticsearch formatted output to a file
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --disable-dump=false \ # disables dumping of teh current state if a test fails
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -kubecfg=/path/to/gardener/kubeconfig \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -shoot-name=&amp;lt;shoot-name&amp;gt; \ # Name of the shoot to test
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -project-namespace=&amp;lt;gardener project namespace&amp;gt; \ # Name of the gardener project the test shoot resides
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -ginkgo.focus=&amp;#34;\[RELEASE\]&amp;#34; \ # Run all tests that are tagged as release
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -ginkgo.skip=&amp;#34;\[SERIAL\]|\[DISRUPTIVE\]&amp;#34; # Exclude all tests that are tagged SERIAL or DISRUPTIVE
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="add-a-new-test">Add a new test&lt;/h2>
&lt;p>To add a new test the framework requires the following steps (step 1. and 2. can be skipped if the test is added to an existing package):&lt;/p>
&lt;ol>
&lt;li>Create a new test file e.g. &lt;code>test/testmachinery/shoot/security/my-sec-test.go&lt;/code>&lt;/li>
&lt;li>Import the test into the appropriate test suite (gardener or shoot): &lt;code>import _ &amp;quot;github.com/gardener/gardener/test/testmachinery/shoot/security&amp;quot;&lt;/code>&lt;/li>
&lt;li>Define your test with the testframework. The framework will automatically add its initialization, cleanup and dump functions.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-golang" data-lang="golang">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> _ = ginkgo.Describe(&lt;span style="color:#a31515">&amp;#34;my suite&amp;#34;&lt;/span>, &lt;span style="color:#00f">func&lt;/span>(){
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> f := framework.NewShootFramework(&lt;span style="color:#00f">nil&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> f.Beta().CIt(&lt;span style="color:#a31515">&amp;#34;my first test&amp;#34;&lt;/span>, &lt;span style="color:#00f">func&lt;/span>(ctx context.Context) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> f.ShootClient.Get(xx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">// testing ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> })
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>})
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The newly created test can be tested by focusing the test with the default ginkgo focus &lt;code>f.Beta().FCIt(&amp;quot;my first test&amp;quot;, func(ctx context.Context)&lt;/code>
and run the shoot test suite with:&lt;/p>
&lt;pre tabindex="0">&lt;code>go test -timeout=0 -mod=vendor ./test/testmachinery/suites/shoot \
--v -ginkgo.v -ginkgo.progress -ginkgo.no-color \
--report-file=/tmp/report.json \ # write elasticsearch formatted output to a file
--disable-dump=false \ # disables dumping of the current state if a test fails
-kubecfg=/path/to/gardener/kubeconfig \
-shoot-name=&amp;lt;shoot-name&amp;gt; \ # Name of the shoot to test
-project-namespace=&amp;lt;gardener project namespace&amp;gt; \
-fenced=&amp;lt;true|false&amp;gt; # Tested shoot is running in a fenced environment and cannot be reached by gardener
&lt;/code>&lt;/pre>&lt;p>or for the gardener suite with:&lt;/p>
&lt;pre tabindex="0">&lt;code>go test -timeout=0 -mod=vendor ./test/testmachinery/suites/gardener \
--v -ginkgo.v -ginkgo.progress -ginkgo.no-color \
--report-file=/tmp/report.json \ # write elasticsearch formatted output to a file
--disable-dump=false \ # disables dumping of the current state if a test fails
-kubecfg=/path/to/gardener/kubeconfig \
-project-namespace=&amp;lt;gardener project namespace&amp;gt;
&lt;/code>&lt;/pre>&lt;p>⚠️ Make sure that you do not commit any focused specs as this feature is only intended for local development! Ginkgo will fail the test suite if there are any focused specs.&lt;/p>
&lt;p>Alternatively, a test can be triggered by specifying a ginkgo focus regex with the name of the test e.g.&lt;/p>
&lt;pre tabindex="0">&lt;code>go test -timeout=0 -mod=vendor ./test/testmachinery/suites/gardener \
--v -ginkgo.v -ginkgo.progress -ginkgo.no-color \
--report-file=/tmp/report.json \ # write elasticsearch formatted output to a file
-kubecfg=/path/to/gardener/kubeconfig \
-project-namespace=&amp;lt;gardener project namespace&amp;gt; \
-ginkgo.focus=&amp;#34;my first test&amp;#34; # regex to match test cases
&lt;/code>&lt;/pre>&lt;h2 id="test-labels">Test Labels&lt;/h2>
&lt;p>Every test should be labeled by using the predefined labels available with every framework to have consistent labeling across
all test machinery tests.&lt;/p>
&lt;p>The labels are applied to every new &lt;code>It()/CIt()&lt;/code> definition by:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-golang" data-lang="golang">&lt;span style="display:flex;">&lt;span>f := framework.NewCommonFramework()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>f.Default().Serial().It(&lt;span style="color:#a31515">&amp;#34;my test&amp;#34;&lt;/span>) =&amp;gt; &lt;span style="color:#a31515">&amp;#34;[DEFAULT] [SERIAL] my test&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>f := framework.NewShootFramework()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>f.Default().Serial().It(&lt;span style="color:#a31515">&amp;#34;my test&amp;#34;&lt;/span>) =&amp;gt; &lt;span style="color:#a31515">&amp;#34;[DEFAULT] [SERIAL] [SHOOT] my test&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>f := framework.NewGardenerFramework()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>f.Default().Serial().It(&lt;span style="color:#a31515">&amp;#34;my test&amp;#34;&lt;/span>) =&amp;gt; &lt;span style="color:#a31515">&amp;#34;[DEFAULT] [GARDENER] [SERIAL] my test&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Labels:&lt;/p>
&lt;ul>
&lt;li>&lt;em>Beta&lt;/em>: Newly created tests with no experience on stableness should be first labeled as beta tests.
They should be watched (and probably improved) until stable enough to be promoted to &lt;em>Default&lt;/em>.&lt;/li>
&lt;li>&lt;em>Default&lt;/em>: Tests that were &lt;em>Beta&lt;/em> before and proved to be stable are promoted to &lt;em>Default&lt;/em> eventually.
&lt;em>Default&lt;/em> tests run more often, produce alerts and are &lt;em>considered&lt;/em> during the release decision although they don&amp;rsquo;t necessarily block a release.&lt;/li>
&lt;li>&lt;em>Release&lt;/em>: Test are release relevant. A failing &lt;em>Release&lt;/em> test blocks the release pipeline.
Therefore these tests need to be stable. Only tests proven to be stable will eventually be promoted to &lt;em>Release&lt;/em>.&lt;/li>
&lt;/ul>
&lt;p>Behavior Labels:&lt;/p>
&lt;ul>
&lt;li>&lt;em>Serial&lt;/em>: The test should always be executed in serial with no other tests running as it may impact other tests.&lt;/li>
&lt;li>&lt;em>Destructive&lt;/em>: The test is destructive. Which means that is runs with no other tests and may break gardener or the shoot.
Only create such tests if really necessary as the execution will be expensive (neither gardener nor the shoot can be reused in this case for other tests).&lt;/li>
&lt;/ul>
&lt;h2 id="framework">Framework&lt;/h2>
&lt;p>The framework directory contains all the necessary functions / utilities for running test machinery tests.
For example, there are methods for creation/deletion of shoots, waiting for shoot deletion/creation, downloading/installing/deploying helm charts, logging, etc.&lt;/p>
&lt;p>The framework itself consists of 3 different framework that expect different prerequisites and offer context specific functionality.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>CommonFramework&lt;/strong>: The common framework is the base framework that handles logging and setup of commonly needed resources like helm.
It also contains common functions for interacting with kubernetes clusters like &lt;code>Waiting for resources to be ready&lt;/code> or &lt;code>Exec into a running pod&lt;/code>.&lt;/li>
&lt;li>&lt;strong>GardenerFramework&lt;/strong> contains all functions of the common framework and expects a running gardener instance with the provided gardener kubeconfig and a project namespace.
It also contains functions to interact with gardener like &lt;code>Waiting for a shoot to be reconciled&lt;/code> or &lt;code>Patch a shoot&lt;/code> or &lt;code>Get a seed&lt;/code>.&lt;/li>
&lt;li>&lt;strong>ShootFramework&lt;/strong>: contains all functions of the common and the gardener framework.
It expects a running shoot cluster defined by the shoot&amp;rsquo;s name and namespace(project namespace).
This framework contains functions to directly interact with the specific shoot.&lt;/li>
&lt;/ul>
&lt;p>The whole framework also includes commonly used checks, ginkgo wrapper, etc. as well as commonly used tests.
Theses common application tests (like the guestbook test) can be used within multiple tests to have a default application (with ingress, deployment, stateful backend) to test external factors.&lt;/p>
&lt;p>&lt;strong>Config&lt;/strong>&lt;/p>
&lt;p>Every framework commandline flag can also be defined by a configuration file (the value of the configuration file is only used if flag is not specified by commandline).
The test suite searches for a configuration file (yaml is preferred) if the command line flag &lt;code>--config=/path/to/config/file&lt;/code> is provided.
A framework can be defined in the configuration file by just using the flag name as root key e.g.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>verbose: debug
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubecfg: /kubeconfig/path
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>project-namespace: garden-it
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Report&lt;/strong>&lt;/p>
&lt;p>The framework automatically writes the default ginkgo default report to stdout and a specifically structured elastichsearch bulk report file to a specified location.
The elastichsearch bulk report will write one json document per testcase and injects metadata of the whole testsuite.
An example document for one test case would look like the following document:&lt;/p>
&lt;pre tabindex="0">&lt;code>{
&amp;#34;suite&amp;#34;: {
&amp;#34;name&amp;#34;: &amp;#34;Shoot Test Suite&amp;#34;,
&amp;#34;phase&amp;#34;: &amp;#34;Succeeded&amp;#34;,
&amp;#34;tests&amp;#34;: 3,
&amp;#34;failures&amp;#34;: 1,
&amp;#34;errors&amp;#34;: 0,
&amp;#34;time&amp;#34;: 87.427
},
&amp;#34;name&amp;#34;: &amp;#34;Shoot application testing [DEFAULT] [RELEASE] [SHOOT] should download shoot kubeconfig successfully&amp;#34;,
&amp;#34;shortName&amp;#34;: &amp;#34;should download shoot kubeconfig successfully&amp;#34;,
&amp;#34;labels&amp;#34;: [
&amp;#34;DEFAULT&amp;#34;,
&amp;#34;RELEASE&amp;#34;,
&amp;#34;SHOOT&amp;#34;
],
&amp;#34;phase&amp;#34;: &amp;#34;Succeeded&amp;#34;,
&amp;#34;time&amp;#34;: 0.724512057
}
&lt;/code>&lt;/pre>&lt;p>&lt;strong>Resources&lt;/strong>&lt;/p>
&lt;p>The resources directory contains all the templates, helm config files (e.g., repositories.yaml, charts, and cache index which are downloaded upon the start of the test), shoot configs, etc.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>resources
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── charts
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── repository
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>│   └── repositories.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>└── templates
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── guestbook-app.yaml.tpl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── logger-app.yaml.tpl
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>There are two special directories that are dynamically filled with the correct test files:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>charts:&lt;/strong> the charts will be downloaded and saved in this directory&lt;/li>
&lt;li>&lt;strong>repository&lt;/strong> contains the repository.yaml file that the target helm repos will be read from and the cache where the &lt;code>stable-index.yaml&lt;/code> file will be created&lt;/li>
&lt;/ul>
&lt;h3 id="system-tests">System Tests&lt;/h3>
&lt;p>This directory contains the system tests that have a special meaning for the testmachinery with their own Test Definition.
Currently these system tests consists of:&lt;/p>
&lt;ul>
&lt;li>Shoot creation&lt;/li>
&lt;li>Shoot deletion&lt;/li>
&lt;li>Shoot Kubernetes update&lt;/li>
&lt;li>Gardener Full reconcile check&lt;/li>
&lt;/ul>
&lt;h4 id="shoot-creation-test">Shoot Creation test&lt;/h4>
&lt;p>Create Shoot test is meant to test shoot creation.&lt;/p>
&lt;p>&lt;strong>Example Run&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>go test -mod=vendor -timeout=0 ./test/testmachinery/system/shoot_creation \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --v -ginkgo.v -ginkgo.progress \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -kubecfg=$HOME/.kube/config \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -shoot-name=$SHOOT_NAME \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -cloud-profile=$CLOUDPROFILE \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -seed=$SEED \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -secret-binding=$SECRET_BINDING \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -provider-type=$PROVIDER_TYPE \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -region=$REGION \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -k8s-version=$K8S_VERSION \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -project-namespace=$PROJECT_NAMESPACE \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -annotations=$SHOOT_ANNOTATIONS \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -infrastructure-provider-config-filepath=$INFRASTRUCTURE_PROVIDER_CONFIG_FILEPATH \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -controlplane-provider-config-filepath=$CONTROLPLANE_PROVIDER_CONFIG_FILEPATH \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -workers-config-filepath=$$WORKERS_CONFIG_FILEPATH \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -worker-zone=$ZONE \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -networking-pods=$NETWORKING_PODS \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -networking-services=$NETWORKING_SERVICES \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -networking-nodes=$NETWORKING_NODES \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -start-hibernated=$START_HIBERNATED
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="shoot-deletion-test">Shoot Deletion test&lt;/h4>
&lt;p>Delete Shoot test is meant to test the deletion of a shoot.&lt;/p>
&lt;p>&lt;strong>Example Run&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>go test -mod=vendor -timeout=0 -ginkgo.v -ginkgo.progress \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ./test/testmachinery/system/shoot_deletion \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -kubecfg=$HOME/.kube/config \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -shoot-name=$SHOOT_NAME \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -project-namespace=$PROJECT_NAMESPACE
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="shoot-update-test">Shoot Update test&lt;/h4>
&lt;p>The Update Shoot test is meant to test the kubernetes version update of a existing shoot.
If no specific version is provided the next patch version is automatically selected.
If there is no available newer version this test is a noop.&lt;/p>
&lt;p>&lt;strong>Example Run&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>go test -mod=vendor -timeout=0 ./test/testmachinery/system/shoot_update \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --v -ginkgo.v -ginkgo.progress \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -kubecfg=$HOME/.kube/config \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -shoot-name=$SHOOT_NAME \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -project-namespace=$PROJECT_NAMESPACE \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -version=$K8S_VERSION
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="gardener-full-reconcile-test">Gardener Full Reconcile test&lt;/h4>
&lt;p>The Gardener Full Reconcile test is meant to test if all shoots of a gardener instance are successfully reconciled.&lt;/p>
&lt;p>&lt;strong>Example Run&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>go test -mod=vendor -timeout=0 ./test/testmachinery/system/complete_reconcile \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> --v -ginkgo.v -ginkgo.progress \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -kubecfg=$HOME/.kube/config \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -project-namespace=$PROJECT_NAMESPACE \
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> -gardenerVersion=$GARDENER_VERSION # needed to validate the last acted gardener version of a shoot
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Version Skew Policy</title><link>https://gardener.cloud/docs/gardener/deployment/version_skew_policy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/version_skew_policy/</guid><description>
&lt;h1 id="version-skew-policy">Version Skew Policy&lt;/h1>
&lt;p>This document describes the maximum version skew supported between various Gardener components.&lt;/p>
&lt;h2 id="supported-gardener-versions">Supported Gardener Versions&lt;/h2>
&lt;p>Gardener versions are expressed as &lt;code>x.y.z&lt;/code>, where &lt;code>x&lt;/code> is the major version, &lt;code>y&lt;/code> is the minor version, and &lt;code>z&lt;/code> is the patch version, following Semantic Versioning terminology.&lt;/p>
&lt;p>The Gardener project maintains release branches for the most recent three minor releases.&lt;/p>
&lt;p>Applicable fixes, including security fixes, may be backported to those three release branches, depending on severity and feasibility.
Patch releases are cut from those branches at a regular cadence, plus additional urgent releases when required.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/development/process/#releases">this document&lt;/a>.&lt;/p>
&lt;h3 id="supported-version-skew">Supported Version Skew&lt;/h3>
&lt;p>Technically, we follow the same &lt;a href="https://kubernetes.io/releases/version-skew-policy/">policy&lt;/a> as the Kubernetes project.
However, given that our release cadence is much more frequent compared to Kubernetes (every &lt;code>14d&lt;/code> vs. every &lt;code>120d&lt;/code>), in many cases it is possible to skip a version.
Still, to be on the safe side, it is highly recommended to follow the described policy.&lt;/p>
&lt;h4 id="gardener-apiserver">gardener-apiserver&lt;/h4>
&lt;p>In multi-instance setups of Gardener, the newest and oldest &lt;code>gardener-apiserver&lt;/code> instances must be within one minor version.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;ul>
&lt;li>newest &lt;code>gardener-apiserver&lt;/code> is at &lt;strong>1.37&lt;/strong>&lt;/li>
&lt;li>other &lt;code>gardener-apiserver&lt;/code> instances are supported at &lt;strong>1.37&lt;/strong> and &lt;strong>v1.36&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h4 id="gardener-controller-manager-gardener-scheduler-gardener-admission-controller-gardenlet">gardener-controller-manager, gardener-scheduler, gardener-admission-controller, gardenlet&lt;/h4>
&lt;p>&lt;code>gardener-controller-manager&lt;/code>, &lt;code>gardener-scheduler&lt;/code>, &lt;code>gardener-admission-controller&lt;/code>, and &lt;code>gardenlet&lt;/code> must not be newer than the &lt;code>gardener-apiserver&lt;/code> instances they communicate with.
They are expected to match the &lt;code>gardener-apiserver&lt;/code> minor version, but may be up to one minor version older (to allow live upgrades).&lt;/p>
&lt;p>Example:&lt;/p>
&lt;ul>
&lt;li>&lt;code>gardener-apiserver&lt;/code> is at &lt;strong>v1.37&lt;/strong>&lt;/li>
&lt;li>&lt;code>gardener-controller-manager&lt;/code>, &lt;code>gardener-scheduler&lt;/code>, &lt;code>gardener-admission-controller&lt;/code>, and &lt;code>gardenlet&lt;/code> are supported at &lt;strong>1.37&lt;/strong> and &lt;strong>v1.36&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h3 id="supported-component-upgrade-order">Supported Component Upgrade Order&lt;/h3>
&lt;p>The supported version skew between components has implications on the order in which components must be upgraded.
This section describes the order in which components must be upgraded to transition an existing Gardener installation from version &lt;strong>1.37&lt;/strong> to version &lt;strong>1.38&lt;/strong>.&lt;/p>
&lt;h4 id="gardener-apiserver-1">gardener-apiserver&lt;/h4>
&lt;p>Pre-requisites:&lt;/p>
&lt;ul>
&lt;li>In a single-instance setup, the existing &lt;code>gardener-apiserver&lt;/code> instance is &lt;strong>1.37&lt;/strong>&lt;/li>
&lt;li>In a multi-instance setup, all &lt;code>gardener-apiserver&lt;/code> instances are at &lt;strong>1.37&lt;/strong> or &lt;strong>1.38&lt;/strong> (this ensures maximum skew of 1 minor version between the oldest and newest &lt;code>gardener-apiserver&lt;/code> instance)&lt;/li>
&lt;li>The &lt;code>gardener-controller-manager&lt;/code>, &lt;code>gardener-scheduler&lt;/code>, &lt;code>gardener-admission-controller&lt;/code>, and &lt;code>gardenlet&lt;/code> instances that communicate with this &lt;code>gardener-apiserver&lt;/code> are at version &lt;strong>1.37&lt;/strong> (this ensures they are not newer than the existing API server version and are within 1 minor version of the new API server version)&lt;/li>
&lt;/ul>
&lt;p>Action:&lt;/p>
&lt;ul>
&lt;li>Upgrade &lt;code>gardener-apiserver&lt;/code> to &lt;strong>1.38&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h4 id="gardener-controller-manager-gardener-scheduler-gardener-admission-controller-gardenlet-1">gardener-controller-manager, gardener-scheduler, gardener-admission-controller, gardenlet&lt;/h4>
&lt;p>Pre-requisites:&lt;/p>
&lt;ul>
&lt;li>The &lt;code>gardener-apiserver&lt;/code> instances these components communicate with are at &lt;strong>1.38&lt;/strong> (in multi-instance setups in which these components can communicate with any &lt;code>gardener-apiserver&lt;/code> instance in the cluster, all &lt;code>gardener-apiserver&lt;/code> instances must be upgraded before upgrading these components)&lt;/li>
&lt;/ul>
&lt;p>Action:&lt;/p>
&lt;ul>
&lt;li>Upgrade &lt;code>gardener-controller-manager&lt;/code>, &lt;code>gardener-scheduler&lt;/code>, &lt;code>gardener-admission-controller&lt;/code>, and &lt;code>gardenlet&lt;/code> to &lt;strong>1.38&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="supported-kubernetes-versions">Supported Kubernetes Versions&lt;/h2>
&lt;p>Please refer to &lt;a href="https://gardener.cloud/docs/gardener/usage/supported_k8s_versions/">this document&lt;/a>.&lt;/p></description></item></channel></rss>