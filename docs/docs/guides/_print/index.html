<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.83.1"><link rel=canonical type=text/html href=https://gardener.cloud/docs/guides/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/guides/index.xml><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=icon type=image/x-icon href=https://gardener.cloud/images/favicon.ico><link rel=icon type=image/png href=https://gardener.cloud/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://gardener.cloud/images/favicon-16x16.png sizes=16x16><title>How-To Guides | Gardener</title><meta name=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:title" content="How-To Guides"><meta property="og:description" content="Learn how to execute concrete tasks"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/guides/"><meta itemprop=name content="How-To Guides"><meta itemprop=description content="Learn how to execute concrete tasks"><meta name=twitter:card content="summary"><meta name=twitter:title content="How-To Guides"><meta name=twitter:description content="Learn how to execute concrete tasks"><link rel=preload href=/scss/main.min.122f9effc36493edf1f25030c2ce7965b16b3b0eaeb02528b9a50e0fa9110c15.css as=style><link href=/scss/main.min.122f9effc36493edf1f25030c2ce7965b16b3b0eaeb02528b9a50e0fa9110c15.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://unpkg.com/lunr@2.3.8/lunr.min.js integrity=sha384-vRQ9bDyE0Wnu+lMfm57BlYLO0/XauFuKpVsZPs7KEDwYKktWi5+Kz3MP8++DFlRY crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.e805037dd6e84808bb2bcdf45389e524.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/guides/>Return to the regular view of this page</a>.</p></div><h1 class=title>How-To Guides</h1><div class=lead>Learn how to execute concrete tasks</div><div class=content></div></div><div class=td-content><h1 id=pg-ed30d4942f1afb26454fab9f11a445e1>1 - Set Up Client Tools</h1></div><div class=td-content><h1 id=pg-33170d28a5e3a076ed0ed50fed19149e>1.1 - Automated deployment</h1><div class=lead>Automated deployment with kubectl</div><h2 id=introduction>Introduction</h2><p>With kubectl you can easily deploy an image from your local environment.</p><p>However, what if you want to use a automated deployment script on a CI server (e.g. Jenkins), but don&rsquo;t want to store
the KUBECONFIG on that server?</p><p>You can use kubectl and connect to the API-server of your cluster.</p><h2 id=prerequisites>Prerequisites</h2><ol><li><p>Create a service account user</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>kubectl create serviceaccount deploy-user -n default
</code></pre></div></li><li><p>Bind a role to the newly created serviceuser</p><blockquote><p><strong>!!! Warning !!!</strong> In this example the preconfigured role <code>edit</code> and the namespace <code>default</code> is being used, please adjust the role to a more strict scope! see <a href=https://kubernetes.io/docs/admin/authorization/rbac/>https://kubernetes.io/docs/admin/authorization/rbac/</a></p></blockquote><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>kubectl create rolebinding deploy-default-role --clusterrole=edit --serviceaccount=default:deploy-user --namespace=default
</code></pre></div></li><li><p>Get the URL of your API-server</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>APISERVER=$(kubectl config view | grep server | cut -f 2- -d &#34;:&#34; | tr -d &#34; &#34;)
</code></pre></div></li><li><p>Get the service account</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>SERVICEACCOUNT=$(kubectl get serviceaccount deploy-user -n default -o=jsonpath={.secrets[0].name})
</code></pre></div></li><li><p>Generate a token for the serviceaccount</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>TOKEN=$(kubectl get secret -n default $SERVICEACCOUNT -o=jsonpath={.data.token} | base64 -D)
</code></pre></div></li></ol><h2 id=usage>Usage</h2><p>You can deploy your app without setting the kubeconfig locally, you just need to pass the environment variables (e.g. store them in the Jenkins credentials store)</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>kubectl --server=${APIServer} --token=${TOKEN} --insecure-skip-tls-verify=true apply --filename myapp.yaml
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-6cfaa64f7452ed600c92ebe35f6920a7>1.2 - Kubeconfig context as bash prompt</h1><div class=lead>Expose the active kubeconfig into bash</div><p>Use the Kubernetes command-line tool, <em>kubectl</em>, to deploy and manage applications on Kubernetes.
Using kubectl, you can inspect cluster resources; create, delete, and update components</p><p><img src=/__resources/howto-kubeconfig-bash_526f23.gif alt=port-forward></p><p>By default, the kubectl configuration is located at <code>~/.kube/config</code>.</p><p>Suppose you have two clusters, one for development work and one for scratch work.</p><p>How to handle this easily without copying the used configuration always to the right place?</p><h2 id=export-the-kubeconfig-enviroment-variable>Export the KUBECONFIG enviroment variable</h2><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>bash$ <span class=nb>export</span> <span class=nv>KUBECONFIG</span><span class=o>=</span>&lt;PATH-TO-M&gt;-CONFIG&gt;/kubeconfig-dev.yaml
</code></pre></div><p>How to determine which cluster is used by the kubectl command?</p><h2 id=determine-active-cluster>Determine active cluster</h2><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>bash$ kubectl cluster-info
Kubernetes master is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com
KubeDNS is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com/api/v1/proxy/namespaces/kube-system/services/kube-dns

To further debug and diagnose cluster problems, use <span class=s1>&#39;kubectl cluster-info dump&#39;</span>.
bash$ 
</code></pre></div><h2 id=display-cluster-in-the-bash---linux-and-alike>Display cluster in the bash - Linux and alike</h2><p>I found this tip on Stackoverflow and find it worth to be added here.
Edit your <code>~/.bash_profile</code> and add the following code snippet to show the current k8s
context in the shell&rsquo;s prompt.</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>prompt_k8s<span class=o>(){</span>
  <span class=nv>k8s_current_context</span><span class=o>=</span><span class=k>$(</span>kubectl config current-context 2&gt; /dev/null<span class=k>)</span>
  <span class=k>if</span> <span class=o>[[</span> <span class=nv>$?</span> -eq <span class=m>0</span> <span class=o>]]</span> <span class=p>;</span> <span class=k>then</span> <span class=nb>echo</span> -e <span class=s2>&#34;(</span><span class=si>${</span><span class=nv>k8s_current_context</span><span class=si>}</span><span class=s2>) &#34;</span><span class=p>;</span> <span class=k>fi</span>
<span class=o>}</span>
 
 
<span class=nv>PS1</span><span class=o>+=</span><span class=s1>&#39;$(prompt_k8s)&#39;</span>

</code></pre></div><p>After this your bash command prompt contains the active KUBECONFIG context and you always know
which cluster is active - <em>develop</em> or <em>production</em>.</p><p>e.g.</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>bash$ <span class=nb>export</span> <span class=nv>KUBECONFIG</span><span class=o>=</span>/Users/d023280/Documents/workspace/gardener-ui/kubeconfig_gardendev.yaml 
bash <span class=o>(</span>garden_dev<span class=o>)</span>$ 
</code></pre></div><p>Note the <strong>(garden_dev)</strong> prefix in the bash command prompt.</p><p><strong>This helps immensely to avoid thoughtless mistakes.</strong></p><h2 id=display-cluster-in-the-powershell---windows>Display cluster in the PowerShell - Windows</h2><p>Display current k8s cluster in the title of PowerShell window.</p><p>Create a <a href=https://superuser.com/a/1045659>profile</a> file for your shell under <code>%UserProfile%\Documents\Windows­PowerShell\Microsoft.PowerShell_profile.ps1</code></p><p>Copy following code to <code>Microsoft.PowerShell_profile.ps1</code></p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh> <span class=k>function</span> prompt_k8s <span class=o>{</span>
     <span class=nv>$k8s_current_context</span> <span class=o>=</span> <span class=o>(</span>kubectl config current-context<span class=o>)</span> <span class=p>|</span> Out-String
     <span class=k>if</span><span class=o>(</span><span class=nv>$?</span><span class=o>)</span> <span class=o>{</span>
         <span class=k>return</span> <span class=nv>$k8s_current_context</span>
     <span class=o>}</span><span class=k>else</span> <span class=o>{</span>
         <span class=k>return</span> <span class=s2>&#34;No K8S contenxt found&#34;</span>
     <span class=o>}</span>
 <span class=o>}</span>

 <span class=nv>$host</span>.ui.rawui.WindowTitle <span class=o>=</span> prompt_k8s
</code></pre></div><p><img src=/__resources/howto-bash_kubeconfig_powershell_bccbee.png alt=port-forward></p><p>If you want to switch to different cluster, you can set <code>KUBECONFIG</code> to new value, and re-run the file <code>Microsoft.PowerShell_profile.ps1</code></p></div><div class=td-content style=page-break-before:always><h1 id=pg-4811effaf016a84906632f2b2585ac68>1.3 - Organizing Access Using kubeconfig Files</h1><h1 id=organizing-access-using-kubeconfig-files>Organizing Access Using kubeconfig Files</h1><p>The kubectl command-line tool uses <code>kubeconfig</code> files to find the information it needs to choose a cluster and
communicate with the API server of a cluster.</p><h2 id=problem>Problem</h2><p>If you&rsquo;ve become aware of a security breach that affects you, you may want to revoke or cycle credentials
in case anything was leaked. However, this is not possible with the initial or master <code>kubeconfig</code> from your
cluster.</p><p><img src=/__resources/teaser_52b532.svg alt=teaser></p><h2 id=pitfall>Pitfall</h2><p>Never distribute the <code>kubeconfig</code>, which you can download directly within the Gardener dashboard, for a productive cluster.</p><p><img src=/__resources/kubeconfig-initial_bd3079.png alt=kubeconfig-dont></p><h2 id=create-custom-kubeconfig-file-for-each-user>Create custom kubeconfig file for each user</h2><p>Create a separate <code>kubeconfig</code> for each user. One of the big advantages is, that you can revoke them and control
the permissions better. A limitation to single namespaces is also possible here.</p><p>The script creates a new <code>ServiceAccount</code> with read privileges in the whole cluster (Secretes are excluded).
To run the script <a href=https://stedolan.github.io/jq/>jq</a>, a lightweight and flexible command-line JSON processor, must
be installed.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=cp>#!/bin/bash
</span><span class=cp></span>
<span class=k>if</span> <span class=o>[[</span> -z <span class=s2>&#34;</span><span class=nv>$1</span><span class=s2>&#34;</span> <span class=o>]]</span> <span class=p>;</span><span class=k>then</span>
  <span class=nb>echo</span> <span class=s2>&#34;usage: </span><span class=nv>$0</span><span class=s2> &lt;username&gt;&#34;</span>
  <span class=nb>exit</span> <span class=m>1</span>
<span class=k>fi</span>

<span class=nv>user</span><span class=o>=</span><span class=nv>$1</span>
kubectl create sa <span class=si>${</span><span class=nv>user</span><span class=si>}</span>
<span class=nv>secret</span><span class=o>=</span><span class=k>$(</span>kubectl get sa <span class=si>${</span><span class=nv>user</span><span class=si>}</span> -o json <span class=p>|</span> jq -r .secrets<span class=o>[]</span>.name<span class=k>)</span>
kubectl get secret <span class=si>${</span><span class=nv>secret</span><span class=si>}</span> -o json <span class=p>|</span> jq -r <span class=s1>&#39;.data[&#34;ca.crt&#34;]&#39;</span> <span class=p>|</span> base64 -D &gt; ca.crt

<span class=nv>user_token</span><span class=o>=</span><span class=k>$(</span>kubectl get secret <span class=si>${</span><span class=nv>secret</span><span class=si>}</span> -o json <span class=p>|</span> jq -r <span class=s1>&#39;.data[&#34;token&#34;]&#39;</span> <span class=p>|</span> base64 -D<span class=k>)</span>
<span class=nv>c</span><span class=o>=</span><span class=sb>`</span>kubectl config current-context<span class=sb>`</span>
<span class=nv>cluster_name</span><span class=o>=</span><span class=sb>`</span>kubectl config get-contexts <span class=nv>$c</span> <span class=p>|</span> awk <span class=s1>&#39;{print $3}&#39;</span> <span class=p>|</span> tail -n 1<span class=sb>`</span>
<span class=nv>endpoint</span><span class=o>=</span><span class=sb>`</span>kubectl config view -o <span class=nv>jsonpath</span><span class=o>=</span><span class=s2>&#34;{.clusters[?(@.name == \&#34;</span><span class=si>${</span><span class=nv>cluster_name</span><span class=si>}</span><span class=s2>\&#34;)].cluster.server}&#34;</span><span class=sb>`</span>

<span class=c1># Set up the config</span>
<span class=nv>KUBECONFIG</span><span class=o>=</span>k8s-<span class=si>${</span><span class=nv>user</span><span class=si>}</span>-conf kubectl config set-cluster <span class=si>${</span><span class=nv>cluster_name</span><span class=si>}</span> <span class=se>\
</span><span class=se></span>    --embed-certs<span class=o>=</span><span class=nb>true</span> <span class=se>\
</span><span class=se></span>    --server<span class=o>=</span><span class=si>${</span><span class=nv>endpoint</span><span class=si>}</span> <span class=se>\
</span><span class=se></span>    --certificate-authority<span class=o>=</span>./ca.crt

<span class=nv>KUBECONFIG</span><span class=o>=</span>k8s-<span class=si>${</span><span class=nv>user</span><span class=si>}</span>-conf kubectl config set-credentials <span class=si>${</span><span class=nv>user</span><span class=si>}</span>-<span class=si>${</span><span class=nv>cluster_name</span><span class=p>#cluster-</span><span class=si>}</span> --token<span class=o>=</span><span class=si>${</span><span class=nv>user_token</span><span class=si>}</span>
<span class=nv>KUBECONFIG</span><span class=o>=</span>k8s-<span class=si>${</span><span class=nv>user</span><span class=si>}</span>-conf kubectl config set-context <span class=si>${</span><span class=nv>user</span><span class=si>}</span>-<span class=si>${</span><span class=nv>cluster_name</span><span class=p>#cluster-</span><span class=si>}</span> <span class=se>\
</span><span class=se></span>    --cluster<span class=o>=</span><span class=si>${</span><span class=nv>cluster_name</span><span class=si>}</span> <span class=se>\
</span><span class=se></span>    --user<span class=o>=</span><span class=si>${</span><span class=nv>user</span><span class=si>}</span>-<span class=si>${</span><span class=nv>cluster_name</span><span class=p>#cluster-</span><span class=si>}</span>
<span class=nv>KUBECONFIG</span><span class=o>=</span>k8s-<span class=si>${</span><span class=nv>user</span><span class=si>}</span>-conf kubectl config use-context <span class=si>${</span><span class=nv>user</span><span class=si>}</span>-<span class=si>${</span><span class=nv>cluster_name</span><span class=p>#cluster-</span><span class=si>}</span>

cat <span class=s>&lt;&lt;EOF | kubectl create -f -
</span><span class=s>apiVersion: rbac.authorization.k8s.io/v1
</span><span class=s>kind: ClusterRoleBinding
</span><span class=s>metadata:
</span><span class=s>  name: view-${user}-global
</span><span class=s>subjects:
</span><span class=s>- kind: ServiceAccount
</span><span class=s>  name: ${user}
</span><span class=s>  namespace: default
</span><span class=s>roleRef:
</span><span class=s>  kind: ClusterRole
</span><span class=s>  name: view
</span><span class=s>  apiGroup: rbac.authorization.k8s.io
</span><span class=s>
</span><span class=s>EOF</span>


<span class=nb>echo</span> <span class=s2>&#34;done! Test with: &#34;</span>
<span class=nb>echo</span> <span class=s2>&#34;export KUBECONFIG=k8s-</span><span class=si>${</span><span class=nv>user</span><span class=si>}</span><span class=s2>-conf&#34;</span>
<span class=nb>echo</span> <span class=s2>&#34;kubectl get pods&#34;</span>
</code></pre></div><p>If <strong>edit</strong> or <strong>admin</strong> rights are to be assigned, the <code>ClusterRoleBinding</code> must be adapted in the <code>roleRef</code> section
with the roles listed below.</p><p>Furthermore, you can restrict this to a single namespace by not creating a <code>ClusterRoleBinding</code> but only a <code>RoleBinding</code>
within the desired namespace.</p><table><thead><tr><th>Default ClusterRole</th><th>Default ClusterRoleBinding</th><th>Description</th></tr></thead><tbody><tr><td>cluster-admin</td><td>system:masters group</td><td>Allows super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding&rsquo;s namespace, including the namespace itself.</td></tr><tr><td>admin</td><td>None</td><td>Allows admin access, intended to be granted within a namespace using a RoleBinding. If used in a RoleBinding, allows read/write access to most resources in a namespace, including the ability to create roles and rolebindings within the namespace. It does not allow write access to resource quota or to the namespace itself.</td></tr><tr><td>edit</td><td>None</td><td>Allows read/write access to most objects in a namespace. It does not allow viewing or modifying roles or rolebindings.</td></tr><tr><td>view</td><td>None</td><td>Allows read-only access to see most objects in a namespace. It does not allow viewing roles or rolebindings. It does not allow viewing secrets, since those are escalating.</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-acb3d3deb29a8ddc337a39814f4ddbcb>1.4 - Use a Helm chart to deploy some application or service</h1><p>Basically, <a href=https://helm.sh/docs/topics/charts/>Helm Charts</a> can be installed as described e.g. in the Helm
<a href=https://helm.sh/docs/intro/quickstart/>QuickStart Guide</a>. However, our clusters come with
<a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/>RBAC</a> enabled by default hence Helm must be installed as follows:</p><h2 id=create-a-service-account>Create a Service Account</h2><p>Create a service account via the following command:</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>cat <span class=s>&lt;&lt;EOF | kubectl create -f -
</span><span class=s>apiVersion: v1
</span><span class=s>kind: ServiceAccount
</span><span class=s>metadata:
</span><span class=s> name: helm
</span><span class=s> namespace: kube-system
</span><span class=s>---
</span><span class=s>apiVersion: rbac.authorization.k8s.io/v1beta1
</span><span class=s>kind: ClusterRoleBinding
</span><span class=s>metadata:
</span><span class=s> name: helm
</span><span class=s>roleRef:
</span><span class=s> apiGroup: rbac.authorization.k8s.io
</span><span class=s> kind: ClusterRole
</span><span class=s> name: cluster-admin
</span><span class=s>subjects:
</span><span class=s> - kind: ServiceAccount
</span><span class=s>   name: helm
</span><span class=s>   namespace: kube-system
</span><span class=s>EOF</span>
</code></pre></div><h2 id=initialize-helm>Initialize Helm</h2><p>Initialise Helm via <code>helm init --service-account helm</code>. You can now use <code>helm</code>.</p><h2 id=in-case-of-failure>In case of failure</h2><p>In case you have already executed <code>helm init</code>, but without the above service account, you will get the following error:
<code>Error: User "system:serviceaccount:kube-system:default" cannot list configmaps in the namespace "kube-system". (get configmaps)</code> (e.g. when you run <code>helm list</code>). You will now need to delete the Tiller deployment (Helm backend
implicitly deployed to the Kubernetes cluster when you call <code>helm init</code>) as well as the local Helm files (usually
<code>$HELM_HOME</code> is set to <code>~/.helm</code>):</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>kubectl delete deployment tiller-deploy --namespace<span class=o>=</span>kube-system
kubectl delete service tiller-deploy --namespace<span class=o>=</span>kube-system 
rm -rf ~/.helm/
</code></pre></div><p>Now follow the instructions above. For more details see this
<a href=https://github.com/kubernetes/helm/issues/2687>Kubernetes Helm issue #2687</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f7165d236b09de8503d3a17a03402a0e>2 - Install Gardener</h1></div><div class=td-content><h1 id=pg-8f3887d5e8aba7697aef426ea1bf6e5b>2.1 - Gardener Certificate Management</h1><div class=lead>Configure Certificate Management For Shoot Clusters</div><h1 id=gardener-certificate-management>Gardener Certificate Management</h1><h2 id=introduction>Introduction</h2><p>Gardener comes with an extension that enables shoot owners to request X.509 compliant certificates for shoot domains.</p><h2 id=extension-installation>Extension Installation</h2><p>The <code>Shoot-Cert-Service</code> extension can be deployed and configured via Gardener&rsquo;s native resource <a href=/docs/concepts/extensions/controllerregistration>ControllerRegistration</a>.</p><h3 id=prerequisites>Prerequisites</h3><p>To let the <code>Shoot-Cert-Service</code> operate properly, you need to have:</p><ul><li>a <a href=https://github.com/gardener/external-dns-management>DNS service</a> in your seed</li><li>contact details and optionally a private key for a pre-existing <a href=https://letsencrypt.org/>Let&rsquo;s Encrypt</a> account</li></ul><h3 id=controllerregistration>ControllerRegistration</h3><p>An example of a <code>ControllerRegistration</code> for the <code>Shoot-Cert-Service</code> can be found here: <a href=https://github.com/gardener/gardener-extension-shoot-cert-service/blob/master/example/controller-registration.yaml>https://github.com/gardener/gardener-extension-shoot-cert-service/blob/master/example/controller-registration.yaml</a></p><p>The <code>ControllerRegistration</code> contains a Helm chart which eventually deploy the <code>Shoot-Cert-Service</code> to seed clusters. It offers some configuration options, mainly to set up a default issuer for shoot clusters. With a default issuer, pre-existing Let&rsquo;s Encrypt accounts can be used and shared with shoot clusters (See &ldquo;One Account or Many?&rdquo; of the <a href=https://letsencrypt.org/docs/integration-guide/>Integration Guide</a>).</p><blockquote><p>Please keep the Let&rsquo;s Encrypt <a href=https://letsencrypt.org/docs/rate-limits/>Rate Limits</a> in mind when using this shared account model. Depending on the amount of shoots and domains it is recommended to use an account with increased rate limits.</p></blockquote><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>core.gardener.cloud/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ControllerRegistration</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span><span class=w>  </span><span class=nt>values</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>certificateConfig</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>defaultIssuer</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>acme</span><span class=p>:</span><span class=w>
</span><span class=w>            </span><span class=nt>email</span><span class=p>:</span><span class=w> </span><span class=l>foo@example.com</span><span class=w>
</span><span class=w>            </span><span class=nt>privateKey</span><span class=p>:</span><span class=w> </span><span class=p>|-</span><span class=sd>
</span><span class=sd>            -----BEGIN RSA PRIVATE KEY-----
</span><span class=sd>            ...
</span><span class=sd>            -----END RSA PRIVATE KEY-----
</span><span class=sd>            server: https://acme-v02.api.letsencrypt.org/directory</span><span class=w>            
</span><span class=w>        </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>default-issuer</span><span class=w>
</span><span class=w></span><span class=c>#       restricted: true # restrict default issuer to any sub-domain of shoot.spec.dns.domain</span><span class=w>
</span><span class=w>
</span><span class=w></span><span class=c>#     defaultRequestsPerDayQuota: 50</span><span class=w>
</span><span class=w>
</span><span class=w></span><span class=c>#     precheckNameservers: 8.8.8.8,8.8.4.4</span><span class=w>
</span><span class=w>
</span><span class=w></span><span class=c>#     caCertificates: | # optional custom CA certificates when using private ACME provider</span><span class=w>
</span><span class=w></span><span class=c>#     -----BEGIN CERTIFICATE-----</span><span class=w>
</span><span class=w></span><span class=c>#     ...</span><span class=w>
</span><span class=w></span><span class=c>#     -----END CERTIFICATE-----</span><span class=w>
</span><span class=w></span><span class=c>#</span><span class=w>
</span><span class=w></span><span class=c>#     -----BEGIN CERTIFICATE-----</span><span class=w>
</span><span class=w></span><span class=c>#     ...</span><span class=w>
</span><span class=w></span><span class=c>#     -----END CERTIFICATE-----</span><span class=w>
</span><span class=w>
</span><span class=w>      </span><span class=nt>shootIssuers</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>enabled</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w> </span><span class=c># if true, allows to specify issuers in the shoot clusters</span><span class=w>
</span><span class=w>
</span></code></pre></div><h4 id=enablement>Enablement</h4><p>If the <code>Shoot-Cert-Service</code> should be enabled for every shoot cluster in your Gardener managed environment, you need to globally enable it in the <code>ControllerRegistration</code>:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>core.gardener.cloud/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ControllerRegistration</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span><span class=w>  </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>globallyEnabled</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>    </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Extension</span><span class=w>
</span><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>shoot-cert-service</span><span class=w>
</span></code></pre></div><p>Alternatively, you&rsquo;re given the option to only enable the service for certain shoots:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Shoot</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>core.gardener.cloud/v1beta1</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>extensions</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>shoot-cert-service</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-c9046c7017ce6bf2d170fafb306feca9>2.2 - Gardener DNS Management for Shoots</h1><div class=lead>Configure DNS Management For Shoot Clusters</div><h1 id=gardener-dns-management-for-shoots>Gardener DNS Management for Shoots</h1><h2 id=introduction>Introduction</h2><p>Gardener allows Shoot clusters to request DNS names for Ingresses and Services out of the box.
To support this the gardener must be installed with the <code>shoot-dns-service</code>
extension.
This extension uses the seed&rsquo;s dns management infrastructure to maintain DNS
names for shoot clusters. So, far only the external DNS domain of a shoot
(already used for the kubernetes api server and ingress DNS names) can be used
for managed DNS names.</p><h2 id=configuration>Configuration</h2><p>A general description for configuring the DNS management of the
gardener can be found <a href=/docs/concepts/extensions/dns>here</a>.</p><p>To generally enable the DNS management for shoot objects the
<code>shoot-dns-service</code> extension must be registered by providing an
appropriate <a href=https://github.com/gardener/gardener-extension-shoot-dns-service/blob/master/example/controller-registration.yaml>extension registration</a> in the garden cluster.</p><p>Here it is possible to decide whether the extension should be always available
for all shoots or whether the extension must be separately enabled per shoot.</p><p>If the extension should be used for all shoots the registration must set the <em>globallyEnabled</em> flag to <code>true</code>.</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Extension</span><span class=w>
</span><span class=w>      </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>shoot-dns-service</span><span class=w>
</span><span class=w>      </span><span class=nt>globallyEnabled</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></code></pre></div><h3 id=providing-base-domains-usable-for-a-shoot>Providing Base Domains usable for a Shoot</h3><p>So, far only the external DNS domain of a shoot already used
for the kubernetes api server and ingress DNS names can be used for managed
DNS names. This is either the shoot domain as subdomain of the default domain
configured for the gardener installation, or a dedicated domain with dedicated
access credentials configured for a dedicated shoot via the shoot manifest.</p><p>Alternatively, you can specify <code>DNSProviders</code> and its credentials
<code>Secret</code> directly in the shoot, if this feature is enabled.
By default, <code>DNSProvider</code> replication is disabled, but it can be enabled globally in the <code>ControllerDeployment</code>
or for a shoot cluster in the shoot manifest (details see further below).</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>core.gardener.cloud/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ControllerDeployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>extension-shoot-dns-service</span><span class=w>
</span><span class=w></span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>helm</span><span class=w>
</span><span class=w></span><span class=nt>providerConfig</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>chart</span><span class=p>:</span><span class=w> </span><span class=l>...</span><span class=w>
</span><span class=w>  </span><span class=nt>values</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=l>...</span><span class=w>
</span><span class=w>    </span><span class=nt>dnsProviderReplication</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>enabled</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></code></pre></div><p>See <a href=https://github.com/gardener/external-dns-management/tree/master/examples>example files (20-* and 30-*)</a>
for details for the various provider types.</p><h3 id=shoot-feature-gate>Shoot Feature Gate</h3><p>If the shoot DNS feature is not globally enabled by default (depends on the
extension registration on the garden cluster), it must be enabled per shoot.</p><p>To enable the feature for a shoot, the shoot manifest must explicitly add the
<code>shoot-dns-service</code> extension.</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nn>...</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>extensions</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>shoot-dns-service</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span></code></pre></div><h4 id=enabledisable-dns-provider-replication-for-a-shoot>Enable/disable DNS provider replication for a shoot</h4><p>The DNSProvider` replication feature enablement can be overwritten in the
shoot manifest, e.g.</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>Kind</span><span class=p>:</span><span class=w> </span><span class=l>Shoot</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>extensions</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>shoot-dns-service</span><span class=w>
</span><span class=w>      </span><span class=nt>providerConfig</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>service.dns.extensions.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w>        </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>DNSConfig</span><span class=w>
</span><span class=w>        </span><span class=nt>dnsProviderReplication</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>enabled</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-009818d3feade374ae615681087e6434>2.3 - Hardening the Gardener Community Setup</h1><h1 id=hardening-the-gardener-community-setup>Hardening the Gardener Community Setup</h1><h2 id=context>Context</h2><p>Gardener stakeholders in the Open Source community usually use the <a href=https://github.com/gardener/landscape-setup>Gardener Setup Scripts</a>, to create a Garden cluster based on Kubernetes v1.9 which then can be used to create Shoot clusters based on Kubernetes v1.10, v1.11 and v1.12. Shoot clusters can play the following roles in a Gardener landscape:</p><ul><li>Seed cluster</li><li>Shoot cluster</li></ul><p>As Alban Crequy from Kinvolk has recommended in his recent Gardener blog <a href=/docs/guides/applications/insecure-configuration>Auditing Kubernetes for Secure Setup</a> the Gardener Team at SAP has applied several means to harden the Gardener landscapes at SAP.</p><h2 id=recommendations>Recommendations</h2><h3 id=mitigation-for-gardener-cve-2018-2475>Mitigation for Gardener CVE-2018-2475</h3><p>The following recommendations describe how you can harden your Gardener Community Setup by adding a Seed cluster hardened with network policies.</p><ul><li>Use the Gardener Setup Scripts to create a Garden cluster in a dedicated IaaS account</li><li>Create a Shoot cluster in a different IaaS account</li><li>As a precaution you should not deploy the Kubernetes dashboard on this Shoot cluster</li><li>Register this newly created Shoot cluster as a Seed cluster in the Gardener</li><li>End user Shoot clusters can then be created using this newly created Seed cluster (which in turn is a Shoot cluster).</li></ul><p>A tutorial on how to create a shooted seed cluster can be found <a href=/docs/guides/install_gardener/setup-seed>here</a>.</p><p>The rational behind this activity is, that Calico network policies harden this Seed cluster but the community installer uses Flannel which does not offer these features for the Garden cluster.</p><p>When you have added a hardened Seed cluster you are expected not be vulnerable to the Gardener <a href=https://groups.google.com/forum/#!topic/gardener/Pom2Y70cDpw>CVE-2018-2475</a> anymore.</p><h3 id=mitigation-for-kubernetes-cve-2018-1002105>Mitigation for Kubernetes CVE-2018-1002105</h3><p>In addition when you follow the recommendations in the <a href=https://groups.google.com/forum/#!topic/gardener/2icxEz0RAK4>recent Gardener Security Announcement</a> you are expected not be vulnerable to the Kubernetes CVE-2018-1002105 with your hardened Gardener Community Setup.</p><h2 id=alternative-approach>Alternative Approach</h2><p>For this alternative approach there is no Gardener blog available, it is not part of the Gardener Setup Scripts, but it was tested by the Gardener Team at SAP. Use GKE to host a Garden cluster based on Kubernetes v1.10, v1.11 and v1.12 (without the Kubernetes dashboard) in a dedicated GCP account. If you do this by your own, please ensure that the network policies are turned on, which might not be the case by default. Then you can apply the security configuration which Alban Crequy from Kinvolk has recommended in his <a href=/docs/guides/applications/insecure-configuration>blog</a> directly in the Garden cluster and create Shoot clusters from there in a different IaaS account.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-867ade3d960e2003b35a223af7b58a46>2.4 - Landscape Setup</h1><h1 id=---deprecated--->&mdash;DEPRECATED&mdash;</h1><p><strong>This project is outdated and won&rsquo;t be updated anymore. Please use <a href=https://github.com/gardener/garden-setup>https://github.com/gardener/garden-setup</a> instead!</strong></p><h1 id=gardener-setup-scripts>Gardener Setup Scripts</h1><p>This README is the installation manual for a simple Gardener setup. The installation scripts in this repo are embedded in a configuration template in the <a href=https://github.com/gardener/landscape-setup-template>landscape-setup-template</a> project. You can find further information there.</p><p>We do recommend this simplified setup for demonstration purposes only. For productive workloads we do recommend that all components (Gardener/Seed/Shoot) run in their own IaaS accounts and that network policies are enabled and properly tested on the seed clusters. A documentation on how to do this is currently work in progress.</p><ul><li><a href=#---deprecated--->&mdash;DEPRECATED&mdash;</a></li><li><a href=#gardener-setup-scripts>Gardener Setup Scripts</a></li><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#gardener-installation>Gardener Installation</a><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#kubectl-aliases>Kubectl Aliases</a></li></ul></li><li><a href=#step-1-clone-the-repositories-and-get-dependencies>Step 1: Clone the Repositories and get Dependencies</a><ul><li><a href=#submodule-management>Submodule Management</a></li></ul></li><li><a href=#step-2-configure-the-landscape>Step 2: Configure the Landscape</a><ul><li><a href=#building-the-landscapeyaml-file>Building the &lsquo;landscape.yaml&rsquo; File</a></li><li><a href=#the-base-cluster>The Base Cluster</a><ul><li><a href=#kubify>Kubify</a></li><li><a href=#shoot-cluster>Shoot Cluster</a></li><li><a href=#using-an-arbitrary-base-cluster>Using an Arbitrary Base Cluster</a></li></ul></li></ul></li><li><a href=#step-3-build-and-run-docker-container>Step 3: Build and Run Docker Container</a></li><li><a href=#step-4-10-deploying-components>Step 4-10: Deploying Components</a><ul><li><a href=#undeploying-components>Undeploying Components</a></li><li><a href=#the-all-component>The &lsquo;all&rsquo; Component</a></li></ul></li><li><a href=#step-4-10-deploying-components-detailed>Step 4-10: Deploying Components (detailed)</a><ul><li><a href=#step-4-kubify--etcd>Step 4: Kubify / etcd</a></li><li><a href=#step-5-generate-certificates>Step 5: Generate Certificates</a></li><li><a href=#step-6-deploy-tiller>Step 6: Deploy tiller</a></li><li><a href=#step-7-deploy-gardener>Step 7: Deploy Gardener</a></li><li><a href=#step-8-register-garden-cluster-as-seed-cluster>Step 8: Register Garden Cluster as Seed Cluster</a><ul><li><a href=#configuring-additional-seeds>Configuring Additional Seeds</a></li><li><a href=#creating-a-shoot>Creating a Shoot</a></li></ul></li><li><a href=#step-9-install-identity-and-dashboard>Step 9: Install Identity and Dashboard</a><ul><li><a href=#create-cname-entry>Create CNAME Entry</a></li></ul></li><li><a href=#step-10-apply-valid-certificates>Step 10: Apply Valid Certificates</a></li><li><a href=#letsencrypt-quota-limits>Letsencrypt Quota Limits</a><ul><li><a href=#accessing-the-dashboard>Accessing the Dashboard</a></li></ul></li></ul></li></ul></li><li><a href=#tearing-down-the-landscape>Tearing Down the Landscape</a></li><li><a href=#cleanup>Cleanup</a></li></ul><h1 id=prerequisites>Prerequisites</h1><p>Before getting started make sure you have the following at hand:</p><ul><li>You need a cloud account with sufficient quota to set up a Kubernetes cluster with a couple of VMs. <strong>The Gardener supports AWS, Azure, GCP, and Openstack, but this simplified setup currently only supports AWS and Openstack.</strong></li><li>A Linux machine (virtual machine is fine) or a Mac with basic tools such as a git client and the Docker runtime installed.</li></ul><h1 id=gardener-installation>Gardener Installation</h1><p>Follow these steps to install Gardener. Do not proceed to the next step in case of errors.</p><h2 id=tldr>TL;DR</h2><p>If you are already familiar with the installation procedure and just want a short summary of the commands you have to use, here it is:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># setup</span>
git clone  --recursive https://github.com/gardener/landscape-setup-template.git landscape
<span class=c1># fill in landscape/landscape_config.yaml now</span>
<span class=nb>cd</span> landscape/setup
./docker_run.sh
deploy all

<span class=c1># -------------------------------------------------------------------</span>

<span class=c1># teardown</span>
undeploy all
./cleanup.sh
</code></pre></div><p>Otherwise, follow the detailed guide below.</p><h3 id=kubectl-aliases>Kubectl Aliases</h3><p>The following aliases can be used within the docker container:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>k =&gt; kubectl
ks =&gt; kubectl -n kube-system
kg =&gt; kubectl -n garden
kn =&gt; kubectl -n
ka =&gt; kubectl get --all-namespaces
</code></pre></div><p>Bash completion works for all of them except for <code>ka</code>.</p><h2 id=step-1-clone-the-repositories-and-get-dependencies>Step 1: Clone the Repositories and get Dependencies</h2><p>Get the <code>landscape-setup-template</code> from GitHub and initialize the submodules:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>git clone  --recursive https://github.com/gardener/landscape-setup-template.git landscape
<span class=nb>cd</span> landscape
</code></pre></div><p>After step 2, this repository will contain all passwords and keys for your landscape. You will be in trouble if you loose them so we recommend that you store this landscape configuration in a private repository. It might be a good idea to change the origin so you do not accidentally publish your secrets to the public template repository.</p><h3 id=submodule-management>Submodule Management</h3><p>This project needs the <a href=https://github.com/gardener/gardener>Gardener</a> and <a href=https://github.com/gardener/dashboard>dashboard</a> as submodules. To avoid conflicts between the checked out versions and the ones specified in the <code>landscape_base.yaml</code> file, automatic version management has been added. As long as the <code>managed</code> field in the chart area of each submodule is set to <code>true</code>, the version specified in the <code>tag</code> field will be checked out before deploying.</p><p>To check vor the correct version, the <code>VERSION</code> file in the submodule&rsquo;s main folder is read and compared to the tag in the config file. If the <code>VERSION</code> file doesn&rsquo;t exist, the component is added as a submodule. If it exists, but the versions differ, the fitting version will be checked out. If it exists and the versions are identical, nothing is done. The automatic version management will fail if a) the component is already added as a submodule, but the <code>VERSION</code> file is missing or b) the landscape folder is not a git repo.</p><p>It is also possible to trigger the version update manually: call the <code>manage_submodule</code> function with <code>gardener</code> or <code>dashboard</code> as an argument, or run the <code>manage_submodules.sh</code> script which will update Gardener and dashboard. Both will only work from inside the docker container / with sourced <code>init.sh</code> file.</p><h2 id=step-2-configure-the-landscape>Step 2: Configure the Landscape</h2><p>There is a <code>landscape_config.yaml</code> file in the landscape project. This is the only file that you need to modify - all other configuration files will be derived from this and the <code>landscape_base.yaml</code> file. The latter one contains the merging instructions as well as technical configurations and it shouldn&rsquo;t be touched unless you know what you are doing.</p><h4 id=building-the-landscapeyaml-file>Building the &lsquo;landscape.yaml&rsquo; File</h4><p>Both config files - <code>landscape_config.yaml</code> and <code>landscape_base.yaml</code> - are merged into one <code>landscape.yaml</code> file which is then used as configuration for the scripts. Sourcing the <code>init.sh</code> file (which happens automatically when entering the docker image) will perform this merge <strong>unless the file already exists.</strong> This means if you change something in one of the original config files after the <code>landscape.yaml</code> file has already been created, you need to manually rebuild it in order for the changes to take effect.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>./build_landscape_yaml.sh
</code></pre></div><p>This script will recreate the <code>landscape.yaml</code> file. It will also source the <code>init.sh</code> file again, as some of the environment variables are extracted from this file.</p><h4 id=the-base-cluster>The Base Cluster</h4><p>Gardener extends the Kubernetes apiserver, so in order to deploy it, you need a Kubernetes cluster first. This setup gives you two options for this:</p><h5 id=kubify>Kubify</h5><p>You can use <a href=https://github.com/gardener/kubify>Kubify</a> to create the initial cluster. Kubify uses Terraform to create the cluster and it is integrated into this project - you don&rsquo;t need to create the cluster yourself, just make sure you fill out all relevant parts of the config file.</p><h5 id=shoot-cluster>Shoot Cluster</h5><p>A shoot cluster is a cluster created by a Gardener instance and it can be used as a base cluster for this project. These flags have to be set for the kube-apiserver (if not set, the Gardener will still work, but the dashboard won&rsquo;t):</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml>--<span class=l>oidc-issuer-url=https://identity.ingress.&lt;your cluster domain&gt;</span><span class=w>
</span><span class=w></span>--<span class=l>oidc-client-id=kube-kubectl</span><span class=w>
</span><span class=w></span>--<span class=l>oidc-username-claim=email</span><span class=w>
</span><span class=w></span>--<span class=l>oidc-groups-claim=groups</span><span class=w>
</span></code></pre></div><p>For a shoot this can be done by setting <code>issuerUrl</code>, <code>clientID</code>, <code>usernameClaim</code>, and <code>groupsClaim</code> in <code>spec.kubernetes.kubeAPIServer.oidcConfig</code> in the shoot manifest.</p><p>Also make sure that the CIDRs of your base cluster and the from your Gardener spawned shoots don&rsquo;t overlap - if you want to be able to create shoots from the Gardener dashboard later, then don&rsquo;t use the default CIDRs for this base cluster.</p><p>Some fields in the <code>landscape_config.yaml</code> are marked with <code># kubify only</code>, they can be ignored when using a shoot as the base cluster. The etcd server address defaults to the address for Kubify and needs to be changed for the shoot setup (see the comments in the config file).</p><p>The <em>kubeconfig</em> for the base cluster is expected to be named <code>kubeconfig</code> and reside in the directory containing this project&rsquo;s directory (next to the <code>landscape_config.yaml</code> file).</p><h5 id=using-an-arbitrary-base-cluster>Using an Arbitrary Base Cluster</h5><p>While this setup has only been tested for clusters created by Kubify or the Gardener (shoot clusters), it is theoretically possible to use the shoot setup method to deploy the Gardener to an arbitrary kubernetes cluster.</p><h2 id=step-3-build-and-run-docker-container>Step 3: Build and Run Docker Container</h2><p>First, <code>cd</code> into the folder containing this project.</p><p>Then run the container:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>./docker_run.sh
</code></pre></div><p>After this,</p><ul><li>you will be connected to the container via an interactive shell</li><li>the landscape folder will be mounted in that container</li><li>your current working directory will be <code>setup</code> folder</li><li><code>setup/init.sh</code> is sourced, meaning<ul><li>the environment variables will be set</li><li>kubectl will be configured to communicate with your cluster</li><li><code>landscape.yaml</code> file will have been created if it didn&rsquo;t exist before</li></ul></li></ul><p>The <code>docker_run.sh</code> script searches for the image locally and pulls it from an image repository, if it isn&rsquo;t found.
If pulling the image doesn&rsquo;t work for whatever reason, you can use the <code>docker_build.sh</code> script to build the image locally.</p><h2 id=step-4-10-deploying-components>Step 4-10: Deploying Components</h2><p>The Gardener deployment is splitted into components. A single component can be easily deployed using</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy &lt;component name&gt;
</code></pre></div><p>Please take care that most of the components depend on each other and therefore have to be deployed in the order given below.</p><p>The <code>deploy</code> command is added to the <code>PATH</code> environment variable and can thus be called from anywhere. Bash auto-completion can be used for the component names.</p><h4 id=undeploying-components>Undeploying Components</h4><p>It is also possible to &ldquo;undeploy&rdquo; a component using</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>undeploy &lt;component name&gt;
</code></pre></div><p>Components need to be undeployed in the inverse order. Do not undeploy components without undeploying their successors in the component order first. Take care to delete all shoots before undeploying the <code>gardener</code> or <code>seed-config</code> components (although both undeploy scripts will check for that and trigger a deletion themselves).</p><h4 id=the-all-component>The &lsquo;all&rsquo; Component</h4><p>The <code>all</code> component is a special component: it serves as a dummy to deploy several components in one go. Usually, manual intervention between deploying components is not necessary and most of them are deployed directly one after the other, so the <code>all</code> component makes the &ldquo;normal&rdquo; use-case easier.</p><p>For better control which components are deployed, a component range can be given as an argument. The argument should have the form <code>&lt;start component name>:&lt;end component name></code> and then the start component, the end component, and all components in between will be deployed. The order of the arguments is taken from the environment variables with the <code>$COMPONENT_ORDER_</code> prefix. The variable with the suffix that matches the <code>clusters.base_cluster</code> entry in the config file will be used.</p><p>It is also possible to drop one part of the range or to drop the whole argument. For missing parts the defaults will be used, which are the first and the last component of the active component order, respectively.</p><p>The <code>undeploy</code> command can also be used with the <code>all</code> component, but take care that the component order is inverted.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># Examples</span>
<span class=c1># (start and end component are always inclusive)</span>
deploy all                          <span class=c1># deploys all components</span>
deploy all gardener:dashboard       <span class=c1># deploys &#39;gardener&#39; through &#39;dashboard&#39; </span>
deploy all gardener:                <span class=c1># deploys all components starting from &#39;gardener&#39;</span>
deploy all :gardener                <span class=c1># deploys all components up to &#39;gardener&#39;</span>

<span class=c1># (all undeploy commands use the inverse component order)</span>
undeploy all                        <span class=c1># undeploys all components</span>
undeploy all :helm-tiller           <span class=c1># undeploys all components up to &#39;helm-tiller&#39;</span>
undeploy all dashboard:cert         <span class=c1># undeploys &#39;dashboard&#39; through &#39;cert&#39;</span>
</code></pre></div><h2 id=step-4-10-deploying-components-detailed>Step 4-10: Deploying Components (detailed)</h2><h3 id=step-4-kubify--etcd>Step 4: Kubify / etcd</h3><p>If you want to create a Kubify cluster, deploy the component:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy kubify
</code></pre></div><p>The script will wait some time for the cluster to come up and then partly validate that the cluster is ready.</p><p>If you get errors during the cluster setup, just try to run the command again.</p><p>Once completed the following command should show all deployed pods:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>root@c41327633d6d:/landscape# kubectl get pods --all-namespaces
NAMESPACE       NAME                                                                  READY     STATUS    RESTARTS   AGE
kube-system     etcd-operator-75dcfcf4f7-xkm4h                                        1/1       Running   0          6m
kube-system     heapster-c8fb4f746-tvts6                                              2/2       Running   0          2m
kube-system     kube-apiserver-hcdnc                                                  1/1       Running   0          6m
[...]
</code></pre></div><hr><p>If you already have a cluster, you don&rsquo;t need Kubify. To deploy an etcd in your cluster, run</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy etcd
</code></pre></div><p>This component is not meant to be used in combination with Kubify and might require manual steps to make it work.</p><p>It should also be possible to plug in your own etcd - check the deploy scripts for the <code>etcd</code> and <code>gardener</code> components for information on where to put the certificates, etc.</p><h3 id=step-5-generate-certificates>Step 5: Generate Certificates</h3><p>This step will generate a self-signed cluster CA and sign some certificates with it.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy cert
</code></pre></div><h3 id=step-6-deploy-tiller>Step 6: Deploy tiller</h3><p>Tiller is needed to deploy the Helm charts of Gardener and other components.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy helm-tiller
</code></pre></div><h3 id=step-7-deploy-gardener>Step 7: Deploy Gardener</h3><p>Now we can deploy Gardener. If the previous steps were executed successfully this should be completed in a couple of seconds.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy gardener
</code></pre></div><p>You might see a couple of messages like these:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>Gardener API server not yet reachable. Waiting...
</code></pre></div><p>while the script waits for the Gardener to start. Once Gardener is up when the deployment script finished you can verify the correct setup by running the following command:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>kubectl get shoots
No resources found. 
</code></pre></div><p>As we do not have a seed cluster yet we cannot create any shoot clusters.
The Gardener itself is installed in the <code>garden</code> namespace:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>kubectl get po -n garden
NAME                                          READY     STATUS    RESTARTS   AGE
gardener-apiserver-56cc665667-nvrjl           1/1       Running   0          6m
gardener-controller-manager-5c9f8db55-hfcts   1/1       Running   0          6m
</code></pre></div><h3 id=step-8-register-garden-cluster-as-seed-cluster>Step 8: Register Garden Cluster as Seed Cluster</h3><p>In heterogeneous productive environments one would run Gardener and seed in separate clusters but for simplicity and resource consumption reasons we will register the Gardener cluster that we have just created also as the seed cluster. Make sure that the <code>seed_config</code> in the landscape file is correct and matches the region that you are using. Keep in mind that image ids differ between regions as well. Also, valid credentials for the seed provider have to be specified in the <code>authentication</code> part of the <code>landscape_config.yaml</code> file (the etcd backups of the shoot clusters are stored on the seed).</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy seed-config
</code></pre></div><h4 id=configuring-additional-seeds>Configuring Additional Seeds</h4><p>By default, this step will create a seed for the cloud provider the Gardener has been deployed on and thus creating shoots on this provider will be possible. If you want to create shoots on other cloud providers, you will have to configure additional seeds. There are two options for that:</p><p>If the seed-config deploy script is called without any arguments (as shown above), it will create seeds for all providers specified in the <code>seed_config.seeds</code> section in the <code>landscape_config.yaml</code> file. By default, the only entry in that list is the cloud provider chosen for the Gardener cluster, but you can extend the list.</p><p>It is also possible to provide the seed-config deploy script with additional arguments specifying which seeds should be created. Multiple arguments can be given and the script will ignore the list in the <code>landscape_config.yaml</code> file when called with arguments. Only the specified seeds will be created, already existing seeds are not affected. If a given seed already exists, it will be updated to the current configuration.</p><p>In both cases, the corresponding variant nodes in <code>authentication</code> and <code>seed_config</code> have to be filled out in the config file.</p><p>Valid values for seeds are <code>aws</code>, <code>az</code> (for Azure), <code>gcp</code>, and <code>openstack</code>. Please note, that while it is possible to create seeds for any cloud provider on any cloud provider, shoot creation may not work across cloud providers for every combination. It should always work if seed (Gardener cluster in this setup) and shoot are on the same provider, though.</p><h4 id=creating-a-shoot>Creating a Shoot</h4><p>That&rsquo;s it! If everything went fine you should now be able to create shoot clusters.
You can start with a sample <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot-aws.yaml>manifest</a> and create a shoot cluster by standard Kubernetes means:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl apply -f shoot-aws.yaml
</code></pre></div><h3 id=step-9-install-identity-and-dashboard>Step 9: Install Identity and Dashboard</h3><p>Creating clusters based on a shoot manifest is quite nice but also a little complex. While almost all aspects of a shoot cluster can be configured, it can be quite difficult for beginners, so go on and install the dashboard:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy identity
<span class=o>[</span>...<span class=o>]</span>
deploy dashboard
<span class=o>[</span>...<span class=o>]</span>
</code></pre></div><h4 id=create-cname-entry>Create CNAME Entry</h4><p>Dashboard and identity need a CNAME entry pointing the domain <code>*.ingress.&lt;your cluster domain></code> to your cluster&rsquo;s nginx ingress ip/hostname. Kubify creates this entry automatically. If you are not using kubify to create your base cluster, you can create the CNAME entry with the corresponding component:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy cname
</code></pre></div><p>The script uses the AWS CLI to create the entry, so it will only work for route53.</p><h3 id=step-10-apply-valid-certificates>Step 10: Apply Valid Certificates</h3><p>The following command will install the <a href=https://github.com/jetstack/cert-manager>cert-manager</a> and request valid letsencrypt certificates for both the identity and dashboard ingresses:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy certmanager
</code></pre></div><p>After a few minutes valid certificates should be installed.</p><h3 id=letsencrypt-quota-limits>Letsencrypt Quota Limits</h3><p>Letsencrypt <a href=https://letsencrypt.org/docs/rate-limits/>limits</a> how many certificates you can get for the same host within a short time. To avoid hitting these limits, you can use the letsencrypt staging server for testing, which has a significantly higher rate limit but produces untrusted certificates.<br>The <code>charts.[certmanager].live</code> field in the config file allows to switch between live and staging server (remember to rebuild the <code>landscape.yaml</code> file after you changed something in the <code>landscape_config.yaml</code> file).</p><h4 id=accessing-the-dashboard>Accessing the Dashboard</h4><p>After step 9 you will be able to access the Gardener dashboard. There is a difference in how you access it depending on whether you used the letsencrypt live server or the staging one (and thus have untrusted credentials in the latter case).</p><p>The <code>print_dashboard_urls.sh</code> script constructs two URLs from the domain name given in the <code>landscape.yaml</code> file and prints them.</p><p>If you have trusted certificates, just use the second one (the one for the dashboard) and everything will be fine.</p><p>If you used the letsencrypt staging server, you will need to visit the first link first. Your browser will show a warning regarding untrusted certificates, you need to ignore that warning. You will then see a nearly blank page with some 404 message. After that, you can open the dashboard link, ignore the certificate warning again and should be able to login.
If you skip the first link, you will still be able to see the dashboard, but the login button probably won&rsquo;t work.
While you will be able to login and create projects with the untrusted certificates from the letsencrypt staging server, creating secrets or shoots won&rsquo;t be possible. You&rsquo;ll need trusted certificates for that.</p><p>To log into the dashboard, use the options you have specified in the identity chart part of the <code>landscape_config.yaml</code>.</p><h1 id=tearing-down-the-landscape>Tearing Down the Landscape</h1><p>Make sure that you delete all shoot clusters prior to tearing down the cluster. Not deleting <code>project</code> resources before deleting the Gardener can also cause troubles, because the namespaces associated with the projects have a finalizer which can&rsquo;t be handled anymore when the Gardener is gone. Both, shoots and projects, can be deleted using the <code>delete_all.sh</code> script (give &lsquo;shoots&rsquo; or &lsquo;projects&rsquo; as an argument). To delete a single shoot/project, use <a href=https://github.com/gardener/gardener/blob/master/hack/delete>this</a> script.</p><p>The following command should not return any shoot clusters:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>kubectl get shoots --all-namespaces
No resources found.
</code></pre></div><p>If you created your base cluster with the Kubify component, you can destroy it using the undeploy command:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>undeploy kubify
</code></pre></div><h1 id=cleanup>Cleanup</h1><p>After destroying the Kubify cluster, there will be some files left that prevent you from simply starting the project up again.</p><p><strong>ATTENTION: Only do this if you are sure the cluster has been completely destroyed!</strong>
Since this removes the terraform state, an automated deletion of resources won&rsquo;t be possible anymore - you will have to clean up any leftovers manually.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>./cleanup.sh
</code></pre></div><p>This will reset your landscape folder to its initial state (including the deletion of <code>landscape.yaml</code>).</p><p>The script takes an optional &ldquo;-y&rdquo; argument to skip the confirmation.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d7002605ea6dad33b86146a592a09963>2.5 - Manually adding a node to an existing cluster</h1><div class=lead>How to add a node to an existing cluster without the support of Gardener</div><h1 id=manually-adding-a-node-to-an-existing-cluster>Manually adding a node to an existing cluster</h1><p>Gardener has an excellent ability to <a href=https://github.com/gardener/documentation/blob/master/components/mcm/>automatically scale machines</a> for the cluster. From the point of view
of scalability, there is no need for manual intervention.</p><p>This tutorial is useful for those end-users who need specifically configured nodes, which are not yet supported
by Gardener. For example: an end-user who wants some workload that requires <code>runnc</code> instead of <code>runc</code> as container
runtime.</p><p><img src=/__resources/teaser_bdea32.svg alt=teaser></p><h2 id=disclaimer>Disclaimer</h2><blockquote><p>Here we will look at the steps on how to add a node to an existing cluster without the support of Gardener.
Such a node will not be managed by Gardener, and if it goes down for any reason, Gardener will not be
responsible to replace it.</p></blockquote><h2 id=how>How</h2><ol><li><p>Create a new instance in the same VPC/network as other machines in the cluster. You should be able to ssh into the machine. So save its private key, and assign a public IP to it. If adding a public IP is not preferred, then ssh into any other machine in the cluster, and then ssh from there into the new machine using its private key.</p><p>To ssh into a machine which is already in the cluster, use the steps defined <a href=/docs/guides/monitoring_and_troubleshooting/shell-to-node title=ssh-into-node>here</a>.</p><p>Attach the same IAM role to the new machine which is attached to the existing machines in the cluster. This is required by kubelet in the new machine so that it can contact the cloud provider to query the node&rsquo;s name.</p></li><li><p>On the new machine, create file <code>/var/lib/kubelet/kubeconfig-bootstrap</code> with the following content:</p></li></ol><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Config</span><span class=w>
</span><span class=w></span><span class=nt>current-context</span><span class=p>:</span><span class=w> </span><span class=l>kubelet-bootstrap@default</span><span class=w>
</span><span class=w></span><span class=nt>clusters</span><span class=p>:</span><span class=w>
</span><span class=w></span>- <span class=nt>cluster</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>certificate-authority-data</span><span class=p>:</span><span class=w> </span><span class=l>&lt;CA Certificate&gt;</span><span class=w>
</span><span class=w>    </span><span class=nt>server</span><span class=p>:</span><span class=w> </span><span class=l>&lt;Server&gt;</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>contexts</span><span class=p>:</span><span class=w>
</span><span class=w></span>- <span class=nt>context</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>cluster</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w>    </span><span class=nt>user</span><span class=p>:</span><span class=w> </span><span class=l>kubelet-bootstrap</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>kubelet-bootstrap@default</span><span class=w>
</span><span class=w></span><span class=nt>users</span><span class=p>:</span><span class=w>
</span><span class=w></span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>kubelet-bootstrap</span><span class=w>
</span><span class=w>  </span><span class=nt>user</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>as-user-extra</span><span class=p>:</span><span class=w> </span>{}<span class=w>
</span><span class=w>    </span><span class=nt>token</span><span class=p>:</span><span class=w> </span><span class=l>&lt;Token&gt;</span><span class=w>
</span></code></pre></div><ol start=3><li>ssh into an existing node, and run these commands to get the values of <ca certificate>and <server>to be replaced in above file:</li></ol><ul><li>&lt;Servr></li></ul><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash> /opt/bin/hyperkube kubectl <span class=se>\
</span><span class=se></span>   --kubeconfig /var/lib/kubelet/kubeconfig-real <span class=se>\
</span><span class=se></span>   config view <span class=se>\
</span><span class=se></span>   -o go-template<span class=o>=</span><span class=s1>&#39;{{index .clusters 0 &#34;cluster&#34; &#34;server&#34;}}&#39;</span> <span class=se>\
</span><span class=se></span>   --raw
</code></pre></div><ul><li>&lt;CA Certificate></li></ul><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>/opt/bin/hyperkube kubectl <span class=se>\
</span><span class=se></span>   --kubeconfig /var/lib/kubelet/kubeconfig-real <span class=se>\
</span><span class=se></span>   config view <span class=se>\
</span><span class=se></span>   -o go-template<span class=o>=</span><span class=s1>&#39;{{index .clusters 0 &#34;cluster&#34; &#34;certificate-authority-data&#34;}}&#39;</span> <span class=se>\
</span><span class=se></span>   --raw
</code></pre></div><ol start=4><li><p>&lt;Token><br>The kubelet on the new machine needs a bootstrap token to authenticate with the kube-apiserver when adding itself to the cluster. Kube-apiserver uses a secret in the <code>kube-system</code> namespace to authenticate this token. This token is valid for 90 minutes from the time of creation, and the corresponding secret captures this detail in its <code>.data.expiration</code> field. The name of this secret is of the format <code>bootstrap-token-*</code>. Gardener takes care of creating new bootstrap tokens, and the corresponding secrets.
To get an unexpired token, find the secrets with the name format <code>bootstrap-token-*</code> in the <code>kube-system</code> namespace in the cluster, and pick the one with minimum age. Eg. <code>bootstrap-token-abcdef</code>.<br>Run these commands to get the token:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash> <span class=nv>tokenid</span><span class=o>=</span><span class=k>$(</span>kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template<span class=o>=</span><span class=s1>&#39;{{index .data &#34;token-id&#34;}}&#39;</span> <span class=p>|</span> base64 --decode<span class=k>)</span>

 <span class=nv>tokensecret</span><span class=o>=</span><span class=k>$(</span>kubectl get secret bootstrap-token-abcdef -n kube-system -o go-template<span class=o>=</span><span class=s1>&#39;{{index .data &#34;token-secret&#34;}}&#39;</span> <span class=p>|</span> base64 --decode<span class=k>)</span>

 <span class=nb>echo</span> <span class=nv>$tokenid</span>.<span class=nv>$tokensecret</span>
</code></pre></div><p>The value of $TOKEN will be <code>tokenid.tokensecret</code>. Replace $TOKEN in above file with this value</p></li><li><p>Copy contents of the files - <code>/var/lib/kubelet/config/kubelet</code>, <code>/var/lib/kubelet/ca.crt</code> and <code>/etc/systemd/system/kubelet.service</code> - from an existing node to the new node</p></li><li><p>Run the following command in the new node to start the kubelet:</p></li></ol><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>systemctl <span class=nb>enable</span> kubelet <span class=o>&amp;&amp;</span> systemctl start kubelet
</code></pre></div><p>The new node should be added to the existing cluster within a couple of minutes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b226b144fd3cd9bed81a4429318310db>2.6 - Setting up a Seed Cluster</h1><div class=lead>How to configure a Kubernetes cluster as a Gardener seed</div><h1 id=the-seed-cluster>The Seed Cluster</h1><p>The <a href=https://github.com/gardener/landscape-setup-template>landscape-setup-template</a> is meant to provide an as-simple-as-possible Gardener installation. Therefore it just registers the cluster where the Gardener is deployed on as a seed cluster. While this is easy, it might be insecure. Clusters created with Kubify don&rsquo;t have network policies, for example. See <a href=/docs/guides/install_gardener/secure-setup>Hardening the Gardener Community Setup</a> for more information.</p><p>To have network policies on the seed cluster and avoid having the seed on the same cluster as the Gardener, the easiest option is probably to simply create a shoot and then register that shoot as seed. This way you can also leverage other advantages of shooted clusters for your seed, e.g. autoscaling.</p><h2 id=setting-up-the-shoot>Setting up the Shoot</h2><p>The first step is to create a shoot cluster. Unfortunately, the Gardener dashboard currently does not allow to change the CIDRs for the created shoot clusters, and your shoots won&rsquo;t work if they have overlapping CIDR ranges with their corresponding seed cluster. So either your seed cluster is deployed with different CIDRs - not using the dashboard, but <code>kubectl apply</code> and a yaml file - or all of your shoots on that seed need to be created this way. In order to be able to use the dashboard for the shoots, it makes sense to create the seed with different CIDRs.</p><p>So, create yourself a shoot with modified CIDRs. You can find templates for the shoot manifest <a href=https://github.com/gardener/gardener/tree/master/example>here</a>. You could, for example, change the CIDRs to this:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=w>      </span><span class=l>...</span><span class=w>
</span><span class=w>      </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>internal</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=m>10.254.112.0</span><span class=l>/22</span><span class=w>
</span><span class=w>        </span><span class=nt>nodes</span><span class=p>:</span><span class=w> </span><span class=m>10.254.0.0</span><span class=l>/19</span><span class=w>
</span><span class=w>        </span><span class=nt>pods</span><span class=p>:</span><span class=w> </span><span class=m>10.255.0.0</span><span class=l>/17</span><span class=w>
</span><span class=w>        </span><span class=nt>public</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=m>10.254.96.0</span><span class=l>/22</span><span class=w>
</span><span class=w>        </span><span class=nt>services</span><span class=p>:</span><span class=w> </span><span class=m>10.255.128.0</span><span class=l>/17</span><span class=w>
</span><span class=w>        </span><span class=nt>vpc</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>cidr</span><span class=p>:</span><span class=w> </span><span class=m>10.254.0.0</span><span class=l>/16</span><span class=w>
</span><span class=w>        </span><span class=nt>workers</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=m>10.254.0.0</span><span class=l>/19</span><span class=w>
</span><span class=w>      </span><span class=l>...</span><span class=w>
</span></code></pre></div><p>Also make sure that your new seed cluster has enough resources for the expected number of shoots.</p><h2 id=registering-the-shoot-as-seed>Registering the Shoot as Seed</h2><p>The seed itself is a Kubernetes resource that can be deployed via a yaml file, but it has some dependencies. You can find templated versions of these files in the <a href=https://github.com/gardener/landscape-setup/tree/0.5.0/components/seed-config>seed-config component</a> of the landscape-setup-template project. If you have set up your Gardener using this project, there should also be rendered versions of these files in the <code>state/seed-config/</code> directory of your landscape folder (they are probably easier to work with). Examples for all these files can also be found in the aforementioned example folder in the Gardener repo.</p><h3 id=1-seed-namespace>1. Seed Namespace</h3><p>First, you should create a namespace for your new seed and everything that belongs to it. This is not necessary, but it will keep your cluster organized. For this example, the namespace will be called <code>seed-test</code>.</p><h3 id=2-cloud-provider-secret>2. Cloud Provider Secret</h3><p>The Gardener needs to create resources on the seed and thus needs a kubeconfig for it. It is provided with the cloud provider secret (below is an example for AWS).</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Secret</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>test-seed-secret</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>seed-test</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>cloudprofile.garden.sapcloud.io/name</span><span class=p>:</span><span class=w> </span><span class=l>aws </span><span class=w>
</span><span class=w></span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>Opaque</span><span class=w>
</span><span class=w></span><span class=nt>data</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>accessKeyID</span><span class=p>:</span><span class=w> </span><span class=l>&lt;base64-encoded AWS access key&gt;</span><span class=w>
</span><span class=w>  </span><span class=nt>secretAccessKey</span><span class=p>:</span><span class=w> </span><span class=l>&lt;base64-encoded AWS secret key&gt;</span><span class=w>
</span><span class=w>  </span><span class=nt>kubeconfig</span><span class=p>:</span><span class=w> </span><span class=l>&lt;base64-encoded kubeconfig&gt;</span><span class=w>
</span></code></pre></div><p>Deploy the secret into your seed namespace. Apart from the kubeconfig, also infrastructure credentials are required. They will only be used for the etcd backup, so in case for AWS, S3 privileges should be sufficient.</p><h3 id=3-secretbinding-for-cloud-provider-secret>3. Secretbinding for Cloud Provider Secret</h3><p>Create a secretbinding for your cloud provider secret:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>core.gardener.cloud/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>SecretBinding</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>test-seed-secret</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>seed-test</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>cloudprofile.garden.sapcloud.io/name</span><span class=p>:</span><span class=w> </span><span class=l>aws</span><span class=w>
</span><span class=w></span><span class=nt>secretRef</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>test-seed-secret</span><span class=w>
</span><span class=w></span><span class=c># namespace: only required if in different namespace than referenced secret</span><span class=w>
</span><span class=w></span><span class=nt>quotas</span><span class=p>:</span><span class=w> </span><span class=p>[]</span><span class=w>
</span></code></pre></div><p>You can give it the same name as the referenced secret.</p><h3 id=4-cloudprofile>4. Cloudprofile</h3><p>The cloudprofile contains the information which shoots can be created with this seed. You could create a new cloudprofile, but you can also just reference the existing cloudprofile if you don&rsquo;t want to change anything.</p><h3 id=5-seed>5. Seed</h3><p>Now the seed resource can be created. Choose a name, reference cloudprofile and secretbinding, fill in your ingress domain, and set the CIDRs to the same values as in the underlying shoot cluster.</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>core.gardener.cloud/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Seed</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>aws-secure</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>provider</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>aws</span><span class=w>
</span><span class=w>    </span><span class=nt>region</span><span class=p>:</span><span class=w> </span><span class=l>eu-west-1</span><span class=w>
</span><span class=w>  </span><span class=nt>secretRef</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>test-seed-secret</span><span class=w>
</span><span class=w>    </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>seed-test</span><span class=w>
</span><span class=w>  </span><span class=nt>dns</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>ingressDomain</span><span class=p>:</span><span class=w> </span><span class=l>ingress.&lt;your cluster domain&gt;</span><span class=w>
</span><span class=w>  </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>nodes</span><span class=p>:</span><span class=w> </span><span class=m>10.254.0.0</span><span class=l>/19</span><span class=w>
</span><span class=w>    </span><span class=nt>pods</span><span class=p>:</span><span class=w> </span><span class=m>10.255.0.0</span><span class=l>/17</span><span class=w>
</span><span class=w>    </span><span class=nt>services</span><span class=p>:</span><span class=w> </span><span class=m>10.255.128.0</span><span class=l>/17</span><span class=w>
</span></code></pre></div><h3 id=6-hide-original-seed>6. Hide Original Seed</h3><p>In the dashboard, it is not possible to select the seed for a shoot (it is possible when deploying the shoot using a yaml file, however). Since both seeds probably reference the same cloudprofile, the Gardener will try to distribute the shoots equally among both seeds.</p><p>To solve this problem, edit the original seed and set its <code>spec.visible</code> field to <code>false</code>. This will prevent the Gardener from choosing this seed, so now all shoots created via the dashboard should have their control plane on the new, more secure seed.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-db9e41f7ef6a1a47f4ca49bdf49d64db>3 - Administer Client (Shoot) Clusters</h1></div><div class=td-content><h1 id=pg-20b7481bda85a0ae6f7a88f4e470d6fc>3.1 - Hibernate a Cluster</h1><p>Clusters are only needed 24 hours a day if they run productive workload. So whenever you do development in a cluster, or just use it for tests or demo purposes, you can save much money if you scale-down your Kubernetes resources whenever you don&rsquo;t need them. However, scaling them down manually can become time-consuming the more resources you have.</p><p>Gardener offers a clever way to automatically scale-down all resources to zero: cluster hibernation. You can either hibernate a cluster by pushing a button or by defining a hibernation schedule.</p><blockquote><p>To save costs, it&rsquo;s recommended to define a hibernation schedule before the creation of a cluster. You can hibernate your cluster or wake up your cluster manually even if there&rsquo;s a schedule for its hibernation.</p></blockquote><ul><li><a href=#what-is-hibernated>What is hibernated?</a></li><li><a href=#what-isnt-affected-by-the-hibernation>What isn’t affected by the hibernation?</a></li><li><a href=#hibernate-your-cluster-manually>Hibernate your cluster manually</a></li><li><a href=#wake-up-your-cluster-manually>Wake up your cluster manually</a></li><li><a href=#create-a-schedule-to-hibernate-your-cluster>Create a schedule to hibernate your cluster</a></li></ul><h2 id=what-is-hibernated>What is hibernated?</h2><p>When a cluster is hibernated, Gardener scales down worker nodes and deletes the cluster&rsquo;s control plane to free resources at the IaaS provider. This affects:</p><ul><li>Your workload, for example, pods, deployments, custom resources.</li><li>The virtual machines running your workload.</li><li>The resources of the control plane of your cluster.</li></ul><h2 id=what-isnt-affected-by-the-hibernation>What isn’t affected by the hibernation?</h2><p>To scale up everything where it was before hibernation, Gardener doesn’t delete state-related information, that is, information stored in persistent volumes. The cluster state as persistent in <code>etcd</code> is also preserved.</p><h2 id=hibernate-your-cluster-manually>Hibernate your cluster manually</h2><ol><li><p>On the Gardener dashboard, choose <em>CLUSTERS</em> > <em>[YOUR-CLUSTER]</em> > <em>OVERVIEW</em> > <em>Lifecycle</em> > <em>Hibernation</em> > <em>Hibernate Cluster</em>.</p><p><img src=/__resources/Hibernate-Cluster_817c08.png alt="Hibernate Cluster"></p></li><li><p>To confirm the hibernation, enter the name of your cluster and choose <em>HIBERNATE</em>.</p></li></ol><blockquote><p>You can also hibernate your cluster by setting <code>spec.hibernation.enabled</code> to <code>true</code> in the cluster&rsquo;s YAML file. To change it on the dashboard, choose <em>CLUSTERS</em> > <em>[YOUR-CLUSTER]</em> > <em>YAML</em>.</p></blockquote><p>As soon as the cluster was hibernated successfully, its status is shown as <code>ZZZ</code> in the list of clusters:</p><p><img src=/__resources/Hibernation-Status_44c3ff.png alt="Hibernation Status"></p><h2 id=wake-up-your-cluster-manually>Wake up your cluster manually</h2><ol><li><p>On the Gardener dashboard, choose <em>CLUSTERS</em> > <em>[YOUR-CLUSTER]</em> > <em>OVERVIEW</em> > <em>Lifecycle</em> > <em>Hibernation</em> > <em>Wake up Cluster</em>.</p><p><img src=/__resources/Wake-up-Cluster_e04b9c.png alt="Wake up Cluster"></p></li><li><p>To confirm waking up the cluster, choose <em>Wake Up</em>.</p></li></ol><blockquote><p>You can also wake up your cluster by setting <code>spec.hibernation.enabled</code> to <code>false</code> in the cluster&rsquo;s YAML file. To change it on the dashboard, choose <em>CLUSTERS</em> > <em>[YOUR-CLUSTER]</em> > <em>YAML</em>.</p></blockquote><h2 id=create-a-schedule-to-hibernate-your-cluster>Create a schedule to hibernate your cluster</h2><ol><li><p>You can create a hibernation schedule for a cluster in the creation dialog of the Gardener dashboard. To create it later or to change it, choose <em>CLUSTERS</em> > <em>[YOUR-CLUSTER]</em> > <em>OVERVIEW</em> > <em>Lifecycle</em> > <em>Hibernation</em> > <em>Configure Hibernation Schedule</em>.</p><p><img src=/__resources/Hibernation-Schedule_6c5794.png alt="Hibernation Schedule"></p></li><li><p>Changes made on the Gardener dashboard for your cluster are immediately written to the cluster&rsquo;s YAML file on tab <em>CLUSTERS</em> > <em>[YOUR-CLUSTER]</em> > <em>YAML</em>. You can find the hibernation schedule in field <code>spec.hibernation.schedules</code>. The schedule is defined like a cron job in Linux. More information: <a href=https://gardener.cloud/api-reference/core/#core.gardener.cloud/v1beta1.HibernationSchedule>HibernationSchedule</a>.</p></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-2341371c4780b4857a5d435af93bb356>3.2 - Create / Delete a Shoot cluster</h1><h1 id=create-a-shoot-cluster>Create a Shoot Cluster</h1><p>As you have already prepared an <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>example Shoot manifest</a> in the steps described in the development documentation, please
open another Terminal pane/window with the <code>KUBECONFIG</code> environment variable pointing to the Garden development cluster and send the manifest to the Kubernetes API server:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl apply -f your-shoot-aws.yaml
</code></pre></div><p>You should see that the Gardener has immediately picked up your manifest and started to deploy the Shoot cluster.</p><p>In order to investigate what is happening in the Seed cluster, please download its proper Kubeconfig yourself (see next paragraph). The namespace of the Shoot cluster in the Seed cluster will look like that: <code>shoot-johndoe-johndoe-1</code>, whereas the first <code>johndoe</code> is your namespace in the Garden cluster (also called &ldquo;project&rdquo;) and the <code>johndoe-1</code> suffix is the actual name of the Shoot cluster.</p><p>To connect to the newly created Shoot cluster, you must download its Kubeconfig as well. Please connect to the proper Seed cluster, navigate to the Shoot namespace, and download the Kubeconfig from the <code>kubecfg</code> secret in that namespace.</p><h1 id=delete-a-shoot-cluster>Delete a Shoot Cluster</h1><p>In order to delete your cluster, you have to set an annotation confirming the deletion first, and trigger the deletion after that. You can use the prepared <code>delete shoot</code> script which takes the Shoot name as first parameter. The namespace can be specified by the second parameter, but it is optional. If you don&rsquo;t state it, it defaults to your namespace (the username you are logged in with to your machine).</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ ./hack/usage/delete shoot johndoe-1 johndoe
</code></pre></div><p>( <code>hack</code> bash script can be found here <a href=https://github.com/gardener/gardener/blob/master/hack/usage/delete>https://github.com/gardener/gardener/blob/master/hack/usage/delete</a>)</p><h1 id=configure-a-shoot-cluster-alert-receiver>Configure a Shoot cluster alert receiver</h1><p>The receiver of the Shoot alerts can be configured from the <code>.spec.monitoring.alerting.emailReceivers</code> section in the Shoot specification. The value of the field has to be a list of valid mail addresses.</p><p>The alerting for the Shoot clusters is handled by the Prometheus Alertmanager. The Alertmanager will be deployed next to the control plane when the <code>Shoot</code> resource specifies <code>.spec.monitoring.alerting.emailReceivers</code> and if a <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-alerting.yaml>SMTP secret</a> exists.</p><p>If the field gets removed then the Alertmanager will be also removed during the next reconcilation of the cluster. The opposite is also valid if the field is added to an existing cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-89160b6c49ea807fa2f877159a87c52e>3.3 - Create a kubernetes cluster in AWS with Gardener</h1><h3 id=prerequisites>Prerequisites</h3><ul><li>You need an AWS account.</li><li>You have access to the Gardener dashboard and have permissions to create projects.</li></ul><ol><li><p>Go to the Gardener dashboard and create a <em>Project</em>.</p><img src=/__resources/new_gardener_project_7fad98.jpg></li><li><p>To copy the policy for AWS from the Gardener dashboard, choose <em>Secrets</em>, click on the help icon <img src=/__resources/help_icon_1702f8.jpg> for AWS secrets, and choose copy <img src=/__resources/copy_icon_de5735.png>.</p><img src=/__resources/gardener_copy_policy_f15862.jpg></li><li><p>To <a href=https://console.aws.amazon.com/iam/home?#/policies>create a new policy</a> in AWS, paste the policy that you copied from the Gardener dashboard to this custom policy.</p><img src=/__resources/create_policy_e1a9eb.jpg>
<img src=/__resources/review_policy_8fef03.jpg></li><li><p><a href="https://console.aws.amazon.com/iam/home?#/users$new?step=details">Create a new technical user</a>.</p><img src=/__resources/adduser_2275d8.jpg>
<img src=/__resources/attachpolicy_1949de.jpg>
<img src=/__resources/finishuser_8d9cdb.jpg><blockquote><p>Note: After the user is created, <code>Access key ID</code> and <code>Secret access key</code> are generated and displayed. Remember to save them. The <code>Access key ID</code> is used later to create secrets for Gardener.</p></blockquote><img src=/__resources/savekeys_71cc3a.jpg></li><li><p>On the Gardener dashboard, choose <em>Secrets</em> and then the plus sign <img src=/__resources/plus_icon_b32acc.jpg> in the AWS frame to add a new AWS secret.</p></li><li><p>Copy the <code>Access Key ID</code> and <code>Secret Access Key</code> you saved when you created the technical user on AWS.</p><img src=/__resources/add_AWS_Secret_f9bae6.jpg>
<img src=/__resources/secret_stored_c892d2.jpg></li><li><p>To create a new cluster, choose <em>Clusters</em> and then the plus sign in the lower right corner.</p><img src=/__resources/new_cluster_912a2a.jpg></li><li><p>On tab <em>INFRASTRUCTURE</em>, choose the secret you created before. The technical user related to the chosen Secret is used to create infrastructure resources.</p><img src=/__resources/create_cluster2_bb2856.jpg>
<img src=/__resources/create_cluster3_999a44.jpg>
<img src=/__resources/create_cluster4_6687f6.jpg>
<img src=/__resources/create_cluster5_1e9c7d.jpg></li><li><p>Copy kubeconfig.</p><img src=/__resources/copy_kubeconfig_b54c68.jpg></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-a6bc1ee3493ef535ad63d0a033804a7b>3.4 - Create a kubernetes cluster on GCP with Gardener</h1><h3 id=prerequisites>Prerequisites</h3><ul><li>You need a GCP account.</li><li>You have access to the Gardener dashboard and have permissions to create projects.</li></ul><ol><li><p>Go to the Gardener dashboard and create a <em>Project</em>.</p><img src=/__resources/new_gardener_project_999b96.jpg></li><li><p>To check which roles are required by Gardener, choose <em>Secrets</em>, and click on the help button <img src=/__resources/help_icon_cc678c.jpg> for GCP.
<img src=/__resources/gardenergcpsecret1_e23a0d.jpg></p><img src=/__resources/gardenergcpsecret2_57b46b.jpg></li><li><p><a href=https://console.cloud.google.com/iam-admin/serviceaccounts>Create a new service account in GCP</a> and assign the roles required by Gardener.</p><img src=/__resources/gcpcreateserviceaccount0_f15ed5.jpg>
<img src=/__resources/gcpcreateserviceaccount1_3d835d.jpg></li><li><p>To create a key for the service account, choose <em>Actions</em> and then <em>Create key</em>.</p><img src=/__resources/gcpcreatekey_995d2b.jpg></li><li><p>Save the private key of the service account in JSON format.
<img src=/__resources/gcpdownloadkey_49f630.jpg></p><blockquote><p>Note: Save the key of the user, it’s used later to create secrets for Gardener.</p></blockquote></li><li><p><a href=https://console.developers.google.com/apis/library/compute.googleapis.com>Enable the Google compute API</a>.</p><img src=/__resources/gcpcomputeengineapi_adc0f5.jpg></li><li><p><a href=https://console.developers.google.com/apis/api/iam.googleapis.com/overview>Enable the Google IAM API</a>.
<img src=/__resources/gcpiamapi_586344.jpg></p></li><li><p>On the Gardener dashboard, choose <em>Secrets</em> and then the plus sign <img src=/__resources/plus_icon_e14f00.jpg> in the GCP frame to add a new GCP secret.</p><img src=/__resources/gardenergcpsecret01_84d788.jpg>
<img src=/__resources/gardeneraddgcpsecret_c01910.jpg></li><li><p>To create a new cluster, choose <em>Clusters</em> and then the plus sign in the lower right corner.</p><img src=/__resources/new_cluster_26eb5a.jpg></li><li><p>On tab <em>INFRASTRUCTURE</em>, choose the secret you created before.</p><img src=/__resources/gcpcreatecluster1_ec5273.jpg>
<img src=/__resources/gcpcreatecluster2_f48def.jpg>
<img src=/__resources/create_cluster4_82c018.jpg>
<img src=/__resources/gcpcreatecluster2_f48def.jpg></li><li><p>Copy kubeconfig.</p><img src=/__resources/copy_kubeconfig_3b7b54.jpg></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-c4bf829bf283322bf7b7a67cb455fb03>3.5 - Create a Shoot cluster into existing AWS VPC</h1><h1 id=create-a-shoot-cluster-into-existing-aws-vpc>Create a Shoot cluster into existing AWS VPC</h1><p>Gardener can create a new VPC, or use an existing one for your Shoot cluster. Depending on your needs you may want to create Shoot(s) into already created VPC.
The tutorial describes how to create a Shoot cluster into existing AWS VPC. The steps are identical for Alicloud, Azure, and GCP. Please note that the existing VPC must be in the same region like the shoot cluster that you want to deploy into the VPC.</p><h2 id=tldr>TL;DR</h2><p>If <code>.spec.provider.infrastructureConfig.networks.vpc.cidr</code> is specified, Gardener will create a new VPC with the given CIDR block and respectively will delete it on Shoot deletion.<br>If <code>.spec.provider.infrastructureConfig.networks.vpc.id</code> is specified, Gardener will use the existing VPC and respectively won&rsquo;t delete it on Shoot deletion.</p><blockquote><p>It&rsquo;s not recommended to create a Shoot cluster into VPC that is managed by Gardener (that is created for another Shoot cluster). In this case the deletion of the initial Shoot cluster will fail to delete the VPC because there will be resources attached to it.<br>Gardener won&rsquo;t delete any manually created (unmanaged) resources in your cloud provider account.</p></blockquote><h2 id=1-configure-aws-cli>1. Configure AWS CLI</h2><p>The <code>aws configure</code> command is a convenient way to setup your AWS CLI. It will prompt you for your credentials and settings which will be used in the following AWS CLI invocations.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ aws configure
AWS Access Key ID <span class=o>[</span>None<span class=o>]</span>: &lt;ACCESS_KEY_ID&gt;
AWS Secret Access Key <span class=o>[</span>None<span class=o>]</span>: &lt;SECRET_ACCESS_KEY&gt;
Default region name <span class=o>[</span>None<span class=o>]</span>: &lt;DEFAULT_REGION&gt;
Default output format <span class=o>[</span>None<span class=o>]</span>: &lt;DEFAULT_OUTPUT_FORMAT&gt;
</code></pre></div><h2 id=2-create-vpc>2. Create VPC</h2><p>Create the VPC by running the following command:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ aws ec2 create-vpc --cidr-block &lt;cidr-block&gt;
<span class=o>{</span>
  <span class=s2>&#34;Vpc&#34;</span>: <span class=o>{</span>
      <span class=s2>&#34;VpcId&#34;</span>: <span class=s2>&#34;vpc-ff7bbf86&#34;</span>,
      <span class=s2>&#34;InstanceTenancy&#34;</span>: <span class=s2>&#34;default&#34;</span>,
      <span class=s2>&#34;Tags&#34;</span>: <span class=o>[]</span>,
      <span class=s2>&#34;CidrBlockAssociations&#34;</span>: <span class=o>[</span>
          <span class=o>{</span>
              <span class=s2>&#34;AssociationId&#34;</span>: <span class=s2>&#34;vpc-cidr-assoc-6e42b505&#34;</span>,
              <span class=s2>&#34;CidrBlock&#34;</span>: <span class=s2>&#34;10.0.0.0/16&#34;</span>,
              <span class=s2>&#34;CidrBlockState&#34;</span>: <span class=o>{</span>
                  <span class=s2>&#34;State&#34;</span>: <span class=s2>&#34;associated&#34;</span>
              <span class=o>}</span>
          <span class=o>}</span>
      <span class=o>]</span>,
      <span class=s2>&#34;Ipv6CidrBlockAssociationSet&#34;</span>: <span class=o>[]</span>,
      <span class=s2>&#34;State&#34;</span>: <span class=s2>&#34;pending&#34;</span>,
      <span class=s2>&#34;DhcpOptionsId&#34;</span>: <span class=s2>&#34;dopt-38f7a057&#34;</span>,
      <span class=s2>&#34;CidrBlock&#34;</span>: <span class=s2>&#34;10.0.0.0/16&#34;</span>,
      <span class=s2>&#34;IsDefault&#34;</span>: <span class=nb>false</span>
  <span class=o>}</span>
<span class=o>}</span>
</code></pre></div><p>Gardener requires the VPC to have enabled <a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html>DNS support</a>, i.e the attributes <code>enableDnsSupport</code> and <code>enableDnsHostnames</code> must be set to true. <code>enableDnsSupport</code> attribute is enabled by default, <code>enableDnsHostnames</code> - not. Set the <code>enableDnsHostnames</code> attribute to true:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ aws ec2 modify-vpc-attribute --vpc-id vpc-ff7bbf86 --enable-dns-hostnames
</code></pre></div><h2 id=3-create-internet-gateway>3. Create Internet Gateway</h2><p>Gardener also requires that an internet gateway is attached to the VPC. You can create one using:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ aws ec2 create-internet-gateway
<span class=o>{</span>
    <span class=s2>&#34;InternetGateway&#34;</span>: <span class=o>{</span>
        <span class=s2>&#34;Tags&#34;</span>: <span class=o>[]</span>,
        <span class=s2>&#34;InternetGatewayId&#34;</span>: <span class=s2>&#34;igw-c0a643a9&#34;</span>,
        <span class=s2>&#34;Attachments&#34;</span>: <span class=o>[]</span>
    <span class=o>}</span>
<span class=o>}</span>
</code></pre></div><p>and attach it to the VPC using:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ aws ec2 attach-internet-gateway --internet-gateway-id igw-c0a643a9 --vpc-id vpc-ff7bbf86
</code></pre></div><h2 id=4-create-the-shoot>4. Create the Shoot</h2><p>Prepare your Shoot manifest (you could check the <a href=https://github.com/gardener/gardener/tree/master/example>example manifests</a>). Please make sure that you choose the
region in which you had created the VPC earlier (step 2). Also, put your VPC ID in the <code>.spec.provider.infrastructureConfig.networks.vpc.id</code> field:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>region</span><span class=p>:</span><span class=w> </span><span class=l>&lt;aws-region-of-vpc&gt;</span><span class=w>
</span><span class=w>  </span><span class=nt>provider</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>aws</span><span class=w>
</span><span class=w>    </span><span class=nt>infrastructureConfig</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>aws.provider.extensions.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w>      </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>InfrastructureConfig</span><span class=w>
</span><span class=w>      </span><span class=nt>networks</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>vpc</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>id</span><span class=p>:</span><span class=w> </span><span class=l>vpc-ff7bbf86</span><span class=w>
</span><span class=w>    </span><span class=c># ...</span><span class=w>
</span></code></pre></div><p>Apply your Shoot manifest.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl apply -f your-shoot-aws.yaml
</code></pre></div><p>Ensure that the Shoot cluster is properly created.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl get shoot <span class=nv>$SHOOT_NAME</span> -n <span class=nv>$SHOOT_NAMESPACE</span>
NAME           CLOUDPROFILE   VERSION   SEED   DOMAIN           OPERATION   PROGRESS   APISERVER   CONTROL   NODES   SYSTEM   AGE
&lt;SHOOT_NAME&gt;   aws            1.15.0    aws    &lt;SHOOT_DOMAIN&gt;   Succeeded   <span class=m>100</span>        True        True      True    True     20m
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-8347c81da1563e74d1ecc9dc9652dce3>3.6 - Create Shoot Clusters in Alibaba Cloud</h1><h1 id=create-shoot-clusters-in-alibaba-cloud>Create Shoot Clusters in Alibaba Cloud</h1><h2 id=prerequisites>Prerequisites</h2><ul><li>You need an Alibaba Cloud account.</li><li>You have access to the Gardener dashboard and have permissions to create projects.</li></ul><h2 id=procedure>Procedure</h2><ol><li><p>Go to the Gardener dashboard and create a project.</p><p><img src=/__resources/Create-new-project-in-Gardener_75be9e.png alt="Create new project in Gardener"></p><blockquote><p>To be able to add shoot clusters to this project, you must first create a technical user on Alibaba cloud with sufficient permissions.</p></blockquote></li><li><p>To copy the policy for Alibaba Cloud from the Gardener dashboard, choose <em>Secrets</em>, click on the help button ( <img src=/__resources/Help-icon_698de6.png alt="Help icon"> ) for Alibaba Cloud secrets, and choose copy ( <img src=/__resources/Copy-icon_ed02ce.png alt="Copy icon"> ).</p><p><img src=/__resources/Alibaba-Cloud---copy-required-policies-from-Gardener-dashboard_3f407f.png alt="Alibaba Cloud - copy required policies from Gardener dashboard"></p></li><li><p>To create a custom policy in Alibaba cloud, log on to your Alibaba account and choose <em>RAM</em> > <em>Permissions</em> > <em>Policies</em>. Paste policy that you copied from the Gardener dashboard to this custom policy.</p><p><img src=/__resources/Alibaba---Create-Custom-Policy_077b8d.png alt="Alibaba - Create Custom Policy"></p></li><li><p>In the Alibaba cloud console, choose <em>RAM</em> > <em>Users</em> and create a new technical user.</p><blockquote><p>After the user is created, <code>AccessKeyId</code> and <code>AccessKeySecret</code> are generated and displayed. Remember to save them. The <code>AccessKey</code> is used later to create secrets for Gardener.</p></blockquote><p><img src=/__resources/Alibaba---Create-Technical-User_24ce9b.png alt="Alibaba - Create Technical User"></p></li><li><p>Choose <em>RAM</em> > <em>Permissions</em> > <em>Grants</em> and assign the policy you’ve created before to the technical user.</p><p><img src=/__resources/Alibaba---Grant-Permissions_843ef6.png alt="Alibaba - Grant Permissions"></p></li><li><p>On the Gardener dashboard, choose <em>Secrets</em> and then the plus sign in the Alibaba Cloud frame to add a new Alibaba Cloud secret.</p></li><li><p>Copy the <code>AccessKeyId</code> and <code>AccessKeySecret</code> you saved when you created the technical user on Alibaba Cloud Console.</p><p><img src=/__resources/Gardener---Add-Alibaba-Cloud-Secret_9b8786.png alt="Gardener - Add Alibaba Cloud Secret"></p><blockquote><p>After successfully creating secrets for Alibaba Cloud, you can see them in the corresponding dropdown list whenever you create a shoot cluster on Alibaba cloud.</p></blockquote></li><li><p>Choose <em>Clusters</em> and then the plus sign in the lower right.</p></li><li><p>On tab <em>INFRASTRUCTURE</em>, choose the secret you created before. The technical user related to the chosen Secret is used to create infrastructure resources on Alibaba Cloud.</p><p><img src=/__resources/Gardener---Assign-Alibaba-Cloud-Secret_0fef77.png alt="Gardener - Assign Alibaba Cloud Secret"></p></li></ol><h2 id=result>Result</h2><p>You can now create shoot clusters on Alibaba cloud.</p><blockquote><p>The size of persistent volumes in your shoot cluster must at least be 20 GiB large. If you choose smaller sizes in your Kubernetes PV definition, the allocation of cloud disk space on Alibaba cloud fails.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-968bb710e1dfc6a63a927387741229e2>3.7 - Create shoot clusters in Azure</h1><h1 id=create-shoot-clusters-in-azure>Create shoot clusters in Azure</h1><h3 id=prerequisites>Prerequisites</h3><ul><li>You need an Azure account.</li><li>You have access to the Gardener dashboard and have permissions to create projects.</li></ul><p>Before you can provision and access a Kubernetes cluster on Azure, you need to add the account credentials in Gardener.
Gardener needs the credentials to provision and operate the Azure infrastructure for your Kubernetes cluster.</p><blockquote><p>Ensure that the account has the <strong>contributor</strong> role.</p></blockquote><ol><li><p>Request a new service account on Azure.</p><p>You must <a href=https://jira.multicloud.int.sap/plugins/servlet/desk/portal/2>request a new service account</a> first, if you don&rsquo;t have one.</p><img src=/__resources/request_spn_7fb44f.jpg><p>Naming Convention for the Service UserID: <code>azrspn_&lt;PartOfSubName>_usecase</code></p><img src=/__resources/request_spn1_548907.jpg></li><li><p>Get properties of your Azure account/Service Principal.</p><ul><li><p>Tenant ID</p><p>The TenantID is also called DirectoryID - <a href=https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/Properties>https://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/Properties</a>.
<img src=/__resources/azuregettenantid_6606e1.jpg></p></li><li><p>ClientID
Select the subscription.
<img src=/__resources/azureselectsubscription_050a9a.jpg></p></li><li><p>Select the SPN.
<img src=/__resources/azureselectspn_556ee7.jpg></p></li></ul><p><em>Note:</em> The ClientID is also called ApplicationID.
<img src=/__resources/azuregetclientid_3e957d.jpg></p><ul><li>Client Secret
Secrets for the Azure Account/Service Principal can be genereted/rotated via the Azure Portal.
Access the <a href=https://portal.azure.com>Azure Portal</a> and navigate to the <code>Active Directory</code> service.
Within the service navigate to <code>App registrations</code> and select your service principal.
In the detail view navigate to <code>Certificates & secrets</code>. In the section, you can generate a new secret for the Service Principal.</li></ul></li><li><p>On the Gardener dashboard, choose <em>Secrets</em> and then the plus sign <img src=/__resources/plus_icon_615f3f.jpg> in the Azure frame to add a new Azure secret.</p><img src=/__resources/gardenernewazuresecret_7f1e76.jpg></li><li><p>Provide the details for the Azure service account.<br>After processing the ticket, you’ll receive the Service Principle credentials via email.
Copy &ldquo;Key Value&rdquo; from the email into &ldquo;Client Secret&rdquo;.</p><img src=/__resources/gardeneraddazuresecret_71956d.jpg></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-d0b7dd8dea7acc30bf970486784ab447>3.8 - Manage DNS Providers</h1><div class=lead>Manage DNS providers for DNS records in your shoot cluster</div><h1 id=dns-providers>DNS Providers</h1><h2 id=introduction>Introduction</h2><p>Gardener can manage DNS records on your behalf, so that you can request them via different resource types (see <a href=/docs/guides/administer_shoots/dns_names>here</a>) within the shoot cluster. The domains for which you are permitted to request records, are however restricted and depend on the DNS provider configuration.</p><h2 id=shoot-provider>Shoot provider</h2><p>By default, every shoot cluster is equipped with a default provider. It is the very same provider that manages the shoot cluster&rsquo;s <code>kube-apiserver</code> public DNS record (DNS address in your Kubeconfig).</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>kind: Shoot
...
dns:
  domain: shoot.project.default-domain.gardener.cloud
</code></pre></div><p>You are permitted to request any sub-domain of <code>.dns.domain</code> that is not already taken (e.g. <code>api.shoot.project.default-domain.gardener.cloud</code>, <code>*.ingress.shoot.project.default-domain.gardener.cloud</code>) with this provider.</p><h2 id=additional-providers>Additional providers</h2><p>If you need to request DNS records for domains not managed by the <a href=#Shoot-provider>default provider</a>, additional providers can
be configured in the shoot specification.
Alternatively, if it is enabled, it can be added as <code>DNSProvider</code> resources to the shoot cluster.</p><h3 id=additional-providers-in-the-shoot-specification>Additional providers in the shoot specification</h3><p>To add a providers in the shoot spec, you need set them in the <code>spec.dns.providers</code> list.</p><p>For example:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Shoot</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>dns</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>domain</span><span class=p>:</span><span class=w> </span><span class=l>shoot.project.default-domain.gardener.cloud</span><span class=w>
</span><span class=w>    </span><span class=nt>providers</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>my-aws-account</span><span class=w>
</span><span class=w>      </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>aws-route53</span><span class=w>
</span><span class=w>    </span>- <span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>my-gcp-account</span><span class=w>
</span><span class=w>      </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>google-clouddns</span><span class=w>
</span></code></pre></div><blockquote><p>Please consult the <a href=https://gardener.cloud/docs/references/core/#core.gardener.cloud/v1beta1.DNSProvider>API-Reference</a> to get a complete list of supported fields and configuration options.</p></blockquote><p>Referenced secrets should exist in the project namespace in the Garden cluster and must comply with the provider specific credentials format. The <strong>External-DNS-Management</strong> project provides corresponding examples (<a href=https://github.com/gardener/external-dns-management/tree/master/examples>20-secret-&lt;provider-name>-credentials.yaml</a>) for known providers.</p><h3 id=additional-providers-as-resources-in-the-shoot-cluster>Additional providers as resources in the shoot cluster</h3><p>If it is not enabled globally, you have to enable the feature in the shoot manifest:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>Kind</span><span class=p>:</span><span class=w> </span><span class=l>Shoot</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>extensions</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>shoot-dns-service</span><span class=w>
</span><span class=w>      </span><span class=nt>providerConfig</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>service.dns.extensions.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w>        </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>DNSConfig</span><span class=w>
</span><span class=w>        </span><span class=nt>dnsProviderReplication</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>enabled</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span></code></pre></div><p>To add a provider directly in the shoot cluster, provide a <code>DNSProvider</code> in any namespace together
with <code>Secret</code> containing the credentials.</p><p>For example if the domain is hosted with AWS Route 53 (provider type <code>aws-route53</code>):</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>dns.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>DNSProvider</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>dns.gardener.cloud/class</span><span class=p>:</span><span class=w> </span><span class=l>garden</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>my-own-domain</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>my-namespace</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>aws-route53</span><span class=w>
</span><span class=w>  </span><span class=nt>secretRef</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>my-own-domain-credentials</span><span class=w>
</span><span class=w>  </span><span class=nt>domains</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>include</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=l>my.own.domain.com</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Secret</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>my-own-domain-credentials</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>my-namespace</span><span class=w>
</span><span class=w></span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>Opaque</span><span class=w>
</span><span class=w></span><span class=nt>data</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=c># replace &#39;...&#39; with values encoded as base64</span><span class=w>
</span><span class=w>  </span><span class=nt>AWS_ACCESS_KEY_ID</span><span class=p>:</span><span class=w> </span><span class=l>...</span><span class=w>
</span><span class=w>  </span><span class=nt>AWS_SECRET_ACCESS_KEY</span><span class=p>:</span><span class=w> </span><span class=l>...</span><span class=w>
</span></code></pre></div><p>The <strong>External-DNS-Management</strong> project provides examples with more details for <code>DNSProviders</code> (30-provider-&lt;provider-name>.yaml)
and credential <code>Secrets</code> (20-secret-&lt;provider-name>.yaml) at <a href=https://github.com/gardener/external-dns-management/tree/master/examples>https://github.com/gardener/external-dns-management//examples</a>
for all supported provider types.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3b636667756f7ecb306629285f7d8177>3.9 - Request DNS Names</h1><div class=lead>Requesting DNS Names for Ingresses and Services in Shoot Clusters</div><h1 id=request-dns-names-in-shoot-clusters>Request DNS Names in Shoot Clusters</h1><h2 id=introduction>Introduction</h2><p>Within a shoot cluster, it is possible to request DNS records via the following resource types:</p><ul><li><a href=https://kubernetes.io/docs/concepts/services-networking/ingress/>Ingress</a></li><li><a href=https://kubernetes.io/docs/concepts/services-networking/service/>Service</a></li><li><a href=https://github.com/gardener/external-dns-management/blob/master/examples/40-entry-dns.yaml>DNSEntry</a></li></ul><p>It is necessary that the Gardener installation your shoot cluster runs in is equipped with a <code>shoot-dns-service</code> extension. This extension uses the seed&rsquo;s dns management infrastructure to maintain DNS names for shoot clusters. Please ask your Gardener operator if the extension is available in your environment.</p><h2 id=shoot-feature-gate>Shoot Feature Gate</h2><p>In some Gardener setups the <code>shoot-dns-service</code> extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Shoot</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>extensions</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>shoot-dns-service</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span></code></pre></div><h2 id=dns-providers-domain-scope>DNS providers, domain scope</h2><p>Gardener can only manage DNS records on your behalf if you have proper DNS providers in place. Please consult <a href=https://github.com/gardener/gardener-extension-shoot-dns-service/blob/master/docs/dns_providers>this page</a> for more information.</p><h2 id=request-dns-records-via-serviceingress-resources>Request DNS records via Service/Ingress resources</h2><p>To request a DNS name for an Ingress or Service object in the shoot cluster
it must be annotated with the DNS class <code>garden</code> and an annotation denoting
the desired DNS names.</p><p>For a Service (it must have the type <code>LoadBalancer</code>) this looks like this:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Service</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>dns.gardener.cloud/class</span><span class=p>:</span><span class=w> </span><span class=l>garden</span><span class=w>
</span><span class=w>    </span><span class=nt>dns.gardener.cloud/dnsnames</span><span class=p>:</span><span class=w> </span><span class=l>my.subdomain.for.shootsomain.cloud</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>my-service</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w>      </span><span class=nt>protocol</span><span class=p>:</span><span class=w> </span><span class=l>TCP</span><span class=w>
</span><span class=w>      </span><span class=nt>targetPort</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>LoadBalancer</span><span class=w>
</span></code></pre></div><p>The <em>dnsnames</em> annotation accepts a comma-separated list of DNS names, if
multiple names are required.</p><p>For an Ingress, the dns names are already declared in the specification.
Nevertheless the <em>dnsnames</em> annotation must be present. Here a subset of the
dns names of the ingress can be specified. If DNS names for all names are
desired, the value <code>all</code> can be used.</p><p>If one of the accepted dns names is a direct subname of the shoot&rsquo;s ingress
domain, this is already handled by the standard wildcard entry for the ingress
domain. Therefore this name should be excluded from the <em>dnsnames</em> list in the
annotation. If only this dns name is configured in the ingress, no explicit
dns entry is required, and the dns annotations should be omitted at all.</p><h2 id=request-dns-records-via-dnsentry-resources>Request DNS records via DNSEntry resources</h2><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>dns.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>DNSEntry</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>dns.gardener.cloud/class</span><span class=p>:</span><span class=w> </span><span class=l>garden</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>dns</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>dnsName</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;my.subdomain.for.shootsomain.cloud&#34;</span><span class=w>
</span><span class=w>  </span><span class=nt>ttl</span><span class=p>:</span><span class=w> </span><span class=m>600</span><span class=w>
</span><span class=w>  </span><span class=c># txt records, either text or targets must be specified</span><span class=w>
</span><span class=w></span><span class=c># text:</span><span class=w>
</span><span class=w></span><span class=c># - foo-bar</span><span class=w>
</span><span class=w>  </span><span class=nt>targets</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=c># target records (CNAME or A records)</span><span class=w>
</span><span class=w>  </span>- <span class=m>8.8.8.8</span><span class=w>
</span></code></pre></div><h2 id=dns-record-events>DNS record events</h2><p>The DNS controller publishes Kubernetes events for the resource which requested the DNS record (Ingress, Service, DNSEntry). These events reveal more information about the DNS requests being processed and are especially useful to check any kind of misconfiguration, e.g. requests for a domain you don&rsquo;t own.</p><p>Events for a successfully created DNS record:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ kubectl -n default describe service my-service

Events:
  Type    Reason          Age                From                    Message
  ----    ------          ----               ----                    -------
  Normal  dns-annotation  19s                dns-controller-manager  my.subdomain.for.shootsomain.cloud: dns entry is pending
  Normal  dns-annotation  19s (x3 over 19s)  dns-controller-manager  my.subdomain.for.shootsomain.cloud: dns entry pending: waiting for dns reconciliation
  Normal  dns-annotation  9s (x3 over 10s)   dns-controller-manager  my.subdomain.for.shootsomain.cloud: dns entry active
</code></pre></div><p>Please note, events vanish after their retention period (usually <code>1h</code>).</p><h2 id=dnsentry-status>DNSEntry status</h2><p><code>DNSEntry</code> resources offer a <code>.status</code> sub-resource which can be used to check the current state of the object.</p><p>Status of a erroneous <code>DNSEntry</code>.</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>  status:
    message: No responsible provider found
    observedGeneration: 3
    provider: remote
    state: Error
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-7ca3e283b0aefb6725b7adf88b74c963>3.10 - Request X.509 Certificates</h1><div class=lead>X.509 Certificates For TLS Communication</div><h1 id=request-x509-certificates>Request X.509 Certificates</h1><h2 id=introduction>Introduction</h2><p>Dealing with applications on Kubernetes which offer service endpoints (e.g. HTTP) may also require you to enable a
secured communication via SSL/TLS. Gardener let&rsquo;s you request a commonly trusted X.509 certificate for your application
endpoint. Furthermore, Gardener takes care about the renewal process for your requested certificate.</p><p>Let&rsquo;s get the basics straight first. If this is too long for you, you can read below how to get certificates by</p><ul><li><a href=#request-a-certificate-via-certificate>Certificate Resources</a></li><li><a href=#request-a-certificate-via-ingress>Ingress</a></li><li><a href=#request-a-certificate-via-service>Service</a></li></ul><h2 id=restrictions>Restrictions</h2><h3 id=service-scope>Service Scope</h3><p>This service enables users to request managed X.509 certificates with the help of <a href=https://tools.ietf.org/html/rfc8555>ACME</a> and <a href=https://letsencrypt.org/>Let&rsquo;s Encrypt</a>.
It <strong>does not</strong> equip or manage DNS records for cluster assets like <code>Services</code> or <code>Ingresses</code>. Thus, you can obtain a valid certificate but your service might still not be resolvable or reachable due to missing DNS configuration. Please consult <a href=https://github.com/gardener/gardener-extension-shoot-dns-service/tree/master/docs/usage/dns_names.md>this page</a> if your services require managed DNS records.</p><h3 id=supported-domains>Supported Domains</h3><p>Certificates may be obtained for any subdomain of your shoot&rsquo;s domain (see <code>.spec.dns.domain</code> of your shoot resource) with the default <code>issuer</code>. For custom domains, please consult <a href=#Custom-Domains>custom domains</a>.</p><h3 id=character-restrictions>Character Restrictions</h3><p>Due to the ACME protocol specification, at least one domain of the domains you request a certificate for must not exceed a character limit of 64 (CN - Common Name).</p><p>For example, the following request is invalid:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>cert.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Certificate</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cert-invalid</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>commonName</span><span class=p>:</span><span class=w> </span><span class=l>morethan64characters.ingress.shoot.project.default-domain.gardener.cloud</span><span class=w>
</span></code></pre></div><p>But it is valid to request a certificate for this domain if you have at least one domain which does not exceed the mentioned limit:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>cert.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Certificate</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cert-example</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>commonName</span><span class=p>:</span><span class=w> </span><span class=l>short.ingress.shoot.project.default-domain.gardener.cloud</span><span class=w>
</span><span class=w>  </span><span class=nt>dnsNames</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=l>morethan64characters.ingress.shoot.project.default-domain.gardener.cloud</span><span class=w>
</span></code></pre></div><h2 id=certificate-resources>Certificate Resources</h2><p>Every X.509 certificate is represented by a Kubernetes custom resource <code>certificate.cert.gardener.cloud</code> in your cluster. A <code>Certificate</code> resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener&rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.</p><blockquote><p>Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.</p></blockquote><p>Certificates can either be requested by creating <code>Certificate</code> resources in the Kubernetes cluster or by configuring <code>Ingress</code> or <code>Service</code> (type <code>LoadBalancer</code>) resources. If the latter is used, a <code>Certificate</code> resource will automatically be created by Gardener&rsquo;s certificate service.</p><p>If you&rsquo;re interested in the current progress of your request, you&rsquo;re advised to consult the <code>Certificate</code>&rsquo;s <code>status</code> subresource. You&rsquo;ll also find error descriptions in the <code>status</code> in case the issuance failed.</p><p>Certificate status example:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>cert.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Certificate</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span><span class=w></span><span class=nt>status</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>commonName</span><span class=p>:</span><span class=w> </span><span class=l>short.ingress.shoot.project.default-domain.gardener.cloud</span><span class=w>
</span><span class=w>  </span><span class=nt>expirationDate</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;2020-02-27T15:39:10Z&#34;</span><span class=w>
</span><span class=w>  </span><span class=nt>issuerRef</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>garden</span><span class=w>
</span><span class=w>    </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>shoot--foo--bar</span><span class=w>
</span><span class=w>  </span><span class=nt>lastPendingTimestamp</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;2019-11-29T16:38:40Z&#34;</span><span class=w>
</span><span class=w>  </span><span class=nt>observedGeneration</span><span class=p>:</span><span class=w> </span><span class=m>11</span><span class=w>
</span><span class=w>  </span><span class=nt>state</span><span class=p>:</span><span class=w> </span><span class=l>Ready</span><span class=w>
</span></code></pre></div><h2 id=custom-domains>Custom Domains</h2><p>If you want to request certificates for domains other then any subdomain of <code>shoot.spec.dns.domain</code>, the following configuration is required:</p><h3 id=dns-provider>DNS provider</h3><p>In order to issue certificates for a custom domain you need to specify a DNS provider which is permitted to create DNS records for subdomains of your requested domain in the certificate. For example, if you request a certificate for <code>host.example.com</code> your DNS provider must be capable of managing subdomains of <code>host.example.com</code>.</p><p>DNS providers are normally specified in the shoot manifest.</p><p>If the <code>DNSProvider</code> replication feature is enabled, an provider can alternatively defined in
the shoot cluster.</p><h4 id=provider-in-the-shoot-manifest>Provider in the shoot manifest</h4><p>Example for a provider in the shoot manifest:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Shoot</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>dns</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>providers</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>aws-route53</span><span class=w> </span><span class=c># consult the DNS provisioning controllers group (dnscontrollers) in https://github.com/gardener/external-dns-management#using-the-dns-controller-manager for possible values</span><span class=w>
</span><span class=w>      </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>provider-example-com</span><span class=w> </span><span class=c># contains credentials for service account, see any 20-secret-&lt;provider&gt;-credentials.yaml in https://github.com/gardener/external-dns-management/tree/master/examples</span><span class=w>
</span></code></pre></div><p>The secret referenced by <code>secretName</code> can also be conveniently created via the Gardener dashboard.</p><h4 id=provider-resouce-in-the-shoot-cluster>Provider resouce in the shoot cluster</h4><p><em>Prerequiste</em>: The <code>DNSProvider</code> replication feature has to be enabled.
It is either enabled globally in the <code>ControllerDeployment</code> or in the shoot manifest
with:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nn>...</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>extensions</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>shoot-dns-service</span><span class=w>
</span><span class=w>      </span><span class=nt>providerConfig</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>service.dns.extensions.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w>        </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>DNSConfig</span><span class=w>
</span><span class=w>        </span><span class=nt>dnsProviderReplication</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>enabled</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span></code></pre></div><p>Example for specifying a <code>DNSProvider</code> resource and its <code>Secret</code> in any namespace of the shoot cluster:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>dns.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>DNSProvider</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>dns.gardener.cloud/class</span><span class=p>:</span><span class=w> </span><span class=l>garden  </span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>my-own-domain</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>my-namespace</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>aws-route53</span><span class=w>
</span><span class=w>  </span><span class=nt>secretRef</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>my-own-domain-credentials</span><span class=w>
</span><span class=w>  </span><span class=nt>domains</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>include</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=l>my.own.domain.com</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Secret</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>my-own-domain-credentials</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>my-namespace</span><span class=w>
</span><span class=w></span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>Opaque</span><span class=w>
</span><span class=w></span><span class=nt>data</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=c># replace &#39;...&#39; with values encoded as base64</span><span class=w>
</span><span class=w>  </span><span class=nt>AWS_ACCESS_KEY_ID</span><span class=p>:</span><span class=w> </span><span class=l>...</span><span class=w>
</span><span class=w>  </span><span class=nt>AWS_SECRET_ACCESS_KEY</span><span class=p>:</span><span class=w> </span><span class=l>...</span><span class=w>
</span></code></pre></div><h3 id=issuer>Issuer</h3><p>Another prerequisite to request certificates for custom domains is a dedicated issuer.</p><p>The custom issuers are specified normally in the shoot manifest.</p><p>If the <code>shootIssuers</code> feature is enabled, it can alternatively be defined in the shoot cluster.</p><h4 id=issuer-in-the-shoot-manifest>Issuer in the shoot manifest</h4><p>Example for an issuer in the shoot manifest:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Shoot</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>extensions</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>shoot-cert-service</span><span class=w>
</span><span class=w>    </span><span class=nt>providerConfig</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>service.cert.extensions.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w>      </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>CertConfig</span><span class=w>
</span><span class=w>      </span><span class=nt>issuers</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>email</span><span class=p>:</span><span class=w> </span><span class=l>your-email@example.com</span><span class=w>
</span><span class=w>          </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>custom-issuer</span><span class=w> </span><span class=c># issuer name must be specified in every custom issuer request, must not be &#34;garden&#34;</span><span class=w>
</span><span class=w>          </span><span class=nt>server</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;https://acme-v02.api.letsencrypt.org/directory&#39;</span><span class=w>
</span><span class=w>          </span><span class=nt>privateKeySecretName</span><span class=p>:</span><span class=w> </span><span class=l>my-privatekey</span><span class=w> </span><span class=c># referenced resource, the private key must be stored in the secret at `data.privateKey`</span><span class=w>
</span><span class=w>      </span><span class=c>#shootIssuers:</span><span class=w>
</span><span class=w>      </span><span class=c>#  enabled: true # if true, allows to specify issuers in the shoot cluster</span><span class=w>
</span><span class=w>  </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>my-privatekey</span><span class=w>
</span><span class=w>    </span><span class=nt>resourceRef</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w>      </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Secret</span><span class=w>
</span><span class=w>      </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>custom-issuer-privatekey</span><span class=w> </span><span class=c># name of secret in Gardener project</span><span class=w>
</span></code></pre></div><h4 id=heading></h4><p><em>Prerequiste</em>: The <code>shootIssuers</code> feature has to be enabled.
It is either enabled globally in the <code>ControllerDeployment</code> or in the shoot manifest
with:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Shoot</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>extensions</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>shoot-cert-service</span><span class=w>
</span><span class=w>    </span><span class=nt>providerConfig</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>service.cert.extensions.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w>      </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>CertConfig</span><span class=w>
</span><span class=w>      </span><span class=nt>shootIssuers</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>enabled</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w> </span><span class=c># if true, allows to specify issuers in the shoot cluster</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span></code></pre></div><p>Example for specifying an <code>Issuer</code> resource and its <code>Secret</code> directly in any
namespace of the shoot cluster:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>cert.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Issuer</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>my-own-issuer</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>my-namespace</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>acme</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>domains</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>include</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=l>my.own.domain.com</span><span class=w>
</span><span class=w>    </span><span class=nt>email</span><span class=p>:</span><span class=w> </span><span class=l>some.user@my.own.domain.com</span><span class=w>
</span><span class=w>    </span><span class=nt>privateKeySecretRef</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>my-own-issuer-secret</span><span class=w>
</span><span class=w>      </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>my-namespace</span><span class=w>
</span><span class=w>    </span><span class=nt>server</span><span class=p>:</span><span class=w> </span><span class=l>https://acme-v02.api.letsencrypt.org/directory</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Secret</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>my-own-issuer-secret</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>my-namespace</span><span class=w>
</span><span class=w></span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>Opaque</span><span class=w>
</span><span class=w></span><span class=nt>data</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>privateKey</span><span class=p>:</span><span class=w> </span><span class=l>...</span><span class=w> </span><span class=c># replace &#39;...&#39; with valus encoded as base64</span><span class=w>
</span></code></pre></div><h2 id=examples>Examples</h2><h3 id=request-a-certificate-via-certificate>Request a certificate via Certificate</h3><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>cert.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Certificate</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cert-example</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>commonName</span><span class=p>:</span><span class=w> </span><span class=l>short.ingress.shoot.project.default-domain.gardener.cloud</span><span class=w>
</span><span class=w>  </span><span class=nt>dnsNames</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=l>morethan64characters.ingress.shoot.project.default-domain.gardener.cloud</span><span class=w>
</span><span class=w>  </span><span class=nt>secretRef</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cert-example</span><span class=w>
</span><span class=w>    </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=c># issuerRef:</span><span class=w>
</span><span class=w></span><span class=c>#   name: custom-issuer</span><span class=w>
</span></code></pre></div><table><thead><tr><th style=text-align:left>Path</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><code>spec.commonName</code> (required)</td><td style=text-align:left>Specifies for which domain the certificate request will be created. This entry must comply with the <a href=#Character-Restrictions>64 character</a> limit.</td></tr><tr><td style=text-align:left><code>spec.dnsName</code></td><td style=text-align:left>Additional domains the certificate should be valid for. Entries in this list can be longer than 64 characters.</td></tr><tr><td style=text-align:left><code>spec.secretRef</code></td><td style=text-align:left>Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it&rsquo;ll be created automatically as soon as the X.509 certificate has been issued.</td></tr><tr><td style=text-align:left><code>spec.issuerRef</code></td><td style=text-align:left>Specifies the issuer you want to use. Only necessary if you request certificates for <a href=#Custom-Domains>custom domains</a>.</td></tr></tbody></table><h3 id=request-a-wildcard-certificate-via-certificate>Request a wildcard certificate via Certificate</h3><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>cert.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Certificate</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cert-wildcard</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>commonName</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;*.ingress.shoot.project.default-domain.gardener.cloud&#39;</span><span class=w>
</span><span class=w>  </span><span class=nt>secretRef</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>cert-wildcard</span><span class=w>
</span><span class=w>    </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=c># issuerRef:</span><span class=w>
</span><span class=w></span><span class=c>#   name: custom-issuer</span><span class=w>
</span></code></pre></div><table><thead><tr><th style=text-align:left>Path</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><code>spec.commonName</code> (required)</td><td style=text-align:left>Specifies for which domain the certificate request will be created. This entry must comply with the <a href=#Character-Restrictions>64 character</a> limit.</td></tr><tr><td style=text-align:left><code>spec.secretRef</code></td><td style=text-align:left>Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it&rsquo;ll be created automatically as soon as the X.509 certificate has been issued.</td></tr><tr><td style=text-align:left><code>spec.issuerRef</code></td><td style=text-align:left>Specifies the issuer you want to use. Only necessary if you request certificates for <a href=#Custom-Domains>custom domains</a>.</td></tr></tbody></table><h3 id=request-a-certificate-via-ingress>Request a certificate via Ingress</h3><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>vuejs-ingress</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>cert.gardener.cloud/purpose</span><span class=p>:</span><span class=w> </span><span class=l>managed</span><span class=w>
</span><span class=w>  </span><span class=c># cert.gardener.cloud/issuer: custom-issuer</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=c># Must not exceed 64 characters.</span><span class=w>
</span><span class=w>  </span>- <span class=nt>hosts</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=l>short.ingress.shoot.project.default-domain.gardener.cloud</span><span class=w>
</span><span class=w>    </span>- <span class=l>morethan64characters.ingress.shoot.project.default-domain.gardener.cloud</span><span class=w>
</span><span class=w>    </span><span class=c># Certificate and private key reside in this secret.</span><span class=w>
</span><span class=w>    </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>testsecret-tls</span><span class=w>
</span><span class=w>  </span><span class=nt>rules</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>host</span><span class=p>:</span><span class=w> </span><span class=l>morethan64characters.ingress.shoot.project.default-domain.gardener.cloud</span><span class=w>
</span><span class=w>    </span><span class=nt>http</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>backend</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>serviceName</span><span class=p>:</span><span class=w> </span><span class=l>vuejs-svc</span><span class=w>
</span><span class=w>          </span><span class=nt>servicePort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span></code></pre></div><table><thead><tr><th style=text-align:left>Path</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><code>metadata.annotations</code></td><td style=text-align:left>Annotations should have <code>cert.gardener.cloud/purpose: managed</code> to activate the certificate service on this resource. <code>cert.gardener.cloud/issuer: &lt;name></code> is optional and may be specified if the certificate is request for a <a href=#Custom-Domains>custom domains</a>.</td></tr><tr><td style=text-align:left><code>spec.tls[].hosts</code></td><td style=text-align:left>Specifies for which domains the certificate request will be created. The first entry is always taken to fill the <code>Common Name</code> field and must therefore comply with the <a href=#Character-Restrictions>64 character</a> limit.</td></tr><tr><td style=text-align:left><code>spec.tls[].secretName</code></td><td style=text-align:left>Specifies the secret which contains the certificate/key pair to be used by this Ingress. If the secret is not available yet, it&rsquo;ll be created automatically as soon as the certificate has been issued. Once configured, you&rsquo;re not advised to change the name while the Ingress is still managed by the certificate service.</td></tr></tbody></table><h3 id=request-a-wildcard-certificate-via-ingress>Request a wildcard certificate via Ingress</h3><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>vuejs-ingress</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>cert.gardener.cloud/purpose</span><span class=p>:</span><span class=w> </span><span class=l>managed</span><span class=w>
</span><span class=w>  </span><span class=c># cert.gardener.cloud/issuer: custom-issuer</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=c># Must not exceed 64 characters.</span><span class=w>
</span><span class=w>  </span>- <span class=nt>hosts</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=s2>&#34;*.ingress.shoot.project.default-domain.gardener.cloud&#34;</span><span class=w>
</span><span class=w>    </span><span class=c># Certificate and private key reside in this secret.</span><span class=w>
</span><span class=w>    </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>testsecret-tls</span><span class=w>
</span><span class=w>  </span><span class=nt>rules</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>host</span><span class=p>:</span><span class=w> </span><span class=l>morethan64characters.ingress.shoot.project.default-domain.gardener.cloud</span><span class=w>
</span><span class=w>    </span><span class=nt>http</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>backend</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>serviceName</span><span class=p>:</span><span class=w> </span><span class=l>vuejs-svc</span><span class=w>
</span><span class=w>          </span><span class=nt>servicePort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span></code></pre></div><blockquote><p>Domains must not overlap when requesting a wildcard certificate. For example, requests for <code>*.example.com</code> must not contain <code>foo.example.com</code> at the same time.</p></blockquote><table><thead><tr><th style=text-align:left>Path</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><code>metadata.annotations</code></td><td style=text-align:left>Annotations should have <code>cert.gardener.cloud/purpose: managed</code> to activate the certificate service on this resource. <code>cert.gardener.cloud/issuer: &lt;name></code> is optional and may be specified if the certificate is request for a <a href=#Custom-Domains>custom domains</a>.</td></tr><tr><td style=text-align:left><code>spec.tls[].hosts</code></td><td style=text-align:left>Specifies for which domains the certificate request will be created. The first entry is always taken to fill the <code>Common Name</code> field and must therefore comply with the <a href=#Character-Restrictions>64 character</a> limit.</td></tr><tr><td style=text-align:left><code>spec.tls[].secretName</code></td><td style=text-align:left>Specifies the secret which contains the certificate/key pair to be used by this Ingress. If the secret is not available yet, it&rsquo;ll be created automatically as soon as the certificate has been issued. Once configured, you&rsquo;re not advised to change the name while the Ingress is still managed by the certificate service.</td></tr></tbody></table><h3 id=request-a-certificate-via-service>Request a certificate via Service</h3><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Service</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>cert.gardener.cloud/secretname</span><span class=p>:</span><span class=w> </span><span class=l>test-service-secret</span><span class=w>
</span><span class=w>  </span><span class=c># cert.gardener.cloud/issuer: custom-issuer</span><span class=w>
</span><span class=w>    </span><span class=nt>dns.gardener.cloud/dnsnames</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;service.shoot.project.default-domain.gardener.cloud, morethan64characters.svc.shoot.project.default-domain.gardener.cloud&#34;</span><span class=w>
</span><span class=w>    </span><span class=nt>dns.gardener.cloud/ttl</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;600&#34;</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>test-service</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>http</span><span class=w>
</span><span class=w>      </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w>      </span><span class=nt>protocol</span><span class=p>:</span><span class=w> </span><span class=l>TCP</span><span class=w>
</span><span class=w>      </span><span class=nt>targetPort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>LoadBalancer</span><span class=w>
</span></code></pre></div><table><thead><tr><th style=text-align:left>Path</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><code>metadata.annotations[cert.gardener.cloud/secretname]</code></td><td style=text-align:left>Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it&rsquo;ll be created automatically as soon as the certificate has been issued.</td></tr><tr><td style=text-align:left><code>metadata.annotations[cert.gardener.cloud/issuer]</code></td><td style=text-align:left>Optional and may be specified if the certificate is request for a <a href=#Custom-Domains>custom domains</a>.</td></tr><tr><td style=text-align:left><code>metadata.annotations[dns.gardener.cloud/dnsnames]</code></td><td style=text-align:left>Specifies for which domains the certificate request will be created. The first entry is always taken to fill the <code>Common Name</code> field and must therefore comply with the <a href=#Character-Restrictions>64 character</a> limit.</td></tr></tbody></table><h3 id=request-a-wildcard-certificate-via-service>Request a wildcard certificate via Service</h3><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Service</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>cert.gardener.cloud/secretname</span><span class=p>:</span><span class=w> </span><span class=l>test-service-secret</span><span class=w>
</span><span class=w>  </span><span class=c># cert.gardener.cloud/issuer: custom-issuer</span><span class=w>
</span><span class=w>    </span><span class=nt>dns.gardener.cloud/dnsnames</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;*.service.shoot.project.default-domain.gardener.cloud&#34;</span><span class=w>
</span><span class=w>    </span><span class=nt>dns.gardener.cloud/ttl</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;600&#34;</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>test-service</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>http</span><span class=w>
</span><span class=w>      </span><span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w>      </span><span class=nt>protocol</span><span class=p>:</span><span class=w> </span><span class=l>TCP</span><span class=w>
</span><span class=w>      </span><span class=nt>targetPort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>LoadBalancer</span><span class=w>
</span></code></pre></div><blockquote><p>Domains must not overlap when requesting a wildcard certificate. For example, requests for <code>*.example.com</code> must not contain <code>foo.example.com</code> at the same time.</p></blockquote><table><thead><tr><th style=text-align:left>Path</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><code>metadata.annotations[cert.gardener.cloud/secretname]</code></td><td style=text-align:left>Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it&rsquo;ll be created automatically as soon as the certificate has been issued.</td></tr><tr><td style=text-align:left><code>metadata.annotations[cert.gardener.cloud/issuer]</code></td><td style=text-align:left>Optional and may be specified if the certificate is request for a <a href=#Custom-Domains>custom domains</a>.</td></tr><tr><td style=text-align:left><code>metadata.annotations[dns.gardener.cloud/dnsnames]</code></td><td style=text-align:left>Specifies for which domains the certificate request will be created. The first entry is always taken to fill the <code>Common Name</code> field and must therefore comply with the <a href=#Character-Restrictions>64 character</a> limit.</td></tr></tbody></table><h2 id=quotas>Quotas</h2><p>For security reasons there may be a default quota on the certificate requests per day set globally in the controller
registration of the shoot-cert-service.</p><p>The default quota only applies if there is no explicit quota defined for the issuer itself with the field
<code>requestsPerDayQuota</code>, e.g.:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Shoot</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>extensions</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>shoot-cert-service</span><span class=w>
</span><span class=w>    </span><span class=nt>providerConfig</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>service.cert.extensions.gardener.cloud/v1alpha1</span><span class=w>
</span><span class=w>      </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>CertConfig</span><span class=w>
</span><span class=w>      </span><span class=nt>issuers</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>email</span><span class=p>:</span><span class=w> </span><span class=l>your-email@example.com</span><span class=w>
</span><span class=w>          </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>custom-issuer</span><span class=w> </span><span class=c># issuer name must be specified in every custom issuer request, must not be &#34;garden&#34;</span><span class=w>
</span><span class=w>          </span><span class=nt>server</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;https://acme-v02.api.letsencrypt.org/directory&#39;</span><span class=w>
</span><span class=w>          </span><span class=nt>requestsPerDayQuota</span><span class=p>:</span><span class=w> </span><span class=m>10</span><span class=w>
</span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-c59fbc42a6186b5b6f2f75de63a4efc7>3.11 - Shoot Cluster Maintenance</h1><div class=lead>Understanding and configuring Gardener&rsquo;s Day-2 operations for Shoot clusters.</div><h1 id=shoot-cluster-maintenance>Shoot Cluster Maintenance</h1><p>Day two operations for shoot clusters are related to:</p><ul><li>The Kubernetes version of the control plane and the worker nodes</li><li>the operating system version of the worker nodes</li></ul><div class="notices note"><p>When referring to an update of the "operating system version" in this document, the update of the machine image of the shoot cluster's worker nodes is meant. For example, Amazon Machine Images (AMI) for AWS.</p></div><p>The following table summarizes what options Gardener offers to maintain these versions:</p><table><thead><tr><th style=text-align:left></th><th style=text-align:left>Auto-Update</th><th style=text-align:left>Forceful Updates</th><th style=text-align:left>Manual updates</th></tr></thead><tbody><tr><td style=text-align:left>Kubernetes version</td><td style=text-align:left>Patches only</td><td style=text-align:left>Patches and consecutive minor updates only</td><td style=text-align:left>yes</td></tr><tr><td style=text-align:left>Operating system version</td><td style=text-align:left>yes</td><td style=text-align:left>yes</td><td style=text-align:left>yes</td></tr></tbody></table><h2 id=allowed-target-versions-in-the-cloudprofile>Allowed Target Versions in the <code>CloudProfile</code></h2><p>Administrators maintain the allowed target versions that you can update to in the <code>CloudProfile</code> for each IaaS-Provider. Users with access to a Gardener project can check supported target versions with:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>kubectl get cloudprofile [IAAS-SPECIFIC-PROFILE] -o yaml
</code></pre></div><table><thead><tr><th style=text-align:left>Path</th><th style=text-align:left>Description</th><th style=text-align:left>More information</th></tr></thead><tbody><tr><td style=text-align:left><code>spec.kubernetes.versions</code></td><td style=text-align:left>The supported Kubernetes version <code>major.minor.patch</code>.</td><td style=text-align:left><a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/release/versioning.md#patch-releases>Patch releases</a></td></tr><tr><td style=text-align:left><code>spec.machineImages</code></td><td style=text-align:left>The supported operating system versions for worker nodes.</td><td></td></tr></tbody></table><p>Both the Kubernetes version, and the operating system version follow semantic versioning that allows Gardener to handle updates automatically.</p><p>More information: <a href=http://semver.org/>Semantic Versioning</a>.</p><h3 id=impact-of-version-classifications-on-updates>Impact of Version Classifications on Updates</h3><p>Gardener allows to classify versions in the <code>CloudProfile</code> as <code>preview</code>, <code>supported</code>, <code>deprecated</code>, or <code>expired</code>. During maintenance operations, <code>preview</code> versions are excluded from updates, because they’re often recently released versions that haven’t yet undergone thorough testing and may contain bugs or security issues.</p><p>More information: <a href=https://github.com/gardener/gardener/blob/master/docs/usage/shoot_versions.md#version-classifications>Version Classifications</a>.</p><h2 id=let-gardener-manage-your-updates>Let Gardener manage your updates</h2><h3 id=the-maintenance-window>The Maintenance Window</h3><p>Gardener can manage updates for you automatically. It offers users to specify a <em>maintenance window</em> during which updates are scheduled:</p><ul><li>The time interval of the maintenance window can’t be less than 30 minutes or more than 6 hours.</li><li>If there’s no maintenance window specified during the creation of a shoot cluster, Gardener chooses a maintenance window randomly to spread the load.</li></ul><p>You can either specify the maintenance window in the shoot cluster specification (<code>.spec.maintenance.timeWindow</code>) or the start time of the maintenance window using the Gardener dashboard (<strong>CLUSTERS</strong> > <strong>[YOUR-CLUSTER]</strong> > <strong>OVERVIEW</strong> > <strong>Lifecycle</strong> > <strong>Maintenance</strong>).</p><h3 id=auto-update-and-forceful-updates>Auto-Update and Forceful Updates</h3><p>To trigger updates during the maintenance window automatically, Gardener offers the following methods:</p><ul><li><p><em>Auto-update</em>:<br>Gardener starts an update during the next maintenance window whenever there’s a version available in the <code>CloudProfile</code> that is higher than the one of your shoot cluster specification, and that isn’t classified as <code>preview</code> version. For Kubernetes versions, auto-update only updates to higher patch levels.</p><p>You can either activate auto-update on the Gardener dashboard (<strong>CLUSTERS</strong> > <strong>[YOUR-CLUSTER]</strong> > <strong>OVERVIEW</strong> > <strong>Lifecycle</strong> > <strong>Maintenance</strong>) or in the shoot cluster specification:</p><ul><li><code>.spec.maintenance.autoUpdate.kubernetesVersion: true</code></li><li><code>.spec.maintenance.autoUpdate.machineImageVersion: true</code></li></ul></li><li><p><em>Forceful updates</em>:<br>In the maintenance window, Gardener compares the current version given in the shoot cluster specification with the version list in the <code>CloudProfile</code>. If the version has an expiration date and if the date is before the start of the maintenance window, Gardener starts an update to the highest version available in the <code>CloudProfile</code> that isn’t classified as <code>preview</code> version. The highest version in <code>CloudProfile</code> can’t have an expiration date. For Kubernetes versions, Gardener only updates to higher patch levels or consecutive minor versions.</p></li></ul><p>If you don’t want to wait for the next maintenance window, you can annotate the shoot cluster specification with <code>shoot.gardener.cloud/operation: maintain</code>. Gardener then checks immediately if there’s an auto-update or a forceful update needed.</p><div class="notices info"><p>Forceful version updates are even executed if the auto-update for the Kubernetes version, or the auto-update for the machine image version is deactivated (set to `false`).</p></div><p>With expiration dates, administrators can give shoot cluster owners more time for testing before the actual version update happens, which allows smoother transitions to new versions.</p><h3 id=kubernetes-update-paths>Kubernetes Update Paths</h3><p>The bigger the delta of the Kubernetes source version and the Kubernetes target version, the better it must be planned and executed by operators. Gardener only provides automatic support for updates that can be applied safely to the cluster workload:</p><table><thead><tr><th style=text-align:left>Update Type</th><th style=text-align:left>Example</th><th style=text-align:left>Update method</th></tr></thead><tbody><tr><td style=text-align:left>Patches</td><td style=text-align:left><code>1.10.12</code> to <code>1.10.13</code></td><td style=text-align:left>auto-update or Forceful update</td></tr><tr><td style=text-align:left>Update to consecutive minor version</td><td style=text-align:left><code>1.10.12</code> to <code>1.11.10</code></td><td style=text-align:left>Forceful update</td></tr><tr><td style=text-align:left>Other</td><td style=text-align:left><code>1.10.12</code> to <code>1.12.0</code></td><td style=text-align:left>manual update</td></tr></tbody></table><p>Gardener doesn’t support automatic updates of nonconsecutive minor versions, because Kubernetes doesn’t guarantee updateability in this case. However, multiple minor version updates are possible if not only the minor source version is expired, but also the minor target version is expired. Gardener then updates the Kubernetes version first to the expired target version, and waits for the next maintenance window to update this version to the next minor target version.</p><div class="notices warning"><p>The administrator who maintains the `CloudProfile` has to ensure that the list of Kubernetes versions consists of consecutive minor versions, for example, from `1.10.x` to `1.11.y`. If the minor version increases in bigger steps, for example, from `1.10.x` to `1.12.y`, shoot cluster updates fail during the maintenance window.</p></div><h2 id=manual-updates>Manual Updates</h2><p>To update the Kubernetes version or the node operating system manually, change the <code>.spec.kubernetes.version</code> field or the <code>.spec.provider.workers.machine.image.version</code> field correspondingly.</p><p>Manual updates are required if you would like to do a minor update of the Kubernetes version. Gardener doesn’t do such updates automatically as they can have breaking changes that could impact the cluster workload.</p><p>Manual updates are either executed immediately (default) or can be confined to the maintenance time window.<br>Choosing the latter option, causes changes to the cluster (for example, node pool rolling-updates) and the subsequent reconciliation, to only predictably happen during a defined time window (available since <a href=https://github.com/gardener/gardener/releases/tag/v1.4.0>Gardener version 1.4</a>).</p><p>More information: <a href=https://github.com/gardener/gardener/blob/master/docs/usage/shoot_maintenance.md#confine-specification-changesupdates-roll-out>Confine Specification Changes/Update Roll Out</a>.</p><div class="notices warning"><p>Before applying such update on minor or major releases, operators should check for all the breaking changes introduced in the target Kubernetes release changelog.</p></div><h2 id=examples>Examples</h2><p>In the examples for the <code>CloudProfile</code> and the shoot cluster specification, only the fields relevant for the example are shown.</p><h3 id=auto-update-of-kubernetes-version>Auto-Update of Kubernetes Version</h3><p>Let&rsquo;s assume Kubernetes version <code>1.10.5</code> and <code>1.11.0</code> were added in the following <code>CloudProfile</code>:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>kubernetes</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>versions</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>1.11.0</span><span class=w>
</span><span class=w>    </span>- <span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>1.10.5</span><span class=w>
</span><span class=w>    </span>- <span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>1.10.0</span><span class=w>
</span></code></pre></div><p>Before this change, the shoot cluster specification looked like this:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>kubernetes</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>1.10.0</span><span class=w>
</span><span class=w>  </span><span class=nt>maintenance</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>timeWindow</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>begin</span><span class=p>:</span><span class=w> </span><span class=m>220000+0000</span><span class=w>
</span><span class=w>      </span><span class=nt>end</span><span class=p>:</span><span class=w> </span><span class=m>230000+0000</span><span class=w>
</span><span class=w>    </span><span class=nt>autoUpdate</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>kubernetesVersion</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span></code></pre></div><p>As a consequence, the shoot cluster is updated to Kubernetes version <code>1.10.5</code> between 22:00-23:00 UTC. Your shoot cluster isn&rsquo;t updated automatically to <code>1.11.0</code> even though it&rsquo;s the highest Kubernetes version in the <code>CloudProfile</code>, because Gardener does only do automatic updates of the Kubernetes patch level.</p><h3 id=forceful-update-due-to-expired-kubernetes-version>Forceful Update Due to Expired Kubernetes Version</h3><p>Let&rsquo;s assume the following <code>CloudProfile</code>:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>kubernetes</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>versions</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>1.12.8</span><span class=w>
</span><span class=w>    </span>- <span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>1.11.10</span><span class=w>
</span><span class=w>    </span>- <span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>1.10.13</span><span class=w>
</span><span class=w>    </span>- <span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>1.10.12</span><span class=w>
</span><span class=w>      </span><span class=nt>expirationDate</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;2019-04-13T08:00:00Z&#34;</span><span class=w>
</span></code></pre></div><p>Let&rsquo;s assume the shoot cluster has the following specification:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>kubernetes</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>1.10.12</span><span class=w>
</span><span class=w>  </span><span class=nt>maintenance</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>timeWindow</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>begin</span><span class=p>:</span><span class=w> </span><span class=m>220000+0100</span><span class=w>
</span><span class=w>      </span><span class=nt>end</span><span class=p>:</span><span class=w> </span><span class=m>230000+0100</span><span class=w>
</span><span class=w>    </span><span class=nt>autoUpdate</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>kubernetesVersion</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span></code></pre></div><p>The shoot cluster specification refers a Kubernetes version that has an <code>expirationDate</code>. In the maintenance window on <code>2019-04-12</code>, the Kubernetes version stays the same as it’s still not expired. But in the maintenance window on <code>2019-04-14</code> the Kubernetes version of the shoot cluster is updated to <code>1.10.13</code> (independently of the value of <code>.spec.maintenance.autoUpdate.kubernetesVersion</code>).</p><h3 id=forceful-update-to-new-minor-kubernetes-version>Forceful Update to New Minor Kubernetes Version</h3><p>Let&rsquo;s assume the following <code>CloudProfile</code>:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>kubernetes</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>versions</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>1.12.8</span><span class=w>
</span><span class=w>    </span>- <span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>1.11.10</span><span class=w>
</span><span class=w>    </span>- <span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>1.11.09</span><span class=w>
</span><span class=w>    </span>- <span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>1.10.12</span><span class=w>
</span><span class=w>      </span><span class=nt>expirationDate</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;2019-04-13T08:00:00Z&#34;</span><span class=w>
</span></code></pre></div><p>Let&rsquo;s assume the shoot cluster has the following specification:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>kubernetes</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>1.10.12</span><span class=w>
</span><span class=w>  </span><span class=nt>maintenance</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>timeWindow</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>begin</span><span class=p>:</span><span class=w> </span><span class=m>220000+0100</span><span class=w>
</span><span class=w>      </span><span class=nt>end</span><span class=p>:</span><span class=w> </span><span class=m>230000+0100</span><span class=w>
</span><span class=w>    </span><span class=nt>autoUpdate</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>kubernetesVersion</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span></code></pre></div><p>The shoot cluster specification refers a Kubernetes version that has an <code>expirationDate</code>. In the maintenance window on <code>2019-04-14</code>, the Kubernetes version of the shoot cluster is updated to <code>1.11.10</code>, which is the highest patch version of minor target version <code>1.11</code> that follows source version <code>1.10</code>.</p><h3 id=automatic-update-from-expired-machine-image-version>Automatic Update from Expired Machine Image Version</h3><p>Let&rsquo;s assume the following <code>CloudProfile</code>:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>machineImages</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>coreos</span><span class=w>
</span><span class=w>    </span><span class=nt>versions</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>2191.5.0</span><span class=w>
</span><span class=w>    </span>- <span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>2191.4.1</span><span class=w>
</span><span class=w>    </span>- <span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>2135.6.0</span><span class=w>
</span><span class=w>      </span><span class=nt>expirationDate</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;2019-04-13T08:00:00Z&#34;</span><span class=w>
</span></code></pre></div><p>Let&rsquo;s assume the shoot cluster has the following specification:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>provider</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>aws</span><span class=w>
</span><span class=w>    </span><span class=nt>workers</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>name</span><span class=w>
</span><span class=w>      </span><span class=nt>maximum</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>      </span><span class=nt>minimum</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>      </span><span class=nt>maxSurge</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>      </span><span class=nt>maxUnavailable</span><span class=p>:</span><span class=w> </span><span class=m>0</span><span class=w>
</span><span class=w>      </span><span class=nt>image</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>coreos</span><span class=w>
</span><span class=w>        </span><span class=nt>version</span><span class=p>:</span><span class=w> </span><span class=m>2135.6.0</span><span class=w>
</span><span class=w>        </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>m5.large</span><span class=w>
</span><span class=w>      </span><span class=nt>volume</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>gp2</span><span class=w>
</span><span class=w>        </span><span class=nt>size</span><span class=p>:</span><span class=w> </span><span class=l>20Gi</span><span class=w>
</span><span class=w>  </span><span class=nt>maintenance</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>timeWindow</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>begin</span><span class=p>:</span><span class=w> </span><span class=m>220000+0100</span><span class=w>
</span><span class=w>      </span><span class=nt>end</span><span class=p>:</span><span class=w> </span><span class=m>230000+0100</span><span class=w>
</span><span class=w>    </span><span class=nt>autoUpdate</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>machineImageVersion</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span></code></pre></div><p>The shoot cluster specification refers a machine image version that has an <code>expirationDate</code>. In the maintenance window on <code>2019-04-12</code>, the machine image version stays the same as it’s still not expired. But in the maintenance window on <code>2019-04-14</code> the machine image version of the shoot cluster is updated to <code>2191.5.0</code> (independently of the value of <code>.spec.maintenance.autoUpdate.machineImageVersion</code>) as version <code>2135.6.0</code> is expired.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d33d61beb996b59f7a315dad30ff50b3>3.12 - Trigger Shoot operations</h1><h1 id=trigger-shoot-operations>Trigger shoot operations</h1><p>You can trigger a few explicit operations by annotating the <code>Shoot</code> with an operation annotation.
This might allow you to induct certain behavior without the need to change the <code>Shoot</code> specification.
Some of the operations can also not be caused by changing something in the shoot specification because they can&rsquo;t properly be reflected here.
Note, once the triggered operation is considered by the controllers, the annotation will be automatically removed and you have to add it each time you want to trigger the operation.</p><p>Please note: If <code>.spec.maintenance.confineSpecUpdateRollout=true</code> then the only way to trigger a shoot reconciliation is by setting the <code>reconcile</code> operation, see below.</p><h2 id=immediate-reconciliation>Immediate reconciliation</h2><p>Annotate the shoot with <code>gardener.cloud/operation=reconcile</code> to make the <code>gardenlet</code> start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation<span class=o>=</span>reconcile
</code></pre></div><h2 id=immediate-maintenance>Immediate maintenance</h2><p>Annotate the shoot with <code>gardener.cloud/operation=maintain</code> to make the <code>gardener-controller-manager</code> start maintaining your shoot immediately (possibly without being in its maintenance time window).
If no reconciliation starts then nothing needed to be maintained:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation<span class=o>=</span>maintain
</code></pre></div><h2 id=retry-failed-operation>Retry failed operation</h2><p>Annotate the shoot with <code>gardener.cloud/operation=retry</code> to make the <code>gardenlet</code> start a new reconciliation loop on a failed shoot.
Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation<span class=o>=</span>retry
</code></pre></div><h2 id=rotate-kubeconfig-credentials>Rotate kubeconfig credentials</h2><p>Annotate the shoot with <code>gardener.cloud/operation=rotate-kubeconfig-credentials</code> to make the <code>gardenlet</code> exchange the credentials in your shoot cluster&rsquo;s kubeconfig.
This operation is now allowed for shoot clusters that are already in deletion.
Please note that only the token (and basic auth password, if enabled) are exchanged. The cluster CAs remain the same.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation<span class=o>=</span>rotate-kubeconfig-credentials
</code></pre></div><h2 id=restart-systemd-services-on-particular-worker-nodes>Restart systemd services on particular worker nodes</h2><p>It is possible to make Gardener restart particular systemd services on your shoot worker nodes if needed.
The annotation is not set on the <code>Shoot</code> resource but directly on the <code>Node</code> object you want to target.
For example, the following will restart both the <code>kubelet</code> and the <code>docker</code> services:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl annotate node &lt;node-name&gt; worker.gardener.cloud/restart-systemd-services<span class=o>=</span>kubelet,docker
</code></pre></div><p>It may take up to a minute until the service is restarted.
The annotation will be removed from the <code>Node</code> object after all specified systemd services have been restarted.
It will also be removed even if the restart of one or more services failed.</p><blockquote><p>ℹ️ In the example mentioned above, you could additionally verify when/whether the kubelet restarted by using <code>kubectl describe node &lt;node-name></code> and looking for such a <code>Starting kubelet</code> event.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-5548ef0d8f57778e2c7a910d3ae783e3>3.13 - Trigger Shoot operations</h1><h1 id=trigger-shoot-operations>Trigger shoot operations</h1><p>You can trigger a few explicit operations by annotating the <code>Shoot</code> with an operation annotation.
This might allow you to induct certain behavior without the need to change the <code>Shoot</code> specification.
Some of the operations can also not be caused by changing something in the shoot specification because they can&rsquo;t properly be reflected here.</p><p>Please note: If <code>.spec.maintenance.confineSpecUpdateRollout=true</code> then the only way to trigger a shoot reconciliation is by setting the <code>reconcile</code> operation, see below.</p><h2 id=immediate-reconciliation>Immediate reconciliation</h2><p>Annotate the shoot with <code>gardener.cloud/operation=reconcile</code> to make the <code>gardenlet</code> start a reconciliation operation without changing the shoot spec and possibly without being in its maintenance time window:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation<span class=o>=</span>reconcile
</code></pre></div><h2 id=immediate-maintenance>Immediate maintenance</h2><p>Annotate the shoot with <code>gardener.cloud/operation=maintain</code> to make the <code>gardener-controller-manager</code> start maintaining your shoot immediately (possibly without being in its maintenance time window).
If no reconciliation starts then nothing needed to be maintained:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation<span class=o>=</span>maintain
</code></pre></div><h2 id=retry-failed-operation>Retry failed operation</h2><p>Annotate the shoot with <code>gardener.cloud/operation=retry</code> to make the <code>gardenlet</code> start a new reconciliation loop on a failed shoot.
Failed shoots are only reconciled again if a new Gardener version is deployed, the shoot specification is changed or this annotation is set</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation<span class=o>=</span>retry
</code></pre></div><h2 id=rotate-kubeconfig-credentials>Rotate kubeconfig credentials</h2><p>Annotate the shoot with <code>gardener.cloud/operation=rotate-kubeconfig-credentials</code> to make the <code>gardenlet</code> exchange the credentials in your shoot cluster&rsquo;s kubeconfig.
Please note that only the token (and basic auth password, if enabled) are exchanged. The cluster CAs remain the same.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl -n garden-&lt;project-name&gt; annotate shoot &lt;shoot-name&gt; gardener.cloud/operation<span class=o>=</span>rotate-kubeconfig-credentials
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-87218383e3eaa4076a198b81967714ae>4 - Monitor and Troubleshoot</h1></div><div class=td-content><h1 id=pg-e97e6a2625fdc17259ec0a4ae54ea0db>4.1 - Get a Shell to a Gardener Shoot Worker Node</h1><div class=lead>Describes the methods for getting shell access to worker nodes.</div><h1 id=get-a-shell-to-a-kubernetes-node>Get a Shell to a Kubernetes Node</h1><p>To troubleshoot certain problems in a Kubernetes cluster, operators need access to the host of the Kubernetes node to troubleshoot
problems. This can be required if a node misbehaves or fails to join the cluster in the first place.</p><p>With access to the host, it is for instance possible to check the <code>kubelet</code> logs and interact with common tools such as <code>systemctl</code>and <code>journalctl</code>.</p><p>The first section of this guide explores options to get a shell to the node of a Gardener Kubernetes cluster.
The options described in the second section do not rely on Kubernetes capabilities to get shell access to a node and thus can also be used if an instance failed to join the cluster.</p><p>This guide only covers how to get access to the host, but does not cover troubleshooting methods.</p><ul><li><a href=#get-a-shell-to-a-kubernetes-node>Get a Shell to a Kubernetes Node</a></li><li><a href=#get-a-shell-to-an-operational-cluster-node>Get a Shell to an operational cluster node</a><ul><li><a href=#gardener-dashboard>Gardener Dashboard</a></li><li><a href=#gardenctl-shell>gardenctl shell</a></li><li><a href=#gardener-ops-toolbelt>Gardener Ops Toolbelt</a></li><li><a href=#custom-root-pod>Custom root pod</a></li></ul></li><li><a href=#ssh-access-to-a-node-that-failed-to-join-the-cluster>SSH access to a node that failed to join the cluster</a><ul><li><a href=#identifying-the-problematic-instance>Identifying the problematic instance</a></li><li><a href=#gardenctl-ssh>gardenctl ssh</a></li><li><a href=#ssh-with-manually-created-bastion-on-aws>SSH with manually created Bastion on AWS</a><ul><li><a href=#create-the-bastion-security-group>Create the Bastion Security Group</a></li><li><a href=#create-the-bastion-instance>Create the bastion instance</a></li></ul></li><li><a href=#connecting-to-the-target-instance>Connecting to the target instance</a></li><li><a href=#cleanup>Cleanup</a></li></ul></li></ul><h1 id=get-a-shell-to-an-operational-cluster-node>Get a Shell to an operational cluster node</h1><p>The following describes four different approaches to get a shell to an operational Shoot worker node.
As a prerequisite to troubleshooting a Kubernetes node, the node must have joined the cluster successfully and be able to run a pod.
All of the described approaches involve scheduling a pod with root permissions and mounting the root filesystem.</p><h2 id=gardener-dashboard>Gardener Dashboard</h2><p><strong>Prerequisite</strong>: the terminal feature is configured for the Gardener dashboard.</p><p>Navigate to the cluster overview page and find the <code>Terminal</code> in the <code>Access</code> tile.</p><img style=margin-left:0;width:80%;height:auto alt="Access Tile" src=/__resources/9fb6ca4ff9b7480f93debba833f48590_9d3149.png><br><p>Select the target Cluster (Garden, Seed / Control Plane, Shoot cluster) depending on the requirements and
access rights (only certain users have access to the Seed Control Plane).</p><p>To open the terminal configuration, click on the top right-hand corner of the screen.</p><img style=margin-left:0 alt="Terminal configuration" src=/__resources/db573582bfc544d294cbde8906a74e07_671e84.png><br><p>Set the Terminal Runtime to &ldquo;Privileged.
Also specify the target node from the drop-down menu.</p><img style=margin-left:0;width:50%;height:auto alt="Dashboard terminal pod configuration" src=/__resources/f7b10d48edf44c17ba838ff5c429e39d_595683.png><br><p>The dashboard then schedules a pod and opens a shell session to the node.</p><p>To get access to common binaries installed on the host, prefix the command with <code>chroot /hostroot</code>.
Note that the path depends on where the root path is mounted in the container.
In the default image used by the Dashboard, it is under <code>/hostroot</code>.</p><img style=margin-left:0 alt="Dashboard terminal pod configuration" src=/__resources/3da659e9cc4744a2ad3e1c6a50d39c04_1a182a.png><br><h2 id=gardenctl-shell>gardenctl shell</h2><p><strong>Prerequisite</strong>: <code>kubectl</code> and <a href=https://github.com/gardener/gardenctl>gardenctl are available and configured</a>.</p><p>First, target a Garden cluster containing all the Shoot definitions.</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ gardenctl target garden &lt;target-garden&gt;
</code></pre></div><p>Target an available Shoot by name.
This sets up the context and configures the <code>kubeconfig</code> file of the Shoot cluster.
Subsequent commands will execute in this context.</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ gardenctl target shoot &lt;target-shoot&gt;
</code></pre></div><p>Get the nodes of the Shoot cluster.</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ gardenctl kubectl get nodes 
</code></pre></div><p>Pick a node name from the list above and get a root shell access to it.</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ gardenctl shell &lt;target-node&gt;
</code></pre></div><h2 id=gardener-ops-toolbelt>Gardener Ops Toolbelt</h2><p><strong>Prerequisite</strong>: <code>kubectl</code> is available.</p><p>The <a href=https://github.com/gardener/ops-toolbelt>Gardener ops-toolbelt</a> can be used as a convenient way to deploy a root pod to a node.
The pod uses an image that is bundled with a bunch of useful <a href=https://github.com/gardener/ops-toolbelt/tree/master/dockerfile-configs>troubleshooting tools</a>.
This is also the same image that is used by default when using the Gardener Dashboard terminal feature as described in the <a href=#gardener-dashboard>previous section</a>.</p><p>The easiest way to use the <a href=https://github.com/gardener/ops-toolbelt>Gardener ops-toolbelt</a> is to execute
the <a href=https://github.com/gardener/ops-toolbelt/blob/master/hacks/ops-pod><code>ops-pod</code> script</a> in the <code>hacks</code> folder.
To get root shell access to a node, execute the aforementioned script by supplying the target node name as an argument:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ &lt;path-to-ops-toolbelt-repo&gt;/hacks/ops-pod &lt;target-node&gt;
</code></pre></div><h2 id=custom-root-pod>Custom root pod</h2><p>Alternatively, a pod can be <a href=https://kubernetes.io/docs/concepts/configuration/assign-pod-node/>assigned</a> to a target node and a shell can
be opened via <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/>standard Kubernetes means</a>.
To enable root access to the node, the pod specification requires proper <code>securityContext</code> and <code>volume</code> properties.</p><p>For instance you can use the following pod manifest, after changing <target-node-name>with the name of the node you want this pod attached to:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>privileged-pod</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>nodeSelector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>kubernetes.io/hostname</span><span class=p>:</span><span class=w> </span><span class=l>&lt;target-node-name&gt;</span><span class=w>
</span><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>busybox</span><span class=w>
</span><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>busybox</span><span class=w>
</span><span class=w>    </span><span class=nt>stdin</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>    </span><span class=nt>securityContext</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>privileged</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>    </span><span class=nt>volumeMounts</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>host-root-volume</span><span class=w>
</span><span class=w>      </span><span class=nt>mountPath</span><span class=p>:</span><span class=w> </span><span class=l>/host</span><span class=w>
</span><span class=w>      </span><span class=nt>readOnly</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>  </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>host-root-volume</span><span class=w>
</span><span class=w>    </span><span class=nt>hostPath</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>/</span><span class=w>
</span><span class=w>  </span><span class=nt>hostNetwork</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>  </span><span class=nt>hostPID</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>  </span><span class=nt>restartPolicy</span><span class=p>:</span><span class=w> </span><span class=l>Never</span><span class=w>
</span></code></pre></div><h1 id=ssh-access-to-a-node-that-failed-to-join-the-cluster>SSH access to a node that failed to join the cluster</h1><p>This section explores two options that can be used to get SSH access to a node that failed to join the cluster.
As it is not possible to schedule a pod on the node, the Kubernetes-based methods explored so far cannot be used in this scenario.</p><p>Additionally, Gardener typically provisions worker instances in a private subnet of the VPC, hence - there is no public IP address that could be used for direct SSH access.</p><p>For this scenario, cloud providers typically have extensive documentation (e.g <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstances.html>AWS</a> & <a href=https://cloud.google.com/compute/docs/instances/connecting-to-instance>GCP</a>
and in <a href=https://cloud.google.com/compute/docs/instances/connecting-advanced#vpn>some cases tooling support</a>).
However, these approaches are mostly cloud provider specific, require interaction via their CLI and API or sometimes
the installation of a <a href=https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-install-ssm-agent.html>cloud provider specific agent</a> one the node.</p><p>Alternatively, <code>gardenctl</code> can be used providing a cloud provider agnostic and out-of-the-box support to get ssh access to an instance in a private subnet.
Currently <code>gardenctl</code> supports AWS, GCP, Openstack, Azure and Alibaba Cloud.</p><h2 id=identifying-the-problematic-instance>Identifying the problematic instance</h2><p>First, the problematic instance has to be identified.
In Gardener, worker pools can be created in different cloud provider regions, zones and accounts.</p><p>The instance would typically show up as successfully started / running in the cloud provider dashboard or API and it is not immediately obvious which one has a problem.
Instead, we can use the Gardener API / CRDs to obtain the faulty instance identifier in a cloud-agnostic way.</p><p>Gardener uses the <a href=https://github.com/gardener/machine-controller-manager>Machine Controller Manager</a> to create the Shoot worker nodes.
For each worker node, the Machine Controller Manager creates a <code>Machine</code> CRD in the Shoot namespace in the respective <code>Seed</code> cluster.
Usually the problematic instance can be identified as the respective <code>Machine</code> CRD has status <code>pending</code>.</p><p>The instance / node name can be obtained from the <code>Machine</code> <code>.status</code> field:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ kubectl get machine &lt;machine-name&gt; -o json | jq -r .status.node
</code></pre></div><p>This is all the information needed to go ahead and use <code>gardenctl ssh</code> to get a shell to the node.
In addition, the used cloud provider, the specific identifier of the instance and the instance region can be identified from the <code>Machine</code> CRD.</p><p>Get the identifier of the instance via:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ kubectl get machine &lt;machine-name&gt; -o json | jq -r .spec.providerID // e.g aws:///eu-north-1/i-069733c435bdb4640
</code></pre></div><p>The identifier shows that the instance belongs to the cloud provider <code>aws</code> with the ec2 instance-id <code>i-069733c435bdb4640</code> in region <code>eu-north-1</code>.</p><p>To get more information about the instance, check out the <code>MachineClass</code> (e.g <code>AWSMachineClass</code>) that is associated with each <code>Machine</code> CRD in the <code>Shoot</code> namespace of the <code>Seed</code> cluster.
The <code>AWSMachineClass</code> contains the machine image (<a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html>ami</a>), machine-type, iam information, network-interfaces, subnets, security groups and attached volumes.</p><p>Of course, the information can also be used to get the instance with the cloud provider CLI / API.</p><h2 id=gardenctl-ssh>gardenctl ssh</h2><p>Using the node name of the problematic instance, we can use the <code>gardenctl ssh</code> command to get SSH access to the cloud provider
instance via an automatically set up <a href=https://en.wikipedia.org/wiki/Bastion_host>bastion host</a>.
<code>gardenctl</code> takes care of spinning up the <code>bastion</code> instance, setting up the SSH keys, ports and security groups and opens a root shell on the target instance.
After the SSH session has ended, <code>gardenctl</code> deletes the created cloud provider resources.</p><p>Use the following commands:</p><p>First, target a Garden cluster containing all the Shoot definitions.</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ gardenctl target garden &lt;target-garden&gt;
</code></pre></div><p>Target an available Shoot by name.
This sets up the context, configures the <code>kubeconfig</code> file of the Shoot cluster and downloads the cloud provider credentials.
Subsequent commands will execute in this context.</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ gardenctl target shoot &lt;target-shoot&gt;
</code></pre></div><p>This uses the cloud provider credentials to spin up the bastion and to open a shell on the target instance.</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ gardenctl ssh &lt;target-node&gt;
</code></pre></div><h2 id=ssh-with-manually-created-bastion-on-aws>SSH with manually created Bastion on AWS</h2><p>In case you are not using <code>gardenctl</code> or want to control the bastion instance yourself, you can also manually set it up.
The steps described here are generally the same as <a href=https://github.com/gardener/gardenctl/blob/10a537942b94234914758c0f6d053dc1cf218ecd/pkg/cmd/ssh_aws.go#L53-L52>those used by <code>gardenctl</code> internally</a>.
Despite some cloud provider specifics they can be generalized to the following list:</p><ul><li>Open port 22 on the target instance.</li><li>Create an instance / VM in a public subnet (bastion instance needs to have public ip address).</li><li>Set-up security groups, roles and open port 22 for the bastion instance.</li></ul><p>The following diagram shows an overview how the SSH access to the target instance works:</p><img style=margin-left:0 alt="SSH Bastion diagram" src=/__resources/913441003e5641bc90249bdc07d55656_a35abd.png><br><p>This guide demonstrates the setup of a bastion on AWS.</p><p><strong>Prerequisites:</strong></p><ul><li>The <code>AWS CLI</code> is set up.</li><li>Obtain target instance-id (see <a href=#identifying-the-problematic-instance>here</a>).</li><li>Obtain the VPC ID the Shoot resources are created in. This can be found in the <code>Infrastructure</code> CRD in the <code>Shoot</code> namespace in the <code>Seed</code>.</li><li>Make sure that port 22 on the target instance is open (default for Gardener deployed instances).<ul><li>Extract security group via</li></ul><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ aws ec2 describe-instances --instance-ids &lt;instance-id&gt;
</code></pre></div><ul><li>Check for rule that allows inbound connections on port 22:</li></ul><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ aws ec2 describe-security-groups --group-ids=&lt;security-group-id&gt;
</code></pre></div><ul><li>If not available, create the rule with the following comamnd:</li></ul><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ aws ec2 authorize-security-group-ingress --group-id &lt;security-group-id&gt;  --protocol tcp --port 22 --cidr 0.0.0.0/0
</code></pre></div></li></ul><h3 id=create-the-bastion-security-group>Create the Bastion Security Group</h3><ul><li><p>The common name of the security group is <code>&lt;shoot-name>-bsg</code>. Create the security group:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ aws ec2 create-security-group --group-name &lt;bastion-security-group-name&gt;  --description ssh-access --vpc-id &lt;VPC-ID&gt;
</code></pre></div></li><li><p>Optionally, create identifying tags for the security group:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ aws ec2 create-tags --resources &lt;bastion-security-group-id&gt; --tags Key=component,Value=&lt;tag&gt;
</code></pre></div></li><li><p>Create permission in the bastion security group that allows ssh access on port 22.</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ aws ec2 authorize-security-group-ingress --group-id &lt;bastion-security-group-id&gt;  --protocol tcp --port 22 --cidr 0.0.0.0/0
</code></pre></div></li><li><p>Create an IAM role for the bastion instance with the name <code>&lt;shoot-name>-bastions</code>:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ aws iam create-role --role-name &lt;shoot-name&gt;-bastions
</code></pre></div><p>The content should be:</p></li></ul><div class=highlight><pre class=chroma><code class=language-json data-lang=json><span class=p>{</span>
<span class=nt>&#34;Version&#34;</span><span class=p>:</span> <span class=s2>&#34;2012-10-17&#34;</span><span class=p>,</span>
<span class=nt>&#34;Statement&#34;</span><span class=p>:</span> <span class=p>[</span>
    <span class=p>{</span>
        <span class=nt>&#34;Effect&#34;</span><span class=p>:</span> <span class=s2>&#34;Allow&#34;</span><span class=p>,</span>
        <span class=nt>&#34;Action&#34;</span><span class=p>:</span> <span class=p>[</span>
            <span class=s2>&#34;ec2:DescribeRegions&#34;</span>
        <span class=p>],</span>
        <span class=nt>&#34;Resource&#34;</span><span class=p>:</span> <span class=p>[</span>
            <span class=s2>&#34;*&#34;</span>
        <span class=p>]</span>
    <span class=p>}</span>
<span class=p>]</span>
<span class=p>}</span>
</code></pre></div><ul><li><p>Create the instance profile with name <code>&lt;shoot-name>-bastions</code>:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ aws iam create-instance-profile --instance-profile-name &lt;name&gt;
</code></pre></div></li><li><p>Add the created role to the instance profile:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ aws iam add-role-to-instance-profile --instance-profile-name &lt;instance-profile-name&gt; --role-name &lt;role-name&gt;
</code></pre></div></li></ul><h3 id=create-the-bastion-instance>Create the bastion instance</h3><p>Next, in order to be able to <code>ssh</code> into the bastion instance, the instance has to be set up with a user with a public ssh key.
Create a user <code>gardener</code> that has the same Gardener-generated public ssh key as the target instance.</p><ul><li><p>First, we need to get the public part of the <code>Shoot</code> ssh-key.
The ssh-key is stored in a secret in the the project namespace in the Garden cluster.
The name is: <code>&lt;shoot-name>-ssh-publickey</code>.
Get the key via:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\&#34;id_rsa.pub\&#34;
</code></pre></div></li><li><p>A script handed over as <code>user-data</code> to the bastion <code>ec2</code> instance, can be used to create the <code>gardener</code> user and add the ssh-key.
For your convenience, you can use the following script to generate the <code>user-data</code>.</p></li></ul><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=cp>#!/bin/bash -eu
</span><span class=cp></span>saveUserDataFile <span class=o>()</span> <span class=o>{</span>
  <span class=nv>ssh_key</span><span class=o>=</span><span class=nv>$1</span>

cat &gt; gardener-bastion-userdata.sh <span class=s>&lt;&lt;EOF
</span><span class=s>#!/bin/bash -eu
</span><span class=s>id gardener || useradd gardener -mU
</span><span class=s>mkdir -p /home/gardener/.ssh
</span><span class=s>echo &#34;$ssh_key&#34; &gt; /home/gardener/.ssh/authorized_keys
</span><span class=s>chown gardener:gardener /home/gardener/.ssh/authorized_keys
</span><span class=s>echo &#34;gardener ALL=(ALL) NOPASSWD:ALL&#34; &gt;/etc/sudoers.d/99-gardener-user
</span><span class=s>EOF</span>
<span class=o>}</span>


<span class=k>if</span> <span class=o>[</span> -p /dev/stdin <span class=o>]</span><span class=p>;</span> <span class=k>then</span>
    <span class=nb>read</span> -r input
    cat <span class=p>|</span> saveUserDataFile <span class=s2>&#34;</span><span class=nv>$input</span><span class=s2>&#34;</span>
<span class=k>else</span>
    pbpaste <span class=p>|</span> saveUserDataFile <span class=s2>&#34;</span><span class=nv>$input</span><span class=s2>&#34;</span>
<span class=k>fi</span>
</code></pre></div><ul><li><p>Use the script by handing-over the public ssh-key of the <code>Shoot</code> cluster:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\&#34;id_rsa.pub\&#34; | ./generate-userdata.sh
</code></pre></div><p>This generates a file called <code>gardener-bastion-userdata.sh</code> in the same directory containing the <code>user-data</code>.</p></li><li><p>The following information is needed to create the bastion instance:</p><p><code>bastion-IAM-instance-profile-name</code></p><ul><li>Use the created instance profile with name <code>&lt;shoot-name>-bastions</code></li></ul><p><code>image-id</code></p><ul><li>Possible use the same image-id as for the target instance (or any other image). Has cloud provider specific format (AWS: <code>ami</code>).</li></ul><p><code>ssh-public-key-name</code></p><ul><li>This is the ssh key pair already created in the Shoot&rsquo;s cloud provider account by Gardener during the <code>Infrastructure</code> CRD reconciliation.</li><li>The name is usually: <code>&lt;shoot-name>-ssh-publickey</code></li></ul><p><code>subnet-id</code></p><ul><li>Choose a subnet that is attached to an <code>Internet Gateway</code> and <code>NAT Gateway</code> (bastion instance must have a public IP).</li><li>The Gardener created public subnet with the name <code>&lt;shoot-name>-public-utility-&lt;xy></code> can be used.
Please check the created subnets with the cloud provider.</li></ul><p><code>bastion-security-group-id</code></p><ul><li>Use the id of the created bastion security group.</li></ul><p><code>file-path-to-userdata</code></p><ul><li><p>Use the filepath to <code>user-data</code> file generated in the previous step.</p></li><li><p><code>bastion-instance-name</code></p><ul><li>Optional to tag the instance.</li><li>Usually <code>&lt;shoot-name>-bastions</code></li></ul></li></ul></li><li><p>Create the bastion instance via:</p></li></ul><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ ec2 run-instances --iam-instance-profile Name=&lt;bastion-IAM-instance-profile-name&gt; --image-id &lt;image-id&gt;  --count 1 --instance-type t3.nano --key-name &lt;ssh-public-key-name&gt;  --security-group-ids &lt;bastion-security-group-id&gt; --subnet-id &lt;subnet-id&gt; --associate-public-ip-address --user-data &lt;file-path-to-userdata&gt; --tag-specifications ResourceType=instance,Tags=[{Key=Name,Value=&lt;bastion-instance-name&gt;},{Key=component,Value=&lt;mytag&gt;}] ResourceType=volume,Tags=[{Key=component,Value=&lt;mytag&gt;}]&#34;
</code></pre></div><p>Capture the <code>instance-id</code> from the reponse and wait until the <code>ec2</code> instance is running and has a public ip address.</p><h2 id=connecting-to-the-target-instance>Connecting to the target instance</h2><p>Save the private key of the ssh-key-pair in a temporary local file for later use.</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ umask 077

$ kubectl get secret &lt;shoot-name&gt;.ssh-keypair -o json | jq -r .data.\&#34;id_rsa\&#34; | base64 -d &gt; id_rsa.key
</code></pre></div><p>Use the private ssh key to ssh into the bastion instance.</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ ssh -i &lt;path-to-private-key&gt; gardener@&lt;public-bastion-instance-ip&gt; 
</code></pre></div><p>If that works, connect from your local terminal to the target instance via the bastion.</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ ssh  -i &lt;path-to-private-key&gt; -o ProxyCommand=&#34;ssh -W %h:%p -i &lt;private-key&gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no gardener@&lt;public-ip-bastion&gt;&#34; gardener@&lt;private-ip-target-instance&gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no
</code></pre></div><h2 id=cleanup>Cleanup</h2><p>Do not forget to cleanup the created resources. Otherwise Gardener will eventually fail to delete the Shoot.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9722c3efab308a9aee16ce3e552f405f>4.2 - How to debug a pod</h1><div class=lead>Your pod doesn&rsquo;t run as expected. Are there any log files? Where? How could I debug a pod?</div><h2 id=introduction>Introduction</h2><p>Kubernetes offers powerful options to get more details about startup or runtime failures of pods as e.g. described in
<a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/>Application Introspection and Debugging</a>
or <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/>Debug Pods and Replication Controllers</a>.</p><p>In order to identify pods with potential issus you could e.g. run <code>kubectl get pods --all-namespaces | grep -iv Running</code> to filter
out the pods which are not in the state <code>Running</code>. One of frequent error state is <code>CrashLoopBackOff</code>, which tells that
a pod crashes right after the start. Kubernetes then tries to restart the pod, but often the pod startup fails again.</p><p><strong>Here is a short list of possible reasons which might lead to a pod crash:</strong></p><ol><li>error during image pull caused by e.g. wrong/missing secrets or wrong/missing image</li><li>the app runs in an error state caused e.g. by missing environmental variables (ConfigMaps) or secrets</li><li>liveness probe failed</li><li>too high resource consumption (memory and/or CPU) or too strict quota settings</li><li>persistent volumes can&rsquo;t be created/mounted</li><li>the container image is not updated</li></ol><p>Basically, the commands <code>kubectl logs ...</code> and <code>kubectl describe ...</code> with additional parameters are used to get more
detailed information. By calling e.g. <code>kubectl logs --help</code> you get more detailed information about the command and its
parameters.</p><p>In the next sections you&rsquo;ll find some basic approaches to get some ideas what went wrong.</p><p>Remarks:</p><ul><li>Even if the pods seem to be running as the status <code>Running</code> indicates, a high counter of the <code>Restarts</code> shows potential problems</li><li>There is as well an <a href=https://kubernetes.io/docs/tutorials/kubernetes-basics/explore-intro/>interactive Tutorial Troubleshooting with Kubectl</a>
available which explains basic debugging activities</li><li>The examples below are deployed into the namespace <code>default</code>. In case you want to change it use the optional
parameter <code>--namespace &lt;your-namespace></code> to select the target namespace. They require Kubernetes release ≥ <em>1.8</em>.</li></ul><h2 id=prerequisites>Prerequisites</h2><p>Your deployment was successful (no logical/syntactical errors in the manifest files), but the pod(s) aren&rsquo;t running.</p><h2 id=error-caused-by-wrong-image-name>Error caused by wrong image name</h2><p>You run <code>kubectl describe pod &lt;your-pod> &lt;your-namespace></code> to get detailed information about the pod startup.</p><p>In the <code>Events</code> section, you get an error message like <code>Failed to pull image ...</code> and <code>Reason: Failed</code>. The pod is
in state <code>ImagePullBackOff</code>.</p><p>The example below is based on <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/>demo in Kubernetes documentation</a>. In all examples the <code>default</code> namespace is used.</p><p>First, cleanup with</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl delete pod termination-demo
</code></pre></div><p>Next, create a resource based on the yaml content below</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod </span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo-container</span><span class=w>
</span><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>debiann</span><span class=w>
</span><span class=w>    </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;/bin/sh&#34;</span><span class=p>]</span><span class=w>
</span><span class=w>    </span><span class=nt>args</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;-c&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log&#34;</span><span class=p>]</span><span class=w>
</span></code></pre></div><p><code>kubectl describe pod termination-demo</code> lists the following content in the <code>Event</code> section</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>Events:
  FirstSeen	LastSeen	Count	From							SubObjectPath					Type		Reason			Message
  ---------	--------	-----	----							-------------					--------	------			-------
  2m		2m		1	default-scheduler											Normal		Scheduled		Successfully assigned termination-demo to ip-10-250-17-112.eu-west-1.compute.internal
  2m		2m		1	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded <span class=k>for</span> volume <span class=s2>&#34;default-token-sgccm&#34;</span> 
  2m		1m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers<span class=o>{</span>termination-demo-container<span class=o>}</span>	Normal		Pulling			pulling image <span class=s2>&#34;debiann&#34;</span>
  2m		1m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers<span class=o>{</span>termination-demo-container<span class=o>}</span>	Warning		Failed			Failed to pull image <span class=s2>&#34;debiann&#34;</span>: rpc error: <span class=nv>code</span> <span class=o>=</span> Unknown <span class=nv>desc</span> <span class=o>=</span> Error: image library/debiann:latest not found
  2m		54s		10	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Warning		FailedSync		Error syncing pod
  2m		54s		6	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers<span class=o>{</span>termination-demo-container<span class=o>}</span>	Normal		BackOff			Back-off pulling image <span class=s2>&#34;debiann&#34;</span>
</code></pre></div><p>The error message with <code>Reason: Failed</code> tells that there is an error during pulling the image. A closer look at the
image name indicates a misspelling.</p><h2 id=app-runs-in-an-error-state-caused-by-missing-configmaps-or-secrets>App runs in an error state caused by missing ConfigMaps or Secrets</h2><p>This example illustrates the behavior in case of the app expecting environment variables but the corresponding
Kubernetes artifacts are missing.</p><p>First, cleanup with</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=l>kubectl delete deployment termination-demo</span><span class=w>
</span><span class=w></span><span class=l>kubectl delete configmaps app-env</span><span class=w>
</span></code></pre></div><p>Next, deploy this manifest</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>     </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo-container</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>debian</span><span class=w>
</span><span class=w>        </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;/bin/sh&#34;</span><span class=p>]</span><span class=w>
</span><span class=w>        </span><span class=nt>args</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;-c&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;sed \&#34;s/foo/bar/\&#34; &lt; $MYFILE&#34;</span><span class=p>]</span><span class=w>
</span></code></pre></div><p>Now, the command <code>kubectl get pods</code> lists the pod <code>termination-demo-xxx</code> in the state <code>Error</code> or <code>CrashLoopBackOff</code>.
The command <code>kubectl describe pod termination-demo-xxx</code> tells that there is no error during startup but gives no clue
about what caused the crash.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>Events:
  FirstSeen	LastSeen	Count	From							SubObjectPath					Type		Reason		Message
  ---------	--------	-----	----							-------------					--------	------		-------
  19m		19m		1	default-scheduler											Normal		Scheduled	Successfully assigned termination-demo-5fb484867d-xz2x9 to ip-10-250-17-112.eu-west-1.compute.internal
  19m		19m		1	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded <span class=k>for</span> volume <span class=s2>&#34;default-token-sgccm&#34;</span> 
  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers<span class=o>{</span>termination-demo-container<span class=o>}</span>	Normal		Pulling		pulling image <span class=s2>&#34;debian&#34;</span>
  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers<span class=o>{</span>termination-demo-container<span class=o>}</span>	Normal		Pulled		Successfully pulled image <span class=s2>&#34;debian&#34;</span>
  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers<span class=o>{</span>termination-demo-container<span class=o>}</span>	Normal		Created		Created container
  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers<span class=o>{</span>termination-demo-container<span class=o>}</span>	Normal		Started		Started container
  19m		14m		24	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers<span class=o>{</span>termination-demo-container<span class=o>}</span>	Warning		BackOff		Back-off restarting failed container
  19m		4m		69	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Warning		FailedSync	Error syncing pod
</code></pre></div><p>The command <code>kubectl get logs termination-demo-xxx</code> gives access to the output, the application writes on stderr and
stdout. In this case, you get an output like</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>/bin/sh: 1: cannot open : No such file
</code></pre></div><p>So you need to have a closer look at the application. In this case the environmental variable <code>MYFILE</code>is missing. To fix this
issue you could e.g. add a ConfigMap to your deployment as it is shown in the manifest listed below.</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ConfigMap</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>app-env</span><span class=w>
</span><span class=w></span><span class=nt>data</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>MYFILE</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/etc/profile&#34;</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>     </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo-container</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>debian</span><span class=w>
</span><span class=w>        </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;/bin/sh&#34;</span><span class=p>]</span><span class=w>
</span><span class=w>        </span><span class=nt>args</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;-c&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;sed \&#34;s/foo/bar/\&#34; &lt; $MYFILE&#34;</span><span class=p>]</span><span class=w>
</span><span class=w>        </span><span class=nt>envFrom</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>configMapRef</span><span class=p>:</span><span class=w>
</span><span class=w>            </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>app-env </span><span class=w>
</span></code></pre></div><p>Note that once you fix the error and re-run the scenario, you might still see the pod in <code>CrashLoopBackOff</code> status.
It is because the container finishes the command <code>sed ...</code> and runs to completion. In order to keep the container in <code>Running</code> status,
a long running task is required, e.g.</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ConfigMap</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>app-env</span><span class=w>
</span><span class=w></span><span class=nt>data</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>MYFILE</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;/etc/profile&#34;</span><span class=w>
</span><span class=w>  </span><span class=nt>SLEEP</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;5&#34;</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>     </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo-container</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>debian</span><span class=w>
</span><span class=w>        </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;/bin/sh&#34;</span><span class=p>]</span><span class=w>
</span><span class=w>        </span><span class=c># args: [&#34;-c&#34;, &#34;sed \&#34;s/foo/bar/\&#34; &lt; $MYFILE&#34;]</span><span class=w>
</span><span class=w>        </span><span class=nt>args</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;-c&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;while true; do sleep $SLEEP; echo sleeping; done;&#34;</span><span class=p>]</span><span class=w>
</span><span class=w>        </span><span class=nt>envFrom</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>configMapRef</span><span class=p>:</span><span class=w>
</span><span class=w>            </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>app-env</span><span class=w>
</span></code></pre></div><h2 id=too-high-resource-consumption-or-too-strict-quota-settings>Too high resource consumption or too strict quota settings</h2><p>You can optionally specify the amount of memory and/or CPU your container gets during runtime. In case these settings are missing,
the default requests settings are taken: CPU: 0m (in Milli CPU) and RAM: 0Gi which indicate no other limits than the
ones of the node(s) itself. Find more details in <a href=https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/>Configure Default Memory Requests and
Limits for a Namespace</a>,</p><p>In case your application needs more resources, Kubernetes distinguishes between <code>requests</code> and <code>limit</code> settings: <code>requests</code>
specify the guaranteed amount of resource, whereas <code>limit</code> tells Kubernetes the maximum amount of resource the container might
need. Mathematically both settings could be described by the relation <code>0 &lt;= requests &lt;= limit</code>. For both settings you need to
consider the total amount of resources the available nodes provide. For a detailed description of the concept see <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md>Resource Quality of
Service in Kubernetes</a>.</p><p>Use <code>kubectl describe nodes</code> to get a first overview of the resource consumption of your cluster. Of special interest are the
figures indicating the amount of CPU and Memory Requests at the bottom of the output.</p><p>The next example demonstrates what happens in case the CPU request is too high in order to be managed by your cluster.</p><p>First, cleanup with</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=l>kubectl delete deployment termination-demo</span><span class=w>
</span><span class=w></span><span class=l>kubectl delete configmaps app-env</span><span class=w>
</span></code></pre></div><p>Next, adapt the <code>cpu</code> in the yaml below to be slightly higher than the remaining cpu resources in your cluster and deploy
this manifest. In this example <code>600m</code> (milli CPUs) are requested in a Kubernetes system with a single 2 Core worker
node which results in an error message.</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>     </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>termination-demo-container</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>debian</span><span class=w>
</span><span class=w>        </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;/bin/sh&#34;</span><span class=p>]</span><span class=w>
</span><span class=w>        </span><span class=nt>args</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;-c&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log&#34;</span><span class=p>]</span><span class=w>
</span><span class=w>        </span><span class=nt>resources</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>requests</span><span class=p>:</span><span class=w>
</span><span class=w>            </span><span class=nt>cpu</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;600m&#34;</span><span class=w> 
</span></code></pre></div><p>The command <code>kubectl get pods</code> lists the pod <code>termination-demo-xxx</code> in the state <code>Pending</code>. More details on why this happens
could be found by using the command <code>kubectl describe pod termination-demo-xxx</code>:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl describe po termination-demo-fdb7bb7d9-mzvfw
Name:           termination-demo-fdb7bb7d9-mzvfw
Namespace:      default
...
Containers:
  termination-demo-container:
    Image:      debian
    Port:       &lt;none&gt;
    Host Port:  &lt;none&gt;
    Command:
      /bin/sh
    Args:
      -c
      sleep <span class=m>10</span> <span class=o>&amp;&amp;</span> <span class=nb>echo</span> Sleep expired &gt; /dev/termination-log
    Requests:
      cpu:        <span class=m>6</span>
    Environment:  &lt;none&gt;
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-t549m <span class=o>(</span>ro<span class=o>)</span>
Conditions:
  Type           Status
  PodScheduled   False
Events:
  Type     Reason            Age               From               Message
  ----     ------            ----              ----               -------
  Warning  FailedScheduling  9s <span class=o>(</span>x7 over 40s<span class=o>)</span>  default-scheduler  0/2 nodes are available: <span class=m>2</span> Insufficient cpu.
</code></pre></div><p>More details in</p><ul><li><a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>Managing Compute Resources for Containters</a></li><li><a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md>Resource Quality of Service in Kubernetes</a></li></ul><p>Remark:</p><ul><li>This example works similarly when specifying a too high request for memory</li><li>In case you configured an autoscaler range when creating your Kubernetes cluster another worker node will be started automatically if you didn&rsquo;t reach the maximum number of worker nodes</li><li>If your app is running out of memory (the memory settings are too small), you typically find <code>OOMKilled</code> (Out Of Memory) message in the <code>Events</code> section fo the <code>kubectl describe pod ...</code> output</li></ul><h2 id=why-was-the-container-image-not-updated>Why was the container image not updated?</h2><p>You applied a fix in your app, created a new container image and pushed it into your container repository. After
redeploying your Kubernetes manifests you expected to get the updated app, but still the same bug is in the new
deployment present.</p><p>This behavior is related to how Kubernetes decides whether to pull a new docker image or to use the cached one.</p><p>In case you didn&rsquo;t change the image tag, the default image policy <em>IfNotPresent</em> tells Kubernetes to use the cached
image (see <a href=https://kubernetes.io/docs/concepts/containers/images/>Images</a>).</p><p>As a best practice you should not use the tag <code>latest</code> and change the image tag whenever you changed anything in your
image (see <a href=https://kubernetes.io/docs/concepts/configuration/overview/#container-images>Configuration Best Practices</a>).</p><p>Find more details in <a href=https://github.com/gardener/documentation/blob/master/website/documentation/guides/applications/imagePullPolicy>FAQ Container Image not updating</a></p><h2 id=links>Links</h2><ul><li><a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/>Application Introspection and Debugging</a></li><li><a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/>Debug Pods and Replication Controllers</a></li><li><a href=https://kubernetes.io/docs/concepts/cluster-administration/logging/>Logging Architecture</a></li><li><a href=https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/>Configure Default Memory Requests and Limits for a Namespace</a></li><li><a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>Managing Compute Resources for Containters</a></li><li><a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md>Resource Quality of Service in Kubernetes</a></li><li><a href=https://kubernetes.io/docs/tutorials/kubernetes-basics/explore-intro/>Interactive Tutorial Troubleshooting with Kubectl</a></li><li><a href=https://kubernetes.io/docs/concepts/containers/images/>Images</a></li><li><a href=https://kubernetes.io/docs/concepts/configuration/overview/#container-images>Kubernetes Best Practises</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b5575668e5d9ec9a137723e26819e36b>4.3 - tail -f /var/log/my-application.log</h1><div class=lead>Aggregate log files from different pods</div><h2 id=problem>Problem</h2><p>One thing that always bothered me was that I couldn&rsquo;t get logs of several pods at once with <code>kubectl</code>. A simple
<code>tail -f &lt;path-to-logfile></code> isn&rsquo;t possible at all. Certainly you can use <code>kubectl logs -f &lt;pod-id></code>, but it doesn&rsquo;t
help if you want to monitor more than one pod at a time.</p><p>This is something you really need a lot, at least if you run several instances of a pod behind a <code>deployment</code>.
This is even more so if you don&rsquo;t have a Kibana setup or similar.</p><img src=/__resources/howto-kubetail_dd56d4.png width=100%><h2 id=solution>Solution</h2><p>Luckily, there are smart developers out there who always come up with solutions. The <strong>finding of the week</strong> is
a small bash script that allows you to aggregate log files of several pods at the same time in
a simple way. The script is called <code>kubetail</code> and is available at
<a href=https://github.com/johanhaleby/kubetail>GitHub</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-54886e62cc8767ab592a5259b6e86b39>5 - Applications</h1></div><div class=td-content><h1 id=pg-cdf7f44ffe002ba4c8c2fba6647dcc99>5.1 - Access a port of a pod locally</h1><h2 id=question>Question</h2><p>You deployed an application with a web UI or an internal endpoint in your Kubernetes (K8s) cluster. How can I access
this endpoint <strong>without an external load balancer</strong> (e.g. Ingress)?
This tutorial presents two options:</p><ul><li>Using Kubernetes port forward</li><li>Using Kubernetes apiserver proxy</li></ul><p>Please note that the options described here are mostly for quick testing or troubleshooting your application. For enabling access to your application for productive environment, please refer to <a href=https://github.com/gardener/documentation/blob/master/website/documentation/guides/applications/service-access/>Access my service</a></p><h2 id=solution-1-using-kubernetes-port-forward>Solution 1: Using Kubernetes port forward</h2><p>You could use the port forwarding functionality of <code>kubectl</code> to access the pods from your
local host <strong>without involving a service</strong>.</p><p>To access any pod follow these steps:</p><ul><li>Run <code>kubectl get pods</code></li><li>Note down the name of the pod in question as <code>&lt;your-pod-name></code></li><li>Run <code>kubectl port-forward &lt;your-pod-name> &lt;local-port>:&lt;your-app-port></code></li><li>Run a web browser or curl locally and enter the URL <code>http(s)://localhost:&lt;local-port></code></li></ul><p>In addition, <code>kubectl port-forward</code> allows to use a resource name. such as a deployment or service name, to select a matching pod to port forward.
Find more details in the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/>Kubernetes documentation</a>.</p><p>The main drawback of this approach is that the pod&rsquo;s name will change as soon as it is restarted. Moreover, you need
to have a web browser on your client and you need to make sure that the local port is not already used by an
application running on your system. Finally, sometimes port forwarding is canceled due to non obvious reasons.
This leads to a kind of shaky approach. A more robust approach is to access the application using kube-proxy.</p><p><img src=/__resources/howto-port-forward_521e20.svg alt=port-forward></p><h2 id=solution-2-using-apiserver-proxy>Solution 2: Using apiserver proxy</h2><p>There are several different proxies used with Kubernetes, <a href=https://kubernetes.io/docs/concepts/cluster-administration/proxies/>the official documentation</a> provides a good overview.</p><p>In this tutorial we are using apiserver proxy to enable access to services running in Kubernetes without using an Ingress. <strong>Different from the first solution, a service is required for this solution</strong> .</p><p>Use the following URL to access a service via apiserver proxy. For details about apiserver proxy URLs read
<a href=https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services>Discovering builtin services</a></p><p><code>https://&lt;cluster-master>/api/v1/namespace/&lt;namespace>/services/&lt;service>:&lt;service-port>/proxy/&lt;service-endpoint></code></p><p><strong>Example:</strong></p><table><thead><tr><th>cluster-master</th><th style=text-align:center>namespace</th><th style=text-align:right>service</th><th style=text-align:right>service-port</th><th style=text-align:right>service-endpoint</th><th style=text-align:right>url to access service</th></tr></thead><tbody><tr><td>api.testclstr.cpet.k8s.sapcloud.io</td><td style=text-align:center>default</td><td style=text-align:right>nginx-svc</td><td style=text-align:right>80</td><td style=text-align:right>/</td><td style=text-align:right><code>http://api.testclstr.cpet.k8s.sapcloud.io/api/v1/namespaces/default/services/nginx-svc:80/proxy/</code></td></tr><tr><td>api.testclstr.cpet.k8s.sapcloud.io</td><td style=text-align:center>default</td><td style=text-align:right>docker-nodejs-svc</td><td style=text-align:right>4500</td><td style=text-align:right>/cpu?baseNumber=4</td><td style=text-align:right><code>https://api.testclstr.cpet.k8s.sapcloud.io/api/v1/namespaces/default/services/docker-nodejs-svc:4500/proxy/cpu?baseNumber=4</code></td></tr></tbody></table><p>There are applications, which do <strong>not</strong> yet support relative URLs like <a href=https://github.com/prometheus/prometheus/issues/1583>Prometheus</a> (as of end of November, 2017).
This typically leads to missing JavaScript objects when trying to open the URL in a browser. In this case use the
<code>port-forward</code> approach described above.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a7ec86eaf7d4bebd0f35775f37c982c6>5.2 - Access service from outside Kubernetes cluster</h1><div class=lead>Is there an ingress deployed and how is it configured</div><h2 id=tldr>TL;DR</h2><p>To expose your application / service for access from outside the cluster, following options exist:</p><ul><li>Kubernetes Service of type <code>LoadBalancer</code></li><li>Kubernetes Service of type &lsquo;NodePort&rsquo; + Ingress</li></ul><p>This tutorial discusses how to enable access to your application from outside the Kubernetes cluster (sometimes called
North-South traffic). For internal communication amongst pods and services (sometimes called East-West traffic) there
are many examples, <a href=https://cloudnativelabs.github.io/post/2017-04-18-kubernetes-networking/>here</a> is one brief example.</p><h2 id=service-types>Service Types</h2><p>A Service in Kubernetes is an abstraction defining a logical set of Pods and an access policy.<br>Services can be exposed in different ways by specifying a <strong>type</strong> in the service spec,
and different types determine accessibility from inside and outside of cluster.</p><ul><li>ClusterIP</li><li>NodePort</li><li>LoadBalancer</li></ul><p>Type <code>ExternalName</code> is a special case of service and not discussed here.</p><h3 id=type-clusterip>Type ClusterIP</h3><p>A service of type <code>ClusterIP</code> exposes a service on an internal IP in the cluster, which makes the service <strong>only reachable</strong>
from within the cluster. This is the default value if no type is specified.</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx-deployment</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>nginx:1.13.12</span><span class=w>
</span><span class=w>        </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>containerPort</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Service</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx-svc</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>ClusterIP </span><span class=w> </span><span class=c># use ClusterIP as type here</span><span class=w>
</span><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span></code></pre></div><p>Execute following commands to create deployment and service</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl create -f &lt;Your yaml file name&gt;
</code></pre></div><p>Checking the service status</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl get svc nginx-svc
NAME        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>   AGE
nginx-svc   ClusterIP   100.66.125.61   &lt;none&gt;        80/TCP    45m
</code></pre></div><p>As shown above, the service is assigned with a cluster ip address and port 80 as defined in configuration file.<br>You can test the service like this:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># list all existing pods in cluster</span>
$ kubectl get pods
NAME                                           READY     STATUS        RESTARTS   AGE
docker-nodejs-app-76b77494-vwv4d               1/1       Running       <span class=m>0</span>          11d
nginx-deployment-74d949bf69-nvdzs              1/1       Running       <span class=m>0</span>          1h
privileged-pod                                 1/1       Running       <span class=m>0</span>          11d

<span class=c1># test service from within the cluster on the same pod</span>
$ kubectl <span class=nb>exec</span> -it nginx-deployment-74d949bf69-nvdzs  curl 100.66.125.61:80
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
<span class=m>100</span>   <span class=m>612</span>  <span class=m>100</span>   <span class=m>612</span>    <span class=m>0</span>     <span class=m>0</span>  1006k      <span class=m>0</span> --:--:-- --:--:-- --:--:--  597k
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body <span class=o>{</span>
        width: 35em<span class=p>;</span>
        margin: <span class=m>0</span> auto<span class=p>;</span>
        font-family: Tahoma, Verdana, Arial, sans-serif<span class=p>;</span>
    <span class=o>}</span>
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;
...
</code></pre></div><blockquote><p><i class="fa fa-gittip" aria-hidden=true></i> Tip</p><ul><li>The service is also accessible from any other container (even from different pods) within the same cluster, e.g. <code>kubectl -it exec &lt;another POD_NAME> curl &lt;YourServiceClusterIP:YourPort></code>.
You need to make sure command <code>curl</code> is installed in the container.</li><li>You can also find out the dns name of the ClusterIP by command <code>kubectl exec -it &lt;POD_NAME> nslookup &lt;ClusterIP></code>,
replace the IP address with the resolved name in your test.
The resolved name typically looks like <code>nginx-svc.default.svc.cluster.local</code> where <code>nginx-svc</code> is the name of your
service defined in the configuration file.</li></ul></blockquote><h3 id=type-nodeport>Type NodePort</h3><p>Follow the previous example, just replace the type with <code>NodePort</code></p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nn>...</span><span class=w>
</span><span class=w> </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>   </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>NodePort</span><span class=w>
</span><span class=w>   </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>     </span>- <span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w></span><span class=nn>...</span><span class=w>
</span></code></pre></div><p>A service of type <code>NodePort</code> is a ClusterIP service with an additional capability: it is reachable at the IP address
of the node as well as at the assigned cluster IP on the services network.
The way this is accomplished is pretty straightforward: when Kubernetes creates a NodePort service kube-proxy allocates
a port in the range 30000–32767 and opens this port on every node (thus the name “NodePort”).
Connections to this port are forwarded to the service’s cluster IP. If we create the service above and run
<code>kubectl get svc &lt;your-service></code>, we can see the NodePort that has been allocated for it.</p><p>Note that in the in following example, in addition to port 80, port <strong>32521</strong> has been opened as well on the node, in contrast to
the output of &ldquo;ClusterIP&rdquo; case where only port 80 is opened.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl get svc nginx-svc
NAME        TYPE       CLUSTER-IP       EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>        AGE
nginx-svc   NodePort   100.70.105.182   &lt;none&gt;        80:32521/TCP   16m
</code></pre></div><p>Therefore you can access the service <em>from within the cluster</em> in two ways:</p><ul><li>Access via ClusterIP:port</li></ul><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1>#via ClusterIP</span>
kubectl <span class=nb>exec</span> -it nginx-deployment-74d949bf69-7n6bs curl 100.70.105.182:80

<span class=c1>#via internal name of ClusterIP</span>
kubectl <span class=nb>exec</span> -it nginx-deployment-74d949bf69-7n6bs curl nginx-svc.default.svc.cluster.local:80
</code></pre></div><ul><li>Access via NodeIP:NodePort</li></ul><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>
<span class=c1># First find out the Node IP address</span>
$ kubectl describe node
Name:               ip-10-250-20-203.eu-central-1.compute.internal
Roles:              node
Addresses:
  InternalIP:   10.250.20.203
  InternalDNS:  ip-10-250-20-203.eu-central-1.compute.internal
  Hostname:     ip-10-250-20-203.eu-central-1.compute.internal
...


<span class=c1>#via NodeIP:NodePort</span>
kubectl <span class=nb>exec</span> -it nginx-deployment-74d949bf69-7n6bs curl 10.250.20.203:32521

<span class=c1>#via internal name of NodeIP</span>
kubectl <span class=nb>exec</span> -it nginx-deployment-74d949bf69-7n6bs curl ip-10-250-20-203.eu-central-1.compute.internal:32521
</code></pre></div><h3 id=type-loadbalancer>Type LoadBalancer</h3><p>The <code>LoadBalancer</code> type is the simplest approach, which is created by specifying type as <code>LoadBalancer</code>.</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx-deployment</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>nginx:1.13.12</span><span class=w>
</span><span class=w>        </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>containerPort</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Service</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx-svc</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>LoadBalancer </span><span class=w> </span><span class=c># use LoadBalancer as type here</span><span class=w>
</span><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span></code></pre></div><p>Once the service is created, it has an external IP address as shown here:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl get services -l <span class=nv>app</span><span class=o>=</span>nginx-app -o wide
NAME        TYPE           CLUSTER-IP       EXTERNAL-IP                                                                  PORT<span class=o>(</span>S<span class=o>)</span>        AGE       SELECTOR
nginx-svc   LoadBalancer   100.67.182.148   a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com   80:31196/TCP   9m        <span class=nv>app</span><span class=o>=</span>nginx-app
</code></pre></div><p>A service of type LoadBalancer <strong>combines the capabilities of a NodePort with the ability to setup a complete ingress path</strong>.<br>Hence the service can be accessible from outside the cluster without the need for additional components like an Ingress.</p><p>To test the external IP run this curl command from your local machine:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>
$ curl http://a54a62300696611e88ba00af02406931-1787163476.eu-central-1.elb.amazonaws.com

StatusCode        : <span class=m>200</span>
StatusDescription : OK
Content           : &lt;!DOCTYPE html&gt;
                    &lt;html&gt;
                    &lt;head&gt;
                    &lt;title&gt;Welcome to nginx!&lt;/title&gt;
                    &lt;style&gt;
                        body <span class=o>{</span>
                            width: 35em<span class=p>;</span>
                            margin: <span class=m>0</span> auto<span class=p>;</span>
                            font-family: Tahoma, Verdana, Arial, sans-serif<span class=p>;</span>
                        <span class=o>}</span>
                    &lt;/style&gt;
                    &lt;...
RawContent        : HTTP/1.1 <span class=m>200</span> OK
...
</code></pre></div><p>Obviously the service can also is accessed from within the cluster. You can test this in the same way as described in section <code>NodePort</code>.</p><h2 id=loadbalancer-vs-ingress>LoadBalancer vs. Ingress</h2><p>As presented in the previous section, only the service type LoadBalancer enables access from outside the cluster.
However this approach has its own limitation.
You cannot configure a LoadBalancer to terminate HTTPS traffic, virtual hosts or path-based routing. In Kubernetes 1.2
a separate resource called <a href=https://kubernetes.io/docs/concepts/services-networking/ingress/#alternatives>Ingress</a> is
introduced for this purpose.</p><p>You might need to enable the <code>Nginx Ingress</code> add-ons in your gardener dashboard to use some of those functionnality.</p><h3 id=why-an-ingress>Why an Ingress</h3><p>LoadBalancer services are all about extending a service to support external clients. By contrast an Ingress is a
a separate resource that configures a LoadBalancer in a more flexible way.
The Ingress API supports TLS termination, virtual hosts, and path-based routing. It can easily set up a load balancer to
handle multiple backend services. In addition routing traffic is realised in a different way. In the case of the LoadBalancer service, the traffic entering through the
external load balancer is forwarded to the kube-proxy that in turn forwards
the traffic to the selected pods. In contrast, the Ingress LoadBalancer forwards the traffic straight to the selected
pods which is more efficient.</p><p>Typically a service of type LoadBalancer costs at least 40$ per month. This means if your applications needs 10 of them
you already pay 400$ per month just for load balancing.</p><h3 id=how-to-use-the-ingress>How to use the ingress?</h3><p>In the cluster, a nginx-ingress controller has been deployed for you as an LoadBalancer and also registered the DNS
record. Depending on how your cluster is defined, the DNS registration is performed under following conventions:</p><ul><li><strong>k8s-hana.ondemand.com</strong></li></ul><p><code>&lt;gardener_cluster_name>.&lt;gardener_project_name>.shoot.canary.k8s-hana.ondemand.com</code>.</p><p>Both <code>&lt;gardener_cluster_name></code> and <code>&lt;gardener_project_name></code> are defined in Gardener which can be determined on Gardener dashboard.</p><p>This results in the following default DNS endpoints:</p><ul><li><code>api.&lt;cluster_domain></code> Kubernetes API</li><li><code>*.ingress.&lt;cluster_domain></code> Internal nginx ingress</li></ul><h3 id=example-configure-an-ingress-resource-with-service-type-nodeport>Example: Configure an Ingress resource with Service type: NodePort</h3><p>With the configuration below you can reach your service <strong>nginx-svc</strong> with:</p><p><code>http://test.ingress.&amp;lt;GARDENER-CLUSTER-NAME&amp;gt;.&amp;lt;GARDENER-PROJECT-NAME&amp;gt;.shoot.canary.k8s-hana.ondemand.com</code></p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx-deployment</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>nginx:1.13.12</span><span class=w>
</span><span class=w>        </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>containerPort</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Service</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginx-svc</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>NodePort</span><span class=w>
</span><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>nginx-app</span><span class=w>
</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>nginxsvc-ingress</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>rules</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>host</span><span class=p>:</span><span class=w> </span><span class=l>nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com</span><span class=w>
</span><span class=w>    </span><span class=nt>http</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>backend</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>serviceName</span><span class=p>:</span><span class=w> </span><span class=l>nginx-svc</span><span class=w>
</span><span class=w>          </span><span class=nt>servicePort</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span></code></pre></div><p>Show the newly created ingress and test it :</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl get ingress
NAME                    HOSTS                                                               ADDRESS         PORTS     AGE
nginxsvc-ingress        nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com  10.250.20.203   <span class=m>80</span>        29s

$ curl nginxsvc.ingress.eecluster.cpet.shoot.canary.k8s-hana.ondemand.com

StatusCode        : <span class=m>200</span>
StatusDescription : OK
Content           : &lt;!DOCTYPE html&gt;
                    &lt;html&gt;
                    &lt;head&gt;
                    &lt;title&gt;Welcome to nginx!&lt;/title&gt;
                    &lt;style&gt;
                        body <span class=o>{</span>
                            width: 35em<span class=p>;</span>
                            margin: <span class=m>0</span> auto<span class=p>;</span>
                            font-family: Tahoma, Verdana, Arial, sans-serif<span class=p>;</span>
...
</code></pre></div><h2 id=reference>Reference:</h2><ul><li><a href=https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types>Concepts: Kubernetes Service</a></li><li><a href=https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#exposing-the-service>Concepts: Connecting Applications with Services</a></li><li><a href=https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/>Tutorial: Using a Service to Expose Your App</a></li><li><a href=https://kubernetes.io/docs/tutorials/services/source-ip>Tutorial: Using Source IP</a></li><li><a href=https://medium.com/google-cloud/understanding-kubernetes-networking-pods-7117dd28727>Kubernetes Networking</a></li><li><a href=http://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster/>Accessing Kubernetes Pods from Outside of the Cluster</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a9c93d6cbc648f7853fd4d0aae5fd5f3>5.3 - Auditing Kubernetes for Secure Setup</h1><div class=lead>A few insecure configurations in Kubernetes</div><h1 id=auditing-kubernetes-for-secure-setup>Auditing Kubernetes for Secure Setup</h1><pre><code>&lt;object type=&quot;image/svg+xml&quot; data=&quot;./images/teaser.svg&quot; style=&quot;;visibility:hidden; margin: 3rem auto;display: block;&quot; class=&quot;inline reveal-fast drop-shadow&quot;&gt;&lt;/object&gt;
</code></pre><h2 id=increasing-the-security-of-all-gardener-stakeholders>Increasing the Security of all Gardener Stakeholders</h2><p>In summer 2018, the <a href=https://github.com/gardener/gardener>Gardener project team</a> asked <a href=https://kinvolk.io/>Kinvolk</a> to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work is to increase the security of
all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a
Gardener managed shoot cluster resides in the corresponding seed cluster. This is a
<a href=https://kubernetes.io/blog/2018/05/17/gardener/#kubernetes-control-plane>Control-Plane-as-a-Service</a> with
a <a href=https://kubernetes.io/blog/2018/05/17/gardener/#network-air-gap>network air gap</a>.</p><p>Along the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation,
as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service
architecture.</p><h2 id=major-findings>Major Findings</h2><p>From this experience, we’d like to share a few examples of security issues that could happen on a Kubernetes
installation and how to fix them.</p><p>Alban Crequy (<a href=https://kinvolk.io/>Kinvolk</a>) and Dirk Marwinski (<a href=https://www.sap.com>SAP SE</a>) gave a presentation entitled <a href=https://kccncchina2018english.sched.com/event/H2Hd/hardening-multi-cloud-kubernetes-clusters-as-a-service-dirk-marwinski-sap-se-alban-crequy-kinvolk-gmbh>Hardening Multi-Cloud Kubernetes Clusters as a Service</a> at KubeCon 2018 in Shanghai presenting some of the findings.</p><p>Here is a summary of the findings:</p><ul><li><p>Privilege escalation due to insecure configuration of the Kubernetes
API server</p><ul><li>Root cause: Same certificate authority (CA) is used for both the
API server and the proxy that allows accessing the API server.</li><li>Risk: Users can get access to the API server.</li><li>Recommendation: Always use different CAs.</li></ul></li><li><p>Exploration of the control plane network with malicious
HTTP-redirects</p><ul><li><p>Root cause: See detailed description below.</p></li><li><p>Risk: Provoked error message contains full HTTP payload from an
existing endpoint which can be exploited. The contents of the
payload depends on your setup, but can potentially be user data,
configuration data, and credentials.</p></li><li><p>Recommendation:</p><ul><li>Use the latest version of Gardener</li><li>Ensure the seed cluster&rsquo;s container network supports
network policies. Clusters that have been created with
<a href=https://github.com/gardener/kubify>Kubify</a> are not
protected as Flannel is used there which doesn&rsquo;t support
network policies.</li></ul></li></ul></li><li><p>Reading private AWS metadata via Grafana</p><ul><li>Root cause: It is possible to configuring a new custom data
source in Grafana, we could send HTTP requests to target the
control</li><li>Risk: Users can get the &ldquo;user-data&rdquo; for the seed cluster from
the metadata service and retrieve a kubeconfig for that
Kubernetes cluster</li><li>Recommendation: Lockdown Grafana features to only what&rsquo;s
necessary in this setup, block all unnecessary outgoing traffic,
move Grafana to a different network, lockdown unauthenticated
endpoints</li></ul></li></ul><h2 id=scenario-1-privilege-escalation-with-insecure-api-server>Scenario 1: Privilege Escalation with Insecure API Server</h2><p>In most configurations, different components connect directly to the Kubernetes API server, often using a <code>kubeconfig</code> with a client
certificate. The API server is started with the flag:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>/hyperkube apiserver --client-ca-file<span class=o>=</span>/srv/kubernetes/ca/ca.crt ...
</code></pre></div><p>The API server will check whether the client certificate presented by kubectl, kubelet, scheduler or another component
is really signed by the configured certificate authority for clients.</p><figure><img src=./images/image3.png title="The API server can have many clients of various kinds" class="drop-shadow reveal-fast" style=max-width:500px;visibility:hidden><figcaption style=text-align:center;font-style:italic>The API server can have many clients of various kinds</figcaption></figure><p>However, it is possible to configure the API server differently for use with an intermediate authenticating proxy. The
proxy will authenticate the client with its own custom method and then issue HTTP requests to the API server with
additional HTTP headers specifying the user name and group name. The API server should only accept HTTP requests with HTTP headers from a legitimate proxy. To allow the API server to check incoming requests, you need pass on a list of certificate authorities (CAs) to it. Requests coming from a proxy are only accepted if they use a client certificate that is signed by one of the CAs of that list.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>--requestheader-client-ca-file<span class=o>=</span>/srv/kubernetes/ca/ca-proxy.crt
--requestheader-username-headers<span class=o>=</span>X-Remote-User
--requestheader-group-headers<span class=o>=</span>X-Remote-Group
</code></pre></div><figure><img src=./images/image2.png title="API server clients can reach the API server through an authenticating proxy" class="drop-shadow reveal-fast" style=max-width:500px;visibility:hidden><figcaption style=text-align:center;font-style:italic>API server clients can reach the API server through an authenticating proxy</figcaption></figure><p>So far, so good. But what happens if malicious user “Mallory” tries to connect directly to the API server and reuses
the HTTP headers to pretend to be someone else?</p><figure><img src=./images/image8.png title="What happens when a client bypasses the proxy, connecting directly to the API server?" class="drop-shadow reveal-fast" style=max-width:500px;visibility:hidden><figcaption style=text-align:center;font-style:italic>What happens when a client bypasses the proxy, connecting directly to the API server?</figcaption></figure><p>With a correct configuration, Mallory’s kubeconfig will have a certificate signed by the API server certificate authority
but not signed by the proxy certificate authority. So the API server will not accept the extra HTTP header
“X-Remote-Group: system:masters”.</p><p>You only run into an issue when the same certificate authority is used for both the API server and the proxy. Then, any Kubernetes
client certificate can be used to take the role of different user or group as the API server will accept the user header and
group header.</p><p>The <code>kubectl</code> tool does not normally add those HTTP headers but it’s pretty easy to generate the corresponding HTTP
requests manually.</p><p>We worked on <a href=https://github.com/kubernetes/website/pull/10093>improving the Kubernetes documentation</a> to make clearer
that this configuration should be avoided.</p><h2 id=scenario-2-exploration-of-the-control-plane-network-with-malicious-http-redirects>Scenario 2: Exploration of the Control Plane Network with Malicious HTTP-Redirects</h2><p>The API server is a central component of Kubernetes and many components initiate connections to it, including the Kubelet
running on worker nodes. Most of the requests from those clients will end up updating Kubernetes objects (pods, services,
deployments, and so on) in the etcd database but the API server usually does not need to initiate TCP connections itself.</p><figure><img src=./images/image7.png title="The API server is mostly a component that receives requests" class="drop-shadow reveal-fast" style=max-width:500px;visibility:hidden><figcaption style=text-align:center;font-style:italic>The API server is mostly a component that receives requests</figcaption></figure><p>However, there are exceptions. Some <code>kubectl</code> commands will trigger the API server to open a new
connection to the Kubelet. <code>Kubectl exec</code> is one of those commands. In order to get the standard I/Os from the pod,
the API server will start an HTTP connection to the Kubelet on the worker node where the pod is running. Depending on
the container runtime used, it can be done in different ways, but one way to do it is for the Kubelet to reply with a
HTTP-302 redirection to the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/container-runtime-interface.md>Container Runtime Interface (CRI)</a>.
Basically, the Kubelet is telling the API server to get the streams from CRI itself directly instead of forwarding. The
redirection from the Kubelet will only change the port and path from the URL; the IP address will not be changed because
the Kubelet and the CRI component run on the same worker node.</p><figure><img src=./images/image1.png title="But the API server also initiates some connections, for example, to worker nodes" class="drop-shadow reveal-fast" style=max-width:500px;visibility:hidden><figcaption style=text-align:center;font-style:italic>But the API server also initiates some connections, for example, to worker nodes</figcaption></figure><p>It’s often quite easy for users of a Kubernetes cluster to get access to worker nodes and tamper with the Kubelet. They
could be given explicit SSH access or they could be given a kubeconfig with enough privileges to create privileged pods
or even just pods with “host” volumes.</p><p>In contrast, users — even those with “system:masters” permissions or “root” rights — are often not given access to the control plane.
On setups like for example GKE or Gardener, the control plane is running on separate nodes, with a different administrative
access. It could be hosted on a different cloud provider account. So users are not free to explore the internal network
in the control plane.</p><p>What would happen if a user was tampering with the Kubelet to make it maliciously redirect <code>kubectl exec</code> requests to
a different random endpoint? Most likely the given endpoint would not speak the streaming server protocol, so there would
be an error. However, the full HTTP payload from the endpoint is included in the error message printed by kubectl exec.</p><figure><img src=./images/image6.png title="The API server is tricked to connect to other components" class="drop-shadow reveal-fast" style=max-width:500px;visibility:hidden><figcaption style=text-align:center;font-style:italic>The API server is tricked to connect to other components</figcaption></figure><p>The impact of this issue depends on the specific setup. But in many configurations, we could find a metadata service
(such as the <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html>AWS metadata service</a>)
containing user data, configurations and credentials. The setup we explored had a different AWS account and a different
<a href=https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html>EC2 instance profile</a>
for the worker nodes and the control plane. This issue allowed users to get access to the AWS metadata service in the
context of the control plane, which they should not have access to.</p><p>We have reported this issue to the <a href=https://kubernetes.io/docs/reference/issues-security/security/>Kubernetes Security mailing list</a>
and the public pull request that addresses the issue has been merged <a href=https://github.com/kubernetes/kubernetes/pull/66516>PR#66516</a>.
It provides a way to enforce HTTP redirect validation (disabled by default).</p><p>But there are several other ways that users could trigger the API server to generate HTTP requests and get the reply
payload back, so it is advised to isolate the API server and other components from the network as additional precautious measures.
Depending on where the API server runs, it could be with <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>Kubernetes Network Policies</a>,
<a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html>EC2 Security Groups</a> or just
iptables directly. Following the <a href=https://en.wikipedia.org/wiki/Defense_in_depth_(computing)>defense in depth principle</a>,
it is a good idea to apply the API server HTTP redirect validation when it is available as well as firewall rules.</p><p>In Gardener, this has been fixed with Kubernetes network policies along with changes to ensure the API server does
not need to contact the metadata service. You can see more details in the <a href=https://groups.google.com/forum/#!forum/gardener>announcements on the Gardener mailing list</a>.
This is tracked in <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-2475">CVE-2018-2475</a>.</p><p><em>To be protected from this issue, stakeholders should:</em></p><ul><li><em>Use the latest version of Gardener</em></li><li><em>Ensure the seed cluster’s container network supports network policies. Clusters that have been created with
<a href=https://github.com/gardener/kubify>Kubify</a> are not protected as Flannel is used there which doesn’t support network
policies.</em></li></ul><h2 id=scenario-3-reading-private-aws-metadata-via-grafana>Scenario 3: Reading Private AWS Metadata via Grafana</h2><p>For our tests, we had access to a Kubernetes setup where users are not only given access to the API server in the control
plane, but also to a Grafana instance that is used to gather data from their Kubernetes clusters via Prometheus. The control
plane is managed and users don’t have access to the nodes that it runs. They can only access the API server and Grafana
via a load balancer. The internal network of the control plane is therefore hidden to users.</p><figure><img src=./images/image5.png title="Prometheus and Grafana can be used to monitor worker nodes" class="drop-shadow reveal-fast" style=max-width:500px;visibility:hidden><figcaption style=text-align:center;font-style:italic>Prometheus and Grafana can be used to monitor worker nodes</figcaption></figure><p>Unfortunately, that setup was not protecting the control plane network from nosy users. By configuring a new custom
data source in Grafana, we could send HTTP requests to target the control plane network, for example the AWS metadata
service. The reply payload is not displayed on the Grafana Web UI but it is possible to access it from the debugging
console of the Chrome browser.</p><figure><img src=./images/image9.png title="Credentials can be retrieved from the debugging console of Chrome" class="drop-shadow reveal-fast" style=max-width:500px;visibility:hidden><figcaption style=text-align:center;font-style:italic>Credentials can be retrieved from the debugging console of Chrome</figcaption></figure><figure><img src=./images/image4.png title="Adding a Grafana data source is a way to issue HTTP requests to arbitrary targets" class="drop-shadow reveal-fast" style=max-width:500px;visibility:hidden><figcaption style=text-align:center;font-style:italic>Adding a Grafana data source is a way to issue HTTP requests to arbitrary targets</figcaption></figure><p>In that installation, users could get the “user-data” for the seed cluster from the metadata service and retrieve a
kubeconfig for that Kubernetes cluster.</p><p>There are many possible measures to avoid this situation: lockdown Grafana features to only what’s necessary in this setup, block all
unnecessary outgoing traffic, move Grafana to a different network, lockdown unauthenticated endpoints.</p><h2 id=conclusion>Conclusion</h2><p>The three scenarios above show pitfalls with a Kubernetes setup. A lot of them were specific to the Kubernetes
installation: different cloud providers or different configurations will show different weakness. Users should no longer be given access to Grafana.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2e8efeb071c3d7a7cfbd4334db1fb8b8>5.4 - Container image not pulled</h1><div class=lead>Wrong Container Image or Invalid Registry Permissions</div><h2 id=problem>Problem</h2><p>Two of the most common problems are specifying the wrong container image or trying to use
private images without providing registry credentials.</p><p><strong>Note:</strong> There is no observable difference in Pod status between a missing image and incorrect registry permissions.
In either case, Kubernetes will report an ErrImagePull status for the Pods. For this reason, this article deals with
both scenarios.</p><h2 id=example>Example</h2><p>Let&rsquo;s see an example. We&rsquo;ll create a pod named <em>fail</em> referencing a non-existent Docker image:</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>kubectl run -i --tty fail --image<span class=o>=</span>tutum/curl:1.123456
</code></pre></div><p>the command prompt doesn&rsquo;t return and you can press <code>ctrl+c</code></p><h2 id=error-analysis>Error analysis</h2><p>We can then inspect our Pods and see that we have one Pod with a status of <strong>ErrImagePull</strong> or <strong>ImagePullBackOff</strong>.</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>$ <span class=o>(</span>minikube<span class=o>)</span> kubectl get pods
NAME                      READY     STATUS         RESTARTS   AGE
client-5b65b6c866-cs4ch   1/1       Running        <span class=m>1</span>          1m
fail-6667d7685d-7v6w8     0/1       ErrImagePull   <span class=m>0</span>          &lt;invalid&gt;
vuejs-578574b75f-5x98z    1/1       Running        <span class=m>0</span>          1d
$ <span class=o>(</span>minikube<span class=o>)</span> 

</code></pre></div><p>For some additional information, we can <code>describe</code> the failing Pod.</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>kubectl describe pod fail-6667d7685d-7v6w8
</code></pre></div><p>As you can see in the events section, your image can&rsquo;t be pulled</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>Name:		fail-6667d7685d-7v6w8
Namespace:	default
Node:		minikube/192.168.64.10
Start Time:	Wed, 22 Nov 2017 10:01:59 +0100
Labels:		pod-template-hash=2223832418
		run=fail
Annotations:	kubernetes.io/created-by={&#34;kind&#34;:&#34;SerializedReference&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;reference&#34;:{&#34;kind&#34;:&#34;ReplicaSet&#34;,&#34;namespace&#34;:&#34;default&#34;,&#34;name&#34;:&#34;fail-6667d7685d&#34;,&#34;uid&#34;:&#34;cc4ccb3f-cf63-11e7-afca-4a7a1fa05b3f&#34;,&#34;a...
.
.
.
.
Events:
  FirstSeen	LastSeen	Count	From			SubObjectPath		Type		Reason			Message
  ---------	--------	-----	----			-------------		--------	------			-------
  1m		1m		1	default-scheduler				Normal		Scheduled		Successfully assigned fail-6667d7685d-7v6w8 to minikube
  1m		1m		1	kubelet, minikube				Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded for volume &#34;default-token-9fr6r&#34; 
  1m		6s		4	kubelet, minikube	spec.containers{fail}	Normal		Pulling			pulling image &#34;tutum/curl:1.123456&#34;
  1m		5s		4	kubelet, minikube	spec.containers{fail}	Warning		Failed			Failed to pull image &#34;tutum/curl:1.123456&#34;: rpc error: code = Unknown desc = Error response from daemon: manifest for tutum/curl:1.123456 not found
  1m		&lt;invalid&gt;	10	kubelet, minikube				Warning		FailedSync		Error syncing pod
  1m		&lt;invalid&gt;	6	kubelet, minikube	spec.containers{fail}	Normal		BackOff			Back-off pulling image &#34;tutum/curl:1.123456&#34;
</code></pre></div><p><strong>Why couldn&rsquo;t Kubernetes pull the image?</strong>
There are three primary candidates besides network connectivity issues:</p><ul><li>The image tag is incorrect</li><li>The image doesn&rsquo;t exist</li><li>Kubernetes doesn&rsquo;t have permissions to pull that image</li></ul><p>If you don&rsquo;t notice a typo in your image tag, then it&rsquo;s time to test using your local machine. I usually start by
running <strong>docker pull on my local development machine</strong> with the exact same image tag. In this case, I would
run <code>docker pull tutum/curl:1.123456</code></p><p>If this succeeds, then it probably means that Kubernetes doesn&rsquo;t have correct permissions to pull that image.</p><p>Add the docker registry user/pwd to your cluster</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>kubectl create secret docker-registry dockersecret --docker-server<span class=o>=</span>https://index.docker.io/v1/ --docker-username<span class=o>=</span>&lt;username&gt; --docker-password<span class=o>=</span>&lt;password&gt; --docker-email<span class=o>=</span>&lt;email&gt;
</code></pre></div><p>If the exact image tag fails, then I will test without an explicit image tag - <code>docker pull tutum/curl</code> - which will attempt to pull the latest tag. If this succeeds, then that means
the originally specified tag doesn&rsquo;t exist. Go to the Docker registry and check which tags are available for this image.</p><p>If <code>docker pull tutum/curl</code> (without an exact tag) fails, then we have a bigger problem -
that image does not exist at all in our image registry.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e7486173c78c00fc1dfc4e45f04c5c7f>5.5 - Container image not updating</h1><div class=lead>Updating Images in your cluster during development</div><h2 id=preface>Preface</h2><p>A container image should use a fixed tag or the content hash of the image. It should not use the tags <strong>latest</strong>,
<strong>head</strong>, <strong>canary</strong>, or other tags that are designed to be <em>floating</em>.</p><h2 id=problem>Problem</h2><p>Many Kubernetes users have run into this problem.
The story goes something like this:</p><ul><li>Deploy any image using an image tag (e.g. cp-enablement/awesomeapp:1.0)</li><li>Fix a bug in awesomeapp</li><li>Build a new image and push it with the <strong>same tag</strong> (cp-enablement/awesomeapp:1.0)</li><li>Update your deployment</li><li>Realize that the bug is still present</li><li>Rinse and repeat steps 3 to 5 until you recognize this doesn&rsquo;t work</li></ul><p>The problem relates to how Kubernetes decides whether to do a <em>docker pull</em> when starting a container.
Since we tagged our image as <em>:1.0</em>, the default pull policy is <strong>IfNotPresent</strong>. The Kubelet already has a local
copy of cp-enablement/awesomeapp:1.0, hence it doesn&rsquo;t attempt to do a docker pull. When the new Pods come up,
they still use the old broken Docker image.</p><p>There are three ways to resolve this:</p><ul><li><del>Switch to using the tag :latest</del> (DO NOT DO THIS!)</li><li>Specify ImagePullPolicy: always (not recomended).</li><li><strong>Use unique tags (best practice)</strong></li></ul><h2 id=solution>Solution</h2><p>In the quest to automate myself out of a job, I created a bash script that runs anytime to create a new tag
and push the build result to the registry.</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh><span class=cp>#!/usr/bin/env bash
</span><span class=cp></span>
<span class=c1># Set the docker image name and the corresponding repository</span>
<span class=c1># Ensure that you change them in the deployment.yml as well.</span>
<span class=c1># You must be logged in with docker login…</span>
<span class=c1>#</span>
<span class=c1># CHANGE THIS TO YOUR Docker.io SETTINGS</span>
<span class=c1>#</span>
<span class=nv>PROJECT</span><span class=o>=</span>awesomeapp
<span class=nv>REPOSITORY</span><span class=o>=</span>cp-enablement

<span class=c1># exit if any subcommand or pipeline returns a non-zero status.</span>
<span class=nb>set</span> -e

<span class=c1># set debug mode</span>
<span class=c1>#set -x</span>

<span class=c1># build my nodeJS app</span>
<span class=c1>#</span>
npm run build

<span class=c1># get latest version IDs from the Docker.io registry and increment them</span>
<span class=c1>#</span>
<span class=nv>VERSION</span><span class=o>=</span><span class=k>$(</span>curl https://registry.hub.docker.com/v1/repositories/<span class=nv>$REPOSITORY</span>/<span class=nv>$PROJECT</span>/tags  <span class=p>|</span> sed -e <span class=s1>&#39;s/[][]//g&#39;</span> -e <span class=s1>&#39;s/&#34;//g&#39;</span> -e <span class=s1>&#39;s/ //g&#39;</span> <span class=p>|</span> tr <span class=s1>&#39;}&#39;</span> <span class=s1>&#39;\n&#39;</span>  <span class=p>|</span> awk -F: <span class=s1>&#39;{print $3}&#39;</span> <span class=p>|</span> grep v<span class=p>|</span> tail -n 1<span class=k>)</span>
<span class=nv>VERSION</span><span class=o>=</span><span class=si>${</span><span class=nv>VERSION</span><span class=p>:</span><span class=nv>1</span><span class=si>}</span>
<span class=o>((</span>VERSION++<span class=o>))</span>
<span class=nv>VERSION</span><span class=o>=</span><span class=s2>&#34;v</span><span class=nv>$VERSION</span><span class=s2>&#34;</span>


<span class=c1># build a new docker image</span>
<span class=c1>#</span>
<span class=nb>echo</span> <span class=s1>&#39;&gt;&gt;&gt; Building new image&#39;</span>
<span class=c1># Due to a bug in Docker we need to analyse the log to find out if build passed (see https://github.com/dotcloud/docker/issues/1875)</span>
docker build --no-cache<span class=o>=</span><span class=nb>true</span> -t <span class=nv>$REPOSITORY</span>/<span class=nv>$PROJECT</span>:<span class=nv>$VERSION</span> . <span class=p>|</span> tee /tmp/docker_build_result.log
<span class=nv>RESULT</span><span class=o>=</span><span class=k>$(</span>cat /tmp/docker_build_result.log <span class=p>|</span> tail -n 1<span class=k>)</span>
<span class=k>if</span> <span class=o>[[</span> <span class=s2>&#34;</span><span class=nv>$RESULT</span><span class=s2>&#34;</span> !<span class=o>=</span> *Successfully* <span class=o>]]</span><span class=p>;</span>
<span class=k>then</span>
  <span class=nb>exit</span> -1
<span class=k>fi</span>


<span class=nb>echo</span> <span class=s1>&#39;&gt;&gt;&gt; Push new image&#39;</span>
docker push <span class=nv>$REPOSITORY</span>/<span class=nv>$PROJECT</span>:<span class=nv>$VERSION</span>


</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-a7727382924a6be0bac7859384e0cf01>5.6 - Custom Seccomp profile</h1><h1 id=custom-seccomp-profile>Custom Seccomp profile</h1><h2 id=context>Context</h2><p><a href=https://en.wikipedia.org/wiki/Seccomp>Seccomp</a> (secure computing mode) is a security facility in the Linux kernel for restricting the set of system calls applications can make.</p><p>Starting from Kubernetes v1.3.0 the Seccomp feature is in <code>Alpha</code>. To configure it on a <code>Pod</code>, the following annotations can be used:</p><ul><li><code>seccomp.security.alpha.kubernetes.io/pod: &lt;seccomp-profile></code> where <code>&lt;seccomp-profile></code> is the seccomp profile to apply to all containers in a <code>Pod</code>.</li><li><code>container.seccomp.security.alpha.kubernetes.io/&lt;container-name>: &lt;seccomp-profile></code> where <code>&lt;seccomp-profile></code> is the seccomp profile to apply to <code>&lt;container-name></code> in a <code>Pod</code>.</li></ul><p>More details can be found in the <code>PodSecurityPolicy</code> <a href=https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp>documentation</a>.</p><h2 id=installation-of-custom-profile>Installation of custom profile</h2><p>By default, kubelet loads custom Seccomp profiles from <code>/var/lib/kubelet/seccomp/</code>. There are two ways in which Seccomp profiles can be added to a <code>Node</code>:</p><ul><li>to be baked in the machine image</li><li>to be added at runtime.</li></ul><p>This guide focuses on creating those profiles via a <code>DaemonSet</code>.</p><p>Create a file called <code>seccomp-profile.yaml</code> with the following content:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ConfigMap</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>seccomp-profile</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span><span class=w></span><span class=nt>data</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>my-profile.json</span><span class=p>:</span><span class=w> </span><span class=p>|</span><span class=sd>
</span><span class=sd>    {
</span><span class=sd>      &#34;defaultAction&#34;: &#34;SCMP_ACT_ALLOW&#34;,
</span><span class=sd>      &#34;syscalls&#34;: [
</span><span class=sd>        {
</span><span class=sd>          &#34;name&#34;: &#34;chmod&#34;,
</span><span class=sd>          &#34;action&#34;: &#34;SCMP_ACT_ERRNO&#34;
</span><span class=sd>        }
</span><span class=sd>      ]
</span><span class=sd>    }</span><span class=w>    
</span></code></pre></div><blockquote><p>The policy above is a very simple one and not siutable for complex applications. The <a href=https://github.com/moby/moby/blob/v17.05.0-ce/profiles/seccomp/default.json>default docker profile</a> can be used a reference. Feel free to modify it to your needs.</p></blockquote><p>Apply the <code>ConfigMap</code> in your cluster:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ kubectl apply -f seccomp-profile.yaml
configmap/seccomp-profile created
</code></pre></div><p>The next steps is to create the <code>DaemonSet</code> seccomp installer. It&rsquo;s going to copy the policy from above in <code>/var/lib/kubelet/seccomp/my-profile.json</code>.</p><p>Create a file called <code>seccomp-installer.yaml</code> with the following content:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>DaemonSet</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>seccomp</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>kube-system</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>security</span><span class=p>:</span><span class=w> </span><span class=l>seccomp</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>security</span><span class=p>:</span><span class=w> </span><span class=l>seccomp</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>security</span><span class=p>:</span><span class=w> </span><span class=l>seccomp</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>initContainers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>installer</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>alpine:3.10.0</span><span class=w>
</span><span class=w>        </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s2>&#34;/bin/sh&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;-c&#34;</span><span class=p>,</span><span class=w> </span><span class=s2>&#34;cp -r -L /seccomp/*.json /host/seccomp/&#34;</span><span class=p>]</span><span class=w>
</span><span class=w>        </span><span class=nt>volumeMounts</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>profiles</span><span class=w>
</span><span class=w>          </span><span class=nt>mountPath</span><span class=p>:</span><span class=w> </span><span class=l>/seccomp</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>hostseccomp</span><span class=w>
</span><span class=w>          </span><span class=nt>mountPath</span><span class=p>:</span><span class=w> </span><span class=l>/host/seccomp</span><span class=w>
</span><span class=w>          </span><span class=nt>readOnly</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>pause</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>k8s.gcr.io/pause:3.1</span><span class=w>
</span><span class=w>      </span><span class=nt>terminationGracePeriodSeconds</span><span class=p>:</span><span class=w> </span><span class=m>5</span><span class=w>
</span><span class=w>      </span><span class=nt>volumes</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>hostseccomp</span><span class=w>
</span><span class=w>        </span><span class=nt>hostPath</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>path</span><span class=p>:</span><span class=w> </span><span class=l>/var/lib/kubelet/seccomp</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>profiles</span><span class=w>
</span><span class=w>        </span><span class=nt>configMap</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>seccomp-profile</span><span class=w>
</span></code></pre></div><p>Create the installer and wait until it&rsquo;s ready on all <code>Nodes</code>:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ kubectl apply -f seccomp-installer.yaml
daemonset.apps/seccomp-installer created

$ kubectl -n kube-system get pods -l security=seccomp
NAME                      READY   STATUS    RESTARTS   AGE
seccomp-installer-wjbxq   1/1     Running   0          21s
</code></pre></div><h2 id=create-a-pod-using-custom-seccomp-profile>Create a Pod using custom Seccomp profile</h2><p>Finally we want to create a profile which uses our new Seccomp profile <code>my-profile.json</code>.</p><p>Create a file called <code>my-seccomp-pod.yaml</code> with the following content:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>seccomp-app</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>seccomp.security.alpha.kubernetes.io/pod</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;localhost/my-profile.json&#34;</span><span class=w>
</span><span class=w>    </span><span class=c># you can specify seccomp profile per container. If you add another profile you can configure</span><span class=w>
</span><span class=w>    </span><span class=c># it for a specific container - &#39;pause&#39; in this case.</span><span class=w>
</span><span class=w>    </span><span class=c># container.seccomp.security.alpha.kubernetes.io/pause: &#34;localhost/some-other-profile.json&#34;</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>pause</span><span class=w>
</span><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>k8s.gcr.io/pause:3.1</span><span class=w>
</span></code></pre></div><p>Create the <code>Pod</code> and see that&rsquo;s running:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ kubectl apply -f my-seccomp-pod.yaml
pod/seccomp-app created

$ kubectl get pod seccomp-app
NAME         READY   STATUS    RESTARTS   AGE
seccomp-app  1/1     Running   0          42s
</code></pre></div><h2 id=throubleshooting>Throubleshooting</h2><p>If an invalid or not existing profile is used then the <code>Pod</code> will be stuck in <code>ContainerCreating</code> phase:</p><p><code>broken-seccomp-pod.yaml</code>:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Pod</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>broken-seccomp</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>seccomp.security.alpha.kubernetes.io/pod</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;localhost/not-existing-profile.json&#34;</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>pause</span><span class=w>
</span><span class=w>    </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>k8s.gcr.io/pause:3.1</span><span class=w>
</span></code></pre></div><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>$ kubectl apply -f broken-seccomp-pod.yaml
pod/broken-seccomp created

$ kubectl get pod broken-seccomp
NAME            READY   STATUS              RESTARTS   AGE
broken-seccomp  1/1     ContainerCreating   0          2m

$ kubectl describe pod broken-seccomp
Name:               broken-seccomp
Namespace:          default
....
Events:
  Type     Reason                  Age               From                     Message
  ----     ------                  ----              ----                     -------
  Normal   Scheduled               18s               default-scheduler        Successfully assigned kube-system/broken-seccomp to docker-desktop
  Warning  FailedCreatePodSandBox  4s (x2 over 18s)  kubelet, docker-desktop  Failed create pod sandbox: rpc error: code = Unknown desc = failed to make sandbox docker config for pod &#34;broken-seccomp&#34;: failed to generate sandbox security options
for sandbox &#34;broken-seccomp&#34;: failed to generate seccomp security options for container: cannot load seccomp profile &#34;/var/lib/kubelet/seccomp/not-existing-profile.json&#34;: open /var/lib/kubelet/seccomp/not-existing-profile.json: no such file or directory
</code></pre></div><h2 id=further-reading>Further reading</h2><ul><li><a href=https://en.wikipedia.org/wiki/Seccomp>https://en.wikipedia.org/wiki/Seccomp</a></li><li><a href=https://docs.docker.com/engine/security/seccomp>https://docs.docker.com/engine/security/seccomp</a></li><li><a href=https://lwn.net/Articles/656307/>https://lwn.net/Articles/656307/</a></li><li><a href=http://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf>http://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-8de2e5545164d701b7df8e8a18ef00b2>5.7 - Dockerfile pitfalls</h1><div class=lead>Common Dockerfile pitfalls</div><h2 id=using-latest-tag-for-an-image>Using latest tag for an image</h2><p>Many Dockerfiles use the FROM package:latest pattern at the top of their Dockerfiles to pull the latest
image from a Docker registry.</p><h3 id=bad-dockerfile>Bad Dockerfile</h3><div class=highlight><pre class=chroma><code class=language-Dockerfile data-lang=Dockerfile><span class=k>FROM</span><span class=s> alpine</span><span class=err>
</span></code></pre></div><p>While simple, using the latest tag for an image means that your build
can suddenly break if that image gets updated. This can lead to problems where everything builds fine
locally (because your local cache thinks it is the latest) while a build server may fail, because some
Pipelines makes a clean pull on every build. Additionally, troubleshooting can prove to be
difficult, since the maintainer of the Dockerfile didn&rsquo;t actually make any changes.</p><h3 id=good-dockerfile>Good Dockerfile</h3><p>A digest takes the place of the tag when pulling an image. This will ensure your Dockerfile remains immutable.</p><div class=highlight><pre class=chroma><code class=language-Dockerfile data-lang=Dockerfile><span class=k>FROM</span><span class=s> alpine@sha256:7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430</span><span class=err>
</span></code></pre></div><h2 id=running-aptapkyum-update>Running apt/apk/yum update</h2><p>Running apt-get install is one of those things virtually every Debian-based Dockerfile will have to
satiate some external package requirements your code needs to run. But, using apt-get as an example, comes with
its own problems.</p><p><strong>apt-get upgrade</strong></p><p>This will update all your packages to their latests versions, which can be bad because it prevents your Dockerfile
from creating consistent, immutable builds.</p><p><strong>apt-get update in a different line than running your apt-get install command.</strong></p><p>Running apt-get update as a single line entry will get cached by the build and won&rsquo;t actually run every
time you need to run apt-get install. Instead, make sure you run apt-get update in the same line with all
the packages to ensure all are updated correctly.</p><h2 id=avoid-big-container-images>Avoid big container images</h2><p>Building small container image will reduce the time needed to start or restart pods. An image based on the popular
<a href=http://alpinelinux.org/>Alpine Linux project</a> is much smaller
than most distribution based images (~5MB). For most popular languages
and products, there are usually an official Alpine Linux image, e.g. <a href=https://hub.docker.com/_/golang/>golang</a>,
<a href=https://hub.docker.com/_/node/>nodejs</a> and <a href=https://hub.docker.com/_/postgres/>postgres</a>.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$  docker images
REPOSITORY                                                      TAG                     IMAGE ID            CREATED             SIZE
postgres                                                        9.6.9-alpine            6583932564f8        <span class=m>13</span> days ago         39.26 MB
postgres                                                        9.6                     d92dad241eff        <span class=m>13</span> days ago         235.4 MB
postgres                                                        10.4-alpine             93797b0f31f4        <span class=m>13</span> days ago         39.56 MB
</code></pre></div><p>In addition, for compiled languages such as Go or C++ which does not requires build time tooling during runtime, it
is recommended to avoid build time tooling in the final images. With Docker&rsquo;s support for
<a href=https://docs.docker.com/engine/userguide/eng-image/multistage-build/>multi-stages builds</a>
this can be easily achieved with minimal effort. Such an example can be found <a href=https://docs.docker.com/develop/develop-images/multistage-build/#name-your-build-stages>here</a>.<br>Google&rsquo;s <a href=https://github.com/GoogleContainerTools/distroless>distroless</a> image is also a good base image.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d65e7a77c3c4bf464f24d76782f40592>5.8 - HTTPS with self Signed Certificate</h1><h2 id=configuring-ingress-with-front-end-tls>Configuring ingress with front-end TLS</h2><p>It is alyways recommended to enable encryption for services to prevent traffic interception and
man-in-the-middle attacks - even in DEV environments.</p><p><img src="https://github.com/freegroup/kube-https/raw/master/images/ingress-https.png?raw=true" alt=Screen title=Screenshot></p><p>You should configure front-end Transport Layer Security (TLS) so that the ingress controller can secure
access to a service from the client to the load balancer by using HTTPS.</p><p>We will use basic procedure here. If your configuration requires advanced security options, please refer
to official CloudFlare&rsquo;s <a href=https://github.com/cloudflare/cfssl>cfssl</a> documentation.</p><h2 id=before-you-begin>Before you begin</h2><p>At first, you need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate
with your cluster. If you do not already have a cluster, you can create one by using Gardener</p><h3 id=install-cfssl>Install CFSSL</h3><p>The first step in securing Docker and Kubernetes is to set up a PKI infrastructure for managing TLS certificates.</p><h3 id=initialize-a-ca>Initialize a CA</h3><p>Before we can generate any certs we need to initialize a CA.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>mkdir cfssl
<span class=nb>cd</span> cfssl
cfssl print-defaults config &gt; ca-config.json
cfssl print-defaults csr &gt; ca-csr.json
</code></pre></div><h3 id=configure-ca-options>Configure CA options</h3><p>Now we can configure signing options inside ca-config.json config file. Default options
contain following preconfigured fields:</p><ul><li>profiles: www with server auth (TLS Web Server Authentication) X509 V3 extension and client with client auth (TLS Web Client Authentication) X509 V3 extension.</li><li>expiry: with 8760h default value (or 365 days)</li></ul><p>For compliance let&rsquo;s edit the <code>ca-config.json</code> file and rename <em><strong>www</strong></em> profile into <strong>server</strong></p><p>Edit the <code>ca-csr.json</code> to your needs. See example below. Keep in mind that the <strong>hosts</strong> entries must match
all your ingress entries.</p><p>example <code>ca-csr.json</code></p><div class=highlight><pre class=chroma><code class=language-json data-lang=json><span class=p>{</span>
    <span class=nt>&#34;CN&#34;</span><span class=p>:</span> <span class=s2>&#34;Gardener Self Signed CA&#34;</span><span class=p>,</span>
    <span class=nt>&#34;hosts&#34;</span><span class=p>:</span> <span class=p>[</span>
        <span class=s2>&#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com&#34;</span><span class=p>,</span>
        <span class=s2>&#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com&#34;</span>
    <span class=p>],</span>
    <span class=nt>&#34;key&#34;</span><span class=p>:</span> <span class=p>{</span>
        <span class=nt>&#34;algo&#34;</span><span class=p>:</span> <span class=s2>&#34;rsa&#34;</span><span class=p>,</span>
        <span class=nt>&#34;size&#34;</span><span class=p>:</span> <span class=mi>2048</span>
    <span class=p>},</span>
    <span class=nt>&#34;names&#34;</span><span class=p>:</span> <span class=p>[</span>
        <span class=p>{</span>
            <span class=nt>&#34;C&#34;</span><span class=p>:</span> <span class=s2>&#34;US&#34;</span><span class=p>,</span>
            <span class=nt>&#34;ST&#34;</span><span class=p>:</span> <span class=s2>&#34;CA&#34;</span><span class=p>,</span>
            <span class=nt>&#34;L&#34;</span><span class=p>:</span> <span class=s2>&#34;San Francisco&#34;</span>
        <span class=p>}</span>
    <span class=p>]</span>
<span class=p>}</span>
</code></pre></div><p>And generate CA with defined options:</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>cfssl gencert -initca ca-csr.json <span class=p>|</span> cfssljson -bare ca -
</code></pre></div><p>You&rsquo;ll get following files:</p><ul><li>ca-key.pem</li><li>ca.csr</li><li>ca.pem</li></ul><p><strong>Note: Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within your CA.</strong></p><h2 id=generate-server-certificate>Generate server certificate</h2><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>cfssl print-defaults csr &gt; server.json
</code></pre></div><p>Most important values for server certificate are Common Name (CN) and hosts. We have to substitute them, for example:</p><div class=highlight><pre class=chroma><code class=language-json data-lang=json><span class=p>{</span>
    <span class=nt>&#34;CN&#34;</span><span class=p>:</span> <span class=s2>&#34;Gardener Self Signed CA&#34;</span><span class=p>,</span>
    <span class=nt>&#34;hosts&#34;</span><span class=p>:</span> <span class=p>[</span>
        <span class=s2>&#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com&#34;</span><span class=p>,</span>
        <span class=s2>&#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com&#34;</span>
    <span class=p>],</span>
    <span class=nt>&#34;key&#34;</span><span class=p>:</span> <span class=p>{</span>
        <span class=nt>&#34;algo&#34;</span><span class=p>:</span> <span class=s2>&#34;rsa&#34;</span><span class=p>,</span>
        <span class=nt>&#34;size&#34;</span><span class=p>:</span> <span class=mi>2048</span>
    <span class=p>},</span>
    <span class=nt>&#34;names&#34;</span><span class=p>:</span> <span class=p>[</span>
        <span class=p>{</span>
            <span class=nt>&#34;C&#34;</span><span class=p>:</span> <span class=s2>&#34;US&#34;</span><span class=p>,</span>
            <span class=nt>&#34;ST&#34;</span><span class=p>:</span> <span class=s2>&#34;CA&#34;</span><span class=p>,</span>
            <span class=nt>&#34;L&#34;</span><span class=p>:</span> <span class=s2>&#34;San Francisco&#34;</span>
        <span class=p>}</span>
    <span class=p>]</span>
<span class=p>}</span>

</code></pre></div><p>Now we are ready to generate server certificate and private key:</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>cfssl gencert -ca<span class=o>=</span>ca.pem -ca-key<span class=o>=</span>ca-key.pem -config<span class=o>=</span>ca-config.json -profile<span class=o>=</span>server server.json <span class=p>|</span> cfssljson -bare server
</code></pre></div><p>You&rsquo;ll get following files:</p><ul><li>server-key.pem</li><li>server.csr</li><li>server.pem</li></ul><h2 id=configure-kubernetes-ingress-with-tls>Configure Kubernetes ingress with TLS</h2><p>To configure front-end TLS, you need to create a TLS certificate (already done above), create a Kubernetes secret, update
applicable .yaml files, apply your .yaml file changes, regenerate ingress controllers, and visit the application.</p><h2 id=create-kubernetes-secret>Create Kubernetes secret</h2><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>kubectl create secret tls tls-secret --key ./server-key.pem --cert server.pem
</code></pre></div><h2 id=create-service--ingress>Create Service / Ingress</h2><p>now you can referenc ethe TLS secret within your ingress definition</p><p>example ingress definition</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Service</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>node-server</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>node-svc</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>NodePort</span><span class=w>
</span><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>node-server</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>extensions/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>node-ingress</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>hosts</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=l>ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com</span><span class=w>
</span><span class=w>    </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>tls-secret</span><span class=w>
</span><span class=w>  </span><span class=nt>rules</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>host</span><span class=p>:</span><span class=w> </span><span class=l>ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com</span><span class=w>
</span><span class=w>    </span><span class=nt>http</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>backend</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>serviceName</span><span class=p>:</span><span class=w> </span><span class=l>node-svc</span><span class=w>
</span><span class=w>          </span><span class=nt>servicePort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span><span class=w>
</span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-d9556873b49aa46143a17e6cf119ba76>5.9 - HTTPS with self Signed Certificate</h1><h2 id=configuring-ingress-with-front-end-tls>Configuring ingress with front-end TLS</h2><p>It is always recommended to enable encryption for services to prevent traffic interception and
man-in-the-middle attacks - even in DEV environments.</p><p><img src="https://raw.githubusercontent.com/freegroup/kube-https/master/images/ingress-https.png?raw=true" alt=Screen title=Screenshot></p><p>You should configure front-end Transport Layer Security (TLS) so that the ingress controller can secure
access to a service from the client to the load balancer by using HTTPS.</p><p>We will use basic procedure here. If your configuration requires advanced security options, please refer
to official CloudFlare&rsquo;s <a href=https://github.com/cloudflare/cfssl>cfssl</a> documentation.</p><h2 id=before-you-begin>Before you begin</h2><p>At first, you need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate
with your cluster. If you do not already have a cluster, you can create one by using Gardener</p><h3 id=install-cfssl>Install CFSSL</h3><p>The first step in securing Docker and Kubernetes is to set up a PKI infrastructure for managing TLS certificates.</p><h3 id=initialize-a-ca>Initialize a CA</h3><p>Before we can generate any certs we need to initialize a CA.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>mkdir cfssl
<span class=nb>cd</span> cfssl
cfssl print-defaults config &gt; ca-config.json
cfssl print-defaults csr &gt; ca-csr.json
</code></pre></div><h3 id=configure-ca-options>Configure CA options</h3><p>Now we can configure signing options inside ca-config.json config file. Default options
contain following preconfigured fields:</p><ul><li>profiles: www with server auth (TLS Web Server Authentication) X509 V3 extension and client with client auth (TLS Web Client Authentication) X509 V3 extension.</li><li>expiry: with 8760h default value (or 365 days)</li></ul><p>For compliance let&rsquo;s edit the <code>ca-config.json</code> file and rename <em><strong>www</strong></em> profile into <strong>server</strong></p><p>Edit the <code>ca-csr.json</code> to your needs. See example below. Keep in mind that the <strong>hosts</strong> entries must match
all your ingress entries.</p><p>example <code>ca-csr.json</code></p><div class=highlight><pre class=chroma><code class=language-json data-lang=json><span class=p>{</span>
    <span class=nt>&#34;CN&#34;</span><span class=p>:</span> <span class=s2>&#34;Gardener Self Signed CA&#34;</span><span class=p>,</span>
    <span class=nt>&#34;hosts&#34;</span><span class=p>:</span> <span class=p>[</span>
        <span class=s2>&#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com&#34;</span><span class=p>,</span>
        <span class=s2>&#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com&#34;</span>
    <span class=p>],</span>
    <span class=nt>&#34;key&#34;</span><span class=p>:</span> <span class=p>{</span>
        <span class=nt>&#34;algo&#34;</span><span class=p>:</span> <span class=s2>&#34;rsa&#34;</span><span class=p>,</span>
        <span class=nt>&#34;size&#34;</span><span class=p>:</span> <span class=mi>2048</span>
    <span class=p>},</span>
    <span class=nt>&#34;names&#34;</span><span class=p>:</span> <span class=p>[</span>
        <span class=p>{</span>
            <span class=nt>&#34;C&#34;</span><span class=p>:</span> <span class=s2>&#34;US&#34;</span><span class=p>,</span>
            <span class=nt>&#34;ST&#34;</span><span class=p>:</span> <span class=s2>&#34;CA&#34;</span><span class=p>,</span>
            <span class=nt>&#34;L&#34;</span><span class=p>:</span> <span class=s2>&#34;San Francisco&#34;</span>
        <span class=p>}</span>
    <span class=p>]</span>
<span class=p>}</span>
</code></pre></div><p>And generate CA with defined options:</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>cfssl gencert -initca ca-csr.json <span class=p>|</span> cfssljson -bare ca -
</code></pre></div><p>You&rsquo;ll get following files:</p><ul><li>ca-key.pem</li><li>ca.csr</li><li>ca.pem</li></ul><p><strong>Note: Please keep ca-key.pem file in safe. This key allows to create any kind of certificates within your CA.</strong></p><h2 id=generate-server-certificate>Generate server certificate</h2><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>cfssl print-defaults csr &gt; server.json
</code></pre></div><p>Most important values for server certificate are Common Name (CN) and hosts. We have to substitute them, for example:</p><div class=highlight><pre class=chroma><code class=language-json data-lang=json><span class=p>{</span>
    <span class=nt>&#34;CN&#34;</span><span class=p>:</span> <span class=s2>&#34;Gardener Self Signed CA&#34;</span><span class=p>,</span>
    <span class=nt>&#34;hosts&#34;</span><span class=p>:</span> <span class=p>[</span>
        <span class=s2>&#34;ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com&#34;</span><span class=p>,</span>
        <span class=s2>&#34;api.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com&#34;</span>
    <span class=p>],</span>
    <span class=nt>&#34;key&#34;</span><span class=p>:</span> <span class=p>{</span>
        <span class=nt>&#34;algo&#34;</span><span class=p>:</span> <span class=s2>&#34;rsa&#34;</span><span class=p>,</span>
        <span class=nt>&#34;size&#34;</span><span class=p>:</span> <span class=mi>2048</span>
    <span class=p>},</span>
    <span class=nt>&#34;names&#34;</span><span class=p>:</span> <span class=p>[</span>
        <span class=p>{</span>
            <span class=nt>&#34;C&#34;</span><span class=p>:</span> <span class=s2>&#34;US&#34;</span><span class=p>,</span>
            <span class=nt>&#34;ST&#34;</span><span class=p>:</span> <span class=s2>&#34;CA&#34;</span><span class=p>,</span>
            <span class=nt>&#34;L&#34;</span><span class=p>:</span> <span class=s2>&#34;San Francisco&#34;</span>
        <span class=p>}</span>
    <span class=p>]</span>
<span class=p>}</span>

</code></pre></div><p>Now we are ready to generate server certificate and private key:</p><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>cfssl gencert -ca<span class=o>=</span>ca.pem -ca-key<span class=o>=</span>ca-key.pem -config<span class=o>=</span>ca-config.json -profile<span class=o>=</span>server server.json <span class=p>|</span> cfssljson -bare server
</code></pre></div><p>You&rsquo;ll get following files:</p><ul><li>server-key.pem</li><li>server.csr</li><li>server.pem</li></ul><h2 id=configure-kubernetes-ingress-with-tls>Configure Kubernetes ingress with TLS</h2><p>To configure front-end TLS, you need to create a TLS certificate (already done above), create a Kubernetes secret, update
applicable .yaml files, apply your .yaml file changes, regenerate ingress controllers, and visit the application.</p><h2 id=create-kubernetes-secret>Create Kubernetes secret</h2><div class=highlight><pre class=chroma><code class=language-sh data-lang=sh>kubectl create secret tls tls-secret --key ./server-key.pem --cert server.pem
</code></pre></div><h2 id=create-service--ingress>Create Service / Ingress</h2><p>now you can referenc ethe TLS secret within your ingress definition</p><p>example ingress definition</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Service</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>node-server</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>node-svc</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>default</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>NodePort</span><span class=w>
</span><span class=w>  </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>port</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>node-server</span><span class=w>
</span><span class=w></span><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>extensions/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>node-ingress</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>tls</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>hosts</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=l>ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com</span><span class=w>
</span><span class=w>    </span><span class=nt>secretName</span><span class=p>:</span><span class=w> </span><span class=l>tls-secret</span><span class=w>
</span><span class=w>  </span><span class=nt>rules</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>host</span><span class=p>:</span><span class=w> </span><span class=l>ui.ingress.https-test.cpet.shoot.canary.k8s-hana.ondemand.com</span><span class=w>
</span><span class=w>    </span><span class=nt>http</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>backend</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>serviceName</span><span class=p>:</span><span class=w> </span><span class=l>node-svc</span><span class=w>
</span><span class=w>          </span><span class=nt>servicePort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span><span class=w>
</span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-f45e1e3bb19a23ceeebe6d769078b409>5.10 - Integrity and Immutability</h1><div class=lead>Ensure that you get always the right image</div><h2 id=introduction>Introduction</h2><p>When transferring data among networked systems, <strong>trust is a central concern</strong>. In particular, when communicating over an
untrusted medium such as the internet, it is critical to ensure the <strong>integrity and immutability</strong> of all the data a
system operates on. Especially if you use Docker Engine to push and pull images (data) to a <strong>public registry</strong>.</p><p>This immutability offers me a guarantee that any and all containers that I instantiate will be absolutely identical
at inception. Surprise surprise, deterministic operations.</p><h2 id=a-lesson-in-deterministic-ops>A Lesson in Deterministic Ops</h2><p>Docker Tags are about as reliable and disposable as this guy down here.</p><p><img src=/__resources/howto-content-trust_866756.svg alt=docker-labels></p><p>Seems simple enough. You have probably already deployed hundreds of YAML&rsquo;s or started endless count of Docker container.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>docker run --name mynginx1 -P -d nginx:1.13.9
</code></pre></div><p>or</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>rss-site</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class=m>1</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>web</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>web</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>front-end</span><span class=w>
</span><span class=w>          </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>nginx:1.13.9</span><span class=w>
</span><span class=w>          </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>            </span>- <span class=nt>containerPort</span><span class=p>:</span><span class=w> </span><span class=m>80</span><span class=w>
</span></code></pre></div><p><strong>But Tags are mutable and humans are prone to error. Not a good combination.</strong> Here we’ll dig into why the use of tags can
be dangerous and how to deploy your containers across a pipeline and across environments, you guessed it, with
<strong>determinism in mind</strong>.</p><p>I want to ensure that whether it’s today or 5 years from now, that specific deployment uses the very same image that
I defined. Any updates or newer versions of an image should be executed as a new deployment. <strong>The solution: digest</strong></p><p>A digest takes the place of the tag when pulling an image, for example, to pull the above image by digest, run the
following command:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>docker run --name mynginx1 -P -d nginx@sha256:4771d09578c7c6a65299e110b3ee1c0a2592f5ea2618d23e4ffe7a4cab1ce5de
</code></pre></div><p>You can now make sure that the same image is always loaded at every deployment. It doesn&rsquo;t matter if the TAG of the
image has been changed or not. <strong>This solves the problem of repeatability.</strong></p><h2 id=content-trust>Content Trust</h2><p>However, there’s an additionally hidden danger. It is possible for an attacker to replace a server image with another
one infected with malware.</p><p><img src=/__resources/howto-content-trust-replace_615330.svg alt=docker-content-trust></p><p><a href=https://docs.docker.com/engine/security/trust/content_trust/>Docker Content trust</a> gives
you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.</p><p>Prior to version 1.8, Docker didn’t have a way to verify the authenticity of a server image. But in v1.8, a new feature
called <strong>Docker Content Trust</strong> was introduced to automatically sign and verify the signature of a publisher.</p><p>So, as soon as a server image is downloaded, it is cross-checked with the signature of the publisher to see
if someone tampered with it in any way. <strong>This solves the problem of trust.</strong></p><p>In addition you should scan all images for known vulnerabilities, this can fill another book</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7d122ee5ae2c428bfe3d1293dd385721>5.11 - Kubernetes Antipatterns</h1><div class=lead>Common Antipatterns for Kubernetes and Docker</div><p><img src=/__resources/howto-antipattern_faca4e.png alt=antipattern></p><p>This HowTo covers common kubernetes antipatterns that we have seen over the past months.</p><h2 id=running-as-root-user>Running as root user.</h2><p>Whenever possible, do not run containers as root user. One could be
tempted to say that Kubernetes Pods and Node are well separated. Host and containers running on it
share the same kernel. If a container is compromised, the root user in the container has full control over the
underlying node.</p><p>Watch the very good presentation by Liz Rice at the KubeCon 2018</p><iframe width=560 height=315 src=https://www.youtube.com/embed/ltrV-Qmh3oY frameborder=0 allow="autoplay; encrypted-media" allowfullscreen></iframe><p>Use <code>RUN groupadd -r anygroup && useradd -r -g anygroup myuser</code> to create a group
and add a user to it. Use the <code>USER</code> command to switch to this user. Note that you may also consider to provide
<a href=https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user>an explicit UID/GID</a> if required.</p><p>For Example:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>ARG GF_UID=&#34;500&#34;
ARG GF_GID=&#34;500&#34;

# add group &amp; user
RUN groupadd -r -g $GF_GID appgroup &amp;&amp; \
   useradd appuser -r -u $GF_UID -g appgroup

USER appuser

</code></pre></div><h2 id=store-data-or-logs-in-containers>Store data or logs in containers</h2><p>Containers are ideal for stateless applications
and should be transient. This means that no data or logs should be stored in the
container, as they are lost when the container is closed. Use persistence volumes instead to persist data outside
of containers. Using an <a href=https://www.elastic.co/de/elk-stack>ELK stack</a> is another good option for storing and processing logs.</p><h2 id=using-pod-ip-addresses>Using pod IP addresses</h2><p>Each pod is assigned an IP address. It is necessary
for pods to communicate with each other to build an application, e.g. an application
must communicate with a database. Existing pods are terminated and new pods are
constantly started. If you would rely on the IP address of a pod or container, you would need to update the application
configuration constantly. This makes the application fragile. Create
services instead. They provide a logical name that can be assigned independently of the
varying number and IP addresses of containers. Services are the basic concept for load
balancing within Kubernetes.</p><h2 id=more-than-one-process-in-a-container>More than one process in a container</h2><p>A docker file provides a <code>CMD</code> and <code>ENTRYPOINT</code> to
start the image. <code>CMD</code> is often used around a script that makes a configuration and then
starts the container. Do not try to start multiple processes with this script. It is
important to consider the separation of concerns when creating docker images. Running multiple processes in a single pod makes
managing your containers, collecting logs and updating each process more difficult.
You can split the image into multiple containers and manage them independently - even in one pod.
Bear in mind that Kubernetes only monitors the process with PID=1. If more than
one process is started within a container, then these no longer fall under the control of Kubernetes.</p><h2 id=creating-images-in-a-running-container>Creating images in a running container</h2><p>A new image can be created with the <code>docker commit</code>
command. This is useful if changes have been made to the container and you want to persist
them for later error analysis. However, images created like this are not reproducible and
completely worthless for a CI/CD environment. Furthermore, another developer cannot recognize
which components the image contains. Instead, always make changes to the docker file, close
existing containers and start a new container with the updated image.</p><h2 id=saving-passwords-in-docker-image-->Saving passwords in docker image 💀</h2><p>Do not save passwords in a Docker file. They are in plain
text and are checked into a repository. That makes them completely vulnerable even if you are using
a private repository like the Artifactory.
Always use <a href=https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure>Secrets or ConfigMaps</a>
to provision passwords or inject them by mounting a persistent volume.</p><h2 id=using-the-latest-tag>Using the &lsquo;latest&rsquo; tag</h2><p>Starting an image with <em>tomcat</em> is tempting. If no tags are specified, a container is
started with the tomcat:latest image. This image may no longer be up to date and refers to an
older version instead. Running a production application requires complete control of the environment
with exact versions of the image. Make sure you always use a tag or even better the <strong>sha256 hash</strong>
of the image e.g. <code>tomcat@sha256:c34ce3c1fcc0c7431e1392cc3abd0dfe2192ffea1898d5250f199d3ac8d8720f</code>.
Why use the sha256 hash? Tags are not immutable and can be overwritten by a developer at any time. In this case
you don&rsquo;t have complete control over your image - which is bad.</p><h2 id=different-images-per-environment>Different images per environment</h2><p>Don&rsquo;t create different images for development, testing, staging
and production environments. The image should be the <strong>source of truth</strong> and should only be created once
and pushed to the repository. This image:tag should be used for different environments in the future.</p><h2 id=depend-on-start-order-of-pods>Depend on start order of pods</h2><p>Applications often depend on containers being started in a certain order.
For example, a database container must be up and running before an application can connect to it. The application
should be resilient to such changes, as the db pod can be unreachable or restarted at any time. The
application container should be able to handle such situations without terminating or crashing.</p><h2 id=additional-anti-patterns-and-patterns>Additional anti-patterns and patterns&mldr;</h2><p>In the community vast experience have been collected to improve stability and usability of Docker and Kubernetes.
Refer to the following link for more information</p><ul><li><a href=https://github.com/gravitational/workshop/blob/master/k8sprod.md>Kubernetes Production Patterns</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ffdcb3c3c73c058191fdf52cbca5e94c>5.12 - Namespace Isolation</h1><p>&mldr;or <strong>DENY all traffic from other namespaces</strong></p><p>You can configure a <strong>NetworkPolicy</strong> to deny all the traffic from other namespaces while allowing all the traffic
coming from the same namespace the pod was deployed into.</p><img src=/__resources/howto-namespaceisolation_00dff7.png width=100%><p><strong>There are many reasons why you may chose to employ Kubernetes network policies:</strong></p><ul><li>Isolate multi-tenant deployments</li><li>Regulatory compliance</li><li>Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each other</li></ul><p>Kubernetes <strong>network policies</strong> are application centric compared to infrastructure/network centric standard firewalls.
<strong>There are no explicit CIDRs or IP addresses used</strong> for matching source or destination IP’s. <strong>Network policies build up on labels
and selectors</strong> which are key concepts of Kubernetes that are used to organize (for e.g all DB tier pods of an app) and
select subsets of objects.</p><h2 id=example>Example</h2><p>We create two nginx HTTP-Servers in two namespaces and block all traffic between the two namespaces. E.g. you are
unable to get content from <em>namespace1</em> if you are sitting in <em>namespace2</em>.</p><h2 id=setup-the-namespaces>Setup the namespaces</h2><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># create two namespaces for test purpose</span>
kubectl create ns customer1
kubectl create ns customer2

<span class=c1># create a standard HTTP web server</span>
kubectl run nginx --image<span class=o>=</span>nginx --replicas<span class=o>=</span><span class=m>1</span> --port<span class=o>=</span><span class=m>80</span> -n<span class=o>=</span>customer1
kubectl run nginx --image<span class=o>=</span>nginx --replicas<span class=o>=</span><span class=m>1</span> --port<span class=o>=</span><span class=m>80</span> -n<span class=o>=</span>customer2

<span class=c1># expose the port 80 for external access</span>
kubectl expose deployment nginx --port<span class=o>=</span><span class=m>80</span> --type<span class=o>=</span>NodePort -n<span class=o>=</span>customer1
kubectl expose deployment nginx --port<span class=o>=</span><span class=m>80</span> --type<span class=o>=</span>NodePort -n<span class=o>=</span>customer2

</code></pre></div><hr><h2 id=test-without-np>Test without NP</h2><img src=/__resources/howto-namespaceisolation-without_8a0a23.png width=80%><p>Create a pod with <em>curl</em> preinstalled inside the namespace <em>customer1</em></p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># create a &#34;bash&#34; pod in one namespace</span>
kubectl run -i --tty client --image<span class=o>=</span>tutum/curl -n<span class=o>=</span>customer1
</code></pre></div><p>try to <em>curl</em> the exposed nginx server to get the default index.html page. <strong>Execute this in the bash prompt of the
pod created above.</strong></p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># get the index.html from the nginx of the namespace &#34;customer1&#34; =&gt; success</span>
curl http://nginx.customer1
<span class=c1># get the index.html from the nginx of the namespace &#34;customer2&#34; =&gt; success</span>
curl http://nginx.customer2
</code></pre></div><p>Both calls are done in a pod within namespace customer1 and both nginx servers are always reachable, no matter in what namespace.</p><hr><h2 id=test-with-np>Test with NP</h2><img src=/__resources/howto-namespaceisolation-with_3a6dd1.png width=80%><p>Install the <strong>NetworkPolicy</strong> from your shell</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>NetworkPolicy</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>deny-from-other-namespaces</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>podSelector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>ingress</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>from</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>podSelector</span><span class=p>:</span><span class=w> </span>{}<span class=w>
</span></code></pre></div><ul><li>it applies the policy to ALL pods in the named namespace as the <code>spec.podSelector.matchLabels</code> is empty and therefore selects all pods.</li><li>it allows traffic from ALL pods in the named namespace, as <code>spec.ingress.from.podSelector</code> is empty and therefore selects all pods.</li></ul><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>kubectl apply -f ./network-policy.yaml -n=customer1
kubectl apply -f ./network-policy.yaml -n=customer2
</code></pre></div><p>after this <code>curl http://nginx.customer2</code> shouldn&rsquo;t work anymore if you are a service inside the namespace <em>customer1</em> and
vice versa</p><p><em>Note</em>: This policy, once applied, will also disable all external traffic to these pods. For example you can create a service of type <code>LoadBalancer</code> in namespace <code>customer1</code> that match the nginx pod. When you request the service by its <code>&lt;EXTERNAL_IP>:&lt;PORT></code>, then the network policy will deny the ingress traffic from the service and the request will time out.</p><h2 id=more>More</h2><p>You can get more information how to configure the <strong>NetworkPolicies</strong> on:</p><ul><li><a href=https://docs.projectcalico.org/v3.0/getting-started/kubernetes/tutorials/advanced-policy>Calico WebSite</a></li><li><a href=https://github.com/ahmetb/kubernetes-network-policy-recipes>Kubernetes NP Recipes</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-90f02665c7456e1a24afc104f91bea70>5.13 - Orchestration of container startup</h1><div class=lead>How to orchestrate startup sequence of multiple containers</div><h2 id=disclaimer>Disclaimer</h2><p>If an application depends on other services deployed separately do not rely on a certain start sequence of containers
but ensure that the application can cope with unavailability of the services it depends on.</p><h2 id=introduction>Introduction</h2><p>Kubernetes offers a feature called <a href=https://kubernetes.io/docs/concepts/workloads/pods/init-containers/>InitContainers</a> to perform some tasks during a pod&rsquo;s initialization.
In this tutorial we demonstrate how to use it orchestrate starting sequence of multiple containers. The tutorial uses the example app <a href=https://medium.com/@xcoulon/deploying-your-first-web-app-on-minikube-6e98d2884b3a>url-shortener</a> which consists of two components:</p><ul><li>postgresql database</li><li>webapp which depends on postgresql database and provides two endpoints: create a short url from a given location, and redirect from a given short URL to the corresponding target location.</li></ul><p>This app represents the minimal example where an application relies on another service or database. In this example, if the application starts before database is ready, the application will fail as shown below:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl logs webapp-958cf5567-h247n
<span class=nv>time</span><span class=o>=</span><span class=s2>&#34;2018-06-12T11:02:42Z&#34;</span> <span class=nv>level</span><span class=o>=</span>info <span class=nv>msg</span><span class=o>=</span><span class=s2>&#34;Connecting to Postgres database using: host=`postgres:5432` dbname=`url_shortener_db` username=`user`\n&#34;</span>
<span class=nv>time</span><span class=o>=</span><span class=s2>&#34;2018-06-12T11:02:42Z&#34;</span> <span class=nv>level</span><span class=o>=</span>fatal <span class=nv>msg</span><span class=o>=</span><span class=s2>&#34;failed to start: failed to open connection to database: dial tcp: lookup postgres on 100.64.0.10:53: no such host\n&#34;</span>


$ kubectl get po -w
NAME                                READY     STATUS    RESTARTS   AGE
webapp-958cf5567-h247n   0/1       Pending   <span class=m>0</span>         0s
webapp-958cf5567-h247n   0/1       Pending   <span class=m>0</span>         0s
webapp-958cf5567-h247n   0/1       ContainerCreating   <span class=m>0</span>         0s
webapp-958cf5567-h247n   0/1       ContainerCreating   <span class=m>0</span>         1s
webapp-958cf5567-h247n   0/1       Error     <span class=m>0</span>         2s
webapp-958cf5567-h247n   0/1       Error     <span class=m>1</span>         3s
webapp-958cf5567-h247n   0/1       CrashLoopBackOff   <span class=m>1</span>         4s
webapp-958cf5567-h247n   0/1       Error     <span class=m>2</span>         18s
webapp-958cf5567-h247n   0/1       CrashLoopBackOff   <span class=m>2</span>         29s
webapp-958cf5567-h247n   0/1       Error     <span class=m>3</span>         43s
webapp-958cf5567-h247n   0/1       CrashLoopBackOff   <span class=m>3</span>         56s

</code></pre></div><p>If the <code>restartPolicy</code> is set to <code>Always</code> (default) in yaml, the application will continue to restart the pod with an exponential back-off delay in case of failure.</p><h2 id=using-initcontaniner>Using InitContaniner</h2><p>To avoid such situation, <code>InitContainers</code> can be defined which are executed prior to the application container. If one InitContainers fails, the application container won&rsquo;t be triggered.</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>apps/v1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Deployment</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>webapp</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>selector</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>matchLabels</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>webapp</span><span class=w>
</span><span class=w>  </span><span class=nt>template</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>labels</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>app</span><span class=p>:</span><span class=w> </span><span class=l>webapp</span><span class=w>
</span><span class=w>    </span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>initContainers</span><span class=p>:</span><span class=w>  </span><span class=c># check if DB is ready, and only continue when true</span><span class=w>
</span><span class=w>      </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>check-db-ready</span><span class=w>
</span><span class=w>        </span><span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>postgres:9.6.5</span><span class=w>
</span><span class=w>        </span><span class=nt>command</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=s1>&#39;sh&#39;</span><span class=p>,</span><span class=w> </span><span class=s1>&#39;-c&#39;</span><span class=p>,</span><span class=w>  </span><span class=s1>&#39;until pg_isready -h postgres -p 5432;  do echo waiting for database; sleep 2; done;&#39;</span><span class=p>]</span><span class=w>
</span><span class=w>      </span><span class=nt>containers</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>image</span><span class=p>:</span><span class=w> </span><span class=l>xcoulon/go-url-shortener:0.1.0</span><span class=w>
</span><span class=w>        </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>go-url-shortener</span><span class=w>
</span><span class=w>        </span><span class=nt>env</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>POSTGRES_HOST</span><span class=w>
</span><span class=w>          </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=l>postgres</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>POSTGRES_PORT</span><span class=w>
</span><span class=w>          </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;5432&#34;</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>POSTGRES_DATABASE</span><span class=w>
</span><span class=w>          </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=l>url_shortener_db</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>POSTGRES_USER</span><span class=w>
</span><span class=w>          </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=l>user</span><span class=w>
</span><span class=w>        </span>- <span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>POSTGRES_PASSWORD</span><span class=w>
</span><span class=w>          </span><span class=nt>value</span><span class=p>:</span><span class=w> </span><span class=l>mysecretpassword</span><span class=w>
</span><span class=w>        </span><span class=nt>ports</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>containerPort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span></code></pre></div><p>In above example, the <code>InitContainers</code> uses docker image <code>postgres:9.6.5</code> which is different from the application container.
This also brings the advantage of not having to include unnecessary tools (e.g. pg_isready) in the application container.</p><p>With introduction of <code>InitContainers</code>, the pod startup will look like following in case database is not available yet:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>$ kubectl get po -w
NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-5cc79d6bfd-t9n8h   1/1       Running   <span class=m>0</span>          5d
privileged-pod                      1/1       Running   <span class=m>0</span>          4d
webapp-fdcb49cbc-4gs4n   0/1       Pending   <span class=m>0</span>         0s
webapp-fdcb49cbc-4gs4n   0/1       Pending   <span class=m>0</span>         0s
webapp-fdcb49cbc-4gs4n   0/1       Init:0/1   <span class=m>0</span>         0s
webapp-fdcb49cbc-4gs4n   0/1       Init:0/1   <span class=m>0</span>         1s


$ kubectl  logs webapp-fdcb49cbc-4gs4n
Error from server <span class=o>(</span>BadRequest<span class=o>)</span>: container <span class=s2>&#34;go-url-shortener&#34;</span> in pod <span class=s2>&#34;webapp-fdcb49cbc-4gs4n&#34;</span> is waiting to start: PodInitializing
</code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-02cf553d7991e467aa6ce1be8575e57c>5.14 - Out-Dated HTML and JS files delivered</h1><div class=lead>Why is my application always outdated?</div><h2 id=problem>Problem</h2><p><strong>After updating your HTML and JavaScript sources in your web application,
the kubernetes cluster delivers outdated versions - why?</strong></p><h2 id=preamble>Preamble</h2><p>By default, Kubernetes service pods are not accessible from the external
network, but only from other pods within the same Kubernetes cluster.</p><p>The Gardener cluster has a built-in configuration for HTTP load balancing called <strong>Ingress</strong>,
defining rules for external connectivity to Kubernetes services. Users who want external access
to their Kubernetes services create an ingress resource that defines rules,
including the URI path, backing service name, and other information. The Ingress controller
can then automatically program a frontend load balancer to enable Ingress configuration.</p><p><img src=/__resources/howto-nginx_f7c046.svg alt=nginx></p><h2 id=example-ingress-configuration>Example Ingress Configuration</h2><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>vuejs-ingress</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>rules</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>host</span><span class=p>:</span><span class=w> </span><span class=l>test.ingress.&lt;GARDENER-CLUSTER&gt;.&lt;GARDENER-PROJECT&gt;.shoot.canary.k8s-hana.ondemand.com</span><span class=w>
</span><span class=w>    </span><span class=nt>http</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>backend</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>serviceName</span><span class=p>:</span><span class=w> </span><span class=l>vuejs-svc</span><span class=w>
</span><span class=w>          </span><span class=nt>servicePort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span></code></pre></div><p>where:</p><ul><li><strong>&lt;GARDENER-CLUSTER></strong>: The cluster name in the Gardener</li><li><strong>&lt;GARDENER-PROJECT></strong>: You project name in the Gardener</li></ul><h2 id=what-is-the-underlying-problem>What is the underlying problem?</h2><p>The ingress controller we are using is <strong>NGINX</strong>.</p><blockquote><p>NGINX is a software load balancer, web server, and <strong>content cache</strong> built on top of open
source NGINX.</p></blockquote><p><strong>NGINX caches the content as specified in the HTTP header.</strong> If the HTTP header is missing,
it is assumed that the cache is <strong>forever</strong> and NGINX never updates the content in the
stupidest case.</p><h2 id=solution>Solution</h2><p>In general you can avoid this pitfall with one of the solutions below:</p><ul><li>use a cache buster + HTTP-Cache-Control(prefered)</li><li>use HTTP-Cache-Control with a lower retention period</li><li>disable the caching in the ingress (just for dev purpose)</li></ul><p>Learning how to set the HTTP header or setup a cache buster is left to the read as an exercise
for your web framework (e.g. Express/NodeJS, SpringBoot,&mldr;)</p><p>Here an example how to disable the cache control for your ingress done with an annotation in your
ingress YAML (during development).</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nn>---</span><span class=w>
</span><span class=w></span><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>networking.k8s.io/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>Ingress</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>annotations</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>ingress.kubernetes.io/cache-enable</span><span class=p>:</span><span class=w> </span><span class=s2>&#34;false&#34;</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>vuejs-ingress</span><span class=w>
</span><span class=w></span><span class=nt>spec</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>rules</span><span class=p>:</span><span class=w>
</span><span class=w>  </span>- <span class=nt>host</span><span class=p>:</span><span class=w> </span><span class=l>test.ingress.&lt;GARDENER-CLUSTER&gt;.&lt;GARDENER-PROJECT&gt;.shoot.canary.k8s-hana.ondemand.com</span><span class=w>
</span><span class=w>    </span><span class=nt>http</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=nt>paths</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>backend</span><span class=p>:</span><span class=w>
</span><span class=w>          </span><span class=nt>serviceName</span><span class=p>:</span><span class=w> </span><span class=l>vuejs-svc</span><span class=w>
</span><span class=w>          </span><span class=nt>servicePort</span><span class=p>:</span><span class=w> </span><span class=m>8080</span><span class=w>
</span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-424dc73fc8ed567a9cf1396d57f3cb2c>5.15 - Storing secrets in git 💀</h1><div class=lead>Never ever commit a kubeconfig.yaml into github</div><h2 id=problem>Problem</h2><p>If you commit sensitive data, such as a <code>kubeconfig.yaml</code> or <code>SSH key</code> into a Git repository, you can remove it from
the history. To entirely remove unwanted files from a repository&rsquo;s history you can use the git <code>filter-branch</code> command.</p><p>The git filter-branch command rewrite your repository&rsquo;s history, which changes the SHAs for
existing commits that you alter and any dependent commits. Changed commit SHAs may affect open pull requests
in your repository. <strong>I recommend merging or closing all open pull requests before removing files from your repository.</strong></p><blockquote><p><strong>Warning:</strong> - if someone has already checked out the repository, then of course he has the secret on his computer. So ALWAYS revoke the OAuthToken/Password or whatever it was imediately.</p></blockquote><h2 id=purging-a-file-from-your-repositorys-history>Purging a file from your repository&rsquo;s history</h2><blockquote><p><strong>Warning:</strong> If you run <code>git filter-branch</code> after stashing changes, you won&rsquo;t be able to retrieve your changes with other
stash commands. Before running git filter-branch, we recommend unstashing any changes you&rsquo;ve made. To unstash the
last set of changes you&rsquo;ve stashed, run <code>git stash show -p | git apply -R</code>. For more information, see Git Tools Stashing.</p></blockquote><p>To illustrate how <code>git filter-branch</code> works, we&rsquo;ll show you how to remove your file with sensitive data from the
history of your repository and add it to .gitignore to ensure that it is not accidentally re-committed.</p><p><strong>Navigate into the repository&rsquo;s working directory.</strong></p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=nb>cd</span> YOUR-REPOSITORY
</code></pre></div><p><strong>Run the following command, replacing <code>PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA</code> with the path to the file you want to remove,
not just its filename.</strong></p><p>These arguments will:</p><ul><li>Force Git to process, but not check out, the entire history of every branch and tag</li><li>Remove the specified file, as well as any empty commits generated as a result</li><li>Overwrite your existing tags</li></ul><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>git filter-branch --force --index-filter <span class=se>\
</span><span class=se></span><span class=s1>&#39;git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA&#39;</span> <span class=se>\
</span><span class=se></span>--prune-empty --tag-name-filter cat -- --all

</code></pre></div><p><strong>Add your file with sensitive data to <code>.gitignore</code> to ensure that you don&rsquo;t accidentally commit it again.</strong></p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash> <span class=nb>echo</span> <span class=s2>&#34;YOUR-FILE-WITH-SENSITIVE-DATA&#34;</span> &gt;&gt; .gitignore
</code></pre></div><p><strong>Double-check that you&rsquo;ve removed everything you wanted to from your repository&rsquo;s history, and that all of your
branches are checked out.</strong></p><p><strong>Once you&rsquo;re happy with the state of your repository, force-push your local changes to overwrite your GitHub repository,
as well as all the branches you&rsquo;ve pushed up:</strong></p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>git push origin --force --all
</code></pre></div><p><strong>In order to remove the sensitive file from your tagged releases, you&rsquo;ll also need to force-push against your Git tags:</strong></p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>git push origin --force --tags
</code></pre></div><blockquote><p><strong>Warning:</strong> Tell your collaborators to <strong>rebase, not merge</strong>, any branches they created off of your old (tainted) repository history.
One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging.</p></blockquote><p>References:</p><ul><li><a href=https://help.github.com/articles/removing-sensitive-data-from-a-repository/>https://help.github.com/articles/removing-sensitive-data-from-a-repository/</a></li></ul><style>blockquote{border:1px solid red;padding:10px;margin-top:40px;margin-bottom:40px}blockquote p{font-size:1.5rem;color:#000}</style></div><div class=td-content style=page-break-before:always><h1 id=pg-77bb872706e47f8307662d7339184bb5>5.16 - Using Prometheus and Grafana to monitor K8s</h1><div class=lead>How to deploy and configure Prometheus and Grafana to collect and monitor kubelet container metrics</div><h2 id=disclaimer>Disclaimer</h2><p>This post is meant to give a basic end-to-end description for deploying and using Prometheus and Grafana. Both
applications offer a wide range of flexibility which needs to be considered in case you have specific requirenments.
Such advanced details are not in the scope of this post.</p><h2 id=introduction>Introduction</h2><p><a href=https://prometheus.io/>Prometheus</a> is an open-source systems monitoring and alerting toolkit for recording numeric
time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented
architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a
particular strength.</p><p>Prometheus graduates within CNCF <a href=https://prometheus.io/blog/2018/08/09/prometheus-graduates-within-cncf/>second hosted project</a>.</p><p>The following characteristics make Prometheus a good match for monitoring Kubernetes clusters:</p><ul><li><p>Pull-based monitoring<br>Prometheus is a <a href=https://prometheus.io/blog/2016/07/23/pull-does-not-scale-or-does-it/>pull-based</a> monitoring system,
which means that the Prometheus server dynamically discovers and pulls metrics from your services running in
Kubernetes.</p></li><li><p>Labels
Prometheus and Kubernetes share the same label (key-value) concept that can be used to select objects in the system.<br>Labels are used to identify time series and sets of label matchers can be used in the query language
( <a href=https://prometheus.io/docs/prometheus/latest/querying/basics/>PromQL</a> ) to select the time series to be aggregated..</p></li><li><p>Exporters<br>There are many <a href=https://prometheus.io/docs/instrumenting/exporters/>exporters</a> available which enable integration of
databases or even other monitoring systems not already providing a way to export metrics to Prometheus.
One prominent exporter is the so called <a href=https://github.com/prometheus/node_exporter>node-exporter</a>, which allows to
monitor hardware and OS related metrics of Unix systems.</p></li><li><p>Powerful query language<br>The Prometheus query language <a href=https://prometheus.io/docs/prometheus/latest/querying/basics/>PromQL</a> lets the user
select and aggregate time series data in real time. Results can either be shown as a graph, viewed
as tabular data in the Prometheus expression browser, or consumed by external systems via the <a href=https://prometheus.io/docs/prometheus/latest/querying/api/>HTTP API</a>.</p></li></ul><p>Find query examples on <a href=https://github.com/infinityworks/prometheus-example-queries/blob/master/README.md>Prometheus Query Examples</a>.</p><p>One very popular open-source visualization tool not only for Prometheus is <a href=https://grafana.com>Grafana</a>. Grafana is a
metric analytics and visualization suite. It is popular for for visualizing time series data for infrastructure
and application analytics but many use it in other domains including industrial sensors, home automation, weather, and
process control [see <a href=http://docs.grafana.org/>Grafana Documentation</a>].</p><p>Grafana accesses data via <a href=http://docs.grafana.org/guides/basic_concepts/>Data Sources</a>. The continuously growing
list of supported backends includes Prometheus.</p><p>Dashboards are created by combining panels, e.g. <a href=http://docs.grafana.org/reference/graph/>Graph</a> and <a href=http://docs.grafana.org/reference/dashlist/>Dashlist</a>.</p><p>In this example we describe an End-To-End scenario including the deployment of Prometheus and a basic monitoring
configuration as the one provided for Kubernetes clusters created by Gardener.</p><p>If you miss elements on the Prometheus web page when accessing it via its service URL <code>https://&lt;your K8s FQN>/api/v1/namespaces/&lt;your-prometheus-namespace>/services/prometheus-prometheus-server:80/proxy</code>
this is probably caused by Prometheus issue <a href=https://github.com/prometheus/prometheus/issues/1583>#1583</a>
To workaround this issue setup a port forward <code>kubectl port-forward -n &lt;your-prometheus-namespace> &lt;prometheus-pod> 9090:9090</code>
on your client and access the Prometheus UI from there with your locally installed web browser. This issue is not relevant
in case you use the service type <code>LoadBalancer</code>.</p><h2 id=preparation>Preparation</h2><p>The deployment of <a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus</a> and <a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana</a> is based on Helm charts.<br>Make sure to implement the <a href=/docs/guides/client_tools/helm>Helm settings</a> before deploying the Helm charts.</p><p>The Kubernetes clusters provided by <a href=https://github.com/gardener>Gardener</a> use role based
access control (<a href=https://kubernetes.io/docs/admin/authorization/rbac/>RBAC</a>). To authorize the Prometheus
node-exporter to access hardware and OS relevant metrics of your cluster&rsquo;s worker nodes specific artifacts need to be
deployed.</p><p>Bind the prometheus service account to the <code>garden.sapcloud.io:monitoring:prometheus</code> cluster role by running the command
<code>kubectl apply -f crbinding.yaml</code>.</p><p>Content of <code>crbinding.yaml</code></p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class=l>rbac.authorization.k8s.io/v1beta1</span><span class=w>
</span><span class=w></span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterRoleBinding</span><span class=w>
</span><span class=w></span><span class=nt>metadata</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>&lt;your-prometheus-name&gt;-server</span><span class=w>
</span><span class=w></span><span class=nt>roleRef</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>apiGroup</span><span class=p>:</span><span class=w> </span><span class=l>rbac.authorization.k8s.io</span><span class=w>
</span><span class=w>  </span><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ClusterRole</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>garden.sapcloud.io:monitoring:prometheus</span><span class=w>
</span><span class=w></span><span class=nt>subjects</span><span class=p>:</span><span class=w>
</span><span class=w></span>- <span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class=l>ServiceAccount</span><span class=w>
</span><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=l>&lt;your-prometheus-name&gt;-server</span><span class=w>
</span><span class=w>  </span><span class=nt>namespace</span><span class=p>:</span><span class=w> </span><span class=l>&lt;your-prometheus-namespace&gt;</span><span class=w>
</span></code></pre></div><h2 id=deployment-of-prometheus-and-grafana>Deployment of Prometheus and Grafana</h2><p>Only minor changes are needed to deploy <a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus</a>
and <a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana</a> based on Helm charts.</p><p>Copy the following configuration into a file called values.yaml and deploy Prometheus: <code>helm install &lt;your-prometheus-name> --namespace &lt;your-prometheus-namespace> stable/prometheus -f values.yaml</code></p><p>Typically, Prometheus and Grafana are deployed into the same namespace. There is no technical reason behind this so feel
free to choose different namespaces.</p><p>Content of <code>values.yaml</code> for Prometheus:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>rbac</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>create</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w> </span><span class=c># Already created in Preparation step</span><span class=w>
</span><span class=w></span><span class=nt>nodeExporter</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>enabled</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w> </span><span class=c># The node-exporter is already deployed by default</span><span class=w>
</span><span class=w>
</span><span class=w></span><span class=nt>server</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>global</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>scrape_interval</span><span class=p>:</span><span class=w> </span><span class=l>30s</span><span class=w>
</span><span class=w>    </span><span class=nt>scrape_timeout</span><span class=p>:</span><span class=w> </span><span class=l>30s</span><span class=w>
</span><span class=w>
</span><span class=w></span><span class=nt>serverFiles</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>prometheus.yml</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>rule_files</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=l>/etc/config/rules</span><span class=w>
</span><span class=w>      </span>- <span class=l>/etc/config/alerts      </span><span class=w>
</span><span class=w>    </span><span class=nt>scrape_configs</span><span class=p>:</span><span class=w>
</span><span class=w>    </span>- <span class=nt>job_name</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;kube-kubelet&#39;</span><span class=w>
</span><span class=w>      </span><span class=nt>honor_labels</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span><span class=w>      </span><span class=nt>scheme</span><span class=p>:</span><span class=w> </span><span class=l>https</span><span class=w>
</span><span class=w>
</span><span class=w>      </span><span class=nt>tls_config</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=c># This is needed because the kubelets&#39; certificates are not generated</span><span class=w>
</span><span class=w>      </span><span class=c># for a specific pod IP</span><span class=w>
</span><span class=w>        </span><span class=nt>insecure_skip_verify</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>      </span><span class=nt>bearer_token_file</span><span class=p>:</span><span class=w> </span><span class=l>/var/run/secrets/kubernetes.io/serviceaccount/token</span><span class=w>
</span><span class=w>
</span><span class=w>      </span><span class=nt>kubernetes_sd_configs</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>role</span><span class=p>:</span><span class=w> </span><span class=l>node</span><span class=w>
</span><span class=w>      </span><span class=nt>relabel_configs</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__metrics_path__</span><span class=w>
</span><span class=w>        </span><span class=nt>replacement</span><span class=p>:</span><span class=w> </span><span class=l>/metrics</span><span class=w>
</span><span class=w>      </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_node_address_InternalIP]</span><span class=w>
</span><span class=w>        </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>instance</span><span class=w>
</span><span class=w>      </span>- <span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>labelmap</span><span class=w>
</span><span class=w>        </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>__meta_kubernetes_node_label_(.+)</span><span class=w>
</span><span class=w>
</span><span class=w>    </span>- <span class=nt>job_name</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;kube-kubelet-cadvisor&#39;</span><span class=w>
</span><span class=w>      </span><span class=nt>honor_labels</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span><span class=w>      </span><span class=nt>scheme</span><span class=p>:</span><span class=w> </span><span class=l>https</span><span class=w>
</span><span class=w>
</span><span class=w>      </span><span class=nt>tls_config</span><span class=p>:</span><span class=w>
</span><span class=w>      </span><span class=c># This is needed because the kubelets&#39; certificates are not generated</span><span class=w>
</span><span class=w>      </span><span class=c># for a specific pod IP</span><span class=w>
</span><span class=w>        </span><span class=nt>insecure_skip_verify</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>      </span><span class=nt>bearer_token_file</span><span class=p>:</span><span class=w> </span><span class=l>/var/run/secrets/kubernetes.io/serviceaccount/token</span><span class=w>
</span><span class=w>
</span><span class=w>      </span><span class=nt>kubernetes_sd_configs</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>role</span><span class=p>:</span><span class=w> </span><span class=l>node</span><span class=w>
</span><span class=w>      </span><span class=nt>relabel_configs</span><span class=p>:</span><span class=w>
</span><span class=w>      </span>- <span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__metrics_path__</span><span class=w>
</span><span class=w>        </span><span class=nt>replacement</span><span class=p>:</span><span class=w> </span><span class=l>/metrics/cadvisor</span><span class=w>
</span><span class=w>      </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_node_address_InternalIP]</span><span class=w>
</span><span class=w>        </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>instance</span><span class=w>
</span><span class=w>      </span>- <span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>labelmap</span><span class=w>
</span><span class=w>        </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>__meta_kubernetes_node_label_(.+)</span><span class=w>
</span><span class=w>
</span><span class=w>    </span><span class=c># Example scrape config for probing services via the Blackbox Exporter.</span><span class=w>
</span><span class=w>    </span><span class=c>#</span><span class=w>
</span><span class=w>    </span><span class=c># Relabelling allows to configure the actual service scrape endpoint using the following annotations:</span><span class=w>
</span><span class=w>    </span><span class=c>#</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/probe`: Only probe services that have a value of `true`</span><span class=w>
</span><span class=w>    </span>- <span class=nt>job_name</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;kubernetes-services&#39;</span><span class=w>
</span><span class=w>      </span><span class=nt>metrics_path</span><span class=p>:</span><span class=w> </span><span class=l>/probe</span><span class=w>
</span><span class=w>      </span><span class=nt>params</span><span class=p>:</span><span class=w>
</span><span class=w>        </span><span class=nt>module</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>http_2xx]</span><span class=w>
</span><span class=w>      </span><span class=nt>kubernetes_sd_configs</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>role</span><span class=p>:</span><span class=w> </span><span class=l>service</span><span class=w>
</span><span class=w>      </span><span class=nt>relabel_configs</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_service_annotation_prometheus_io_probe]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>keep</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__address__]</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__param_target</span><span class=w>
</span><span class=w>        </span>- <span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__address__</span><span class=w>
</span><span class=w>          </span><span class=nt>replacement</span><span class=p>:</span><span class=w> </span><span class=l>blackbox</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__param_target]</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>instance</span><span class=w>
</span><span class=w>        </span>- <span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>labelmap</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>__meta_kubernetes_service_label_(.+)</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_namespace]</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes_namespace</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_service_name]</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes_name</span><span class=w>
</span><span class=w>    </span><span class=c># Example scrape config for pods</span><span class=w>
</span><span class=w>    </span><span class=c>#</span><span class=w>
</span><span class=w>    </span><span class=c># Relabelling allows to configure the actual service scrape endpoint using the following annotations:</span><span class=w>
</span><span class=w>    </span><span class=c>#</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/scrape`: Only scrape pods that have a value of `true`</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/path`: If the metrics path is not `/metrics` override this.</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.</span><span class=w>
</span><span class=w>    </span>- <span class=nt>job_name</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;kubernetes-pods&#39;</span><span class=w>
</span><span class=w>      </span><span class=nt>kubernetes_sd_configs</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>role</span><span class=p>:</span><span class=w> </span><span class=l>pod</span><span class=w>
</span><span class=w>      </span><span class=nt>relabel_configs</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_pod_annotation_prometheus_io_scrape]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>keep</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_pod_annotation_prometheus_io_path]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__metrics_path__</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>(.+)</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>(.+):(?:\d+);(\d+)</span><span class=w>
</span><span class=w>          </span><span class=nt>replacement</span><span class=p>:</span><span class=w> </span><span class=l>${1}:${2}</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__address__</span><span class=w>
</span><span class=w>        </span>- <span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>labelmap</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>__meta_kubernetes_pod_label_(.+)</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_namespace]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes_namespace</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_pod_name]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes_pod_name</span><span class=w>
</span><span class=w>    </span><span class=c># Scrape config for service endpoints.</span><span class=w>
</span><span class=w>    </span><span class=c>#</span><span class=w>
</span><span class=w>    </span><span class=c># The relabeling allows the actual service scrape endpoint to be configured</span><span class=w>
</span><span class=w>    </span><span class=c># via the following annotations:</span><span class=w>
</span><span class=w>    </span><span class=c>#</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/scrape`: Only scrape services that have a value of `true`</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need</span><span class=w>
</span><span class=w>    </span><span class=c># to set this to `https` &amp; most likely set the `tls_config` of the scrape config.</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/path`: If the metrics path is not `/metrics` override this.</span><span class=w>
</span><span class=w>    </span><span class=c># * `prometheus.io/port`: If the metrics are exposed on a different port to the</span><span class=w>
</span><span class=w>    </span><span class=c># service then set this appropriately.</span><span class=w>
</span><span class=w>    </span>- <span class=nt>job_name</span><span class=p>:</span><span class=w> </span><span class=s1>&#39;kubernetes-service-endpoints&#39;</span><span class=w>
</span><span class=w>      </span><span class=nt>kubernetes_sd_configs</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>role</span><span class=p>:</span><span class=w> </span><span class=l>endpoints</span><span class=w>
</span><span class=w>      </span><span class=nt>relabel_configs</span><span class=p>:</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_service_annotation_prometheus_io_scrape]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>keep</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=kc>true</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_service_annotation_prometheus_io_scheme]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__scheme__</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>(https?)</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_service_annotation_prometheus_io_path]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__metrics_path__</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>(.+)</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__address__, __meta_kubernetes_service_annotation_prometheus_io_port]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>__address__</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>(.+)(?::\d+);(\d+)</span><span class=w>
</span><span class=w>          </span><span class=nt>replacement</span><span class=p>:</span><span class=w> </span><span class=l>$1:$2</span><span class=w>
</span><span class=w>        </span>- <span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>labelmap</span><span class=w>
</span><span class=w>          </span><span class=nt>regex</span><span class=p>:</span><span class=w> </span><span class=l>__meta_kubernetes_service_label_(.+)</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_namespace]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes_namespace</span><span class=w>
</span><span class=w>        </span>- <span class=nt>source_labels</span><span class=p>:</span><span class=w> </span><span class=p>[</span><span class=l>__meta_kubernetes_service_name]</span><span class=w>
</span><span class=w>          </span><span class=nt>action</span><span class=p>:</span><span class=w> </span><span class=l>replace</span><span class=w>
</span><span class=w>          </span><span class=nt>target_label</span><span class=p>:</span><span class=w> </span><span class=l>kubernetes_name</span><span class=w> </span><span class=c># Add your additional configuration here...</span><span class=w>
</span></code></pre></div><p>Next, deploy Grafana. Since the deployment in this post is based on the Helm default values, the settings below are set
explicitly in case the default changed.
Deploy Grafana via <code>helm install grafana --namespace &lt;your-prometheus-namespace> stable/grafana -f values.yaml</code>. Here, the same namespace is chosen for Prometheus and for Grafana.</p><p>Content of <code>values.yaml</code> for Grafana:</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml><span class=nt>server</span><span class=p>:</span><span class=w>
</span><span class=w>  </span><span class=nt>ingress</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>enabled</span><span class=p>:</span><span class=w> </span><span class=kc>false</span><span class=w>
</span><span class=w>  </span><span class=nt>service</span><span class=p>:</span><span class=w>
</span><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class=l>ClusterIP</span><span class=w>
</span></code></pre></div><p>Check the running state of the pods on the Kubernetes Dashboard or by running <code>kubectl get pods -n &lt;your-prometheus-namespace></code>.
In case of errors check the log files of the pod(s) in question.</p><p>The text output of Helm after the deployment of Prometheus and Grafana contains very useful information, e.g. the user
and password of the Grafana Admin user. The credentials are stored as secrets in the namespace <code>&lt;your-prometheus-namespace></code>
and could be decoded via <code>kubectl get secret --namespace &lt;my-grafana-namespace> grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo</code>.</p><h2 id=basic-functional-tests>Basic functional tests</h2><p>To access the web UI of both applications use port forwarding of port 9090.</p><p>Setup port forwarding for port 9090:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl port-forward -n &lt;your-prometheus-namespace&gt; &lt;your-prometheus-server-pod&gt; 9090:9090
</code></pre></div><p>Open <code>http://localhost:9090</code> in your web browser. Select Graph from the top tab and enter the following expressing to show the overall CPU usage for a server
(see <a href=https://github.com/infinityworks/prometheus-example-queries/blob/master/README.md>Prometheus Query Examples</a>)</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>100 * (1 - avg by(instance)(irate(node_cpu{mode=&#39;idle&#39;}[5m])))
</code></pre></div><p>This should show some data in a graph.</p><p>To show the same data in Grafana setup port forwarding for port 3000 for the
Grafana pod and open the Grafana Web UI by opening http://localhost:3000 in a browser.
Enter the credentials of the admin user.</p><p>Next, you need to enter the server name of your Prometheus deployment. This name is shown directly after the
installation via helm.</p><p>Run</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>helm status &lt;your-prometheus-name&gt;
</code></pre></div><p>to find this name. Below this server name is referenced by <code>&lt;your-prometheus-server-name></code>.</p><p>First, you need to add your Prometheus server as data source.</p><ul><li>select <em>Dashboards → Data Sources</em></li><li>select <em>Add data source</em></li><li>enter
<em>Name</em>: <code>&lt;your-prometheus-datasource-name></code><br><em>Type</em>: Prometheus<br><em>URL</em>: <code>http://&lt;your-prometheus-server-name></code><br>_Access: <code>proxy</code></li><li>select <em>Save & Test</em></li></ul><p>In case of failure check the Prometheus URL in the Kubernetes Dashboard.</p><p>To add a Graph follow these steps:</p><ul><li>in the left corner, select <em>Dashboards → New</em> to create a new dashboard</li><li>select <em>Graph</em> to create a new graph</li><li>next, select the <em>Panel Title → Edit</em></li><li>select your Prometheus Data Source in the drop down list</li><li>enter the expression <code>100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m])))</code> in the entry field A</li><li>select the floppy disk symbol (Save) on top</li></ul><p>Now you should have a very basic Prometheus and Grafana setup for your Kubernetes cluster.</p><p>As a next step you can implement monitoring for your applications by implementing the <a href=https://prometheus.io/docs/instrumenting/clientlibs/>Prometheus client API</a>.</p><h2 id=links>Links</h2><ul><li><a href=https://prometheus.io/>Prometheus</a></li><li><a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus Helm Chart</a></li><li><a href=https://www.weave.works/blog/prometheus-kubernetes-perfect-match/>Prometheus and Kubernetes: A Perfect Match</a></li><li><a href=https://grafana.com>Grafana</a></li><li><a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana Helm Chart</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-9fd36c058dee824f8b5ff5b67dfb92f5>6 - Gardener Cookies</h1><h1 id=green-tea-matcha-cookies>Green Tea Matcha Cookies</h1><p>For a team event during the Christmas season we decided to completely reinterpret the topic <code>cookies</code>. :-)</p><img style=width:50%;height:auto;margin:0,auto src=/__resources/cookie-00_bfd721.jpg><p>Matcha cookies have the delicate flavor and color of green tea. These soft, pillowy and chewy green tea cookies
are perfect with tea. And of course they fit perfectly to our logo</p><h2 id=ingredients>Ingredients</h2><ul><li>1 stick butter, softened</li><li>⅞ cup of granulated sugar</li><li>1 cup + 2 tablespoons all-purpose flour</li><li>2 eggs</li><li>1¼ tablespoons culinary grade matcha powder</li><li>1 teaspoon baking powder</li><li>Pinch of salt</li></ul><h2 id=instructions>Instructions</h2><ol><li>Cream together the butter and sugar in a large mixing bowl - it should be creamy colored and airy. A hand blender or stand mixer works well for this. This helps the cookie become fluffy and chewy.</li><li>Gently incorporate the eggs to the butter mixture one at a time.</li><li>In a separate bowl, sift together all the dry ingredients.</li><li>Add the dry ingredients to the wet by adding a little at a time and folding or gently mixing the batter together. Keep going until you&rsquo;ve incorporated all the remaining flour mixture. The dough should be a beautiful green color.</li><li>Chill the dough for at least an hour - up to overnight. The longer the better!</li><li>Preheat your oven to 325 F.</li><li>Roll the dough into balls the size of ping pong balls and place them on a non-stick cookie sheet.</li><li>Bake them for 12-15 minutes until the bottoms just start to become golden brown and the cookie no longer looks wet in the middle. Note: you can always bake them at 350 F for a less moist, fluffy cookie. It will bake faster by about 2-4 minutes 350 F so watch them closely.</li><li>Remove and let cool on a rack and enjoy!</li></ol><h2 id=note>Note</h2><p>Make sure you get culinary grade matcha powder. You should be able to find this in Asian or natural grocers</p><img style=width:50%;height:auto src=/__resources/cookie-01_5ce6cc.jpg>
<img style=width:50%;height:auto src=/__resources/cookie-02_2fb6ea.jpg>
<img style=width:50%;height:auto src=/__resources/cookie-03_29d23f.jpg>
<img style=width:50%;height:auto src=/__resources/cookie-05_3785b6.jpg></div><div class=td-content style=page-break-before:always><h1 id=pg-f7745599833058eb85caaa55efe6a4aa>7 - Landscape Setup</h1><h1 id=---deprecated--->&mdash;DEPRECATED&mdash;</h1><p><strong>This project is outdated and won&rsquo;t be updated anymore. Please use <a href=https://github.com/gardener/garden-setup>https://github.com/gardener/garden-setup</a> instead!</strong></p><h1 id=gardener-setup-scripts>Gardener Setup Scripts</h1><p>This README is the installation manual for a simple Gardener setup. The installation scripts in this repo are embedded in a configuration template in the <a href=https://github.com/gardener/landscape-setup-template>landscape-setup-template</a> project. You can find further information there.</p><p>We do recommend this simplified setup for demonstration purposes only. For productive workloads we do recommend that all components (Gardener/Seed/Shoot) run in their own IaaS accounts and that network policies are enabled and properly tested on the seed clusters. A documentation on how to do this is currently work in progress.</p><ul><li><a href=#gardener-setup-scripts>Gardener Setup Scripts</a></li><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#gardener-installation>Gardener Installation</a><ul><li><a href=#tldr>TL;DR</a><ul><li><a href=#kubectl-aliases>Kubectl Aliases</a></li></ul></li><li><a href=#step-1-clone-the-repositories-and-get-dependencies>Step 1: Clone the Repositories and get Dependencies</a><ul><li><a href=#submodule-management>Submodule Management</a></li></ul></li><li><a href=#step-2-configure-the-landscape>Step 2: Configure the Landscape</a><ul><li><a href=#building-the-landscapeyaml-file>Building the &lsquo;landscape.yaml&rsquo; File</a></li><li><a href=#the-base-cluster>The Base Cluster</a><ul><li><a href=#kubify>Kubify</a></li><li><a href=#shoot-cluster>Shoot Cluster</a></li><li><a href=#using-an-arbitrary-base-cluster>Using an Arbitrary Base Cluster</a></li></ul></li></ul></li><li><a href=#step-3-build-and-run-docker-container>Step 3: Build and Run Docker Container</a></li><li><a href=#step-4-10-deploying-components>Step 4-10: Deploying Components</a><ul><li><a href=#undeploying-components>Undeploying Components</a></li><li><a href=#the-all-component>The &lsquo;all&rsquo; Component</a></li></ul></li><li><a href=#step-4-10-deploying-components-detailed>Step 4-10: Deploying Components (detailed)</a><ul><li><a href=#step-4-kubify--etcd>Step 4: Kubify / etcd</a></li><li><a href=#step-5-generate-certificates>Step 5: Generate Certificates</a></li><li><a href=#step-6-deploy-tiller>Step 6: Deploy tiller</a></li><li><a href=#step-7-deploy-gardener>Step 7: Deploy Gardener</a></li><li><a href=#step-8-register-garden-cluster-as-seed-cluster>Step 8: Register Garden Cluster as Seed Cluster</a><ul><li><a href=#configuring-additional-seeds>Configuring Additional Seeds</a></li><li><a href=#creating-a-shoot>Creating a Shoot</a></li></ul></li><li><a href=#step-9-install-identity-and-dashboard>Step 9: Install Identity and Dashboard</a><ul><li><a href=#create-cname-entry>Create CNAME Entry</a></li></ul></li><li><a href=#step-10-apply-valid-certificates>Step 10: Apply Valid Certificates</a></li><li><a href=#letsencrypt-quota-limits>Letsencrypt Quota Limits</a><ul><li><a href=#accessing-the-dashboard>Accessing the Dashboard</a></li></ul></li></ul></li></ul></li><li><a href=#tearing-down-the-landscape>Tearing Down the Landscape</a></li><li><a href=#cleanup>Cleanup</a></li></ul><h1 id=prerequisites>Prerequisites</h1><p>Before getting started make sure you have the following at hand:</p><ul><li>You need a cloud account with sufficient quota to set up a Kubernetes cluster with a couple of VMs. <strong>The Gardener supports AWS, Azure, GCP, and Openstack, but this simplified setup currently only supports AWS and Openstack.</strong></li><li>A Linux machine (virtual machine is fine) or a Mac with basic tools such as a git client and the Docker runtime installed.</li></ul><h1 id=gardener-installation>Gardener Installation</h1><p>Follow these steps to install Gardener. Do not proceed to the next step in case of errors.</p><h2 id=tldr>TL;DR</h2><p>If you are already familiar with the installation procedure and just want a short summary of the commands you have to use, here it is:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># setup</span>
git clone  --recursive https://github.com/gardener/landscape-setup-template.git landscape
<span class=c1># fill in landscape/landscape_config.yaml now</span>
<span class=nb>cd</span> landscape/setup
./docker_run.sh
deploy all

<span class=c1># -------------------------------------------------------------------</span>

<span class=c1># teardown</span>
undeploy all
./cleanup.sh
</code></pre></div><p>Otherwise, follow the detailed guide below.</p><h3 id=kubectl-aliases>Kubectl Aliases</h3><p>The following aliases can be used within the docker container:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>k =&gt; kubectl
ks =&gt; kubectl -n kube-system
kg =&gt; kubectl -n garden
kn =&gt; kubectl -n
ka =&gt; kubectl get --all-namespaces
</code></pre></div><p>Bash completion works for all of them except for <code>ka</code>.</p><h2 id=step-1-clone-the-repositories-and-get-dependencies>Step 1: Clone the Repositories and get Dependencies</h2><p>Get the <code>landscape-setup-template</code> from GitHub and initialize the submodules:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>git clone  --recursive https://github.com/gardener/landscape-setup-template.git landscape
<span class=nb>cd</span> landscape
</code></pre></div><p>After step 2, this repository will contain all passwords and keys for your landscape. You will be in trouble if you loose them so we recommend that you store this landscape configuration in a private repository. It might be a good idea to change the origin so you do not accidentally publish your secrets to the public template repository.</p><h3 id=submodule-management>Submodule Management</h3><p>This project needs the <a href=https://github.com/gardener/gardener>Gardener</a> and <a href=https://github.com/gardener/dashboard>dashboard</a> as submodules. To avoid conflicts between the checked out versions and the ones specified in the <code>landscape_base.yaml</code> file, automatic version management has been added. As long as the <code>managed</code> field in the chart area of each submodule is set to <code>true</code>, the version specified in the <code>tag</code> field will be checked out before deploying.</p><p>To check vor the correct version, the <code>VERSION</code> file in the submodule&rsquo;s main folder is read and compared to the tag in the config file. If the <code>VERSION</code> file doesn&rsquo;t exist, the component is added as a submodule. If it exists, but the versions differ, the fitting version will be checked out. If it exists and the versions are identical, nothing is done. The automatic version management will fail if a) the component is already added as a submodule, but the <code>VERSION</code> file is missing or b) the landscape folder is not a git repo.</p><p>It is also possible to trigger the version update manually: call the <code>manage_submodule</code> function with <code>gardener</code> or <code>dashboard</code> as an argument, or run the <code>manage_submodules.sh</code> script which will update Gardener and dashboard. Both will only work from inside the docker container / with sourced <code>init.sh</code> file.</p><h2 id=step-2-configure-the-landscape>Step 2: Configure the Landscape</h2><p>There is a <code>landscape_config.yaml</code> file in the landscape project. This is the only file that you need to modify - all other configuration files will be derived from this and the <code>landscape_base.yaml</code> file. The latter one contains the merging instructions as well as technical configurations and it shouldn&rsquo;t be touched unless you know what you are doing.</p><h4 id=building-the-landscapeyaml-file>Building the &lsquo;landscape.yaml&rsquo; File</h4><p>Both config files - <code>landscape_config.yaml</code> and <code>landscape_base.yaml</code> - are merged into one <code>landscape.yaml</code> file which is then used as configuration for the scripts. Sourcing the <code>init.sh</code> file (which happens automatically when entering the docker image) will perform this merge <strong>unless the file already exists.</strong> This means if you change something in one of the original config files after the <code>landscape.yaml</code> file has already been created, you need to manually rebuild it in order for the changes to take effect.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>./build_landscape_yaml.sh
</code></pre></div><p>This script will recreate the <code>landscape.yaml</code> file. It will also source the <code>init.sh</code> file again, as some of the environment variables are extracted from this file.</p><h4 id=the-base-cluster>The Base Cluster</h4><p>Gardener extends the Kubernetes apiserver, so in order to deploy it, you need a Kubernetes cluster first. This setup gives you two options for this:</p><h5 id=kubify>Kubify</h5><p>You can use <a href=https://github.com/gardener/kubify>Kubify</a> to create the initial cluster. Kubify uses Terraform to create the cluster and it is integrated into this project - you don&rsquo;t need to create the cluster yourself, just make sure you fill out all relevant parts of the config file.</p><h5 id=shoot-cluster>Shoot Cluster</h5><p>A shoot cluster is a cluster created by a Gardener instance and it can be used as a base cluster for this project. These flags have to be set for the kube-apiserver (if not set, the Gardener will still work, but the dashboard won&rsquo;t):</p><div class=highlight><pre class=chroma><code class=language-yaml data-lang=yaml>--<span class=l>oidc-issuer-url=https://identity.ingress.&lt;your cluster domain&gt;</span><span class=w>
</span><span class=w></span>--<span class=l>oidc-client-id=kube-kubectl</span><span class=w>
</span><span class=w></span>--<span class=l>oidc-username-claim=email</span><span class=w>
</span><span class=w></span>--<span class=l>oidc-groups-claim=groups</span><span class=w>
</span></code></pre></div><p>For a shoot this can be done by setting <code>issuerUrl</code>, <code>clientID</code>, <code>usernameClaim</code>, and <code>groupsClaim</code> in <code>spec.kubernetes.kubeAPIServer.oidcConfig</code> in the shoot manifest.</p><p>Also make sure that the CIDRs of your base cluster and the from your Gardener spawned shoots don&rsquo;t overlap - if you want to be able to create shoots from the Gardener dashboard later, then don&rsquo;t use the default CIDRs for this base cluster.</p><p>Some fields in the <code>landscape_config.yaml</code> are marked with <code># kubify only</code>, they can be ignored when using a shoot as the base cluster. The etcd server address defaults to the address for Kubify and needs to be changed for the shoot setup (see the comments in the config file).</p><p>The <em>kubeconfig</em> for the base cluster is expected to be named <code>kubeconfig</code> and reside in the directory containing this project&rsquo;s directory (next to the <code>landscape_config.yaml</code> file).</p><h5 id=using-an-arbitrary-base-cluster>Using an Arbitrary Base Cluster</h5><p>While this setup has only been tested for clusters created by Kubify or the Gardener (shoot clusters), it is theoretically possible to use the shoot setup method to deploy the Gardener to an arbitrary kubernetes cluster.</p><h2 id=step-3-build-and-run-docker-container>Step 3: Build and Run Docker Container</h2><p>First, <code>cd</code> into the folder containing this project.</p><p>Then run the container:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>./docker_run.sh
</code></pre></div><p>After this,</p><ul><li>you will be connected to the container via an interactive shell</li><li>the landscape folder will be mounted in that container</li><li>your current working directory will be <code>setup</code> folder</li><li><code>setup/init.sh</code> is sourced, meaning<ul><li>the environment variables will be set</li><li>kubectl will be configured to communicate with your cluster</li><li><code>landscape.yaml</code> file will have been created if it didn&rsquo;t exist before</li></ul></li></ul><p>The <code>docker_run.sh</code> script searches for the image locally and pulls it from an image repository, if it isn&rsquo;t found.
If pulling the image doesn&rsquo;t work for whatever reason, you can use the <code>docker_build.sh</code> script to build the image locally.</p><h2 id=step-4-10-deploying-components>Step 4-10: Deploying Components</h2><p>The Gardener deployment is splitted into components. A single component can be easily deployed using</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy &lt;component name&gt;
</code></pre></div><p>Please take care that most of the components depend on each other and therefore have to be deployed in the order given below.</p><p>The <code>deploy</code> command is added to the <code>PATH</code> environment variable and can thus be called from anywhere. Bash auto-completion can be used for the component names.</p><h4 id=undeploying-components>Undeploying Components</h4><p>It is also possible to &ldquo;undeploy&rdquo; a component using</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>undeploy &lt;component name&gt;
</code></pre></div><p>Components need to be undeployed in the inverse order. Do not undeploy components without undeploying their successors in the component order first. Take care to delete all shoots before undeploying the <code>gardener</code> or <code>seed-config</code> components (although both undeploy scripts will check for that and trigger a deletion themselves).</p><h4 id=the-all-component>The &lsquo;all&rsquo; Component</h4><p>The <code>all</code> component is a special component: it serves as a dummy to deploy several components in one go. Usually, manual intervention between deploying components is not necessary and most of them are deployed directly one after the other, so the <code>all</code> component makes the &ldquo;normal&rdquo; use-case easier.</p><p>For better control which components are deployed, a component range can be given as an argument. The argument should have the form <code>&lt;start component name>:&lt;end component name></code> and then the start component, the end component, and all components in between will be deployed. The order of the arguments is taken from the environment variables with the <code>$COMPONENT_ORDER_</code> prefix. The variable with the suffix that matches the <code>clusters.base_cluster</code> entry in the config file will be used.</p><p>It is also possible to drop one part of the range or to drop the whole argument. For missing parts the defaults will be used, which are the first and the last component of the active component order, respectively.</p><p>The <code>undeploy</code> command can also be used with the <code>all</code> component, but take care that the component order is inverted.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash><span class=c1># Examples</span>
<span class=c1># (start and end component are always inclusive)</span>
deploy all                          <span class=c1># deploys all components</span>
deploy all gardener:dashboard       <span class=c1># deploys &#39;gardener&#39; through &#39;dashboard&#39; </span>
deploy all gardener:                <span class=c1># deploys all components starting from &#39;gardener&#39;</span>
deploy all :gardener                <span class=c1># deploys all components up to &#39;gardener&#39;</span>

<span class=c1># (all undeploy commands use the inverse component order)</span>
undeploy all                        <span class=c1># undeploys all components</span>
undeploy all :helm-tiller           <span class=c1># undeploys all components up to &#39;helm-tiller&#39;</span>
undeploy all dashboard:cert         <span class=c1># undeploys &#39;dashboard&#39; through &#39;cert&#39;</span>
</code></pre></div><h2 id=step-4-10-deploying-components-detailed>Step 4-10: Deploying Components (detailed)</h2><h3 id=step-4-kubify--etcd>Step 4: Kubify / etcd</h3><p>If you want to create a Kubify cluster, deploy the component:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy kubify
</code></pre></div><p>The script will wait some time for the cluster to come up and then partly validate that the cluster is ready.</p><p>If you get errors during the cluster setup, just try to run the command again.</p><p>Once completed the following command should show all deployed pods:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>root@c41327633d6d:/landscape# kubectl get pods --all-namespaces
NAMESPACE       NAME                                                                  READY     STATUS    RESTARTS   AGE
kube-system     etcd-operator-75dcfcf4f7-xkm4h                                        1/1       Running   0          6m
kube-system     heapster-c8fb4f746-tvts6                                              2/2       Running   0          2m
kube-system     kube-apiserver-hcdnc                                                  1/1       Running   0          6m
[...]
</code></pre></div><hr><p>If you already have a cluster, you don&rsquo;t need Kubify. To deploy an etcd in your cluster, run</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy etcd
</code></pre></div><p>This component is not meant to be used in combination with Kubify and might require manual steps to make it work.</p><p>It should also be possible to plug in your own etcd - check the deploy scripts for the <code>etcd</code> and <code>gardener</code> components for information on where to put the certificates, etc.</p><h3 id=step-5-generate-certificates>Step 5: Generate Certificates</h3><p>This step will generate a self-signed cluster CA and sign some certificates with it.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy cert
</code></pre></div><h3 id=step-6-deploy-tiller>Step 6: Deploy tiller</h3><p>Tiller is needed to deploy the Helm charts of Gardener and other components.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy helm-tiller
</code></pre></div><h3 id=step-7-deploy-gardener>Step 7: Deploy Gardener</h3><p>Now we can deploy Gardener. If the previous steps were executed successfully this should be completed in a couple of seconds.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy gardener
</code></pre></div><p>You might see a couple of messages like these:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>Gardener API server not yet reachable. Waiting...
</code></pre></div><p>while the script waits for the Gardener to start. Once Gardener is up when the deployment script finished you can verify the correct setup by running the following command:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>kubectl get shoots
No resources found. 
</code></pre></div><p>As we do not have a seed cluster yet we cannot create any shoot clusters.
The Gardener itself is installed in the <code>garden</code> namespace:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>kubectl get po -n garden
NAME                                          READY     STATUS    RESTARTS   AGE
gardener-apiserver-56cc665667-nvrjl           1/1       Running   0          6m
gardener-controller-manager-5c9f8db55-hfcts   1/1       Running   0          6m
</code></pre></div><h3 id=step-8-register-garden-cluster-as-seed-cluster>Step 8: Register Garden Cluster as Seed Cluster</h3><p>In heterogeneous productive environments one would run Gardener and seed in separate clusters but for simplicity and resource consumption reasons we will register the Gardener cluster that we have just created also as the seed cluster. Make sure that the <code>seed_config</code> in the landscape file is correct and matches the region that you are using. Keep in mind that image ids differ between regions as well. Also, valid credentials for the seed provider have to be specified in the <code>authentication</code> part of the <code>landscape_config.yaml</code> file (the etcd backups of the shoot clusters are stored on the seed).</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy seed-config
</code></pre></div><h4 id=configuring-additional-seeds>Configuring Additional Seeds</h4><p>By default, this step will create a seed for the cloud provider the Gardener has been deployed on and thus creating shoots on this provider will be possible. If you want to create shoots on other cloud providers, you will have to configure additional seeds. There are two options for that:</p><p>If the seed-config deploy script is called without any arguments (as shown above), it will create seeds for all providers specified in the <code>seed_config.seeds</code> section in the <code>landscape_config.yaml</code> file. By default, the only entry in that list is the cloud provider chosen for the Gardener cluster, but you can extend the list.</p><p>It is also possible to provide the seed-config deploy script with additional arguments specifying which seeds should be created. Multiple arguments can be given and the script will ignore the list in the <code>landscape_config.yaml</code> file when called with arguments. Only the specified seeds will be created, already existing seeds are not affected. If a given seed already exists, it will be updated to the current configuration.</p><p>In both cases, the corresponding variant nodes in <code>authentication</code> and <code>seed_config</code> have to be filled out in the config file.</p><p>Valid values for seeds are <code>aws</code>, <code>az</code> (for Azure), <code>gcp</code>, and <code>openstack</code>. Please note, that while it is possible to create seeds for any cloud provider on any cloud provider, shoot creation may not work across cloud providers for every combination. It should always work if seed (Gardener cluster in this setup) and shoot are on the same provider, though.</p><h4 id=creating-a-shoot>Creating a Shoot</h4><p>That&rsquo;s it! If everything went fine you should now be able to create shoot clusters.
You can start with a sample <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot-aws.yaml>manifest</a> and create a shoot cluster by standard Kubernetes means:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>kubectl apply -f shoot-aws.yaml
</code></pre></div><h3 id=step-9-install-identity-and-dashboard>Step 9: Install Identity and Dashboard</h3><p>Creating clusters based on a shoot manifest is quite nice but also a little complex. While almost all aspects of a shoot cluster can be configured, it can be quite difficult for beginners, so go on and install the dashboard:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy identity
<span class=o>[</span>...<span class=o>]</span>
deploy dashboard
<span class=o>[</span>...<span class=o>]</span>
</code></pre></div><h4 id=create-cname-entry>Create CNAME Entry</h4><p>Dashboard and identity need a CNAME entry pointing the domain <code>*.ingress.&lt;your cluster domain></code> to your cluster&rsquo;s nginx ingress ip/hostname. Kubify creates this entry automatically. If you are not using kubify to create your base cluster, you can create the CNAME entry with the corresponding component:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy cname
</code></pre></div><p>The script uses the AWS CLI to create the entry, so it will only work for route53.</p><h3 id=step-10-apply-valid-certificates>Step 10: Apply Valid Certificates</h3><p>The following command will install the <a href=https://github.com/jetstack/cert-manager>cert-manager</a> and request valid letsencrypt certificates for both the identity and dashboard ingresses:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>deploy certmanager
</code></pre></div><p>After a few minutes valid certificates should be installed.</p><h3 id=letsencrypt-quota-limits>Letsencrypt Quota Limits</h3><p>Letsencrypt <a href=https://letsencrypt.org/docs/rate-limits/>limits</a> how many certificates you can get for the same host within a short time. To avoid hitting these limits, you can use the letsencrypt staging server for testing, which has a significantly higher rate limit but produces untrusted certificates.<br>The <code>charts.[certmanager].live</code> field in the config file allows to switch between live and staging server (remember to rebuild the <code>landscape.yaml</code> file after you changed something in the <code>landscape_config.yaml</code> file).</p><h4 id=accessing-the-dashboard>Accessing the Dashboard</h4><p>After step 9 you will be able to access the Gardener dashboard. There is a difference in how you access it depending on whether you used the letsencrypt live server or the staging one (and thus have untrusted credentials in the latter case).</p><p>The <code>print_dashboard_urls.sh</code> script constructs two URLs from the domain name given in the <code>landscape.yaml</code> file and prints them.</p><p>If you have trusted certificates, just use the second one (the one for the dashboard) and everything will be fine.</p><p>If you used the letsencrypt staging server, you will need to visit the first link first. Your browser will show a warning regarding untrusted certificates, you need to ignore that warning. You will then see a nearly blank page with some 404 message. After that, you can open the dashboard link, ignore the certificate warning again and should be able to login.
If you skip the first link, you will still be able to see the dashboard, but the login button probably won&rsquo;t work.
While you will be able to login and create projects with the untrusted certificates from the letsencrypt staging server, creating secrets or shoots won&rsquo;t be possible. You&rsquo;ll need trusted certificates for that.</p><p>To log into the dashboard, use the options you have specified in the identity chart part of the <code>landscape_config.yaml</code>.</p><h1 id=tearing-down-the-landscape>Tearing Down the Landscape</h1><p>Make sure that you delete all shoot clusters prior to tearing down the cluster. Not deleting <code>project</code> resources before deleting the Gardener can also cause troubles, because the namespaces associated with the projects have a finalizer which can&rsquo;t be handled anymore when the Gardener is gone. Both, shoots and projects, can be deleted using the <code>delete_all.sh</code> script (give &lsquo;shoots&rsquo; or &lsquo;projects&rsquo; as an argument). To delete a single shoot/project, use <a href=https://github.com/gardener/gardener/blob/master/hack/delete>this</a> script.</p><p>The following command should not return any shoot clusters:</p><div class=highlight><pre class=chroma><code class=language-fallback data-lang=fallback>kubectl get shoots --all-namespaces
No resources found.
</code></pre></div><p>If you created your base cluster with the Kubify component, you can destroy it using the undeploy command:</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>undeploy kubify
</code></pre></div><h1 id=cleanup>Cleanup</h1><p>After destroying the Kubify cluster, there will be some files left that prevent you from simply starting the project up again.</p><p><strong>ATTENTION: Only do this if you are sure the cluster has been completely destroyed!</strong>
Since this removes the terraform state, an automated deletion of resources won&rsquo;t be possible anymore - you will have to clean up any leftovers manually.</p><div class=highlight><pre class=chroma><code class=language-bash data-lang=bash>./cleanup.sh
</code></pre></div><p>This will reset your landscape folder to its initial state (including the deletion of <code>landscape.yaml</code>).</p><p>The script takes an optional &ldquo;-y&rdquo; argument to skip the confirmation.</p></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=/blog/>Blogs</a></li><li><a href=/community/>Community</a></li><li><a href=/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2021 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.min.js integrity=sha384-+YQ4JLhjyBLPDQt//I+STsc9iw4uQqACwlvpslubQzn4u2UU2UFM80nGisd026JF crossorigin=anonymous></script><script src=/js/main.min.3b172c13b62c2bea8b1c9d2599cddc8cf89718a92d792c680871c81ba43d8c85.js integrity="sha256-OxcsE7YsK+qLHJ0lmc3cjPiXGKkteSxoCHHIG6Q9jIU=" crossorigin=anonymous></script></body></html>