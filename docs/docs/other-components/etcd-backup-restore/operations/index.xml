<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener – Operations</title><link>https://gardener.cloud/docs/other-components/etcd-backup-restore/operations/</link><description>Recent content in Operations on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://gardener.cloud/docs/other-components/etcd-backup-restore/operations/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Generating Ssl Certificates</title><link>https://gardener.cloud/docs/other-components/etcd-backup-restore/operations/generating_ssl_certificates/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-backup-restore/operations/generating_ssl_certificates/</guid><description>
&lt;h1 id="generating-certificates">Generating certificates&lt;/h1>
&lt;p>If you wish to enable TLS authentication for either etcd or etcdbr server or both, please follow this guide. The SSL certificate configurations given here are meant to facilitate smooth deployment of the etcd setup via the provided &lt;a href="https://github.com/gardener/etcd-backup-restore/tree/master/chart/etcd-backup-restore">helm chart&lt;/a>.&lt;/p>
&lt;h2 id="certificates-structure">Certificates structure&lt;/h2>
&lt;p>While deploying the etcd setup via the provided helm chart, TLS can be enabled for the etcd server and/or etcd-backup-restore server by adding the certificate data to the &lt;code>values.yaml&lt;/code> file as necessary. This data is converted into the respective secrets and mounted onto the pod&amp;rsquo;s containers according to the following directory structure:&lt;/p>
&lt;ul>
&lt;li>&lt;code>etcd&lt;/code> container&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>└── var
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── etcd Contains the CA and server TLS certs for etcd server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> | └── ssl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> | ├── ca
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> | | └── ca.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> | └── tls
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> | ├── tls.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> | └── tls.key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── etcdbr Contains the CA and server TLS certs for etcd backup-restore server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── ssl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── ca
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> | └── ca.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── tls
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── tls.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── tls.key
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;br>
&lt;ul>
&lt;li>&lt;code>backup-restore&lt;/code> container&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>└── var
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ├── etcd Contains the CA and server certs for etcd server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> | └── ssl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> | ├── ca
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> | | └── ca.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> | └── tls
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> | ├── tls.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> | └── tls.key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── etcdbr Contains the CA cert for etcd backup-restore server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── ssl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── ca
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> └── ca.crt
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="generating-the-certificates">Generating the certificates&lt;/h2>
&lt;h3 id="installing-openssl">Installing openssl&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="font-weight:bold">#&lt;/span> For Mac users
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>brew install openssl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>&lt;span style="font-weight:bold">#&lt;/span> For other flavours of Unix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apk install openssl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>mkdir openssl &amp;amp;&amp;amp; cd openssl
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="generating-certs-for-etcd-server-authentication">Generating certs for etcd server authentication&lt;/h3>
&lt;h4 id="generating-ca-cert-bundle">Generating CA cert bundle&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>openssl genrsa -out ca.key 2048
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl req -new -key ca.key -subj &amp;#34;/CN=etcd&amp;#34; -out ca.csr
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>cat &amp;gt; ca.csr.conf &amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[ v3_ext ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>keyUsage=critical,digitalSignature,keyEncipherment,keyCertSign,cRLSign
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>basicConstraints=critical,CA:TRUE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EOF
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt -sha256 -days 3653 -extensions v3_ext -extfile ca.csr.conf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>&lt;span style="font-weight:bold">#&lt;/span> view contents of the generated certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl x509 -in ca.crt -noout -text
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="generating-tls-key-pair">Generating TLS key-pair&lt;/h4>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>openssl genrsa -out server.key 2048
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>&lt;span style="font-weight:bold">#&lt;/span> In the &lt;span style="color:#a31515">`&lt;/span>alt_names&lt;span style="color:#a31515">`&lt;/span> section of server.csr.conf, replace all occurrences of &lt;span style="color:#a31515">`&lt;/span>mynamespace&lt;span style="color:#a31515">`&lt;/span> with the namespace into which you&lt;span style="">&amp;#39;&lt;/span>ll deploy the helm chart
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cat &amp;gt; server.csr.conf &amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[ req ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default_bits = 2048
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>prompt = no
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default_md = sha256
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>req_extensions = req_ext
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>distinguished_name = dn
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>[ dn ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>CN = etcd-server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>[ req_ext ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>subjectAltName = @alt_names
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>[ alt_names ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS.1 = main-etcd-0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS.2 = main-etcd-local
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS.3 = main-etcd-client
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS.4 = main-etcd-client.mynamespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS.5 = main-etcd-client.mynamespace.svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS.6 = main-etcd-client.mynamespace.svc.cluster.local
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>[ v3_ext ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>keyUsage=critical,digitalSignature,keyEncipherment
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>extendedKeyUsage=serverAuth,clientAuth
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>basicConstraints=critical,CA:FALSE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>subjectAltName=@alt_names
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>EOF
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>openssl req -new -key server.key -out server.csr -config server.csr.conf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -sha256 -days 3653 -extensions v3_ext -extfile server.csr.conf
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="">&lt;/span>&lt;span style="font-weight:bold">#&lt;/span> view contents of the generated certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>openssl x509 -in server.crt -noout -text
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="generating-certs-for-etcdbr-server-authentication">Generating certs for etcdbr server authentication&lt;/h3>
&lt;p>Follow the same steps as &lt;a href="#generating-certs-for-etcd-server-authentication">generating certs for etcd&lt;/a>, but replace all occurrences of &lt;code>etcd&lt;/code> with &lt;code>etcdbr&lt;/code> for the &lt;code>CN&lt;/code> fields and replace &lt;code>main-etcd-client&lt;/code> with &lt;code>main-backup-client&lt;/code> in the DNS names if you want to access the TLS-enabled backup-restore server via service. You will also need to add &lt;code>localhost&lt;/code> to the SAN DNS list if you&amp;rsquo;re deploying the etcd setup via the provided helm chart. If deploying by any other means, or if testing locally, please tweak the config accordingly.&lt;/p>
&lt;h4 id="generating-ca-cert-bundle-1">Generating CA cert bundle&lt;/h4>
&lt;p>Follow the same steps as &lt;a href="#generating-CA-cert-bundle">generating CA cert for etcd&lt;/a>, but replace&lt;code>CN=etcd&lt;/code> by &lt;code>CN=etcdbr&lt;/code> while creating the &lt;code>ca.csr&lt;/code>.&lt;/p>
&lt;h4 id="generating-tls-key-pair-1">Generating TLS key-pair&lt;/h4>
&lt;p>Follow the same steps as &lt;a href="#generating-tls-key-pair">generating TLS key-pair for etcd&lt;/a>, but modify the &lt;code>[ dn ]&lt;/code> section in &lt;code>server.csr.conf&lt;/code> from &lt;code>CN = etcd&lt;/code> by &lt;code>CN = etcdbr&lt;/code>. Also change the &lt;code>[ alt_names ]&lt;/code> section to the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>[ alt_names ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS.1 = localhost
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS.2 = main-backup-0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS.3 = main-backup-local
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS.4 = main-backup-client
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS.5 = main-backup-client.mynamespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS.6 = main-backup-client.mynamespace.svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DNS.7 = main-backup-client.mynamespace.svc.cluster.local
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here, we add &lt;code>localhost&lt;/code> to the DNS entries so that the etcd bootstrap script may be allowed to trigger data initialization on the backup sidecar via HTTPS.&lt;/p>
&lt;h2 id="running-etcdbrctl-server-with-tls-enabled">Running &lt;code>etcdbrctl server&lt;/code> with TLS enabled&lt;/h2>
&lt;p>If you wish to develop/test &lt;code>etcdbrctl&lt;/code> locally with TLS enabled, you can follow the steps to create the certs and pass them to the &lt;code>etcdbrctl server&lt;/code> via the following flags.&lt;/p>
&lt;h3 id="for-etcd-tls">For etcd TLS&lt;/h3>
&lt;p>Pass the CA certificate file via &lt;code>--cacert&lt;/code> flag, and etcd server TLS certificate and key via &lt;code>--cert&lt;/code> and &lt;code>--key&lt;/code> flags respectively.&lt;/p>
&lt;h3 id="for-etcdbr-tls">For etcdbr TLS&lt;/h3>
&lt;p>Pass the etcd backup-restore server TLS certificate and key via &lt;code>--server-cert&lt;/code> and &lt;code>--server-key&lt;/code> respectively.&lt;/p></description></item><item><title>Docs: Leader Election</title><link>https://gardener.cloud/docs/other-components/etcd-backup-restore/operations/leader_election/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-backup-restore/operations/leader_election/</guid><description>
&lt;h3 id="leading-etcd-main-containers-sidecar-is-the-backup-leader">Leading ETCD main container’s sidecar is the backup leader&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>The &lt;code>backup-restore sidecar&lt;/code> poll its corresponding etcd main container to see if it is the leading member in the etcd cluster. This information is used by the backup-restore sidecars to decide that sidecar of the leading etcd main container is the &lt;code>backup leader&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Only &lt;code>backup leader&lt;/code> sidecar among the members have the responsibility to take/upload the snapshots(full as well as incremental) for a given Etcd cluster as well as to &lt;a href="https://github.com/gardener/etcd-druid/tree/master/docs/proposals/multi-node#defragmentation">trigger the defragmentation&lt;/a> for each Etcd cluster member.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="work-flow">Work flow&lt;/h3>
&lt;p>Backup-restore can be in following 3 states:&lt;/p>
&lt;ol>
&lt;li>Follower:
&lt;ul>
&lt;li>When the corresponding etcd main container is a &lt;code>Follower&lt;/code> etcd.&lt;/li>
&lt;li>The default state for every backup-restore member is the &lt;code>Follower&lt;/code> State.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Leader:
&lt;ul>
&lt;li>When the corresponding etcd main container becomes the &lt;code>leader&lt;/code> then sidecar will also become &lt;code>leading sidecar&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Unknown:
&lt;ul>
&lt;li>When there is no etcd leader present in the cluster or in the case of a quorum loss.&lt;/li>
&lt;li>When corresponding etcd main container is down and &lt;a href="https://github.com/etcd-io/etcd/blob/f82b5cb7768dacad9fb310232c1383b4e6718378/client/v3/maintenance.go#L53">endpoint status&lt;/a> api call fails.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h3 id="backup">Backup&lt;/h3>
&lt;ul>
&lt;li>Only &lt;code>backup leader&lt;/code> among the backup-restore members have the responsibility to take/upload the snapshots(full as well as incremental) for a given Etcd cluster.&lt;/li>
&lt;li>The &lt;code>backup leader&lt;/code> also has the responsibility to &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/master/docs/operations/getting_started.md#taking-scheduled-snapshot">garbage-collect&lt;/a> the backups from the object storage bucket according to a configured garbage collection policy.&lt;/li>
&lt;/ul>
&lt;h3 id="member-lease">Member-lease&lt;/h3>
&lt;p>Each backup-restore member has the responsibility to renew its member lease periodically. This is intended to simulate as heartbeat which indicate that the backup-restore cluster members are in &lt;code>Healthy&lt;/code> State.&lt;/p>
&lt;h3 id="defragmentation">Defragmentation&lt;/h3>
&lt;p>Defragmentation for all etcd cluster members is triggered by the &lt;code>leading backup-restore&lt;/code> sidecar. The defragmentation is performed only when etcd cluster is in full health and it is done in a rolling manner for each member to avoid disruption.At first, &lt;code>leading backup-restore&lt;/code> sidecar triggers defragmentation on all etcd follower members one by one and at last, on itself.&lt;/p>
&lt;h3 id="complete-work-flow-leader-election-state-diagram">Complete work flow leader-election state diagram.&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/leaderElection_stateDiagram_c49851.png" alt="leader-election">&lt;/p></description></item><item><title>Docs: Manual Restoration</title><link>https://gardener.cloud/docs/other-components/etcd-backup-restore/operations/manual_restoration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-backup-restore/operations/manual_restoration/</guid><description>
&lt;h1 id="manual-restoration-of-etcd-data">Manual restoration of etcd data&lt;/h1>
&lt;p>Please make sure that you have read through the &lt;a href="https://gardener.cloud/docs/other-components/etcd-backup-restore/proposals/restoration/">documentation on etcd data restoration&lt;/a> before reading this document and attempting to perform a manual restoration of etcd data.&lt;/p>
&lt;p>As mentioned in previously, automatic restoration will be triggered if the etcd data gets corrupted, as the etcd process will crash. But if for some reason, restoration is not automatically triggered when it should have, you may choose to manually restore the etcd data.&lt;/p>
&lt;h2 id="steps-to-perform-restoration">Steps to perform restoration&lt;/h2>
&lt;p>You may choose to follow different methods of restoration, based on your etcd + backup sidecar setup:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Deploying the &lt;a href="https://github.com/gardener/etcd-backup-restore/tree/master/chart/etcd-backup-restore">provided helm chart&lt;/a>, in which etcdbrctl is started in &lt;code>server&lt;/code> mode&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Exec into the &lt;code>etcd&lt;/code> container of the &lt;code>main-etcd-0&lt;/code> pod and delete the &lt;code>member&lt;/code> directory under the data directory in order to invalidate it&lt;/p>
&lt;ul>
&lt;li>&lt;code>rm -rf /var/etcd/data/new.etcd/member&lt;/code>&lt;/li>
&lt;li>You may choose to rename the &lt;code>member&lt;/code> directory instead of deleting it, if you wish to retain the old data for debugging&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>This will crash the etcd container and when it restarts, the backup sidecar will perform a validation of the data directory, and seeing that the data is corrupt, it will restore the data from the latest backup&lt;/p>
&lt;ul>
&lt;li>⚠️ Keep in mind that the latest backup in the object storage bucket might not be up-to-date with the latest etcd data, and you could see a maximum data loss corresponding to the delta snapshot interval. For instance, setting &lt;code>delta-snapshot-interval=5m&lt;/code> could result in a maximum data loss worth 5 minutes.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>If for some reason, automatic restoration isn&amp;rsquo;t getting triggered even after removing the &lt;code>member&lt;/code> directory, it may be required to temporarily modify the etcdbrctl command to &lt;code>restore&lt;/code> mode to force a manual restoration of data, and then change the container spec back to its original form once the restoration is successful. Do not change any field in the container spec other than the &lt;code>command&lt;/code> field, which is detailed below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>command:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- etcdbrctl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- restore
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- --data-dir=&amp;lt;same as previous value&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- --storage-provider=&amp;lt;same as previous value&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- --store-prefix=&amp;lt;same as previous value&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- --embedded-etcd-quota-bytes=&amp;lt;same as previous value&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- --snapstore-temp-directory=&amp;lt;same as previous value&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>⚠️ Edit etcd-main statefulset to change command field. After saving it, ideally statefulset controller will recreate the etcd-main-0 pod itself. But, since it doesn&amp;rsquo;t handle the case of unhealthy pod, delete etcd-main-0 pod maually using commnad &lt;code>kubectl -n &amp;lt;namespace&amp;gt; delete etcd-main-0&lt;/code>. Now, check etcd and backup-restore sidecar logs for successful restoration.&lt;/p>
&lt;p>Once the spec is changed, monitor the logs to make sure restoration occurs. Once restoration is complete, change the container spec back to its previous state and restart the pod. This should purge any previous issues with etcd or backup sidecar, and start snapshotting successfully.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Deploying etcd and etcdbrctl separately, where etcdbrctl is started in &lt;code>server&lt;/code> mode&lt;/p>
&lt;ol>
&lt;li>If using &lt;a href="https://github.com/gardener/etcd-custom-image/blob/master/etcd_bootstrap_script.sh">this bootstrap script&lt;/a> for starting etcd, then deleting the &lt;code>member&lt;/code> directory under the etcd data directory should kill the etcd process, and subsequently the script finishes execution and exits. You will have to re-run the script and allow it to trigger data validation anf restoration by etcdbrctl.&lt;/li>
&lt;li>If not using the bootstrap script, then:
&lt;ol>
&lt;li>Delete the &lt;code>member&lt;/code> directory and wait for etcd to crash&lt;/li>
&lt;li>&lt;code>curl http://localhost:8080/initialization/status&lt;/code>, assuming etcdbrctl is running on port 8080&lt;/li>
&lt;li>&lt;code>curl http://localhost:8080/initialization/start&lt;/code>&lt;/li>
&lt;li>Wait for the restoration to finish, by observing the logs from etcdbrctl&lt;/li>
&lt;li>Again, &lt;code>curl http://localhost:8080/initialization/status&lt;/code> to complete the initialization process, and etcdbrctl will resume regular snapshotting after this&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>Deploying etcd and etcdbrctl separately, where etcdbrctl is started in &lt;code>snapshot&lt;/code> mode&lt;/p>
&lt;ol>
&lt;li>Delete the &lt;code>member&lt;/code> directory and wait for etcd to crash&lt;/li>
&lt;li>Kill the etcdbrctl process&lt;/li>
&lt;li>Run etcdbrctl in &lt;code>restore&lt;/code> mode to perform a restoration of the data
&lt;ul>
&lt;li>It is highly recommended to run etcdbrctl in &lt;code>initialize&lt;/code> mode rather than &lt;code>restore&lt;/code> mode, as this performs the necessary validation checks on the data directory taking the decision to trigger a restoration.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Start etcd again&lt;/li>
&lt;li>Restart etcdbrctl in &lt;code>snapshot&lt;/code> mode&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>⚠️ If running etcdbrctl in &lt;code>snapshot&lt;/code> mode, it is necessary to stop this snapshotter process before triggering a restoration, to avoid data inconsistency.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>⚠️ If in doubt about the validity of the etcd data, operators must first visually confirm any inconsistency in revision numbers between the latest snapshot-set in the object store and the running etcd instance using the &lt;code>etcdctl&lt;/code> tool before deciding to perform a manual restoration.&lt;/p>
&lt;p>⚠️ In order to successfully perform a restoration, the data directory must NOT contain the &lt;code>member&lt;/code> directory, else the restoration will fail.&lt;/p>
&lt;p>⚠️ &lt;strong>Do not tamper with the object store in any way.&lt;/strong> Data once lost from the object store, cannot be recovered. The object store is considered as the source of truth for the restorer.&lt;/p></description></item><item><title>Docs: Metrics</title><link>https://gardener.cloud/docs/other-components/etcd-backup-restore/operations/metrics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-backup-restore/operations/metrics/</guid><description>
&lt;h1 id="monitoring">Monitoring&lt;/h1>
&lt;p>etcd-backup-restore uses &lt;a href="http://prometheus.io/">Prometheus&lt;/a> for metrics reporting. The metrics can be used for real-time monitoring and debugging. It won&amp;rsquo;t persist its metrics; if a member restarts, the metrics will be reset.&lt;/p>
&lt;p>The simplest way to see the available metrics is to cURL the metrics endpoint &lt;code>/metrics&lt;/code>. The format is described &lt;a href="http://prometheus.io/docs/instrumenting/exposition_formats/">here&lt;/a>.&lt;/p>
&lt;p>Follow the &lt;a href="http://prometheus.io/docs/introduction/getting_started/">Prometheus getting started doc&lt;/a> to spin up a Prometheus server to collect etcd metrics.&lt;/p>
&lt;p>The naming of metrics follows the suggested &lt;a href="http://prometheus.io/docs/practices/naming/">Prometheus best practices&lt;/a>. All etcd-backup-restore related metrics are put under namespace &lt;code>etcdbr&lt;/code>.&lt;/p>
&lt;h2 id="etcd-metrics">ETCD metrics&lt;/h2>
&lt;p>The metrics under the &lt;code>etcd&lt;/code> prefix/namespace are carried forward from etcd library that we use. These metrics do not include details of the &lt;code>etcd&lt;/code> deployment on which &lt;code>etcd-backup-restore&lt;/code> utility operates. Instead, it helps in monitoring the &lt;code>embedded etcd&lt;/code> we spawn during restoration process.&lt;/p>
&lt;h3 id="snapshot">Snapshot&lt;/h3>
&lt;p>These metrics describe the status of the snapshotter. In order to detect outages or problems for troubleshooting, these metrics should be closely monitored. The below mentioned metrics are listed as collection of series using prometheus labels &lt;code>kind&lt;/code> and &lt;code>succeeded&lt;/code>. &lt;code>Kind&lt;/code> label indicates the snapshot kind i.e. full snapshot or incremental/delta snapshot in the context. And succeeded indicates whether the metrics is for successful operation or erroneous operation.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Type&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>etcdbr_snapshot_duration_seconds&lt;/td>
&lt;td>Total latency distribution of saving snapshot to object store.&lt;/td>
&lt;td>Histogram&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>etcdbr_snapshot_gc_total&lt;/td>
&lt;td>Total number of garbage collected snapshots.&lt;/td>
&lt;td>Counter&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>etcdbr_snapshot_latest_revision&lt;/td>
&lt;td>Revision number of latest snapshot taken.&lt;/td>
&lt;td>Gauge&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>etcdbr_snapshot_latest_timestamp&lt;/td>
&lt;td>Timestamp of latest snapshot taken.&lt;/td>
&lt;td>Gauge&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>etcdbr_snapshot_required&lt;/td>
&lt;td>Indicates whether a new snapshot is required to be taken.&lt;/td>
&lt;td>Gauge&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Abnormally high snapshot duration (&lt;code>etcdbr_snapshot_duration_seconds&lt;/code>) indicates disk issues and low network bandwidth.&lt;/p>
&lt;p>&lt;code>etcdbr_snapshot_latest_timestamp&lt;/code> indicates the time when last snapshot was taken. If it has been a long time since a snapshot has been taken, then it indicates either the snapshots are being skipped because of no updates on etcd or ⚠️ something fishy is going on and a possible data loss might occur on the next restoration.&lt;/p>
&lt;p>&lt;code>etcdbr_snapshot_gc_total&lt;/code> gives the total number of snapshots garbage collected since bootstrap. You can use this in coordination with &lt;code>etcdbr_snapshot_duration_seconds_count&lt;/code> to get number of snapshots in object store.&lt;/p>
&lt;p>&lt;code>etcdbr_snapshot_required&lt;/code> indicates whether a new snapshot is required to be taken. Acts as a boolean flag where zero value implies &amp;lsquo;false&amp;rsquo; and non-zero values imply &amp;rsquo;true&amp;rsquo;. ⚠️ This metric does not work as expected for the case where delta snapshots are disabled (by setting the etcdbrctl flag &lt;code>delta-snapshot-period&lt;/code> to 0).&lt;/p>
&lt;h3 id="defragmentation">Defragmentation&lt;/h3>
&lt;p>The metrics for defragmentation is of type histogram, which gives the number of times defragmentation was triggered. ⚠️ The defragmentation latency should be as low as possible, since
defragmentation is indeed a costly operation, which results in unavailability of etcd for the period.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Type&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>etcdbr_defragmentation_duration_seconds&lt;/td>
&lt;td>Total latency distribution of defragmentation of etcd data directory.&lt;/td>
&lt;td>Histogram&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="validation-and-restoration">Validation and Restoration&lt;/h3>
&lt;p>Two major steps in initialization of etcd data directory are validation and restoration. It is necessary to monitor the count and time duration of these calls, from a high availability perspective.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Type&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>etcdbr_validation_duration_seconds&lt;/td>
&lt;td>Total latency distribution of validating data directory.&lt;/td>
&lt;td>Histogram&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>etcdbr_restoration_duration_seconds&lt;/td>
&lt;td>Total latency distribution of restoring from snapshot.&lt;/td>
&lt;td>Histogram&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="snapstore">Snapstore&lt;/h3>
&lt;p>These bucket-related metrics provide information about the latest set of delta snapshots stored in the snapstore. They provide a rough estimation of the amount of time required to perform a restoration from the latest set of snapshots.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Type&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>etcdbr_snapstore_latest_deltas_total&lt;/td>
&lt;td>Total number of delta snapshots taken since the latest full snapshot.&lt;/td>
&lt;td>Gauge&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>etcdbr_snapstore_latest_deltas_revisions_total&lt;/td>
&lt;td>Total number of revisions stored in delta snapshots taken since the latest full snapshot.&lt;/td>
&lt;td>Gauge&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;code>etcdbr_snapstore_latest_deltas_revisions_total&lt;/code> indicates the total number of etcd revisions (events) stored in the latest set of delta snapshots. The amount of time it would take to perform an etcd data restoration with the latest set of snapshots is directly proportional to this value.&lt;/p>
&lt;h3 id="network">Network&lt;/h3>
&lt;p>These metrics describe the status of the network usage. We use &lt;code>/proc/&amp;lt;etcdbr-pid&amp;gt;/net/dev&lt;/code> to get network usage details for the etcdbr process. Currently these metrics are only supported on linux-based distributions.&lt;/p>
&lt;p>All these metrics are under subsystem &lt;code>network&lt;/code>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Type&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>etcdbr_network_transmitted_bytes&lt;/td>
&lt;td>The total number of bytes received over network.&lt;/td>
&lt;td>Counter&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>etcdbr_network_received_bytes&lt;/td>
&lt;td>The total number of bytes received over network.&lt;/td>
&lt;td>Counter&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;code>etcdbr_network_transmitted_bytes&lt;/code> counts the total number of bytes transmitted. Usually this reflects the data uploaded to object store as part of snapshot uploads.&lt;/p>
&lt;p>&lt;code>etcdbr_network_received_bytes&lt;/code> counts the total number of bytes received. Usually this reflects the data received as part of snapshots from actual &lt;code>etcd&lt;/code>. There could be a sudden spike in this at the time of restoration as well.&lt;/p>
&lt;h2 id="grpc-requests">gRPC requests&lt;/h2>
&lt;p>These metrics are exposed via &lt;a href="https://github.com/grpc-ecosystem/go-grpc-prometheus">go-grpc-prometheus&lt;/a>.&lt;/p>
&lt;h2 id="prometheus-supplied-metrics">Prometheus supplied metrics&lt;/h2>
&lt;p>The Prometheus client library provides a number of metrics under the &lt;code>go&lt;/code> and &lt;code>process&lt;/code> namespaces.&lt;/p></description></item></channel></rss>