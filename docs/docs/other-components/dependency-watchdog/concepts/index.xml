<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener â€“ Concepts</title><link>https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/</link><description>Recent content in Concepts on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Prober</title><link>https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/prober/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/prober/</guid><description>
&lt;h1 id="prober">Prober&lt;/h1>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Prober starts asynchronous and periodic probes for every shoot cluster. The first probe is the api-server probe which checks the reachability of the API Server from the control plane. The second probe is the lease probe which is done after the api server probe is successful and checks if the number of expired node leases is below a certain threshold.
If the lease probe fails, it will scale down the dependent kubernetes resources. Once the connectivity to &lt;code>kube-apiserver&lt;/code> is reestablished and the number of expired node leases are within the accepted threshold, the prober will then proactively scale up the dependent kubernetes resources it had scaled down earlier. The failure threshold fraction for lease probe
and dependent kubernetes resources are defined in &lt;a href="https://github.com/gardener/dependency-watchdog/blob/master/example/04-dwd-prober-configmap.yaml">configuration&lt;/a> that is passed to the prober.&lt;/p>
&lt;h3 id="origin">Origin&lt;/h3>
&lt;p>In a shoot cluster (a.k.a data plane) each node runs a kubelet which periodically renewes its lease. Leases serve as heartbeats informing Kube Controller Manager that the node is alive. The connectivity between the kubelet and the Kube ApiServer can break for different reasons and not recover in time.&lt;/p>
&lt;p>As an example, consider a large shoot cluster with several hundred nodes. There is an issue with a NAT gateway on the shoot cluster which prevents the Kubelet from any node in the shoot cluster to reach its control plane Kube ApiServer. As a consequence, Kube Controller Manager transitioned the nodes of this shoot cluster to &lt;code>Unknown&lt;/code> state.&lt;/p>
&lt;p>&lt;a href="https://github.com/gardener/machine-controller-manager">Machine Controller Manager&lt;/a> which also runs in the shoot control plane reacts to any changes to the Node status and then takes action to recover backing VMs/machine(s). It waits for a grace period and then it will begin to replace the unhealthy machine(s) with new ones.&lt;/p>
&lt;p>This replacement of healthy machines due to a broken connectivity between the worker nodes and the control plane Kube ApiServer results in undesired downtimes for customer workloads that were running on these otherwise healthy nodes. It is therefore required that there be an actor which detects the connectivity loss between the the kubelet and shoot cluster&amp;rsquo;s Kube ApiServer and proactively scales down components in the shoot control namespace which could exacerbate the availability of nodes in the shoot cluster.&lt;/p>
&lt;h2 id="dependency-watchdog-prober-in-gardener">Dependency Watchdog Prober in Gardener&lt;/h2>
&lt;p>Prober is a central component which is deployed in the &lt;code>garden&lt;/code> namespace in the seed cluster. Control plane components for a shoot are deployed in a dedicated shoot namespace for the shoot within the seed cluster.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/prober-components.excalidraw_9c37de.png">
&lt;blockquote>
&lt;p>NOTE: If you are not familiar with what gardener components like seed, shoot then please see the &lt;a href="https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/prober/#appendix">appendix&lt;/a> for links.&lt;/p>
&lt;/blockquote>
&lt;p>Prober periodically probes Kube ApiServer via two separate probes:&lt;/p>
&lt;ol>
&lt;li>API Server Probe: Local cluster DNS name which resolves to the ClusterIP of the Kube Apiserver&lt;/li>
&lt;li>Lease Probe: Checks for number of expired leases to be within the specified threshold. The threshold defines the limit after which DWD can say that the kubelets are not able to reach the API server.&lt;/li>
&lt;/ol>
&lt;h2 id="behind-the-scene">Behind the scene&lt;/h2>
&lt;p>For all active shoot clusters (which have not been hibernated or deleted or moved to another seed via control-plane-migration), prober will schedule a probe to run periodically. During each run of a probe it will do the following:&lt;/p>
&lt;ol>
&lt;li>Checks if the Kube ApiServer is reachable via local cluster DNS. This should always succeed and will fail only when the Kube ApiServer has gone down. If the Kube ApiServer is down then there can be no further damage to the existing shoot cluster (barring new requests to the Kube Api Server).&lt;/li>
&lt;li>Only if the probe is able to reach the Kube ApiServer via local cluster DNS, will it attempt to check the number of expired node leases in the shoot. The node lease renewal is done by the Kubelet, and so we can say that the lease probe is checking if the kubelet is able to reach the API server. If the number of expired node leases reaches
the threshold, then the probe fails.&lt;/li>
&lt;li>If and when a lease probe fails, then it will initiate a scale-down operation for dependent resources as defined in the prober configuration.&lt;/li>
&lt;li>In subsequent runs it will keep performing the lease probe. If it is successful, then it will start the scale-up operation for dependent resources as defined in the configuration.&lt;/li>
&lt;/ol>
&lt;h3 id="prober-lifecycle">Prober lifecycle&lt;/h3>
&lt;p>A reconciler is registered to listen to all events for &lt;a href="https://gardener.cloud/docs/gardener/api-reference/extensions/#extensions.gardener.cloud/v1alpha1.Cluster">Cluster&lt;/a> resource.&lt;/p>
&lt;p>When a &lt;code>Reconciler&lt;/code> receives a request for a &lt;code>Cluster&lt;/code> change, it will query the extension kube-api server to get the &lt;code>Cluster&lt;/code> resource.&lt;/p>
&lt;p>In the following cases it will either remove an existing probe for this cluster or skip creating a new probe:&lt;/p>
&lt;ol>
&lt;li>Cluster is marked for deletion.&lt;/li>
&lt;li>Hibernation has been enabled for the cluster.&lt;/li>
&lt;li>There is an ongoing seed migration for this cluster.&lt;/li>
&lt;li>If a new cluster is created with no workers.&lt;/li>
&lt;li>If an update is made to the cluster by removing all workers (in other words making it worker-less).&lt;/li>
&lt;/ol>
&lt;p>If none of the above conditions are true and there is no existing probe for this cluster then a new probe will be created, registered and started.&lt;/p>
&lt;h3 id="probe-failure-identification">Probe failure identification&lt;/h3>
&lt;p>DWD probe can either be a success or it could return an error. If the API server probe fails, the lease probe is not done and the probes will be retried. If the error is a &lt;code>TooManyRequests&lt;/code> error due to requests to the Kube-API-Server being throttled,
then the probes are retried after a backOff of &lt;code>backOffDurationForThrottledRequests&lt;/code>.&lt;/p>
&lt;p>If the lease probe fails, then the error could be due to failure in listing the leases. In this case, no scaling operations are performed. If the error in listing the leases is a &lt;code>TooManyRequests&lt;/code> error due to requests to the Kube-API-Server being throttled,
then the probes are retried after a backOff of &lt;code>backOffDurationForThrottledRequests&lt;/code>.&lt;/p>
&lt;p>If there is no error in listing the leases, then the Lease probe fails if the number of expired leases reaches the threshold fraction specified in the &lt;a href="https://github.com/gardener/dependency-watchdog/blob/master/example/04-dwd-prober-configmap.yaml">configuration&lt;/a>.
A lease is considered expired in the following scenario:-&lt;/p>
&lt;pre tabindex="0">&lt;code> time.Now() &amp;gt;= lease.Spec.RenewTime + (p.config.KCMNodeMonitorGraceDuration.Duration * expiryBufferFraction)
&lt;/code>&lt;/pre>&lt;p>Here, &lt;code>lease.Spec.RenewTime&lt;/code> is the time when current holder of a lease has last updated the lease. &lt;code>config&lt;/code> is the probe config generated from the &lt;a href="https://github.com/gardener/dependency-watchdog/blob/master/example/04-dwd-prober-configmap.yaml">configuration&lt;/a> and
&lt;code>KCMNodeMonitorGraceDuration&lt;/code> is amount of time which KCM allows a running Node to be unresponsive before marking it unhealthy (See &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/#:~:text=Amount%20of%20time%20which%20we%20allow%20running%20Node%20to%20be%20unresponsive%20before%20marking%20it%20unhealthy.%20Must%20be%20N%20times%20more%20than%20kubelet%27s%20nodeStatusUpdateFrequency%2C%20where%20N%20means%20number%20of%20retries%20allowed%20for%20kubelet%20to%20post%20node%20status.">ref&lt;/a>)
. &lt;code>expiryBufferFraction&lt;/code> is a hard coded value of &lt;code>0.75&lt;/code>. Using this fraction allows the prober to intervene before KCM marks a node as unknown, but at the same time allowing kubelet sufficient retries to renew the node lease (Kubelet renews the lease every &lt;code>10s&lt;/code> See &lt;a href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#:~:text=The%20lease%20is%20currently%20renewed%20every%2010s%2C%20per%20KEP%2D0009.">ref&lt;/a>).&lt;/p>
&lt;h2 id="appendix">Appendix&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/gardener/gardener/blob/master/docs">Gardener&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/14-reversed-cluster-vpn.md">Reverse Cluster VPN&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Weeder</title><link>https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/weeder/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/weeder/</guid><description>
&lt;h1 id="weeder">Weeder&lt;/h1>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Weeder watches for an update to service endpoints and on receiving such an event it will create a time-bound watch for all configured dependent pods that need to be actively recovered in case they have not yet recovered from &lt;code>CrashLoopBackoff&lt;/code> state. In a nutshell it accelerates recovery of pods when an upstream service recovers.&lt;/p>
&lt;p>An interference in automatic recovery for dependent pods is required because kubernetes pod restarts a container with an exponential backoff when the pod is in &lt;code>CrashLoopBackOff&lt;/code> state. This backoff could become quite large if the service stays down for long. Presence of weeder would not let that happen as it&amp;rsquo;ll restart the pod.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>Before we understand how Weeder works, we need to be familiar with kubernetes &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/">services &amp;amp; endpoints&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>NOTE: If a kubernetes service is created with selectors then kubernetes will create corresponding endpoint resource which will have the same name as that of the service. In weeder implementation service and endpoint name is used interchangeably.&lt;/p>
&lt;/blockquote>
&lt;h2 id="config">Config&lt;/h2>
&lt;p>Weeder can be configured via command line arguments and a weeder configuration. See &lt;a href="https://gardener.cloud/docs/other-components/dependency-watchdog/deployment/configure/#weeder">configure weeder&lt;/a>.&lt;/p>
&lt;h2 id="internals">Internals&lt;/h2>
&lt;p>Weeder keeps a watch on the events for the specified endpoints in the config. For every endpoints a list of &lt;code>podSelectors&lt;/code> can be specified. It cretes a weeder object per endpoints resource when it receives a satisfactory &lt;code>Create&lt;/code> or &lt;code>Update&lt;/code> event. Then for every podSelector it creates a goroutine. This goroutine keeps a watch on the pods with labels as per the podSelector and kills any pod which turn into &lt;code>CrashLoopBackOff&lt;/code>. Each weeder lives for &lt;code>watchDuration&lt;/code> interval which has a default value of 5 mins if not explicitly set.&lt;/p>
&lt;p>To understand the actions taken by the weeder lets use the following diagram as a reference.
&lt;img src="https://gardener.cloud/__resources/weeder-components.excalidraw_0c0a69.png">
Let us also assume the following configuration for the weeder:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>watchDuration: 2m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>servicesAndDependantSelectors:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> etcd-main-client: &lt;span style="color:#008000"># name of the service/endpoint for etcd statefulset that weeder will receive events for.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> podSelectors: &lt;span style="color:#008000"># all pods matching the label selector are direct dependencies for etcd service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - matchExpressions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: gardener.cloud/role
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operator: In
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> values:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - controlplane
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: role
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operator: In
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> values:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kube-apiserver: &lt;span style="color:#008000"># name of the service/endpoint for kube-api-server pods that weeder will receive events for. &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> podSelectors: &lt;span style="color:#008000"># all pods matching the label selector are direct dependencies for kube-api-server service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - matchExpressions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: gardener.cloud/role
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operator: In
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> values:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - controlplane
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: role
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operator: NotIn
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> values:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - main
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - apiserver
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Only for the sake of demonstration lets pick the first service -&amp;gt; dependent pods tuple (&lt;code>etcd-main-client&lt;/code> as the service endpoint).&lt;/p>
&lt;blockquote>
&lt;ol>
&lt;li>Assume that there are 3 replicas for etcd statefulset.&lt;/li>
&lt;li>Time here is just for showing the series of events&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;code>t=0&lt;/code> -&amp;gt; all etcd pods go down&lt;/li>
&lt;li>&lt;code>t=10&lt;/code> -&amp;gt; kube-api-server pods transition to CrashLoopBackOff&lt;/li>
&lt;li>&lt;code>t=100&lt;/code> -&amp;gt; all etcd pods recover together&lt;/li>
&lt;li>&lt;code>t=101&lt;/code> -&amp;gt; Weeder sees &lt;code>Update&lt;/code> event for &lt;code>etcd-main-client&lt;/code> endpoints resource&lt;/li>
&lt;li>&lt;code>t=102&lt;/code> -&amp;gt; go routine created to keep watch on kube-api-server pods&lt;/li>
&lt;li>&lt;code>t=103&lt;/code> -&amp;gt; Since kube-api-server pods are still in CrashLoopBackOff, weeder deletes the pods to accelerate the recovery.&lt;/li>
&lt;li>&lt;code>t=104&lt;/code> -&amp;gt; new kube-api-server pod created by replica-set controller in kube-controller-manager&lt;/li>
&lt;/ul>
&lt;h3 id="points-to-note">Points to Note&lt;/h3>
&lt;ul>
&lt;li>Weeder only respond on &lt;code>Update&lt;/code> events where a &lt;code>notReady&lt;/code> endpoints resource turn to &lt;code>Ready&lt;/code>. Thats why there was no weeder action at time &lt;code>t=10&lt;/code> in the example above.
&lt;ul>
&lt;li>&lt;code>notReady&lt;/code> -&amp;gt; no backing pod is Ready&lt;/li>
&lt;li>&lt;code>Ready&lt;/code> -&amp;gt; atleast one backing pod is Ready&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Weeder doesn&amp;rsquo;t respond on &lt;code>Delete&lt;/code> events&lt;/li>
&lt;li>Weeder will always wait for the entire &lt;code>watchDuration&lt;/code>. If the dependent pods transition to CrashLoopBackOff after the watch duration or even after repeated deletion of these pods they do not recover then weeder will exit. Quality of service offered via a weeder is only Best-Effort.&lt;/li>
&lt;/ul></description></item></channel></rss>