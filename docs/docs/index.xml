<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener – Docs</title><link>https://gardener.cloud/docs/</link><description>Recent content in Docs on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 22 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://gardener.cloud/docs/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: API Reference</title><link>https://gardener.cloud/docs/other-components/etcd-druid/api-reference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-druid/api-reference/</guid><description>
&lt;p>Packages:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1">druid.gardener.cloud/v1alpha1&lt;/a>
&lt;/li>
&lt;/ul>
&lt;h2 id="druid.gardener.cloud/v1alpha1">druid.gardener.cloud/v1alpha1&lt;/h2>
&lt;p>
&lt;p>Package v1alpha1 is the v1alpha1 version of the etcd-druid API.&lt;/p>
&lt;/p>
Resource Types:
&lt;ul>&lt;/ul>
&lt;h3 id="druid.gardener.cloud/v1alpha1.BackupSpec">BackupSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec">EtcdSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>BackupSpec defines parameters associated with the full and delta snapshots of etcd.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>port&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Port define the port on which etcd-backup-restore server will be exposed.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>tls&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig">
TLSConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>image&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Image defines the etcd container image and tag&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>store&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec">
StoreSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Store defines the specification of object store provider for storing backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>resources&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core">
Kubernetes core/v1.ResourceRequirements
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Resources defines compute Resources required by backup-restore container.
More info: &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/">https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>compactionResources&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core">
Kubernetes core/v1.ResourceRequirements
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>CompactionResources defines compute Resources required by compaction job.
More info: &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/">https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>fullSnapshotSchedule&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>FullSnapshotSchedule defines the cron standard schedule for full snapshots.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>garbageCollectionPolicy&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.GarbageCollectionPolicy">
GarbageCollectionPolicy
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>GarbageCollectionPolicy defines the policy for garbage collecting old backups&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>garbageCollectionPeriod&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>GarbageCollectionPeriod defines the period for garbage collecting old backups&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>deltaSnapshotPeriod&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>DeltaSnapshotPeriod defines the period after which delta snapshots will be taken&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>deltaSnapshotMemoryLimit&lt;/code>&lt;/br>
&lt;em>
k8s.io/apimachinery/pkg/api/resource.Quantity
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>DeltaSnapshotMemoryLimit defines the memory limit after which delta snapshots will be taken&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>compression&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompressionSpec">
CompressionSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>SnapshotCompression defines the specification for compression of Snapshots.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>enableProfiling&lt;/code>&lt;/br>
&lt;em>
bool
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>EnableProfiling defines if profiling should be enabled for the etcd-backup-restore-sidecar&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>etcdSnapshotTimeout&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>EtcdSnapshotTimeout defines the timeout duration for etcd FullSnapshot operation&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>leaderElection&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.LeaderElectionSpec">
LeaderElectionSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>LeaderElection defines parameters related to the LeaderElection configuration.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.ClientService">ClientService
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig">EtcdConfig&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>ClientService defines the parameters of the client service that a user can specify&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>annotations&lt;/code>&lt;/br>
&lt;em>
map[string]string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Annotations specify the annotations that should be added to the client service&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>labels&lt;/code>&lt;/br>
&lt;em>
map[string]string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Labels specify the labels that should be added to the client service&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.CompactionMode">CompactionMode
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SharedConfig">SharedConfig&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>CompactionMode defines the auto-compaction-mode: ‘periodic’ or ‘revision’.
‘periodic’ for duration based retention and ‘revision’ for revision number based retention.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.CompressionPolicy">CompressionPolicy
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompressionSpec">CompressionSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>CompressionPolicy defines the type of policy for compression of snapshots.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.CompressionSpec">CompressionSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec">BackupSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>CompressionSpec defines parameters related to compression of Snapshots(full as well as delta).&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>enabled&lt;/code>&lt;/br>
&lt;em>
bool
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>policy&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompressionPolicy">
CompressionPolicy
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.Condition">Condition
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus">EtcdCopyBackupsTaskStatus&lt;/a>,
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus">EtcdStatus&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>Condition holds the information about the state of a resource.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>type&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.ConditionType">
ConditionType
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Type of the Etcd condition.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.ConditionStatus">
ConditionStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Status of the condition, one of True, False, Unknown.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastTransitionTime&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta">
Kubernetes meta/v1.Time
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Last time the condition transitioned from one status to another.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastUpdateTime&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta">
Kubernetes meta/v1.Time
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Last time the condition was updated.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>reason&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>The reason for the condition’s last transition.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>message&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>A human-readable message indicating details about the transition.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.ConditionStatus">ConditionStatus
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition">Condition&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>ConditionStatus is the status of a condition.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.ConditionType">ConditionType
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition">Condition&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>ConditionType is the type of condition.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.CrossVersionObjectReference">CrossVersionObjectReference
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus">EtcdStatus&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>CrossVersionObjectReference contains enough information to let you identify the referred resource.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>kind&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Kind of the referent&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>name&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Name of the referent&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>apiVersion&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>API version of the referent&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.Etcd">Etcd
&lt;/h3>
&lt;p>
&lt;p>Etcd is the Schema for the etcds API&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>metadata&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
Refer to the Kubernetes API documentation for the fields of the
&lt;code>metadata&lt;/code> field.
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>spec&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec">
EtcdSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;br/>
&lt;br/>
&lt;table>
&lt;tr>
&lt;td>
&lt;code>selector&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>selector is a label query over pods that should match the replica count.
It must match the pod template’s labels.
More info: &lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>labels&lt;/code>&lt;/br>
&lt;em>
map[string]string
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>annotations&lt;/code>&lt;/br>
&lt;em>
map[string]string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>etcd&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig">
EtcdConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>backup&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec">
BackupSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>sharedConfig&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SharedConfig">
SharedConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>schedulingConstraints&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SchedulingConstraints">
SchedulingConstraints
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>replicas&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>priorityClassName&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>PriorityClassName is the name of a priority class that shall be used for the etcd pods.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>storageClass&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>StorageClass defines the name of the StorageClass required by the claim.
More info: &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1">https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>storageCapacity&lt;/code>&lt;/br>
&lt;em>
k8s.io/apimachinery/pkg/api/resource.Quantity
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>StorageCapacity defines the size of persistent volume.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>volumeClaimTemplate&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>VolumeClaimTemplate defines the volume claim template to be created&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus">
EtcdStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdConfig">EtcdConfig
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec">EtcdSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdConfig defines parameters associated etcd deployed&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>quota&lt;/code>&lt;/br>
&lt;em>
k8s.io/apimachinery/pkg/api/resource.Quantity
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Quota defines the etcd DB quota.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>defragmentationSchedule&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>DefragmentationSchedule defines the cron standard schedule for defragmentation of etcd.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>serverPort&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>clientPort&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>image&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Image defines the etcd container image and tag&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>authSecretRef&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core">
Kubernetes core/v1.SecretReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>metrics&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.MetricsLevel">
MetricsLevel
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Metrics defines the level of detail for exported metrics of etcd, specify ‘extensive’ to include histogram metrics.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>resources&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core">
Kubernetes core/v1.ResourceRequirements
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Resources defines the compute Resources required by etcd container.
More info: &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/">https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>clientUrlTls&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig">
TLSConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ClientUrlTLS contains the ca, server TLS and client TLS secrets for client communication to ETCD cluster&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>peerUrlTls&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig">
TLSConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>PeerUrlTLS contains the ca and server TLS secrets for peer communication within ETCD cluster
Currently, PeerUrlTLS does not require client TLS secrets for gardener implementation of ETCD cluster.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>etcdDefragTimeout&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>EtcdDefragTimeout defines the timeout duration for etcd defrag call&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>heartbeatDuration&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>HeartbeatDuration defines the duration for members to send heartbeats. The default value is 10s.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>clientService&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.ClientService">
ClientService
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ClientService defines the parameters of the client service that a user can specify&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask">EtcdCopyBackupsTask
&lt;/h3>
&lt;p>
&lt;p>EtcdCopyBackupsTask is a task for copying etcd backups from a source to a target store.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>metadata&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
Refer to the Kubernetes API documentation for the fields of the
&lt;code>metadata&lt;/code> field.
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>spec&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec">
EtcdCopyBackupsTaskSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;br/>
&lt;br/>
&lt;table>
&lt;tr>
&lt;td>
&lt;code>sourceStore&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec">
StoreSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>SourceStore defines the specification of the source object store provider for storing backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>targetStore&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec">
StoreSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>TargetStore defines the specification of the target object store provider for storing backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>maxBackupAge&lt;/code>&lt;/br>
&lt;em>
uint32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>MaxBackupAge is the maximum age in days that a backup must have in order to be copied.
By default all backups will be copied.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>maxBackups&lt;/code>&lt;/br>
&lt;em>
uint32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>MaxBackups is the maximum number of backups that will be copied starting with the most recent ones.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>waitForFinalSnapshot&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec">
WaitForFinalSnapshotSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>WaitForFinalSnapshot defines the parameters for waiting for a final full snapshot before copying backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus">
EtcdCopyBackupsTaskStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec">EtcdCopyBackupsTaskSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask">EtcdCopyBackupsTask&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdCopyBackupsTaskSpec defines the parameters for the copy backups task.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>sourceStore&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec">
StoreSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>SourceStore defines the specification of the source object store provider for storing backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>targetStore&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec">
StoreSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>TargetStore defines the specification of the target object store provider for storing backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>maxBackupAge&lt;/code>&lt;/br>
&lt;em>
uint32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>MaxBackupAge is the maximum age in days that a backup must have in order to be copied.
By default all backups will be copied.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>maxBackups&lt;/code>&lt;/br>
&lt;em>
uint32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>MaxBackups is the maximum number of backups that will be copied starting with the most recent ones.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>waitForFinalSnapshot&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec">
WaitForFinalSnapshotSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>WaitForFinalSnapshot defines the parameters for waiting for a final full snapshot before copying backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus">EtcdCopyBackupsTaskStatus
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask">EtcdCopyBackupsTask&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdCopyBackupsTaskStatus defines the observed state of the copy backups task.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>conditions&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition">
[]Condition
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Conditions represents the latest available observations of an object’s current state.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>observedGeneration&lt;/code>&lt;/br>
&lt;em>
int64
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ObservedGeneration is the most recent generation observed for this resource.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastError&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>LastError represents the last occurred error.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdMemberConditionStatus">EtcdMemberConditionStatus
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberStatus">EtcdMemberStatus&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdMemberConditionStatus is the status of an etcd cluster member.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdMemberStatus">EtcdMemberStatus
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus">EtcdStatus&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdMemberStatus holds information about a etcd cluster membership.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>name&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Name is the name of the etcd member. It is the name of the backing &lt;code>Pod&lt;/code>.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>id&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ID is the ID of the etcd member.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>role&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdRole">
EtcdRole
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Role is the role in the etcd cluster, either &lt;code>Leader&lt;/code> or &lt;code>Member&lt;/code>.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberConditionStatus">
EtcdMemberConditionStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Status of the condition, one of True, False, Unknown.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>reason&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>The reason for the condition’s last transition.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastTransitionTime&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta">
Kubernetes meta/v1.Time
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>LastTransitionTime is the last time the condition’s status changed.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdRole">EtcdRole
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberStatus">EtcdMemberStatus&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdRole is the role of an etcd cluster member.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdSpec">EtcdSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Etcd">Etcd&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdSpec defines the desired state of Etcd&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>selector&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>selector is a label query over pods that should match the replica count.
It must match the pod template’s labels.
More info: &lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>labels&lt;/code>&lt;/br>
&lt;em>
map[string]string
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>annotations&lt;/code>&lt;/br>
&lt;em>
map[string]string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>etcd&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig">
EtcdConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>backup&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec">
BackupSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>sharedConfig&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SharedConfig">
SharedConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>schedulingConstraints&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SchedulingConstraints">
SchedulingConstraints
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>replicas&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>priorityClassName&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>PriorityClassName is the name of a priority class that shall be used for the etcd pods.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>storageClass&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>StorageClass defines the name of the StorageClass required by the claim.
More info: &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1">https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>storageCapacity&lt;/code>&lt;/br>
&lt;em>
k8s.io/apimachinery/pkg/api/resource.Quantity
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>StorageCapacity defines the size of persistent volume.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>volumeClaimTemplate&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>VolumeClaimTemplate defines the volume claim template to be created&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdStatus">EtcdStatus
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Etcd">Etcd&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdStatus defines the observed state of Etcd.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>observedGeneration&lt;/code>&lt;/br>
&lt;em>
int64
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ObservedGeneration is the most recent generation observed for this resource.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>etcd&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CrossVersionObjectReference">
CrossVersionObjectReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>conditions&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition">
[]Condition
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Conditions represents the latest available observations of an etcd’s current state.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>serviceName&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ServiceName is the name of the etcd service.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastError&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>LastError represents the last occurred error.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>clusterSize&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Cluster size is the size of the etcd cluster.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>currentReplicas&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>CurrentReplicas is the current replica count for the etcd cluster.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>replicas&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Replicas is the replica count of the etcd resource.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>readyReplicas&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ReadyReplicas is the count of replicas being ready in the etcd cluster.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>ready&lt;/code>&lt;/br>
&lt;em>
bool
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Ready is &lt;code>true&lt;/code> if all etcd replicas are ready.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>updatedReplicas&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>UpdatedReplicas is the count of updated replicas in the etcd cluster.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>labelSelector&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>LabelSelector is a label query over pods that should match the replica count.
It must match the pod template’s labels.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>members&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberStatus">
[]EtcdMemberStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Members represents the members of the etcd cluster&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>peerUrlTLSEnabled&lt;/code>&lt;/br>
&lt;em>
bool
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>PeerUrlTLSEnabled captures the state of peer url TLS being enabled for the etcd member(s)&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.GarbageCollectionPolicy">GarbageCollectionPolicy
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec">BackupSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>GarbageCollectionPolicy defines the type of policy for snapshot garbage collection.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.LeaderElectionSpec">LeaderElectionSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec">BackupSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>LeaderElectionSpec defines parameters related to the LeaderElection configuration.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>reelectionPeriod&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ReelectionPeriod defines the Period after which leadership status of corresponding etcd is checked.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>etcdConnectionTimeout&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>EtcdConnectionTimeout defines the timeout duration for etcd client connection during leader election.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.MetricsLevel">MetricsLevel
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig">EtcdConfig&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MetricsLevel defines the level ‘basic’ or ‘extensive’.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.SchedulingConstraints">SchedulingConstraints
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec">EtcdSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>SchedulingConstraints defines the different scheduling constraints that must be applied to the
pod spec in the etcd statefulset.
Currently supported constraints are Affinity and TopologySpreadConstraints.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>affinity&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#affinity-v1-core">
Kubernetes core/v1.Affinity
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Affinity defines the various affinity and anti-affinity rules for a pod
that are honoured by the kube-scheduler.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>topologySpreadConstraints&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#topologyspreadconstraint-v1-core">
[]Kubernetes core/v1.TopologySpreadConstraint
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>TopologySpreadConstraints describes how a group of pods ought to spread across topology domains,
that are honoured by the kube-scheduler.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.SecretReference">SecretReference
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig">TLSConfig&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>SecretReference defines a reference to a secret.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>SecretReference&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core">
Kubernetes core/v1.SecretReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>
(Members of &lt;code>SecretReference&lt;/code> are embedded into this type.)
&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>dataKey&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>DataKey is the name of the key in the data map containing the credentials.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.SharedConfig">SharedConfig
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec">EtcdSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>SharedConfig defines parameters shared and used by Etcd as well as backup-restore sidecar.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>autoCompactionMode&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompactionMode">
CompactionMode
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>AutoCompactionMode defines the auto-compaction-mode:‘periodic’ mode or ‘revision’ mode for etcd and embedded-Etcd of backup-restore sidecar.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>autoCompactionRetention&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>AutoCompactionRetention defines the auto-compaction-retention length for etcd as well as for embedded-Etcd of backup-restore sidecar.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.StorageProvider">StorageProvider
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec">StoreSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>StorageProvider defines the type of object store provider for storing backups.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.StoreSpec">StoreSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec">BackupSpec&lt;/a>,
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec">EtcdCopyBackupsTaskSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>StoreSpec defines parameters related to ObjectStore persisting backups&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>container&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Container is the name of the container the backup is stored at.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>prefix&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Prefix is the prefix used for the store.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>provider&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StorageProvider">
StorageProvider
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Provider is the name of the backup provider.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>secretRef&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core">
Kubernetes core/v1.SecretReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>SecretRef is the reference to the secret which used to connect to the backup store.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.TLSConfig">TLSConfig
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec">BackupSpec&lt;/a>,
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig">EtcdConfig&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>TLSConfig hold the TLS configuration details.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>tlsCASecretRef&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SecretReference">
SecretReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>serverTLSSecretRef&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core">
Kubernetes core/v1.SecretReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>clientTLSSecretRef&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core">
Kubernetes core/v1.SecretReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec">WaitForFinalSnapshotSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec">EtcdCopyBackupsTaskSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>WaitForFinalSnapshotSpec defines the parameters for waiting for a final full snapshot before copying backups.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>enabled&lt;/code>&lt;/br>
&lt;em>
bool
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Enabled specifies whether to wait for a final full snapshot before copying backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>timeout&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Timeout is the timeout for waiting for a final full snapshot. When this timeout expires, the copying of backups
will be performed anyway. No timeout or 0 means wait forever.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;hr/>
&lt;p>&lt;em>
Generated with &lt;a href="https://github.com/ahmetb/gen-crd-api-reference-docs">gen-crd-api-reference-docs&lt;/a>
&lt;/em>&lt;/p></description></item><item><title>Docs: Architecture</title><link>https://gardener.cloud/docs/dashboard/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/dashboard/architecture/</guid><description>
&lt;h1 id="dashboard-architecture-overview">Dashboard Architecture Overview&lt;/h1>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>The dashboard &lt;code>frontend&lt;/code> is a Single Page Application (SPA) built with &lt;a href="https://vuejs.org/">Vue.js&lt;/a>. The dashboard &lt;code>backend&lt;/code> is a web server built with &lt;a href="http://expressjs.com">Express&lt;/a> and &lt;a href="https://nodejs.org/">Node.js&lt;/a>. The &lt;code>backend&lt;/code> serves the bundled &lt;code>frontend&lt;/code> as static content. The dashboard uses &lt;a href="https://socket.io/">Socket.IO&lt;/a> to enable real-time, bidirectional and event-based communication between the &lt;code>frontend&lt;/code> and the &lt;code>backend&lt;/code>. For the communication from the &lt;code>backend&lt;/code> to different &lt;code>kube-apiservers&lt;/code> the http/2 network protocol is used. Authentication at the &lt;code>apiserver&lt;/code> of the garden cluster is done via JWT tokens. These can either be an ID Token issued by an OpenID Connect Provider or the token of a Kubernetes Service Account.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/architecture-1_09ad4f.png">
&lt;h2 id="frontend">Frontend&lt;/h2>
&lt;p>The dashboard &lt;code>frontend&lt;/code> consists of many Vue.js single file components that manage their state via a &lt;a href="https://vuex.vuejs.org/">centralized store&lt;/a>. The store defines mutations to modify the state synchronously. If several mutations have to be combined or the state in the &lt;code>backend&lt;/code> has to be modified at the same time, the store provides asynchronous actions to do this job. The synchronization of the data with the &lt;code>backend&lt;/code> is done by plugins that also use actions.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/architecture-2_718ce1.png">
&lt;h2 id="backend">Backend&lt;/h2>
&lt;p>The &lt;code>backend&lt;/code> is currently a monolithic Node.js application, but it performs several tasks that are actually independent.&lt;/p>
&lt;ul>
&lt;li>Static web server for the &lt;code>frontend&lt;/code> single page application&lt;/li>
&lt;li>Forward real time events of the &lt;code>apiserver&lt;/code> to the &lt;code>frontend&lt;/code>&lt;/li>
&lt;li>Provide an HTTP API&lt;/li>
&lt;li>Initiate and manage the end user login flow in order to obtain an ID Token&lt;/li>
&lt;li>Bidirectional integration with the GitHub issue management&lt;/li>
&lt;/ul>
&lt;img src="https://gardener.cloud/__resources/architecture-3_c848c2.png">
&lt;p>It is planned to split the &lt;code>backend&lt;/code> into several independent containers to increase stability and performance.&lt;/p>
&lt;h2 id="authentication">Authentication&lt;/h2>
&lt;p>The following diagram shows the authorization code flow in the Gardener dashboard. When the user clicks the login button, he is redirected to the authorization endpoint of the openid connect provider. In the case of &lt;a href="https://dexidp.io/">Dex IDP&lt;/a>, authentication is delegated to the connected IDP. After a successful login, the OIDC provider redirects back to the dashboard &lt;code>backend&lt;/code> with a one time authorization code. With this code, the dashboard &lt;code>backend&lt;/code> can now request an ID token for the logged in user. The ID token is encrypted and stored as a secure &lt;code>httpOnly&lt;/code> session cookie.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/architecture-4_371bde.png"></description></item><item><title>Docs: Components</title><link>https://gardener.cloud/docs/getting-started/observability/components/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/observability/components/</guid><description>
&lt;h2 id="core-components">Core Components&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/core-components_287ae2.png" alt="">&lt;/p>
&lt;p>The core Observability components which Gardener offers out-of-the-box are:&lt;/p>
&lt;ul>
&lt;li>Prometheus - for Metrics and Alerting&lt;/li>
&lt;li>Vali - a Loki fork for Logging&lt;/li>
&lt;li>Plutono - a Grafana fork for Dashboard visualization&lt;/li>
&lt;/ul>
&lt;p>Both forks are done from the last version with an Apache license.&lt;/p>
&lt;h3 id="control-plane-components-on-the-seed">Control Plane Components on the Seed&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/control-plane-components_1b8262.png" alt="">&lt;/p>
&lt;p>Prometheus, Plutono, and Vali are all located in the seed cluster. They run next to the control plane of your cluster.&lt;/p>
&lt;p>The next sections will explore those components in detail.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Gardener only provides monitoring for Gardener-deployed components. If you need logging or monitoring for your workload, then you need to deploy your own monitoring stack into your shoot cluster.
&lt;/div>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Gardener only provides a monitoring stack if the cluster is not of &lt;code>purpose: testing&lt;/code>. For more information, see &lt;a href="https://gardener.cloud/docs/gardener/shoot_purposes/">Shoot Cluster Purpose&lt;/a>.
&lt;/div>
&lt;h3 id="logging-into-plutono">Logging into Plutono&lt;/h3>
&lt;p>Let us start by giving some visual hints on how to access Plutono. &lt;a href="https://github.com/credativ/plutono#plutono">Plutono&lt;/a> allows us to query logs and metrics and visualise those in form of dashboards. Plutono is shipped ready-to-use with a Gardener shoot cluster.&lt;/p>
&lt;p>In order to access the Gardener provided dashboards, open the &lt;code>Plutono&lt;/code> link provided in the Gardener dashboard and use the username and password provided next to it.&lt;/p>
&lt;p>The password you can use to log in can be retrieved as shown below:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/access-plutono_07ba1b.png" alt="">&lt;/p>
&lt;h3 id="accessing-the-dashboards">Accessing the Dashboards&lt;/h3>
&lt;p>After logging in, you will be greeted with a Plutono welcome screen. Navigate to &lt;code>General/Home&lt;/code>, as depicted with the red arrow in the next picture:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/welcome_plutono_c44ade.png" alt="">&lt;/p>
&lt;p>Then you will be able to select the dashboards. Some interesting ones to look at are:&lt;/p>
&lt;ul>
&lt;li>The &lt;code>Kubernetes Control Plane Status&lt;/code> dashboard allows you to check control plane availability during a certain time frame.&lt;/li>
&lt;li>The &lt;code>API Server&lt;/code> dashboard gives you an overview on which requests are done towards your apiserver and how long they take.&lt;/li>
&lt;li>With the &lt;code>Node Details&lt;/code> dashboard you can analyze CPU/Network pressure or memory usage for nodes.&lt;/li>
&lt;li>The &lt;code>Network Problem Detector&lt;/code> dashboard illustrates the results of periodic networking checks between nodes and to the APIServer.&lt;/li>
&lt;/ul>
&lt;p>Here is a picture with the &lt;code>Kubernetes Control Plane Status&lt;/code> dashboard.
&lt;img src="https://gardener.cloud/__resources/plutono_855b74.png" alt="">&lt;/p>
&lt;h3 id="prometheus">Prometheus&lt;/h3>
&lt;p>&lt;a href="https://prometheus.io/">Prometheus&lt;/a> is a monitoring system and a time series database. It can be queried using PromQL, the so called Prometheus Querying Language.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/prometheus_707368.png" alt="">&lt;/p>
&lt;p>This example query describes the current uptime status of the kube apiserver.&lt;/p>
&lt;h4 id="prometheus-and-plutono">Prometheus and Plutono&lt;/h4>
&lt;p>Time series data from Prometheus can be made visible with Plutono. Here we see how the query above which describes the uptime of a Kubernetes cluster is visualized with a Plutono dashboard.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/prometheus-plutono_662ad5.png" alt="">&lt;/p>
&lt;h3 id="vali-logs-via-plutono">Vali Logs via Plutono&lt;/h3>
&lt;p>Vali is our logging solution. In order to access the logs provided by Vali, you need to:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="https://gardener.cloud/docs/getting-started/observability/components/#logging-into-plutono">Log into Plutono&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Choose &lt;code>Explore&lt;/code>, which is depicted as the little compass symbol:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://gardener.cloud/__resources/explore_loki_1ca701.png" alt="">&lt;/p>
&lt;ol start="3">
&lt;li>Select &lt;code>Vali&lt;/code> at the top left, as shown here:&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://gardener.cloud/__resources/select_vali_62dd46.png" alt="">&lt;/p>
&lt;p>There you can browse logs or events of the control plane components.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/vali-logs_eb9bd0.png" alt="">&lt;/p>
&lt;p>Here are some examples of helpful queries:&lt;/p>
&lt;ul>
&lt;li>&lt;code>{container_name=&amp;quot;cluster-autoscaler&amp;quot; }&lt;/code> to get cluster-autoscaler logs and see why certain node groups were scaled up.&lt;/li>
&lt;li>&lt;code>{container_name=&amp;quot;kube-apiserver&amp;quot;} |~ &amp;quot;error&amp;quot; &lt;/code> to get the logs of the kube-apiserver container and filter for errors.&lt;/li>
&lt;li>&lt;code>{unit=&amp;quot;kubelet.service&amp;quot;, nodename=&amp;quot;ip-123&amp;quot;}&lt;/code> to get the kubelet logs of a specific node.&lt;/li>
&lt;li>&lt;code>{unit=&amp;quot;containerd.service&amp;quot;, nodename=&amp;quot;ip-123&amp;quot;}&lt;/code> to retrieve the containerd logs for a specific node.&lt;/li>
&lt;/ul>
&lt;p>Choose &lt;code>Help &amp;gt;&lt;/code> in order to see what options exist to filter the results.&lt;/p>
&lt;p>For more information on how to retrieve K8s events from the past, see &lt;a href="https://gardener.cloud/docs/gardener/logging-usage/#how-to-access-the-logs">How to Access Logs&lt;/a>.&lt;/p>
&lt;h2 id="detailed-view">Detailed View&lt;/h2>
&lt;h3 id="data-flow">Data Flow&lt;/h3>
&lt;p>Our monitoring and logging solutions Vali and Prometheus both run next to the control plane of the shoot cluster.&lt;/p>
&lt;h4 id="data-flow---logging">Data Flow - Logging&lt;/h4>
&lt;p>The following diagram allows a more detailed look at Vali and the data flow.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/data-flow-logging_e097b8.png" alt="">&lt;/p>
&lt;p>On the very left, we see Plutono as it displays the logs. Vali is aggregating the logs from different sources.&lt;/p>
&lt;p>Valitail and Fluentbit send the logs to Vali, which in turn stores them.&lt;/p>
&lt;p>&lt;em>Valitail&lt;/em>&lt;/p>
&lt;p>Valitail is a systemd service that runs on each node. It scrapes kubelet, containerd, kernel logs, and the logs of the pods in the kube-system namespace.&lt;/p>
&lt;p>&lt;em>Fluentbit&lt;/em>&lt;/p>
&lt;p>Fluentbit runs as a daemonset on each seed node. It scrapes logs of the kubernetes control plane components, like apiserver or etcd.&lt;/p>
&lt;p>It also scrapes logs of the Gardener deployed components which run next to the control plane of the cluster, like the machine-controller-manager or the cluster autoscaler. Debugging those components, for example, would be helpful when finding out why certain worker groups got scaled up or why nodes were replaced.&lt;/p>
&lt;h4 id="data-flow---monitoring">Data Flow - Monitoring&lt;/h4>
&lt;p>Next to each shoot&amp;rsquo;s control plane, we deploy an instance of Prometheus in the seed.&lt;/p>
&lt;p>Gardener uses &lt;a href="https://prometheus.io/">Prometheus&lt;/a> for storing and accessing shoot-related metrics and alerting.&lt;/p>
&lt;p>The diagram below shows the data flow of metrics.
Plutono uses PromQL queries to query data from Prometheus. It then visualises those metrics in dashboards.
Prometheus itself scrapes various targets for metrics, as seen in the diagram below by the arrows pointing to the Prometheus instance.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/data-flow-monitoring_6d7cf3.png" alt="">&lt;/p>
&lt;p>Let us have a look what metrics we scrape for debugging purposes:&lt;/p>
&lt;p>&lt;strong>Container performance metrics&lt;/strong>&lt;/p>
&lt;p>cAdvisor is an open-source agent integrated into the kubelet binary that monitors resource usage and analyzes the performance of containers. It collects statistics about the CPU, memory, file, and network usage for all containers running on a given node. We use it to scrape data for all pods running in the kube-system namespace in the shoot cluster.&lt;/p>
&lt;p>&lt;strong>Hardware and kernel-related metrics&lt;/strong>&lt;/p>
&lt;p>The &lt;a href="https://prometheus.io/docs/guides/node-exporter/">Prometheus Node Exporter&lt;/a> runs as a daemonset in the kube-system namespace of your shoot cluster. It exposes a wide variety of hardware and kernel-related metrics. Some of the metrics we scrape are, for example, the current usage of the filesystem (&lt;code>node_filesystem_free_bytes&lt;/code>) or current CPU usage (&lt;code>node_cpu_seconds_total&lt;/code>). Both can help you identify if nodes are running out of hardware resources, which could lead to your workload experiencing downtimes.&lt;/p>
&lt;p>&lt;strong>Control plane component specific metrics&lt;/strong>&lt;/p>
&lt;p>The different control plane pods (for example, etcd, API server, and kube-controller-manager) emit metrics over the &lt;code>/metrics&lt;/code> endpoint. This includes metrics like how long webhooks take, the request count of the apiserver and storage information, like how many and what kind of objects are stored in etcd.&lt;/p>
&lt;p>&lt;strong>Metrics about the state of Kubernetes objects&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics&lt;/a> is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects. It is not concerned with metrics about the Kubernetes components, but rather it exposes metrics calculated from the status of Kubernetes objects (for example, resource requests or health of pods).&lt;/p>
&lt;p>In the following image a few example metrics, which are exposed by the various components, are listed:
&lt;img src="https://gardener.cloud/__resources/data-flow-monitoring-2_4e4e76.png" alt="">&lt;/p>
&lt;p>We only store metrics for Gardener deployed components. Those include the Kubernetes control plane, Gardener managed system components (e.g., pods) in the kube-system namespace of the shoot cluster or systemd units on the nodes. We do not gather metrics for workload deployed in the shoot cluster. This is also shown in the picture below.&lt;/p>
&lt;p>This means that for any workload you deploy into your shoot cluster, you need to deploy monitoring and logging yourself.&lt;/p>
&lt;p>Logs or metrics are kept up to 14 days or when a configured space limit is reached.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/data-flow-monitoring-3_38a9ab.png" alt="">&lt;/p></description></item><item><title>Docs: Hibernation</title><link>https://gardener.cloud/docs/getting-started/features/hibernation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/hibernation/</guid><description>
&lt;h2 id="hibernation">Hibernation&lt;/h2>
&lt;p>Some clusters need to be up all the time - typically, they would be hosting some kind of production workload. Others might be used for development purposes or testing during business hours only. Keeping them up and running all the time is a waste of money. Gardener can help you here with its &amp;ldquo;hibernation&amp;rdquo; feature. Essentially, hibernation means to shut down all components of a cluster.&lt;/p>
&lt;h2 id="how-hibernation-works">How Hibernation Works&lt;/h2>
&lt;p>The hibernation flow for a shoot attempts to reduce the resources consumed as much as possible. Hence everything not state-related is being decommissioned.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/hibernation_21d43d.gif" alt="">&lt;/p>
&lt;h3 id="data-plane">Data Plane&lt;/h3>
&lt;p>All nodes will be drained and the VMs will be deleted. As a result, all pods will be &amp;ldquo;stuck&amp;rdquo; in a &lt;code>Pending&lt;/code> state since no new nodes are added. Of course, PVC / PV holding data is not deleted.&lt;/p>
&lt;p>Services of type &lt;code>LoadBalancer&lt;/code> will keep their external IP addresses.&lt;/p>
&lt;h3 id="control-plane">Control Plane&lt;/h3>
&lt;p>All components will be scaled down and no pods will remain running. ETCD data is kept safe on the disk.&lt;/p>
&lt;p>The DNS records routing traffic for the API server are also destroyed. Trying to connect to a hibernated cluster via kubectl will result in a DNS lookup failure / no-such-host message.&lt;/p>
&lt;p>When waking up a cluster, all control plane components will be scaled up again and the DNS records will be re-created. Nodes will be created again and pods scheduled to run on them.&lt;/p>
&lt;h2 id="how-to-configure--trigger-hibernation">How to Configure / Trigger Hibernation&lt;/h2>
&lt;p>The easiest way to configure hibernation schedules is via the dashboard. Of course, this is reflected in the shoot&amp;rsquo;s spec and can also be maintained there. Before a cluster is hibernated, constraints in the shoot&amp;rsquo;s status will be evaluated. There might be conditions (mostly revolving around mutating / validating webhooks) that would block a successful wake-up. In such a case, the constraint will block hibernation in the first place.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/trigger-hibernation_eef81a.png" alt="">&lt;/p>
&lt;p>To wake-up or hibernate a shoot immediately, the dashboard can be used or a patch to the shoot&amp;rsquo;s spec can be applied directly.&lt;/p></description></item><item><title>Docs: Tutorials</title><link>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/tutorials/kubernetes-cluster-on-aws-with-gardener/kubernetes-cluster-on-aws-with-gardener/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/tutorials/kubernetes-cluster-on-aws-with-gardener/kubernetes-cluster-on-aws-with-gardener/</guid><description>
&lt;h3 id="overview">Overview&lt;/h3>
&lt;p>Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on AWS.&lt;/p>
&lt;h3 id="prerequisites">Prerequisites&lt;/h3>
&lt;ul>
&lt;li>You have created an &lt;a href="https://aws.amazon.com/">AWS account&lt;/a>.&lt;/li>
&lt;li>You have access to the Gardener dashboard and have permissions to create projects.&lt;/li>
&lt;/ul>
&lt;h3 id="steps">Steps&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Go to the Gardener dashboard and create a &lt;em>Project&lt;/em>.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/new-gardener-project_ad03bc.png">
&lt;/li>
&lt;li>
&lt;p>Choose &lt;em>Secrets&lt;/em>, then the plus icon &lt;img src="https://gardener.cloud/__resources/plus-icon_3b1f20.png"> and select &lt;em>AWS&lt;/em>.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/create-secret-aws_79dc1a.png">
&lt;/li>
&lt;li>
&lt;p>To copy the policy for AWS from the Gardener dashboard, click on the help icon &lt;img src="https://gardener.cloud/__resources/help-icon_01486c.png"> for AWS secrets, and choose copy &lt;img src="https://gardener.cloud/__resources/copy-icon_0f5ab8.png">.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/gardener-copy-policy_a52965.png">
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://console.aws.amazon.com/iam/home?#/policies">Create a new policy&lt;/a> in AWS:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Choose &lt;em>Create policy&lt;/em>.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/amazon-create-policy_5ef114.png">
&lt;/li>
&lt;li>
&lt;p>Paste the policy that you copied from the Gardener dashboard to this custom policy.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/amazon-create-policy-json_7d6327.png">
&lt;/li>
&lt;li>
&lt;p>Choose &lt;em>Next&lt;/em> until you reach the Review section.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Fill in the name and description, then choose &lt;em>Create policy&lt;/em>.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/amazon-review-policy_6fba71.png">
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://console.aws.amazon.com/iam/home?#/users$new?step=details">Create a new technical user&lt;/a> in AWS:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Type in a username and select the access key credential type.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/add-user_775731.png">
&lt;/li>
&lt;li>
&lt;p>Choose &lt;em>Attach an existing policy&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Select &lt;em>GardenerAccess&lt;/em> from the policy list.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Choose &lt;em>Next&lt;/em> until you reach the Review section.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;img src="https://gardener.cloud/__resources/attach-policy_a6a81f.png">
&lt;img src="https://gardener.cloud/__resources/finish-user_a9e956.png">
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Note: After the user is created, &lt;code>Access key ID&lt;/code> and &lt;code>Secret access key&lt;/code> are generated and displayed. Remember to save them. The &lt;code>Access key ID&lt;/code> is used later to create secrets for Gardener.
&lt;/div>
&lt;img src="https://gardener.cloud/__resources/save-keys_f23816.png">
&lt;/li>
&lt;li>
&lt;p>On the Gardener dashboard, choose &lt;em>Secrets&lt;/em> and then the plus sign &lt;img src="https://gardener.cloud/__resources/plus-icon_3b1f20.png">. Select &lt;em>AWS&lt;/em> from the drop down menu to add a new AWS secret.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create your secret.&lt;/p>
&lt;ol>
&lt;li>Type the name of your secret.&lt;/li>
&lt;li>Copy and paste the &lt;code>Access Key ID&lt;/code> and &lt;code>Secret Access Key&lt;/code> you saved when you created the technical user on AWS.&lt;/li>
&lt;li>Choose &lt;em>Add secret&lt;/em>.
&lt;img src="https://gardener.cloud/__resources/add-aws-secret_ed47ad.png">&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>After completing these steps, you should see your newly created secret in the &lt;em>Infrastructure Secrets&lt;/em> section.&lt;/p>
&lt;/blockquote>
&lt;img src="https://gardener.cloud/__resources/secret-stored_a4c7f9.png">
&lt;/li>
&lt;li>
&lt;p>To create a new cluster, choose &lt;em>Clusters&lt;/em> and then the plus sign in the upper right corner.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/new-cluster_353d7b.png">
&lt;/li>
&lt;li>
&lt;p>In the &lt;em>Create Cluster&lt;/em> section:&lt;/p>
&lt;ol>
&lt;li>Select &lt;em>AWS&lt;/em> in the &lt;em>Infrastructure&lt;/em> tab.&lt;/li>
&lt;li>Type the name of your cluster in the &lt;em>Cluster Details&lt;/em> tab.&lt;/li>
&lt;li>Choose the secret you created before in the &lt;em>Infrastructure Details&lt;/em> tab.&lt;/li>
&lt;li>Choose &lt;em>Create&lt;/em>.&lt;/li>
&lt;/ol>
&lt;img src="https://gardener.cloud/__resources/create-cluster_7a45a2.png">
&lt;/li>
&lt;li>
&lt;p>Wait for your cluster to get created.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/processing-cluster_522005.png">
&lt;/li>
&lt;/ol>
&lt;h3 id="result">Result&lt;/h3>
&lt;p>After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/copy-kubeconfig_752d59.png"></description></item><item><title>Docs: Alerts</title><link>https://gardener.cloud/docs/getting-started/observability/alerts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/observability/alerts/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>In this overview, we want to present two ways to receive alerts for control plane and Gardener managed system-components:&lt;/p>
&lt;ul>
&lt;li>Predefined Gardener alerts&lt;/li>
&lt;li>Custom alerts&lt;/li>
&lt;/ul>
&lt;h3 id="predefined-control-plane-alerts">Predefined Control Plane Alerts&lt;/h3>
&lt;p>In the shoot spec it is possible to configure &lt;code>emailReceivers&lt;/code>. On this email address you will automatically receive email notifications for 16 predefined alerts of your control plane.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/predefined-alert_a01383.png" alt="">&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/monitoring/alerting/">Alerting&lt;/a>.&lt;/p>
&lt;h3 id="custom-alerts---federation">Custom Alerts - Federation&lt;/h3>
&lt;p>If you need more customization for alerts for control plane metrics, you have the option to deploy your own Prometheus into your shoot control plane.&lt;/p>
&lt;p>Then you can use federation, which is a Prometheus feature, to forward the metrics from the Gardener managed Prometheus to your custom deployed Prometheus. Since as a shoot owner you do not have access to the control plane pods, this is the only way to get those metrics.&lt;/p>
&lt;p>The credentials and endpoint for the Gardener managed Prometheus are exposed over the Gardener dashboard or programmatically in the garden project as a secret (&lt;code>&amp;lt;shoot-name&amp;gt;.monitoring&lt;/code>).&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/custom-alerts_653ef2.png" alt="">&lt;/p></description></item><item><title>Docs: Workerless Shoots</title><link>https://gardener.cloud/docs/getting-started/features/workerless-shoots/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/workerless-shoots/</guid><description>
&lt;h2 id="controlplane-as-a-service">Controlplane as a Service&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/workerless-shoots_0af834.png" alt="">&lt;/p>
&lt;p>Sometimes, there may be use cases for Kubernetes clusters that don&amp;rsquo;t require pods but only features of the control plane. Gardener can create the so-called &amp;ldquo;workerless&amp;rdquo; shoots, which are exactly that. A Kubernetes cluster without nodes (and without any controller related to them).&lt;/p>
&lt;p>In a scenario where you already have multiple clusters, you can use it for orchestration (leases) or factor out components that require many CRDs.&lt;/p>
&lt;p>As part of the control plane, the following components are deployed in the seed cluster for workerless shoot:&lt;/p>
&lt;ul>
&lt;li>etcds&lt;/li>
&lt;li>kube-apiserver&lt;/li>
&lt;li>kube-controller-manager&lt;/li>
&lt;li>gardener-resource-manager&lt;/li>
&lt;li>Logging and monitoring components&lt;/li>
&lt;li>Extension components (to find out if they support workerless shoots, see the &lt;a href="https://gardener.cloud/docs/gardener/extensions/extension/#what-is-required-to-register-and-support-an-extension-type">Extensions&lt;/a> documentation)&lt;/li>
&lt;/ul></description></item><item><title>Docs: Credential Rotation</title><link>https://gardener.cloud/docs/getting-started/features/credential-rotation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/credential-rotation/</guid><description>
&lt;h2 id="keys">Keys&lt;/h2>
&lt;p>There are plenty of keys in Gardener. The ETCD needs one to store resources like secrets encrypted at rest. Gardener generates certificate authorities (CAs) to ensure secured communication between the various components and actors and service account tokens are signed with a dedicated key. There is also an SSH key pair to allow debugging of nodes and the observability stack has its own passwords too.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/keys_ef3249.png" alt="">&lt;/p>
&lt;p>All of these keys share a common property: they are managed by Gardener. Rotating them, however, is potentially very disruptive. Hence, Gardener does not do it automatically, but offers you means to perform these tasks easily. For a single cluster, you may conveniently use the dashboard. Of course, it is also possible to do the same by annotating the shoot resource accordingly:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-credentials-start
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-credentials-complete​
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Where possible, the rotation happens in two phases. Phase 1 introduces new keys while the old ones are still valid. Users can safely exchange keys / CA bundles wherever they are used. Afterwards, phase 2 will invalidate the old keys / CA bundles.&lt;/p>
&lt;h2 id="rotation-phases">Rotation Phases&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/rotation-phases_317f9b.png" alt="">&lt;/p>
&lt;p>At the beginning, only the old set of credentials exists. By triggering the rotation, new credentials are created in phase 1 and both sets are valid. Now, all clients have to update and start using the new credentials. Only afterwards it is safe to trigger phase 2, which invalidates the old credentials.&lt;/p>
&lt;p>The shoot&amp;rsquo;s status will always show the current status / phase of the rotation.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/shoot_credentials_rotation/">Credentials Rotation for Shoot Clusters&lt;/a>.&lt;/p>
&lt;h2 id="user-provided-credentials">User-Provided Credentials&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/user-provided-keys_976909.png" alt="">&lt;/p>
&lt;p>You grant Gardener permissions to create resources by handing over cloud provider keys. These keys are stored in a secret and referenced to a shoot via a SecretBinding. Gardener uses the keys to create the network for the cluster resources, routes, VMs, disks, and IP addresses.&lt;/p>
&lt;p>When you rotate credentials, the new keys have to be stored in the same secret and the shoot needs to reconcile successfully to ensure the replication to every controller. Afterwards, the old keys can be deleted safely from Gardener&amp;rsquo;s perspective.&lt;/p>
&lt;p>While the reconciliation can be triggered manually, there is no need for it (if you&amp;rsquo;re not in a hurry). Each shoot reconciles once within 24h and the new keys will be picked up during the next maintenance window.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
It is not possible to move a shoot to a different infrastructure account (at all!).
&lt;/div></description></item><item><title>Docs: Shoot Status</title><link>https://gardener.cloud/docs/getting-started/observability/shoot-status/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/observability/shoot-status/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>In this topic you can see various shoot statuses and how you can use them to monitor your shoot cluster.&lt;/p>
&lt;h2 id="shoot-status---conditions">Shoot Status - Conditions&lt;/h2>
&lt;p>You can retrieve the shoot status by using &lt;code>kubectl get shoot -oyaml&lt;/code>&lt;/p>
&lt;p>It contains conditions, which give you information about the healthiness of your cluster. Those conditions are also forwarded to the Gardener dashboard and show your cluster as healthy or unhealthy.&lt;/p>
&lt;img style="width: 60%; height: auto; margin: 0, auto" src="https://gardener.cloud/__resources/shoot-status-1_81dd83.png"/>
&lt;h2 id="shoot-status---constraints">Shoot Status - Constraints&lt;/h2>
&lt;p>The shoot status also contains constraints. If these constraints are met, your cluster operations are impaired and the cluster is likely to fail at some point. Please watch them and act accordingly.&lt;/p>
&lt;img style="width: 60%; height: auto; margin: 0, auto" src="https://gardener.cloud/__resources/shoot-status-2_afb3ee.png"/>
&lt;h2 id="shoot-status---last-operation">Shoot Status - Last Operation&lt;/h2>
&lt;p>The &lt;code>lastOperation&lt;/code>, &lt;code>lastErrors&lt;/code>, and &lt;code>lastMaintenance&lt;/code> give you information on what was last happening in your clusters. This is especially useful when you are facing an error.&lt;/p>
&lt;p>In this example, nodes are being recreated and not all machines have reached the desired state yet.&lt;/p>
&lt;img style="width: 60%; height: auto; margin: 0, auto" src="https://gardener.cloud/__resources/shoot-status-3_61cd5c.png"/>
&lt;h2 id="shoot-status---credentials-rotation">Shoot Status - Credentials Rotation&lt;/h2>
&lt;p>You can also see the status of the last credentials rotation. Here you can also programmatically derive when the last rotation was down in order to trigger the next rotation.&lt;/p>
&lt;img style="width: 60%; height: auto; margin: 0, auto" src="https://gardener.cloud/__resources/shoot-status-4_5f74e6.png"/></description></item><item><title>Docs: External DNS Management</title><link>https://gardener.cloud/docs/getting-started/features/dns-management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/dns-management/</guid><description>
&lt;h2 id="external-dns-management">External DNS Management&lt;/h2>
&lt;p>When you deploy to Kubernetes, there is no native management of external DNS. Instead, the cloud-controller-manager requests (mostly IPv4) addresses for every service of type LoadBalancer. Of course, the Ingress resource helps here, but how is the external DNS entry for the ingress controller managed?&lt;/p>
&lt;p>Essentially, some sort of automation for DNS management is missing.&lt;/p>
&lt;h2 id="automating-dns-management">Automating DNS Management&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/automate-dns-management_f9812b.png" alt="">&lt;/p>
&lt;p>From a user&amp;rsquo;s perspective, it is desirable to work with already known resources and concepts. Hence, the DNS management offered by Gardener plugs seamlessly into Kubernetes resources and you do not need to &amp;ldquo;leave&amp;rdquo; the context of the shoot cluster.&lt;/p>
&lt;p>To request a DNS record creation / update, a Service or Ingress resource is annotated accordingly. The shoot-dns-service extension will (if configured) will pick up the request and create a DNSEntry resource + reconcile it to have an actual DNS record created at a configured DNS provider. Gardener supports the following providers:&lt;/p>
&lt;ul>
&lt;li>aws-route53&lt;/li>
&lt;li>azure-dns&lt;/li>
&lt;li>azure-private-dns&lt;/li>
&lt;li>google-clouddns&lt;/li>
&lt;li>openstack-designate&lt;/li>
&lt;li>alicloud-dns&lt;/li>
&lt;li>cloudflare-dns&lt;/li>
&lt;/ul>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/guides/networking/dns-extension/">DNS Names&lt;/a>.&lt;/p>
&lt;h2 id="dns-provider">DNS Provider&lt;/h2>
&lt;p>For the above to work, we need some ingredients. Primarily, this is implemented via a so-called DNSProvider. Every shoot has a default provider that is used to set up the API server&amp;rsquo;s public DNS record. It can be used to request sub-domains as well.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/dns-provider_2d18ba.png" alt="">&lt;/p>
&lt;p>In addition, a shoot can reference credentials to a DNS provider. Those can be used to manage custom domains.&lt;/p>
&lt;p>Please have a look at the &lt;a href="https://gardener.cloud/docs/guides/networking/dns-extension/">documentation&lt;/a> for further details.&lt;/p></description></item><item><title>Docs: Certificate Management</title><link>https://gardener.cloud/docs/getting-started/features/certificate-management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/certificate-management/</guid><description>
&lt;h2 id="certificate-management">Certificate Management&lt;/h2>
&lt;p>For proper consumption, any service should present a TLS certificate to its consumers. However, self-signed certificates are not fit for this purpose - the certificate should be signed by a CA trusted by an application&amp;rsquo;s userbase. Luckily, Issuers like Let&amp;rsquo;s Encrypt and others help here by offering a signing service that issues certificates based on the ACME challenge (Automatic Certificate Management Environment).&lt;/p>
&lt;p>There are plenty of tools you can use to perform the challenge. For Kubernetes, cert-manager certainly is the most common, however its configuration is rather cumbersome and error prone. So let&amp;rsquo;s see how a Gardener extension can help here.&lt;/p>
&lt;h2 id="manage-certificates-with-gardener">Manage Certificates with Gardener&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/manage-certificates_b8392b.png" alt="">&lt;/p>
&lt;p>You may annotate a Service or Ingress resource to trigger the cert-manager to request a certificate from the any configured issuer (e.g. Let&amp;rsquo;s Encrypt) and perform the challenge. A Gardener operator can add a default issuer for convenience.
With the DNS extension discussed previously, setting up the DNS TXT record for the ACME challenge is fairly easy. The requested certificate can be customized by the means of several other annotations known to the controller. Most notably, it is possible to specify SANs via &lt;code>cert.gardener.cloud/dnsnames&lt;/code> to accommodate domain names that have more than 64 characters (the limit for the CN field).&lt;/p>
&lt;p>The user&amp;rsquo;s request for a certificate manifests as a &lt;code>certificate&lt;/code> resource. The status, issuer, and other properties can be checked there.&lt;/p>
&lt;p>Once successful, the resulting certificate will be stored in a secret and is ready for usage.&lt;/p>
&lt;p>With additional configuration, it is also possible to define custom issuers of certificates.&lt;/p>
&lt;p>For more information, see the &lt;a href="https://gardener.cloud/docs/guides/networking/certificate-extension/">Manage certificates with Gardener for public domain&lt;/a> topic and the &lt;a href="https://github.com/gardener/cert-management#follow-cname">cert-management repository&lt;/a>.&lt;/p></description></item><item><title>Docs: FAQ</title><link>https://gardener.cloud/docs/other-components/machine-controller-manager/faq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/machine-controller-manager/faq/</guid><description>
&lt;h1 id="frequently-asked-questions">Frequently Asked Questions&lt;/h1>
&lt;p>The answers in this FAQ apply to the newest (HEAD) version of Machine Controller Manager. If
you&amp;rsquo;re using an older version of MCM please refer to corresponding version of
this document. Few of the answers assume that the MCM being used is in conjuction with &lt;a href="https://github.com/gardener/autoscaler">cluster-autoscaler&lt;/a>:&lt;/p>
&lt;h1 id="table-of-contents">Table of Contents:&lt;/h1>
&lt;!--- TOC BEGIN -->
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#basics">Basics&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#what-is-machine-controller-manager">What is Machine Controller Manager?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#Why-is-my-machine-deleted">Why is my machine deleted?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#What-are-the-different-sub-controllers-in-MCM">What are the different sub-controllers in MCM?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#What-is-safety-controller-in-MCM">What is Safety Controller in MCM?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-to">How to?&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-to-install-MCM-in-a-kubernetes-cluster">How to install MCM in a Kubernetes cluster?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-to-better-control-the-rollout-process-of-the-worker-nodes">How to better control the rollout process of the worker nodes?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-to-scale-down-machinedeployment-by-selective-deletion-of-machines">How to scale down MachineDeployment by selective deletion of machines?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-to-force-delete-a-machine">How to force delete a machine?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-to-pause-the-ongoing-rolling-update-of-the-machinedeployment">How to pause the ongoing rolling-update of the machinedeployment?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-to-delete-machine-object-immedietly-if-i-dont-have-access-to-it">How to delete machine object immedietly if I don&amp;rsquo;t have access to it?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-to-avoid-garbage-collection-of-your-node">How to avoid garbage collection of your node?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-to-trigger-rolling-update-of-a-machinedeployment">How to trigger rolling update of a machinedeployment?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#internals">Internals&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#What-is-the-high-level-design-of-MCM">What is the high level design of MCM?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#What-are-the-different-configuration-options-in-MCM">What are the different configuration options in MCM?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#What-are-the-different-timeoutsconfigurations-in-a-machines-lifecycle">What are the different timeouts/configurations in a machine&amp;rsquo;s lifecycle?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-is-the-drain-of-a-machine-implemented">How is the drain of a machine implemented?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-are-the-stateful-applications-drained-during-machine-deletion">How are the stateful applications drained during machine deletion?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-does-maxEvictRetries-configuration-work-with-drainTimeout-configuration">How does maxEvictRetries configuration work with drainTimeout configuration?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#What-are-the-different-phases-of-a-machine">What are the different phases of a machine?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#what-health-checks-are-performed-on-a-machine">What health checks are performed on a machine?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection">How does rate limiting replacement of machine work in MCM ? How is it related to meltdown protection?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-mcm-responds-when-scale-outscale-in-is-done-during-rolling-update-of-a-machinedeployment">How MCM responds when scale-out/scale-in is done during rolling update of a machinedeployment?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-some-unhealthy-machines-are-drained-quickly-">How some unhealthy machines are drained quickly?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-does-mcm-prioritize-the-machines-for-deletion-on-scale-down-of-machinedeployment">How does MCM prioritize the machines for deletion on scale-down of machinedeployment?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#troubleshooting">Troubleshooting&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#My-machine-is-stuck-in-deletion-for-1-hr-why">My machine is stuck in deletion for 1 hr, why?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#My-machine-is-not-joining-the-cluster-why">My machine is not joining the cluster, why?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#developer">Developer&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-should-I-test-my-code-before-submitting-a-PR">How should I test my code before submitting a PR?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#I-need-to-change-the-APIs-what-are-the-recommended-steps">I need to change the APIs, what are the recommended steps?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-can-I-update-the-dependencies-of-MCM">How can I update the dependencies of MCM?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#in-the-context-of-gardener">In the context of Gardener&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-can-I-configure-MCM-using-Shoot-resource">How can I configure MCM using Shoot resource?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-is-my-worker-pool-spread-across-zones">How is my worker-pool spread across zones?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--- TOC END -->
&lt;h1 id="basics">Basics&lt;/h1>
&lt;h3 id="what-is-machine-controller-manager">What is Machine Controller Manager?&lt;/h3>
&lt;p>Machine Controller Manager aka MCM is a bunch of controllers used for the lifecycle management of the worker machines. It reconciles a set of CRDs such as &lt;code>Machine&lt;/code>, &lt;code>MachineSet&lt;/code>, &lt;code>MachineDeployment&lt;/code> which depicts the functionality of &lt;code>Pod&lt;/code>, &lt;code>Replicaset&lt;/code>, &lt;code>Deployment&lt;/code> of the core Kubernetes respectively. Read more about it at &lt;a href="https://github.com/gardener/machine-controller-manager/tree/master/docs">README&lt;/a>.&lt;/p>
&lt;ul>
&lt;li>Gardener uses MCM to manage its Kubernetes nodes of the shoot cluster. However, by design, MCM can be used independent of Gardener.&lt;/li>
&lt;/ul>
&lt;h3 id="why-is-my-machine-deleted">Why is my machine deleted?&lt;/h3>
&lt;p>A machine is deleted by MCM generally for 2 reasons-&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Machine is unhealthy for at least &lt;code>MachineHealthTimeout&lt;/code> period. The default &lt;code>MachineHealthTimeout&lt;/code> is 10 minutes.&lt;/p>
&lt;ul>
&lt;li>By default, a machine is considered unhealthy if any of the following node conditions - &lt;code>DiskPressure&lt;/code>, &lt;code>KernelDeadlock&lt;/code>, &lt;code>FileSystem&lt;/code>, &lt;code>Readonly&lt;/code> is set to &lt;code>true&lt;/code>, or &lt;code>KubeletReady&lt;/code> is set to &lt;code>false&lt;/code>. However, this is something that is configurable using the following &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/deployment/out-of-tree/deployment.yaml#L30">flag&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Machine is scaled down by the &lt;code>MachineDeployment&lt;/code> resource.&lt;/p>
&lt;ul>
&lt;li>This is very usual when an external controller cluster-autoscaler (aka CA) is used with MCM. CA deletes the under-utilized machines by scaling down the &lt;code>MachineDeployment&lt;/code>. Read more about cluster-autoscaler&amp;rsquo;s scale down behavior &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#how-does-scale-down-work">here&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="what-are-the-different-sub-controllers-in-mcm">What are the different sub-controllers in MCM?&lt;/h3>
&lt;p>MCM mainly contains the following sub-controllers:&lt;/p>
&lt;ul>
&lt;li>&lt;code>MachineDeployment Controller&lt;/code>: Responsible for reconciling the &lt;code>MachineDeployment&lt;/code> objects. It manages the lifecycle of the &lt;code>MachineSet&lt;/code> objects.&lt;/li>
&lt;li>&lt;code>MachineSet Controller&lt;/code>: Responsible for reconciling the &lt;code>MachineSet&lt;/code> objects. It manages the lifecycle of the &lt;code>Machine&lt;/code> objects.&lt;/li>
&lt;li>&lt;code>Machine Controller&lt;/code>: responsible for reconciling the &lt;code>Machine&lt;/code> objects. It manages the lifecycle of the actual VMs/machines created in cloud/on-prem. This controller has been moved out of tree. Please refer an AWS machine controller for more info - &lt;a href="https://github.com/gardener/machine-controller-manager-provider-gcp">link&lt;/a>.&lt;/li>
&lt;li>Safety-controller: Responsible for handling the unidentified/unknown behaviors from the cloud providers. Please read more about its functionality &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#what-is-safety-controller">below&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="what-is-safety-controller-in-mcm">What is Safety Controller in MCM?&lt;/h3>
&lt;p>&lt;code>Safety Controller&lt;/code> contains following functions:&lt;/p>
&lt;ul>
&lt;li>Orphan VM handler:
&lt;ul>
&lt;li>It lists all the VMs in the cloud matching the &lt;code>tag&lt;/code> of given cluster name and maps the VMs with the &lt;code>machine&lt;/code> objects using the &lt;code>ProviderID&lt;/code> field. VMs without any backing &lt;code>machine&lt;/code> objects are logged and deleted after confirmation.&lt;/li>
&lt;li>This handler runs every 30 minutes and is configurable via &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L112">machine-safety-orphan-vms-period&lt;/a> flag.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Freeze mechanism:
&lt;ul>
&lt;li>&lt;code>Safety Controller&lt;/code> freezes the &lt;code>MachineDeployment&lt;/code> and &lt;code>MachineSet&lt;/code> controller if the number of &lt;code>machine&lt;/code> objects goes beyond a certain threshold on top of &lt;code>Spec.Replicas&lt;/code>. It can be configured by the flag &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L102-L103">&amp;ndash;safety-up or &amp;ndash;safety-down&lt;/a> and also &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L113">machine-safety-overshooting-period&lt;/a>.&lt;/li>
&lt;li>&lt;code>Safety Controller&lt;/code> freezes the functionality of the MCM if either of the &lt;code>target-apiserver&lt;/code> or the &lt;code>control-apiserver&lt;/code> is not reachable.&lt;/li>
&lt;li>&lt;code>Safety Controller&lt;/code> unfreezes the MCM automatically once situation is resolved to normal. A &lt;code>freeze&lt;/code> label is applied on &lt;code>MachineDeployment&lt;/code>/&lt;code>MachineSet&lt;/code> to enforce the freeze condition.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="how-to">How to?&lt;/h1>
&lt;h3 id="how-to-install-mcm-in-a-kubernetes-cluster">How to install MCM in a Kubernetes cluster?&lt;/h3>
&lt;p>MCM can be installed in a cluster with following steps:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Apply all the CRDs from &lt;a href="https://github.com/gardener/machine-controller-manager/tree/master/kubernetes/crds">here&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Apply all the deployment, role-related objects from &lt;a href="https://github.com/gardener/machine-controller-manager/tree/master/kubernetes/deployment/out-of-tree">here&lt;/a>.&lt;/p>
&lt;ul>
&lt;li>Control cluster is the one where the &lt;code>machine-*&lt;/code> objects are stored. Target cluster is where all the node objects are registered.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="how-to-better-control-the-rollout-process-of-the-worker-nodes">How to better control the rollout process of the worker nodes?&lt;/h3>
&lt;p>MCM allows configuring the rollout of the worker machines using &lt;code>maxSurge&lt;/code> and &lt;code>maxUnavailable&lt;/code> fields. These fields are applicable only during the rollout process and means nothing in general scale up/down scenarios.
The overall process is very similar to how the &lt;code>Deployment Controller&lt;/code> manages pods during &lt;code>RollingUpdate&lt;/code>.&lt;/p>
&lt;ul>
&lt;li>&lt;code>maxSurge&lt;/code> refers to the number of additional machines that can be added on top of the &lt;code>Spec.Replicas&lt;/code> of MachineDeployment &lt;em>during rollout process&lt;/em>.&lt;/li>
&lt;li>&lt;code>maxUnavailable&lt;/code> refers to the number of machines that can be deleted from &lt;code>Spec.Replicas&lt;/code> field of the MachineDeployment &lt;em>during rollout process&lt;/em>.&lt;/li>
&lt;/ul>
&lt;h3 id="how-to-scale-down-machinedeployment-by-selective-deletion-of-machines">How to scale down MachineDeployment by selective deletion of machines?&lt;/h3>
&lt;p>During scale down, triggered via &lt;code>MachineDeployment&lt;/code>/&lt;code>MachineSet&lt;/code>, MCM prefers to delete the &lt;code>machine/s&lt;/code> which have the least priority set.
Each &lt;code>machine&lt;/code> object has an annotation &lt;code>machinepriority.machine.sapcloud.io&lt;/code> set to &lt;code>3&lt;/code> by default. Admin can reduce the priority of the given machines by changing the annotation value to &lt;code>1&lt;/code>. The next scale down by &lt;code>MachineDeployment&lt;/code> shall delete the machines with the least priority first.&lt;/p>
&lt;h3 id="how-to-force-delete-a-machine">How to force delete a machine?&lt;/h3>
&lt;p>A machine can be force deleted by adding the label &lt;code>force-deletion: &amp;quot;True&amp;quot;&lt;/code> on the &lt;code>machine&lt;/code> object before executing the actual delete command. During force deletion, MCM skips the drain function and simply triggers the deletion of the machine. This label should be used with caution as it can violate the PDBs for pods running on the machine.&lt;/p>
&lt;h3 id="how-to-pause-the-ongoing-rolling-update-of-the-machinedeployment">How to pause the ongoing rolling-update of the machinedeployment?&lt;/h3>
&lt;p>An ongoing rolling-update of the machine-deployment can be paused by using &lt;code>spec.paused&lt;/code> field. See the example below:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: machine.sapcloud.io/v1alpha1
kind: MachineDeployment
metadata:
name: test-machine-deployment
spec:
paused: true
&lt;/code>&lt;/pre>&lt;p>It can be unpaused again by removing the &lt;code>Paused&lt;/code> field from the machine-deployment.&lt;/p>
&lt;h3 id="how-to-delete-machine-object-immedietly-if-i-dont-have-access-to-it">How to delete machine object immedietly if I don&amp;rsquo;t have access to it?&lt;/h3>
&lt;p>If the user doesn&amp;rsquo;t have access to the machine objects (like in case of Gardener clusters) and they would like to replace a node immedietly then they can place the annotation &lt;code>node.machine.sapcloud.io/trigger-deletion-by-mcm: &amp;quot;true&amp;quot;&lt;/code> on their node. This will start the replacement of the machine with a new node.&lt;/p>
&lt;p>On the other hand if the user deletes the node object immedietly then replacement will start only after &lt;code>MachineHealthTimeout&lt;/code>.&lt;/p>
&lt;p>This annotation can also be used if the user wants to expedite the &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection">replacement of unhealthy nodes&lt;/a>&lt;/p>
&lt;p>&lt;code>NOTE&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>node.machine.sapcloud.io/trigger-deletion-by-mcm: &amp;quot;false&amp;quot;&lt;/code> annotation is NOT acted upon by MCM , neither does it mean that MCM will not replace this machine.&lt;/li>
&lt;li>this annotation would delete the desired machine but another machine would be created to maintain &lt;code>desired replicas&lt;/code> specified for the machineDeployment/machineSet. Currently if the user doesn&amp;rsquo;t have access to machineDeployment/machineSet then they cannot remove a machine without replacement.&lt;/li>
&lt;/ul>
&lt;h3 id="how-to-avoid-garbage-collection-of-your-node">How to avoid garbage collection of your node?&lt;/h3>
&lt;p>MCM provides an in-built safety mechanism to garbage collect VMs which have no corresponding machine object. This is done to save costs and is one of the key features of MCM.
However, sometimes users might like to add nodes directly to the cluster without the help of MCM and would prefer MCM to not garbage collect such VMs.
To do so they should remove/not-use tags on their VMs containing the following strings:&lt;/p>
&lt;ol>
&lt;li>&lt;code>kubernetes.io/cluster/&lt;/code>&lt;/li>
&lt;li>&lt;code>kubernetes.io/role/&lt;/code>&lt;/li>
&lt;li>&lt;code>kubernetes-io-cluster-&lt;/code>&lt;/li>
&lt;li>&lt;code>kubernetes-io-role-&lt;/code>&lt;/li>
&lt;/ol>
&lt;h3 id="how-to-trigger-rolling-update-of-a-machinedeployment">How to trigger rolling update of a machinedeployment?&lt;/h3>
&lt;p>Rolling update can be triggered for a machineDeployment by updating one of the following:&lt;/p>
&lt;ul>
&lt;li>&lt;code>.spec.template.annotations&lt;/code>&lt;/li>
&lt;li>&lt;code>.spec.template.spec.class.name&lt;/code>&lt;/li>
&lt;/ul>
&lt;h1 id="internals">Internals&lt;/h1>
&lt;h3 id="what-is-the-high-level-design-of-mcm">What is the high level design of MCM?&lt;/h3>
&lt;p>Please refer the following &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/#design-of-machine-controller-manager">document&lt;/a>.&lt;/p>
&lt;h3 id="what-are-the-different-configuration-options-in-mcm">What are the different configuration options in MCM?&lt;/h3>
&lt;p>MCM allows configuring many knobs to fine-tune its behavior according to the user&amp;rsquo;s need.
Please refer to the &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go">link&lt;/a> to check the exact configuration options.&lt;/p>
&lt;h3 id="what-are-the-different-timeoutsconfigurations-in-a-machines-lifecycle">What are the different timeouts/configurations in a machine&amp;rsquo;s lifecycle?&lt;/h3>
&lt;p>A machine&amp;rsquo;s lifecycle is governed by mainly following timeouts, which can be configured &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/machine_objects/machine-deployment.yaml#L30-L34">here&lt;/a>.&lt;/p>
&lt;ul>
&lt;li>&lt;code>MachineDrainTimeout&lt;/code>: Amount of time after which drain times out and the machine is force deleted. Default ~2 hours.&lt;/li>
&lt;li>&lt;code>MachineHealthTimeout&lt;/code>: Amount of time after which an unhealthy machine is declared &lt;code>Failed&lt;/code> and the machine is replaced by &lt;code>MachineSet&lt;/code> controller.&lt;/li>
&lt;li>&lt;code>MachineCreationTimeout&lt;/code>: Amount of time after which a machine creation is declared &lt;code>Failed&lt;/code> and the machine is replaced by the &lt;code>MachineSet&lt;/code> controller.&lt;/li>
&lt;li>&lt;code>NodeConditions&lt;/code>: List of node conditions which if set to true for &lt;code>MachineHealthTimeout&lt;/code> period, the machine is declared &lt;code>Failed&lt;/code> and replaced by &lt;code>MachineSet&lt;/code> controller.&lt;/li>
&lt;li>&lt;code>MaxEvictRetries&lt;/code>: An integer number depicting the number of times a failed &lt;em>eviction&lt;/em> should be retried on a pod during drain process. A pod is &lt;em>deleted&lt;/em> after &lt;code>max-retries&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h3 id="how-is-the-drain-of-a-machine-implemented">How is the drain of a machine implemented?&lt;/h3>
&lt;p>MCM imports the functionality from the upstream Kubernetes-drain library. Although, few parts have been modified to make it work best in the context of MCM. Drain is executed before machine deletion for graceful migration of the applications.
Drain internally uses the &lt;code>EvictionAPI&lt;/code> to evict the pods and triggers the &lt;code>Deletion&lt;/code> of pods after &lt;code>MachineDrainTimeout&lt;/code>. Please note:&lt;/p>
&lt;ul>
&lt;li>Stateless pods are evicted in parallel.&lt;/li>
&lt;li>Stateful applications (with PVCs) are serially evicted. Please find more info in this &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-are-the-stateful-applications-drained-during-machine-deletion">answer below&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="how-are-the-stateful-applications-drained-during-machine-deletion">How are the stateful applications drained during machine deletion?&lt;/h3>
&lt;p>Drain function serially evicts the stateful-pods. It is observed that serial eviction of stateful pods yields better overall availability of pods as the underlying cloud in most cases detaches and reattaches disks serially anyways.
It is implemented in the following manner:&lt;/p>
&lt;ul>
&lt;li>Drain lists all the pods with attached volumes. It evicts very first stateful-pod and waits for its related entry in Node object&amp;rsquo;s &lt;code>.status.volumesAttached&lt;/code> to be removed by KCM. It does the same for all the stateful-pods.&lt;/li>
&lt;li>It waits for &lt;code>PvDetachTimeout&lt;/code> (default 2 minutes) for a given pod&amp;rsquo;s PVC to be removed, else moves forward.&lt;/li>
&lt;/ul>
&lt;h3 id="how-does-maxevictretries-configuration-work-with-draintimeout-configuration">How does &lt;code>maxEvictRetries&lt;/code> configuration work with &lt;code>drainTimeout&lt;/code> configuration?&lt;/h3>
&lt;p>It is recommended to only set &lt;code>MachineDrainTimeout&lt;/code>. It satisfies the related requirements. &lt;code>MaxEvictRetries&lt;/code> is auto-calculated based on &lt;code>MachineDrainTimeout&lt;/code>, if &lt;code>maxEvictRetries&lt;/code> is not provided. Following will be the overall behavior of both configurations together:&lt;/p>
&lt;ul>
&lt;li>If &lt;code>maxEvictRetries&lt;/code> isn&amp;rsquo;t set and only &lt;code>maxDrainTimeout&lt;/code> is set:
&lt;ul>
&lt;li>MCM auto calculates the &lt;code>maxEvictRetries&lt;/code> based on the &lt;code>drainTimeout&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If &lt;code>drainTimeout&lt;/code> isn&amp;rsquo;t set and only &lt;code>maxEvictRetries&lt;/code> is set:
&lt;ul>
&lt;li>Default &lt;code>drainTimeout&lt;/code> and user provided &lt;code>maxEvictRetries&lt;/code> for each pod is considered.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If both &lt;code>maxEvictRetries&lt;/code> and &lt;code>drainTimoeut&lt;/code> are set:
&lt;ul>
&lt;li>Then both will be respected.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If none are set:
&lt;ul>
&lt;li>Defaults are respected.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="what-are-the-different-phases-of-a-machine">What are the different phases of a machine?&lt;/h3>
&lt;p>A phase of a &lt;code>machine&lt;/code> can be identified with &lt;code>Machine.Status.CurrentStatus.Phase&lt;/code>. Following are the possible phases of a &lt;code>machine&lt;/code> object:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>Pending&lt;/code>: Machine creation call has succeeded. MCM is waiting for machine to join the cluster.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>CrashLoopBackOff&lt;/code>: Machine creation call has failed. MCM will retry the operation after a minor delay.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Running&lt;/code>: Machine creation call has succeeded. Machine has joined the cluster successfully and corresponding node doesn&amp;rsquo;t have &lt;code>node.gardener.cloud/critical-components-not-ready&lt;/code> taint.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Unknown&lt;/code>: Machine &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#what-health-checks-are-performed-on-a-machine">health checks&lt;/a> are failing, eg &lt;code>kubelet&lt;/code> has stopped posting the status.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Failed&lt;/code>: Machine health checks have failed for a prolonged time. Hence it is declared failed by &lt;code>Machine&lt;/code> controller in a &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection">rate limited fashion&lt;/a>. &lt;code>Failed&lt;/code> machines get replaced immediately.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Terminating&lt;/code>: Machine is being terminated. Terminating state is set immediately when the deletion is triggered for the &lt;code>machine&lt;/code> object. It also includes time when it&amp;rsquo;s being drained.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;code>NOTE&lt;/code>: No phase means the machine is being created on the cloud-provider.&lt;/p>
&lt;p>Below is a simple phase transition diagram:
&lt;img src="https://gardener.cloud/__resources/machine_phase_transition_1435f5.png" alt="image">&lt;/p>
&lt;h3 id="what-health-checks-are-performed-on-a-machine">What health checks are performed on a machine?&lt;/h3>
&lt;p>Health check performed on a machine are:&lt;/p>
&lt;ul>
&lt;li>Existense of corresponding node obj&lt;/li>
&lt;li>Status of certain user-configurable node conditions.
&lt;ul>
&lt;li>These conditions can be specified using the flag &lt;code>--node-conditions&lt;/code> for OOT MCM provider or can be specified per machine object.&lt;/li>
&lt;li>The default user configurable node conditions can be found &lt;a href="https://github.com/gardener/machine-controller-manager/blob/91eec24516b8339767db5a40e82698f9fe0daacd/pkg/util/provider/app/options/options.go#L60">here&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>True&lt;/code> status of &lt;code>NodeReady&lt;/code> condition . This condition shows kubelet&amp;rsquo;s status&lt;/li>
&lt;/ul>
&lt;p>If any of the above checks fails , the machine turns to &lt;code>Unknown&lt;/code> phase.&lt;/p>
&lt;h3 id="how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection">How does rate limiting replacement of machine work in MCM? How is it related to meltdown protection?&lt;/h3>
&lt;p>Currently MCM replaces only &lt;code>1&lt;/code> &lt;code>Unkown&lt;/code> machine at a time per machinedeployment. This means until the particular &lt;code>Unknown&lt;/code> machine get terminated and its replacement joins, no other &lt;code>Unknown&lt;/code> machine would be removed.&lt;/p>
&lt;p>The above is achieved by enabling &lt;code>Machine&lt;/code> controller to turn machine from &lt;code>Unknown&lt;/code> -&amp;gt; &lt;code>Failed&lt;/code> only if the above condition is met. &lt;code>MachineSet&lt;/code> controller on the other hand marks &lt;code>Failed&lt;/code> machine as &lt;code>Terminating&lt;/code> immediately.&lt;/p>
&lt;p>One reason for this rate limited replacement was to ensure that in case of network failures , where node&amp;rsquo;s kubelet can&amp;rsquo;t reach out to kube-apiserver , all nodes are not removed together i.e. &lt;code>meltdown protection&lt;/code>.
In gardener context however, &lt;a href="https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/prober/#origin">DWD&lt;/a> is deployed to deal with this scenario, but to stay protected from corner cases , this mechanism has been introduced in MCM.&lt;/p>
&lt;p>&lt;code>NOTE&lt;/code>: Rate limiting replacement is not yet configurable&lt;/p>
&lt;h3 id="how-mcm-responds-when-scale-outscale-in-is-done-during-rolling-update-of-a-machinedeployment">How MCM responds when scale-out/scale-in is done during rolling update of a machinedeployment?&lt;/h3>
&lt;p>&lt;code>Machinedeployment&lt;/code> controller executes the logic of &lt;code>scaling&lt;/code> BEFORE logic of &lt;code>rollout&lt;/code>. It identifies &lt;code>scaling&lt;/code> by comparing the &lt;code>deployment.kubernetes.io/desired-replicas&lt;/code> of each machineset under the machinedeployment with machinedeployment&amp;rsquo;s &lt;code>.spec.replicas&lt;/code>. If the difference is found for any machineSet, a scaling event is detected.&lt;/p>
&lt;p>Case &lt;code>scale-out&lt;/code> -&amp;gt; ONLY New machineSet is scaled out &lt;br>
Case &lt;code>scale-in&lt;/code> -&amp;gt; ALL machineSets(new or old) are scaled in , in proportion to their replica count , any leftover is adjusted in the largest machineSet.&lt;/p>
&lt;p>During update for scaling event, a machineSet is updated if any of the below is true for it:&lt;/p>
&lt;ul>
&lt;li>&lt;code>.spec.Replicas&lt;/code> needs update&lt;/li>
&lt;li>&lt;code>deployment.kubernetes.io/desired-replicas&lt;/code> needs update&lt;/li>
&lt;/ul>
&lt;p>Once scaling is achieved, rollout continues.&lt;/p>
&lt;h3 id="how-does-mcm-prioritize-the-machines-for-deletion-on-scale-down-of-machinedeployment">How does MCM prioritize the machines for deletion on scale-down of machinedeployment?&lt;/h3>
&lt;p>There could be many machines under a machinedeployment with different phases, creationTimestamp. When a scale down is triggered, MCM decides to remove the machine using the following logic:&lt;/p>
&lt;ul>
&lt;li>Machine with least value of &lt;code>machinepriority.machine.sapcloud.io&lt;/code> annotation is picked up.&lt;/li>
&lt;li>If all machines have equal priorities, then following precedence is followed:
&lt;ul>
&lt;li>Terminating &amp;gt; Failed &amp;gt; CrashloopBackoff &amp;gt; Unknown &amp;gt; Pending &amp;gt; Available &amp;gt; Running&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If still there is no match, the machine with oldest creation time (.i.e. creationTimestamp) is picked up.&lt;/li>
&lt;/ul>
&lt;h2 id="how-some-unhealthy-machines-are-drained-quickly-">How some unhealthy machines are drained quickly ?&lt;/h2>
&lt;p>If a node is unhealthy for more than the &lt;code>machine-health-timeout&lt;/code> specified for the &lt;code>machine-controller&lt;/code>, the controller
health-check moves the machine phase to &lt;code>Failed&lt;/code>. By default, the &lt;code>machine-health-timeout&lt;/code> is 10` minutes.&lt;/p>
&lt;p>&lt;code>Failed&lt;/code> machines have their deletion timestamp set and the machine then moves to the &lt;code>Terminating&lt;/code> phase. The node
drain process is initiated. The drain process is invoked either &lt;em>gracefully&lt;/em> or &lt;em>forcefully&lt;/em>.&lt;/p>
&lt;p>The usual drain process is graceful. Pods are evicted from the node and the drain process waits until any existing
attached volumes are mounted on new node. However, if the node &lt;code>Ready&lt;/code> is &lt;code>False&lt;/code> or the &lt;code>ReadonlyFilesystem&lt;/code> is &lt;code>True&lt;/code>
for greater than &lt;code>5&lt;/code> minutes (non-configurable), then a forceful drain is initiated. In a forceful drain, pods are deleted
and &lt;code>VolumeAttachment&lt;/code> objects associated with the old node are also marked for deletion. This is followed by the deletion of the
cloud provider VM associated with the &lt;code>Machine&lt;/code> and then finally ending with the &lt;code>Node&lt;/code> object deletion.&lt;/p>
&lt;p>During the deletion of the VM we only delete the local data disks and boot disks associated with the VM. The disks associated
with persistent volumes are left un-touched as their attach/de-detach, mount/unmount processes are handled by k8s
attach-detach controller in conjunction with the CSI driver.&lt;/p>
&lt;h1 id="troubleshooting">Troubleshooting&lt;/h1>
&lt;h3 id="my-machine-is-stuck-in-deletion-for-1-hr-why">My machine is stuck in deletion for 1 hr, why?&lt;/h3>
&lt;p>In most cases, the &lt;code>Machine.Status.LastOperation&lt;/code> provides information around why a machine can&amp;rsquo;t be deleted.
Though following could be the reasons but not limited to:&lt;/p>
&lt;ul>
&lt;li>Pod/s with misconfigured PDBs block the drain operation. PDBs with &lt;code>maxUnavailable&lt;/code> set to 0, doesn&amp;rsquo;t allow the eviction of the pods. Hence, drain/eviction is retried till &lt;code>MachineDrainTimeout&lt;/code>. Default &lt;code>MachineDrainTimeout&lt;/code> could be as large as ~2hours. Hence, blocking the machine deletion.
&lt;ul>
&lt;li>Short term: User can manually delete the pod in the question, &lt;em>with caution&lt;/em>.&lt;/li>
&lt;li>Long term: Please set more appropriate PDBs which allow disruption of at least one pod.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Expired cloud credentials can block the deletion of the machine from infrastructure.&lt;/li>
&lt;li>Cloud provider can&amp;rsquo;t delete the machine due to internal errors. Such situations are best debugged by using cloud provider specific CLI or cloud console.&lt;/li>
&lt;/ul>
&lt;h3 id="my-machine-is-not-joining-the-cluster-why">My machine is not joining the cluster, why?&lt;/h3>
&lt;p>In most cases, the &lt;code>Machine.Status.LastOperation&lt;/code> provides information around why a machine can&amp;rsquo;t be created.
It could possibly be debugged with following steps:&lt;/p>
&lt;ul>
&lt;li>Firstly make sure all the relevant controllers like &lt;code>kube-controller-manager&lt;/code> , &lt;code>cloud-controller-manager&lt;/code> are running.&lt;/li>
&lt;li>Verify if the machine is actually created in the cloud. User can use the &lt;code>Machine.Spec.ProviderId&lt;/code> to query the machine in cloud.&lt;/li>
&lt;li>A Kubernetes node is generally bootstrapped with the cloud-config. Please verify, if &lt;code>MachineDeployment&lt;/code> is pointing the correct &lt;code>MachineClass&lt;/code>, and &lt;code>MachineClass&lt;/code> is pointing to the correct &lt;code>Secret&lt;/code>. The secret object contains the actual cloud-config in &lt;code>base64&lt;/code> format which will be used to boot the machine.&lt;/li>
&lt;li>User must also check the logs of the MCM pod to understand any broken logical flow of reconciliation.&lt;/li>
&lt;/ul>
&lt;h3 id="my-rolling-update-is-stuck--why">My rolling update is stuck , why?&lt;/h3>
&lt;p>The following can be the reason:&lt;/p>
&lt;ul>
&lt;li>Insufficient capacity for the new instance type the machineClass mentions.&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#my-machine-is-stuck-in-deletion-for-1-hr-why">Old machines are stuck in deletion&lt;/a>&lt;/li>
&lt;li>If you are using Gardener for setting up kubernetes cluster, then machine object won&amp;rsquo;t turn to &lt;code>Running&lt;/code> state until &lt;code>node-critical-components&lt;/code> are ready. Refer &lt;a href="https://gardener.cloud/docs/gardener/node-readiness/">this&lt;/a> for more details.&lt;/li>
&lt;/ul>
&lt;h1 id="developer">Developer&lt;/h1>
&lt;h3 id="how-should-i-test-my-code-before-submitting-a-pr">How should I test my code before submitting a PR?&lt;/h3>
&lt;ul>
&lt;li>Developer can locally setup the MCM using following &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/local_setup/">guide&lt;/a>&lt;/li>
&lt;li>Developer must also enhance the unit tests related to the incoming changes.&lt;/li>
&lt;li>Developer can locally run the unit test by executing:&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>make test-unit
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Developer can locally run &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/integration_tests/">integration tests&lt;/a> to ensure basic functionality of MCM is not altered.&lt;/li>
&lt;/ul>
&lt;h3 id="i-need-to-change-the-apis-what-are-the-recommended-steps">I need to change the APIs, what are the recommended steps?&lt;/h3>
&lt;p>Developer should add/update the API fields at both of the following places:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/types.go">https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/types.go&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/machine-controller-manager/tree/master/pkg/apis/machine/v1alpha1">https://github.com/gardener/machine-controller-manager/tree/master/pkg/apis/machine/v1alpha1&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Once API changes are done, auto-generate the code using following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>make generate
&lt;/code>&lt;/pre>&lt;p>Please ignore the API-violation errors for now.&lt;/p>
&lt;h3 id="how-can-i-update-the-dependencies-of-mcm">How can I update the dependencies of MCM?&lt;/h3>
&lt;p>MCM uses &lt;code>gomod&lt;/code> for depedency management.
Developer should add/udpate depedency in the go.mod file. Please run following command to automatically tidy the dependencies.&lt;/p>
&lt;pre tabindex="0">&lt;code>make tidy
&lt;/code>&lt;/pre>&lt;h1 id="in-the-context-of-gardener">In the context of Gardener&lt;/h1>
&lt;h3 id="how-can-i-configure-mcm-using-shoot-resource">How can I configure MCM using Shoot resource?&lt;/h3>
&lt;p>All of the knobs of MCM can be configured by the &lt;code>workers&lt;/code> &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L29-L126">section&lt;/a> of the shoot resource.&lt;/p>
&lt;ul>
&lt;li>Gardener creates a &lt;code>MachineDeployment&lt;/code> per zone for each worker-pool under &lt;code>workers&lt;/code> section.&lt;/li>
&lt;li>&lt;code>workers.dataVolumes&lt;/code> allows to attach multiple disks to a machine during creation. Refer the &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L29-L126">link&lt;/a>.&lt;/li>
&lt;li>&lt;code>workers.machineControllerManager&lt;/code> allows configuration of multiple knobs of the &lt;code>MachineDeployment&lt;/code> from the shoot resource.&lt;/li>
&lt;/ul>
&lt;h3 id="how-is-my-worker-pool-spread-across-zones">How is my worker-pool spread across zones?&lt;/h3>
&lt;p>Shoot resource allows the worker-pool to spread across multiple zones using the field &lt;code>workers.zones&lt;/code>. Refer &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L115">link&lt;/a>.&lt;/p>
&lt;ul>
&lt;li>Gardener creates one &lt;code>MachineDeployment&lt;/code> per zone. Each &lt;code>MachineDeployment&lt;/code> is initiated with the following replica:&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>MachineDeployment.Spec.Replicas = (Workers.Minimum)/(Number of availibility zones)
&lt;/code>&lt;/pre></description></item><item><title>Docs: Vertical Pod Autoscaler</title><link>https://gardener.cloud/docs/getting-started/features/vpa/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/vpa/</guid><description>
&lt;h2 id="vertical-pod-autoscaler">Vertical Pod Autoscaler&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/vpa_43b432.gif" alt="">&lt;/p>
&lt;p>When a pod&amp;rsquo;s resource CPU or memory grows, it will hit a limit eventually. Either the pod has resource limits specified or the node will run short of resources. In both cases, the workload might be throttled or even terminated. When this happens, it is often desirable to increase the request or limits. To do this autonomously within certain boundaries is the goal of the Vertical Pod Autoscaler project.&lt;/p>
&lt;p>Since it is not part of the standard Kubernetes API, you have to install the CRDs and controller manually. With Gardener, you can simply flip the switch in the shoot&amp;rsquo;s spec and start creating your VPA objects.&lt;/p>
&lt;p>Please be aware that VPA and HPA operate in similar domains and might interfere.&lt;/p>
&lt;p>A controller &amp;amp; CRDs for vertical pod auto-scaling can be activated via the shoot&amp;rsquo;s spec.&lt;/p></description></item><item><title>Docs: Cluster Autoscaler</title><link>https://gardener.cloud/docs/getting-started/features/cluster-autoscaler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/cluster-autoscaler/</guid><description>
&lt;h2 id="obtaining-aditional-nodes">Obtaining Aditional Nodes&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/additional-nodes_4a5ea7.gif" alt="">&lt;/p>
&lt;p>The scheduler will assign pods to nodes, as long as they have capacity (CPU, memory, Pod limit, # attachable disks, &amp;hellip;). But what happens when all nodes are fully utilized and the scheduler does not find any suitable target?&lt;/p>
&lt;p>&lt;strong>Option 1:&lt;/strong> Evict other pods based on priority. However, this has the downside that other workloads with lower priority might become unschedulable.&lt;/p>
&lt;p>&lt;strong>Option 2:&lt;/strong> Add more nodes. There is an upstream Cluster Autoscaler project that does exactly this. It simulates the scheduling and reacts to pods not being schedulable events. Gardener has forked it to make it work with machine-controller-manager abstraction of how node (groups) are defined in Gardener.
The cluster autoscaler respects the limits (min / max) of any worker pool in a shoot&amp;rsquo;s spec. It can also scale down nodes based on utilization thresholds. For more details, see the &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md">autoscaler documentation&lt;/a>.&lt;/p>
&lt;h2 id="scaling-by-priority">Scaling by Priority&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/priority-scaling_a4bb49.gif" alt="">&lt;/p>
&lt;p>For clusters with more than one node pool, the cluster autoscaler has to decide which group to scale up. By default, it randomly picks from the available / applicable. However, this behavior is customizable by the use of so-called expanders.&lt;/p>
&lt;p>This section will focus on the priority based expander.&lt;/p>
&lt;p>Each worker pool gets a priority and the cluster autoscaler will scale up the one with the highest priority until it reaches its limit.&lt;/p>
&lt;p>To get more information on the current status of the autoscaler, you can check a &amp;ldquo;status&amp;rdquo; configmap in the &lt;code>kube-system&lt;/code> namespace with the following command:&lt;/p>
&lt;p>&lt;code>kubectl get cm -n kube-system cluster-autoscaler-status -oyaml&lt;/code>&lt;/p>
&lt;p>To obtain information about the decision making, you can check the logs of the cluster-autoscaler pod by using the shoot&amp;rsquo;s monitoring stack.&lt;/p>
&lt;p>For more information, see the &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders">cluster-autoscaler FAQ&lt;/a> and the &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/expander/priority/readme.md">Priority based expander for cluster-autoscaler&lt;/a> topic.&lt;/p></description></item><item><title>Docs: Gardenctl V2</title><link>https://gardener.cloud/docs/gardenctl-v2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardenctl-v2/</guid><description>
&lt;h1 id="gardenctl-v2">gardenctl-v2&lt;/h1>
&lt;p>&lt;img src="https://gardener.cloud/__resources/logo_gardener_cli_large_6e11b6.png" alt="">&lt;/p>
&lt;p>&lt;a href="https://api.reuse.software/info/github.com/gardener/gardenctl-v2">&lt;img src="https://api.reuse.software/badge/github.com/gardener/gardenctl-v2" alt="REUSE status">&lt;/a>
&lt;a href="https://goreportcard.com/report/github.com/gardener/gardenctl-v2">&lt;img src="https://goreportcard.com/badge/github.com/gardener/gardenctl-v2" alt="Go Report Card">&lt;/a>
&lt;a href="https://badge.fury.io/gh/gardener%2Fgardenctl-v2">&lt;img src="https://badge.fury.io/gh/gardener%2Fgardenctl-v2.svg" alt="release">&lt;/a>&lt;/p>
&lt;h2 id="what-is-gardenctl">What is &lt;code>gardenctl&lt;/code>?&lt;/h2>
&lt;p>gardenctl is a command-line client for the Gardener. It facilitates the administration of one or many garden, seed and shoot clusters. Use this tool to configure access to clusters and configure cloud provider CLI tools. It also provides support for accessing cluster nodes via ssh.&lt;/p>
&lt;h2 id="installation">Installation&lt;/h2>
&lt;p>Install the latest release from &lt;a href="https://brew.sh/">Homebrew&lt;/a>, &lt;a href="https://chocolatey.org/packages/gardenctl-v2">Chocolatey&lt;/a> or &lt;a href="https://github.com/gardener/gardenctl-v2/releases">GitHub Releases&lt;/a>.&lt;/p>
&lt;h3 id="install-using-package-managers">Install using Package Managers&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Homebrew (macOS and Linux)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>brew install gardener/tap/gardenctl-v2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Chocolatey (Windows)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># default location C:\ProgramData\chocolatey\bin\gardenctl-v2.exe&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>choco install gardenctl-v2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Attention &lt;code>brew&lt;/code> users: &lt;code>gardenctl-v2&lt;/code> uses the same binary name as the legacy &lt;code>gardenctl&lt;/code> (&lt;code>gardener/gardenctl&lt;/code>) CLI. If you have an existing installation you should remove it with &lt;code>brew uninstall gardenctl&lt;/code> before attempting to install &lt;code>gardenctl-v2&lt;/code>. Alternatively, you can choose to link the binary using a different name. If you try to install without removing or relinking the old installation, brew will run into an error and provide instructions how to resolve it.&lt;/p>
&lt;h3 id="install-from-github-release">Install from Github Release&lt;/h3>
&lt;p>If you install via GitHub releases, you need to&lt;/p>
&lt;ul>
&lt;li>put the &lt;code>gardenctl&lt;/code> binary on your path&lt;/li>
&lt;li>and &lt;a href="https://github.com/gardener/gardenlogin#installation">install gardenlogin&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>The other install methods do this for you.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Example for macOS&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># set operating system and architecture&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>os=darwin &lt;span style="color:#008000"># choose between darwin, linux, windows&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>arch=amd64 &lt;span style="color:#008000"># choose between amd64, arm64&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Get latest version. Alternatively set your desired version&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>version=&lt;span style="color:#00f">$(&lt;/span>curl -s https://raw.githubusercontent.com/gardener/gardenctl-v2/master/LATEST&lt;span style="color:#00f">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Download gardenctl&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -LO &lt;span style="color:#a31515">&amp;#34;https://github.com/gardener/gardenctl-v2/releases/download/&lt;/span>&lt;span style="color:#a31515">${&lt;/span>version&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">/gardenctl_v2_&lt;/span>&lt;span style="color:#a31515">${&lt;/span>os&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">_&lt;/span>&lt;span style="color:#a31515">${&lt;/span>arch&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Make the gardenctl binary executable&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>chmod +x &lt;span style="color:#a31515">&amp;#34;./gardenctl_v2_&lt;/span>&lt;span style="color:#a31515">${&lt;/span>os&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">_&lt;/span>&lt;span style="color:#a31515">${&lt;/span>arch&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Move the binary in to your PATH&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo mv &lt;span style="color:#a31515">&amp;#34;./gardenctl_v2_&lt;/span>&lt;span style="color:#a31515">${&lt;/span>os&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">_&lt;/span>&lt;span style="color:#a31515">${&lt;/span>arch&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">&amp;#34;&lt;/span> /usr/local/bin/gardenctl
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="configuration">Configuration&lt;/h2>
&lt;p>&lt;code>gardenctl&lt;/code> requires a configuration file. The default location is in &lt;code>~/.garden/gardenctl-v2.yaml&lt;/code>.&lt;/p>
&lt;p>You can modify this file directly using the &lt;code>gardenctl config&lt;/code> command. It allows adding, modifying and deleting gardens.&lt;/p>
&lt;p>Example &lt;code>config&lt;/code> command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Adapt the path to your kubeconfig file for the garden cluster (not to be mistaken with your shoot cluster)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export KUBECONFIG=~/relative/path/to/kubeconfig.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Fetch cluster-identity of garden cluster from the configmap&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cluster_identity=&lt;span style="color:#00f">$(&lt;/span>kubectl -n kube-system get configmap cluster-identity -ojsonpath={.data.cluster-identity}&lt;span style="color:#00f">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Configure garden cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gardenctl config set-garden $cluster_identity --kubeconfig $KUBECONFIG
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This command will create or update a garden with the provided identity and kubeconfig path of your garden cluster.&lt;/p>
&lt;h3 id="example-config">Example Config&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>gardens:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - identity: landscape-dev &lt;span style="color:#008000"># Unique identity of the garden cluster. See cluster-identity ConfigMap in kube-system namespace of the garden cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeconfig: ~/relative/path/to/kubeconfig.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># name: my-name # An alternative, unique garden name for targeting&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># context: different-context # Overrides the current-context of the garden cluster kubeconfig&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># patterns: ~ # List of regex patterns for pattern targeting&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note: You need to have &lt;a href="https://github.com/gardener/gardenlogin">gardenlogin&lt;/a> installed as &lt;code>kubectl&lt;/code> plugin in order to use the &lt;code>kubeconfig&lt;/code>s for &lt;code>Shoot&lt;/code> clusters provided by &lt;code>gardenctl&lt;/code>.&lt;/p>
&lt;h3 id="config-path-overwrite">Config Path Overwrite&lt;/h3>
&lt;ul>
&lt;li>The &lt;code>gardenctl&lt;/code> config path can be overwritten with the environment variable &lt;code>GCTL_HOME&lt;/code>.&lt;/li>
&lt;li>The &lt;code>gardenctl&lt;/code> config name can be overwritten with the environment variable &lt;code>GCTL_CONFIG_NAME&lt;/code>.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>export GCTL_HOME=/alternate/garden/config/dir
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export GCTL_CONFIG_NAME=myconfig &lt;span style="color:#008000"># without extension!&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># config is expected to be under /alternate/garden/config/dir/myconfig.yaml&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="shell-session">Shell Session&lt;/h3>
&lt;p>The state of gardenctl is bound to a shell session and is not shared across windows, tabs or panes.
A shell session is defined by the environment variable &lt;code>GCTL_SESSION_ID&lt;/code>. If this is not defined,
the value of the &lt;code>TERM_SESSION_ID&lt;/code> environment variable is used instead. If both are not defined,
this leads to an error and gardenctl cannot be executed. The &lt;code>target.yaml&lt;/code> and temporary
&lt;code>kubeconfig.*.yaml&lt;/code> files are store in the following directory &lt;code>${TMPDIR}/garden/${GCTL_SESSION_ID}&lt;/code>.&lt;/p>
&lt;p>You can make sure that &lt;code>GCTL_SESSION_ID&lt;/code> or &lt;code>TERM_SESSION_ID&lt;/code> is always present by adding
the following code to your terminal profile &lt;code>~/.profile&lt;/code>, &lt;code>~/.bashrc&lt;/code> or comparable file.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>bash and zsh: [ -n &lt;span style="color:#a31515">&amp;#34;&lt;/span>$GCTL_SESSION_ID&lt;span style="color:#a31515">&amp;#34;&lt;/span> ] || [ -n &lt;span style="color:#a31515">&amp;#34;&lt;/span>$TERM_SESSION_ID&lt;span style="color:#a31515">&amp;#34;&lt;/span> ] || export GCTL_SESSION_ID=&lt;span style="color:#00f">$(&lt;/span>uuidgen&lt;span style="color:#00f">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>fish: [ -n &lt;span style="color:#a31515">&amp;#34;&lt;/span>$GCTL_SESSION_ID&lt;span style="color:#a31515">&amp;#34;&lt;/span> ] || [ -n &lt;span style="color:#a31515">&amp;#34;&lt;/span>$TERM_SESSION_ID&lt;span style="color:#a31515">&amp;#34;&lt;/span> ] || set -gx GCTL_SESSION_ID (uuidgen)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-ps" data-lang="ps">&lt;span style="display:flex;">&lt;span>powershell: if &lt;span style="color:#a31515">( !(Test-Path Env:GCTL_SESSION_ID) -and !(Test-Path Env:TERM_SESSION_ID) )&lt;/span> { $Env:GCTL_SESSION_ID = [guid]::NewGuid&lt;span style="color:#a31515">()&lt;/span>.ToString&lt;span style="color:#a31515">()&lt;/span> }
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="completion">Completion&lt;/h3>
&lt;p>Gardenctl supports completion that will help you working with the CLI and save you typing effort.
It will also help you find clusters by providing suggestions for gardener resources such as shoots or projects.
Completion is supported for &lt;code>bash&lt;/code>, &lt;code>zsh&lt;/code>, &lt;code>fish&lt;/code> and &lt;code>powershell&lt;/code>.
You will find more information on how to configure your shell completion for gardenctl by executing the help for
your shell completion command. Example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>gardenctl completion bash --help
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="usage">Usage&lt;/h2>
&lt;h3 id="targeting">Targeting&lt;/h3>
&lt;p>You can set a target to use it in subsequent commands. You can also overwrite the target for each command individually.&lt;/p>
&lt;p>Note that this will not affect your KUBECONFIG env variable. To update the KUBECONFIG env for your current target see &lt;a href="https://gardener.cloud/docs/gardenctl-v2/#configure-kubeconfig-for-shoot-clusters">Configure KUBECONFIG&lt;/a> section&lt;/p>
&lt;p>Example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># target control plane&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gardenctl target --garden landscape-dev --project my-project --shoot my-shoot --control-plane
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Find more information in the &lt;a href="https://github.com/gardener/gardenctl-v2/blob/master/docs/usage/targeting.md">documentation&lt;/a>.&lt;/p>
&lt;h3 id="configure-kubeconfig-for-shoot-clusters">Configure KUBECONFIG for Shoot Clusters&lt;/h3>
&lt;p>Generate a script that points KUBECONFIG to the targeted cluster for the specified shell. Use together with &lt;code>eval&lt;/code> to configure your shell. Example for &lt;code>bash&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>eval &lt;span style="color:#00f">$(&lt;/span>gardenctl kubectl-env bash&lt;span style="color:#00f">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="configure-cloud-provider-clis">Configure Cloud Provider CLIs&lt;/h3>
&lt;p>Generate the cloud provider CLI configuration script for the specified shell. Use together with &lt;code>eval&lt;/code> to configure your shell. Example for &lt;code>bash&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>eval &lt;span style="color:#00f">$(&lt;/span>gardenctl provider-env bash&lt;span style="color:#00f">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="ssh">SSH&lt;/h3>
&lt;p>Establish an SSH connection to a Shoot cluster&amp;rsquo;s node.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>gardenctl ssh my-node
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Gardener Teaser</title><link>https://gardener.cloud/docs/resources/videos/gardener-teaser/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/gardener-teaser/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/YI-RyfdQNhw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="Gardener Teaser">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: The Illustrated Guide to Kubernetes</title><link>https://gardener.cloud/docs/resources/videos/fairy-tail/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/fairy-tail/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/4ht22ReBjno" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="The Illustrated Guide to Kubernetes">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Why Kubernetes</title><link>https://gardener.cloud/docs/resources/videos/why-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/why-kubernetes/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/N6r-9ZzFgzw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="Why Kubernetes">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: High Performance Microservices with Kubernetes, Go, and gRPC</title><link>https://gardener.cloud/docs/resources/videos/microservices-in_kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/microservices-in_kubernetes/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/YiNt4kUnnIM" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="High Performance Microservices with Kubernetes, Go, and gRPC">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Building Small Containers</title><link>https://gardener.cloud/docs/resources/videos/small-container/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/small-container/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/wGz_cbtCiEA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="Building Small Containers">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Organizing with Namespaces</title><link>https://gardener.cloud/docs/resources/videos/namespace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/namespace/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/xpnZX3if9Tc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="Organizing with Namespaces">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Readiness != Liveness</title><link>https://gardener.cloud/docs/resources/videos/livecheck-readiness/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/livecheck-readiness/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/mxEvAPQRwhw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="Readiness != Liveness">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: The Ins and Outs of Networking</title><link>https://gardener.cloud/docs/resources/videos/in-out-networking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/in-out-networking/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/y2bhV81MfKQ" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="The Ins and Outs of Networking">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Changing alerting settings</title><link>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/alerting/</link><pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/alerting/</guid><description>
&lt;h1 id="changing-alerting-settings">Changing alerting settings&lt;/h1>
&lt;p>Certificates are normally renewed automatically 30 days before they expire.
As a second line of defense, there is an alerting in Prometheus activated if the certificate is a few days
before expiration. By default, the alert is triggered 15 days before expiration.&lt;/p>
&lt;p>You can configure the days in the &lt;code>providerConfig&lt;/code> of the extension.
Setting it to 0 disables the alerting.&lt;/p>
&lt;p>In this example, the days are changed to 3 days before expiration.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extensions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - type: shoot-cert-service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: CertConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> alerting:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> certExpirationAlertDays: 3
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Best Practices</title><link>https://gardener.cloud/docs/guides/high-availability/best-practices/</link><pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/high-availability/best-practices/</guid><description>
&lt;h1 id="implementing-high-availability-and-tolerating-zone-outages">Implementing High Availability and Tolerating Zone Outages&lt;/h1>
&lt;p>Developing highly available workload that can tolerate a zone outage is no trivial task. You will find here various recommendations to get closer to that goal. While many recommendations are general enough, the examples are specific in how to achieve this in a Gardener-managed cluster and where/how to tweak the different control plane components. If you do not use Gardener, it may be still a worthwhile read.&lt;/p>
&lt;p>First however, what is a zone outage? It sounds like a clear-cut &amp;ldquo;thing&amp;rdquo;, but it isn&amp;rsquo;t. There are many things that can go haywire. Here are some examples:&lt;/p>
&lt;ul>
&lt;li>Elevated cloud provider API error rates for individual or multiple services&lt;/li>
&lt;li>Network bandwidth reduced or latency increased, usually also effecting storage sub systems as they are network attached&lt;/li>
&lt;li>No networking at all, no DNS, machines shutting down or restarting, &amp;hellip;&lt;/li>
&lt;li>Functional issues, of either the entire service (e.g. all block device operations) or only parts of it (e.g. LB listener registration)&lt;/li>
&lt;li>All services down, temporarily or permanently (the proverbial burning down data center 🔥)&lt;/li>
&lt;/ul>
&lt;p>This and everything in between make it hard to prepare for such events, but you can still do a lot. The most important recommendation is to not target specific issues exclusively - tomorrow another service will fail in an unanticipated way. Also, focus more on &lt;a href="https://research.google/pubs/pub50828">meaningful availability&lt;/a> than on internal signals (useful, but not as relevant as the former). Always prefer automation over manual intervention (e.g. leader election is a pretty robust mechanism, auto-scaling may be required as well, etc.).&lt;/p>
&lt;p>Also remember that HA is costly - you need to balance it against the cost of an outage as silly as this may sound, e.g. running all this excess capacity &amp;ldquo;just in case&amp;rdquo; vs. &amp;ldquo;going down&amp;rdquo; vs. a risk-based approach in between where you have means that will kick in, but they are not guaranteed to work (e.g. if the cloud provider is out of resource capacity). Maybe some of your components must run at the highest possible availability level, but others not - that&amp;rsquo;s a decision only you can make.&lt;/p>
&lt;h2 id="control-plane">Control Plane&lt;/h2>
&lt;p>The Kubernetes cluster control plane is managed by Gardener (as pods in separate infrastructure clusters to which you have no direct access) and can be set up with no failure tolerance (control plane pods will be recreated best-effort when resources are available) or one of the &lt;a href="https://gardener.cloud/docs/guides/high-availability/control-plane/">failure tolerance types &lt;code>node&lt;/code> or &lt;code>zone&lt;/code>&lt;/a>.&lt;/p>
&lt;p>Strictly speaking, static workload does not depend on the (high) availability of the control plane, but static workload doesn&amp;rsquo;t rhyme with Cloud and Kubernetes and also means, that when you possibly need it the most, e.g. during a zone outage, critical self-healing or auto-scaling functionality won&amp;rsquo;t be available to you and your workload, if your control plane is down as well. That&amp;rsquo;s why, even though the resource consumption is significantly higher, we generally recommend to use the failure tolerance type &lt;code>zone&lt;/code> for the control planes of productive clusters, at least in all regions that have 3+ zones. Regions that have only 1 or 2 zones don&amp;rsquo;t support the failure tolerance type &lt;code>zone&lt;/code> and then your second best option is the failure tolerance type &lt;code>node&lt;/code>, which means a zone outage can still take down your control plane, but individual node outages won&amp;rsquo;t.&lt;/p>
&lt;p>In the &lt;code>shoot&lt;/code> resource it&amp;rsquo;s merely only this what you need to add:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> controlPlane:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> highAvailability:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> failureTolerance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: zone &lt;span style="color:#008000"># valid values are `node` and `zone` (only available if your control plane resides in a region with 3+ zones)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This setting will scale out all control plane components for a Gardener cluster as necessary, so that no single zone outage can take down the control plane for longer than just a few seconds for the fail-over to take place (e.g. lease expiration and new leader election or readiness probe failure and endpoint removal). Components run highly available in either active-active (servers) or active-passive (controllers) mode at all times, the persistence (ETCD), which is consensus-based, will tolerate the loss of one zone and still maintain quorum and therefore remain operational. These are all patterns that we will revisit down below also for your own workload.&lt;/p>
&lt;h2 id="worker-pools">Worker Pools&lt;/h2>
&lt;p>Now that you have configured your Kubernetes cluster control plane in HA, i.e. spread it across multiple zones, you need to do the same for your own workload, but in order to do so, you need to spread your nodes across multiple zones first.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> minimum: 6
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maximum: 60
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> zones:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Prefer regions with at least 2, better 3+ zones and list the zones in the &lt;code>zones&lt;/code> section for each of your worker pools. Whether you need 2 or 3 zones at a minimum depends on your fail-over concept:&lt;/p>
&lt;ul>
&lt;li>Consensus-based software components (like ETCD) depend on maintaining a quorum of &lt;code>(n/2)+1&lt;/code>, so you need at least 3 zones to tolerate the outage of 1 zone.&lt;/li>
&lt;li>Primary/Secondary-based software components need just 2 zones to tolerate the outage of 1 zone.&lt;/li>
&lt;li>Then there are software components that can scale out horizontally. They are probably fine with 2 zones, but you also need to think about the load-shift and that the remaining zone must then pick up the work of the unhealthy zone. With 2 zones, the remaining zone must cope with an increase of 100% load. With 3 zones, the remaining zones must only cope with an increase of 50% load (per zone).&lt;/li>
&lt;/ul>
&lt;p>In general, the question is also whether you have the fail-over capacity already up and running or not. If not, i.e. you depend on re-scheduling to a healthy zone or auto-scaling, be aware that during a zone outage, you will see a resource crunch in the healthy zones. If you have no automation, i.e. only human operators (a.k.a. &amp;ldquo;red button approach&amp;rdquo;), you probably will not get the machines you need and even with automation, it may be tricky. But holding the capacity available at all times is costly. In the end, that&amp;rsquo;s a decision only you can make. If you made that decision, please adapt the &lt;code>minimum&lt;/code>, &lt;code>maximum&lt;/code>, &lt;code>maxSurge&lt;/code> and &lt;code>maxUnavailable&lt;/code> settings for your worker pools accordingly (visit &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/#on-specproviderworkersminimum-maximum-maxsurge-maxunavailable-zones-and-machinecontrollermanager">this section&lt;/a> for more information).&lt;/p>
&lt;p>Also, consider fall-back worker pools (with different/alternative machine types) and &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders">cluster autoscaler expanders&lt;/a> using a &lt;a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/expander/priority/readme.md">priority-based strategy&lt;/a>.&lt;/p>
&lt;p>Gardener-managed clusters deploy the &lt;a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">cluster autoscaler&lt;/a> or CA for short and you can &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#clusterautoscaler">tweak the general CA knobs&lt;/a> for Gardener-managed clusters like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clusterAutoscaler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expander: &lt;span style="color:#a31515">&amp;#34;least-waste&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scanInterval: 10s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownDelayAfterAdd: 60m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownDelayAfterDelete: 0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownDelayAfterFailure: 3m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownUnneededTime: 30m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownUtilizationThreshold: 0.5
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you want to be ready for a sudden spike or have some buffer in general, &lt;a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-can-i-configure-overprovisioning-with-cluster-autoscaler">over-provision nodes by means of &amp;ldquo;placeholder&amp;rdquo; pods&lt;/a> with &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption">low priority&lt;/a> and appropriate resource requests. This way, they will demand nodes to be provisioned for them, but if any pod comes up with a regular/higher priority, the low priority pods will be evicted to make space for the more important ones. Strictly speaking, this is not related to HA, but it may be important to keep this in mind as you generally want critical components to be rescheduled as fast as possible and if there is no node available, it may take 3 minutes or longer to do so (depending on the cloud provider). Besides, not only zones can fail, but also individual nodes.&lt;/p>
&lt;h2 id="replicas-horizontal-scaling">Replicas (Horizontal Scaling)&lt;/h2>
&lt;p>Now let&amp;rsquo;s talk about your workload. In most cases, this will mean to run multiple replicas. If you cannot do that (a.k.a. you have a singleton), that&amp;rsquo;s a bad situation to be in. Maybe you can run a spare (secondary) as backup? If you cannot, you depend on quick detection and rescheduling of your singleton (more on that below).&lt;/p>
&lt;p>Obviously, things get messier with persistence. If you have persistence, you should ideally replicate your data, i.e. let your spare (secondary) &amp;ldquo;follow&amp;rdquo; your main (primary). If your software doesn&amp;rsquo;t support that, you have to deploy other means, e.g. &lt;a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots">volume snapshotting&lt;/a> or side-backups (specific to the software you deploy; keep the backups regional, so that you can switch to another zone at all times). If you have to do those, your HA scenario becomes more a DR scenario and terms like RPO and RTO become relevant to you:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Recovery Point Objective (RPO)&lt;/strong>: Potential data loss, i.e. how much data will you lose at most (time between backups)&lt;/li>
&lt;li>&lt;strong>Recovery Time Objective (RTO)&lt;/strong>: Time until recovery, i.e. how long does it take you to be operational again (time to restore)&lt;/li>
&lt;/ul>
&lt;p>Also, keep in mind that your persistent volumes are usually zonal, i.e. once you have a volume in one zone, it&amp;rsquo;s bound to that zone and you cannot get up your pod in another zone w/o first recreating the volume yourself (Kubernetes won&amp;rsquo;t help you here directly).&lt;/p>
&lt;p>Anyway, best avoid that, if you can (from technical and cost perspective). The best solution (and also the most costly one) is to run multiple replicas in multiple zones and keep your data replicated at all times, so that your RPO is always 0 (best). That&amp;rsquo;s what we do for Gardener-managed cluster HA control planes (ETCD) as any data loss may be disastrous and lead to orphaned resources (in addition, we deploy side cars that do side-backups for disaster recovery, with full and incremental snapshots with an RPO of 5m).&lt;/p>
&lt;p>So, how to run with multiple replicas? That&amp;rsquo;s the easiest part in Kubernetes and the two most important resources, &lt;code>Deployments&lt;/code> and &lt;code>StatefulSet&lt;/code>, support that out of the box:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Deployment | StatefulSet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> replicas: ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The problem comes with the number of replicas. It&amp;rsquo;s easy only if the number is static, e.g. 2 for active-active/passive or 3 for consensus-based software components, but what with software components that can scale out horizontally? Here you usually do not set the number of replicas statically, but make use of the &lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale">horizontal pod autoscaler&lt;/a> or HPA for short (built-in; part of the kube-controller-manager). There are also other options like the &lt;a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">cluster proportional autoscaler&lt;/a>, but while the former works based on metrics, the latter is more a guestimate approach that derives the number of replicas from the number of nodes/cores in a cluster. Sometimes useful, but often blind to the actual demand.&lt;/p>
&lt;p>So, HPA it is then for most of the cases. However, what is the resource (e.g. CPU or memory) that drives the number of desired replicas? Again, this is up to you, but not always are CPU or memory the best choices. In some cases, &lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#scaling-on-custom-metrics">custom metrics&lt;/a> may be more appropriate, e.g. requests per second (it was also for us).&lt;/p>
&lt;p>You will have to create specific &lt;code>HorizontalPodAutoscaler&lt;/code> resources for your scale target and can &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#horizontalpodautoscalerconfig">tweak the general HPA knobs&lt;/a> for Gardener-managed clusters like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeControllerManager:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> horizontalPodAutoscaler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> syncPeriod: 15s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tolerance: 0.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> downscaleStabilization: 5m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> initialReadinessDelay: 30s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpuInitializationPeriod: 5m0s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="resources-vertical-scaling">Resources (Vertical Scaling)&lt;/h2>
&lt;p>While it is important to set a sufficient number of replicas, it is also important to give the pods sufficient resources (CPU and memory). This is especially true when you think about HA. When a zone goes down, you might need to get up replacement pods, if you don&amp;rsquo;t have them running already to take over the load from the impacted zone. Likewise, e.g. with active-active software components, you can expect the remaining pods to receive more load. If you cannot scale them out horizontally to serve the load, you will probably need to scale them out (or rather up) vertically. This is done by the &lt;a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">vertical pod autoscaler&lt;/a> or VPA for short (not built-in; part of the &lt;a href="https://github.com/kubernetes/autoscaler">kubernetes/autoscaler&lt;/a> repository).&lt;/p>
&lt;p>A few caveats though:&lt;/p>
&lt;ul>
&lt;li>You cannot use HPA and VPA on the same metrics as they would influence each other, which would lead to pod trashing (more replicas require fewer resources; fewer resources require more replicas)&lt;/li>
&lt;li>Scaling horizontally doesn&amp;rsquo;t cause downtimes (at least not when out-scaling and only one replica is affected when in-scaling), but scaling vertically does (if the pod runs OOM anyway, but also when new recommendations are applied, resource requests for existing pods may be changed, which causes the pods to be rescheduled). Although the discussion is going on for a very long time now, that is still not supported in-place yet (see &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/1287-in-place-update-pod-resources/README.md">KEP 1287&lt;/a>, &lt;a href="https://github.com/kubernetes/kubernetes/pull/102884">implementation in Kubernetes&lt;/a>, &lt;a href="https://github.com/kubernetes/autoscaler/issues/4016">implementation in VPA&lt;/a>).&lt;/li>
&lt;/ul>
&lt;p>VPA is a useful tool and Gardener-managed clusters deploy a VPA by default for you (HPA is supported anyway as it&amp;rsquo;s built into the kube-controller-manager). You will have to create specific &lt;code>VerticalPodAutoscaler&lt;/code> resources for your scale target and can &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#verticalpodautoscaler">tweak the general VPA knobs&lt;/a> for Gardener-managed clusters like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> verticalPodAutoscaler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictAfterOOMThreshold: 10m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionRateBurst: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionRateLimit: -1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionTolerance: 0.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> recommendationMarginFraction: 0.15
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> updaterInterval: 1m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> recommenderInterval: 1m0s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>While horizontal pod autoscaling is relatively straight-forward, it takes a long time to master vertical pod autoscaling. We saw &lt;a href="https://github.com/kubernetes/autoscaler/issues/4498">performance issues&lt;/a>, hard-coded behavior (on OOM, memory is bumped by +20% and it may take a few iterations to reach a good level), unintended pod disruptions by applying new resource requests (after 12h all targeted pods will receive new requests even though individually they would be fine without, which also drives active-passive resource consumption up), difficulties to deal with spiky workload in general (due to the algorithmic approach it takes), recommended requests may exceed node capacity, limit scaling is proportional and therefore often questionable, and more. VPA is a double-edged sword: useful and necessary, but not easy to handle.&lt;/p>
&lt;p>For the Gardener-managed components, we mostly removed limits. Why?&lt;/p>
&lt;ul>
&lt;li>CPU limits have almost always only downsides. They cause needless CPU throttling, which is not even easily visible. CPU requests turn into &lt;code>cpu shares&lt;/code>, so if the node has capacity, the pod may consume the freely available CPU, but not if you have set limits, which curtail the pod by means of &lt;code>cpu quota&lt;/code>. There are only certain scenarios in which they may make sense, e.g. if you set requests=limits and thereby define a pod with &lt;code>guaranteed&lt;/code> &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod">QoS&lt;/a>, which influences your &lt;code>cgroup&lt;/code> placement. However, that is difficult to do for the components you implement yourself and practically impossible for the components you just consume, because what&amp;rsquo;s the correct value for requests/limits and will it hold true also if the load increases and what happens if a zone goes down or with the next update/version of this component? If anything, CPU limits caused outages, not helped prevent them.&lt;/li>
&lt;li>As for memory limits, they are slightly more useful, because CPU is compressible and memory is not, so if one pod runs berserk, it may take others down (with CPU, &lt;code>cpu shares&lt;/code> make it as fair as possible), depending on which OOM killer strikes (a complicated topic by itself). You don&amp;rsquo;t want the operating system OOM killer to strike as the result is unpredictable. Better, it&amp;rsquo;s the cgroup OOM killer or even the &lt;code>kubelet&lt;/code>&amp;rsquo;s eviction, if the consumption is slow enough as &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#interactions-of-pod-priority-and-qos">it takes priorities into consideration&lt;/a> even. If your component is critical and a singleton (e.g. node daemon set pods), you are better off also without memory limits, because letting the pod go OOM because of artificial/wrong memory limits can mean that the node becomes unusable. Hence, such components also better run only with no or a very high memory limit, so that you can catch the occasional memory leak (bug) eventually, but under normal operation, if you cannot decide about a true upper limit, rather not have limits and cause endless outages through them or when you need the pods the most (during a zone outage) where all your assumptions went out of the window.&lt;/li>
&lt;/ul>
&lt;p>The downside of having poor or no limits and poor and no requests is that nodes may &amp;ldquo;die&amp;rdquo; more often. Contrary to the expectation, even for managed services, the managed service is not responsible or cannot guarantee the health of a node under all circumstances, since the end user defines what is run on the nodes (shared responsibility). If the workload exhausts any resource, it will be the end of the node, e.g. by compressing the CPU too much (so that the &lt;code>kubelet&lt;/code> fails to do its work), exhausting the main memory too fast, disk space, file handles, or any other resource.&lt;/p>
&lt;p>The &lt;code>kubelet&lt;/code> allows for &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources">explicit reservation of resources&lt;/a> for operating system daemons (&lt;code>system-reserved&lt;/code>) and Kubernetes daemons (&lt;code>kube-reserved&lt;/code>) that are subtracted from the actual node resources and become the allocatable node resources for your workload/pods. All managed services configure these settings &amp;ldquo;by rule of thumb&amp;rdquo; (a balancing act), but cannot guarantee that the values won&amp;rsquo;t waste resources or always will be sufficient. You will have to fine-tune them eventually and adapt them to your needs. In addition, you can configure &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction">soft and hard eviction thresholds&lt;/a> to give the &lt;code>kubelet&lt;/code> some headroom to evict &amp;ldquo;greedy&amp;rdquo; pods in a controlled way. These settings can be configured for Gardener-managed clusters like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubelet:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> systemReserved: &lt;span style="color:#008000"># explicit resource reservation for operating system daemons&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpu: 100m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memory: 1Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ephemeralStorage: 1Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pid: 1000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeReserved: &lt;span style="color:#008000"># explicit resource reservation for Kubernetes daemons&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpu: 100m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memory: 1Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ephemeralStorage: 1Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pid: 1000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionSoft: &lt;span style="color:#008000"># soft, i.e. graceful eviction (used if the node is about to run out of resources, avoiding hard evictions)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memoryAvailable: 200Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSAvailable: 10%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSInodesFree: 10%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSAvailable: 10%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSInodesFree: 10%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionSoftGracePeriod: &lt;span style="color:#008000"># caps pod&amp;#39;s `terminationGracePeriodSeconds` value during soft evictions (specific grace periods)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memoryAvailable: 1m30s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSAvailable: 1m30s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSInodesFree: 1m30s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSAvailable: 1m30s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSInodesFree: 1m30s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionHard: &lt;span style="color:#008000"># hard, i.e. immediate eviction (used if the node is out of resources, avoiding the OS generally run out of resources fail processes indiscriminately)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memoryAvailable: 100Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSAvailable: 5%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSInodesFree: 5%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSAvailable: 5%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSInodesFree: 5%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionMinimumReclaim: &lt;span style="color:#008000"># additional resources to reclaim after hitting the hard eviction thresholds to not hit the same thresholds soon after again&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memoryAvailable: 0Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSAvailable: 0Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSInodesFree: 0Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSAvailable: 0Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSInodesFree: 0Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionMaxPodGracePeriod: 90 &lt;span style="color:#008000"># caps pod&amp;#39;s `terminationGracePeriodSeconds` value during soft evictions (general grace periods)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionPressureTransitionPeriod: 5m0s &lt;span style="color:#008000"># stabilization time window to avoid flapping of node eviction state&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can tweak these settings also individually per worker pool (&lt;code>spec.provider.workers.kubernetes.kubelet...&lt;/code>), which makes sense especially with different machine types (and also workload that you may want to schedule there).&lt;/p>
&lt;p>Physical memory is not compressible, but you can overcome this issue to some degree (alpha since Kubernetes &lt;code>v1.22&lt;/code> in combination with the feature gate &lt;code>NodeSwap&lt;/code> on the &lt;code>kubelet&lt;/code>) with swap memory. You can read more in this &lt;a href="https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap-alpha">introductory blog&lt;/a> and the &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory">docs&lt;/a>. If you chose to use it (still only alpha at the time of this writing) you may want to consider also the risks associated with swap memory:&lt;/p>
&lt;ul>
&lt;li>Reduced performance predictability&lt;/li>
&lt;li>Reduced performance up to page trashing&lt;/li>
&lt;li>Reduced security as secrets, normally held only in memory, could be swapped out to disk&lt;/li>
&lt;/ul>
&lt;p>That said, the various options mentioned above are only remotely related to HA and will not be further explored throughout this document, but just to remind you: if a zone goes down, load patterns will shift, existing pods will probably receive more load and will require more resources (especially because it is often practically impossible to set &amp;ldquo;proper&amp;rdquo; resource requests, which drive node allocation - limits are always ignored by the scheduler) or more pods will/must be placed on the existing and/or new nodes and then these settings, which are generally critical (especially if you switch on &lt;a href="https://gardener.cloud/docs/gardener/shoot_scheduling_profiles/">bin-packing for Gardener-managed clusters&lt;/a> as a cost saving measure), will become even more critical during a zone outage.&lt;/p>
&lt;h2 id="probes">Probes&lt;/h2>
&lt;p>Before we go down the rabbit hole even further and talk about how to spread your replicas, we need to talk about probes first, as they will become relevant later. Kubernetes supports three kinds of probes: &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes">startup, liveness, and readiness probes&lt;/a>. If you are a &lt;a href="https://twitter.com/thockin/status/1615468485987143682">visual thinker&lt;/a>, also check out this &lt;a href="https://speakerdeck.com/thockin/kubernetes-pod-probes">slide deck&lt;/a> by &lt;a href="https://www.linkedin.com/in/tim-hockin-6501072">Tim Hockin&lt;/a> (Kubernetes networking SIG chair).&lt;/p>
&lt;p>Basically, the &lt;code>startupProbe&lt;/code> and the &lt;code>livenessProbe&lt;/code> help you restart the container, if it&amp;rsquo;s unhealthy for whatever reason, by letting the &lt;code>kubelet&lt;/code> that orchestrates your containers on a node know, that it&amp;rsquo;s unhealthy. The former is a special case of the latter and only applied at the startup of your container, if you need to handle the startup phase differently (e.g. with very slow starting containers) from the rest of the lifetime of the container.&lt;/p>
&lt;p>Now, the &lt;code>readinessProbe&lt;/code> helps you manage the ready status of your container and thereby pod (any container that is not ready turns the pod not ready). This again has impact on endpoints and pod disruption budgets:&lt;/p>
&lt;ul>
&lt;li>If the pod is not ready, the endpoint will be removed and the pod will not receive traffic anymore&lt;/li>
&lt;li>If the pod is not ready, the pod counts into the pod disruption budget and if the budget is exceeded, no further voluntary pod disruptions will be permitted for the remaining ready pods (e.g. no eviction, no voluntary horizontal or vertical scaling, if the pod runs on a node that is about to be drained or in draining, draining will be paused until the max drain timeout passes)&lt;/li>
&lt;/ul>
&lt;p>As you can see, all of these probes are (also) related to HA (mostly the &lt;code>readinessProbe&lt;/code>, but depending on your workload, you can also leverage &lt;code>livenessProbe&lt;/code> and &lt;code>startupProbe&lt;/code> into your HA strategy). If Kubernetes doesn&amp;rsquo;t know about the individual status of your container/pod, it won&amp;rsquo;t do anything for you (right away). That said, later/indirectly something might/will happen via the node status that can also be ready or not ready, which influences the pods and load balancer listener registration (a not ready node will not receive cluster traffic anymore), but this process is worker pool global and reacts delayed and also doesn&amp;rsquo;t discriminate between the containers/pods on a node.&lt;/p>
&lt;p>In addition, Kubernetes also offers &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate">pod readiness gates&lt;/a> to amend your pod readiness with additional custom conditions (normally, only the sum of the container readiness matters, but pod readiness gates additionally count into the overall pod readiness). This may be useful if you want to block (by means of pod disruption budgets that we will talk about next) the roll-out of your workload/nodes in case some (possibly external) condition fails.&lt;/p>
&lt;h2 id="pod-disruption-budgets">Pod Disruption Budgets&lt;/h2>
&lt;p>One of the most important resources that help you on your way to HA are &lt;a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb">pod disruption budgets&lt;/a> or PDB for short. They tell Kubernetes how to deal with voluntary pod disruptions, e.g. during the deployment of your workload, when the nodes are rolled, or just in general when a pod shall be evicted/terminated. Basically, if the budget is reached, they block all voluntary pod disruptions (at least for a while until possibly other timeouts act or things happen that leave Kubernetes no choice anymore, e.g. the node is forcefully terminated). You should always define them for your workload.&lt;/p>
&lt;p>Very important to note is that they are based on the &lt;code>readinessProbe&lt;/code>, i.e. even if all of your replicas are &lt;code>lively&lt;/code>, but not enough of them are &lt;code>ready&lt;/code>, this blocks voluntary pod disruptions, so they are very critical and useful. Here an example (you can specify either &lt;code>minAvailable&lt;/code> or &lt;code>maxUnavailable&lt;/code> in absolute numbers or as percentage):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: policy/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: PodDisruptionBudget
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maxUnavailable: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> selector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And please do not specify a PDB of &lt;code>maxUnavailable&lt;/code> being 0 or similar. That&amp;rsquo;s pointless, even detrimental, as it blocks then even useful operations, forces always the hard timeouts that are less graceful and it doesn&amp;rsquo;t make sense in the context of HA. You cannot &amp;ldquo;force&amp;rdquo; HA by preventing voluntary pod disruptions, you must work with the pod disruptions in a resilient way. Besides, PDBs are really only about voluntary pod disruptions - something bad can happen to a node/pod at any time and PDBs won&amp;rsquo;t make this reality go away for you.&lt;/p>
&lt;p>PDBs will not always work as expected and can also get in your way, e.g. if the PDB is violated or would be violated, it may possibly block whatever you are trying to do to salvage the situation, e.g. drain a node or deploy a patch version (if the PDB is or would be violated, not even unhealthy pods would be evicted as they could theoretically become healthy again, which Kubernetes doesn&amp;rsquo;t know). In order to overcome this issue, it is now possible (alpha since Kubernetes &lt;code>v1.26&lt;/code> in combination with the feature gate &lt;code>PDBUnhealthyPodEvictionPolicy&lt;/code> on the API server) to configure the so-called &lt;a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy">unhealthy pod eviction policy&lt;/a>. The default is still &lt;code>IfHealthyBudget&lt;/code> as a change in default would have changed the behavior (as described above), but you can now also set &lt;code>AlwaysAllow&lt;/code> at the PDB (&lt;code>spec.unhealthyPodEvictionPolicy&lt;/code>). For more information, please check out &lt;a href="https://github.com/kubernetes/kubernetes/issues/72320">this discussion&lt;/a>, &lt;a href="https://github.com/kubernetes/kubernetes/pull/105296">the PR&lt;/a> and &lt;a href="https://groups.google.com/g/kubernetes-sig-apps/c/_joO4swogKY?pli=1">this document&lt;/a> and balance the pros and cons for yourself. In short,
the new &lt;code>AlwaysAllow&lt;/code> option is probably the better choice in most of the cases while &lt;code>IfHealthyBudget&lt;/code> is useful only if you have frequent temporary transitions or for special cases where you have already implemented controllers that depend on the old behavior.&lt;/p>
&lt;h2 id="pod-topology-spread-constraints">Pod Topology Spread Constraints&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints">Pod topology spread constraints&lt;/a> or PTSC for short (no official abbreviation exists, but we will use this in the following) are enormously helpful to distribute your replicas across multiple zones, nodes, or any other user-defined topology domain. They complement and improve on &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">pod (anti-)affinities&lt;/a> that still exist and can be used in combination.&lt;/p>
&lt;p>PTSCs are an improvement, because they allow for &lt;code>maxSkew&lt;/code> and &lt;code>minDomains&lt;/code>. You can steer the &amp;ldquo;level of tolerated imbalance&amp;rdquo; with &lt;code>maxSkew&lt;/code>, e.g. you probably want that to be at least 1, so that you can perform a rolling update, but this all depends on your &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment">deployment&lt;/a> (&lt;code>maxUnavailable&lt;/code> and &lt;code>maxSurge&lt;/code>), etc. &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#rolling-updates">Stateful sets&lt;/a> are a bit different (&lt;code>maxUnavailable&lt;/code>) as they are bound to volumes and depend on them, so there usually cannot be 2 pods requiring the same volume. &lt;code>minDomains&lt;/code> is a hint to tell the scheduler how far to spread, e.g. if all nodes in one zone disappeared because of a zone outage, it may &amp;ldquo;appear&amp;rdquo; as if there are only 2 zones in a 3 zones cluster and the scheduling decisions may end up wrong, so a &lt;code>minDomains&lt;/code> of 3 will tell the scheduler to spread to 3 zones before adding another replica in one zone. Be careful with this setting as it also means, if one zone is down the &amp;ldquo;spread&amp;rdquo; is already at least 1, if pods run in the other zones. This is useful where you have exactly as many replicas as you have zones and you do not want any imbalance. Imbalance is critical as if you end up with one, nobody is going to do the (active) re-balancing for you (unless you deploy and configure additional non-standard components such as the &lt;a href="https://github.com/kubernetes-sigs/descheduler">descheduler&lt;/a>). So, for instance, if you have something like a DBMS that you want to spread across 2 zones (active-passive) or 3 zones (consensus-based), you better specify &lt;code>minDomains&lt;/code> of 2 respectively 3 to force your replicas into at least that many zones before adding more replicas to another zone (if supported).&lt;/p>
&lt;p>Anyway, PTSCs are critical to have, but not perfect, so we saw (unsurprisingly, because that&amp;rsquo;s how the scheduler works), that the scheduler may block the deployment of new pods because it takes the decision pod-by-pod (see for instance &lt;a href="https://github.com/kubernetes/kubernetes/issues/109364">#109364&lt;/a>).&lt;/p>
&lt;h2 id="pod-affinities-and-anti-affinities">Pod Affinities and Anti-Affinities&lt;/h2>
&lt;p>As said, you can combine PTSCs with &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">pod affinities and/or anti-affinities&lt;/a>. Especially &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">inter-pod (anti-)affinities&lt;/a> may be helpful to place pods &lt;em>apart&lt;/em>, e.g. because they are fall-backs for each other or you do not want multiple potentially resource-hungry &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#besteffort">&amp;ldquo;best-effort&amp;rdquo;&lt;/a> or &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#burstable">&amp;ldquo;burstable&amp;rdquo;&lt;/a> pods side-by-side (noisy neighbor problem), or &lt;em>together&lt;/em>, e.g. because they form a unit and you want to reduce the failure domain, reduce the network latency, and reduce the costs.&lt;/p>
&lt;h2 id="topology-aware-hints">Topology Aware Hints&lt;/h2>
&lt;p>While &lt;a href="https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints">topology aware hints&lt;/a> are not directly related to HA, they are very relevant in the HA context. Spreading your workload across multiple zones may increase network latency and cost significantly, if the traffic is not shaped. Topology aware hints (beta since Kubernetes &lt;code>v1.23&lt;/code>, replacing the now deprecated topology aware traffic routing with topology keys) help to route the traffic within the originating zone, if possible. Basically, they tell &lt;code>kube-proxy&lt;/code> how to setup your routing information, so that clients can talk to endpoints that are located within the same zone.&lt;/p>
&lt;p>Be aware however, that there are some limitations. Those are called &lt;a href="https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/#safeguards">safeguards&lt;/a> and if they strike, the hints are off and traffic is routed again randomly. Especially controversial is the balancing limitation as there is the assumption, that the load that hits an endpoint is determined by the allocatable CPUs in that topology zone, but that&amp;rsquo;s not always, if even often, the case (see for instance &lt;a href="https://github.com/kubernetes/kubernetes/issues/113731">#113731&lt;/a> and &lt;a href="https://github.com/kubernetes/kubernetes/issues/110714">#110714&lt;/a>). So, this limitation hits far too often and your hints are off, but then again, it&amp;rsquo;s about network latency and cost optimization first, so it&amp;rsquo;s better than nothing.&lt;/p>
&lt;h2 id="networking">Networking&lt;/h2>
&lt;p>We have talked about networking only to some small degree so far (&lt;code>readiness&lt;/code> probes, pod disruption budgets, topology aware hints). The most important component is probably your ingress load balancer - everything else is managed by Kubernetes. AWS, Azure, GCP, and also OpenStack offer multi-zonal load balancers, so make use of them. In Azure and GCP, LBs are regional whereas in AWS and OpenStack, they need to be bound to a zone, which the cloud-controller-manager does by observing the zone labels at the nodes (please note that this behavior is not always working as expected, see &lt;a href="https://github.com/kubernetes/cloud-provider-aws/issues/569">#570&lt;/a> where the AWS cloud-controller-manager is not readjusting to newly observed zones).&lt;/p>
&lt;p>Please be reminded that even if you use a service mesh like &lt;a href="https://istio.io">Istio&lt;/a>, the off-the-shelf installation/configuration usually never comes with productive settings (to simplify first-time installation and improve first-time user experience) and you will have to fine-tune your installation/configuration, much like the rest of your workload.&lt;/p>
&lt;h2 id="relevant-cluster-settings">Relevant Cluster Settings&lt;/h2>
&lt;p>Following now a summary/list of the more relevant settings you may like to tune for Gardener-managed clusters:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> controlPlane:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> highAvailability:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> failureTolerance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: zone &lt;span style="color:#008000"># valid values are `node` and `zone` (only available if your control plane resides in a region with 3+ zones)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeAPIServer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> defaultNotReadyTolerationSeconds: 300
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> defaultUnreachableTolerationSeconds: 300
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubelet:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeScheduler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> featureGates:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> MinDomainsInPodTopologySpread: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeControllerManager:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeMonitorGracePeriod: 40s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> horizontalPodAutoscaler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> syncPeriod: 15s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tolerance: 0.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> downscaleStabilization: 5m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> initialReadinessDelay: 30s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpuInitializationPeriod: 5m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> verticalPodAutoscaler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictAfterOOMThreshold: 10m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionRateBurst: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionRateLimit: -1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionTolerance: 0.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> recommendationMarginFraction: 0.15
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> updaterInterval: 1m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> recommenderInterval: 1m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clusterAutoscaler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expander: &lt;span style="color:#a31515">&amp;#34;least-waste&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scanInterval: 10s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownDelayAfterAdd: 60m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownDelayAfterDelete: 0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownDelayAfterFailure: 3m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownUnneededTime: 30m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownUtilizationThreshold: 0.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> minimum: 6
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maximum: 60
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maxSurge: 3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maxUnavailable: 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> zones:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - ... &lt;span style="color:#008000"># list of zones you want your worker pool nodes to be spread across, see above&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubelet:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ... &lt;span style="color:#008000"># similar to `kubelet` above (cluster-wide settings), but here per worker pool (pool-specific settings), see above&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineControllerManager: &lt;span style="color:#008000"># optional, it allows to configure the machine-controller settings.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineCreationTimeout: 20m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineHealthTimeout: 10m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineDrainTimeout: 60h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> systemComponents:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> coreDNS:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> autoscaling:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mode: horizontal &lt;span style="color:#008000"># valid values are `horizontal` (driven by CPU load) and `cluster-proportional` (driven by number of nodes/cores)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="on-speccontrolplanehighavailabilityfailuretolerancetype">On &lt;code>spec.controlPlane.highAvailability.failureTolerance.type&lt;/code>&lt;/h4>
&lt;p>If set, determines the degree of failure tolerance for your control plane. &lt;code>zone&lt;/code> is preferred, but only available if your control plane resides in a region with 3+ zones. See &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/#control-plane">above&lt;/a> and the &lt;a href="https://gardener.cloud/docs/guides/high-availability/control-plane/">docs&lt;/a>.&lt;/p>
&lt;h4 id="on-speckuberneteskubeapiserverdefaultunreachabletolerationseconds-and-defaultnotreadytolerationseconds">On &lt;code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds&lt;/code> and &lt;code>defaultNotReadyTolerationSeconds&lt;/code>&lt;/h4>
&lt;p>This is a very interesting &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver">API server setting&lt;/a> that lets Kubernetes decide how fast to evict pods from nodes whose status condition of type &lt;code>Ready&lt;/code> is either &lt;code>Unknown&lt;/code> (node status unknown, a.k.a unreachable) or &lt;code>False&lt;/code> (&lt;code>kubelet&lt;/code> not ready) (see &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#condition">node status conditions&lt;/a>; please note that &lt;code>kubectl&lt;/code> shows both values as &lt;code>NotReady&lt;/code> which is a somewhat &amp;ldquo;simplified&amp;rdquo; visualization).&lt;/p>
&lt;p>You can also override the cluster-wide API server settings &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions">individually per pod&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tolerations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: &lt;span style="color:#a31515">&amp;#34;node.kubernetes.io/unreachable&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operator: &lt;span style="color:#a31515">&amp;#34;Exists&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> effect: &lt;span style="color:#a31515">&amp;#34;NoExecute&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tolerationSeconds: 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: &lt;span style="color:#a31515">&amp;#34;node.kubernetes.io/not-ready&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operator: &lt;span style="color:#a31515">&amp;#34;Exists&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> effect: &lt;span style="color:#a31515">&amp;#34;NoExecute&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tolerationSeconds: 0
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will evict pods on unreachable or not-ready nodes immediately, but be cautious: &lt;code>0&lt;/code> is very aggressive and may lead to unnecessary disruptions. Again, you must decide for your own workload and balance out the pros and cons (e.g. long startup time).&lt;/p>
&lt;p>Please note, these settings replace &lt;code>spec.kubernetes.kubeControllerManager.podEvictionTimeout&lt;/code> that was deprecated with Kubernetes &lt;code>v1.26&lt;/code> (and acted as an upper bound).&lt;/p>
&lt;h4 id="on-speckuberneteskubeschedulerfeaturegatesmindomainsinpodtopologyspread">On &lt;code>spec.kubernetes.kubeScheduler.featureGates.MinDomainsInPodTopologySpread&lt;/code>&lt;/h4>
&lt;p>Required to be enabled for &lt;code>minDomains&lt;/code> to work with PTSCs (beta since Kubernetes &lt;code>v1.25&lt;/code>, but off by default). See &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/#pod-topology-spread-constraints">above&lt;/a> and the &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#topologyspreadconstraints-field">docs&lt;/a>. This tells the scheduler, how many topology domains to expect (=zones in the context of this document).&lt;/p>
&lt;h4 id="on-speckuberneteskubecontrollermanagernodemonitorgraceperiod">On &lt;code>spec.kubernetes.kubeControllerManager.nodeMonitorGracePeriod&lt;/code>&lt;/h4>
&lt;p>This is another very interesting &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager">kube-controller-manager setting&lt;/a> that can help you speed up or slow down how fast a node shall be considered &lt;code>Unknown&lt;/code> (node status unknown, a.k.a unreachable) when the &lt;code>kubelet&lt;/code> is not updating its status anymore (see &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#condition">node status conditions&lt;/a>), which effects eviction (see &lt;code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds&lt;/code> and &lt;code>defaultNotReadyTolerationSeconds&lt;/code> above). The shorter the time window, the faster Kubernetes will act, but the higher the chance of flapping behavior and pod trashing, so you may want to balance that out according to your needs, otherwise stick to the default which is a reasonable compromise.&lt;/p>
&lt;h4 id="on-speckuberneteskubecontrollermanagerhorizontalpodautoscaler">On &lt;code>spec.kubernetes.kubeControllerManager.horizontalPodAutoscaler...&lt;/code>&lt;/h4>
&lt;p>This configures horizontal pod autoscaling in Gardener-managed clusters. See &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/#replicas-horizontal-scaling">above&lt;/a> and the &lt;a href="https://kubernetes.io/de/docs/tasks/run-application/horizontal-pod-autoscale">docs&lt;/a> for the detailed fields.&lt;/p>
&lt;h4 id="on-speckubernetesverticalpodautoscaler">On &lt;code>spec.kubernetes.verticalPodAutoscaler...&lt;/code>&lt;/h4>
&lt;p>This configures vertical pod autoscaling in Gardener-managed clusters. See &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/#resources-vertical-scaling">above&lt;/a> and the &lt;a href="https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md">docs&lt;/a> for the detailed fields.&lt;/p>
&lt;h4 id="on-speckubernetesclusterautoscaler">On &lt;code>spec.kubernetes.clusterAutoscaler...&lt;/code>&lt;/h4>
&lt;p>This configures node auto-scaling in Gardener-managed clusters. See &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/#worker-pools">above&lt;/a> and the &lt;a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md">docs&lt;/a> for the detailed fields, especially about &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders">expanders&lt;/a>, which may become life-saving in case of a zone outage when a resource crunch is setting in and everybody rushes to get machines in the healthy zones.&lt;/p>
&lt;p>In case of a zone outage, it is critical to understand how the cluster autoscaler will put a worker pool in one zone into &amp;ldquo;back-off&amp;rdquo; and what the consequences for your workload will be. Unfortunately, the official cluster autoscaler documentation does not explain these details, but you can find hints in the &lt;a href="https://github.com/kubernetes/autoscaler/blob/b94f340af58eb063df9ebfcd65835f9a499a69a2/cluster-autoscaler/config/autoscaling_options.go#L214-L219">source code&lt;/a>:&lt;/p>
&lt;p>If a node fails to come up, the node group (worker pool in that zone) will go into &amp;ldquo;back-off&amp;rdquo;, at first 5m, then &lt;a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/utils/backoff/exponential_backoff.go#L77-L82">exponentially longer&lt;/a> until the maximum of 30m is reached. The &amp;ldquo;back-off&amp;rdquo; is reset after 3 hours. This in turn means, that nodes must be first considered &lt;code>Unknown&lt;/code>, which happens when &lt;code>spec.kubernetes.kubeControllerManager.nodeMonitorGracePeriod&lt;/code> lapses (e.g. at the beginning of a zone outage). Then they must either remain in this state until &lt;code>spec.provider.workers.machineControllerManager.machineHealthTimeout&lt;/code> lapses for them to be recreated, which will fail in the unhealthy zone, or &lt;code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds&lt;/code> lapses for the pods to be evicted (usually faster than node replacements, depending on your configuration), which will trigger the cluster autoscaler to create more capacity, but very likely in the same zone as it tries to balance its node groups at first, which will fail in the unhealthy zone. It will be considered failed only when &lt;code>maxNodeProvisionTime&lt;/code> lapses (usually close to &lt;code>spec.provider.workers.machineControllerManager.machineCreationTimeout&lt;/code>) and only then put the node group into &amp;ldquo;back-off&amp;rdquo; and not retry for 5m (at first and then exponentially longer). Only then you can expect new node capacity to be brought up somewhere else.&lt;/p>
&lt;p>During the time of ongoing node provisioning (before a node group goes into &amp;ldquo;back-off&amp;rdquo;), the cluster autoscaler may have &amp;ldquo;virtually scheduled&amp;rdquo; pending pods onto those new upcoming nodes and will not reevaluate these pods anymore unless the node provisioning fails (which will fail during a zone outage, but the cluster autoscaler cannot know that and will therefore reevaluate its decision only after it has given up on the new nodes).&lt;/p>
&lt;p>It&amp;rsquo;s critical to keep that in mind and accommodate for it. If you have already capacity up and running, the reaction time is usually much faster with leases (whatever you set) or endpoints (&lt;code>spec.kubernetes.kubeControllerManager.nodeMonitorGracePeriod&lt;/code>), but if you depend on new/fresh capacity, the above should inform you how long you will have to wait for it and for how long pods might be pending (because capacity is generally missing and pending pods may have been &amp;ldquo;virtually scheduled&amp;rdquo; to new nodes that won&amp;rsquo;t come up until the node group goes eventually into &amp;ldquo;back-off&amp;rdquo; and nodes in the healthy zones come up).&lt;/p>
&lt;h4 id="on-specproviderworkersminimum-maximum-maxsurge-maxunavailable-zones-and-machinecontrollermanager">On &lt;code>spec.provider.workers.minimum&lt;/code>, &lt;code>maximum&lt;/code>, &lt;code>maxSurge&lt;/code>, &lt;code>maxUnavailable&lt;/code>, &lt;code>zones&lt;/code>, and &lt;code>machineControllerManager&lt;/code>&lt;/h4>
&lt;p>Each worker pool in Gardener may be configured differently. Among many other settings like machine type, root disk, Kubernetes version, &lt;code>kubelet&lt;/code> settings, and many more you can also specify the lower and upper bound for the number of machines (&lt;code>minimum&lt;/code> and &lt;code>maximum&lt;/code>), how many machines may be added additionally during a rolling update (&lt;code>maxSurge&lt;/code>) and how many machines may be in termination/recreation during a rolling update (&lt;code>maxUnavailable&lt;/code>), and of course across how many zones the nodes shall be spread (&lt;code>zones&lt;/code>).&lt;/p>
&lt;p>Gardener divides &lt;code>minimum&lt;/code>, &lt;code>maximum&lt;/code>, &lt;code>maxSurge&lt;/code>, &lt;code>maxUnavailable&lt;/code> values by the number of zones specified for this worker pool. This fact must be considered when you plan the sizing of your worker pools.&lt;/p>
&lt;p>&lt;em>Example:&lt;/em>&lt;/p>
&lt;pre tabindex="0">&lt;code> provider:
workers:
- name: ...
minimum: 6
maximum: 60
maxSurge: 3
maxUnavailable: 0
zones: [&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;, &amp;#34;c&amp;#34;]
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>The resulting &lt;code>MachineDeployment&lt;/code>s &lt;strong>per zone&lt;/strong> will get &lt;code>minimum: 2&lt;/code>, &lt;code>maximum: 20&lt;/code>, &lt;code>maxSurge: 1&lt;/code>, &lt;code>maxUnavailable: 0&lt;/code>.&lt;/li>
&lt;li>If another zone is added all values will be divided by &lt;code>4&lt;/code>, resulting in:
&lt;ul>
&lt;li>Less workers per zone.&lt;/li>
&lt;li>⚠️ One &lt;code>MachineDeployment&lt;/code> with &lt;code>maxSurge: 0&lt;/code>, i.e. there will be a replacement of nodes without rolling updates.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Interesting is also the configuration for Gardener&amp;rsquo;s machine-controller-manager or MCM for short that provisions, monitors, terminates, replaces, or updates machines that back your nodes:&lt;/p>
&lt;ul>
&lt;li>The shorter &lt;code>machineCreationTimeout&lt;/code> is, the faster MCM will retry to create a machine/node, if the process is stuck on cloud provider side. It is set to useful/practical timeouts for the different cloud providers and you probably don&amp;rsquo;t want to change those (in the context of HA at least). Please align with the cluster autoscaler&amp;rsquo;s &lt;code>maxNodeProvisionTime&lt;/code>.&lt;/li>
&lt;li>The shorter &lt;code>machineHealthTimeout&lt;/code> is, the faster MCM will replace machines/nodes in case the kubelet isn&amp;rsquo;t reporting back, which translates to &lt;code>Unknown&lt;/code>, or reports back with &lt;code>NotReady&lt;/code>, or the &lt;a href="https://github.com/kubernetes/node-problem-detector">node-problem-detector&lt;/a> that Gardener deploys for you reports a non-recoverable issue/condition (e.g. read-only file system). If it is too short however, you risk node and pod trashing, so be careful.&lt;/li>
&lt;li>The shorter &lt;code>machineDrainTimeout&lt;/code> is, the faster you can get rid of machines/nodes that MCM decided to remove, but this puts a cap on the grace periods and PDBs. They are respected up until the drain timeout lapses - then the machine/node will be forcefully terminated, whether or not the pods are still in termination or not even terminated because of PDBs. Those PDBs will then be violated, so be careful here as well. Please align with the cluster autoscaler&amp;rsquo;s &lt;code>maxGracefulTerminationSeconds&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>Especially the last two settings may help you recover faster from cloud provider issues.&lt;/p>
&lt;h4 id="on-specsystemcomponentscorednsautoscaling">On &lt;code>spec.systemComponents.coreDNS.autoscaling&lt;/code>&lt;/h4>
&lt;p>DNS is critical, in general and also within a Kubernetes cluster. Gardener-managed clusters deploy &lt;a href="https://coredns.io">CoreDNS&lt;/a>, a graduated CNCF project. Gardener supports 2 auto-scaling modes for it, &lt;code>horizontal&lt;/code> (using HPA based on CPU) and &lt;code>cluster-proportional&lt;/code> (using &lt;a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">cluster proportional autoscaler&lt;/a> that scales the number of pods based on the number of nodes/cores, not to be confused with the cluster autoscaler that scales nodes based on their utilization). Check out the &lt;a href="https://gardener.cloud/docs/gardener/dns-autoscaling/">docs&lt;/a>, especially the &lt;a href="https://gardener.cloud/docs/gardener/dns-autoscaling/#trade-offs-of-horizontal-and-cluster-proportional-dns-autoscaling">trade-offs&lt;/a> why you would chose one over the other (&lt;code>cluster-proportional&lt;/code> gives you more configuration options, if CPU-based horizontal scaling is insufficient to your needs). Consider also Gardener&amp;rsquo;s feature &lt;a href="https://gardener.cloud/docs/gardener/node-local-dns/">node-local DNS&lt;/a> to decouple you further from the DNS pods and stabilize DNS. Again, that&amp;rsquo;s not strictly related to HA, but may become important during a zone outage, when load patterns shift and pods start to initialize/resolve DNS records more frequently in bulk.&lt;/p>
&lt;h2 id="more-caveats">More Caveats&lt;/h2>
&lt;p>Unfortunately, there are a few more things of note when it comes to HA in a Kubernetes cluster that may be &amp;ldquo;surprising&amp;rdquo; and hard to mitigate:&lt;/p>
&lt;ul>
&lt;li>If the &lt;code>kubelet&lt;/code> restarts, it will report all pods as &lt;code>NotReady&lt;/code> on startup until it reruns its probes (&lt;a href="https://github.com/kubernetes/kubernetes/issues/100277">#100277&lt;/a>), which leads to temporary endpoint and load balancer target removal (&lt;a href="https://github.com/kubernetes/kubernetes/issues/102367">#102367&lt;/a>). This topic is somewhat controversial. Gardener uses rolling updates and a jitter to spread necessary &lt;code>kubelet&lt;/code> restarts as good as possible.&lt;/li>
&lt;li>If a &lt;code>kube-proxy&lt;/code> pod on a node turns &lt;code>NotReady&lt;/code>, all load balancer traffic to all pods (on this node) under services with &lt;code>externalTrafficPolicy&lt;/code> &lt;code>local&lt;/code> will cease as the load balancer will then take this node out of serving. This topic is somewhat controversial as well. So, please remember that &lt;code>externalTrafficPolicy&lt;/code> &lt;code>local&lt;/code> not only has the disadvantage of imbalanced traffic spreading, but also a dependency to the kube-proxy pod that may and will be unavailable during updates. Gardener uses rolling updates to spread necessary &lt;code>kube-proxy&lt;/code> updates as good as possible.&lt;/li>
&lt;/ul>
&lt;p>These are just a few additional considerations. They may or may not affect you, but other intricacies may. It&amp;rsquo;s a reminder to be watchful as Kubernetes may have one or two relevant quirks that you need to consider (and will probably only find out over time and with extensive testing).&lt;/p>
&lt;h2 id="meaningful-availability">Meaningful Availability&lt;/h2>
&lt;p>Finally, let&amp;rsquo;s go back to where we started. We recommended to measure &lt;a href="https://research.google/pubs/pub50828">meaningful availability&lt;/a>. For instance, in Gardener, we do not trust only internal signals, but track also whether Gardener or the control planes that it manages are externally available through the external DNS records and load balancers, SNI-routing Istio gateways, etc. (the same path all users must take). It&amp;rsquo;s a huge difference whether the API server&amp;rsquo;s internal readiness probe passes or the user can actually reach the API server and it does what it&amp;rsquo;s supposed to do. Most likely, you will be in a similar spot and can do the same.&lt;/p>
&lt;p>What you do with these signals is another matter. Maybe there are some actionable metrics and you can trigger some active fail-over, maybe you can only use it to improve your HA setup altogether. In our case, we also use it to deploy mitigations, e.g. via our &lt;a href="https://github.com/gardener/dependency-watchdog">dependency-watchdog&lt;/a> that watches, for instance, Gardener-managed API servers and shuts down components like the controller managers to avert cascading knock-off effects (e.g. melt-down if the &lt;code>kubelets&lt;/code> cannot reach the API server, but the controller managers can and start taking down nodes and pods).&lt;/p>
&lt;p>Either way, understanding how users perceive your service is key to the improvement process as a whole. Even if you are not struck by a zone outage, the measures above and tracking the meaningful availability will help you improve your service.&lt;/p>
&lt;p>Thank you for your interest.&lt;/p></description></item><item><title>Docs: Chaos Engineering</title><link>https://gardener.cloud/docs/guides/high-availability/chaos-engineering/</link><pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/high-availability/chaos-engineering/</guid><description>
&lt;!-- BEGIN of section that must be kept in sync with sibling tutorial -->
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Gardener provides &lt;a href="https://chaostoolkit.org">&lt;code>chaostoolkit&lt;/code>&lt;/a> modules to simulate &lt;em>compute&lt;/em> and &lt;em>network&lt;/em> outages for various cloud providers such as &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/aws">AWS&lt;/a>, &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/azure">Azure&lt;/a>, &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/gcp">GCP&lt;/a>, &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/openstack">OpenStack/Converged Cloud&lt;/a>, and &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/vsphere">VMware vSphere&lt;/a>, as well as &lt;em>pod disruptions&lt;/em> for &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/k8s">any Kubernetes cluster&lt;/a>.&lt;/p>
&lt;p>The API, parameterization, and implementation is as homogeneous as possible across the different cloud providers, so that you have only minimal effort. As a Gardener user, you benefit from an &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/garden">additional &lt;code>garden&lt;/code> module&lt;/a> that leverages the generic modules, but exposes their functionality in the most simple, homogeneous, and secure way (no need to specify cloud provider credentials, cluster credentials, or filters explicitly; retrieves credentials and stores them in memory only).&lt;/p>
&lt;h2 id="installation">Installation&lt;/h2>
&lt;p>The name of the package is &lt;code>chaosgarden&lt;/code> and it was developed and tested with Python 3.9+. It&amp;rsquo;s being published to &lt;a href="https://pypi.org/project/chaosgarden">PyPI&lt;/a>, so that you can comfortably install it via Python&amp;rsquo;s package installer &lt;a href="https://pip.pypa.io/en/stable">pip&lt;/a> (you may want to &lt;a href="https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/#creating-a-virtual-environment">create a virtual environment&lt;/a> before installing it):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>pip install chaosgarden
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ℹ️ If you want to use the &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/vsphere">VMware vSphere module&lt;/a>, please note the remarks in &lt;a href="https://github.com/gardener/chaos-engineering/blob/main/requirements.txt">&lt;code>requirements.txt&lt;/code>&lt;/a> for &lt;code>vSphere&lt;/code>. Those are not contained in the published PyPI package.&lt;/p>
&lt;p>The package can be used directly from Python scripts and supports this usage scenario with additional convenience that helps launch actions and probes in background (more on actions and probes later), so that you can compose also complex scenarios with ease.&lt;/p>
&lt;!-- END of section that must be kept in sync with sibling tutorial -->
&lt;p>If this technology is new to you, you will probably prefer the &lt;a href="https://chaostoolkit.org">&lt;code>chaostoolkit&lt;/code>&lt;/a> &lt;a href="https://chaostoolkit.org/reference/usage/cli">CLI&lt;/a> in combination with &lt;a href="https://chaostoolkit.org/reference/api/experiment">experiment files&lt;/a>, so we need to &lt;a href="https://chaostoolkit.org/reference/usage/install/#install-the-cli">install the CLI&lt;/a> next:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>pip install chaostoolkit
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please verify that it was installed properly by running:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>chaos --help
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!-- BEGIN of section that must be kept in sync with sibling tutorial -->
&lt;h2 id="usage">Usage&lt;/h2>
&lt;p>ℹ️ We assume you are using Gardener and run Gardener-managed shoot clusters. You can also use the generic cloud provider and Kubernetes &lt;code>chaosgarden&lt;/code> modules, but configuration and secrets will then differ. Please see the &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs">module docs&lt;/a> for details.&lt;/p>
&lt;!-- END of section that must be kept in sync with sibling tutorial -->
&lt;h3 id="a-simple-experiment">A Simple Experiment&lt;/h3>
&lt;p>The most important command is the &lt;a href="https://chaostoolkit.org/reference/usage/run">&lt;code>run&lt;/code>&lt;/a> command, but before we can use it, we need to compile an experiment file first. Let&amp;rsquo;s start with a simple one, invoking only a read-only 📖 action from &lt;code>chaosgarden&lt;/code> that lists cloud provider machines and networks (depends on cloud provider) for the &amp;ldquo;first&amp;rdquo; zone of one of your shoot clusters.&lt;/p>
&lt;p>Let&amp;rsquo;s assume, your project is called &lt;code>my-project&lt;/code> and your shoot is called &lt;code>my-shoot&lt;/code>, then we need to create the following experiment:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;title&amp;#34;: &lt;span style="color:#a31515">&amp;#34;assess-filters-impact&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;description&amp;#34;: &lt;span style="color:#a31515">&amp;#34;assess-filters-impact&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;method&amp;#34;: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;action&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;name&amp;#34;: &lt;span style="color:#a31515">&amp;#34;assess-filters-impact&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;provider&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;python&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;module&amp;#34;: &lt;span style="color:#a31515">&amp;#34;chaosgarden.garden.actions&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;func&amp;#34;: &lt;span style="color:#a31515">&amp;#34;assess_cloud_provider_filters_impact&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;arguments&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;zone&amp;#34;: 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;configuration&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;garden_project&amp;#34;: &lt;span style="color:#a31515">&amp;#34;my-project&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;garden_shoot&amp;#34;: &lt;span style="color:#a31515">&amp;#34;my-shoot&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!-- BEGIN of section that must be kept in sync with sibling tutorial -->
&lt;p>We are not yet there and need one more thing to do before we can run it: We need to &amp;ldquo;target&amp;rdquo; the Gardener landscape resp. Gardener API server where you have created your shoot cluster (not to be confused with your shoot cluster API server). If you do not know what this is or how to download the Gardener API server &lt;code>kubeconfig&lt;/code>, please follow &lt;a href="https://gardener.cloud/docs/dashboard/project-operations/#prerequisites">these instructions&lt;/a>. You can either download your &lt;em>personal&lt;/em> credentials or &lt;em>project&lt;/em> credentials (see &lt;a href="https://gardener.cloud/docs/dashboard/automated-resource-management/#prerequisites">creation of a &lt;code>serviceaccount&lt;/code>&lt;/a>) to interact with Gardener. For now (fastest and most convenient way, but generally not recommended), let&amp;rsquo;s use your &lt;em>personal&lt;/em> credentials, but if you later plan to automate your experiments, please use proper &lt;em>project&lt;/em> credentials (a &lt;code>serviceaccount&lt;/code> is not bound to your person, but to the project, and can be restricted using &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac">RBAC roles and role bindings&lt;/a>, which is why we recommend this for production).&lt;/p>
&lt;p>To download your &lt;em>personal&lt;/em> credentials, open the Gardener Dashboard and click on your avatar in the upper right corner of the page. Click &amp;ldquo;My Account&amp;rdquo;, then look for the &amp;ldquo;Access&amp;rdquo; pane, then &amp;ldquo;Kubeconfig&amp;rdquo;, then press the &amp;ldquo;Download&amp;rdquo; button and save the &lt;code>kubeconfig&lt;/code> to disk. Run the following command next:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>export KUBECONFIG=path/to/kubeconfig
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!-- END of section that must be kept in sync with sibling tutorial -->
&lt;p>We are now set and you can run your first experiment:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>chaos run path/to/experiment
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You should see output like this (depends on cloud provider):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-txt" data-lang="txt">&lt;span style="display:flex;">&lt;span>[INFO] Validating the experiment&amp;#39;s syntax
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Installing signal handlers to terminate all active background threads on involuntary signals (note that SIGKILL cannot be handled).
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Experiment looks valid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Running experiment: assess-filters-impact
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Steady-state strategy: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Rollbacks strategy: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] No steady state hypothesis defined. That&amp;#39;s ok, just exploring.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Playing your experiment&amp;#39;s method now...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Action: assess-filters-impact
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Validating client credentials and listing probably impacted instances and/or networks with the given arguments zone=&amp;#39;world-1a&amp;#39; and filters={&amp;#39;instances&amp;#39;: [{&amp;#39;Name&amp;#39;: &amp;#39;tag-key&amp;#39;, &amp;#39;Values&amp;#39;: [&amp;#39;kubernetes.io/cluster/shoot--my-project--my-shoot&amp;#39;]}], &amp;#39;vpcs&amp;#39;: [{&amp;#39;Name&amp;#39;: &amp;#39;tag-key&amp;#39;, &amp;#39;Values&amp;#39;: [&amp;#39;kubernetes.io/cluster/shoot--my-project--my-shoot&amp;#39;]}]}:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] 1 instance(s) would be impacted:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] - i-aabbccddeeff0000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] 1 VPC(s) would be impacted:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] - vpc-aabbccddeeff0000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Let&amp;#39;s rollback...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] No declared rollbacks, let&amp;#39;s move on.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Experiment ended with status: completed
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>🎉 Congratulations! You successfully ran your first &lt;code>chaosgarden&lt;/code> experiment.&lt;/p>
&lt;h3 id="a-destructive-experiment">A Destructive Experiment&lt;/h3>
&lt;p>Now let&amp;rsquo;s break 🪓 your cluster. Be advised that this experiment will be destructive in the sense that we will temporarily network-partition all nodes in one availability zone (machine termination or restart is available with &lt;code>chaosgarden&lt;/code> as well). That means, these nodes and their pods won&amp;rsquo;t be able to &amp;ldquo;talk&amp;rdquo; to other nodes, pods, and services. Also, the API server will become unreachable for them and the API server will report them as unreachable (confusingly shown as &lt;code>NotReady&lt;/code> when you run &lt;code>kubectl get nodes&lt;/code> and &lt;code>Unknown&lt;/code> in the status &lt;code>Ready&lt;/code> condition when you run &lt;code>kubectl get nodes --output yaml&lt;/code>).&lt;/p>
&lt;p>Being unreachable will trigger service endpoint and load balancer de-registration (when the node&amp;rsquo;s grace period lapses) as well as eventually pod eviction and machine replacement (which will continue to fail under test). We won&amp;rsquo;t run the experiment long enough for all of these effects to materialize, but the longer you run it, the more will happen, up to temporarily giving up/going into &amp;ldquo;back-off&amp;rdquo; for the affected worker pool in that zone. You will also see that the Kubernetes cluster autoscaler will try to create a new machine almost immediately, if pods are pending for the affected zone (which will initially fail under test, but may succeed later, which again depends on the runtime of the experiment and whether or not the cluster autoscaler goes into &amp;ldquo;back-off&amp;rdquo; or not).&lt;/p>
&lt;p>But for now, all of this doesn&amp;rsquo;t matter as we want to start &amp;ldquo;small&amp;rdquo;. You can later read up more on the various settings and effects in our &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/">best practices guide on high availability&lt;/a>.&lt;/p>
&lt;p>Please create a new experiment file, this time with this content:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;title&amp;#34;: &lt;span style="color:#a31515">&amp;#34;run-network-failure-simulation&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;description&amp;#34;: &lt;span style="color:#a31515">&amp;#34;run-network-failure-simulation&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;method&amp;#34;: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;action&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;name&amp;#34;: &lt;span style="color:#a31515">&amp;#34;run-network-failure-simulation&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;provider&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;python&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;module&amp;#34;: &lt;span style="color:#a31515">&amp;#34;chaosgarden.garden.actions&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;func&amp;#34;: &lt;span style="color:#a31515">&amp;#34;run_cloud_provider_network_failure_simulation&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;arguments&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;mode&amp;#34;: &lt;span style="color:#a31515">&amp;#34;total&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;zone&amp;#34;: 0,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;duration&amp;#34;: 60
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;rollbacks&amp;#34;: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;action&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;name&amp;#34;: &lt;span style="color:#a31515">&amp;#34;rollback-network-failure-simulation&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;provider&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;python&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;module&amp;#34;: &lt;span style="color:#a31515">&amp;#34;chaosgarden.garden.actions&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;func&amp;#34;: &lt;span style="color:#a31515">&amp;#34;rollback_cloud_provider_network_failure_simulation&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;arguments&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;mode&amp;#34;: &lt;span style="color:#a31515">&amp;#34;total&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;zone&amp;#34;: 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;configuration&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;garden_project&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;env&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;key&amp;#34;: &lt;span style="color:#a31515">&amp;#34;GARDEN_PROJECT&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;garden_shoot&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;env&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;key&amp;#34;: &lt;span style="color:#a31515">&amp;#34;GARDEN_SHOOT&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ℹ️ There is an even more destructive action that terminates or alternatively restarts machines in a given zone 🔥 (immediately or delayed with some randomness/chaos for maximum inconvenience for the nodes and pods). You can find links to all these examples at the end of this tutorial.&lt;/p>
&lt;p>This experiment is very similar, but this time we will break 🪓 your cluster - for &lt;code>60s&lt;/code>. If that&amp;rsquo;s too short to even see a node or pod transition from &lt;code>Ready&lt;/code> to &lt;code>NotReady&lt;/code> (actually &lt;code>Unknown&lt;/code>), then increase the &lt;code>duration&lt;/code>. Depending on the workload that your cluster runs, you may already see effects of the network partitioning, because it is effective immediately. It&amp;rsquo;s just that Kubernetes cannot know immediately and rather assumes that something is failing only &lt;strong>after&lt;/strong> the node&amp;rsquo;s grace period lapses, but the actual workload is impacted immediately.&lt;/p>
&lt;p>Most notably, this experiment also has a &lt;a href="https://chaostoolkit.org/reference/concepts/#rollbacks">&lt;code>rollbacks&lt;/code>&lt;/a> section, which is invoked even if you abort the experiment or it fails unexpectedly, but only if you run the CLI with the option &lt;code>--rollback-strategy always&lt;/code> which we will do soon. Any &lt;code>chaosgarden&lt;/code> action that can undo its activity, will do that implicitly when the &lt;code>duration&lt;/code> lapses, but it is a best practice to always configure a &lt;code>rollbacks&lt;/code> section in case something unexpected happens. Should you be in panic and just want to run the &lt;code>rollbacks&lt;/code> section, you can remove all other actions and the CLI will execute the &lt;code>rollbacks&lt;/code> section immediately.&lt;/p>
&lt;p>One other thing is different in the second experiment as well. We now read the name of the project and the shoot from the environment, i.e. a &lt;a href="https://chaostoolkit.org/reference/api/experiment/#configuration">&lt;code>configuration&lt;/code>&lt;/a> section can automatically expand &lt;a href="https://chaostoolkit.org/reference/api/experiment/#environment-configurations">environment variables&lt;/a>. Also useful to know (not shown here), &lt;code>chaostoolkit&lt;/code> supports &lt;a href="https://chaostoolkit.org/reference/api/experiment/#variable-substitution">variable substitution&lt;/a> too, so that you have to define variables only once. Please note that you can also add a &lt;a href="https://chaostoolkit.org/reference/api/experiment/#secrets">&lt;code>secrets&lt;/code>&lt;/a> section that can also automatically expand &lt;a href="https://chaostoolkit.org/reference/api/experiment/#environment-secrets">environment variables&lt;/a>. For instance, instead of targeting the Gardener API server via &lt;code>$KUBECONFIG&lt;/code>, which is supported by our &lt;code>chaosgarden&lt;/code> package natively, you can also explicitly refer to it in a &lt;code>secrets&lt;/code> section (for brevity reasons not shown here either).&lt;/p>
&lt;p>Let&amp;rsquo;s now run your second experiment (please watch your nodes and pods in parallel, e.g. by running &lt;code>watch kubectl get nodes,pods --output wide&lt;/code> in another terminal):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>export GARDEN_PROJECT=my-project
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export GARDEN_SHOOT=my-shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>chaos run --rollback-strategy always path/to/experiment
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output of the &lt;code>run&lt;/code> command will be similar to the one above, but longer. It will mention either machines or networks that were network-partitioned (depends on cloud provider), but should revert everything back to normal.&lt;/p>
&lt;p>Normally, you would not only run &lt;a href="https://chaostoolkit.org/reference/concepts/#actions">actions&lt;/a> in the &lt;code>method&lt;/code> section, but also &lt;a href="https://chaostoolkit.org/reference/concepts/#probes">probes&lt;/a> as part of a &lt;a href="https://chaostoolkit.org/reference/concepts/#steady-state-hypothesis">steady state hypothesis&lt;/a>. Such steady state hypothesis probes are run before and after the actions to validate that the &amp;ldquo;system&amp;rdquo; was in a healthy state before and gets back to a healthy state after the actions ran, hence show that the &amp;ldquo;system&amp;rdquo; is in a steady state when not under test. Eventually, you will write your own probes that don&amp;rsquo;t even have to be part of a steady state hypothesis. We at Gardener run multi-zone (multiple zones at once) and rolling-zone (strike each zone once) outages with continuous custom probes all within the &lt;code>method&lt;/code> section to validate our KPIs continuously under test (e.g. how long do the individual fail-overs take/how long is the actual outage). The most complex scenarios are even run via Python scripts as all actions and probes can also be invoked directly (which is what the CLI does).&lt;/p>
&lt;!-- BEGIN of section that must be kept in sync with sibling tutorial -->
&lt;h2 id="high-availability">High Availability&lt;/h2>
&lt;p>Developing highly available workload that can tolerate a zone outage is no trivial task. You can find more information on how to achieve this goal in our &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/">best practices guide on high availability&lt;/a>.&lt;/p>
&lt;p>Thank you for your interest in Gardener chaos engineering and making your workload more resilient.&lt;/p>
&lt;h2 id="further-reading">Further Reading&lt;/h2>
&lt;p>Here some links for further reading:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Examples&lt;/strong>: &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/tutorials/experiments">Experiments&lt;/a>, &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/tutorials/scripts">Scripts&lt;/a>&lt;/li>
&lt;li>&lt;strong>Gardener Chaos Engineering&lt;/strong>: &lt;a href="https://github.com/gardener/chaos-engineering">GitHub&lt;/a>, &lt;a href="https://pypi.org/project/chaosgarden">PyPI&lt;/a>, &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/garden">Module Docs for Gardener Users&lt;/a>&lt;/li>
&lt;li>&lt;strong>Chaos Toolkit Core&lt;/strong>: &lt;a href="https://chaostoolkit.org">Home Page&lt;/a>, &lt;a href="https://chaostoolkit.org/reference/usage/install">Installation&lt;/a>, &lt;a href="https://chaostoolkit.org/reference/concepts">Concepts&lt;/a>, &lt;a href="https://github.com/chaostoolkit/chaostoolkit">GitHub&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- END of section that must be kept in sync with sibling tutorial --></description></item><item><title>Docs: Control Plane</title><link>https://gardener.cloud/docs/guides/high-availability/control-plane/</link><pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/high-availability/control-plane/</guid><description>
&lt;h1 id="highly-available-shoot-control-plane">Highly Available Shoot Control Plane&lt;/h1>
&lt;p>Shoot resource offers a way to request for a highly available control plane.&lt;/p>
&lt;h2 id="failure-tolerance-types">Failure Tolerance Types&lt;/h2>
&lt;p>A highly available shoot control plane can be setup with either a failure tolerance of &lt;code>zone&lt;/code> or &lt;code>node&lt;/code>.&lt;/p>
&lt;h3 id="node-failure-tolerance">&lt;code>Node&lt;/code> Failure Tolerance&lt;/h3>
&lt;p>The failure tolerance of a &lt;code>node&lt;/code> will have the following characteristics:&lt;/p>
&lt;ul>
&lt;li>Control plane components will be spread across different nodes within a single availability zone. There will not be
more than one replica per node for each control plane component which has more than one replica.&lt;/li>
&lt;li>&lt;code>Worker pool&lt;/code> should have a minimum of 3 nodes.&lt;/li>
&lt;li>A multi-node etcd (quorum size of 3) will be provisioned, offering zero-downtime capabilities with each member in a
different node within a single availability zone.&lt;/li>
&lt;/ul>
&lt;h3 id="zone-failure-tolerance">&lt;code>Zone&lt;/code> Failure Tolerance&lt;/h3>
&lt;p>The failure tolerance of a &lt;code>zone&lt;/code> will have the following characteristics:&lt;/p>
&lt;ul>
&lt;li>Control plane components will be spread across different availability zones. There will be at least
one replica per zone for each control plane component which has more than one replica.&lt;/li>
&lt;li>Gardener scheduler will automatically select a &lt;code>seed&lt;/code> which has a minimum of 3 zones to host the shoot control plane.&lt;/li>
&lt;li>A multi-node etcd (quorum size of 3) will be provisioned, offering zero-downtime capabilities with each member in a
different zone.&lt;/li>
&lt;/ul>
&lt;h2 id="shoot-spec">Shoot Spec&lt;/h2>
&lt;p>To request for a highly available shoot control plane Gardener provides the following configuration in the shoot spec:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> controlPlane:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> highAvailability:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> failureTolerance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: &amp;lt;node | zone&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Allowed Transitions&lt;/strong>&lt;/p>
&lt;p>If you already have a shoot cluster with non-HA control plane, then the following upgrades are possible:&lt;/p>
&lt;ul>
&lt;li>Upgrade of non-HA shoot control plane to HA shoot control plane with &lt;code>node&lt;/code> failure tolerance.&lt;/li>
&lt;li>Upgrade of non-HA shoot control plane to HA shoot control plane with &lt;code>zone&lt;/code> failure tolerance. However, it is essential that the &lt;code>seed&lt;/code> which is currently hosting the shoot control plane should be &lt;code>multi-zonal&lt;/code>. If it is not, then the request to upgrade will be rejected.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> There will be a small downtime during the upgrade, especially for etcd, which will transition from a single node etcd cluster to a multi-node etcd cluster.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Disallowed Transitions&lt;/strong>&lt;/p>
&lt;p>If you already have a shoot cluster with HA control plane, then the following transitions are not possible:&lt;/p>
&lt;ul>
&lt;li>Upgrade of HA shoot control plane from &lt;code>node&lt;/code> failure tolerance to &lt;code>zone&lt;/code> failure tolerance is currently not supported, mainly because already existing volumes are bound to the zone they were created in originally.&lt;/li>
&lt;li>Downgrade of HA shoot control plane with &lt;code>zone&lt;/code> failure tolerance to &lt;code>node&lt;/code> failure tolerance is currently not supported, mainly because of the same reason as above, that already existing volumes are bound to the respective zones they were created in originally.&lt;/li>
&lt;li>Downgrade of HA shoot control plane with either &lt;code>node&lt;/code> or &lt;code>zone&lt;/code> failure tolerance, to a non-HA shoot control plane is currently not supported, mainly because &lt;a href="https://github.com/gardener/etcd-druid">etcd-druid&lt;/a> does not currently support scaling down of a multi-node etcd cluster to a single-node etcd cluster.&lt;/li>
&lt;/ul>
&lt;h2 id="zone-outage-situation">Zone Outage Situation&lt;/h2>
&lt;p>Implementing highly available software that can tolerate even a zone outage unscathed is no trivial task. You may find our &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/">HA Best Practices&lt;/a> helpful to get closer to that goal. In this document, we collected many options and settings for you that also Gardener internally uses to provide a highly available service.&lt;/p>
&lt;p>During a zone outage, you may be forced to change your cluster setup on short notice in order to compensate for failures and shortages resulting from the outage.
For instance, if the shoot cluster has worker nodes across three zones where one zone goes down, the computing power from these nodes is also gone during that time.
Changing the worker pool (&lt;code>shoot.spec.provider.workers[]&lt;/code>) and infrastructure (&lt;code>shoot.spec.provider.infrastructureConfig&lt;/code>) configuration can eliminate this disbalance, having enough machines in healthy availability zones that can cope with the requests of your applications.&lt;/p>
&lt;p>Gardener relies on a sophisticated reconciliation flow with several dependencies for which various flow steps wait for the &lt;em>readiness&lt;/em> of prior ones.
During a zone outage, this can block the entire flow, e.g., because all three &lt;code>etcd&lt;/code> replicas can never be ready when a zone is down, and required changes mentioned above can never be accomplished.
For this, a special one-off annotation &lt;code>shoot.gardener.cloud/skip-readiness&lt;/code> helps to skip any readiness checks in the flow.&lt;/p>
&lt;blockquote>
&lt;p>The &lt;code>shoot.gardener.cloud/skip-readiness&lt;/code> annotation serves as a last resort if reconciliation is stuck because of important changes during an AZ outage. Use it with caution, only in exceptional cases and after a case-by-case evaluation with your Gardener landscape administrator. If used together with other operations like Kubernetes version upgrades or credential rotation, the annotation may lead to a severe outage of your shoot control plane.&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Manage certificates with Gardener for default domain</title><link>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_default_domain_cert/</link><pubDate>Wed, 20 Jul 2022 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_default_domain_cert/</guid><description>
&lt;h1 id="manage-certificates-with-gardener-for-default-domain">Manage certificates with Gardener for default domain&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Dealing with applications on Kubernetes which offer a secure service endpoints (e.g. HTTPS) also require you to enable a
secured communication via SSL/TLS. With the &lt;a href="https://github.com/gardener/gardener-extension-shoot-cert-service">certificate extension&lt;/a> enabled, Gardener can manage commonly trusted X.509 certificate for your application
endpoint. From initially requesting certificate, it also handeles their renewal in time using the free Let&amp;rsquo;s Encrypt API.&lt;/p>
&lt;p>&lt;strong>There are two senarios with which you can use the certificate extension&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>You want to use a certificate for a subdomain the shoot&amp;rsquo;s default DNS (see &lt;code>.spec.dns.domain&lt;/code> of your shoot resource, e.g. &lt;code>short.ingress.shoot.project.default-domain.gardener.cloud&lt;/code>). If this is your case, please keep reading this article.&lt;/li>
&lt;li>You want to use a certificate for a custom domain. If this is your case, please see &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/">Manage certificates with Gardener for public domain&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>Before you start this guide there are a few requirements you need to fulfill:&lt;/p>
&lt;ul>
&lt;li>You have an existing shoot cluster&lt;/li>
&lt;/ul>
&lt;p>Since you are using the default DNS name, all DNS configuration should already be done and ready.&lt;/p>
&lt;h2 id="issue-a-certificate">Issue a certificate&lt;/h2>
&lt;p>Every X.509 certificate is represented by a Kubernetes custom resource &lt;code>certificate.cert.gardener.cloud&lt;/code> in your cluster. A &lt;code>Certificate&lt;/code> resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener&amp;rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.&lt;/p>
&lt;blockquote>
&lt;p>Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.&lt;/p>
&lt;/blockquote>
&lt;p>Certificates can be requested via 3 resources type&lt;/p>
&lt;ul>
&lt;li>Ingress&lt;/li>
&lt;li>Service (type LoadBalancer)&lt;/li>
&lt;li>certificate (Gardener CRD)&lt;/li>
&lt;/ul>
&lt;p>If either of the first 2 are used, a corresponding &lt;code>Certificate&lt;/code> resource will automatically be created.&lt;/p>
&lt;h3 id="using-an-ingress-resource">Using an ingress Resource&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: networking.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/purpose: managed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/issuer: custom-issuer # optional to specify custom issuer (use namespace/name for shoot issuers)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/follow-cname: &amp;#34;true&amp;#34; # optional, same as spec.followCNAME in certificates&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/secret-labels: &amp;#34;key1=value1,key2=value2&amp;#34; # optional labels for the certificate secret&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/preferred-chain: &amp;#34;chain name&amp;#34; # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-algorithm: ECDSA # optional to specify algorithm for private key, allowed values are &amp;#39;RSA&amp;#39; or &amp;#39;ECDSA&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-size: &amp;#34;384&amp;#34; # optional to specify size of private key, allowed values for RSA are &amp;#34;2048&amp;#34;, &amp;#34;3072&amp;#34;, &amp;#34;4096&amp;#34; and for ECDSA &amp;#34;256&amp;#34; and &amp;#34;384&amp;#34;spec:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tls:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - hosts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Must not exceed 64 characters.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - short.ingress.shoot.project.default-domain.gardener.cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Certificate and private key reside in this secret.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretName: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - host: short.ingress.shoot.project.default-domain.gardener.cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> http:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> paths:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - pathType: Prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: &lt;span style="color:#a31515">&amp;#34;/&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backend:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> number: 8080
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="using-a-service-type-loadbalancer">Using a service type LoadBalancer&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/purpose: managed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Certificate and private key reside in this secret.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/secretname: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># You may add more domains separated by commas (e.g. &amp;#34;service.shoot.project.default-domain.gardener.cloud, amazing.shoot.project.default-domain.gardener.cloud&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns.gardener.cloud/dnsnames: &lt;span style="color:#a31515">&amp;#34;service.shoot.project.default-domain.gardener.cloud&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns.gardener.cloud/ttl: &lt;span style="color:#a31515">&amp;#34;600&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/issuer: custom-issuer # optional to specify custom issuer (use namespace/name for shoot issuers)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/follow-cname: &amp;#34;true&amp;#34; # optional, same as spec.followCNAME in certificates&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/secret-labels: &amp;#34;key1=value1,key2=value2&amp;#34; # optional labels for the certificate secret&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/preferred-chain: &amp;#34;chain name&amp;#34; # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-algorithm: ECDSA # optional to specify algorithm for private key, allowed values are &amp;#39;RSA&amp;#39; or &amp;#39;ECDSA&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-size: &amp;#34;384&amp;#34; # optional to specify size of private key, allowed values for RSA are &amp;#34;2048&amp;#34;, &amp;#34;3072&amp;#34;, &amp;#34;4096&amp;#34; and for ECDSA &amp;#34;256&amp;#34; and &amp;#34;384&amp;#34; name: test-service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ports:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: http
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port: 80
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> protocol: TCP
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> targetPort: 8080
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: LoadBalancer
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="using-the-custom-certificate-resource">Using the custom Certificate resource&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: cert.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: cert-example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> commonName: short.ingress.shoot.project.default-domain.gardener.cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Optionnal if using the default issuer&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: garden
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you&amp;rsquo;re interested in the current progress of your request, you&amp;rsquo;re advised to consult the description, more specifically the &lt;code>status&lt;/code> attribute in case the issuance failed.&lt;/p>
&lt;h2 id="request-a-wildcard-certificate">Request a wildcard certificate&lt;/h2>
&lt;p>In order to avoid the creation of multiples certificates for every single endpoints, you may want to create a wildcard certificate for your shoot&amp;rsquo;s default cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: networking.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/purpose: managed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/commonName: &lt;span style="color:#a31515">&amp;#34;*.ingress.shoot.project.default-domain.gardener.cloud&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tls:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - hosts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - amazing.ingress.shoot.project.default-domain.gardener.cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretName: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - host: amazing.ingress.shoot.project.default-domain.gardener.cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> http:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> paths:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - pathType: Prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: &lt;span style="color:#a31515">&amp;#34;/&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backend:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> number: 8080
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please note that this can also be achived by directly adding an annotation to a Service type LoadBalancer. You could also create a Certificate object with a wildcard domain.&lt;/p>
&lt;h2 id="more-information">More information&lt;/h2>
&lt;p>For more information and more examples about using the certificate extension, please see &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/">Manage certificates with Gardener for public domain&lt;/a>&lt;/p></description></item><item><title>Docs: Manage certificates with Gardener for public domain</title><link>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/</link><pubDate>Wed, 20 Jul 2022 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/</guid><description>
&lt;h1 id="manage-certificates-with-gardener-for-public-domain">Manage certificates with Gardener for public domain&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Dealing with applications on Kubernetes which offer a secure service endpoints (e.g. HTTPS) also require you to enable a
secured communication via SSL/TLS. With the &lt;a href="https://github.com/gardener/gardener-extension-shoot-cert-service">certificate extension&lt;/a> enabled, Gardener can manage commonly trusted X.509 certificate for your application
endpoint. From initially requesting certificate, it also handeles their renewal in time using the free Let&amp;rsquo;s Encrypt API.&lt;/p>
&lt;p>&lt;strong>There are two senarios with which you can use the certificate extension&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>You want to use a certificate for a subdomain the shoot&amp;rsquo;s default DNS (see &lt;code>.spec.dns.domain&lt;/code> of your shoot resource, e.g. &lt;code>short.ingress.shoot.project.default-domain.gardener.cloud&lt;/code>). If this is your case, please see &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_default_domain_cert/">Manage certificates with Gardener for default domain&lt;/a>&lt;/li>
&lt;li>You want to use a certificate for a custom domain. If this is your case, please keep reading this article.&lt;/li>
&lt;/ul>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>Before you start this guide there are a few requirements you need to fulfill:&lt;/p>
&lt;ul>
&lt;li>You have an existing shoot cluster&lt;/li>
&lt;li>Your custom domain is under a &lt;a href="https://www.iana.org/domains/root/db">public top level domain&lt;/a> (e.g. &lt;code>.com&lt;/code>)&lt;/li>
&lt;li>Your custom zone is resolvable with a public resolver via the internet (e.g. &lt;code>8.8.8.8&lt;/code>)&lt;/li>
&lt;li>You have a custom DNS provider configured and working (see &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-dns-service/dns_providers/">&amp;ldquo;DNS Providers&amp;rdquo;&lt;/a>)&lt;/li>
&lt;/ul>
&lt;p>As part of the &lt;a href="https://letsencrypt.org/">Let&amp;rsquo;s Encrypt&lt;/a> &lt;a href="https://tools.ietf.org/html/rfc8555">ACME&lt;/a> challenge validation process, Gardener sets a DNS TXT entry and Let&amp;rsquo;s Encrypt checks if it can both resolve and authenticate it. Therefore, it&amp;rsquo;s important that your DNS-entries are publicly resolvable. You can check this by querying e.g. Googles public DNS server and if it returns an entry your DNS is publicly visible:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># returns the A record for cert-example.example.com using Googles DNS server (8.8.8.8)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dig cert-example.example.com @8.8.8.8 A
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="dns-provider">DNS provider&lt;/h3>
&lt;p>In order to issue certificates for a custom domain you need to specify a DNS provider which is permitted to create DNS records for subdomains of your requested domain in the certificate. For example, if you request a certificate for &lt;code>host.example.com&lt;/code> your DNS provider must be capable of managing subdomains of &lt;code>host.example.com&lt;/code>.&lt;/p>
&lt;p>DNS providers are normally specified in the shoot manifest. To learn more on how to configure one, please see the &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-dns-service/dns_providers/">DNS provider&lt;/a> documentation.&lt;/p>
&lt;h2 id="issue-a-certificate">Issue a certificate&lt;/h2>
&lt;p>Every X.509 certificate is represented by a Kubernetes custom resource &lt;code>certificate.cert.gardener.cloud&lt;/code> in your cluster. A &lt;code>Certificate&lt;/code> resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener&amp;rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.&lt;/p>
&lt;blockquote>
&lt;p>Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.&lt;/p>
&lt;/blockquote>
&lt;p>Certificates can be requested via 3 resources type&lt;/p>
&lt;ul>
&lt;li>Ingress&lt;/li>
&lt;li>Service (type LoadBalancer)&lt;/li>
&lt;li>Gateways (both Istio gateways and from the Gateway API)&lt;/li>
&lt;li>Certificate (Gardener CRD)&lt;/li>
&lt;/ul>
&lt;p>If either of the first 2 are used, a corresponding &lt;code>Certificate&lt;/code> resource will be created automatically.&lt;/p>
&lt;h3 id="using-an-ingress-resource">Using an Ingress Resource&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: networking.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/purpose: managed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Optional but recommended, this is going to create the DNS entry at the same time&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns.gardener.cloud/class: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns.gardener.cloud/ttl: &lt;span style="color:#a31515">&amp;#34;600&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/commonname: &amp;#34;*.example.com&amp;#34; # optional, if not specified the first name from spec.tls[].hosts is used as common name&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/dnsnames: &amp;#34;&amp;#34; # optional, if not specified the names from spec.tls[].hosts are used&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/follow-cname: &amp;#34;true&amp;#34; # optional, same as spec.followCNAME in certificates&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/secret-labels: &amp;#34;key1=value1,key2=value2&amp;#34; # optional labels for the certificate secret&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/issuer: custom-issuer # optional to specify custom issuer (use namespace/name for shoot issuers)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/preferred-chain: &amp;#34;chain name&amp;#34; # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-algorithm: ECDSA # optional to specify algorithm for private key, allowed values are &amp;#39;RSA&amp;#39; or &amp;#39;ECDSA&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-size: &amp;#34;384&amp;#34; # optional to specify size of private key, allowed values for RSA are &amp;#34;2048&amp;#34;, &amp;#34;3072&amp;#34;, &amp;#34;4096&amp;#34; and for ECDSA &amp;#34;256&amp;#34; and &amp;#34;384&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tls:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - hosts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Must not exceed 64 characters.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - amazing.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Certificate and private key reside in this secret.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretName: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - host: amazing.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> http:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> paths:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - pathType: Prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: &lt;span style="color:#a31515">&amp;#34;/&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backend:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> number: 8080
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Replace the &lt;code>hosts&lt;/code> and &lt;code>rules[].host&lt;/code> value again with your own domain and adjust the remaining Ingress attributes in accordance with your deployment (e.g. the above is for an &lt;code>istio&lt;/code> Ingress controller and forwards traffic to a &lt;code>service1&lt;/code> on port 80).&lt;/p>
&lt;h3 id="using-a-service-of-type-loadbalancer">Using a Service of type LoadBalancer&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/secretname: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns.gardener.cloud/dnsnames: example.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns.gardener.cloud/class: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Optional&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns.gardener.cloud/ttl: &lt;span style="color:#a31515">&amp;#34;600&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/commonname: &lt;span style="color:#a31515">&amp;#34;*.example.example.com&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/dnsnames: &lt;span style="color:#a31515">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/follow-cname: &amp;#34;true&amp;#34; # optional, same as spec.followCNAME in certificates&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/secret-labels: &amp;#34;key1=value1,key2=value2&amp;#34; # optional labels for the certificate secret&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/issuer: custom-issuer # optional to specify custom issuer (use namespace/name for shoot issuers)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/preferred-chain: &amp;#34;chain name&amp;#34; # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-algorithm: ECDSA # optional to specify algorithm for private key, allowed values are &amp;#39;RSA&amp;#39; or &amp;#39;ECDSA&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-size: &amp;#34;384&amp;#34; # optional to specify size of private key, allowed values for RSA are &amp;#34;2048&amp;#34;, &amp;#34;3072&amp;#34;, &amp;#34;4096&amp;#34; and for ECDSA &amp;#34;256&amp;#34; and &amp;#34;384&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: test-service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ports:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: http
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port: 80
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> protocol: TCP
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> targetPort: 8080
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: LoadBalancer
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="using-a-gateway-resource">Using a Gateway resource&lt;/h3>
&lt;p>Please see &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/tutorials/istio-gateways/">Istio Gateways&lt;/a> or &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/tutorials/gateway-api-gateways/">Gateway API&lt;/a> for details.&lt;/p>
&lt;h3 id="using-the-custom-certificate-resource">Using the custom Certificate resource&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: cert.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: cert-example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> commonName: amazing.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Optionnal if using the default issuer&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># If delegated domain for DNS01 challenge should be used. This has only an effect if a CNAME record is set for&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># &amp;#39;_acme-challenge.amazing.example.com&amp;#39;.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># For example: If a CNAME record exists &amp;#39;_acme-challenge.amazing.example.com&amp;#39; =&amp;gt; &amp;#39;_acme-challenge.writable.domain.com&amp;#39;,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># the DNS challenge will be written to &amp;#39;_acme-challenge.writable.domain.com&amp;#39;.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#followCNAME: true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># optionally set labels for the secret&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#secretLabels:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># key1: value1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># key2: value2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Optionally specify the preferred certificate chain: if the CA offers multiple certificate chains, prefer the chain with an issuer matching this Subject Common Name. If no match, the default offered chain will be used.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#preferredChain: &amp;#34;ISRG Root X1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Optionally specify algorithm and key size for private key. Allowed algorithms: &amp;#34;RSA&amp;#34; (allowed sizes: 2048, 3072, 4096) and &amp;#34;ECDSA&amp;#34; (allowed sizes: 256, 384)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># If not specified, RSA with 2048 is used.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#privateKey:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># algorithm: ECDSA&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># size: 384&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="supported-attributes">Supported attributes&lt;/h2>
&lt;p>Here is a list of all supported annotations regarding the certificate extension:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Path&lt;/th>
&lt;th>Annotation&lt;/th>
&lt;th>Value&lt;/th>
&lt;th>Required&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>N/A&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/purpose:&lt;/code>&lt;/td>
&lt;td>&lt;code>managed&lt;/code>&lt;/td>
&lt;td>Yes when using annotations&lt;/td>
&lt;td>Flag for Gardener that this specific Ingress or Service requires a certificate&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.commonName&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/commonname:&lt;/code>&lt;/td>
&lt;td>E.g. &amp;ldquo;*.demo.example.com&amp;rdquo; or &lt;br> &amp;ldquo;special.example.com&amp;rdquo;&lt;/td>
&lt;td>Certificate and Ingress : No &lt;br/> Service: Yes, if DNS names unset&lt;/td>
&lt;td>Specifies for which domain the certificate request will be created. If not specified, the names from spec.tls[].hosts are used. This entry must comply with the &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/#Character-Restrictions">64 character&lt;/a> limit.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.dnsNames&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/dnsnames:&lt;/code>&lt;/td>
&lt;td>E.g. &amp;ldquo;special.example.com&amp;rdquo;&lt;/td>
&lt;td>Certificate and Ingress : No &lt;br/> Service: Yes, if common name unset&lt;/td>
&lt;td>Additional domains the certificate should be valid for (Subject Alternative Name). If not specified, the names from spec.tls[].hosts are used. Entries in this list can be longer than 64 characters.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.secretRef.name&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/secretname:&lt;/code>&lt;/td>
&lt;td>&lt;code>any-name&lt;/code>&lt;/td>
&lt;td>Yes for certificate and Service&lt;/td>
&lt;td>Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it&amp;rsquo;ll be created automatically as soon as the certificate has been issued.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.issuerRef.name&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/issuer:&lt;/code>&lt;/td>
&lt;td>E.g. &lt;code>gardener&lt;/code>&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Specifies the issuer you want to use. Only necessary if you request certificates for &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/#Custom-Domains">custom domains&lt;/a>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>N/A&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/revoked:&lt;/code>&lt;/td>
&lt;td>&lt;code>true&lt;/code> otherwise always false&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Use only to revoke a certificate, see &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/#references">reference&lt;/a> for more details&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.followCNAME&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/follow-cname&lt;/code>&lt;/td>
&lt;td>E.g. &lt;code>true&lt;/code>&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Specifies that the usage of a delegated domain for DNS challenges is allowed. Details see &lt;a href="https://github.com/gardener/cert-management#follow-cname">Follow CNAME&lt;/a>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.preferredChain&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/preferred-chain&lt;/code>&lt;/td>
&lt;td>E.g. &lt;code>ISRG Root X1&lt;/code>&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Specifies the Common Name of the issuer for selecting the certificate chain. Details see &lt;a href="https://github.com/gardener/cert-management#preferred-chain">Preferred Chain&lt;/a>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.secretLabels&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/secret-labels&lt;/code>&lt;/td>
&lt;td>for annotation use e.g. &lt;code>key1=value1,key2=value2&lt;/code>&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Specifies labels for the certificate secret.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.privateKey.algorithm&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/private-key-algorithm&lt;/code>&lt;/td>
&lt;td>&lt;code>RSA&lt;/code>, &lt;code>ECDSA&lt;/code>&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Specifies algorithm for private key generation. The default value is depending on configuration of the extension (default of the default is &lt;code>RSA&lt;/code>). You may request a new certificate without privateKey settings to find out the concrete defaults in your Gardener.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.privateKey.size&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/private-key-size&lt;/code>&lt;/td>
&lt;td>&lt;code>&amp;quot;256&amp;quot;&lt;/code>, &lt;code>&amp;quot;384&amp;quot;&lt;/code>, &lt;code>&amp;quot;2048&amp;quot;&lt;/code>, &lt;code>&amp;quot;3072&amp;quot;&lt;/code>, &lt;code>&amp;quot;4096&amp;quot;&lt;/code>&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Specifies size for private key generation. Allowed values for &lt;code>RSA&lt;/code> are &lt;code>2048&lt;/code>, &lt;code>3072&lt;/code>, and &lt;code>4096&lt;/code>. For &lt;code>ECDSA&lt;/code> allowed values are &lt;code>256&lt;/code> and &lt;code>384&lt;/code>. The default values are depending on the configuration of the extension (defaults of the default values are &lt;code>3072&lt;/code> for &lt;code>RSA&lt;/code> and &lt;code>384&lt;/code> for &lt;code>ECDSA&lt;/code> respectively).&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="request-a-wildcard-certificate">Request a wildcard certificate&lt;/h2>
&lt;p>In order to avoid the creation of multiples certificates for every single endpoints, you may want to create a wildcard certificate for your shoot&amp;rsquo;s default cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: networking.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/purpose: managed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/commonName: &lt;span style="color:#a31515">&amp;#34;*.example.com&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tls:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - hosts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - amazing.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretName: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - host: amazing.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> http:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> paths:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - pathType: Prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: &lt;span style="color:#a31515">&amp;#34;/&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backend:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> number: 8080
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please note that this can also be achived by directly adding an annotation to a Service type LoadBalancer. You could also create a Certificate object with a wildcard domain.&lt;/p>
&lt;h2 id="using-a-custom-issuer">Using a custom Issuer&lt;/h2>
&lt;p>Most Gardener deployment with the certification extension enabled have a preconfigured &lt;code>garden&lt;/code> issuer. It is also usually configured to use Let&amp;rsquo;s Encrypt as the certificate provider.&lt;/p>
&lt;p>If you need a custom issuer for a specific cluster, please see &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/custom_shoot_issuer/">Using a custom Issuer&lt;/a>&lt;/p>
&lt;h2 id="quotas">Quotas&lt;/h2>
&lt;p>For security reasons there may be a default quota on the certificate requests per day set globally in the controller
registration of the shoot-cert-service.&lt;/p>
&lt;p>The default quota only applies if there is no explicit quota defined for the issuer itself with the field
&lt;code>requestsPerDayQuota&lt;/code>, e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extensions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - type: shoot-cert-service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: CertConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - email: your-email@example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: custom-issuer &lt;span style="color:#008000"># issuer name must be specified in every custom issuer request, must not be &amp;#34;garden&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: &lt;span style="color:#a31515">&amp;#39;https://acme-v02.api.letsencrypt.org/directory&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requestsPerDayQuota: 10
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="dns-propagation">DNS Propagation&lt;/h2>
&lt;p>As stated before, cert-manager uses the ACME challenge protocol to authenticate that you are the DNS owner for the domain&amp;rsquo;s certificate you are requesting.
This works by creating a DNS TXT record in your DNS provider under &lt;code>_acme-challenge.example.example.com&lt;/code> containing a token to compare with. The TXT record is only applied during the domain validation.
Typically, the record is propagated within a few minutes. But if the record is not visible to the ACME server for any reasons, the certificate request is retried again after several minutes.
This means you may have to wait up to one hour after the propagation problem has been resolved before the certificate request is retried. Take a look in the events with &lt;code>kubectl describe ingress example&lt;/code> for troubleshooting.&lt;/p>
&lt;h2 id="character-restrictions">Character Restrictions&lt;/h2>
&lt;p>Due to restriction of the common name to 64 characters, you may to leave the common name unset in such cases.&lt;/p>
&lt;p>For example, the following request is invalid:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: cert.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: cert-invalid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> commonName: morethan64characters.ingress.shoot.project.default-domain.gardener.cloud
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But it is valid to request a certificate for this domain if you have left the common name unset:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: cert.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: cert-example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dnsNames:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - morethan64characters.ingress.shoot.project.default-domain.gardener.cloud
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/gardener/cert-management">Gardener cert-management&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener-extension-shoot-dns-service">Managing DNS with Gardener&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Using a custom Issuer</title><link>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/custom_shoot_issuer/</link><pubDate>Wed, 20 Jul 2022 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/custom_shoot_issuer/</guid><description>
&lt;h1 id="using-a-custom-issuer">Using a custom Issuer&lt;/h1>
&lt;p>Another possibility to request certificates for custom domains is a dedicated issuer.&lt;/p>
&lt;blockquote>
&lt;p>Note: This is only needed if the default issuer provided by Gardener is restricted to shoot related domains or you are using domain names not visible to public DNS servers. &lt;strong>Which means that your senario most likely doesn&amp;rsquo;t require your to add an issuer&lt;/strong>.&lt;/p>
&lt;/blockquote>
&lt;p>The custom issuers are specified normally in the shoot manifest. If the &lt;code>shootIssuers&lt;/code> feature is enabled, it can alternatively be defined in the shoot cluster.&lt;/p>
&lt;h2 id="custom-issuer-in-the-shoot-manifest">Custom issuer in the shoot manifest&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extensions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - type: shoot-cert-service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: CertConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - email: your-email@example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: custom-issuer &lt;span style="color:#008000"># issuer name must be specified in every custom issuer request, must not be &amp;#34;garden&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: &lt;span style="color:#a31515">&amp;#39;https://acme-v02.api.letsencrypt.org/directory&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> privateKeySecretName: my-privatekey &lt;span style="color:#008000"># referenced resource, the private key must be stored in the secret at `data.privateKey` (optionally, only needed as alternative to auto registration) &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#precheckNameservers: # to provide special set of nameservers to be used for prechecking DNSChallenges for an issuer&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#- dns1.private.company-net:53&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#- dns2.private.company-net:53&amp;#34; &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#shootIssuers:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># if true, allows to specify issuers in the shoot cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#enabled: true &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: my-privatekey
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resourceRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: custom-issuer-privatekey &lt;span style="color:#008000"># name of secret in Gardener project&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you are using an ACME provider for private domains, you may need to change the nameservers used for
checking the availability of the DNS challenge&amp;rsquo;s TXT record before the certificate is requested from the ACME provider.
By default, only public DNS servers may be used for this purpose.
At least one of the &lt;code>precheckNameservers&lt;/code> must be able to resolve the private domain names.&lt;/p>
&lt;h3 id="using-the-custom-issuer">Using the custom issuer&lt;/h3>
&lt;p>To use the custom issuer in a certificate, just specify its name in the spec.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: cert.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: custom-issuer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For source resources like &lt;code>Ingress&lt;/code> or &lt;code>Service&lt;/code> use the &lt;code>cert.gardener.cloud/issuer&lt;/code> annotation.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: networking.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/purpose: managed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/issuer: custom-issuer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="custom-issuer-in-the-shoot-cluster">Custom issuer in the shoot cluster&lt;/h2>
&lt;p>&lt;em>Prerequiste&lt;/em>: The &lt;code>shootIssuers&lt;/code> feature has to be enabled.
It is either enabled globally in the &lt;code>ControllerDeployment&lt;/code> or in the shoot manifest
with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extensions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - type: shoot-cert-service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: CertConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shootIssuers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span> &lt;span style="color:#008000"># if true, allows to specify issuers in the shoot cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Example for specifying an &lt;code>Issuer&lt;/code> resource and its &lt;code>Secret&lt;/code> directly in any
namespace of the shoot cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: cert.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Issuer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-own-issuer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: my-namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> acme:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> domains:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> include:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - my.own.domain.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> email: some.user@my.own.domain.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> privateKeySecretRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-own-issuer-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: my-namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: https://acme-v02.api.letsencrypt.org/directory
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-own-issuer-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: my-namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type: Opaque
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> privateKey: ... &lt;span style="color:#008000"># replace &amp;#39;...&amp;#39; with valus encoded as base64&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="using-the-custom-shoot-issuer">Using the custom shoot issuer&lt;/h3>
&lt;p>To use the custom issuer in a certificate, just specify its name and namespace in the spec.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: cert.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-own-issuer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: my-namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For source resources like &lt;code>Ingress&lt;/code> or &lt;code>Service&lt;/code> use the &lt;code>cert.gardener.cloud/issuer&lt;/code> annotation.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: networking.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/purpose: managed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/issuer: my-namespace/my-own-issuer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Hibernate a Cluster</title><link>https://gardener.cloud/docs/gardener/shoot_hibernate/</link><pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/shoot_hibernate/</guid><description>
&lt;h1 id="hibernate-a-cluster">Hibernate a Cluster&lt;/h1>
&lt;p>Clusters are only needed 24 hours a day if they run productive workload. So whenever you do development in a cluster, or just use it for tests or demo purposes, you can save a lot of money if you scale-down your Kubernetes resources whenever you don&amp;rsquo;t need them. However, scaling them down manually can become time-consuming the more resources you have.&lt;/p>
&lt;p>Gardener offers a clever way to automatically scale-down all resources to zero: cluster hibernation. You can either hibernate a cluster by pushing a button, or by defining a hibernation schedule.&lt;/p>
&lt;blockquote>
&lt;p>To save costs, it&amp;rsquo;s recommended to define a hibernation schedule before the creation of a cluster. You can hibernate your cluster or wake up your cluster manually even if there&amp;rsquo;s a schedule for its hibernation.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/shoot_hibernate/#hibernate-a-cluster">Hibernate a Cluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/shoot_hibernate/#what-is-hibernation">What Is Hibernation?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/shoot_hibernate/#what-isnt-affected-by-the-hibernation">What Isn’t Affected by the Hibernation?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/shoot_hibernate/#hibernate-your-cluster-manually">Hibernate Your Cluster Manually&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/shoot_hibernate/#wake-up-your-cluster-manually">Wake Up Your Cluster Manually&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/shoot_hibernate/#create-a-schedule-to-hibernate-your-cluster">Create a Schedule to Hibernate Your Cluster&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="what-is-hibernation">What Is Hibernation?&lt;/h2>
&lt;p>When a cluster is hibernated, Gardener scales down the worker nodes and the cluster&amp;rsquo;s control plane to free resources at the IaaS provider. This affects:&lt;/p>
&lt;ul>
&lt;li>Your workload, for example, pods, deployments, custom resources.&lt;/li>
&lt;li>The virtual machines running your workload.&lt;/li>
&lt;li>The resources of the control plane of your cluster.&lt;/li>
&lt;/ul>
&lt;h2 id="what-isnt-affected-by-the-hibernation">What Isn’t Affected by the Hibernation?&lt;/h2>
&lt;p>To scale up everything where it was before hibernation, Gardener doesn’t delete state-related information, that is, information stored in persistent volumes. The cluster state as persistent in &lt;code>etcd&lt;/code> is also preserved.&lt;/p>
&lt;h2 id="hibernate-your-cluster-manually">Hibernate Your Cluster Manually&lt;/h2>
&lt;p>The &lt;code>.spec.hibernation.enabled&lt;/code> field specifies whether the cluster needs to be hibernated or not. If the field is set to &lt;code>true&lt;/code>, the cluster&amp;rsquo;s desired state is to be hibernated. If it is set to &lt;code>false&lt;/code> or not specified at all, the cluster&amp;rsquo;s desired state is to be awakened.&lt;/p>
&lt;p>To hibernate your cluster, you can run the following &lt;code>kubectl&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl patch shoot -n $NAMESPACE $SHOOT_NAME -p &amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;hibernation&amp;#34;:{&amp;#34;enabled&amp;#34;: true}}}&amp;#39;
&lt;/code>&lt;/pre>&lt;h2 id="wake-up-your-cluster-manually">Wake Up Your Cluster Manually&lt;/h2>
&lt;p>To wake up your cluster, you can run the following &lt;code>kubectl&lt;/code> command:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl patch shoot -n $NAMESPACE $SHOOT_NAME -p &amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;hibernation&amp;#34;:{&amp;#34;enabled&amp;#34;: false}}}&amp;#39;
&lt;/code>&lt;/pre>&lt;h2 id="create-a-schedule-to-hibernate-your-cluster">Create a Schedule to Hibernate Your Cluster&lt;/h2>
&lt;p>You can specify a hibernation schedule to automatically hibernate/wake up a cluster.&lt;/p>
&lt;p>Let&amp;rsquo;s have a look into the following example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span> hibernation:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> schedules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - start: &lt;span style="color:#a31515">&amp;#34;0 20 * * *&amp;#34;&lt;/span> &lt;span style="color:#008000"># Start hibernation every day at 8PM&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> end: &lt;span style="color:#a31515">&amp;#34;0 6 * * *&amp;#34;&lt;/span> &lt;span style="color:#008000"># Stop hibernation every day at 6AM&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> location: &lt;span style="color:#a31515">&amp;#34;America/Los_Angeles&amp;#34;&lt;/span> &lt;span style="color:#008000"># Specify a location for the cron to run in&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The above section configures a hibernation schedule that hibernates the cluster every day at 08:00 PM and wakes it up at 06:00 AM. The &lt;code>start&lt;/code> or &lt;code>end&lt;/code> fields can be omitted, though at least one of them has to be specified. Hence, it is possible to configure a hibernation schedule that only hibernates or wakes up a cluster. The &lt;code>location&lt;/code> field is the time location used to evaluate the cron expressions.&lt;/p></description></item><item><title>Docs: 01 Multi Node Etcd Clusters</title><link>https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/</guid><description>
&lt;h1 id="multi-node-etcd-cluster-instances-via-etcd-druid">Multi-node etcd cluster instances via etcd-druid&lt;/h1>
&lt;p>This document proposes an approach (along with some alternatives) to support provisioning and management of multi-node etcd cluster instances via &lt;a href="https://github.com/gardener/etcd-druid">etcd-druid&lt;/a> and &lt;a href="https://github.com/gardener/etcd-backup-restore">etcd-backup-restore&lt;/a>.&lt;/p>
&lt;h2 id="content">Content&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#multi-node-etcd-cluster-instances-via-etcd-druid">Multi-node etcd cluster instances via etcd-druid&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#content">Content&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#goal">Goal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#background-and-motivation">Background and Motivation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#single-node-etcd-cluster">Single-node etcd cluster&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#multi-node-etcd-cluster">Multi-node etcd-cluster&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#dynamic-multi-node-etcd-cluster">Dynamic multi-node etcd cluster&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#prior-art">Prior Art&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-operator-from-coreos">ETCD Operator from CoreOS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcdadm-from-kubernetes-sigs">etcdadm from kubernetes-sigs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-cluster-operator-from-improbable-engineering">Etcd Cluster Operator from Improbable-Engineering&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#general-approach-to-etcd-cluster-management">General Approach to ETCD Cluster Management&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping">Bootstrapping&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#assumptions">Assumptions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">Adding a new member to an etcd cluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note">Note&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative">Alternative&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#managing-failures">Managing Failures&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">Removing an existing member from an etcd cluster&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">Restarting an existing member of an etcd cluster&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">Recovering an etcd cluster from failure of majority of members&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#kubernetes-context">Kubernetes Context&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-1">Alternative&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-configuration">ETCD Configuration&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-2">Alternative&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#data-persistence">Data Persistence&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent">Persistent&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">Ephemeral&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#disk">Disk&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#in-memory">In-memory&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#how-to-detect-if-valid-metadata-exists-in-an-etcd-member">How to detect if valid metadata exists in an etcd member&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommendation">Recommendation&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#how-to-detect-if-valid-data-exists-in-an-etcd-member">How to detect if valid data exists in an etcd member&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommendation-1">Recommendation&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#separating-peer-and-client-traffic">Separating peer and client traffic&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#cutting-off-client-requests">Cutting off client requests&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector">Manipulating Client Service podSelector&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#health-check">Health Check&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure">Backup Failure&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-3">Alternative&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">Status&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#members">Members&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note-1">Note&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-name-as-the-key">Member name as the key&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">Member Leases&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">Conditions&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#clustersize">ClusterSize&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-4">Alternative&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#decision-table-for-etcd-druid-based-on-the-status">Decision table for etcd-druid based on the status&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#1-pink-of-health">1. Pink of health&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#2-member-status-is-out-of-sync-with-their-leases">2. Member status is out of sync with their leases&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-1">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-1">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#3-all-members-are-ready-but-allmembersready-condition-is-stale">3. All members are &lt;code>Ready&lt;/code> but &lt;code>AllMembersReady&lt;/code> condition is stale&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-2">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-2">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#4-not-all-members-are-ready-but-allmembersready-condition-is-stale">4. Not all members are &lt;code>Ready&lt;/code> but &lt;code>AllMembersReady&lt;/code> condition is stale&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-3">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-3">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#5-majority-members-are-ready-but-ready-condition-is-stale">5. Majority members are &lt;code>Ready&lt;/code> but &lt;code>Ready&lt;/code> condition is stale&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-4">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-4">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#6-majority-members-are-notready-but-ready-condition-is-stale">6. Majority members are &lt;code>NotReady&lt;/code> but &lt;code>Ready&lt;/code> condition is stale&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-5">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-5">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#7-some-members-have-been-in-unknown-status-for-a-while">7. Some members have been in &lt;code>Unknown&lt;/code> status for a while&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-6">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-6">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#8-some-member-pods-are-not-ready-but-have-not-had-the-chance-to-update-their-status">8. Some member pods are not &lt;code>Ready&lt;/code> but have not had the chance to update their status&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-7">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-7">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#9-quorate-cluster-with-a-minority-of-members-notready">9. Quorate cluster with a minority of members &lt;code>NotReady&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-8">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-8">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#10-quorum-lost-with-a-majority-of-members-notready">10. Quorum lost with a majority of members &lt;code>NotReady&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-9">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-9">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#11-scale-up-of-a-healthy-cluster">11. Scale up of a healthy cluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-10">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-10">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#12-scale-down-of-a-healthy-cluster">12. Scale down of a healthy cluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-11">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-11">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status">13. Superfluous member entries in &lt;code>Etcd&lt;/code> status&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-12">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#decision-table-for-etcd-backup-restore-during-initialization">Decision table for etcd-backup-restore during initialization&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#1-first-member-during-bootstrap-of-a-fresh-etcd-cluster">1. First member during bootstrap of a fresh etcd cluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-13">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-13">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#2-addition-of-a-new-following-member-during-bootstrap-of-a-fresh-etcd-cluster">2. Addition of a new following member during bootstrap of a fresh etcd cluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-14">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-14">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#3-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-and-data">3. Restart of an existing member of a quorate cluster with valid metadata and data&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-15">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-15">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#4-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-but-without-valid-data">4. Restart of an existing member of a quorate cluster with valid metadata but without valid data&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-16">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-16">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#5-restart-of-an-existing-member-of-a-quorate-cluster-without-valid-metadata">5. Restart of an existing member of a quorate cluster without valid metadata&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-17">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-17">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#6-restart-of-an-existing-member-of-a-non-quorate-cluster-with-valid-metadata-and-data">6. Restart of an existing member of a non-quorate cluster with valid metadata and data&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-18">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-18">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#7-restart-of-the-first-member-of-a-non-quorate-cluster-without-valid-data">7. Restart of the first member of a non-quorate cluster without valid data&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-19">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-19">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#8-restart-of-a-following-member-of-a-non-quorate-cluster-without-valid-data">8. Restart of a following member of a non-quorate cluster without valid data&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-20">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-20">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup">Backup&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#leading-etcd-main-containers-sidecar-is-the-backup-leader">Leading ETCD main container’s sidecar is the backup leader&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#independent-leader-election-between-backup-restore-sidecars">Independent leader election between backup-restore sidecars&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#history-compaction">History Compaction&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#defragmentation">Defragmentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-in-etcd-backup-restore">Work-flows in etcd-backup-restore&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-independent-of-leader-election-in-all-members">Work-flows independent of leader election in all members&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member">Work-flows only on the leading member&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#high-availability">High Availability&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#zonal-cluster---single-availability-zone">Zonal Cluster - Single Availability Zone&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-5">Alternative&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#regional-cluster---multiple-availability-zones">Regional Cluster - Multiple Availability Zones&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-6">Alternative&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#poddisruptionbudget">PodDisruptionBudget&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#rolling-updates-to-etcd-members">Rolling updates to etcd members&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#follow-up">Follow Up&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral-volumes">Ephemeral Volumes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#shoot-control-plane-migration">Shoot Control-Plane Migration&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#performance-impact-of-multi-node-etcd-clusters">Performance impact of multi-node etcd clusters&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#metrics-dashboards-and-alerts">Metrics, Dashboards and Alerts&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#costs">Costs&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#future-work">Future Work&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#gardener-ring">Gardener Ring&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#autonomous-shoot-clusters">Autonomous Shoot Clusters&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#optimization-of-recovery-from-non-quorate-cluster-with-some-member-containing-valid-data">Optimization of recovery from non-quorate cluster with some member containing valid data&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#optimization-of-rolling-updates-to-unhealthy-etcd-clusters">Optimization of rolling updates to unhealthy etcd clusters&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="goal">Goal&lt;/h2>
&lt;ul>
&lt;li>Enhance etcd-druid and etcd-backup-restore to support provisioning and management of multi-node etcd cluster instances within a single Kubernetes cluster.&lt;/li>
&lt;li>The etcd CRD interface should be simple to use. It should preferably work with just setting the &lt;code>spec.replicas&lt;/code> field to the desired value and should not require any more configuration in the CRD than currently required for the single-node etcd instances. The &lt;code>spec.replicas&lt;/code> field is part of the &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource">&lt;code>scale&lt;/code> sub-resource&lt;/a> &lt;a href="https://github.com/gardener/etcd-druid/blob/eaf04a2d0e6c7a4f2c8c220182b7a141aabfc70b/api/v1alpha1/etcd_types.go#L299">implementation&lt;/a> in &lt;code>Etcd&lt;/code> CRD.&lt;/li>
&lt;li>The single-node and multi-node scenarios must be automatically identified and managed by &lt;code>etcd-druid&lt;/code> and &lt;code>etcd-backup-restore&lt;/code>.&lt;/li>
&lt;li>The etcd clusters (single-node or multi-node) managed by &lt;code>etcd-druid&lt;/code> and &lt;code>etcd-backup-restore&lt;/code> must automatically recover from failures (even quorum loss) and disaster (e.g. etcd member persistence/data loss) as much as possible.&lt;/li>
&lt;li>It must be possible to dynamically scale an etcd cluster horizontally (even between single-node and multi-node scenarios) by simply scaling the &lt;code>Etcd&lt;/code> scale sub-resource.&lt;/li>
&lt;li>It must be possible to (optionally) schedule the individual members of an etcd clusters on different nodes or even infrastructure availability zones (within the hosting Kubernetes cluster).&lt;/li>
&lt;/ul>
&lt;p>Though this proposal tries to cover most aspects related to single-node and multi-node etcd clusters, there are some more points that are not goals for this document but are still in the scope of either etcd-druid/etcd-backup-restore and/or gardener.
In such cases, a high-level description of how they can be &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#future-work">addressed in the future&lt;/a> are mentioned at the end of the document.&lt;/p>
&lt;h2 id="background-and-motivation">Background and Motivation&lt;/h2>
&lt;h3 id="single-node-etcd-cluster">Single-node etcd cluster&lt;/h3>
&lt;p>At present, &lt;code>etcd-druid&lt;/code> supports only single-node etcd cluster instances.
The advantages of this approach are given below.&lt;/p>
&lt;ul>
&lt;li>The problem domain is smaller.
There are no leader election and quorum related issues to be handled.
It is simpler to setup and manage a single-node etcd cluster.&lt;/li>
&lt;li>Single-node etcd clusters instances have &lt;a href="https://etcd.io/docs/v2/admin_guide/#optimal-cluster-size">less request latency&lt;/a> than multi-node etcd clusters because there is no requirement to replicate the changes to the other members before committing the changes.&lt;/li>
&lt;li>&lt;code>etcd-druid&lt;/code> provisions etcd cluster instances as pods (actually as &lt;code>statefulsets&lt;/code>) in a Kubernetes cluster and Kubernetes is quick (&amp;lt;&lt;code>20s&lt;/code>) to restart container/pods if they go down.&lt;/li>
&lt;li>Also, &lt;code>etcd-druid&lt;/code> is currently only used by gardener to provision etcd clusters to act as back-ends for Kubernetes control-planes and Kubernetes control-plane components (&lt;code>kube-apiserver&lt;/code>, &lt;code>kubelet&lt;/code>, &lt;code>kube-controller-manager&lt;/code>, &lt;code>kube-scheduler&lt;/code> etc.) can tolerate etcd going down and recover when it comes back up.&lt;/li>
&lt;li>Single-node etcd clusters incur less cost (CPU, memory and storage)&lt;/li>
&lt;li>It is easy to cut-off client requests if backups fail by using &lt;a href="https://github.com/gardener/etcd-druid/blob/eaf04a2d0e6c7a4f2c8c220182b7a141aabfc70b/charts/etcd/templates/etcd-statefulset.yaml#L54-L62">&lt;code>readinessProbe&lt;/code> on the &lt;code>etcd-backup-restore&lt;/code> healthz endpoint&lt;/a> to minimize the gap between the latest revision and the backup revision.&lt;/li>
&lt;/ul>
&lt;p>The disadvantages of using single-node etcd clusters are given below.&lt;/p>
&lt;ul>
&lt;li>The &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/master/doc/proposals/design.md#workflow">database verification&lt;/a> step by &lt;code>etcd-backup-restore&lt;/code> can introduce additional delays whenever etcd container/pod restarts (in total ~&lt;code>20-25s&lt;/code>).
This can be much longer if a database restoration is required.
Especially, if there are incremental snapshots that need to be replayed (this can be mitigated by &lt;a href="https://github.com/gardener/etcd-druid/issues/88">compacting the incremental snapshots in the background&lt;/a>).&lt;/li>
&lt;li>Kubernetes control-plane components can go into &lt;code>CrashloopBackoff&lt;/code> if etcd is down for some time. This is mitigated by the &lt;a href="https://github.com/gardener/gardener/blob/9e4a809008fb122a6d02045adc08b9c98b5cd564/charts/seed-bootstrap/charts/dependency-watchdog/templates/endpoint-configmap.yaml#L29-L41">dependency-watchdog&lt;/a>.
But Kubernetes control-plane components require a lot of resources and create a lot of load on the etcd cluster and the apiserver when they come out of &lt;code>CrashloopBackoff&lt;/code>.
Especially, in medium or large sized clusters (&amp;gt; &lt;code>20&lt;/code> nodes).&lt;/li>
&lt;li>Maintenance operations such as updates to etcd (and updates to &lt;code>etcd-druid&lt;/code> of &lt;code>etcd-backup-restore&lt;/code>), rolling updates to the nodes of the underlying Kubernetes cluster and vertical scaling of etcd pods are disruptive because they cause etcd pods to be restarted.
The vertical scaling of etcd pods is somewhat mitigated during scale down by doing it only during the target clusters&amp;rsquo; &lt;a href="https://github.com/gardener/gardener/blob/86aa30dfd095f7960ae50a81d2cee27c0d18408b/charts/seed-controlplane/charts/etcd/templates/etcd-hvpa.yaml#L53">maintenance window&lt;/a>.
But scale up is still disruptive.&lt;/li>
&lt;li>We currently use some form of elastic storage (via &lt;code>persistentvolumeclaims&lt;/code>) for storing which have some upper-bounds on the I/O latency and throughput. This can be potentially be a problem for large clusters (&amp;gt; &lt;code>220&lt;/code> nodes).
Also, some cloud providers (e.g. Azure) take a long time to attach/detach volumes to and from machines which increases the down time to the Kubernetes components that depend on etcd.
It is difficult to use ephemeral/local storage (to achieve better latency/throughput as well as to circumvent volume attachment/detachment) for single-node etcd cluster instances.&lt;/li>
&lt;/ul>
&lt;h3 id="multi-node-etcd-cluster">Multi-node etcd-cluster&lt;/h3>
&lt;p>The advantages of introducing support for multi-node etcd clusters via &lt;code>etcd-druid&lt;/code> are below.&lt;/p>
&lt;ul>
&lt;li>Multi-node etcd cluster is highly-available. It can tolerate disruption to individual etcd pods as long as the quorum is not lost (i.e. more than half the etcd member pods are healthy and ready).&lt;/li>
&lt;li>Maintenance operations such as updates to etcd (and updates to &lt;code>etcd-druid&lt;/code> of &lt;code>etcd-backup-restore&lt;/code>), rolling updates to the nodes of the underlying Kubernetes cluster and vertical scaling of etcd pods can be done non-disruptively by &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/">respecting &lt;code>poddisruptionbudgets&lt;/code>&lt;/a> for the various multi-node etcd cluster instances hosted on that cluster.&lt;/li>
&lt;li>Kubernetes control-plane components do not see any etcd cluster downtime unless quorum is lost (which is expected to be lot less frequent than current frequency of etcd container/pod restarts).&lt;/li>
&lt;li>We can consider using ephemeral/local storage for multi-node etcd cluster instances because individual member restarts can afford to take time to restore from backup before (re)joining the etcd cluster because the remaining members serve the requests in the meantime.&lt;/li>
&lt;li>High-availability across availability zones is also possible by specifying (anti)affinity for the etcd pods (possibly via &lt;a href="https://github.com/gardener/kupid">&lt;code>kupid&lt;/code>&lt;/a>).&lt;/li>
&lt;/ul>
&lt;p>Some disadvantages of using multi-node etcd clusters due to which it might still be desirable, in some cases, to continue to use single-node etcd cluster instances in the gardener context are given below.&lt;/p>
&lt;ul>
&lt;li>Multi-node etcd cluster instances are more complex to manage.
The problem domain is larger including the following.
&lt;ul>
&lt;li>Leader election&lt;/li>
&lt;li>Quorum loss&lt;/li>
&lt;li>Managing rolling changes&lt;/li>
&lt;li>Backups to be taken from only the leading member.&lt;/li>
&lt;li>More complex to cut-off client requests if backups fail to minimize the gap between the latest revision and the backup revision is under control.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Multi-node etcd cluster instances incur more cost (CPU, memory and storage).&lt;/li>
&lt;/ul>
&lt;h3 id="dynamic-multi-node-etcd-cluster">Dynamic multi-node etcd cluster&lt;/h3>
&lt;p>Though it is &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#non-goal">not part of this proposal&lt;/a>, it is conceivable to convert a single-node etcd cluster into a multi-node etcd cluster temporarily to perform some disruptive operation (etcd, &lt;code>etcd-backup-restore&lt;/code> or &lt;code>etcd-druid&lt;/code> updates, etcd cluster vertical scaling and perhaps even node rollout) and convert it back to a single-node etcd cluster once the disruptive operation has been completed. This will necessarily still involve a down-time because scaling from a single-node etcd cluster to a three-node etcd cluster will involve etcd pod restarts, it is still probable that it can be managed with a shorter down time than we see at present for single-node etcd clusters (on the other hand, converting a three-node etcd cluster to five node etcd cluster can be non-disruptive).&lt;/p>
&lt;p>This is &lt;em>definitely not&lt;/em> to argue in favour of such a dynamic approach in all cases (eventually, if/when dynamic multi-node etcd clusters are supported). On the contrary, it makes sense to make use of &lt;em>static&lt;/em> (fixed in size) multi-node etcd clusters for production scenarios because of the high-availability.&lt;/p>
&lt;h2 id="prior-art">Prior Art&lt;/h2>
&lt;h3 id="etcd-operator-from-coreos">ETCD Operator from CoreOS&lt;/h3>
&lt;blockquote>
&lt;p>&lt;a href="https://github.com/coreos/etcd-operator#etcd-operator">etcd operator&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/coreos/etcd-operator#project-status-archived">Project status: archived&lt;/a>&lt;/p>
&lt;p>This project is no longer actively developed or maintained. The project exists here for historical reference. If you are interested in the future of the project and taking over stewardship, please contact &lt;a href="mailto:etcd-dev@googlegroups.com">etcd-dev@googlegroups.com&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;h3 id="etcdadm-from-kubernetes-sigs">etcdadm from kubernetes-sigs&lt;/h3>
&lt;blockquote>
&lt;p>&lt;a href="https://github.com/kubernetes-sigs/etcdadm#etcdadm">etcdadm&lt;/a> is a command-line tool for operating an etcd cluster. It makes it easy to create a new cluster, add a member to, or remove a member from an existing cluster. Its user experience is inspired by kubeadm.&lt;/p>
&lt;/blockquote>
&lt;p>It is a tool more tailored for manual command-line based management of etcd clusters with no API&amp;rsquo;s.
It also makes no assumptions about the underlying platform on which the etcd clusters are provisioned and hence, doesn&amp;rsquo;t leverage any capabilities of Kubernetes.&lt;/p>
&lt;h3 id="etcd-cluster-operator-from-improbable-engineering">Etcd Cluster Operator from Improbable-Engineering&lt;/h3>
&lt;blockquote>
&lt;p>&lt;a href="https://github.com/improbable-eng/etcd-cluster-operator">Etcd Cluster Operator&lt;/a>&lt;/p>
&lt;p>Etcd Cluster Operator is an Operator for automating the creation and management of etcd inside of Kubernetes. It provides a custom resource definition (CRD) based API to define etcd clusters with Kubernetes resources, and enable management with native Kubernetes tooling._&lt;/p>
&lt;/blockquote>
&lt;p>Out of all the alternatives listed here, this one seems to be the only possible viable alternative.
Parts of its design/implementations are similar to some of the approaches mentioned in this proposal. However, we still don&amp;rsquo;t propose to use it as -&lt;/p>
&lt;ol>
&lt;li>The project is still in early phase and is not mature enough to be consumed as is in productive scenarios of ours.&lt;/li>
&lt;li>The resotration part is completely different which makes it difficult to adopt as-is and requries lot of re-work with the current restoration semantics with etcd-backup-restore making the usage counter-productive.&lt;/li>
&lt;/ol>
&lt;h2 id="general-approach-to-etcd-cluster-management">General Approach to ETCD Cluster Management&lt;/h2>
&lt;h3 id="bootstrapping">Bootstrapping&lt;/h3>
&lt;p>There are three ways to bootstrap an etcd cluster which are &lt;a href="https://etcd.io/docs/v3.4.0/op-guide/clustering/#static">static&lt;/a>, &lt;a href="https://etcd.io/docs/v3.4.0/op-guide/clustering/#etcd-discovery">etcd discovery&lt;/a> and &lt;a href="https://etcd.io/docs/v3.4.0/op-guide/clustering/#dns-discovery">DNS discovery&lt;/a>.
Out of these, the static way is the simplest (and probably faster to bootstrap the cluster) and has the least external dependencies.
Hence, it is preferred in this proposal.
But it requires that the initial (during bootstrapping) etcd cluster size (number of members) is already known before bootstrapping and that all of the members are already addressable (DNS,IP,TLS etc.).
Such information needs to be passed to the individual members during startup using the following static configuration.&lt;/p>
&lt;ul>
&lt;li>ETCD_INITIAL_CLUSTER
&lt;ul>
&lt;li>The list of peer URLs including all the members. This must be the same as the advertised peer URLs configuration. This can also be passed as &lt;code>initial-cluster&lt;/code> flag to etcd.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ETCD_INITIAL_CLUSTER_STATE
&lt;ul>
&lt;li>This should be set to &lt;code>new&lt;/code> while bootstrapping an etcd cluster.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ETCD_INITIAL_CLUSTER_TOKEN
&lt;ul>
&lt;li>This is a token to distinguish the etcd cluster from any other etcd cluster in the same network.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="assumptions">Assumptions&lt;/h4>
&lt;ul>
&lt;li>ETCD_INITIAL_CLUSTER can use DNS instead of IP addresses. We need to verify this by deleting a pod (as against scaling down the statefulset) to ensure that the pod IP changes and see if the recreated pod (by the statefulset controller) re-joins the cluster automatically.&lt;/li>
&lt;li>DNS for the individual members is known or computable. This is true in the case of etcd-druid setting up an etcd cluster using a single statefulset. But it may not necessarily be true in other cases (multiple statefulset per etcd cluster or deployments instead of statefulsets or in the case of etcd cluster with members distributed across more than one Kubernetes cluster.&lt;/li>
&lt;/ul>
&lt;h3 id="adding-a-new-member-to-an-etcd-cluster">Adding a new member to an etcd cluster&lt;/h3>
&lt;p>A &lt;a href="https://etcd.io/docs/v3.4.0/op-guide/runtime-configuration/#add-a-new-member">new member can be added&lt;/a> to an existing etcd cluster instance using the following steps.&lt;/p>
&lt;ol>
&lt;li>If the latest backup snapshot exists, restore the member&amp;rsquo;s etcd data to the latest backup snapshot. This can reduce the load on the leader to bring the new member up to date when it joins the cluster.
&lt;ol>
&lt;li>If the latest backup snapshot doesn&amp;rsquo;t exist or if the latest backup snapshot is not accessible (please see &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure">backup failure&lt;/a>) and if the cluster itself is quorate, then the new member can be started with an empty data. But this will will be suboptimal because the new member will fetch all the data from the leading member to get up-to-date.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>The cluster is informed that a new member is being added using the &lt;a href="https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L40">&lt;code>MemberAdd&lt;/code> API&lt;/a> including information like the member name and its advertised peer URLs.&lt;/li>
&lt;li>The new etcd member is then started with &lt;code>ETCD_INITIAL_CLUSTER_STATE=existing&lt;/code> apart from other required configuration.&lt;/li>
&lt;/ol>
&lt;p>This proposal recommends this approach.&lt;/p>
&lt;h4 id="note">Note&lt;/h4>
&lt;ul>
&lt;li>If there are incremental snapshots (taken by &lt;code>etcd-backup-restore&lt;/code>), they cannot be applied because that requires the member to be started in isolation without joining the cluster which is not possible.
This is acceptable if the amount of incremental snapshots are managed to be relatively small.
This adds one more reason to increase the priority of the issue of &lt;a href="https://github.com/gardener/etcd-druid/issues/88">incremental snapshot compaction&lt;/a>.&lt;/li>
&lt;li>There is a time window, between the &lt;code>MemberAdd&lt;/code> call and the new member joining the cluster and getting up to date, where the cluster is &lt;a href="https://etcd.io/docs/v3.3.12/learning/learner/#background">vulnerable to leader elections which could be disruptive&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h4 id="alternative">Alternative&lt;/h4>
&lt;p>With &lt;code>v3.4&lt;/code>, the new &lt;a href="https://etcd.io/docs/v3.3.12/learning/learner/#raft-learner">raft learner approach&lt;/a> can be used to mitigate some of the possible disruptions mentioned &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note">above&lt;/a>.
Then the steps will be as follows.&lt;/p>
&lt;ol>
&lt;li>If the latest backup snapshot exists, restore the member&amp;rsquo;s etcd data to the latest backup snapshot. This can reduce the load on the leader to bring the new member up to date when it joins the cluster.&lt;/li>
&lt;li>The cluster is informed that a new member is being added using the &lt;a href="https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L43">&lt;code>MemberAddAsLearner&lt;/code> API&lt;/a> including information like the member name and its advertised peer URLs.&lt;/li>
&lt;li>The new etcd member is then started with &lt;code>ETCD_INITIAL_CLUSTER_STATE=existing&lt;/code> apart from other required configuration.&lt;/li>
&lt;li>Once the new member (learner) is up to date, it can be promoted to a full voting member by using the &lt;a href="https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L52">&lt;code>MemberPromote&lt;/code> API&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>This approach is new and involves more steps and is not recommended in this proposal.
It can be considered in future enhancements.&lt;/p>
&lt;h3 id="managing-failures">Managing Failures&lt;/h3>
&lt;p>A multi-node etcd cluster may face failures of &lt;a href="https://etcd.io/docs/v3.1.12/op-guide/failures/">diffent kinds&lt;/a> during its life-cycle.
The actions that need to be taken to manage these failures depend on the failure mode.&lt;/p>
&lt;h4 id="removing-an-existing-member-from-an-etcd-cluster">Removing an existing member from an etcd cluster&lt;/h4>
&lt;p>If a member of an etcd cluster becomes unhealthy, it must be explicitly removed from the etcd cluster, as soon as possible.
This can be done by using the &lt;a href="https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L46">&lt;code>MemberRemove&lt;/code> API&lt;/a>.
This ensures that only healthy members participate as voting members.&lt;/p>
&lt;p>A member of an etcd cluster may be removed not just for managing failures but also for other reasons such as -&lt;/p>
&lt;ul>
&lt;li>The etcd cluster is being scaled down. I.e. the cluster size is being reduced&lt;/li>
&lt;li>An existing member is being replaced by a new one for some reason (e.g. upgrades)&lt;/li>
&lt;/ul>
&lt;p>If the majority of the members of the etcd cluster are healthy and the member that is unhealthy/being removed happens to be the &lt;a href="https://etcd.io/docs/v3.1.12/op-guide/failures/#leader-failure">leader&lt;/a> at that moment then the etcd cluster will automatically elect a new leader.
But if only a minority of etcd clusters are healthy after removing the member then the the cluster will no longer be &lt;a href="https://etcd.io/docs/v3.1.12/op-guide/failures/#majority-failure">quorate&lt;/a> and will stop accepting write requests.
Such an etcd cluster needs to be recovered via some kind of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">disaster-recovery&lt;/a>.&lt;/p>
&lt;h4 id="restarting-an-existing-member-of-an-etcd-cluster">Restarting an existing member of an etcd cluster&lt;/h4>
&lt;p>If the existing member of an etcd cluster restarts and retains an uncorrupted data directory after the restart, then it can simply re-join the cluster as an existing member without any API calls or configuration changes.
This is because the relevant metadata (including member ID and cluster ID) are &lt;a href="https://etcd.io/docs/v2/admin_guide/#lifecycle">maintained in the write ahead logs&lt;/a>.
However, if it doesn&amp;rsquo;t retain an uncorrupted data directory after the restart, then it must first be &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">removed&lt;/a> and &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">added&lt;/a> as a new member.&lt;/p>
&lt;h4 id="recovering-an-etcd-cluster-from-failure-of-majority-of-members">Recovering an etcd cluster from failure of majority of members&lt;/h4>
&lt;p>If a majority of members of an etcd cluster fail but if they retain their uncorrupted data directory then they can be simply restarted and they will re-form the existing etcd cluster when they come up.
However, if they do not retain their uncorrupted data directory, then the etcd cluster must be &lt;a href="https://etcd.io/docs/v3.4.0/op-guide/recovery/#restoring-a-cluster">recovered from latest snapshot in the backup&lt;/a>.
This is very similar to &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping">bootstrapping&lt;/a> with the additional initial step of restoring the latest snapshot in each of the members.
However, the same &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note">limitation&lt;/a> about incremental snapshots, as in the case of adding a new member, applies here.
But unlike in the case of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">adding a new member&lt;/a>, not applying incremental snapshots is not acceptable in the case of etcd cluster recovery.
Hence, if incremental snapshots are required to be applied, the etcd cluster must be &lt;a href="https://etcd.io/docs/v3.4.0/op-guide/runtime-configuration/#restart-cluster-from-majority-failure">recovered&lt;/a> in the following steps.&lt;/p>
&lt;ol>
&lt;li>Restore a new single-member cluster using the latest snapshot.&lt;/li>
&lt;li>Apply incremental snapshots on the single-member cluster.&lt;/li>
&lt;li>Take a full snapshot which can now be used while adding the remaining members.&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">Add&lt;/a> new members using the latest snapshot created in the step above.&lt;/li>
&lt;/ol>
&lt;h2 id="kubernetes-context">Kubernetes Context&lt;/h2>
&lt;ul>
&lt;li>Users will provision an etcd cluster in a Kubernetes cluster by creating an etcd CRD resource instance.&lt;/li>
&lt;li>A multi-node etcd cluster is indicated if the &lt;code>spec.replicas&lt;/code> field is set to any value greater than 1. The etcd-druid will add validation to ensure that the &lt;code>spec.replicas&lt;/code> value is an odd number according to the requirements of etcd.&lt;/li>
&lt;li>The etcd-druid controller will provision a statefulset with the etcd main container and the etcd-backup-restore sidecar container. It will pass on the &lt;code>spec.replicas&lt;/code> field from the etcd resource to the statefulset. It will also supply the right pre-computed configuration to both the containers.&lt;/li>
&lt;li>The statefulset controller will create the pods based on the pod template in the statefulset spec and these individual pods will be the members that form the etcd cluster.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://gardener.cloud/__resources/01-multi-node-etcd_1afcbd.png" alt="Component diagram">&lt;/p>
&lt;p>This approach makes it possible to satisfy the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#assumption">assumption&lt;/a> that the DNS for the individual members of the etcd cluster must be known/computable.
This can be achieved by using a &lt;code>headless&lt;/code> service (along with the statefulset) for each etcd cluster instance.
Then we can address individual pods/etcd members via the predictable DNS name of &lt;code>&amp;lt;statefulset_name&amp;gt;-{0|1|2|3|…|n}.&amp;lt;headless_service_name&amp;gt;&lt;/code> from within the Kubernetes namespace (or from outside the Kubernetes namespace by appending &lt;code>.&amp;lt;namespace&amp;gt;.svc.&amp;lt;cluster_domain&amp;gt; suffix)&lt;/code>.
The etcd-druid controller can compute the above configurations automatically based on the &lt;code>spec.replicas&lt;/code> in the etcd resource.&lt;/p>
&lt;p>This proposal recommends this approach.&lt;/p>
&lt;h4 id="alternative-1">Alternative&lt;/h4>
&lt;p>One statefulset is used for each member (instead of one statefulset for all members).
While this approach gives a flexibility to have different pod specifications for the individual members, it makes managing the individual members (e.g. rolling updates) more complicated.
Hence, this approach is not recommended.&lt;/p>
&lt;h2 id="etcd-configuration">ETCD Configuration&lt;/h2>
&lt;p>As mentioned in the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#general-approach-to-etcd-cluster-management">general approach section&lt;/a>, there are differences in the configuration that needs to be passed to individual members of an etcd cluster in different scenarios such as &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping">bootstrapping&lt;/a>, &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">adding&lt;/a> a new member, &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">removing&lt;/a> a member, &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">restarting&lt;/a> an existing member etc.
Managing such differences in configuration for individual pods of a statefulset is tricky in the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#kubernetes-context">recommended approach&lt;/a> of using a single statefulset to manage all the member pods of an etcd cluster.
This is because statefulset uses the same pod template for all its pods.&lt;/p>
&lt;p>The recommendation is for &lt;code>etcd-druid&lt;/code> to provision the base configuration template in a &lt;code>ConfigMap&lt;/code> which is passed to all the pods via the pod template in the &lt;code>StatefulSet&lt;/code>.
The &lt;code>initialization&lt;/code> flow of &lt;code>etcd-backup-restore&lt;/code> (which is invoked every time the etcd container is (re)started) is then enhanced to generate the customized etcd configuration for the corresponding member pod (in a shared &lt;em>volume&lt;/em> between etcd and the backup-restore containers) based on the supplied template configuration.
This will require that &lt;code>etcd-backup-restore&lt;/code> will have to have a mechanism to detect which scenario listed &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-configuration">above&lt;/a> applies during any given member container/pod restart.&lt;/p>
&lt;h3 id="alternative-2">Alternative&lt;/h3>
&lt;p>As mentioned &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-1">above&lt;/a>, one statefulset is used for each member of the etcd cluster.
Then different configuration (generated directly by &lt;code>etcd-druid&lt;/code>) can be passed in the pod templates of the different statefulsets.
Though this approach is advantageous in the context of managing the different configuration, it is not recommended in this proposal because it makes the rest of the management (e.g. rolling updates) more complicated.&lt;/p>
&lt;h2 id="data-persistence">Data Persistence&lt;/h2>
&lt;p>The type of persistence used to store etcd data (including the member ID and cluster ID) has an impact on the steps that are needed to be taken when the member pods or containers (&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">minority&lt;/a> of them or &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">majority&lt;/a>) need to be recovered.&lt;/p>
&lt;h3 id="persistent">Persistent&lt;/h3>
&lt;p>Like the single-node case, &lt;code>persistentvolumes&lt;/code> can be used to persist ETCD data for all the member pods. The individual member pods then get their own &lt;code>persistentvolumes&lt;/code>.
The advantage is that individual members retain their member ID across pod restarts and even pod deletion/recreation across Kubernetes nodes.
This means that member pods that crash (or are unhealthy) can be &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">restarted&lt;/a> automatically (by configuring &lt;code>livenessProbe&lt;/code>) and they will re-join the etcd cluster using their existing member ID without any need for explicit etcd cluster management).&lt;/p>
&lt;p>The disadvantages of this approach are as follows.&lt;/p>
&lt;ul>
&lt;li>The number of persistentvolumes increases linearly with the cluster size which is a cost-related concern.&lt;/li>
&lt;li>Network-mounted persistentvolumes might eventually become a performance bottleneck under heavy load for a latency-sensitive component like ETCD.&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#single-node-etcd-cluster">Volume attach/detach issues&lt;/a> when associated with etcd cluster instances cause downtimes to the target shoot clusters that are backed by those etcd cluster instances.&lt;/li>
&lt;/ul>
&lt;h3 id="ephemeral">Ephemeral&lt;/h3>
&lt;p>The ephemeral volumes use-case is considered as an optimization and may be planned as a follow-up action.&lt;/p>
&lt;h4 id="disk">Disk&lt;/h4>
&lt;p>Ephemeral persistence can be achieved in Kubernetes by using either &lt;a href="https://kubernetes.io/docs/concepts/storage/volumes/#emptydir">&lt;code>emptyDir&lt;/code>&lt;/a> volumes or &lt;a href="https://kubernetes.io/docs/concepts/storage/volumes/#local">&lt;code>local&lt;/code> persistentvolumes&lt;/a> to persist ETCD data.
The advantages of this approach are as follows.&lt;/p>
&lt;ul>
&lt;li>Potentially faster disk I/O.&lt;/li>
&lt;li>The number of persistent volumes does not increase linearly with the cluster size (at least not technically).&lt;/li>
&lt;li>Issues related volume attachment/detachment can be avoided.&lt;/li>
&lt;/ul>
&lt;p>The main disadvantage of using ephemeral persistence is that the individual members may retain their identity and data across container restarts but not across pod deletion/recreation across Kubernetes nodes. If the data is lost then on restart of the member pod, the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">older member (represented by the container) has to be removed and a new member has to be added&lt;/a>.&lt;/p>
&lt;p>Using &lt;code>emptyDir&lt;/code> ephemeral persistence has the disadvantage that the volume doesn&amp;rsquo;t have its own identity.
So, if the member pod is recreated but scheduled on the same node as before then it will not retain the identity as the persistence is lost.
But it has the advantage that scheduling of pods is unencumbered especially during pod recreation as they are free to be scheduled anywhere.&lt;/p>
&lt;p>Using &lt;code>local&lt;/code> persistentvolumes has the advantage that the volume has its own indentity and hence, a recreated member pod will retain its identity if scheduled on the same node.
But it has the disadvantage of tying down the member pod to a node which is a problem if the node becomes unhealthy requiring etcd druid to take additional actions (such as deleting the local persistent volume).&lt;/p>
&lt;p>Based on these constraints, if ephemeral persistence is opted for, it is recommended to use &lt;code>emptyDir&lt;/code> ephemeral persistence.&lt;/p>
&lt;h4 id="in-memory">In-memory&lt;/h4>
&lt;p>In-memory ephemeral persistence can be achieved in Kubernetes by using &lt;code>emptyDir&lt;/code> with &lt;a href="https://kubernetes.io/docs/concepts/storage/volumes/#emptydir">&lt;code>medium: Memory&lt;/code>&lt;/a>.
In this case, a &lt;code>tmpfs&lt;/code> (RAM-backed file-system) volume will be used.
In addition to the advantages of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral persistence&lt;/a>, this approach can achieve the fastest possible &lt;em>disk I/O&lt;/em>.
Similarly, in addition to the disadvantages of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral persistence&lt;/a>, in-memory persistence has the following additional disadvantages.&lt;/p>
&lt;ul>
&lt;li>More memory required for the individual member pods.&lt;/li>
&lt;li>Individual members may not at all retain their data and identity across container restarts let alone across pod restarts/deletion/recreation across Kubernetes nodes.
I.e. every time an etcd container restarts, &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">the old member (represented by the container) will have to be removed and a new member has to be added&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="how-to-detect-if-valid-metadata-exists-in-an-etcd-member">How to detect if valid metadata exists in an etcd member&lt;/h3>
&lt;p>Since the likelyhood of a member not having valid metadata in the WAL files is much more likely in the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral&lt;/a> persistence scenario, one option is to pass the information that ephemeral persistence is being used to the &lt;code>etcd-backup-restore&lt;/code> sidecar (say, via command-line flags or environment variables).&lt;/p>
&lt;p>But in principle, it might be better to determine this from the WAL files directly so that the possibility of corrupted WAL files also gets handled correctly.
To do this, the &lt;a href="https://github.com/etcd-io/etcd/tree/main/server/storage/wal">wal&lt;/a> package has &lt;a href="https://github.com/etcd-io/etcd/blob/57a092b45d0eae6c9e600e62513ffcd2f1f25a92/server/wal/wal.go#L324-L326">some&lt;/a> &lt;a href="https://github.com/etcd-io/etcd/blob/57a092b45d0eae6c9e600e62513ffcd2f1f25a92/server/wal/wal.go#L429-L548">functions&lt;/a> that might be useful.&lt;/p>
&lt;h4 id="recommendation">Recommendation&lt;/h4>
&lt;p>It might be possible that using the &lt;a href="https://github.com/etcd-io/etcd/tree/main/server/storage/wal">wal&lt;/a> package for verifying if valid metadata exists might be performance intensive.
So, the performance impact needs to be measured.
If the performance impact is acceptable (both in terms of resource usage and time), it is recommended to use this way to verify if the member contains valid metadata.
Otherwise, alternatives such as a simple check that WAL folder exists coupled with the static information about use of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent">persistent&lt;/a> or &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral&lt;/a> storage might be considered.&lt;/p>
&lt;h3 id="how-to-detect-if-valid-data-exists-in-an-etcd-member">How to detect if valid data exists in an etcd member&lt;/h3>
&lt;p>The &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#decision-table-for-etcd-backup-restore-during-initialization">initialization sequence&lt;/a> in &lt;code>etcd-backup-restore&lt;/code> already includes &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/c98f76c7c55f7d1039687cc293536d7caf893ba5/pkg/initializer/validator/datavalidator.go#L78-L94">database verification&lt;/a>.
This would suffice to determine if the member has valid data.&lt;/p>
&lt;h3 id="recommendation-1">Recommendation&lt;/h3>
&lt;p>Though &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral&lt;/a> persistence has performance and logistics advantages,
it is recommended to start with &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent">persistent&lt;/a> data for the member pods.
In addition to the reasons and concerns listed above, there is also the additional concern that in case of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure">backup failure&lt;/a>, the risk of additional data loss is a bit higher if ephemeral persistence is used (simultaneous quoram loss is sufficient) when compared to persistent storage (simultaenous quorum loss with majority persistence loss is needed).
The risk might still be acceptable but the idea is to gain experience about how frequently member containers/pods get restarted/recreated, how frequently leader election happens among members of an etcd cluster and how frequently etcd clusters lose quorum.
Based on this experience, we can move towards using &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral&lt;/a> (perhaps even &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#in-memory">in-memory&lt;/a>) persistence for the member pods.&lt;/p>
&lt;h2 id="separating-peer-and-client-traffic">Separating peer and client traffic&lt;/h2>
&lt;p>The current single-node ETCD cluster implementation in &lt;code>etcd-druid&lt;/code> and &lt;code>etcd-backup-restore&lt;/code> uses a single &lt;code>service&lt;/code> object to act as the entry point for the client traffic.
There is no separation or distinction between the client and peer traffic because there is not much benefit to be had by making that distinction.&lt;/p>
&lt;p>In the multi-node ETCD cluster scenario, it makes sense to distinguish between and separate the peer and client traffic.
This can be done by using two &lt;code>services&lt;/code>.&lt;/p>
&lt;ul>
&lt;li>peer
&lt;ul>
&lt;li>To be used for peer communication. This could be a &lt;code>headless&lt;/code> service.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>client
&lt;ul>
&lt;li>To be used for client communication. This could be a normal &lt;code>ClusterIP&lt;/code> service like it is in the single-node case.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>The main advantage of this approach is that it makes it possible (if needed) to allow only peer to peer communication while blocking client communication. Such a thing might be required during some phases of some maintenance tasks (manual or automated).&lt;/p>
&lt;h3 id="cutting-off-client-requests">Cutting off client requests&lt;/h3>
&lt;p>At present, in the single-node ETCD instances, etcd-druid configures the readinessProbe of the etcd main container to probe the healthz endpoint of the etcd-backup-restore sidecar which considers the status of the latest backup upload in addition to the regular checks about etcd and the side car being up and healthy. This has the effect of setting the etcd main container (and hence the etcd pod) as not ready if the latest backup upload failed. This results in the endpoints controller removing the pod IP address from the endpoints list for the service which eventually cuts off ingress traffic coming into the etcd pod via the etcd client service. The rationale for this is to fail early when the backup upload fails rather than continuing to serve requests while the gap between the last backup and the current data increases which might lead to unacceptably large amount of data loss if disaster strikes.&lt;/p>
&lt;p>This approach will not work in the multi-node scenario because we need the individual member pods to be able to talk to each other to maintain the cluster quorum when backup upload fails but need to cut off only client ingress traffic.&lt;/p>
&lt;p>It is recommended to separate the backup health condition tracking taking appropriate remedial actions.
With that, the backup health condition tracking is now separated to the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">&lt;code>BackupReady&lt;/code> condition&lt;/a> in the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">&lt;code>Etcd&lt;/code> resource &lt;code>status&lt;/code>&lt;/a> and the cutting off of client traffic (which could now be done for more reasons than failed backups) can be achieved in a different way described &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector">below&lt;/a>.&lt;/p>
&lt;h4 id="manipulating-client-service-podselector">Manipulating Client Service podSelector&lt;/h4>
&lt;p>The client traffic can be cut off by updating (manually or automatically by some component) the &lt;code>podSelector&lt;/code> of the client service to add an additional label (say, unhealthy or disabled) such that the &lt;code>podSelector&lt;/code> no longer matches the member pods created by the statefulset.
This will result in the client ingress traffic being cut off.
The peer service is left unmodified so that peer communication is always possible.&lt;/p>
&lt;h2 id="health-check">Health Check&lt;/h2>
&lt;p>The etcd main container and the etcd-backup-restore sidecar containers will be configured with livenessProbe and readinessProbe which will indicate the health of the containers and effectively the corresponding ETCD cluster member pod.&lt;/p>
&lt;h3 id="backup-failure">Backup Failure&lt;/h3>
&lt;p>As described &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#cutting-off-client-requests">above&lt;/a> using &lt;code>readinessProbe&lt;/code> failures based on latest backup failure is not viable in the multi-node ETCD scenario.&lt;/p>
&lt;p>Though cutting off traffic by &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector">manipulating client &lt;code>service&lt;/code> &lt;code>podSelector&lt;/code>&lt;/a> is workable, it may not be desirable.&lt;/p>
&lt;p>It is recommended that on backup failure, the leading &lt;code>etcd-backup-restore&lt;/code> sidecar (the one that is responsible for taking backups at that point in time, as explained in the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup">backup section below&lt;/a>, updates the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">&lt;code>BackupReady&lt;/code> condition&lt;/a> in the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">&lt;code>Etcd&lt;/code> status&lt;/a> and raises a high priority alert to the landscape operators but &lt;em>&lt;em>does not&lt;/em>&lt;/em> cut off the client traffic.&lt;/p>
&lt;p>The reasoning behind this decision to not cut off the client traffic on backup failure is to allow the Kubernetes cluster&amp;rsquo;s control plane (which relies on the ETCD cluster) to keep functioning as long as possible and to avoid bringing down the control-plane due to a missed backup.&lt;/p>
&lt;p>The risk of this approach is that with a cascaded sequence of failures (on top of the backup failure), there is a chance of more data loss than the frequency of backup would otherwise indicate.&lt;/p>
&lt;p>To be precise, the risk of such an additional data loss manifests only when backup failure as well as a special case of quorum loss (majority of the members are not ready) happen in such a way that the ETCD cluster needs to be re-bootstrapped from the backup.
As described &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here&lt;/a>, re-bootstrapping the ETCD cluster requires restoration from the latest backup only when a majority of members no longer have uncorrupted data persistence.&lt;/p>
&lt;p>If &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent">persistent storage&lt;/a> is used, this will happen only when backup failure as well as a majority of the disks/volumes backing the ETCD cluster members fail simultaneously.
This would indeed be rare and might be an acceptable risk.&lt;/p>
&lt;p>If &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral storage&lt;/a> is used (especially, &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#in-memory">in-memory&lt;/a>), the data loss will happen if a majority of the ETCD cluster members become &lt;code>NotReady&lt;/code> (requiring a pod restart) at the same time as the backup failure.
This may not be as rare as majority members&amp;rsquo; disk/volume failure.
The risk can be somewhat mitigated at least for planned maintenance operations by postponing potentially disruptive maintenance operations when &lt;code>BackupReady&lt;/code> condition is &lt;code>false&lt;/code> (vertical scaling, rolling updates, evictions due to node roll-outs).&lt;/p>
&lt;p>But in practice (when &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral storage&lt;/a> is used), the current proposal suggests restoring from the latest full backup even when a minority of ETCD members (even a single pod) &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">restart&lt;/a> both to speed up the process of the new member catching up to the latest revision but also to avoid load on the leading member which needs to supply the data to bring the new member up-to-date.
But as described &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">here&lt;/a>, in case of a minority member failure while using ephemeral storage, it is possible to restart the new member with empty data and let it fetch all the data from the leading member (only if backup is not accessible).
Though this is suboptimal, it is workable given the constraints and conditions.
With this, the risk of additional data loss in the case of ephemeral storage is only if backup failure as well as quorum loss happens.
While this is still less rare than the risk of additional data loss in case of persistent storage, the risk might be tolerable. Provided the risk of quorum loss is not too high. This needs to be monitored/evaluated before opting for ephemeral storage.&lt;/p>
&lt;p>Given these constraints, it is better to dynamically avoid/postpone some potentially disruptive operations when &lt;code>BackupReady&lt;/code> condition is &lt;code>false&lt;/code>.
This has the effect of allowing &lt;code>n/2&lt;/code> members to be evicted when the backups are healthy and completely disabling evictions when backups are not healthy.&lt;/p>
&lt;ol>
&lt;li>Skip/postpone potentially disruptive maintenance operations (listed below) when the &lt;code>BackupReady&lt;/code> condition is &lt;code>false&lt;/code>.&lt;/li>
&lt;li>Vertical scaling.&lt;/li>
&lt;li>Rolling updates, Basically, any updates to the &lt;code>StatefulSet&lt;/code> spec which includes vertical scaling.&lt;/li>
&lt;li>Dynamically toggle the &lt;code>minAvailable&lt;/code> field of the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#poddisruptionbudget">&lt;code>PodDisruptionBudget&lt;/code>&lt;/a> between &lt;code>n/2 + 1&lt;/code> and &lt;code>n&lt;/code> (where &lt;code>n&lt;/code> is the ETCD desired cluster size) whenever the &lt;code>BackupReady&lt;/code> condition toggles between &lt;code>true&lt;/code> and &lt;code>false&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>This will mean that &lt;code>etcd-backup-restore&lt;/code> becomes Kubernetes-aware. But there might be reasons for making &lt;code>etcd-backup-restore&lt;/code> Kubernetes-aware anyway (e.g. to update the &lt;code>etcd&lt;/code> resource &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">status&lt;/a> with latest full snapshot details).
This enhancement should keep &lt;code>etcd-backup-restore&lt;/code> backward compatible.
I.e. it should be possible to use &lt;code>etcd-backup-restore&lt;/code> Kubernetes-unaware as before this proposal.
This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as &lt;code>--enable-client-service-updates&lt;/code> which can be defaulted to &lt;code>false&lt;/code> for backward compatibility).&lt;/p>
&lt;h5 id="alternative-3">Alternative&lt;/h5>
&lt;p>The alternative is for &lt;code>etcd-druid&lt;/code> to implement the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector">above functionality&lt;/a>.&lt;/p>
&lt;p>But &lt;code>etcd-druid&lt;/code> is centrally deployed in the host Kubernetes cluster and cannot scale well horizontally.
So, it can potentially be a bottleneck if it is involved in regular health check mechanism for all the etcd clusters it manages.
Also, the recommended approach above is more robust because it can work even if &lt;code>etcd-druid&lt;/code> is down when the backup upload of a particular etcd cluster fails.&lt;/p>
&lt;h2 id="status">Status&lt;/h2>
&lt;p>It is desirable (for the &lt;code>etcd-druid&lt;/code> and landscape administrators/operators) to maintain/expose status of the etcd cluster instances in the &lt;code>status&lt;/code> sub-resource of the &lt;code>Etcd&lt;/code> CRD.
The proposed structure for maintaining the status is as shown in the example below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: druid.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Etcd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: etcd-main
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> replicas: 3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>status:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> conditions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - type: Ready &lt;span style="color:#008000"># Condition type for the readiness of the ETCD cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> status: &lt;span style="color:#a31515">&amp;#34;True&amp;#34;&lt;/span> &lt;span style="color:#008000"># Indicates of the ETCD Cluster is ready or not&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastHeartbeatTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastTransitionTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reason: Quorate &lt;span style="color:#008000"># Quorate|QuorumLost&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - type: AllMembersReady &lt;span style="color:#008000"># Condition type for the readiness of all the member of the ETCD cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> status: &lt;span style="color:#a31515">&amp;#34;True&amp;#34;&lt;/span> &lt;span style="color:#008000"># Indicates if all the members of the ETCD Cluster are ready&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastHeartbeatTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastTransitionTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reason: AllMembersReady &lt;span style="color:#008000"># AllMembersReady|NotAllMembersReady&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - type: BackupReady &lt;span style="color:#008000"># Condition type for the readiness of the backup of the ETCD cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> status: &lt;span style="color:#a31515">&amp;#34;True&amp;#34;&lt;/span> &lt;span style="color:#008000"># Indicates if the backup of the ETCD cluster is ready&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastHeartbeatTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastTransitionTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reason: FullBackupSucceeded &lt;span style="color:#008000"># FullBackupSucceeded|IncrementalBackupSucceeded|FullBackupFailed|IncrementalBackupFailed&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clusterSize: 3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> replicas: 3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> members:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: etcd-main-0 &lt;span style="color:#008000"># member pod name&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> id: 272e204152 &lt;span style="color:#008000"># member Id&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> role: Leader &lt;span style="color:#008000"># Member|Leader&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> status: Ready &lt;span style="color:#008000"># Ready|NotReady|Unknown&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastTransitionTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reason: LeaseSucceeded &lt;span style="color:#008000"># LeaseSucceeded|LeaseExpired|UnknownGracePeriodExceeded|PodNotRead&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: etcd-main-1 &lt;span style="color:#008000"># member pod name&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> id: 272e204153 &lt;span style="color:#008000"># member Id&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> role: Member &lt;span style="color:#008000"># Member|Leader&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> status: Ready &lt;span style="color:#008000"># Ready|NotReady|Unknown&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastTransitionTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reason: LeaseSucceeded &lt;span style="color:#008000"># LeaseSucceeded|LeaseExpired|UnknownGracePeriodExceeded|PodNotRead&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This proposal recommends that &lt;code>etcd-druid&lt;/code> (preferrably, the &lt;code>custodian&lt;/code> controller in &lt;code>etcd-druid&lt;/code>) maintains most of the information in the &lt;code>status&lt;/code> of the &lt;code>Etcd&lt;/code> resources described above.&lt;/p>
&lt;p>One exception to this is the &lt;code>BackupReady&lt;/code> condition which is recommended to be maintained by the &lt;em>leading&lt;/em> &lt;code>etcd-backup-restore&lt;/code> sidecar container.
This will mean that &lt;code>etcd-backup-restore&lt;/code> becomes Kubernetes-aware. But there are other reasons for making &lt;code>etcd-backup-restore&lt;/code> Kubernetes-aware anyway (e.g. to &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#health-check">maintain health conditions&lt;/a>).
This enhancement should keep &lt;code>etcd-backup-restore&lt;/code> backward compatible.
But it should be possible to use &lt;code>etcd-backup-restore&lt;/code> Kubernetes-unaware as before this proposal. This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as &lt;code>--enable-etcd-status-updates&lt;/code> which can be defaulted to &lt;code>false&lt;/code> for backward compatibility).&lt;/p>
&lt;h3 id="members">Members&lt;/h3>
&lt;p>The &lt;code>members&lt;/code> section of the status is intended to be maintained by &lt;code>etcd-druid&lt;/code> (preferraby, the &lt;code>custodian&lt;/code> controller of &lt;code>etcd-druid&lt;/code>) based on the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">&lt;code>leases&lt;/code> of the individual members&lt;/a>.&lt;/p>
&lt;h4 id="note-1">Note&lt;/h4>
&lt;p>An earlier design in this proposal was for the individual &lt;code>etcd-backup-restore&lt;/code> sidecars to update the corresponding &lt;code>status.members&lt;/code> entries themselves. But this was redesigned to use &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member &lt;code>leases&lt;/code>&lt;/a> to avoid conflicts rising from frequent updates and the limitations in the support for &lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">Server-Side Apply&lt;/a> in some versions of Kubernetes.&lt;/p>
&lt;p>The &lt;code>spec.holderIdentity&lt;/code> field in the &lt;code>leases&lt;/code> is used to communicate the ETCD member &lt;code>id&lt;/code> and &lt;code>role&lt;/code> between the &lt;code>etcd-backup-restore&lt;/code> sidecars and &lt;code>etcd-druid&lt;/code>.&lt;/p>
&lt;h4 id="member-name-as-the-key">Member name as the key&lt;/h4>
&lt;p>In an ETCD cluster, the member &lt;code>id&lt;/code> is the &lt;a href="https://etcd.io/docs/v3.4/dev-guide/api_reference_v3/#message-member-etcdserveretcdserverpbrpcproto">unique identifier for a member&lt;/a>.
However, this proposal recommends using a &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#kubernetes-context">single &lt;code>StatefulSet&lt;/code>&lt;/a> whose pods form the members of the ETCD cluster and &lt;code>Pods&lt;/code> of a &lt;code>StatefulSet&lt;/code> have &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#ordinal-index">uniquely indexed names&lt;/a> as well as &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-network-id">uniquely addressible DNS&lt;/a>.&lt;/p>
&lt;p>This proposal recommends that the &lt;code>name&lt;/code> of the member (which is the same as the name of the member &lt;code>Pod&lt;/code>) be used as the unique key to identify a member in the &lt;code>members&lt;/code> array.
This can minimise the need to cleanup superfluous entries in the &lt;code>members&lt;/code> array after the member pods are gone to some extent because the replacement pods for any member will share the same &lt;code>name&lt;/code> and will overwrite the entry with a &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">possibly new&lt;/a> member &lt;code>id&lt;/code>.&lt;/p>
&lt;p>There is still the possibility of not only &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status">superfluous entries in the &lt;code>members&lt;/code> array&lt;/a> but also &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member">superfluous &lt;code>members&lt;/code> in the ETCD cluster&lt;/a> for which there is no corresponding pod in the &lt;code>StatefulSet&lt;/code> anymore.&lt;/p>
&lt;p>For example, if an ETCD cluster is scaled up from &lt;code>3&lt;/code> to &lt;code>5&lt;/code> and the new members were failing constantly due to insufficient resources and then if the ETCD client is scaled back down to &lt;code>3&lt;/code> and failing member pods may not have the chance to clean up their &lt;code>member&lt;/code> entries (from the &lt;code>members&lt;/code> array as well as from the ETCD cluster) leading to superfluous members in the cluster that may have adverse effect on quorum of the cluster.&lt;/p>
&lt;p>Hence, the superfluous entries in both &lt;code>members&lt;/code> array as well as the ETCD cluster need to be &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12">cleaned up&lt;/a> &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member">as appropriate&lt;/a>.&lt;/p>
&lt;h4 id="member-leases">Member Leases&lt;/h4>
&lt;p>One &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/">Kubernetes &lt;code>lease&lt;/code> object&lt;/a> per desired ETCD member is maintained by &lt;code>etcd-druid&lt;/code> (preferrably, the &lt;code>custodian&lt;/code> controller in &lt;code>etcd-druid&lt;/code>).
The &lt;code>lease&lt;/code> objects will be created in the same &lt;code>namespace&lt;/code> as their owning &lt;code>Etcd&lt;/code> object and will have the same &lt;code>name&lt;/code> as the member to which they correspond (which, in turn would be the same as &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-name-as-the-key">the &lt;code>pod&lt;/code> name in which the member ETCD process runs&lt;/a>).&lt;/p>
&lt;p>The &lt;code>lease&lt;/code> objects are created and deleted only by &lt;code>etcd-druid&lt;/code> but are continually renewed within the &lt;code>leaseDurationSeconds&lt;/code> by the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-independent-of-leader-election-in-all-members">individual &lt;code>etcd-backup-restore&lt;/code> sidecars&lt;/a> (corresponding to their members) if the the corresponding ETCD member is ready and is part of the ETCD cluster.&lt;/p>
&lt;p>This will mean that &lt;code>etcd-backup-restore&lt;/code> becomes Kubernetes-aware. But there are other reasons for making &lt;code>etcd-backup-restore&lt;/code> Kubernetes-aware anyway (e.g. to &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#health-check">maintain health conditions&lt;/a>).
This enhancement should keep &lt;code>etcd-backup-restore&lt;/code> backward compatible.
But it should be possible to use &lt;code>etcd-backup-restore&lt;/code> Kubernetes-unaware as before this proposal. This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as &lt;code>--enable-etcd-lease-renewal&lt;/code> which can be defaulted to &lt;code>false&lt;/code> for backward compatibility).&lt;/p>
&lt;p>A &lt;code>member&lt;/code> entry in the &lt;code>Etcd&lt;/code> resource &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">&lt;code>status&lt;/code>&lt;/a> would be marked as &lt;code>Ready&lt;/code> (with &lt;code>reason: LeaseSucceeded&lt;/code>) if the corresponding &lt;code>pod&lt;/code> is ready and the corresponding &lt;code>lease&lt;/code> has not yet expired.
The &lt;code>member&lt;/code> entry would be marked as &lt;code>NotReady&lt;/code> if the corresponding &lt;code>pod&lt;/code> is not ready (with reason &lt;code>PodNotReady&lt;/code>) or as &lt;code>Unknown&lt;/code> if the corresponding &lt;code>lease&lt;/code> has expired (with &lt;code>reason: LeaseExpired&lt;/code>).&lt;/p>
&lt;p>While renewing the lease, the &lt;code>etcd-backup-restore&lt;/code> sidecars also maintain the ETCD member &lt;code>id&lt;/code> and their &lt;code>role&lt;/code> (&lt;code>Leader&lt;/code> or &lt;code>Member&lt;/code>) separated by &lt;code>:&lt;/code> in the &lt;code>spec.holderIdentity&lt;/code> field of the corresponding &lt;code>lease&lt;/code> object since this information is only available to the &lt;code>ETCD&lt;/code> member processes and the &lt;code>etcd-backup-restore&lt;/code> sidecars (e.g. &lt;code>272e204152:Leader&lt;/code> or &lt;code>272e204153:Member&lt;/code>).
When the &lt;code>lease&lt;/code> objects are created by &lt;code>etcd-druid&lt;/code>, the &lt;code>spec.holderIdentity&lt;/code> field would be empty.&lt;/p>
&lt;p>The value in &lt;code>spec.holderIdentity&lt;/code> in the &lt;code>leases&lt;/code> is parsed and copied onto the &lt;code>id&lt;/code> and &lt;code>role&lt;/code> fields of the corresponding &lt;code>status.members&lt;/code> by &lt;code>etcd-druid&lt;/code>.&lt;/p>
&lt;h3 id="conditions">Conditions&lt;/h3>
&lt;p>The &lt;code>conditions&lt;/code> section in the status describe the overall condition of the ETCD cluster.
The condition type &lt;code>Ready&lt;/code> indicates if the ETCD cluster as a whole is ready to serve requests (i.e. the cluster is quorate) even though some minority of the members are not ready.
The condition type &lt;code>AllMembersReady&lt;/code> indicates of all the members of the ETCD cluster are ready.
The distinction between these conditions could be significant for both external consumers of the status as well as &lt;code>etcd-druid&lt;/code> itself.
Some maintenance operations might be safe to do (e.g. rolling updates) only when all members of the cluster are ready.
The condition type &lt;code>BackupReady&lt;/code> indicates of the most recent backup upload (full or incremental) succeeded.
This information also might be significant because some maintenance operations might be safe to do (e.g. anything that involves re-bootstrapping the ETCD cluster) only when backup is ready.&lt;/p>
&lt;p>The &lt;code>Ready&lt;/code> and &lt;code>AllMembersReady&lt;/code> conditions can be maintained by &lt;code>etcd-druid&lt;/code> based on the status in the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#members">&lt;code>members&lt;/code> section&lt;/a>.
The &lt;code>BackupReady&lt;/code> condition will be maintained by the leading &lt;code>etcd-backup-restore&lt;/code> sidecar that is in charge of taking backups.&lt;/p>
&lt;p>More condition types could be introduced in the future if specific purposes arise.&lt;/p>
&lt;h3 id="clustersize">ClusterSize&lt;/h3>
&lt;p>The &lt;code>clusterSize&lt;/code> field contains the current size of the ETCD cluster. It will be actively kept up-to-date by &lt;code>etcd-druid&lt;/code> in all scenarios.&lt;/p>
&lt;ul>
&lt;li>Before &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping">bootstrapping&lt;/a> the ETCD cluster (during cluster creation or later bootstrapping because of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-9">quorum failure&lt;/a>), &lt;code>etcd-druid&lt;/code> will clear the &lt;code>status.members&lt;/code> array and set &lt;code>status.clusterSize&lt;/code> to be equal to &lt;code>spec.replicas&lt;/code>.&lt;/li>
&lt;li>While the ETCD cluster is quorate, &lt;code>etcd-druid&lt;/code> will actively set &lt;code>status.clusterSize&lt;/code> to be equal to length of the &lt;code>status.members&lt;/code> whenever the length of the array changes (say, due to scaling of the ETCD cluster).&lt;/li>
&lt;/ul>
&lt;p>Given that &lt;code>clusterSize&lt;/code> reliably represents the size of the ETCD cluster, it can be used to calculate the &lt;code>Ready&lt;/code> &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">condition&lt;/a>.&lt;/p>
&lt;h3 id="alternative-4">Alternative&lt;/h3>
&lt;p>The alternative is for &lt;code>etcd-druid&lt;/code> to maintain the status in the &lt;code>Etcd&lt;/code> status sub-resource.
But &lt;code>etcd-druid&lt;/code> is centrally deployed in the host Kubernetes cluster and cannot scale well horizontally.
So, it can potentially be a bottleneck if it is involved in regular health check mechanism for all the etcd clusters it manages.
Also, the recommended approach above is more robust because it can work even if &lt;code>etcd-druid&lt;/code> is down when the backup upload of a particular etcd cluster fails.&lt;/p>
&lt;h2 id="decision-table-for-etcd-druid-based-on-the-status">Decision table for etcd-druid based on the status&lt;/h2>
&lt;p>The following decision table describes the various criteria &lt;code>etcd-druid&lt;/code> takes into consideration to determine the different etcd cluster management scenarios and the corresponding reconciliation actions it must take.
The general principle is to detect the scenario and take the minimum action to move the cluster along the path to good health.
The path from any one scenario to a state of good health will typically involve going through multiple reconciliation actions which probably take the cluster through many other cluster management scenarios.
Especially, it is proposed that individual members auto-heal where possible, even in the case of the failure of a majority of members of the etcd cluster and that &lt;code>etcd-druid&lt;/code> takes action only if the auto-healing doesn&amp;rsquo;t happen for a configured period of time.&lt;/p>
&lt;h3 id="1-pink-of-health">1. Pink of health&lt;/h3>
&lt;h4 id="observed-state">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: &lt;code>true&lt;/code>&lt;/li>
&lt;li>AllMembersReady: &lt;code>true&lt;/code>&lt;/li>
&lt;li>BackupReady: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action">Recommended Action&lt;/h4>
&lt;p>Nothing to do&lt;/p>
&lt;h3 id="2-member-status-is-out-of-sync-with-their-leases">2. Member status is out of sync with their leases&lt;/h3>
&lt;h4 id="observed-state-1">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: &lt;code>l&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: &lt;code>true&lt;/code>&lt;/li>
&lt;li>AllMembersReady: &lt;code>true&lt;/code>&lt;/li>
&lt;li>BackupReady: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-1">Recommended Action&lt;/h4>
&lt;p>Mark the &lt;code>l&lt;/code> members corresponding to the expired &lt;code>leases&lt;/code> as &lt;code>Unknown&lt;/code> with reason &lt;code>LeaseExpired&lt;/code> and with &lt;code>id&lt;/code> populated from &lt;code>spec.holderIdentity&lt;/code> of the &lt;code>lease&lt;/code> if they are not already updated so.&lt;/p>
&lt;p>Mark the &lt;code>n - l&lt;/code> members corresponding to the active &lt;code>leases&lt;/code> as &lt;code>Ready&lt;/code> with reason &lt;code>LeaseSucceeded&lt;/code> and with &lt;code>id&lt;/code> populated from &lt;code>spec.holderIdentity&lt;/code> of the &lt;code>lease&lt;/code> if they are not already updated so.&lt;/p>
&lt;p>Please refer &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">here&lt;/a> for more details.&lt;/p>
&lt;h3 id="3-all-members-are-ready-but-allmembersready-condition-is-stale">3. All members are &lt;code>Ready&lt;/code> but &lt;code>AllMembersReady&lt;/code> condition is stale&lt;/h3>
&lt;h4 id="observed-state-2">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: N/A&lt;/li>
&lt;li>AllMembersReady: false&lt;/li>
&lt;li>BackupReady: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-2">Recommended Action&lt;/h4>
&lt;p>Mark the status condition type &lt;code>AllMembersReady&lt;/code> to &lt;code>true&lt;/code>.&lt;/p>
&lt;h3 id="4-not-all-members-are-ready-but-allmembersready-condition-is-stale">4. Not all members are &lt;code>Ready&lt;/code> but &lt;code>AllMembersReady&lt;/code> condition is stale&lt;/h3>
&lt;h4 id="observed-state-3">Observed state&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Cluster Size&lt;/p>
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>StatefulSet&lt;/code> replicas&lt;/p>
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Etcd&lt;/code> status&lt;/p>
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: N/A&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>0 &amp;lt;= r &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>nr&lt;/code> where &lt;code>0 &amp;lt; nr &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>u&lt;/code> where &lt;code>0 &amp;lt; u &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: &lt;code>h&lt;/code> where &lt;code>0 &amp;lt; h &amp;lt; n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: N/A&lt;/li>
&lt;li>AllMembersReady: true&lt;/li>
&lt;li>BackupReady: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>where &lt;code>(nr + u + h) &amp;gt; 0&lt;/code> or &lt;code>r &amp;lt; n&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-3">Recommended Action&lt;/h4>
&lt;p>Mark the status condition type &lt;code>AllMembersReady&lt;/code> to &lt;code>false&lt;/code>.&lt;/p>
&lt;h3 id="5-majority-members-are-ready-but-ready-condition-is-stale">5. Majority members are &lt;code>Ready&lt;/code> but &lt;code>Ready&lt;/code> condition is stale&lt;/h3>
&lt;h4 id="observed-state-4">Observed state&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Cluster Size&lt;/p>
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>StatefulSet&lt;/code> replicas&lt;/p>
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Etcd&lt;/code> status&lt;/p>
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>r &amp;gt; n/2&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>nr&lt;/code> where &lt;code>0 &amp;lt; nr &amp;lt; n/2&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>u&lt;/code> where &lt;code>0 &amp;lt; u &amp;lt; n/2&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: &lt;code>false&lt;/code>&lt;/li>
&lt;li>AllMembersReady: N/A&lt;/li>
&lt;li>BackupReady: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>where &lt;code>0 &amp;lt; (nr + u + h) &amp;lt; n/2&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-4">Recommended Action&lt;/h4>
&lt;p>Mark the status condition type &lt;code>Ready&lt;/code> to &lt;code>true&lt;/code>.&lt;/p>
&lt;h3 id="6-majority-members-are-notready-but-ready-condition-is-stale">6. Majority members are &lt;code>NotReady&lt;/code> but &lt;code>Ready&lt;/code> condition is stale&lt;/h3>
&lt;h4 id="observed-state-5">Observed state&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Cluster Size&lt;/p>
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>StatefulSet&lt;/code> replicas&lt;/p>
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Etcd&lt;/code> status&lt;/p>
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>0 &amp;lt; r &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>nr&lt;/code> where &lt;code>0 &amp;lt; nr &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>u&lt;/code> where &lt;code>0 &amp;lt; u &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: &lt;code>true&lt;/code>&lt;/li>
&lt;li>AllMembersReady: N/A&lt;/li>
&lt;li>BackupReady: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>where &lt;code>(nr + u + h) &amp;gt; n/2&lt;/code> or &lt;code>r &amp;lt; n/2&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-5">Recommended Action&lt;/h4>
&lt;p>Mark the status condition type &lt;code>Ready&lt;/code> to &lt;code>false&lt;/code>.&lt;/p>
&lt;h3 id="7-some-members-have-been-in-unknown-status-for-a-while">7. Some members have been in &lt;code>Unknown&lt;/code> status for a while&lt;/h3>
&lt;h4 id="observed-state-6">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: N/A&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: N/A&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>u&lt;/code> where &lt;code>u &amp;lt;= n&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: N/A&lt;/li>
&lt;li>AllMembersReady: N/A&lt;/li>
&lt;li>BackupReady: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-6">Recommended Action&lt;/h4>
&lt;p>Mark the &lt;code>u&lt;/code> members as &lt;code>NotReady&lt;/code> in &lt;code>Etcd&lt;/code> status with &lt;code>reason: UnknownGracePeriodExceeded&lt;/code>.&lt;/p>
&lt;h3 id="8-some-member-pods-are-not-ready-but-have-not-had-the-chance-to-update-their-status">8. Some member pods are not &lt;code>Ready&lt;/code> but have not had the chance to update their status&lt;/h3>
&lt;h4 id="observed-state-7">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>s&lt;/code> where &lt;code>s &amp;lt; n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: N/A&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: N/A&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: N/A&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: N/A&lt;/li>
&lt;li>AllMembersReady: N/A&lt;/li>
&lt;li>BackupReady: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-7">Recommended Action&lt;/h4>
&lt;p>Mark the &lt;code>n - s&lt;/code> members (corresponding to the pods that are not &lt;code>Ready&lt;/code>) as &lt;code>NotReady&lt;/code> in &lt;code>Etcd&lt;/code> status with &lt;code>reason: PodNotReady&lt;/code>&lt;/p>
&lt;h3 id="9-quorate-cluster-with-a-minority-of-members-notready">9. Quorate cluster with a minority of members &lt;code>NotReady&lt;/code>&lt;/h3>
&lt;h4 id="observed-state-8">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n - f&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>f&lt;/code> where &lt;code>f &amp;lt; n/2&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: true&lt;/li>
&lt;li>AllMembersReady: false&lt;/li>
&lt;li>BackupReady: true&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-8">Recommended Action&lt;/h4>
&lt;p>Delete the &lt;code>f&lt;/code> &lt;code>NotReady&lt;/code> member pods to force restart of the pods if they do not automatically restart via failed &lt;code>livenessProbe&lt;/code>. The expectation is that they will either re-join the cluster as an existing member or remove themselves and join as new members on restart of the container or pod and &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">renew their &lt;code>leases&lt;/code>&lt;/a>.&lt;/p>
&lt;h3 id="10-quorum-lost-with-a-majority-of-members-notready">10. Quorum lost with a majority of members &lt;code>NotReady&lt;/code>&lt;/h3>
&lt;h4 id="observed-state-9">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n - f&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>f&lt;/code> where &lt;code>f &amp;gt;= n/2&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: N/A&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: false&lt;/li>
&lt;li>AllMembersReady: false&lt;/li>
&lt;li>BackupReady: true&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-9">Recommended Action&lt;/h4>
&lt;p>Scale down the &lt;code>StatefulSet&lt;/code> to &lt;code>replicas: 0&lt;/code>. Ensure that all member pods are deleted. Ensure that all the members are removed from &lt;code>Etcd&lt;/code> status. Delete and recreate all the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member &lt;code>leases&lt;/code>&lt;/a>. Recover the cluster from loss of quorum as discussed &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here&lt;/a>.&lt;/p>
&lt;h3 id="11-scale-up-of-a-healthy-cluster">11. Scale up of a healthy cluster&lt;/h3>
&lt;h4 id="observed-state-10">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: &lt;code>d&lt;/code>&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code> where &lt;code>d &amp;gt; n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: 0&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: 0&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: 0&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: true&lt;/li>
&lt;li>AllMembersReady: true&lt;/li>
&lt;li>BackupReady: true&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-10">Recommended Action&lt;/h4>
&lt;p>Add &lt;code>d - n&lt;/code> new members by scaling the &lt;code>StatefulSet&lt;/code> to &lt;code>replicas: d&lt;/code>. The rest of the &lt;code>StatefulSet&lt;/code> spec need not be updated until the next cluster bootstrapping (alternatively, the rest of the &lt;code>StatefulSet&lt;/code> spec can be updated pro-actively once the new members join the cluster. This will trigger a rolling update).&lt;/p>
&lt;p>Also, create the additional &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member &lt;code>leases&lt;/code>&lt;/a> for the &lt;code>d - n&lt;/code> new members.&lt;/p>
&lt;h3 id="12-scale-down-of-a-healthy-cluster">12. Scale down of a healthy cluster&lt;/h3>
&lt;h4 id="observed-state-11">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: &lt;code>d&lt;/code>&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code> where &lt;code>d &amp;lt; n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: 0&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: 0&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: 0&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: true&lt;/li>
&lt;li>AllMembersReady: true&lt;/li>
&lt;li>BackupReady: true&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-11">Recommended Action&lt;/h4>
&lt;p>Remove &lt;code>d - n&lt;/code> existing members (numbered &lt;code>d&lt;/code>, &lt;code>d + 1&lt;/code> &amp;hellip; &lt;code>n&lt;/code>) by scaling the &lt;code>StatefulSet&lt;/code> to &lt;code>replicas: d&lt;/code>. The &lt;code>StatefulSet&lt;/code> spec need not be updated until the next cluster bootstrapping (alternatively, the &lt;code>StatefulSet&lt;/code> spec can be updated pro-actively once the superfluous members exit the cluster. This will trigger a rolling update).&lt;/p>
&lt;p>Also, delete the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member &lt;code>leases&lt;/code>&lt;/a> for the &lt;code>d - n&lt;/code> members being removed.&lt;/p>
&lt;p>The &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status">superfluous entries in the &lt;code>members&lt;/code> array&lt;/a> will be cleaned up as explained &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12">here&lt;/a>.
The superfluous members in the ETCD cluster will be cleaned up by the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member">leading &lt;code>etcd-backup-restore&lt;/code> sidecar&lt;/a>.&lt;/p>
&lt;h3 id="13-superfluous-member-entries-in-etcd-status">13. Superfluous member entries in &lt;code>Etcd&lt;/code> status&lt;/h3>
&lt;h4 id="observed-state-12">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: n&lt;/li>
&lt;li>Ready: n&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>m&lt;/code> where &lt;code>m &amp;gt; n&lt;/code>&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: N/A&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: N/A&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: N/A&lt;/li>
&lt;li>AllMembersReady: N/A&lt;/li>
&lt;li>BackupReady: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-12">Recommended Action&lt;/h4>
&lt;p>Remove the superfluous &lt;code>m - n&lt;/code> member entries from &lt;code>Etcd&lt;/code> status (numbered &lt;code>n&lt;/code>, &lt;code>n+1&lt;/code> &amp;hellip; &lt;code>m&lt;/code>).
Remove the superfluous &lt;code>m - n&lt;/code> &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member &lt;code>leases&lt;/code>&lt;/a> if they exist.
The superfluous members in the ETCD cluster will be cleaned up by the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member">leading &lt;code>etcd-backup-restore&lt;/code> sidecar&lt;/a>.&lt;/p>
&lt;h2 id="decision-table-for-etcd-backup-restore-during-initialization">Decision table for etcd-backup-restore during initialization&lt;/h2>
&lt;p>As discussed above, the initialization sequence of &lt;code>etcd-backup-restore&lt;/code> in a member pod needs to &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-configuration">generate suitable etcd configuration&lt;/a> for its etcd container.
It also might have to handle the etcd database verification and restoration functionality differently in &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">different&lt;/a> &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">scenarios&lt;/a>.&lt;/p>
&lt;p>The initialization sequence itself is proposed to be as follows.
It is an enhancement of the &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/master/doc/proposals/design.md#workflow">existing&lt;/a> initialization sequence.
&lt;img src="https://gardener.cloud/__resources/01-etcd-member-initialization-sequence_364f5e.png" alt="etcd member initialization sequence">&lt;/p>
&lt;p>The details of the decisions to be taken during the initialization are given below.&lt;/p>
&lt;h3 id="1-first-member-during-bootstrap-of-a-fresh-etcd-cluster">1. First member during bootstrap of a fresh etcd cluster&lt;/h3>
&lt;h4 id="observed-state-13">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: &lt;code>false&lt;/code>&lt;/li>
&lt;li>Data directory is valid and up-to-date: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: &lt;code>false&lt;/code>&lt;/li>
&lt;li>Backup has incremental snapshots: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-13">Recommended Action&lt;/h4>
&lt;p>Generate etcd configuration with &lt;code>n&lt;/code> initial cluster peer URLs and initial cluster state new and return success.&lt;/p>
&lt;h3 id="2-addition-of-a-new-following-member-during-bootstrap-of-a-fresh-etcd-cluster">2. Addition of a new following member during bootstrap of a fresh etcd cluster&lt;/h3>
&lt;h4 id="observed-state-14">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>m&lt;/code> where &lt;code>0 &amp;lt; m &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>m&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: &lt;code>false&lt;/code>&lt;/li>
&lt;li>Data directory is valid and up-to-date: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: &lt;code>false&lt;/code>&lt;/li>
&lt;li>Backup has incremental snapshots: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-14">Recommended Action&lt;/h4>
&lt;p>Generate etcd configuration with &lt;code>n&lt;/code> initial cluster peer URLs and initial cluster state new and return success.&lt;/p>
&lt;h3 id="3-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-and-data">3. Restart of an existing member of a quorate cluster with valid metadata and data&lt;/h3>
&lt;h4 id="observed-state-15">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>m&lt;/code> where &lt;code>m &amp;gt; n/2&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>r &amp;gt; n/2&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: &lt;code>true&lt;/code>&lt;/li>
&lt;li>Data directory is valid and up-to-date: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: N/A&lt;/li>
&lt;li>Backup has incremental snapshots: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-15">Recommended Action&lt;/h4>
&lt;p>Re-use previously generated etcd configuration and return success.&lt;/p>
&lt;h3 id="4-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-but-without-valid-data">4. Restart of an existing member of a quorate cluster with valid metadata but without valid data&lt;/h3>
&lt;h4 id="observed-state-16">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>m&lt;/code> where &lt;code>m &amp;gt; n/2&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>r &amp;gt; n/2&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: &lt;code>true&lt;/code>&lt;/li>
&lt;li>Data directory is valid and up-to-date: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: N/A&lt;/li>
&lt;li>Backup has incremental snapshots: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-16">Recommended Action&lt;/h4>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">Remove&lt;/a> self as a member (old member ID) from the etcd cluster as well as &lt;code>Etcd&lt;/code> status. &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">Add&lt;/a> self as a new member of the etcd cluster as well as in the &lt;code>Etcd&lt;/code> status. If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here&lt;/a> for the reason for not restoring incremental snapshots). Generate etcd configuration with &lt;code>n&lt;/code> initial cluster peer URLs and initial cluster state &lt;code>existing&lt;/code> and return success.&lt;/p>
&lt;h3 id="5-restart-of-an-existing-member-of-a-quorate-cluster-without-valid-metadata">5. Restart of an existing member of a quorate cluster without valid metadata&lt;/h3>
&lt;h4 id="observed-state-17">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>m&lt;/code> where &lt;code>m &amp;gt; n/2&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>r &amp;gt; n/2&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: &lt;code>false&lt;/code>&lt;/li>
&lt;li>Data directory is valid and up-to-date: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: N/A&lt;/li>
&lt;li>Backup has incremental snapshots: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-17">Recommended Action&lt;/h4>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">Remove&lt;/a> self as a member (old member ID) from the etcd cluster as well as &lt;code>Etcd&lt;/code> status. &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">Add&lt;/a> self as a new member of the etcd cluster as well as in the &lt;code>Etcd&lt;/code> status. If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here&lt;/a> for the reason for not restoring incremental snapshots). Generate etcd configuration with &lt;code>n&lt;/code> initial cluster peer URLs and initial cluster state &lt;code>existing&lt;/code> and return success.&lt;/p>
&lt;h3 id="6-restart-of-an-existing-member-of-a-non-quorate-cluster-with-valid-metadata-and-data">6. Restart of an existing member of a non-quorate cluster with valid metadata and data&lt;/h3>
&lt;h4 id="observed-state-18">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>m&lt;/code> where &lt;code>m &amp;lt; n/2&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>r &amp;lt; n/2&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: &lt;code>true&lt;/code>&lt;/li>
&lt;li>Data directory is valid and up-to-date: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: N/A&lt;/li>
&lt;li>Backup has incremental snapshots: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-18">Recommended Action&lt;/h4>
&lt;p>Re-use previously generated etcd configuration and return success.&lt;/p>
&lt;h3 id="7-restart-of-the-first-member-of-a-non-quorate-cluster-without-valid-data">7. Restart of the first member of a non-quorate cluster without valid data&lt;/h3>
&lt;h4 id="observed-state-19">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: N/A&lt;/li>
&lt;li>Data directory is valid and up-to-date: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: N/A&lt;/li>
&lt;li>Backup has incremental snapshots: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-19">Recommended Action&lt;/h4>
&lt;p>If backups do not exist, create an empty data and WAL directory. If backups exist, restore the latest full snapshot. Start a single-node embedded etcd with initial cluster peer URLs containing only own peer URL and initial cluster state &lt;code>new&lt;/code>. If incremental snapshots exist, apply them serially (honouring source transactions). Take and upload a full snapshot after incremental snapshots are applied successfully (please see &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here&lt;/a> for more reasons why). Generate etcd configuration with &lt;code>n&lt;/code> initial cluster peer URLs and initial cluster state &lt;code>new&lt;/code> and return success.&lt;/p>
&lt;h3 id="8-restart-of-a-following-member-of-a-non-quorate-cluster-without-valid-data">8. Restart of a following member of a non-quorate cluster without valid data&lt;/h3>
&lt;h4 id="observed-state-20">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>m&lt;/code> where &lt;code>1 &amp;lt; m &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>1 &amp;lt; r &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: N/A&lt;/li>
&lt;li>Data directory is valid and up-to-date: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: N/A&lt;/li>
&lt;li>Backup has incremental snapshots: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-20">Recommended Action&lt;/h4>
&lt;p>If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here&lt;/a> for the reason for not restoring incremental snapshots). Generate etcd configuration with &lt;code>n&lt;/code> initial cluster peer URLs and initial cluster state &lt;code>existing&lt;/code> and return success.&lt;/p>
&lt;h2 id="backup">Backup&lt;/h2>
&lt;p>Only one of the etcd-backup-restore sidecars among the members are required to take the backup for a given ETCD cluster. This can be called a &lt;code>backup leader&lt;/code>. There are two possibilities to ensure this.&lt;/p>
&lt;h3 id="leading-etcd-main-containers-sidecar-is-the-backup-leader">Leading ETCD main container’s sidecar is the backup leader&lt;/h3>
&lt;p>The backup-restore sidecar could poll the etcd cluster and/or its own etcd main container to see if it is the leading member in the etcd cluster.
This information can be used by the backup-restore sidecars to decide that sidecar of the leading etcd main container is the backup leader (i.e. responsible to for taking/uploading backups regularly).&lt;/p>
&lt;p>The advantages of this approach are as follows.&lt;/p>
&lt;ul>
&lt;li>The approach is operationally and conceptually simple. The leading etcd container and backup-restore sidecar are always located in the same pod.&lt;/li>
&lt;li>Network traffic between the backup container and the etcd cluster will always be local.&lt;/li>
&lt;/ul>
&lt;p>The disadvantage is that this approach may not age well in the future if we think about moving the backup-restore container as a separate pod rather than a sidecar container.&lt;/p>
&lt;h3 id="independent-leader-election-between-backup-restore-sidecars">Independent leader election between backup-restore sidecars&lt;/h3>
&lt;p>We could use the etcd &lt;code>lease&lt;/code> mechanism to perform leader election among the backup-restore sidecars. For example, using something like &lt;a href="https://pkg.go.dev/go.etcd.io/etcd/clientv3/concurrency#Election.Campaign">&lt;code>go.etcd.io/etcd/clientv3/concurrency&lt;/code>&lt;/a>.&lt;/p>
&lt;p>The advantage and disadvantages are pretty much the opposite of the approach &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#leading-etcd-main-containers-sidecar-is-the-backup-leader">above&lt;/a>.
The advantage being that this approach may age well in the future if we think about moving the backup-restore container as a separate pod rather than a sidecar container.&lt;/p>
&lt;p>The disadvantages are as follows.&lt;/p>
&lt;ul>
&lt;li>The approach is operationally and conceptually a bit complex. The leading etcd container and backup-restore sidecar might potentially belong to different pods.&lt;/li>
&lt;li>Network traffic between the backup container and the etcd cluster might potentially be across nodes.&lt;/li>
&lt;/ul>
&lt;h2 id="history-compaction">History Compaction&lt;/h2>
&lt;p>This proposal recommends to configure &lt;a href="https://etcd.io/docs/v3.2.17/op-guide/maintenance/#history-compaction">automatic history compaction&lt;/a> on the individual members.&lt;/p>
&lt;h2 id="defragmentation">Defragmentation&lt;/h2>
&lt;p>Defragmentation is already &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/0dfdd50fbfc5ebc88238be3bc79c3ac3fc242c08/cmd/options.go#L209">triggered periodically&lt;/a> by &lt;code>etcd-backup-restore&lt;/code>.
This proposal recommends to enhance this functionality to be performed only by the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup">leading&lt;/a> backup-restore container.
The defragmentation must be performed only when etcd cluster is in full health and must be done in a rolling manner for each members to &lt;a href="https://etcd.io/docs/v3.2.17/op-guide/maintenance/#defragmentation">avoid disruption&lt;/a>.
The leading member should be defragmented last after all the rest of the members have been defragmented to minimise potential leadership changes caused by defragmentation.
If the etcd cluster is unhealthy when it is time to trigger scheduled defragmentation, the defragmentation must be postponed until the cluster becomes healthy. This check must be done before triggering defragmentation for each member.&lt;/p>
&lt;h2 id="work-flows-in-etcd-backup-restore">Work-flows in etcd-backup-restore&lt;/h2>
&lt;p>There are different work-flows in etcd-backup-restore.
Some existing flows like initialization, scheduled backups and defragmentation have been enhanced or modified.
Some new work-flows like status updates have been introduced.
Some of these work-flows are sensitive to which &lt;code>etcd-backup-restore&lt;/code> container is &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup">leading&lt;/a> and some are not.&lt;/p>
&lt;p>The life-cycle of these work-flows is shown below.
&lt;img src="https://gardener.cloud/__resources/01-etcd-backup-restore-work-flows-life-cycle_eec586.png" alt="etcd-backup-restore work-flows life-cycle">&lt;/p>
&lt;h3 id="work-flows-independent-of-leader-election-in-all-members">Work-flows independent of leader election in all members&lt;/h3>
&lt;ul>
&lt;li>Serve the &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/master/pkg/server/httpAPI.go#L101-L107">HTTP API&lt;/a> that all members are expected to support currently but some HTTP API call which are used to take &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/5dfcc1f848a9f325d41a24eae4defb70d997c215/pkg/server/httpAPI.go#L103-L105">out-of-sync delta or full snapshot&lt;/a> should delegate the incoming HTTP requests to the &lt;code>leading-sidecar&lt;/code> and one of the possible approach to achieve this is via an &lt;a href="https://pkg.go.dev/net/http/httputil#ReverseProxy.ServeHTTP">HTTP reverse proxy&lt;/a>.&lt;/li>
&lt;li>Check the health of the respective etcd member and renew the corresponding &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member &lt;code>lease&lt;/code>&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="work-flows-only-on-the-leading-member">Work-flows only on the leading member&lt;/h3>
&lt;ul>
&lt;li>Take &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup">backups&lt;/a> (full and incremental) at configured regular intervals&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#defragmentation">Defragment&lt;/a> all the members sequentially at configured regular intervals&lt;/li>
&lt;li>Cleanup superflous members from the ETCD cluster for which there is no corresponding pod (the &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#ordinal-index">ordinal&lt;/a> in the pod name is greater than the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#clustersize">cluster size&lt;/a>) at regular intervals (or whenever the &lt;code>Etcd&lt;/code> resource &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">status&lt;/a> changes by watching it)
&lt;ul>
&lt;li>The cleanup of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status">superfluous entries in &lt;code>status.members&lt;/code> array&lt;/a> is already covered &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12">here&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="high-availability">High Availability&lt;/h2>
&lt;p>Considering that high-availability is the primary reason for using a multi-node etcd cluster, it makes sense to distribute the individual member pods of the etcd cluster across different physical nodes.
If the underlying Kubernetes cluster has nodes from multiple availability zones, it makes sense to also distribute the member pods across nodes from different availability zones.&lt;/p>
&lt;p>One possibility to do this is via &lt;a href="https://kubernetes.io/docs/reference/scheduling/policies/#priorities">&lt;code>SelectorSpreadPriority&lt;/code>&lt;/a> of &lt;code>kube-scheduler&lt;/code> but this is only &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone">best-effort&lt;/a> and may not always be enforced strictly.&lt;/p>
&lt;p>It is better to use &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">pod anti-affinity&lt;/a> to enforce such distribution of member pods.&lt;/p>
&lt;h3 id="zonal-cluster---single-availability-zone">Zonal Cluster - Single Availability Zone&lt;/h3>
&lt;p>A zonal cluster is configured to consist of nodes belonging to only a single availability zone in a region of the cloud provider.
In such a case, we can at best distribute the member pods of a multi-node etcd cluster instance only across different nodes in the configured availability zone.&lt;/p>
&lt;p>This can be done by specifying &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">pod anti-affinity&lt;/a> in the specification of the member pods using &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#kubernetes-io-hostname">&lt;code>kubernetes.io/hostname&lt;/code>&lt;/a> as the topology key.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: StatefulSet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> affinity:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> podAntiAffinity:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requiredDuringSchedulingIgnoredDuringExecution:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - labelSelector: {} &lt;span style="color:#008000"># podSelector that matches the member pods of the given etcd cluster instance&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologyKey: &lt;span style="color:#a31515">&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The recommendation is to keep &lt;code>etcd-druid&lt;/code> agnostic of such topics related scheduling and cluster-topology and to use &lt;a href="https://github.com/gardener/kupid">kupid&lt;/a> to &lt;a href="https://github.com/gardener/kupid#mutating-higher-order-controllers">orthogonally inject&lt;/a> the desired &lt;a href="https://github.com/gardener/kupid/blob/master/config/samples/cpsp-pod-affinity-anti-affinity.yaml">pod anti-affinity&lt;/a>.&lt;/p>
&lt;h4 id="alternative-5">Alternative&lt;/h4>
&lt;p>Another option is to build the functionality into &lt;code>etcd-druid&lt;/code> to include the required pod anti-affinity when it provisions the &lt;code>StatefulSet&lt;/code> that manages the member pods.
While this has the advantage of avoiding a dependency on an external component like &lt;a href="https://github.com/gardener/kupid">kupid&lt;/a>, the disadvantage is that we might need to address development or testing use-cases where it might be desirable to avoid distributing member pods and schedule them on as less number of nodes as possible.
Also, as mentioned &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#regional-cluster---multiple-availability-zones">below&lt;/a>, &lt;a href="https://github.com/gardener/kupid">kupid&lt;/a> can be used to distribute member pods of an etcd cluster instance across nodes in a single availability zone as well as across nodes in multiple availability zones with very minor variation.
This keeps the solution uniform regardless of the topology of the underlying Kubernetes cluster.&lt;/p>
&lt;h3 id="regional-cluster---multiple-availability-zones">Regional Cluster - Multiple Availability Zones&lt;/h3>
&lt;p>A regional cluster is configured to consist of nodes belonging to multiple availability zones (typically, three) in a region of the cloud provider.
In such a case, we can distribute the member pods of a multi-node etcd cluster instance across nodes belonging to different availability zones.&lt;/p>
&lt;p>This can be done by specifying &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">pod anti-affinity&lt;/a> in the specification of the member pods using &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone">&lt;code>topology.kubernetes.io/zone&lt;/code>&lt;/a> as the topology key.
In Kubernetes clusters using Kubernetes release older than &lt;code>1.17&lt;/code>, the older (and now deprecated) &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#failure-domainbetakubernetesiozone">&lt;code>failure-domain.beta.kubernetes.io/zone&lt;/code>&lt;/a> might have to be used as the topology key.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: StatefulSet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> affinity:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> podAntiAffinity:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requiredDuringSchedulingIgnoredDuringExecution:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - labelSelector: {} &lt;span style="color:#008000"># podSelector that matches the member pods of the given etcd cluster instance&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologyKey: &amp;#34;topology.kubernetes.io/zone
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The recommendation is to keep &lt;code>etcd-druid&lt;/code> agnostic of such topics related scheduling and cluster-topology and to use &lt;a href="https://github.com/gardener/kupid">kupid&lt;/a> to &lt;a href="https://github.com/gardener/kupid#mutating-higher-order-controllers">orthogonally inject&lt;/a> the desired &lt;a href="https://github.com/gardener/kupid/blob/master/config/samples/cpsp-pod-affinity-anti-affinity.yaml">pod anti-affinity&lt;/a>.&lt;/p>
&lt;h4 id="alternative-6">Alternative&lt;/h4>
&lt;p>Another option is to build the functionality into &lt;code>etcd-druid&lt;/code> to include the required pod anti-affinity when it provisions the &lt;code>StatefulSet&lt;/code> that manages the member pods.
While this has the advantage of avoiding a dependency on an external component like &lt;a href="https://github.com/gardener/kupid">kupid&lt;/a>, the disadvantage is that such built-in support necessarily limits what kind of topologies of the underlying cluster will be supported.
Hence, it is better to keep &lt;code>etcd-druid&lt;/code> altogether agnostic of issues related to scheduling and cluster-topology.&lt;/p>
&lt;h3 id="poddisruptionbudget">PodDisruptionBudget&lt;/h3>
&lt;p>This proposal recommends that &lt;code>etcd-druid&lt;/code> should deploy &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets">&lt;code>PodDisruptionBudget&lt;/code>&lt;/a> (&lt;code>minAvailable&lt;/code> set to &lt;code>floor(&amp;lt;cluster size&amp;gt;/2) + 1&lt;/code>) for multi-node etcd clusters (if &lt;code>AllMembersReady&lt;/code> &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">condition&lt;/a> is &lt;code>true&lt;/code>) to ensure that any planned disruptive operation can try and honour the disruption budget to ensure high availability of the etcd cluster while making potentially disrupting maintenance operations.&lt;/p>
&lt;p>Also, it is recommended to toggle the &lt;code>minAvailable&lt;/code> field between &lt;code>floor(&amp;lt;cluster size&amp;gt;/2)&lt;/code> and &lt;code>&amp;lt;number of members with status Ready true&amp;gt;&lt;/code> whenever the &lt;code>AllMembersReady&lt;/code> condition toggles between &lt;code>true&lt;/code> and &lt;code>false&lt;/code>.
This is to disable eviction of any member pods when not all members are &lt;code>Ready&lt;/code>.&lt;/p>
&lt;p>In case of a conflict, the recommendation is to use the highest of the applicable values for &lt;code>minAvailable&lt;/code>.&lt;/p>
&lt;h2 id="rolling-updates-to-etcd-members">Rolling updates to etcd members&lt;/h2>
&lt;p>Any changes to the &lt;code>Etcd&lt;/code> resource spec that might result in a change to &lt;code>StatefulSet&lt;/code> spec or otherwise result in a rolling update of member pods should be applied/propagated by &lt;code>etcd-druid&lt;/code> only when the etcd cluster is fully healthy to reduce the risk of quorum loss during the updates.
This would include vertical autoscaling changes (via, &lt;a href="https://github.com/gardener/hvpa-controller">HVPA&lt;/a>).
If the cluster &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">status&lt;/a> unhealthy (i.e. if either &lt;code>AllMembersReady&lt;/code> or &lt;code>BackupReady&lt;/code> &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">conditions&lt;/a> are &lt;code>false&lt;/code>), &lt;code>etcd-druid&lt;/code> must restore it to full health &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure">before proceeding&lt;/a> with such operations that lead to rolling updates.
This can be further optimized in the future to handle the cases where rolling updates can still be performed on an etcd cluster that is not fully healthy.&lt;/p>
&lt;h2 id="follow-up">Follow Up&lt;/h2>
&lt;h3 id="ephemeral-volumes">Ephemeral Volumes&lt;/h3>
&lt;p>See section &lt;em>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#Ephemeral_Volumes">Ephemeral Volumes&lt;/a>&lt;/em>.&lt;/p>
&lt;h3 id="shoot-control-plane-migration">Shoot Control-Plane Migration&lt;/h3>
&lt;p>This proposal adds support for multi-node etcd clusters but it should not have significant impact on &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md">shoot control-plane migration&lt;/a> any more than what already present in the single-node etcd cluster scenario.
But to be sure, this needs to be discussed further.&lt;/p>
&lt;h3 id="performance-impact-of-multi-node-etcd-clusters">Performance impact of multi-node etcd clusters&lt;/h3>
&lt;p>Multi-node etcd clusters incur a cost on &lt;a href="https://etcd.io/docs/v2/admin_guide/#optimal-cluster-size">write performance&lt;/a> as compared to single-node etcd clusters.
This performance impact needs to be measured and documented.
Here, we should compare different persistence option for the multi-nodeetcd clusters so that we have all the information necessary to take the decision balancing the high-availability, performance and costs.&lt;/p>
&lt;h3 id="metrics-dashboards-and-alerts">Metrics, Dashboards and Alerts&lt;/h3>
&lt;p>There are already metrics exported by etcd and &lt;code>etcd-backup-restore&lt;/code> which are visualized in monitoring dashboards and also used in triggering alerts.
These might have hidden assumptions about single-node etcd clusters.
These might need to be enhanced and potentially new metrics, dashboards and alerts configured to cover the multi-node etcd cluster scenario.&lt;/p>
&lt;p>Especially, a high priority alert must be raised if &lt;code>BackupReady&lt;/code> &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#condition">condition&lt;/a> becomes &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure">&lt;code>false&lt;/code>&lt;/a>.&lt;/p>
&lt;h3 id="costs">Costs&lt;/h3>
&lt;p>Multi-node etcd clusters will clearly involve higher cost (when compared with single-node etcd clusters) just going by the CPU and memory usage for the additional members.
Also, the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#data-persistence">different options&lt;/a> for persistence for etcd data for the members will have different cost implications.
Such cost impact needs to be assessed and documented to help navigate the trade offs between high availability, performance and costs.&lt;/p>
&lt;h2 id="future-work">Future Work&lt;/h2>
&lt;h3 id="gardener-ring">Gardener Ring&lt;/h3>
&lt;p>&lt;a href="https://github.com/gardener/gardener/issues/233">Gardener Ring&lt;/a>, requires provisioning and management of an etcd cluster with the members distributed across more than one Kubernetes cluster.
This cannot be achieved by etcd-druid alone which has only the view of a single Kubernetes cluster.
An additional component that has the view of all the Kubernetes clusters involved in setting up the gardener ring will be required to achieve this.
However, etcd-druid can be used by such a higher-level component/controller (for example, by supplying the initial cluster configuration) such that individual etcd-druid instances in the individual Kubernetes clusters can manage the corresponding etcd cluster members.&lt;/p>
&lt;h3 id="autonomous-shoot-clusters">Autonomous Shoot Clusters&lt;/h3>
&lt;p>&lt;a href="https://github.com/gardener/gardener/issues/2906">Autonomous Shoot Clusters&lt;/a> also will require a highly availble etcd cluster to back its control-plane and the multi-node support proposed here can be leveraged in that context.
However, the current proposal will not meet all the needs of a autonomous shoot cluster.
Some additional components will be required that have the overall view of the autonomous shoot cluster and they can use etcd-druid to manage the multi-node etcd cluster. But this scenario may be different from that of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#gardener-ring">Gardener Ring&lt;/a> in that the individual etcd members of the cluster may not be hosted on different Kubernetes clusters.&lt;/p>
&lt;h3 id="optimization-of-recovery-from-non-quorate-cluster-with-some-member-containing-valid-data">Optimization of recovery from non-quorate cluster with some member containing valid data&lt;/h3>
&lt;p>It might be possible to optimize the actions during the recovery of a non-quorate cluster where some of the members contain valid data and some other don&amp;rsquo;t.
The optimization involves verifying the data of the valid members to determine the data of which member is the most recent (even considering the latest backup) so that the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">full snapshot&lt;/a> can be taken from it before recovering the etcd cluster.
Such an optimization can be attempted in the future.&lt;/p>
&lt;h3 id="optimization-of-rolling-updates-to-unhealthy-etcd-clusters">Optimization of rolling updates to unhealthy etcd clusters&lt;/h3>
&lt;p>As mentioned &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#rolling-updates-to-etcd-members">above&lt;/a>, optimizations to proceed with rolling updates to unhealthy etcd clusters (without first restoring the cluster to full health) can be pursued in future work.&lt;/p></description></item><item><title>Docs: 02 Snapshot Compaction</title><link>https://gardener.cloud/docs/other-components/etcd-druid/proposals/02-snapshot-compaction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-druid/proposals/02-snapshot-compaction/</guid><description>
&lt;h1 id="snapshot-compaction-for-etcd">Snapshot Compaction for Etcd&lt;/h1>
&lt;h2 id="current-problem">Current Problem&lt;/h2>
&lt;p>To ensure recoverability of Etcd, backups of the database are taken at regular interval.
Backups are of two types: Full Snapshots and Incremental Snapshots.&lt;/p>
&lt;h3 id="full-snapshots">Full Snapshots&lt;/h3>
&lt;p>Full snapshot is a snapshot of the complete database at given point in time.The size of the database keeps changing with time and typically the size is relatively large (measured in 100s of megabytes or even in gigabytes. For this reason, full snapshots are taken after some large intervals.&lt;/p>
&lt;h3 id="incremental-snapshots">Incremental Snapshots&lt;/h3>
&lt;p>Incremental Snapshots are collection of events on Etcd database, obtained through running WATCH API Call on Etcd. After some short intervals, all the events that are accumulated through WATCH API Call are saved in a file and named as Incremental Snapshots at relatively short time intervals.&lt;/p>
&lt;h3 id="recovery-from-the-snapshots">Recovery from the Snapshots&lt;/h3>
&lt;h4 id="recovery-from-full-snapshots">Recovery from Full Snapshots&lt;/h4>
&lt;p>As the full snapshots are snapshots of the complete database, the whole database can be recovered from a full snapshot in one go. Etcd provides API Call to restore the database from a full snapshot file.&lt;/p>
&lt;h4 id="recovery-from-incremental-snapshots">Recovery from Incremental Snapshots&lt;/h4>
&lt;p>Delta snapshots are collection of retrospective Etcd events. So, to restore from Incremental snapshot file, the events from the file are needed to be applied sequentially on Etcd database through Etcd Put/Delete API calls. As it is heavily dependent on Etcd calls sequentially, restoring from Incremental Snapshot files can take long if there are numerous commands captured in Incremental Snapshot files.&lt;/p>
&lt;p>Delta snapshots are applied on top of running Etcd database. So, if there is inconsistency between the state of database at the point of applying and the state of the database when the delta snapshot commands were captured, restoration will fail.&lt;/p>
&lt;p>Currently, in Gardener setup, Etcd is restored from the last full snapshot and then the delta snapshots, which were captured after the last full snapshot.&lt;/p>
&lt;p>The main problem with this is that the complete restoration time can be unacceptably large if the rate of change coming into the etcd database is quite high because there are large number of events in the delta snapshots to be applied sequentially.
A secondary problem is that, though auto-compaction is enabled for etcd, it is not quick enough to compact all the changes from the incremental snapshots being re-applied during the relatively short period of time of restoration (as compared to the actual period of time when the incremental snapshots were accumulated). This may lead to the etcd pod (the backup-restore sidecar container, to be precise) to run out of memory and/or storage space even if it is sufficient for normal operations.&lt;/p>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;h3 id="compaction-command">Compaction command&lt;/h3>
&lt;p>To help with the problem mentioned earlier, our proposal is to introduce &lt;code>compact&lt;/code> subcommand with &lt;code>etcdbrctl&lt;/code>. On execution of &lt;code>compact&lt;/code> command, A separate embedded Etcd process will be started where the Etcd data will be restored from the snapstore (exactly as in the restoration scenario today). Then the new Etcd database will be compacted and defragmented using Etcd API calls. The compaction will strip off the Etcd database of old revisions as per the Etcd auto-compaction configuration. The defragmentation will free up the unused fragment memory space released after compaction. Then a full snapshot of the compacted database will be saved in snapstore which then can be used as the base snapshot during any subsequent restoration (or backup compaction).&lt;/p>
&lt;h3 id="how-the-solution-works">How the solution works&lt;/h3>
&lt;p>The newly introduced compact command does not disturb the running Etcd while compacting the backup snapshots. The command is designed to run potentially separately (from the main Etcd process/container/pod). Etcd Druid can be configured to run the newly introduced compact command as a separate job (scheduled periodically) based on total number of Etcd events accumulated after the most recent full snapshot.&lt;/p>
&lt;h3 id="etcd-druid-flags">Etcd-druid flags:&lt;/h3>
&lt;p>Etcd-druid introduces the following flags to configure the compaction job:&lt;/p>
&lt;ul>
&lt;li>&lt;code>--enable-backup-compaction&lt;/code> (default &lt;code>false&lt;/code>): Set this flag to &lt;code>true&lt;/code> to enable the automatic compaction of etcd backups when the threshold value denoted by CLI flag &lt;code>--etcd-events-threshold&lt;/code> is exceeded.&lt;/li>
&lt;li>&lt;code>--compaction-workers&lt;/code> (default &lt;code>3&lt;/code>): Number of worker threads of the CompactionJob controller. The controller creates a backup compaction job if a certain etcd event threshold is reached. If compaction is enabled, the value for this flag must be greater than zero.&lt;/li>
&lt;li>&lt;code>--etcd-events-threshold&lt;/code> (default &lt;code>1000000&lt;/code>): Total number of etcd events that can be allowed before a backup compaction job is triggered.&lt;/li>
&lt;li>&lt;code>--active-deadline-duration&lt;/code> (default &lt;code>3h&lt;/code>): Duration after which a running backup compaction job will be terminated.&lt;/li>
&lt;li>&lt;code>--metrics-scrape-wait-duration&lt;/code> (default &lt;code>0s&lt;/code>): Duration to wait for after compaction job is completed, to allow Prometheus metrics to be scraped.&lt;/li>
&lt;/ul>
&lt;h3 id="points-to-take-care-while-saving-the-compacted-snapshot">&lt;strong>Points to take care while saving the compacted snapshot:&lt;/strong>&lt;/h3>
&lt;p>As compacted snapshot and the existing periodic full snapshots are taken by different processes running in different pods but accessing same store to save the snapshots, some problems may arise:&lt;/p>
&lt;ol>
&lt;li>When uploading the compacted snapshot to the snapstore, there is the problem of how does the restorer know when to start using the newly compacted snapshot. This communication needs to be atomic.&lt;/li>
&lt;li>With a regular schedule for compaction that happens potentially separately from the main etcd pod, is there a need for regular scheduled full snapshots anymore?&lt;/li>
&lt;li>We are planning to introduce new directory structure, under v2 prefix, for saving the snapshots (compacted and full), as mentioned in details below. But for backward compatibility, we also need to consider the older directory, which is currently under v1 prefix, during accessing snapshots.&lt;/li>
&lt;/ol>
&lt;h4 id="how-to-swap-full-snapshot-with-compacted-snapshot-atomically">&lt;strong>How to swap full snapshot with compacted snapshot atomically&lt;/strong>&lt;/h4>
&lt;p>Currently, full snapshots and the subsequent delta snapshots are grouped under same prefix path in the snapstore. When a full snapshot is created, it is placed under a prefix/directory with the name comprising of timestamp. Then subsequent delta snapshots are also pushed into the same directory. Thus each prefix/directory contains a single full snapshot and the subsequent delta snapshots. So far, it is the job of ETCDBR to start main Etcd process and snapshotter process which takes full snapshot and delta snapshot periodically. But as per our proposal, compaction will be running as parallel process to main Etcd process and snapshotter process. So we can&amp;rsquo;t reliably co-ordinate between the processes to achieve switching to the compacted snapshot as the base snapshot atomically.&lt;/p>
&lt;h5 id="current-directory-structure">&lt;strong>Current Directory Structure&lt;/strong>&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>- Backup-192345
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Full-Snapshot-0-1-192345
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-1-100-192355
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-100-200-192365
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-200-300-192375
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- Backup-192789
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Full-Snapshot-0-300-192789
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-300-400-192799
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-400-500-192809
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-500-600-192819
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To solve the problem, proposal is:&lt;/p>
&lt;ol>
&lt;li>ETCDBR will take the first full snapshot after it starts main Etcd Process and snapshotter process. After taking the first full snapshot, snapshotter will continue taking full snapshots. On the other hand, ETCDBR compactor command will be run as periodic job in a separate pod and use the existing full or compacted snapshots to produce further compacted snapshots. Full snapshots and compacted snapshots will be named after same fashion. So, there is no need of any mechanism to choose which snapshots(among full and compacted snapshot) to consider as base snapshots.&lt;/li>
&lt;li>Flatten the directory structure of backup folder. Save all the full snapshots, delta snapshots and compacted snapshots under same directory/prefix. Restorer will restore from full/compacted snapshots and delta snapshots sorted based on the revision numbers in name (or timestamp if the revision numbers are equal).&lt;/li>
&lt;/ol>
&lt;h5 id="proposed-directory-structure">&lt;strong>Proposed Directory Structure&lt;/strong>&lt;/h5>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>Backup :
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Full-Snapshot-0-1-192355 (Taken by snapshotter)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-revision-1-100-192365
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-revision-100-200-192375
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Full-Snapshot-revision-0-200-192379 (Taken by snapshotter)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-revision-200-300-192385
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Full-Snapshot-revision-0-300-192386 (Taken by compaction job)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-revision-300-400-192396
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-revision-400-500-192406
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-revision-500-600-192416
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Full-Snapshot-revision-0-600-192419 (Taken by snapshotter)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Full-Snapshot-revision-0-600-192420 (Taken by compaction job)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="what-happens-to-the-delta-snapshots-that-were-compacted">What happens to the delta snapshots that were compacted?&lt;/h5>
&lt;p>The proposed &lt;code>compaction&lt;/code> sub-command in &lt;code>etcdbrctl&lt;/code> (and hence, the &lt;code>CronJob&lt;/code> provisioned by &lt;code>etcd-druid&lt;/code> that will schedule it at a regular interval) would only upload the compacted full snapshot.
It will not delete the snapshots (delta or full snapshots) that were compacted.
These snapshots which were superseded by a freshly uploaded compacted snapshot would follow the same life-cycle as other older snapshots.
I.e. they will be garbage collected according to the configured backup snapshot retention policy.
For example, if an &lt;code>exponential&lt;/code> retention policy is configured and if compaction is done every &lt;code>30m&lt;/code> then there might be at most &lt;code>48&lt;/code> additional (compacted) full snapshots (&lt;code>24h * 2&lt;/code>) in the backup for the latest day. As time rolls forward to the next day, these additional compacted snapshots (along with the delta snapshots that were compacted into them) will get garbage collected retaining only one full snapshot for the day before according to the retention policy.&lt;/p>
&lt;h5 id="future-work">&lt;strong>Future work&lt;/strong>&lt;/h5>
&lt;p>In the future, we have plan to stop the snapshotter just after taking the first full snapshot. Then, the compaction job will be solely responsible for taking subsequent full snapshots. The directory structure would be looking like following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>Backup :
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Full-Snapshot-0-1-192355 (Taken by snapshotter)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-revision-1-100-192365
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-revision-100-200-192375
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-revision-200-300-192385
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Full-Snapshot-revision-0-300-192386 (Taken by compaction job)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-revision-300-400-192396
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-revision-400-500-192406
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Incremental-Snapshot-revision-500-600-192416
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - Full-Snapshot-revision-0-600-192420 (Taken by compaction job)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="backward-compatibility">Backward Compatibility&lt;/h4>
&lt;ol>
&lt;li>&lt;strong>Restoration&lt;/strong> : The changes to handle the newly proposed backup directory structure must be backward compatible with older structures at least for restoration because we need have to restore from backups in the older structure. This includes the support for restoring from a backup without a metadata file if that is used in the actual implementation.&lt;/li>
&lt;li>&lt;strong>Backup&lt;/strong> : For new snapshots (even on a backup containing the older structure), the new structure may be used. The new structure must be setup automatically including creating the base full snapshot.&lt;/li>
&lt;li>&lt;strong>Garbage collection&lt;/strong> : The existing functionality of garbage collection of snapshots (full and incremental) according to the backup retention policy must be compatible with both old and new backup folder structure. I.e. the snapshots in the older backup structure must be retained in their own structure and the snapshots in the proposed backup structure should be retained in the proposed structure. Once all the snapshots in the older backup structure go out of the retention policy and are garbage collected, we can think of removing the support for older backup folder structure.&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Note:&lt;/strong> Compactor will run parallel to current snapshotter process and work only if there is any full snapshot already present in the store. By current design, a full snapshot will be taken if there is already no full snapshot or the existing full snapshot is older than 24 hours. It is not limitation but a design choice. As per proposed design, the backup storage will contain both periodic full snapshots as well as periodic compacted snapshot. Restorer will pickup the base snapshot whichever is latest one.&lt;/p></description></item><item><title>Docs: 03 Scaling Up An Etcd Cluster</title><link>https://gardener.cloud/docs/other-components/etcd-druid/proposals/03-scaling-up-an-etcd-cluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-druid/proposals/03-scaling-up-an-etcd-cluster/</guid><description>
&lt;h1 id="scaling-up-a-single-node-to-multi-node-etcd-cluster-deployed-by-etcd-druid">Scaling-up a single-node to multi-node etcd cluster deployed by etcd-druid&lt;/h1>
&lt;p>To mark a cluster for scale-up from single node to multi-node etcd, just patch the etcd custom resource&amp;rsquo;s &lt;code>.spec.replicas&lt;/code> from &lt;code>1&lt;/code> to &lt;code>3&lt;/code> (for example).&lt;/p>
&lt;h2 id="challenges-for-scale-up">Challenges for scale-up&lt;/h2>
&lt;ol>
&lt;li>Etcd cluster with single replica don&amp;rsquo;t have any peers, so no peer communication is required hence peer URL may or may not be TLS enabled. However, while scaling up from single node etcd to multi-node etcd, there will be a requirement to have peer communication between members of the etcd cluster. Peer communication is required for various reasons, for instance for members to sync up cluster state, data, and to perform leader election or any cluster wide operation like removal or addition of a member etc. Hence in a multi-node etcd cluster we need to have TLS enable peer URL for peer communication.&lt;/li>
&lt;li>Providing the correct configuration to start new etcd members as it is different from boostrapping a cluster since these new etcd members will join an existing cluster.&lt;/li>
&lt;/ol>
&lt;h2 id="approach">Approach&lt;/h2>
&lt;p>We first went through the etcd doc of &lt;a href="https://etcd.io/docs/v3.4/op-guide/runtime-configuration/#update-advertise-peer-urls">update-advertise-peer-urls&lt;/a> to find out information regarding peer URL updation. Interestingly, etcd doc has mentioned the following:&lt;/p>
&lt;pre tabindex="0">&lt;code>To update the advertise peer URLs of a member, first update it explicitly via member command and then restart the member.
&lt;/code>&lt;/pre>&lt;p>But we can&amp;rsquo;t assume peer URL is not TLS enabled for single-node cluster as it depends on end-user. A user may or may not enable the TLS for peer URL for a single node etcd cluster. So, How do we detect whether peer URL was enabled or not when cluster is marked for scale-up?&lt;/p>
&lt;h2 id="detecting-if-peerurl-tls-is-enabled-or-not">Detecting if peerURL TLS is enabled or not&lt;/h2>
&lt;p>For this we use an annotation in member lease object &lt;code>member.etcd.gardener.cloud/tls-enabled&lt;/code> set by backup-restore sidecar of etcd. As etcd configuration is provided by backup-restore, so it can find out whether TLS is enabled or not and accordingly set this annotation &lt;code>member.etcd.gardener.cloud/tls-enabled&lt;/code> to either &lt;code>true&lt;/code> or &lt;code>false&lt;/code> in member lease object.
And with the help of this annotation and config-map values etcd-druid is able to detect whether there is a change in a peer URL or not.&lt;/p>
&lt;h2 id="etcd-druid-helps-in-scaling-up-etcd-cluster">Etcd-Druid helps in scaling up etcd cluster&lt;/h2>
&lt;p>Now, it is detected whether peer URL was TLS enabled or not for single node etcd cluster. Etcd-druid can now use this information to take action:&lt;/p>
&lt;ul>
&lt;li>If peer URL was already TLS enabled then no action is required from etcd-druid side. Etcd-druid can proceed with scaling up the cluster.&lt;/li>
&lt;li>If peer URL was not TLS enabled then etcd-druid has to intervene and make sure peer URL should be TLS enabled first for the single node before marking the cluster for scale-up.&lt;/li>
&lt;/ul>
&lt;h2 id="action-taken-by-etcd-druid-to-enable-the-peerurl-tls">Action taken by etcd-druid to enable the peerURL TLS&lt;/h2>
&lt;ol>
&lt;li>Etcd-druid will update the &lt;code>etcd-bootstrap&lt;/code> config-map with new config like initial-cluster,initial-advertise-peer-urls etc. Backup-restore will detect this change and update the member lease annotation to &lt;code>member.etcd.gardener.cloud/tls-enabled: &amp;quot;true&amp;quot;&lt;/code>.&lt;/li>
&lt;li>In case the peer URL TLS has been changed to &lt;code>enabled&lt;/code>: Etcd-druid will add tasks to the deployment flow:
&lt;ul>
&lt;li>Check if peer TLS has been enabled for existing StatefulSet pods, by checking the member leases for the annotation &lt;code>member.etcd.gardener.cloud/tls-enabled&lt;/code>.&lt;/li>
&lt;li>If peer TLS enablement is pending for any of the members, then check and patch the StatefulSet with the peer TLS volume mounts, if not already patched. This will cause a rolling update of the existing StatefulSet pods, which allows etcd-backup-restore to update the member peer URL in the etcd cluster.&lt;/li>
&lt;li>Requeue this reconciliation flow until peer TLS has been enabled for all the existing etcd members.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="after-peerurl-is-tls-enabled">After PeerURL is TLS enabled&lt;/h2>
&lt;p>After peer URL TLS enablement for single node etcd cluster, now etcd-druid adds a scale-up annotation: &lt;code>gardener.cloud/scaled-to-multi-node&lt;/code> to the etcd statefulset and etcd-druid will patch the statefulsets &lt;code>.spec.replicas&lt;/code> to &lt;code>3&lt;/code>(for example). The statefulset controller will then bring up new pods(etcd with backup-restore as a sidecar). Now etcd&amp;rsquo;s sidecar i.e backup-restore will check whether this member is already a part of a cluster or not and incase it is unable to check (may be due to some network issues) then backup-restore checks presence of this annotation: &lt;code>gardener.cloud/scaled-to-multi-node&lt;/code> in etcd statefulset to detect scale-up. If it finds out it is the scale-up case then backup-restore adds new etcd member as a &lt;a href="https://etcd.io/docs/v3.3/learning/learner/">learner&lt;/a> first and then starts the etcd learner by providing the correct configuration. Once learner gets in sync with the etcd cluster leader, it will get promoted to a voting member.&lt;/p>
&lt;h2 id="providing-the-correct-etcd-config">Providing the correct etcd config&lt;/h2>
&lt;p>As backup-restore detects that it&amp;rsquo;s a scale-up scenario, backup-restore sets &lt;code>initial-cluster-state&lt;/code> to &lt;code>existing&lt;/code> as this member will join an existing cluster and it calculates the rest of the config from the updated config-map provided by etcd-druid.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/03-scale-up-sequenceDiagram_76558b.png" alt="Sequence diagram">&lt;/p>
&lt;h2 id="future-improvements">Future improvements:&lt;/h2>
&lt;p>The need of restarting etcd pods twice will change in the future. please refer: &lt;a href="https://github.com/gardener/etcd-backup-restore/issues/538">https://github.com/gardener/etcd-backup-restore/issues/538&lt;/a>&lt;/p></description></item><item><title>Docs: Access Restrictions</title><link>https://gardener.cloud/docs/dashboard/access-restrictions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/dashboard/access-restrictions/</guid><description>
&lt;h1 id="access-restrictions">Access Restrictions&lt;/h1>
&lt;p>The dashboard can be configured with access restrictions.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/access-restrictions-1_071de9.png">
&lt;p>Access restrictions are shown for regions that have a matching label in the &lt;code>CloudProfile&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span> regions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: pangaea-north-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> zones:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: pangaea-north-1a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: pangaea-north-1b
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: pangaea-north-1c
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seed.gardener.cloud/eu-access: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>If the user selects the access restriction, &lt;code>spec.seedSelector.matchLabels[key]&lt;/code> will be set.&lt;/li>
&lt;li>When selecting an option, &lt;code>metadata.annotations[optionKey]&lt;/code> will be set.&lt;/li>
&lt;/ul>
&lt;p>The value that is set depends on the configuration. See &lt;em>2.&lt;/em> under &lt;em>Configuration&lt;/em> section below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> support.gardener.cloud/eu-access-for-cluster-addons: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> support.gardener.cloud/eu-access-for-cluster-nodes: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seedSelector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seed.gardener.cloud/eu-access: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In order for the shoot (with enabled access restriction) to be scheduled on a seed, the seed needs to have the label set. E.g.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seed.gardener.cloud/eu-access: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;img src="https://gardener.cloud/__resources/access-restrictions-2_2e2c49.png">
&lt;p>&lt;strong>Configuration&lt;/strong>
As gardener administrator:&lt;/p>
&lt;ol>
&lt;li>you can control the visibility of the chips with the &lt;code>accessRestriction.items[].display.visibleIf&lt;/code> and &lt;code>accessRestriction.items[].options[].display.visibleIf&lt;/code> property. E.g. in this example the access restriction chip is shown if the value is true and the option is shown if the value is false.&lt;/li>
&lt;li>you can control the value of the input field (switch / checkbox) with the &lt;code>accessRestriction.items[].input.inverted&lt;/code> and &lt;code>accessRestriction.items[].options[].input.inverted&lt;/code> property. Setting the &lt;code>inverted&lt;/code> property to &lt;code>true&lt;/code> will invert the value. That means that when selecting the input field the value will be&lt;code>'false'&lt;/code> instead of &lt;code>'true'&lt;/code>.&lt;/li>
&lt;li>you can configure the text that is displayed when no access restriction options are available by setting &lt;code>accessRestriction.noItemsText&lt;/code>
example &lt;code>values.yaml&lt;/code>:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>accessRestriction:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> noItemsText: &lt;span style="color:#00f">No&lt;/span> access restriction options available for region {region} and cloud profile {cloudProfile}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> items:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: seed.gardener.cloud/eu-access
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> display:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> visibleIf: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># title: foo # optional title, if not defined key will be used&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># description: bar # optional description displayed in a tooltip&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> input:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> title: EU Access
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> description: |&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &lt;/span> This service is offered to you with our regular SLAs and 24x7 support for the control plane of the cluster. 24x7 support for cluster add-ons and nodes is only available if you meet the following conditions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> options:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: support.gardener.cloud/eu-access-for-cluster-addons
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> display:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> visibleIf: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># title: bar # optional title, if not defined key will be used&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># description: baz # optional description displayed in a tooltip&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> input:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> title: &lt;span style="color:#00f">No&lt;/span> personal data is used as name or in the content of Gardener or Kubernetes resources (e.g. Gardener project name or Kubernetes namespace, configMap or secret in Gardener or Kubernetes)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> description: |&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &lt;/span> If you can&amp;#39;t comply, only third-level/dev support at usual 8x5 working hours in EEA will be available to you for all cluster add-ons such as DNS and certificates, Calico overlay network and network policies, kube-proxy and services, and everything else that would require direct inspection of your cluster through its API server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inverted: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: support.gardener.cloud/eu-access-for-cluster-nodes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> display:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> visibleIf: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> input:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> title: &lt;span style="color:#00f">No&lt;/span> personal data is stored in any Kubernetes volume except for container file system, emptyDirs, and persistentVolumes (in particular, not on hostPath volumes)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> description: |&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &lt;/span> If you can&amp;#39;t comply, only third-level/dev support at usual 8x5 working hours in EEA will be available to you for all node-related components such as Docker and Kubelet, the operating system, and everything else that would require direct inspection of your nodes through a privileged pod or SSH
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> inverted: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Access to the Garden Cluster for Extensions</title><link>https://gardener.cloud/docs/gardener/extensions/garden-api-access/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/extensions/garden-api-access/</guid><description>
&lt;h1 id="access-to-the-garden-cluster-for-extensions">Access to the Garden Cluster for Extensions&lt;/h1>
&lt;p>Extensions that are installed on seed clusters via a &lt;code>ControllerInstallation&lt;/code> can simply read the kubeconfig file specified by the &lt;code>GARDEN_KUBECONFIG&lt;/code> environment variable to create a garden cluster client.
With this, they use a short-lived token (valid for &lt;code>12h&lt;/code>) associated with a dedicated &lt;code>ServiceAccount&lt;/code> in the &lt;code>seed-&amp;lt;seed-name&amp;gt;&lt;/code> namespace to securely access the garden cluster.
The used &lt;code>ServiceAccounts&lt;/code> are granted permissions in the garden cluster similar to gardenlet clients.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Historically, &lt;code>gardenlet&lt;/code> has been the only component running in the seed cluster that has access to both the seed cluster and the garden cluster.
Accordingly, extensions running on the seed cluster didn&amp;rsquo;t have access to the garden cluster.&lt;/p>
&lt;p>Starting from Gardener &lt;a href="https://github.com/gardener/gardener/releases/v1.74.0">&lt;code>v1.74.0&lt;/code>&lt;/a>, there is a new mechanism for components running on seed clusters to get access to the garden cluster.
For this, &lt;code>gardenlet&lt;/code> runs an instance of the &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/#tokenrequestor-controller">&lt;code>TokenRequestor&lt;/code>&lt;/a> for requesting tokens that can be used to communicate with the garden cluster.&lt;/p>
&lt;h2 id="using-gardenlet-managed-garden-access">Using Gardenlet-Managed Garden Access&lt;/h2>
&lt;p>By default, extensions are equipped with secure access to the garden cluster using a dedicated &lt;code>ServiceAccount&lt;/code> without requiring any additional action.
They can simply read the file specified by the &lt;code>GARDEN_KUBECONFIG&lt;/code> and construct a garden client with it.&lt;/p>
&lt;p>When installing a &lt;a href="https://gardener.cloud/docs/gardener/extensions/controllerregistration/">&lt;code>ControllerInstallation&lt;/code>&lt;/a>, gardenlet creates two secrets in the installation&amp;rsquo;s namespace: a generic garden kubeconfig (&lt;code>generic-garden-kubeconfig-&amp;lt;hash&amp;gt;&lt;/code>) and a garden access secret (&lt;code>garden-access-extension&lt;/code>).
Note that the &lt;code>ServiceAccount&lt;/code> created based on this access secret will be created in the respective &lt;code>seed-*&lt;/code> namespace in the garden cluster and labelled with &lt;code>controllerregistration.core.gardener.cloud/name=&amp;lt;name&amp;gt;&lt;/code>.&lt;/p>
&lt;p>Additionally, gardenlet injects &lt;code>volume&lt;/code>, &lt;code>volumeMounts&lt;/code>, and two environment variables into all (init) containers in all objects in the &lt;code>apps&lt;/code> and &lt;code>batch&lt;/code> API groups:&lt;/p>
&lt;ul>
&lt;li>&lt;code>GARDEN_KUBECONFIG&lt;/code>: points to the path where the generic garden kubeconfig is mounted.&lt;/li>
&lt;li>&lt;code>SEED_NAME&lt;/code>: set to the name of the &lt;code>Seed&lt;/code> where the extension is installed.
This is useful for restricting watches in the garden cluster to relevant objects.&lt;/li>
&lt;/ul>
&lt;p>If an object already contains the &lt;code>GARDEN_KUBECONFIG&lt;/code> environment variable, it is not overwritten and injection of &lt;code>volume&lt;/code> and &lt;code>volumeMounts&lt;/code> is skipped.&lt;/p>
&lt;p>For example, a &lt;code>Deployment&lt;/code> deployed via a &lt;code>ControllerInstallation&lt;/code> will be mutated as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Deployment
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gardener-extension-provider-local
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reference.resources.gardener.cloud/secret-795f7ca6: garden-access-extension
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reference.resources.gardener.cloud/secret-d5f5a834: generic-garden-kubeconfig-81fb3a88
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reference.resources.gardener.cloud/secret-795f7ca6: garden-access-extension
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reference.resources.gardener.cloud/secret-d5f5a834: generic-garden-kubeconfig-81fb3a88
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: gardener-extension-provider-local
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> env:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: GARDEN_KUBECONFIG
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: /var/run/secrets/gardener.cloud/garden/generic-kubeconfig/kubeconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: SEED_NAME
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: local
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumeMounts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - mountPath: /var/run/secrets/gardener.cloud/garden/generic-kubeconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: garden-kubeconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> readOnly: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: garden-kubeconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> projected:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> defaultMode: 420
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - secret:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> items:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: kubeconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: kubeconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: generic-garden-kubeconfig-81fb3a88
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> optional: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - secret:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> items:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: garden-access-extension
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> optional: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The generic garden kubeconfig will look like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusters:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- cluster:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> certificate-authority-data: LS0t...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: https://garden.local.gardener.cloud:6443
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: extension
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>current-context: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>users:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: extension
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tokenFile: /var/run/secrets/gardener.cloud/garden/generic-kubeconfig/token
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="manually-requesting-a-token-for-the-garden-cluster">Manually Requesting a Token for the Garden Cluster&lt;/h2>
&lt;p>Seed components that need to communicate with the garden cluster can request a token in the garden cluster by creating a garden access secret.
This secret has to be labelled with &lt;code>resources.gardener.cloud/purpose=token-requestor&lt;/code> and &lt;code>resources.gardener.cloud/class=garden&lt;/code>, e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: garden-access-example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources.gardener.cloud/purpose: token-requestor
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources.gardener.cloud/class: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> serviceaccount.resources.gardener.cloud/name: example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type: Opaque
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will instruct gardenlet to create a new &lt;code>ServiceAccount&lt;/code> named &lt;code>example&lt;/code> in its own &lt;code>seed-&amp;lt;seed-name&amp;gt;&lt;/code> namespace in the garden cluster, request a token for it, and populate the token in the secret&amp;rsquo;s data under the &lt;code>token&lt;/code> key.&lt;/p>
&lt;h2 id="permissions-in-the-garden-cluster">Permissions in the Garden Cluster&lt;/h2>
&lt;p>Both the &lt;a href="https://gardener.cloud/docs/gardener/deployment/gardenlet_api_access/">&lt;code>SeedAuthorizer&lt;/code> and the &lt;code>SeedRestriction&lt;/code> plugin&lt;/a> handle extensions clients and generally grant the same permissions in the garden cluster to them as to gardenlet clients.
With this, extensions are restricted to work with objects in the garden cluster that are related to seed they are running one just like gardenlet.
Note that if the plugins are not enabled, extension clients are only granted read access to global resources like &lt;code>CloudProfiles&lt;/code> (this is granted to all authenticated users).
There are a few exceptions to the granted permissions as documented &lt;a href="https://gardener.cloud/docs/gardener/deployment/gardenlet_api_access/#rule-exceptions-for-extension-clients">here&lt;/a>.&lt;/p>
&lt;h3 id="additional-permissions">Additional Permissions&lt;/h3>
&lt;p>If an extension needs access to additional resources in the garden cluster (e.g., extension-specific custom resources), permissions need to be granted via the usual RBAC means.
Let&amp;rsquo;s consider the following example: An extension requires the privileges to create &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/authorization-resources/subject-access-review-v1/">&lt;code>authorization.k8s.io/v1.SubjectAccessReview&lt;/code>&lt;/a>s (which is not covered by the &amp;ldquo;default&amp;rdquo; permissions mentioned above).
This requires a human Gardener operator to create a &lt;code>ClusterRole&lt;/code> in the garden cluster with the needed rules:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: rbac.authorization.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterRole
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: extension-create-subjectaccessreviews
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> authorization.gardener.cloud/extensions-serviceaccount-selector: &lt;span style="color:#a31515">&amp;#39;{&amp;#34;matchLabels&amp;#34;:{&amp;#34;controllerregistration.core.gardener.cloud/name&amp;#34;:&amp;#34;&amp;lt;extension-name&amp;gt;&amp;#34;}}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> authorization.gardener.cloud/custom-extensions-permissions: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- apiGroups:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - subjectaccessreviews
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> verbs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - create
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note the label &lt;code>authorization.gardener.cloud/extensions-serviceaccount-selector&lt;/code> which contains a label selector for &lt;code>ServiceAccount&lt;/code>s.&lt;/p>
&lt;p>There is a controller part of &lt;code>gardener-controller-manager&lt;/code> which takes care of maintaining the respective &lt;code>ClusterRoleBinding&lt;/code> resources.
It binds all &lt;code>ServiceAccount&lt;/code>s in the seed namespaces in the garden cluster (i.e., all extension clients) whose labels match.
You can read more about this controller &lt;a href="https://gardener.cloud/docs/gardener/concepts/controller-manager/#-extension-clusterrole--reconciler">here&lt;/a>.&lt;/p>
&lt;h3 id="custom-permissions">Custom Permissions&lt;/h3>
&lt;p>If an extension wants to create a dedicated &lt;code>ServiceAccount&lt;/code> for accessing the garden cluster &lt;strong>without&lt;/strong> automatically inheriting all permissions of the gardenlet, it first needs to create a garden access secret in its extension namespace in the seed cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-custom-component
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: &amp;lt;extension-namespace&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources.gardener.cloud/purpose: token-requestor
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources.gardener.cloud/class: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> serviceaccount.resources.gardener.cloud/name: my-custom-component-extension-foo
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> serviceaccount.resources.gardener.cloud/labels: &lt;span style="color:#a31515">&amp;#39;{&amp;#34;foo&amp;#34;:&amp;#34;bar}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type: Opaque
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>❗️&lt;strong>️Do not prefix the service account name with &lt;code>extension-&lt;/code> to prevent inheriting the gardenlet permissions!&lt;/strong> It is still recommended to add the extension name (e.g., as a suffix) for easier identification where this &lt;code>ServiceAccount&lt;/code> comes from.&lt;/p>
&lt;p>Next, you can follow the same approach &lt;a href="https://gardener.cloud/docs/gardener/extensions/garden-api-access/#additional-permissions">described above&lt;/a>.
However, the &lt;code>authorization.gardener.cloud/extensions-serviceaccount-selector&lt;/code> annotation should &lt;strong>not&lt;/strong> contain &lt;code>controllerregistration.core.gardener.cloud/name=&amp;lt;extension-name&amp;gt;&lt;/code> but rather custom labels, e.g. &lt;code>foo=bar&lt;/code>.&lt;/p>
&lt;p>This way, the created &lt;code>ServiceAccount&lt;/code> will only get the permissions of &lt;a href="https://gardener.cloud/docs/gardener/extensions/garden-api-access/#additional-permissions">above &lt;code>ClusterRole&lt;/code>&lt;/a> and nothing else.&lt;/p>
&lt;h2 id="renewing-all-garden-access-secrets">Renewing All Garden Access Secrets&lt;/h2>
&lt;p>Operators can trigger an automatic renewal of all garden access secrets in a given &lt;code>Seed&lt;/code> and their requested &lt;code>ServiceAccount&lt;/code> tokens, e.g., when rotating the garden cluster&amp;rsquo;s &lt;code>ServiceAccount&lt;/code> signing key.
For this, the &lt;code>Seed&lt;/code> has to be annotated with &lt;code>gardener.cloud/operation=renew-garden-access-secrets&lt;/code>.&lt;/p></description></item><item><title>Docs: Adding Support for a Cloud Provider</title><link>https://gardener.cloud/docs/other-components/machine-controller-manager/cp_support_new/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/machine-controller-manager/cp_support_new/</guid><description>
&lt;h1 id="adding-support-for-a-new-provider">Adding support for a new provider&lt;/h1>
&lt;p>Steps to be followed while implementing a new (hyperscale) provider are mentioned below. This is the easiest way to add new provider support using a blueprint code.&lt;/p>
&lt;p>However, you may also develop your machine controller from scratch, which would provide you with more flexibility. First, however, make sure that your custom machine controller adheres to the &lt;code>Machine.Status&lt;/code> struct defined in the &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/types.go">MachineAPIs&lt;/a>. This will make sure the MCM can act with higher-level controllers like MachineSet and MachineDeployment controller. The key is the &lt;code>Machine.Status.CurrentStatus.Phase&lt;/code> key that indicates the status of the machine object.&lt;/p>
&lt;p>Our strong recommendation would be to follow the steps below. This provides the most flexibility required to support machine management for adding new providers. And if you feel to extend the functionality, feel free to update our &lt;a href="https://github.com/gardener/machine-controller-manager/tree/master/pkg/util/provider">machine controller libraries&lt;/a>.&lt;/p>
&lt;h2 id="setting-up-your-repository">Setting up your repository&lt;/h2>
&lt;ol>
&lt;li>Create a new empty repository named &lt;code>machine-controller-manager-provider-{provider-name}&lt;/code> on GitHub username/project. Do not initialize this repository with a README.&lt;/li>
&lt;li>Copy the remote repository &lt;code>URL&lt;/code> (HTTPS/SSH) to this repository displayed once you create this repository.&lt;/li>
&lt;li>Now, on your local system, create directories as required. {your-github-username} given below could also be {github-project} depending on where you have created the new repository.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>mkdir -p $GOPATH/src/github.com/{your-github-username}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Navigate to this created directory.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cd $GOPATH/src/github.com/{your-github-username}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Clone &lt;a href="https://github.com/gardener/machine-controller-manager-provider-sampleprovider">this repository&lt;/a> on your local machine.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>git clone git@github.com:gardener/machine-controller-manager-provider-sampleprovider.git
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Rename the directory from &lt;code>machine-controller-manager-provider-sampleprovider&lt;/code> to &lt;code>machine-controller-manager-provider-{provider-name}&lt;/code>.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>mv machine-controller-manager-provider-sampleprovider machine-controller-manager-provider-{provider-name}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Navigate into the newly-created directory.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cd machine-controller-manager-provider-{provider-name}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Update the remote &lt;code>origin&lt;/code> URL to the newly created repository&amp;rsquo;s URL you had copied above.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>git remote set-url origin git@github.com:{your-github-username}/machine-controller-manager-provider-{provider-name}.git
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Rename GitHub project from &lt;code>gardener&lt;/code> to &lt;code>{github-org/your-github-username}&lt;/code> wherever you have cloned the repository above. Also, edit all occurrences of the word &lt;code>sampleprovider&lt;/code> to &lt;code>{provider-name}&lt;/code> in the code. Then, use the hack script given below to do the same.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make rename-project PROJECT_NAME={github-org/your-github-username} PROVIDER_NAME={provider-name}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>eg:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> make rename-project PROJECT_NAME=gardener PROVIDER_NAME=AmazonWebServices (or)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> make rename-project PROJECT_NAME=githubusername PROVIDER_NAME=AWS
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Now, commit your changes and push them upstream.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>git add -A
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>git commit -m &lt;span style="color:#a31515">&amp;#34;Renamed SampleProvide to {provider-name}&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>git push origin master
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;h2 id="code-changes-required">Code changes required&lt;/h2>
&lt;p>The contract between the Machine Controller Manager (MCM) and the Machine Controller (MC) AKA driver has been &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/machine_error_codes/">documented here&lt;/a> and the &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/machinecodes/codes/codes.go">machine error codes can be found here&lt;/a>. You may refer to them for any queries.&lt;/p>
&lt;p>⚠️&lt;/p>
&lt;ul>
&lt;li>Keep in mind that &lt;strong>there should be a unique way to map between machine objects and VMs&lt;/strong>. This can be done by mapping machine object names with VM-Name/ tags/ other metadata.&lt;/li>
&lt;li>Optionally, there should also be a unique way to map a VM to its machine class object. This can be done by tagging VM objects with tags/resource groups associated with the machine class.&lt;/li>
&lt;/ul>
&lt;h4 id="steps-to-integrate">Steps to integrate&lt;/h4>
&lt;ol>
&lt;li>Update the &lt;code>pkg/provider/apis/provider_spec.go&lt;/code> specification file to reflect the structure of the &lt;code>ProviderSpec&lt;/code> blob. It typically contains the machine template details in the &lt;code>MachineClass&lt;/code> object. Follow the sample spec provided already in the file. A sample provider specification can be found &lt;a href="https://github.com/gardener/machine-controller-manager-provider-aws/blob/master/pkg/aws/apis/aws_provider_spec.go">here&lt;/a>.&lt;/li>
&lt;li>Fill in the methods described at &lt;code>pkg/provider/core.go&lt;/code> to manage VMs on your cloud provider. Comments are provided above each method to help you fill them up with desired &lt;code>REQUEST&lt;/code> and &lt;code>RESPONSE&lt;/code> parameters.
&lt;ul>
&lt;li>A sample provider implementation for these methods can be found &lt;a href="https://github.com/gardener/machine-controller-manager-provider-aws/blob/master/pkg/aws/core.go">here&lt;/a>.&lt;/li>
&lt;li>Fill in the required methods &lt;code>CreateMachine()&lt;/code>, and &lt;code>DeleteMachine()&lt;/code> methods.&lt;/li>
&lt;li>Optionally fill in methods like &lt;code>GetMachineStatus()&lt;/code>, &lt;code>InitializeMachine&lt;/code>, &lt;code>ListMachines()&lt;/code>, and &lt;code>GetVolumeIDs()&lt;/code>. You may choose to fill these once the working of the required methods seems to be working.&lt;/li>
&lt;li>&lt;code>GetVolumeIDs()&lt;/code> expects VolumeIDs to be decoded from the volumeSpec based on the cloud provider.&lt;/li>
&lt;li>There is also an OPTIONAL method &lt;code>GenerateMachineClassForMigration()&lt;/code> that helps in migration of &lt;code>{ProviderSpecific}MachineClass&lt;/code> to &lt;code>MachineClass&lt;/code> CR (custom resource). This only makes sense if you have an existing implementation (in-tree) acting on different CRD types. You would like to migrate this. If not, you MUST return an error (machine error UNIMPLEMENTED) to avoid processing this step.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Perform validation of APIs that you have described and make it a part of your methods as required at each request.&lt;/li>
&lt;li>Write unit tests to make it work with your implementation by running &lt;code>make test&lt;/code>.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make test
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Tidy the go dependencies.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make tidy
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Update the sample YAML files on the &lt;code>kubernetes/&lt;/code> directory to provide sample files through which the working of the machine controller can be tested.&lt;/li>
&lt;li>Update &lt;code>README.md&lt;/code> to reflect any additional changes&lt;/li>
&lt;/ol>
&lt;h2 id="testing-your-code-changes">Testing your code changes&lt;/h2>
&lt;p>Make sure &lt;code>$TARGET_KUBECONFIG&lt;/code> points to the cluster where you wish to manage machines. Likewise, &lt;code>$CONTROL_NAMESPACE&lt;/code> represents the namespaces where MCM is looking for machine CR objects, and &lt;code>$CONTROL_KUBECONFIG&lt;/code> points to the cluster that holds these machine CRs.&lt;/p>
&lt;ol>
&lt;li>On the first terminal running at &lt;code>$GOPATH/src/github.com/{github-org/your-github-username}/machine-controller-manager-provider-{provider-name}&lt;/code>,
&lt;ul>
&lt;li>Run the machine controller (driver) using the command below.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make start
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>On the second terminal pointing to &lt;code>$GOPATH/src/github.com/gardener&lt;/code>,
&lt;ul>
&lt;li>Clone the &lt;a href="https://github.com/gardener/machine-controller-manager">latest MCM code&lt;/a>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>git clone git@github.com:gardener/machine-controller-manager.git
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Navigate to the newly-created directory.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>cd machine-controller-manager
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Deploy the required CRDs from the machine-controller-manager repo,
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f kubernetes/crds
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Run the machine-controller-manager in the &lt;code>master&lt;/code> branch
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make start
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>On the third terminal pointing to &lt;code>$GOPATH/src/github.com/{github-org/your-github-username}/machine-controller-manager-provider-{provider-name}&lt;/code>
&lt;ul>
&lt;li>Fill in the object files given below and deploy them as described below.&lt;/li>
&lt;li>Deploy the &lt;code>machine-class&lt;/code>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f kubernetes/machine-class.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Deploy the &lt;code>kubernetes secret&lt;/code> if required.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f kubernetes/secret.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Deploy the &lt;code>machine&lt;/code> object and make sure it joins the cluster successfully.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f kubernetes/machine.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Once the machine joins, you can test by deploying a machine-deployment.&lt;/li>
&lt;li>Deploy the &lt;code>machine-deployment&lt;/code> object and make sure it joins the cluster successfully.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f kubernetes/machine-deployment.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Make sure to delete both the &lt;code>machine&lt;/code> and &lt;code>machine-deployment&lt;/code> objects after use.
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl delete -f kubernetes/machine.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl delete -f kubernetes/machine-deployment.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;h2 id="releasing-your-docker-image">Releasing your docker image&lt;/h2>
&lt;ol>
&lt;li>Make sure you have logged into gcloud/docker using the CLI.&lt;/li>
&lt;li>To release your docker image, run the following.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span> make release IMAGE_REPOSITORY=&amp;lt;link-to-image-repo&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>A sample kubernetes deploy file can be found at &lt;code>kubernetes/deployment.yaml&lt;/code>. Update the same (with your desired MCM and MC images) to deploy your MCM pod.&lt;/li>
&lt;/ol></description></item><item><title>Docs: Admission</title><link>https://gardener.cloud/docs/gardener/extensions/admission/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/extensions/admission/</guid><description>
&lt;h1 id="extension-admission">Extension Admission&lt;/h1>
&lt;p>The extensions are expected to validate their respective resources for their extension specific configurations, when the resources are newly created or updated. For example, &lt;a href="https://github.com/gardener/gardener/blob/master/extensions/README.md#infrastructure-provider">provider extensions&lt;/a> would validate &lt;code>spec.provider.infrastructureConfig&lt;/code> and &lt;code>spec.provider.controlPlaneConfig&lt;/code> in the &lt;code>Shoot&lt;/code> resource and &lt;code>spec.providerConfig&lt;/code> in the &lt;code>CloudProfile&lt;/code> resource, &lt;a href="https://github.com/gardener/gardener/blob/master/extensions/README.md#network-plugin">networking extensions&lt;/a> would validate &lt;code>spec.networking.providerConfig&lt;/code> in the &lt;code>Shoot&lt;/code> resource. As best practice, the validation should be performed only if there is a change in the &lt;code>spec&lt;/code> of the resource. Please find an exemplary implementation in the &lt;a href="https://github.com/gardener/gardener-extension-provider-aws/tree/master/pkg/admission/validator">gardener/gardener-extension-provider-aws&lt;/a> repository.&lt;/p>
&lt;p>When a resource is newly created or updated, Gardener adds an extension label for all the extension types referenced in the &lt;code>spec&lt;/code> of the resource. This label is of the form &lt;code>&amp;lt;extension-type&amp;gt;.extensions.gardener.cloud/&amp;lt;extension-name&amp;gt; : &amp;quot;true&amp;quot;&lt;/code>. For example, an extension label for a provider extension type &lt;code>aws&lt;/code> looks like &lt;code>provider.extensions.gardener.cloud/aws : &amp;quot;true&amp;quot;&lt;/code>. The extensions should add object selectors in their admission webhooks for these labels, to filter out the objects they are responsible for. At present, these labels are added to &lt;code>BackupEntry&lt;/code>s, &lt;code>BackupBucket&lt;/code>s, &lt;code>CloudProfile&lt;/code>s, &lt;code>Seed&lt;/code>s, &lt;code>SecretBinding&lt;/code>s and &lt;code>Shoot&lt;/code>s. Please see the &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/apis/core/v1beta1/constants/types_constants.go">types_constants.go&lt;/a> file for the full list of extension labels.&lt;/p></description></item><item><title>Docs: Alerting</title><link>https://gardener.cloud/docs/gardener/monitoring/alerting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/monitoring/alerting/</guid><description>
&lt;h1 id="alerting">Alerting&lt;/h1>
&lt;p>Gardener uses &lt;a href="https://prometheus.io/">Prometheus&lt;/a> to gather metrics from each component. A Prometheus is deployed in each shoot control plane (on the seed) which is responsible for gathering control plane and cluster metrics. Prometheus can be configured to fire alerts based on these metrics and send them to an &lt;a href="https://prometheus.io/docs/alerting/alertmanager/">Alertmanager&lt;/a>. The Alertmanager is responsible for sending the alerts to users and operators. This document describes how to setup alerting for:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/monitoring/alerting/#alerting-for-users">end-users/stakeholders/customers&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/monitoring/alerting/#alerting-for-operators">operators/administrators&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="alerting-for-users">Alerting for Users&lt;/h1>
&lt;p>To receive email alerts as a user, set the following values in the shoot spec:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> monitoring:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> alerting:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> emailReceivers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - john.doe@example.com
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>emailReceivers&lt;/code> is a list of emails that will receive alerts if something is wrong with the shoot cluster.&lt;/p>
&lt;h1 id="alerting-for-operators">Alerting for Operators&lt;/h1>
&lt;p>Currently, Gardener supports two options for alerting:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/monitoring/alerting/#email-alerting">Email Alerting&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/monitoring/alerting/#external-alertmanager">Sending Alerts to an External Alertmanager&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="email-alerting">Email Alerting&lt;/h2>
&lt;p>Gardener provides the option to deploy an Alertmanager into each seed. This Alertmanager is responsible for sending out alerts to operators for each shoot cluster in the seed. Only email alerts are supported by the Alertmanager managed by Gardener. This is configurable by setting the Gardener controller manager configuration values &lt;code>alerting&lt;/code>. See &lt;a href="https://gardener.cloud/docs/gardener/configuration/">Gardener Configuration and Usage&lt;/a> on how to configure the Gardener&amp;rsquo;s SMTP secret. If the values are set, a secret with the label &lt;code>gardener.cloud/role: alerting&lt;/code> will be created in the garden namespace of the garden cluster. This secret will be used by each Alertmanager in each seed.&lt;/p>
&lt;h2 id="external-alertmanager">External Alertmanager&lt;/h2>
&lt;p>The Alertmanager supports different kinds of &lt;a href="https://prometheus.io/docs/alerting/configuration/">alerting configurations&lt;/a>. The Alertmanager provided by Gardener only supports email alerts. If email is not sufficient, then alerts can be sent to an external Alertmanager. Prometheus will send alerts to a URL and then alerts will be handled by the external Alertmanager. This external Alertmanager is operated and configured by the operator (i.e. Gardener does not configure or deploy this Alertmanager). To configure sending alerts to an external Alertmanager, create a secret in the virtual garden cluster in the garden namespace with the label: &lt;code>gardener.cloud/role: alerting&lt;/code>. This secret needs to contain a URL to the external Alertmanager and information regarding authentication. Supported authentication types are:&lt;/p>
&lt;ul>
&lt;li>No Authentication (none)&lt;/li>
&lt;li>Basic Authentication (basic)&lt;/li>
&lt;li>Mutual TLS (certificate)&lt;/li>
&lt;/ul>
&lt;h3 id="remote-alertmanager-examples">Remote Alertmanager Examples&lt;/h3>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> The &lt;code>url&lt;/code> value cannot be prepended with &lt;code>http&lt;/code> or &lt;code>https&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># No Authentication&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> gardener.cloud/role: alerting
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: alerting-auth
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># No Authentication&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> auth_type: base64(none)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> url: base64(external.alertmanager.foo)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Basic Auth&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> auth_type: base64(basic)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> url: base64(extenal.alertmanager.foo)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> username: base64(admin)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> password: base64(password)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Mutual TLS&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> auth_type: base64(certificate)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> url: base64(external.alertmanager.foo)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ca.crt: base64(ca)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tls.crt: base64(certificate)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tls.key: base64(key)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> insecure_skip_verify: base64(false)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Email Alerts (internal alertmanager)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> auth_type: base64(smtp)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> auth_identity: base64(internal.alertmanager.auth_identity)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> auth_password: base64(internal.alertmanager.auth_password)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> auth_username: base64(internal.alertmanager.auth_username)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> from: base64(internal.alertmanager.from)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> smarthost: base64(internal.alertmanager.smarthost)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> to: base64(internal.alertmanager.to)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type: Opaque
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="configuring-your-external-alertmanager">Configuring Your External Alertmanager&lt;/h3>
&lt;p>Please refer to the &lt;a href="https://prometheus.io/docs/alerting/alertmanager/">Alertmanager&lt;/a> documentation on how to configure an Alertmanager.&lt;/p>
&lt;p>We recommend you use at least the following inhibition rules in your Alertmanager configuration to prevent excessive alerts:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>inhibit_rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Apply inhibition if the alert name is the same.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- source_match:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> severity: critical
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> target_match:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> severity: warning
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> equal: [&lt;span style="color:#a31515">&amp;#39;alertname&amp;#39;&lt;/span>, &lt;span style="color:#a31515">&amp;#39;service&amp;#39;&lt;/span>, &lt;span style="color:#a31515">&amp;#39;cluster&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Stop all alerts for type=shoot if there are VPN problems.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- source_match:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service: vpn
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> target_match_re:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> equal: [&lt;span style="color:#a31515">&amp;#39;type&amp;#39;&lt;/span>, &lt;span style="color:#a31515">&amp;#39;cluster&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Stop warning and critical alerts if there is a blocker&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- source_match:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> severity: blocker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> target_match_re:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> severity: ^(critical|warning)$
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> equal: [&lt;span style="color:#a31515">&amp;#39;cluster&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># If the API server is down inhibit no worker nodes alert. No worker nodes depends on kube-state-metrics which depends on the API server.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- source_match:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service: kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> target_match_re:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service: nodes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> equal: [&lt;span style="color:#a31515">&amp;#39;cluster&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># If API server is down inhibit kube-state-metrics alerts.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- source_match:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service: kube-apiserver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> target_match_re:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> severity: info
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> equal: [&lt;span style="color:#a31515">&amp;#39;cluster&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># No Worker nodes depends on kube-state-metrics. Inhibit no worker nodes if kube-state-metrics is down.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- source_match:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service: kube-state-metrics-shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> target_match_re:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service: nodes
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> equal: [&lt;span style="color:#a31515">&amp;#39;cluster&amp;#39;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Below is a graph visualizing the inhibition rules:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/alertInhibitionGraph_ceaef0.png" alt="inhibitionGraph">&lt;/p></description></item><item><title>Docs: Apis</title><link>https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/</guid><description>
&lt;h2 id="specification">Specification&lt;/h2>
&lt;h3 id="providerspec-schema">ProviderSpec Schema&lt;/h3>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.Machine">
&lt;b>Machine&lt;/b>
&lt;/h3>
&lt;p>
&lt;p>Machine is the representation of a physical or virtual machine.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>apiVersion&lt;/code>
&lt;/td>
&lt;td>
string
&lt;/td>
&lt;td>
&lt;code>
machine.sapcloud.io/v1alpha1
&lt;/code>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>kind&lt;/code>
&lt;/td>
&lt;td>
string
&lt;/td>
&lt;td>
&lt;code>Machine&lt;/code>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>metadata&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>ObjectMeta for machine object&lt;/p>
Refer to the Kubernetes API documentation for the fields of the
&lt;code>metadata&lt;/code> field.
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>spec&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSpec">
MachineSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Spec contains the specification of the machine&lt;/p>
&lt;br/>
&lt;br/>
&lt;table>
&lt;tr>
&lt;td>
&lt;code>class&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.ClassSpec">
ClassSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Class contains the machineclass attributes of a machine&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>providerID&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ProviderID represents the provider’s unique ID given to a machine&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>nodeTemplate&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.NodeTemplateSpec">
NodeTemplateSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>NodeTemplateSpec describes the data a node should have when created from a template&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>MachineConfiguration&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineConfiguration">
MachineConfiguration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>
(Members of &lt;code>MachineConfiguration&lt;/code> are embedded into this type.)
&lt;/p>
&lt;em>(Optional)&lt;/em>
&lt;p>Configuration for the machine-controller.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineStatus">
MachineStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Status contains fields depicting the status&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineClass">
&lt;b>MachineClass&lt;/b>
&lt;/h3>
&lt;p>
&lt;p>MachineClass can be used to templatize and re-use provider configuration
across multiple Machines / MachineSets / MachineDeployments.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>apiVersion&lt;/code>
&lt;/td>
&lt;td>
string
&lt;/td>
&lt;td>
&lt;code>
machine.sapcloud.io/v1alpha1
&lt;/code>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>kind&lt;/code>
&lt;/td>
&lt;td>
string
&lt;/td>
&lt;td>
&lt;code>MachineClass&lt;/code>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>metadata&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
Refer to the Kubernetes API documentation for the fields of the
&lt;code>metadata&lt;/code> field.
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>nodeTemplate&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.NodeTemplate">
NodeTemplate
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>NodeTemplate contains subfields to track all node resources and other node info required to scale nodegroup from zero&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>credentialsSecretRef&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23secretreference-v1-core">
Kubernetes core/v1.SecretReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>CredentialsSecretRef can optionally store the credentials (in this case the SecretRef does not need to store them).
This might be useful if multiple machine classes with the same credentials but different user-datas are used.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>providerSpec&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://godoc.org/k8s.io/apimachinery/pkg/runtime%23RawExtension">
k8s.io/apimachinery/pkg/runtime.RawExtension
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Provider-specific configuration to use during node creation.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>provider&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Provider is the combination of name and location of cloud-specific drivers.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>secretRef&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23secretreference-v1-core">
Kubernetes core/v1.SecretReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>SecretRef stores the necessary secrets such as credentials or userdata.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineDeployment">
&lt;b>MachineDeployment&lt;/b>
&lt;/h3>
&lt;p>
&lt;p>MachineDeployment enables declarative updates for machines and MachineSets.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>apiVersion&lt;/code>
&lt;/td>
&lt;td>
string
&lt;/td>
&lt;td>
&lt;code>
machine.sapcloud.io/v1alpha1
&lt;/code>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>kind&lt;/code>
&lt;/td>
&lt;td>
string
&lt;/td>
&lt;td>
&lt;code>MachineDeployment&lt;/code>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>metadata&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Standard object metadata.&lt;/p>
Refer to the Kubernetes API documentation for the fields of the
&lt;code>metadata&lt;/code> field.
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>spec&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentSpec">
MachineDeploymentSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Specification of the desired behavior of the MachineDeployment.&lt;/p>
&lt;br/>
&lt;br/>
&lt;table>
&lt;tr>
&lt;td>
&lt;code>replicas&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Number of desired machines. This is a pointer to distinguish between explicit
zero and not specified. Defaults to 0.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>selector&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Label selector for machines. Existing MachineSets whose machines are
selected by this will be the ones affected by this MachineDeployment.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>template&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineTemplateSpec">
MachineTemplateSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Template describes the machines that will be created.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>strategy&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentStrategy">
MachineDeploymentStrategy
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The MachineDeployment strategy to use to replace existing machines with new ones.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>minReadySeconds&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Minimum number of seconds for which a newly created machine should be ready
without any of its container crashing, for it to be considered available.
Defaults to 0 (machine will be considered available as soon as it is ready)&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>revisionHistoryLimit&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
*int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The number of old MachineSets to retain to allow rollback.
This is a pointer to distinguish between explicit zero and not specified.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>paused&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
bool
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Indicates that the MachineDeployment is paused and will not be processed by the
MachineDeployment controller.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>rollbackTo&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.RollbackConfig">
RollbackConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>DEPRECATED.
The config this MachineDeployment is rolling back to. Will be cleared after rollback is done.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>progressDeadlineSeconds&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
*int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The maximum time in seconds for a MachineDeployment to make progress before it
is considered to be failed. The MachineDeployment controller will continue to
process failed MachineDeployments and a condition with a ProgressDeadlineExceeded
reason will be surfaced in the MachineDeployment status. Note that progress will
not be estimated during the time a MachineDeployment is paused. This is not set
by default, which is treated as infinite deadline.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentStatus">
MachineDeploymentStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Most recently observed status of the MachineDeployment.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineSet">
&lt;b>MachineSet&lt;/b>
&lt;/h3>
&lt;p>
&lt;p>MachineSet TODO&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>apiVersion&lt;/code>
&lt;/td>
&lt;td>
string
&lt;/td>
&lt;td>
&lt;code>
machine.sapcloud.io/v1alpha1
&lt;/code>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>kind&lt;/code>
&lt;/td>
&lt;td>
string
&lt;/td>
&lt;td>
&lt;code>MachineSet&lt;/code>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>metadata&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
Refer to the Kubernetes API documentation for the fields of the
&lt;code>metadata&lt;/code> field.
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>spec&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSetSpec">
MachineSetSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;br/>
&lt;br/>
&lt;table>
&lt;tr>
&lt;td>
&lt;code>replicas&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>selector&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>machineClass&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.ClassSpec">
ClassSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>template&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineTemplateSpec">
MachineTemplateSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>minReadySeconds&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSetStatus">
MachineSetStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.ClassSpec">
&lt;b>ClassSpec&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSetSpec">MachineSetSpec&lt;/a>,
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSpec">MachineSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>ClassSpec is the class specification of machine&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>apiGroup&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>API group to which it belongs&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>kind&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Kind for machine class&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>name&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Name of machine class&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.ConditionStatus">
&lt;b>ConditionStatus&lt;/b>
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentCondition">MachineDeploymentCondition&lt;/a>,
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSetCondition">MachineSetCondition&lt;/a>)
&lt;/p>
&lt;p>
&lt;/p>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.CurrentStatus">
&lt;b>CurrentStatus&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineStatus">MachineStatus&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>CurrentStatus contains information about the current status of Machine.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>phase&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachinePhase">
MachinePhase
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>timeoutActive&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
bool
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastUpdateTime&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23time-v1-meta">
Kubernetes meta/v1.Time
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Last update time of current status&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.LastOperation">
&lt;b>LastOperation&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSetStatus">MachineSetStatus&lt;/a>,
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineStatus">MachineStatus&lt;/a>,
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSummary">MachineSummary&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>LastOperation suggests the last operation performed on the object&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>description&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Description of the current operation&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>errorCode&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ErrorCode of the current operation if any&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastUpdateTime&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23time-v1-meta">
Kubernetes meta/v1.Time
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Last update time of current operation&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>state&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineState">
MachineState
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>State of operation&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>type&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineOperationType">
MachineOperationType
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Type of operation&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineConfiguration">
&lt;b>MachineConfiguration&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSpec">MachineSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachineConfiguration describes the configurations useful for the machine-controller.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>drainTimeout&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1%23Duration">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>MachineDraintimeout is the timeout after which machine is forcefully deleted.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>healthTimeout&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1%23Duration">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>MachineHealthTimeout is the timeout after which machine is declared unhealhty/failed.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>creationTimeout&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1%23Duration">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>MachineCreationTimeout is the timeout after which machinie creation is declared failed.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>maxEvictRetries&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
*int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>MaxEvictRetries is the number of retries that will be attempted while draining the node.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>nodeConditions&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
*string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>NodeConditions are the set of conditions if set to true for MachineHealthTimeOut, machine will be declared failed.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineDeploymentCondition">
&lt;b>MachineDeploymentCondition&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentStatus">MachineDeploymentStatus&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachineDeploymentCondition describes the state of a MachineDeployment at a certain point.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>type&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentConditionType">
MachineDeploymentConditionType
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Type of MachineDeployment condition.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.ConditionStatus">
ConditionStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Status of the condition, one of True, False, Unknown.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastUpdateTime&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23time-v1-meta">
Kubernetes meta/v1.Time
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>The last time this condition was updated.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastTransitionTime&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23time-v1-meta">
Kubernetes meta/v1.Time
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Last time the condition transitioned from one status to another.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>reason&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>The reason for the condition’s last transition.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>message&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>A human readable message indicating details about the transition.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineDeploymentConditionType">
&lt;b>MachineDeploymentConditionType&lt;/b>
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentCondition">MachineDeploymentCondition&lt;/a>)
&lt;/p>
&lt;p>
&lt;/p>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineDeploymentSpec">
&lt;b>MachineDeploymentSpec&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeployment">MachineDeployment&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachineDeploymentSpec is the specification of the desired behavior of the MachineDeployment.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>replicas&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Number of desired machines. This is a pointer to distinguish between explicit
zero and not specified. Defaults to 0.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>selector&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Label selector for machines. Existing MachineSets whose machines are
selected by this will be the ones affected by this MachineDeployment.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>template&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineTemplateSpec">
MachineTemplateSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Template describes the machines that will be created.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>strategy&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentStrategy">
MachineDeploymentStrategy
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The MachineDeployment strategy to use to replace existing machines with new ones.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>minReadySeconds&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Minimum number of seconds for which a newly created machine should be ready
without any of its container crashing, for it to be considered available.
Defaults to 0 (machine will be considered available as soon as it is ready)&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>revisionHistoryLimit&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
*int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The number of old MachineSets to retain to allow rollback.
This is a pointer to distinguish between explicit zero and not specified.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>paused&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
bool
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Indicates that the MachineDeployment is paused and will not be processed by the
MachineDeployment controller.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>rollbackTo&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.RollbackConfig">
RollbackConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>DEPRECATED.
The config this MachineDeployment is rolling back to. Will be cleared after rollback is done.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>progressDeadlineSeconds&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
*int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The maximum time in seconds for a MachineDeployment to make progress before it
is considered to be failed. The MachineDeployment controller will continue to
process failed MachineDeployments and a condition with a ProgressDeadlineExceeded
reason will be surfaced in the MachineDeployment status. Note that progress will
not be estimated during the time a MachineDeployment is paused. This is not set
by default, which is treated as infinite deadline.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineDeploymentStatus">
&lt;b>MachineDeploymentStatus&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeployment">MachineDeployment&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachineDeploymentStatus is the most recently observed status of the MachineDeployment.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>observedGeneration&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int64
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The generation observed by the MachineDeployment controller.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>replicas&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Total number of non-terminated machines targeted by this MachineDeployment (their labels match the selector).&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>updatedReplicas&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Total number of non-terminated machines targeted by this MachineDeployment that have the desired template spec.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>readyReplicas&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Total number of ready machines targeted by this MachineDeployment.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>availableReplicas&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Total number of available machines (ready for at least minReadySeconds) targeted by this MachineDeployment.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>unavailableReplicas&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Total number of unavailable machines targeted by this MachineDeployment. This is the total number of
machines that are still required for the MachineDeployment to have 100% available capacity. They may
either be machines that are running but not yet available or machines that still have not been created.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>conditions&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentCondition">
[]MachineDeploymentCondition
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Represents the latest available observations of a MachineDeployment’s current state.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>collisionCount&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
*int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Count of hash collisions for the MachineDeployment. The MachineDeployment controller uses this
field as a collision avoidance mechanism when it needs to create the name for the
newest MachineSet.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>failedMachines&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineSummary">
[]*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineSummary
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>FailedMachines has summary of machines on which lastOperation Failed&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineDeploymentStrategy">
&lt;b>MachineDeploymentStrategy&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentSpec">MachineDeploymentSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachineDeploymentStrategy describes how to replace existing machines with new ones.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>type&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentStrategyType">
MachineDeploymentStrategyType
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Type of MachineDeployment. Can be “Recreate” or “RollingUpdate”. Default is RollingUpdate.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>rollingUpdate&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.RollingUpdateMachineDeployment">
RollingUpdateMachineDeployment
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Rolling update config params. Present only if MachineDeploymentStrategyType =&lt;/p>
&lt;h2>RollingUpdate.&lt;/h2>
&lt;p>TODO: Update this to follow our convention for oneOf, whatever we decide it
to be.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineDeploymentStrategyType">
&lt;b>MachineDeploymentStrategyType&lt;/b>
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentStrategy">MachineDeploymentStrategy&lt;/a>)
&lt;/p>
&lt;p>
&lt;/p>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineOperationType">
&lt;b>MachineOperationType&lt;/b>
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.LastOperation">LastOperation&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachineOperationType is a label for the operation performed on a machine object.&lt;/p>
&lt;/p>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachinePhase">
&lt;b>MachinePhase&lt;/b>
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.CurrentStatus">CurrentStatus&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachinePhase is a label for the condition of a machine at the current time.&lt;/p>
&lt;/p>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineSetCondition">
&lt;b>MachineSetCondition&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSetStatus">MachineSetStatus&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachineSetCondition describes the state of a machine set at a certain point.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>type&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSetConditionType">
MachineSetConditionType
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Type of machine set condition.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.ConditionStatus">
ConditionStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Status of the condition, one of True, False, Unknown.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastTransitionTime&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23time-v1-meta">
Kubernetes meta/v1.Time
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The last time the condition transitioned from one status to another.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>reason&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The reason for the condition’s last transition.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>message&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>A human readable message indicating details about the transition.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineSetConditionType">
&lt;b>MachineSetConditionType&lt;/b>
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSetCondition">MachineSetCondition&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachineSetConditionType is the condition on machineset object&lt;/p>
&lt;/p>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineSetSpec">
&lt;b>MachineSetSpec&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSet">MachineSet&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachineSetSpec is the specification of a MachineSet.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>replicas&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>selector&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>machineClass&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.ClassSpec">
ClassSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>template&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineTemplateSpec">
MachineTemplateSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>minReadySeconds&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineSetStatus">
&lt;b>MachineSetStatus&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSet">MachineSet&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachineSetStatus holds the most recently observed status of MachineSet.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>replicas&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Replicas is the number of actual replicas.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>fullyLabeledReplicas&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The number of pods that have labels matching the labels of the pod template of the replicaset.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>readyReplicas&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The number of ready replicas for this replica set.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>availableReplicas&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The number of available replicas (ready for at least minReadySeconds) for this replica set.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>observedGeneration&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int64
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ObservedGeneration is the most recent generation observed by the controller.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>machineSetCondition&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSetCondition">
[]MachineSetCondition
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Represents the latest available observations of a replica set’s current state.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastOperation&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.LastOperation">
LastOperation
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>LastOperation performed&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>failedMachines&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.%5B%5Dgithub.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineSummary">
[]github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineSummary
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>FailedMachines has summary of machines on which lastOperation Failed&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineSpec">
&lt;b>MachineSpec&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.Machine">Machine&lt;/a>,
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineTemplateSpec">MachineTemplateSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachineSpec is the specification of a Machine.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>class&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.ClassSpec">
ClassSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Class contains the machineclass attributes of a machine&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>providerID&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ProviderID represents the provider’s unique ID given to a machine&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>nodeTemplate&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.NodeTemplateSpec">
NodeTemplateSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>NodeTemplateSpec describes the data a node should have when created from a template&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>MachineConfiguration&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineConfiguration">
MachineConfiguration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>
(Members of &lt;code>MachineConfiguration&lt;/code> are embedded into this type.)
&lt;/p>
&lt;em>(Optional)&lt;/em>
&lt;p>Configuration for the machine-controller.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineState">
&lt;b>MachineState&lt;/b>
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.LastOperation">LastOperation&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachineState is a current state of the operation.&lt;/p>
&lt;/p>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineStatus">
&lt;b>MachineStatus&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.Machine">Machine&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachineStatus holds the most recently observed status of Machine.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>conditions&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23nodecondition-v1-core">
[]Kubernetes core/v1.NodeCondition
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Conditions of this machine, same as node&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastOperation&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.LastOperation">
LastOperation
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Last operation refers to the status of the last operation performed&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>currentStatus&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.CurrentStatus">
CurrentStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Current status of the machine object&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastKnownState&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>LastKnownState can store details of the last known state of the VM by the plugins.
It can be used by future operation calls to determine current infrastucture state&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineSummary">
&lt;b>MachineSummary&lt;/b>
&lt;/h3>
&lt;p>
&lt;p>MachineSummary store the summary of machine.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>name&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Name of the machine object&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>providerID&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>ProviderID represents the provider’s unique ID given to a machine&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastOperation&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.LastOperation">
LastOperation
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Last operation refers to the status of the last operation performed&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>ownerRef&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>OwnerRef&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.MachineTemplateSpec">
&lt;b>MachineTemplateSpec&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentSpec">MachineDeploymentSpec&lt;/a>,
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSetSpec">MachineSetSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MachineTemplateSpec describes the data a machine should have when created from a template&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>metadata&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Standard object’s metadata.
More info: &lt;a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata">https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata&lt;/a>&lt;/p>
Refer to the Kubernetes API documentation for the fields of the
&lt;code>metadata&lt;/code> field.
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>spec&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSpec">
MachineSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Specification of the desired behavior of the machine.
More info: &lt;a href="https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status">https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status&lt;/a>&lt;/p>
&lt;br/>
&lt;br/>
&lt;table>
&lt;tr>
&lt;td>
&lt;code>class&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.ClassSpec">
ClassSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Class contains the machineclass attributes of a machine&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>providerID&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ProviderID represents the provider’s unique ID given to a machine&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>nodeTemplate&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.NodeTemplateSpec">
NodeTemplateSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>NodeTemplateSpec describes the data a node should have when created from a template&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>MachineConfiguration&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineConfiguration">
MachineConfiguration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>
(Members of &lt;code>MachineConfiguration&lt;/code> are embedded into this type.)
&lt;/p>
&lt;em>(Optional)&lt;/em>
&lt;p>Configuration for the machine-controller.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.NodeTemplate">
&lt;b>NodeTemplate&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineClass">MachineClass&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>NodeTemplate contains subfields to track all node resources and other node info required to scale nodegroup from zero&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>capacity&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23resourcelist-v1-core">
Kubernetes core/v1.ResourceList
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Capacity contains subfields to track all node resources required to scale nodegroup from zero&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>instanceType&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Instance type of the node belonging to nodeGroup&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>region&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Region of the expected node belonging to nodeGroup&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>zone&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Zone of the expected node belonging to nodeGroup&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>architecture&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
*string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>CPU Architecture of the node belonging to nodeGroup&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.NodeTemplateSpec">
&lt;b>NodeTemplateSpec&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineSpec">MachineSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>NodeTemplateSpec describes the data a node should have when created from a template&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>metadata&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
Refer to the Kubernetes API documentation for the fields of the
&lt;code>metadata&lt;/code> field.
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>spec&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23nodespec-v1-core">
Kubernetes core/v1.NodeSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>NodeSpec describes the attributes that a node is created with.&lt;/p>
&lt;br/>
&lt;br/>
&lt;table>
&lt;tr>
&lt;td>
&lt;code>podCIDR&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>PodCIDR represents the pod IP range assigned to the node.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>podCIDRs&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
[]string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>podCIDRs represents the IP ranges assigned to the node for usage by Pods on that node. If this
field is specified, the 0th entry must match the podCIDR field. It may contain at most 1 value for
each of IPv4 and IPv6.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>providerID&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ID of the node assigned by the cloud provider in the format: &lt;providername>://&lt;providerspecificnodeid>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>unschedulable&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
bool
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Unschedulable controls node schedulability of new pods. By default, node is schedulable.
More info: &lt;a href="https://kubernetes.io/docs/concepts/nodes/node/#manual-node-administration">https://kubernetes.io/docs/concepts/nodes/node/#manual-node-administration&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>taints&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23taint-v1-core">
[]Kubernetes core/v1.Taint
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>If specified, the node’s taints.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>configSource&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/%23nodeconfigsource-v1-core">
Kubernetes core/v1.NodeConfigSource
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Deprecated: Previously used to specify the source of the node’s configuration for the DynamicKubeletConfig feature. This feature is removed.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>externalID&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Deprecated. Not all kubelets will set this field. Remove field after 1.13.
see: &lt;a href="https://issues.k8s.io/61966">https://issues.k8s.io/61966&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.RollbackConfig">
&lt;b>RollbackConfig&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentSpec">MachineDeploymentSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>revision&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
int64
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The revision to rollback to. If set to 0, rollback to the last revision.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;br>
&lt;h3 id="machine.sapcloud.io/v1alpha1.RollingUpdateMachineDeployment">
&lt;b>RollingUpdateMachineDeployment&lt;/b>
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#%23machine.sapcloud.io/v1alpha1.MachineDeploymentStrategy">MachineDeploymentStrategy&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>Spec to control the desired behavior of rolling update.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Type&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>maxUnavailable&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://godoc.org/k8s.io/apimachinery/pkg/util/intstr%23IntOrString">
k8s.io/apimachinery/pkg/util/intstr.IntOrString
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The maximum number of machines that can be unavailable during the update.
Value can be an absolute number (ex: 5) or a percentage of desired machines (ex: 10%).
Absolute number is calculated from percentage by rounding down.
This can not be 0 if MaxSurge is 0.
By default, a fixed value of 1 is used.
Example: when this is set to 30%, the old MC can be scaled down to 70% of desired machines
immediately when the rolling update starts. Once new machines are ready, old MC
can be scaled down further, followed by scaling up the new MC, ensuring
that the total number of machines available at all times during the update is at
least 70% of desired machines.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>maxSurge&lt;/code>
&lt;/td>
&lt;td>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/documents/apis/#https://godoc.org/k8s.io/apimachinery/pkg/util/intstr%23IntOrString">
k8s.io/apimachinery/pkg/util/intstr.IntOrString
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>The maximum number of machines that can be scheduled above the desired number of
machines.
Value can be an absolute number (ex: 5) or a percentage of desired machines (ex: 10%).
This can not be 0 if MaxUnavailable is 0.
Absolute number is calculated from percentage by rounding up.
By default, a value of 1 is used.
Example: when this is set to 30%, the new MC can be scaled up immediately when
the rolling update starts, such that the total number of old and new machines do not exceed
130% of desired machines. Once old machines have been killed,
new MC can be scaled up further, ensuring that total number of machines running
at any time during the update is atmost 130% of desired machines.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;hr/>
&lt;p>&lt;em>
Generated with &lt;a href="https://github.com/ahmetb/gen-crd-api-reference-docs">gen-crd-api-reference-docs&lt;/a>
&lt;/em>&lt;/p></description></item><item><title>Docs: APIServer Admission Plugins</title><link>https://gardener.cloud/docs/gardener/concepts/apiserver-admission-plugins/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/apiserver-admission-plugins/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Similar to the kube-apiserver, the gardener-apiserver comes with a few in-tree managed admission plugins.
If you want to get an overview of the what and why of admission plugins then &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">this document&lt;/a> might be a good start.&lt;/p>
&lt;p>This document lists all existing admission plugins with a short explanation of what it is responsible for.&lt;/p>
&lt;h2 id="clusteropenidconnectpreset-openidconnectpreset">&lt;code>ClusterOpenIDConnectPreset&lt;/code>, &lt;code>OpenIDConnectPreset&lt;/code>&lt;/h2>
&lt;p>&lt;em>(both enabled by default)&lt;/em>&lt;/p>
&lt;p>These admission controllers react on &lt;code>CREATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
If the &lt;code>Shoot&lt;/code> does not specify any OIDC configuration (&lt;code>.spec.kubernetes.kubeAPIServer.oidcConfig=nil&lt;/code>), then it tries to find a matching &lt;code>ClusterOpenIDConnectPreset&lt;/code> or &lt;code>OpenIDConnectPreset&lt;/code>, respectively.
If there are multiple matches, then the one with the highest weight &amp;ldquo;wins&amp;rdquo;.
In this case, the admission controller will default the OIDC configuration in the &lt;code>Shoot&lt;/code>.&lt;/p>
&lt;h2 id="controllerregistrationresources">&lt;code>ControllerRegistrationResources&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>ControllerRegistration&lt;/code>s.
It validates that there exists only one &lt;code>ControllerRegistration&lt;/code> in the system that is primarily responsible for a given kind/type resource combination.
This prevents misconfiguration by the Gardener administrator/operator.&lt;/p>
&lt;h2 id="customverbauthorizer">&lt;code>CustomVerbAuthorizer&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Project&lt;/code>s.
It validates whether the user is bound to a RBAC role with the &lt;code>modify-spec-tolerations-whitelist&lt;/code> verb in case the user tries to change the &lt;code>.spec.tolerations.whitelist&lt;/code> field of the respective &lt;code>Project&lt;/code> resource.
Usually, regular project members are not bound to this custom verb, allowing the Gardener administrator to manage certain toleration whitelists on &lt;code>Project&lt;/code> basis.&lt;/p>
&lt;h2 id="deletionconfirmation">&lt;code>DeletionConfirmation&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>DELETE&lt;/code> operations for &lt;code>Project&lt;/code>s, &lt;code>Shoot&lt;/code>s, and &lt;code>ShootState&lt;/code>s.
It validates that the respective resource is annotated with a deletion confirmation annotation, namely &lt;code>confirmation.gardener.cloud/deletion=true&lt;/code>.
Only if this annotation is present it allows the &lt;code>DELETE&lt;/code> operation to pass.
This prevents users from accidental/undesired deletions.
In addition, it applies the &amp;ldquo;four-eyes principle for deletion&amp;rdquo; concept if the &lt;code>Project&lt;/code> is configured accordingly.
Find all information about it &lt;a href="https://gardener.cloud/docs/gardener/projects/#four-eyes-principle-for-resource-deletion">in this document&lt;/a>.&lt;/p>
&lt;p>Furthermore, this admission controller reacts on &lt;code>CREATE&lt;/code> or &lt;code>UPDATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It makes sure that the &lt;code>deletion.gardener.cloud/confirmed-by&lt;/code> annotation is properly maintained in case the &lt;code>Shoot&lt;/code> deletion is confirmed with above mentioned annotation.&lt;/p>
&lt;h2 id="exposureclass">&lt;code>ExposureClass&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>Create&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It mutates &lt;code>Shoot&lt;/code> resources which have an &lt;code>ExposureClass&lt;/code> referenced by merging both their &lt;code>shootSelectors&lt;/code> and/or &lt;code>tolerations&lt;/code> into the &lt;code>Shoot&lt;/code> resource.&lt;/p>
&lt;h2 id="extensionvalidator">&lt;code>ExtensionValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>BackupEntry&lt;/code>s, &lt;code>BackupBucket&lt;/code>s, &lt;code>Seed&lt;/code>s, and &lt;code>Shoot&lt;/code>s.
For all the various extension types in the specifications of these objects, it validates whether there exists a &lt;code>ControllerRegistration&lt;/code> in the system that is primarily responsible for the stated extension type(s).
This prevents misconfigurations that would otherwise allow users to create such resources with extension types that don&amp;rsquo;t exist in the cluster, effectively leading to failing reconciliation loops.&lt;/p>
&lt;h2 id="extensionlabels">&lt;code>ExtensionLabels&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>BackupBucket&lt;/code>s, &lt;code>BackupEntry&lt;/code>s, &lt;code>CloudProfile&lt;/code>s, &lt;code>Seed&lt;/code>s, &lt;code>SecretBinding&lt;/code>s and &lt;code>Shoot&lt;/code>s. For all the various extension types in the specifications of these objects, it adds a corresponding label in the resource. This would allow extension admission webhooks to filter out the resources they are responsible for and ignore all others. This label is of the form &lt;code>&amp;lt;extension-type&amp;gt;.extensions.gardener.cloud/&amp;lt;extension-name&amp;gt; : &amp;quot;true&amp;quot;&lt;/code>. For example, an extension label for provider extension type &lt;code>aws&lt;/code>, looks like &lt;code>provider.extensions.gardener.cloud/aws : &amp;quot;true&amp;quot;&lt;/code>.&lt;/p>
&lt;h2 id="projectvalidator">&lt;code>ProjectValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> operations for &lt;code>Project&lt;/code>s.
It prevents creating &lt;code>Project&lt;/code>s with a non-empty &lt;code>.spec.namespace&lt;/code> if the value in &lt;code>.spec.namespace&lt;/code> does not start with &lt;code>garden-&lt;/code>.&lt;/p>
&lt;p>⚠️ This admission plugin will be removed in a future release and its business logic will be incorporated into the static validation of the &lt;code>gardener-apiserver&lt;/code>.&lt;/p>
&lt;h2 id="resourcequota">&lt;code>ResourceQuota&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller enables &lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/#object-count-quota">object count ResourceQuotas&lt;/a> for Gardener resources, e.g. &lt;code>Shoots&lt;/code>, &lt;code>SecretBindings&lt;/code>, &lt;code>Projects&lt;/code>, etc.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ In addition to this admission plugin, the &lt;a href="https://github.com/kubernetes/kubernetes/blob/release-1.2/docs/design/admission_control_resource_quota.md#resource-quota-controller">ResourceQuota controller&lt;/a> must be enabled for the Kube-Controller-Manager of your Garden cluster.&lt;/p>
&lt;/blockquote>
&lt;h2 id="resourcereferencemanager">&lt;code>ResourceReferenceManager&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>CloudProfile&lt;/code>s, &lt;code>Project&lt;/code>s, &lt;code>SecretBinding&lt;/code>s, &lt;code>Seed&lt;/code>s, and &lt;code>Shoot&lt;/code>s.
Generally, it checks whether referred resources stated in the specifications of these objects exist in the system (e.g., if a referenced &lt;code>Secret&lt;/code> exists).
However, it also has some special behaviours for certain resources:&lt;/p>
&lt;ul>
&lt;li>&lt;code>CloudProfile&lt;/code>s: It rejects removing Kubernetes or machine image versions if there is at least one &lt;code>Shoot&lt;/code> that refers to them.&lt;/li>
&lt;li>&lt;code>Project&lt;/code>s: It sets the &lt;code>.spec.createdBy&lt;/code> field for newly created &lt;code>Project&lt;/code> resources, and defaults the &lt;code>.spec.owner&lt;/code> field in case it is empty (to the same value of &lt;code>.spec.createdBy&lt;/code>).&lt;/li>
&lt;li>&lt;code>Shoot&lt;/code>s: It sets the &lt;code>gardener.cloud/created-by=&amp;lt;username&amp;gt;&lt;/code> annotation for newly created &lt;code>Shoot&lt;/code> resources.&lt;/li>
&lt;/ul>
&lt;h2 id="seedvalidator">&lt;code>SeedValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>DELETE&lt;/code> operations for &lt;code>Seed&lt;/code>s.
Rejects the deletion if &lt;code>Shoot&lt;/code>(s) reference the seed cluster.&lt;/p>
&lt;h2 id="shootdns">&lt;code>ShootDNS&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It tries to assign a default domain to the &lt;code>Shoot&lt;/code>.
It also validates the DNS configuration (&lt;code>.spec.dns&lt;/code>) for shoots.&lt;/p>
&lt;h2 id="shootnodelocaldnsenabledbydefault">&lt;code>ShootNodeLocalDNSEnabledByDefault&lt;/code>&lt;/h2>
&lt;p>&lt;em>(disabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
If enabled, it will enable node local dns within the shoot cluster (for more information, see &lt;a href="https://gardener.cloud/docs/gardener/node-local-dns/">NodeLocalDNS Configuration&lt;/a>) by setting &lt;code>spec.systemComponents.nodeLocalDNS.enabled=true&lt;/code> for newly created Shoots.
Already existing Shoots and new Shoots that explicitly disable node local dns (&lt;code>spec.systemComponents.nodeLocalDNS.enabled=false&lt;/code>)
will not be affected by this admission plugin.&lt;/p>
&lt;h2 id="shootquotavalidator">&lt;code>ShootQuotaValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It validates the resource consumption declared in the specification against applicable &lt;code>Quota&lt;/code> resources.
Only if the applicable &lt;code>Quota&lt;/code> resources admit the configured resources in the &lt;code>Shoot&lt;/code> then it allows the request.
Applicable &lt;code>Quota&lt;/code>s are referred in the &lt;code>SecretBinding&lt;/code> that is used by the &lt;code>Shoot&lt;/code>.&lt;/p>
&lt;h2 id="shootresourcereservation">&lt;code>ShootResourceReservation&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It injects the &lt;code>Kubernetes.Kubelet.KubeReserved&lt;/code> setting for kubelet either as global setting for a shoot or on a per worker pool basis.
If the admission configuration (see &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-admissionconfig.yaml">this example&lt;/a>) for the &lt;code>ShootResourceReservation&lt;/code> plugin contains &lt;code>useGKEFormula: false&lt;/code> (the default), then it sets a static default resource reservation for the shoot.&lt;/p>
&lt;p>If &lt;code>useGKEFormula: true&lt;/code> is set, then the plugin injects resource reservations based on the machine type similar to GKE&amp;rsquo;s &lt;a href="https://cloud.google.com/kubernetes-engine/docs/concepts/plan-node-sizes#resource_reservations">formula for resource reservation&lt;/a> into each worker pool.
Already existing resource reservations are not modified; this also means that resource reservations are not automatically updated if the machine type for a worker pool is changed.
If a shoot contains global resource reservations, then no per worker pool resource reservations are injected.&lt;/p>
&lt;h2 id="shootvpaenabledbydefault">&lt;code>ShootVPAEnabledByDefault&lt;/code>&lt;/h2>
&lt;p>&lt;em>(disabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
If enabled, it will enable the managed &lt;code>VerticalPodAutoscaler&lt;/code> components (for more information, see &lt;a href="https://gardener.cloud/docs/gardener/shoot_autoscaling/#vertical-pod-auto-scaling">Vertical Pod Auto-Scaling&lt;/a>)
by setting &lt;code>spec.kubernetes.verticalPodAutoscaler.enabled=true&lt;/code> for newly created Shoots.
Already existing Shoots and new Shoots that explicitly disable VPA (&lt;code>spec.kubernetes.verticalPodAutoscaler.enabled=false&lt;/code>)
will not be affected by this admission plugin.&lt;/p>
&lt;h2 id="shoottolerationrestriction">&lt;code>ShootTolerationRestriction&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It validates the &lt;code>.spec.tolerations&lt;/code> used in &lt;code>Shoot&lt;/code>s against the whitelist of its &lt;code>Project&lt;/code>, or against the whitelist configured in the admission controller&amp;rsquo;s configuration, respectively.
Additionally, it defaults the &lt;code>.spec.tolerations&lt;/code> in &lt;code>Shoot&lt;/code>s with those configured in its &lt;code>Project&lt;/code>, and those configured in the admission controller&amp;rsquo;s configuration, respectively.&lt;/p>
&lt;h2 id="shootvalidator">&lt;code>ShootValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code>, &lt;code>UPDATE&lt;/code> and &lt;code>DELETE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It validates certain configurations in the specification against the referred &lt;code>CloudProfile&lt;/code> (e.g., machine images, machine types, used Kubernetes version, &amp;hellip;).
Generally, it performs validations that cannot be handled by the static API validation due to their dynamic nature (e.g., when something needs to be checked against referred resources).
Additionally, it takes over certain defaulting tasks (e.g., default machine image for worker pools, default Kubernetes version).&lt;/p>
&lt;h2 id="shootmanagedseed">&lt;code>ShootManagedSeed&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>UPDATE&lt;/code> and &lt;code>DELETE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
It validates certain configuration values in the specification that are specific to &lt;code>ManagedSeed&lt;/code>s (e.g. the nginx-addon of the Shoot has to be disabled, the Shoot VPA has to be enabled).
It rejects the deletion if the &lt;code>Shoot&lt;/code> is referred to by a &lt;code>ManagedSeed&lt;/code>.&lt;/p>
&lt;h2 id="managedseedvalidator">&lt;code>ManagedSeedValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>ManagedSeeds&lt;/code>s.
It validates certain configuration values in the specification against the referred &lt;code>Shoot&lt;/code>, for example Seed provider, network ranges, DNS domain, etc.
Similar to &lt;code>ShootValidator&lt;/code>, it performs validations that cannot be handled by the static API validation due to their dynamic nature.
Additionally, it performs certain defaulting tasks, making sure that configuration values that are not specified are defaulted to the values of the referred &lt;code>Shoot&lt;/code>, for example Seed provider, network ranges, DNS domain, etc.&lt;/p>
&lt;h2 id="managedseedshoot">&lt;code>ManagedSeedShoot&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>DELETE&lt;/code> operations for &lt;code>ManagedSeed&lt;/code>s.
It rejects the deletion if there are &lt;code>Shoot&lt;/code>s that are scheduled onto the &lt;code>Seed&lt;/code> that is registered by the &lt;code>ManagedSeed&lt;/code>.&lt;/p>
&lt;h2 id="shootdnsrewriting">&lt;code>ShootDNSRewriting&lt;/code>&lt;/h2>
&lt;p>&lt;em>(disabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> operations for &lt;code>Shoot&lt;/code>s.
If enabled, it adds a set of common suffixes configured in its admission plugin configuration to the &lt;code>Shoot&lt;/code> (&lt;code>spec.systemComponents.coreDNS.rewriting.commonSuffixes&lt;/code>) (for more information, see &lt;a href="https://gardener.cloud/docs/gardener/dns-search-path-optimization/">DNS Search Path Optimization&lt;/a>).
Already existing &lt;code>Shoot&lt;/code>s will not be affected by this admission plugin.&lt;/p>
&lt;h2 id="namespacedcloudprofilevalidator">&lt;code>NamespacedCloudProfileValidator&lt;/code>&lt;/h2>
&lt;p>&lt;em>(enabled by default)&lt;/em>&lt;/p>
&lt;p>This admission controller reacts on &lt;code>CREATE&lt;/code> and &lt;code>UPDATE&lt;/code> operations for &lt;code>NamespacedCloudProfile&lt;/code>s.
It primarily validates if the referenced parent &lt;code>CloudProfile&lt;/code> exists in the system. In addition, the admission controller ensures that the &lt;code>NamespacedCloudProfile&lt;/code> only configures new machine types, and does not overwrite those from the parent &lt;code>CloudProfile&lt;/code>.&lt;/p></description></item><item><title>Docs: Architecture</title><link>https://gardener.cloud/docs/gardener/concepts/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/architecture/</guid><description>
&lt;h2 id="official-definition---what-is-kubernetes">Official Definition - What is Kubernetes?&lt;/h2>
&lt;blockquote>
&lt;p>&amp;ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.&amp;rdquo;&lt;/p>
&lt;/blockquote>
&lt;h2 id="introduction---basic-principle">Introduction - Basic Principle&lt;/h2>
&lt;p>The foundation of the Gardener (providing &lt;strong>Kubernetes Clusters as a Service&lt;/strong>) is Kubernetes itself, because Kubernetes is the go-to solution to manage software in the Cloud, even when it&amp;rsquo;s Kubernetes itself (see also OpenStack which is provisioned more and more on top of Kubernetes as well).&lt;/p>
&lt;p>While self-hosting, meaning to run Kubernetes components inside Kubernetes, is a popular topic in the community, we apply a special pattern catering to the needs of our cloud platform to provision hundreds or even thousands of clusters. We take a so-called &amp;ldquo;seed&amp;rdquo; cluster and seed the control plane (such as the API server, scheduler, controllers, etcd persistence and others) of an end-user cluster, which we call &amp;ldquo;shoot&amp;rdquo; cluster, as pods into the &amp;ldquo;seed&amp;rdquo; cluster. That means that one &amp;ldquo;seed&amp;rdquo; cluster, of which we will have one per IaaS and region, hosts the control planes of multiple &amp;ldquo;shoot&amp;rdquo; clusters. That allows us to avoid dedicated hardware/virtual machines for the &amp;ldquo;shoot&amp;rdquo; cluster control planes. We simply put the control plane into pods/containers and since the &amp;ldquo;seed&amp;rdquo; cluster watches them, they can be deployed with a replica count of 1 and only need to be scaled out when the control plane gets under pressure, but no longer for HA reasons. At the same time, the deployments get simpler (standard Kubernetes deployment) and easier to update (standard Kubernetes rolling update). The actual &amp;ldquo;shoot&amp;rdquo; cluster consists only of the worker nodes (no control plane) and therefore the users may get full administrative access to their clusters.&lt;/p>
&lt;h2 id="setting-the-scene---components-and-procedure">Setting The Scene - Components and Procedure&lt;/h2>
&lt;p>We provide a central operator UI, which we call the &amp;ldquo;Gardener Dashboard&amp;rdquo;. It talks to a dedicated cluster, which we call the &amp;ldquo;Garden&amp;rdquo; cluster, and uses custom resources managed by an &lt;a href="https://kubernetes.io/docs/concepts/api-extension/custom-resources/#api-server-aggregation">aggregated API server&lt;/a> (one of the general extension concepts of Kubernetes) to represent &amp;ldquo;shoot&amp;rdquo; clusters. In this &amp;ldquo;Garden&amp;rdquo; cluster runs the &amp;ldquo;Gardener&amp;rdquo;, which is basically a Kubernetes controller that watches the custom resources and acts upon them, i.e. creates, updates/modifies, or deletes &amp;ldquo;shoot&amp;rdquo; clusters. The creation follows basically these steps:&lt;/p>
&lt;ul>
&lt;li>Create a namespace in the &amp;ldquo;seed&amp;rdquo; cluster for the &amp;ldquo;shoot&amp;rdquo; cluster, which will host the &amp;ldquo;shoot&amp;rdquo; cluster control plane.&lt;/li>
&lt;li>Generate secrets and credentials, which the worker nodes will need to talk to the control plane.&lt;/li>
&lt;li>Create the infrastructure (using &lt;a href="https://www.terraform.io/">Terraform&lt;/a>), which basically consists out of the network setup.&lt;/li>
&lt;li>Deploy the &amp;ldquo;shoot&amp;rdquo; cluster control plane into the &amp;ldquo;shoot&amp;rdquo; namespace in the &amp;ldquo;seed&amp;rdquo; cluster, containing the &amp;ldquo;machine-controller-manager&amp;rdquo; pod.&lt;/li>
&lt;li>Create machine CRDs in the &amp;ldquo;seed&amp;rdquo; cluster, describing the configuration and the number of worker machines for the &amp;ldquo;shoot&amp;rdquo; (the machine-controller-manager watches the CRDs and creates virtual machines out of it).&lt;/li>
&lt;li>Wait for the &amp;ldquo;shoot&amp;rdquo; cluster API server to become responsive (pods will be scheduled, persistent volumes and load balancers are created by Kubernetes via the respective cloud provider).&lt;/li>
&lt;li>Finally, we deploy &lt;code>kube-system&lt;/code> daemons like &lt;code>kube-proxy&lt;/code> and further add-ons like the &lt;code>dashboard&lt;/code> into the &amp;ldquo;shoot&amp;rdquo; cluster and the cluster becomes active.&lt;/li>
&lt;/ul>
&lt;h2 id="overview-architecture-diagram">Overview Architecture Diagram&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardener-architecture-overview_2bd462.png" alt="Gardener Overview Architecture Diagram">&lt;/p>
&lt;h2 id="detailed-architecture-diagram">Detailed Architecture Diagram&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardener-architecture-detailed_945c90.png" alt="Gardener Detailed Architecture Diagram">&lt;/p>
&lt;p>Note: The &lt;code>kubelet&lt;/code>, as well as the pods inside the &amp;ldquo;shoot&amp;rdquo; cluster, talks through the front-door (load balancer IP; public Internet) to its &amp;ldquo;shoot&amp;rdquo; cluster API server running in the &amp;ldquo;seed&amp;rdquo; cluster. The reverse communication from the API server to the pod, service, and node networks happens through a VPN connection that we deploy into the &amp;ldquo;seed&amp;rdquo; and &amp;ldquo;shoot&amp;rdquo; clusters.&lt;/p></description></item><item><title>Docs: Authentication</title><link>https://gardener.cloud/docs/gardener/api-reference/authentication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/api-reference/authentication/</guid><description>
&lt;p>Packages:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://gardener.cloud/docs/gardener/api-reference/authentication/#authentication.gardener.cloud/v1alpha1">authentication.gardener.cloud/v1alpha1&lt;/a>
&lt;/li>
&lt;/ul>
&lt;h2 id="authentication.gardener.cloud/v1alpha1">authentication.gardener.cloud/v1alpha1&lt;/h2>
&lt;p>
&lt;p>Package v1alpha1 is a version of the API.
“authentication.gardener.cloud/v1alpha1” API is already used for CRD registration and must not be served by the API server.&lt;/p>
&lt;/p>
Resource Types:
&lt;ul>&lt;/ul>
&lt;h3 id="authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequest">AdminKubeconfigRequest
&lt;/h3>
&lt;p>
&lt;p>AdminKubeconfigRequest can be used to request a kubeconfig with admin credentials
for a Shoot cluster.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>metadata&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Standard object metadata.&lt;/p>
Refer to the Kubernetes API documentation for the fields of the
&lt;code>metadata&lt;/code> field.
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>spec&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/gardener/api-reference/authentication/#authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequestSpec">
AdminKubeconfigRequestSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Spec is the specification of the AdminKubeconfigRequest.&lt;/p>
&lt;br/>
&lt;br/>
&lt;table>
&lt;tr>
&lt;td>
&lt;code>expirationSeconds&lt;/code>&lt;/br>
&lt;em>
int64
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ExpirationSeconds is the requested validity duration of the credential. The
credential issuer may return a credential with a different validity duration so a
client needs to check the ‘expirationTimestamp’ field in a response.
Defaults to 1 hour.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/gardener/api-reference/authentication/#authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequestStatus">
AdminKubeconfigRequestStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Status is the status of the AdminKubeconfigRequest.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequestSpec">AdminKubeconfigRequestSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/gardener/api-reference/authentication/#authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequest">AdminKubeconfigRequest&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>AdminKubeconfigRequestSpec contains the expiration time of the kubeconfig.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>expirationSeconds&lt;/code>&lt;/br>
&lt;em>
int64
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ExpirationSeconds is the requested validity duration of the credential. The
credential issuer may return a credential with a different validity duration so a
client needs to check the ‘expirationTimestamp’ field in a response.
Defaults to 1 hour.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequestStatus">AdminKubeconfigRequestStatus
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/gardener/api-reference/authentication/#authentication.gardener.cloud/v1alpha1.AdminKubeconfigRequest">AdminKubeconfigRequest&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>AdminKubeconfigRequestStatus is the status of the AdminKubeconfigRequest containing
the kubeconfig and expiration of the credential.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>kubeconfig&lt;/code>&lt;/br>
&lt;em>
[]byte
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Kubeconfig contains the kubeconfig with cluster-admin privileges for the shoot cluster.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>expirationTimestamp&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#time-v1-meta">
Kubernetes meta/v1.Time
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>ExpirationTimestamp is the expiration timestamp of the returned credential.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="authentication.gardener.cloud/v1alpha1.ViewerKubeconfigRequest">ViewerKubeconfigRequest
&lt;/h3>
&lt;p>
&lt;p>ViewerKubeconfigRequest can be used to request a kubeconfig with viewer credentials (excluding Secrets)
for a Shoot cluster.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>metadata&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Standard object metadata.&lt;/p>
Refer to the Kubernetes API documentation for the fields of the
&lt;code>metadata&lt;/code> field.
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>spec&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/gardener/api-reference/authentication/#authentication.gardener.cloud/v1alpha1.ViewerKubeconfigRequestSpec">
ViewerKubeconfigRequestSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Spec is the specification of the ViewerKubeconfigRequest.&lt;/p>
&lt;br/>
&lt;br/>
&lt;table>
&lt;tr>
&lt;td>
&lt;code>expirationSeconds&lt;/code>&lt;/br>
&lt;em>
int64
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ExpirationSeconds is the requested validity duration of the credential. The
credential issuer may return a credential with a different validity duration so a
client needs to check the ‘expirationTimestamp’ field in a response.
Defaults to 1 hour.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/gardener/api-reference/authentication/#authentication.gardener.cloud/v1alpha1.ViewerKubeconfigRequestStatus">
ViewerKubeconfigRequestStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Status is the status of the ViewerKubeconfigRequest.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="authentication.gardener.cloud/v1alpha1.ViewerKubeconfigRequestSpec">ViewerKubeconfigRequestSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/gardener/api-reference/authentication/#authentication.gardener.cloud/v1alpha1.ViewerKubeconfigRequest">ViewerKubeconfigRequest&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>ViewerKubeconfigRequestSpec contains the expiration time of the kubeconfig.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>expirationSeconds&lt;/code>&lt;/br>
&lt;em>
int64
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ExpirationSeconds is the requested validity duration of the credential. The
credential issuer may return a credential with a different validity duration so a
client needs to check the ‘expirationTimestamp’ field in a response.
Defaults to 1 hour.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="authentication.gardener.cloud/v1alpha1.ViewerKubeconfigRequestStatus">ViewerKubeconfigRequestStatus
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/gardener/api-reference/authentication/#authentication.gardener.cloud/v1alpha1.ViewerKubeconfigRequest">ViewerKubeconfigRequest&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>ViewerKubeconfigRequestStatus is the status of the ViewerKubeconfigRequest containing
the kubeconfig and expiration of the credential.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>kubeconfig&lt;/code>&lt;/br>
&lt;em>
[]byte
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Kubeconfig contains the kubeconfig with viewer privileges (excluding Secrets) for the shoot cluster.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>expirationTimestamp&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#time-v1-meta">
Kubernetes meta/v1.Time
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>ExpirationTimestamp is the expiration timestamp of the returned credential.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;hr/>
&lt;p>&lt;em>
Generated with &lt;a href="https://github.com/ahmetb/gen-crd-api-reference-docs">gen-crd-api-reference-docs&lt;/a>
&lt;/em>&lt;/p></description></item><item><title>Docs: Authentication Gardener Control Plane</title><link>https://gardener.cloud/docs/gardener/deployment/authentication_gardener_control_plane/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/authentication_gardener_control_plane/</guid><description>
&lt;h1 id="authentication-of-gardener-control-plane-components-against-the-garden-cluster">Authentication of Gardener Control Plane Components Against the Garden Cluster&lt;/h1>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> This document refers to Gardener&amp;rsquo;s API server, admission controller, controller manager and scheduler components. Any reference to the term &lt;strong>Gardener control plane component&lt;/strong> can be replaced with any of the mentioned above.&lt;/p>
&lt;/blockquote>
&lt;p>There are several authentication possibilities depending on whether or not &lt;a href="https://github.com/gardener/garden-setup#concept-the-virtual-cluster">the concept of Virtual Garden&lt;/a> is used.&lt;/p>
&lt;h2 id="virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster">Virtual Garden is not used, i.e., the &lt;code>runtime&lt;/code> Garden cluster is also the &lt;code>target&lt;/code> Garden cluster.&lt;/h2>
&lt;h3 id="automounted-service-account-token">Automounted Service Account Token&lt;/h3>
&lt;p>The easiest way to deploy a &lt;strong>Gardener control plane component&lt;/strong> is to not provide a &lt;code>kubeconfig&lt;/code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.&lt;/p>
&lt;h3 id="service-account-token-volume-projection">Service Account Token Volume Projection&lt;/h3>
&lt;p>Another solution is to use &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection">Service Account Token Volume Projection&lt;/a> combined with a &lt;code>kubeconfig&lt;/code> referencing a token file (see the example below).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusters:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- cluster:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> certificate-authority-data: &amp;lt;CA-DATA&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: https://default.kubernetes.svc.cluster.local
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>current-context: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>users:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tokenFile: /var/run/secrets/projected/serviceaccount/token
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will allow for automatic rotation of the service account token by the &lt;code>kubelet&lt;/code>. The configuration can be achieved by setting both &lt;code>.Values.global.&amp;lt;GardenerControlPlaneComponent&amp;gt;.serviceAccountTokenVolumeProjection.enabled: true&lt;/code> and &lt;code>.Values.global.&amp;lt;GardenerControlPlaneComponent&amp;gt;.kubeconfig&lt;/code> in the respective chart&amp;rsquo;s &lt;code>values.yaml&lt;/code> file.&lt;/p>
&lt;h2 id="virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster">Virtual Garden is used, i.e., the &lt;code>runtime&lt;/code> Garden cluster is different from the &lt;code>target&lt;/code> Garden cluster.&lt;/h2>
&lt;h3 id="service-account">Service Account&lt;/h3>
&lt;p>The easiest way to setup the authentication is to create a service account and the respective roles will be bound to this service account in the &lt;code>target&lt;/code> cluster. Then use the generated service account token and craft a &lt;code>kubeconfig&lt;/code>, which will be used by the workload in the &lt;code>runtime&lt;/code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting &lt;code>.Values.global.deployment.virtualGarden.enabled: true&lt;/code> and following these steps:&lt;/p>
&lt;ol>
&lt;li>Deploy the &lt;code>application&lt;/code> part of the charts in the &lt;code>target&lt;/code> cluster.&lt;/li>
&lt;li>Get the service account token and craft the &lt;code>kubeconfig&lt;/code>.&lt;/li>
&lt;li>Set the crafted &lt;code>kubeconfig&lt;/code> and deploy the &lt;code>runtime&lt;/code> part of the charts in the &lt;code>runtime&lt;/code> cluster.&lt;/li>
&lt;/ol>
&lt;h3 id="client-certificate">Client Certificate&lt;/h3>
&lt;p>Another solution is to bind the roles in the &lt;code>target&lt;/code> cluster to a &lt;code>User&lt;/code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both &lt;code>.Values.global.deployment.virtualGarden.enabled: true&lt;/code> and &lt;code>.Values.global.deployment.virtualGarden.&amp;lt;GardenerControlPlaneComponent&amp;gt;.user.name&lt;/code>, then following these steps:&lt;/p>
&lt;ol>
&lt;li>Generate a client certificate for the &lt;code>target&lt;/code> cluster for the respective user.&lt;/li>
&lt;li>Deploy the &lt;code>application&lt;/code> part of the charts in the &lt;code>target&lt;/code> cluster.&lt;/li>
&lt;li>Craft a &lt;code>kubeconfig&lt;/code> using the already generated client certificate.&lt;/li>
&lt;li>Set the crafted &lt;code>kubeconfig&lt;/code> and deploy the &lt;code>runtime&lt;/code> part of the charts in the &lt;code>runtime&lt;/code> cluster.&lt;/li>
&lt;/ol>
&lt;h3 id="projected-service-account-token">Projected Service Account Token&lt;/h3>
&lt;p>This approach requires an already deployed and configured &lt;a href="https://github.com/gardener/oidc-webhook-authenticator">oidc-webhook-authenticator&lt;/a> for the &lt;code>target&lt;/code> cluster. Also, the &lt;code>runtime&lt;/code> cluster should be registered as a trusted identity provider in the &lt;code>target&lt;/code> cluster. Then, projected service accounts tokens from the &lt;code>runtime&lt;/code> cluster can be used to authenticate against the &lt;code>target&lt;/code> cluster. The needed steps are as follows:&lt;/p>
&lt;ol>
&lt;li>Deploy &lt;a href="https://github.com/gardener/oidc-webhook-authenticator">OWA&lt;/a> and establish the needed trust.&lt;/li>
&lt;li>Set &lt;code>.Values.global.deployment.virtualGarden.enabled: true&lt;/code> and &lt;code>.Values.global.deployment.virtualGarden.&amp;lt;GardenerControlPlaneComponent&amp;gt;.user.name&lt;/code>.
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> username value will depend on the trust configuration, e.g., &lt;code>&amp;lt;prefix&amp;gt;:system:serviceaccount:&amp;lt;namespace&amp;gt;:&amp;lt;serviceaccount&amp;gt;&lt;/code>&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>Set &lt;code>.Values.global.&amp;lt;GardenerControlPlaneComponent&amp;gt;.serviceAccountTokenVolumeProjection.enabled: true&lt;/code> and &lt;code>.Values.global.&amp;lt;GardenerControlPlaneComponent&amp;gt;.serviceAccountTokenVolumeProjection.audience&lt;/code>.
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> audience value will depend on the trust configuration, e.g., &lt;code>&amp;lt;client-id-from-trust-config&amp;gt;&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>Craft a kubeconfig (see the example below).&lt;/li>
&lt;li>Deploy the &lt;code>application&lt;/code> part of the charts in the &lt;code>target&lt;/code> cluster.&lt;/li>
&lt;li>Deploy the &lt;code>runtime&lt;/code> part of the charts in the &lt;code>runtime&lt;/code> cluster.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>clusters:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- cluster:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> certificate-authority-data: &amp;lt;CA-DATA&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: https://virtual-garden.api
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: virtual-garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: virtual-garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: virtual-garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: virtual-garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>current-context: virtual-garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>users:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: virtual-garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tokenFile: /var/run/secrets/projected/serviceaccount/token
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Automating Project Resource Management</title><link>https://gardener.cloud/docs/dashboard/automated-resource-management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/dashboard/automated-resource-management/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>The project resource operations that are performed manually in the dashboard or via &lt;code>kubectl&lt;/code> can be automated using the &lt;a href="https://gardener.cloud/docs/gardener/api-reference/">&lt;strong>Gardener API&lt;/strong>&lt;/a> and a &lt;strong>Service Account&lt;/strong> authorized to perform them.&lt;/p>
&lt;h2 id="create-a-service-account">Create a Service Account&lt;/h2>
&lt;h3 id="prerequisites">Prerequisites&lt;/h3>
&lt;ul>
&lt;li>You are logged on to the Gardener Dashboard&lt;/li>
&lt;li>You have &lt;a href="https://gardener.cloud/docs/dashboard/working-with-projects/">created a project&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="steps">Steps&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Select your project and choose &lt;em>MEMBERS&lt;/em> from the menu on the left.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Locate the section &lt;em>Service Accounts&lt;/em> and choose &lt;em>+&lt;/em>.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/01-add-service-account_553867.png" alt="Add service account">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Enter the service account details.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/02-enter-service-account-details_e9e737.png" alt="Enter service account details">&lt;/p>
&lt;p>The following &lt;em>Roles&lt;/em> are available:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Role&lt;/th>
&lt;th style="text-align:left">Granted Permissions&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">&lt;em>Owner&lt;/em>&lt;/td>
&lt;td style="text-align:left">Combines the &lt;em>Admin&lt;/em>, &lt;em>UAM&lt;/em> and &lt;em>Service Account Manager&lt;/em> roles. There can only be one owner per project. You can change the owner on the project administration page.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">&lt;em>Admin&lt;/em>&lt;/td>
&lt;td style="text-align:left">Allows to manage resources inside the project (e.g. secrets, shoots, configmaps and similar) and to manage permissions for service accounts. Note that the &lt;em>Admin&lt;/em> role has read-only access to service accounts.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">&lt;em>Viewer&lt;/em>&lt;/td>
&lt;td style="text-align:left">Provides read access to project details and shoots. Has access to shoots but is not able to create new ones. Cannot read cloud provider secrets.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">&lt;em>UAM&lt;/em>&lt;/td>
&lt;td style="text-align:left">Allows to add/modify/remove human users, service accounts or groups to/from the project member list. In case an external UAM system is connected via a service account, only this account should get the &lt;em>UAM&lt;/em> role.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">&lt;em>&lt;a href="https://gardener.cloud/docs/gardener/service-account-manager/">Service Account Manager&lt;/a>&lt;/em>&lt;/td>
&lt;td style="text-align:left">Allows to manage service accounts inside the project namespace and request tokens for them. The permissions of the created service accounts are instead managed by the &lt;em>Admin&lt;/em> role. For security reasons this role should not be assigned to service accounts. In particular it should be ensured that the service account is not able to refresh service account tokens forever.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;ol start="4">
&lt;li>Choose &lt;em>CREATE&lt;/em>.&lt;/li>
&lt;/ol>
&lt;h2 id="use-the-service-account">Use the Service Account&lt;/h2>
&lt;p>To use the service account, download or copy its &lt;code>kubeconfig&lt;/code>. With it you can connect to the API endpoint of your Gardener project.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/03-download-service-account-kubeconfig_08a0f5.png" alt="Download service account kubeconfig">&lt;/p>
&lt;blockquote>
&lt;p>Note: The downloaded &lt;code>kubeconfig&lt;/code> contains the service account credentials. Treat with care.&lt;/p>
&lt;/blockquote>
&lt;h2 id="delete-the-service-account">Delete the Service Account&lt;/h2>
&lt;p>Choose &lt;em>Delete Service Account&lt;/em> to delete it.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/04-delete-service-account_ecfc83.png" alt="Delete service account">&lt;/p>
&lt;h2 id="related-links">Related Links&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/service-account-manager/">Service Account Manager&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Autoscaling Specifics for Components</title><link>https://gardener.cloud/docs/gardener/autoscaling-specifics-for-components/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/autoscaling-specifics-for-components/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>This document describes the used autoscaling mechanism for several components.&lt;/p>
&lt;h2 id="garden-or-shoot-cluster-etcd">Garden or Shoot Cluster etcd&lt;/h2>
&lt;p>By default, if none of the autoscaling modes is requested the &lt;code>etcd&lt;/code> is deployed with static resources, without autoscaling.&lt;/p>
&lt;p>However, there are two supported autoscaling modes for the Garden or Shoot cluster etcd.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>HVPA&lt;/code>&lt;/p>
&lt;p>In &lt;code>HVPA&lt;/code> mode, the etcd is scaled by the &lt;a href="https://github.com/gardener/hvpa-controller">hvpa-controller&lt;/a>. The gardenlet/gardener-operator is creating an &lt;code>HVPA&lt;/code> resource for the etcd (&lt;code>main&lt;/code> or &lt;code>events&lt;/code>).
The &lt;code>HVPA&lt;/code> enables a vertical scaling for etcd.&lt;/p>
&lt;p>The &lt;code>HVPA&lt;/code> mode is the used autoscaling mode when the &lt;code>HVPA&lt;/code> feature gate is enabled and the &lt;code>VPAForETCD&lt;/code> feature gate is disabled.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>VPA&lt;/code>&lt;/p>
&lt;p>In &lt;code>VPA&lt;/code> mode, the etcd is scaled by a native &lt;code>VPA&lt;/code> resource.&lt;/p>
&lt;p>The &lt;code>VPA&lt;/code> mode is the used autoscaling mode when the &lt;code>VPAForETCD&lt;/code> feature gate is enabled (takes precedence over the &lt;code>HVPA&lt;/code> feature gate).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>[!NOTE]
Starting with release &lt;code>v1.97&lt;/code>, the &lt;code>VPAForETCD&lt;/code> feature gate is enabled by default.&lt;/p>
&lt;/blockquote>
&lt;p>For both of the autoscaling modes downscaling is handled more pessimistically to prevent many subsequent etcd restarts. Thus, for &lt;code>production&lt;/code> and &lt;code>infrastructure&lt;/code> Shoot clusters (or all Garden clusters), downscaling is deactivated for the main etcd. For all other Shoot clusters, lower advertised requests/limits are only applied during the Shoot&amp;rsquo;s maintenance time window.&lt;/p>
&lt;h2 id="shoot-kubernetes-api-server">Shoot Kubernetes API Server&lt;/h2>
&lt;p>There are three supported autoscaling modes for the Shoot Kubernetes API server.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>Baseline&lt;/code>&lt;/p>
&lt;p>In &lt;code>Baseline&lt;/code> mode, the Shoot Kubernetes API server is scaled by active HPA and VPA in passive, recommend-only mode.&lt;/p>
&lt;p>The API server resource requests are computed based on the Shoot&amp;rsquo;s minimum Nodes count:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Range&lt;/th>
&lt;th>Resource Requests&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>[0, 2]&lt;/td>
&lt;td>&lt;code>800m&lt;/code>, &lt;code>800Mi&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>(2, 10]&lt;/td>
&lt;td>&lt;code>1000m&lt;/code>, &lt;code>1100Mi&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>(10, 50]&lt;/td>
&lt;td>&lt;code>1200m&lt;/code>, &lt;code>1600Mi&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>(50, 100]&lt;/td>
&lt;td>&lt;code>2500m&lt;/code>, &lt;code>5200Mi&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>(100, inf.)&lt;/td>
&lt;td>&lt;code>3000m&lt;/code>, &lt;code>5200Mi&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The API server&amp;rsquo;s min replicas count is 2, the max replicas count - 3.&lt;/p>
&lt;p>The &lt;code>Baseline&lt;/code> mode is the used autoscaling mode when the &lt;code>HVPA&lt;/code> and &lt;code>VPAAndHPAForAPIServer&lt;/code> feature gates are not enabled.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>HVPA&lt;/code>&lt;/p>
&lt;p>In &lt;code>HVPA&lt;/code> mode, the Shoot Kubernetes API server is scaled by the &lt;a href="https://github.com/gardener/hvpa-controller">hvpa-controller&lt;/a>. The gardenlet is creating an &lt;code>HVPA&lt;/code> resource for the API server. The &lt;code>HVPA&lt;/code> resource is backed by HPA and VPA both in recommend-only mode. The hvpa-controller is responsible for enabling simultaneous horizontal and vertical scaling by incorporating the recommendations from the HPA and VPA.&lt;/p>
&lt;p>The initial API server resource requests are &lt;code>500m&lt;/code> and &lt;code>1Gi&lt;/code>.
HVPA&amp;rsquo;s HPA is scaling only on CPU (average utilization 80%). HVPA&amp;rsquo;s VPA max allowed values are &lt;code>8&lt;/code> CPU and &lt;code>25G&lt;/code>.&lt;/p>
&lt;p>The API server&amp;rsquo;s min replicas count is 2, the max replicas count - 3.&lt;/p>
&lt;p>The &lt;code>HVPA&lt;/code> mode is the used autoscaling mode when the &lt;code>HVPA&lt;/code> feature gate is enabled (and the &lt;code>VPAAndHPAForAPIServer&lt;/code> feature gate is disabled).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>VPAAndHPA&lt;/code>&lt;/p>
&lt;p>In &lt;code>VPAAndHPA&lt;/code> mode, the Shoot Kubernetes API server is scaled simultaneously by VPA and HPA on the same metric (CPU and memory usage). The pod-trashing cycle between VPA and HPA scaling on the same metric is avoided by configuring the HPA to scale on average usage (not on average utilization) and by picking the target average utilization values in sync with VPA&amp;rsquo;s allowed maximums. This makes possible VPA to first scale vertically on CPU/memory usage. Once all Pods&amp;rsquo; average CPU/memory usage is close to exceed the VPA&amp;rsquo;s allowed maximum CPU/memory (the HPA&amp;rsquo;s target average utilization, 1/7 less than VPA&amp;rsquo;s allowed maximums), HPA is scaling horizontally (by adding a new replica).&lt;/p>
&lt;p>The &lt;code>VPAAndHPA&lt;/code> mode is introduced to address disadvantages with HVPA: additional component; modifies the deployment triggering unnecessary rollouts; vertical scaling only at max replicas; stuck vertical resource requests when scaling in again; etc.&lt;/p>
&lt;p>The initial API server resource requests are &lt;code>250m&lt;/code> and &lt;code>500Mi&lt;/code>.
VPA&amp;rsquo;s max allowed values are &lt;code>7&lt;/code> CPU and &lt;code>28G&lt;/code>. HPA&amp;rsquo;s average target usage values are &lt;code>6&lt;/code> CPU and &lt;code>24G&lt;/code>.&lt;/p>
&lt;p>The API server&amp;rsquo;s min replicas count is 2, the max replicas count - 6.&lt;/p>
&lt;p>The &lt;code>VPAAndHPA&lt;/code> mode is the used autoscaling mode when the &lt;code>VPAAndHPAForAPIServer&lt;/code> feature gate is enabled (takes precedence over the &lt;code>HVPA&lt;/code> feature gate).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In all scaling modes the min replicas count of 2 is imposed by the &lt;a href="https://gardener.cloud/docs/gardener/high-availability/#control-plane-components">High Availability of Shoot Control Plane Components&lt;/a>.&lt;/p>
&lt;p>The gardenlet sets the initial API server resource requests only when the Deployment is not found. When the Deployment exists, it is not overwriting the kube-apiserver container resources.&lt;/p>
&lt;h2 id="disabling-scale-down-for-components-in-the-shoot-control-plane">Disabling Scale Down for Components in the Shoot Control Plane&lt;/h2>
&lt;p>Some Shoot clusters&amp;rsquo; control plane components can be overloaded and can have very high resource usage. The existing autoscaling solution could be imperfect to cover these cases. Scale down actions for such overloaded components could be disruptive.&lt;/p>
&lt;p>To prevent such disruptive scale-down actions it is possible to disable scale down of the etcd, Kubernetes API server and Kubernetes controller manager in the Shoot control plane by annotating the Shoot with &lt;code>alpha.control-plane.scaling.shoot.gardener.cloud/scale-down-disabled=true&lt;/code>.&lt;/p>
&lt;p>There are the following specifics for when disabling scale-down for the Kubernetes API server component:&lt;/p>
&lt;ul>
&lt;li>In &lt;code>Baseline&lt;/code> and &lt;code>HVPA&lt;/code> modes the HPA&amp;rsquo;s min and max replicas count are set to 4.&lt;/li>
&lt;li>In &lt;code>VPAAndHPA&lt;/code> mode if the HPA resource exists and HPA&amp;rsquo;s &lt;code>spec.minReplicas&lt;/code> is not nil then the min replicas count is &lt;code>max(spec.minReplicas, status.desiredReplicas)&lt;/code>. When scale-down is disabled, this allows operators to specify a custom value for HPA &lt;code>spec.minReplicas&lt;/code> and this value not to be reverted by gardenlet. I.e, HPA &lt;em>does&lt;/em> scale down to min replicas but not below min replicas. HPA&amp;rsquo;s max replicas count is 6.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Note: The &lt;code>alpha.control-plane.scaling.shoot.gardener.cloud/scale-down-disabled&lt;/code> annotation is alpha and can be removed anytime without further notice. Only use it if you know what you do.&lt;/p>
&lt;/blockquote>
&lt;h2 id="virtual-kubernetes-api-server-and-gardener-api-server">Virtual Kubernetes API Server and Gardener API Server&lt;/h2>
&lt;p>The virtual Kubernetes API server&amp;rsquo;s autoscaling is same as the Shoot Kubernetes API server&amp;rsquo;s with the following differences:&lt;/p>
&lt;ul>
&lt;li>The initial API server resource requests are &lt;code>600m&lt;/code> and &lt;code>512Mi&lt;/code> in all autoscaling modes.&lt;/li>
&lt;li>The min replicas count is 2 for a non-HA virtual cluster and 3 for an HA virtual cluster. The max replicas count is 6.&lt;/li>
&lt;li>In &lt;code>HVPA&lt;/code> mode, HVPA&amp;rsquo;s HPA is scaling on both CPU and memory (average utilization 80% for both).&lt;/li>
&lt;/ul>
&lt;p>The Gardener API server&amp;rsquo;s autoscaling is the same as the Shoot Kubernetes API server&amp;rsquo;s with the following differences:&lt;/p>
&lt;ul>
&lt;li>The initial API server resource requests are &lt;code>600m&lt;/code> and &lt;code>512Mi&lt;/code> in all autoscaling modes.&lt;/li>
&lt;li>The min replicas count is 2 for a non-HA virtual cluster and 3 for an HA virtual cluster. The max replicas count is 6.&lt;/li>
&lt;li>In &lt;code>HVPA&lt;/code> mode, HVPA&amp;rsquo;s HPA is scaling on both CPU and memory (average utilization 80% for both).&lt;/li>
&lt;li>In &lt;code>HVPA&lt;/code> mode, HVPA&amp;rsquo;s VPA max allowed values are &lt;code>4&lt;/code> CPU and &lt;code>25G&lt;/code>.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Azure Permissions</title><link>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/azure-permissions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/azure-permissions/</guid><description>
&lt;h1 id="azure-permissions">Azure Permissions&lt;/h1>
&lt;p>The following document describes the required Azure actions manage a Shoot cluster on Azure split by the different Azure provider/services.&lt;/p>
&lt;p>Be aware some actions are just required if particilar deployment sceanrios or features e.g. bring your own vNet, use Azure-file, let the Shoot act as Seed etc. should be used.&lt;/p>
&lt;h2 id="microsoftcompute">&lt;code>Microsoft.Compute&lt;/code>&lt;/h2>
&lt;pre tabindex="0">&lt;code># Required if a non zonal cluster based on Availability Set should be used.
Microsoft.Compute/availabilitySets/delete
Microsoft.Compute/availabilitySets/read
Microsoft.Compute/availabilitySets/write
# Required to let Kubernetes manage Azure disks.
Microsoft.Compute/disks/delete
Microsoft.Compute/disks/read
Microsoft.Compute/disks/write
# Required for to fetch meta information about disk and virtual machines sizes.
Microsoft.Compute/locations/diskOperations/read
Microsoft.Compute/locations/operations/read
Microsoft.Compute/locations/vmSizes/read
# Required if csi snapshot capabilities should be used and/or the Shoot should act as a Seed.
Microsoft.Compute/snapshots/delete
Microsoft.Compute/snapshots/read
Microsoft.Compute/snapshots/write
# Required to let Gardener/Machine-Controller-Manager manage the cluster nodes/machines.
Microsoft.Compute/virtualMachines/delete
Microsoft.Compute/virtualMachines/read
Microsoft.Compute/virtualMachines/start/action
Microsoft.Compute/virtualMachines/write
# Required if a non zonal cluster based on VMSS Flex (VMO) should be used.
Microsoft.Compute/virtualMachineScaleSets/delete
Microsoft.Compute/virtualMachineScaleSets/read
Microsoft.Compute/virtualMachineScaleSets/write
&lt;/code>&lt;/pre>&lt;h2 id="microsoftmanagedidentity">&lt;code>Microsoft.ManagedIdentity&lt;/code>&lt;/h2>
&lt;pre tabindex="0">&lt;code># Required if a user provided Azure managed identity should attached to the cluster nodes.
Microsoft.ManagedIdentity/userAssignedIdentities/assign/action
Microsoft.ManagedIdentity/userAssignedIdentities/read
&lt;/code>&lt;/pre>&lt;h2 id="microsoftmarketplaceordering">&lt;code>Microsoft.MarketplaceOrdering&lt;/code>&lt;/h2>
&lt;pre tabindex="0">&lt;code># Required if nodes/machines should be created with images hosted on the Azure Marketplace.
Microsoft.MarketplaceOrdering/offertypes/publishers/offers/plans/agreements/read
Microsoft.MarketplaceOrdering/offertypes/publishers/offers/plans/agreements/write
&lt;/code>&lt;/pre>&lt;h2 id="microsoftnetwork">&lt;code>Microsoft.Network&lt;/code>&lt;/h2>
&lt;pre tabindex="0">&lt;code># Required to let Kubernetes manage services of type &amp;#39;LoadBalancer&amp;#39;.
Microsoft.Network/loadBalancers/backendAddressPools/join/action
Microsoft.Network/loadBalancers/delete
Microsoft.Network/loadBalancers/read
Microsoft.Network/loadBalancers/write
# Required in case the Shoot should use NatGateway(s).
Microsoft.Network/natGateways/delete
Microsoft.Network/natGateways/join/action
Microsoft.Network/natGateways/read
Microsoft.Network/natGateways/write
# Required to let Gardener/Machine-Controller-Manager manage the cluster nodes/machines.
Microsoft.Network/networkInterfaces/delete
Microsoft.Network/networkInterfaces/ipconfigurations/join/action
Microsoft.Network/networkInterfaces/ipconfigurations/read
Microsoft.Network/networkInterfaces/join/action
Microsoft.Network/networkInterfaces/read
Microsoft.Network/networkInterfaces/write
# Required to let Gardener maintain the basic infrastructure of the Shoot cluster and maintaing LoadBalancer services.
Microsoft.Network/networkSecurityGroups/delete
Microsoft.Network/networkSecurityGroups/join/action
Microsoft.Network/networkSecurityGroups/read
Microsoft.Network/networkSecurityGroups/write
# Required for managing LoadBalancers and NatGateways.
Microsoft.Network/publicIPAddresses/delete
Microsoft.Network/publicIPAddresses/join/action
Microsoft.Network/publicIPAddresses/read
Microsoft.Network/publicIPAddresses/write
# Required for managing the basic infrastructure of a cluster and maintaing LoadBalancer services.
Microsoft.Network/routeTables/delete
Microsoft.Network/routeTables/join/action
Microsoft.Network/routeTables/read
Microsoft.Network/routeTables/routes/delete
Microsoft.Network/routeTables/routes/read
Microsoft.Network/routeTables/routes/write
Microsoft.Network/routeTables/write
# Required to let Gardener maintain the basic infrastructure of the Shoot cluster.
# Only a subset is required for the bring your own vNet scenario.
Microsoft.Network/virtualNetworks/delete # not required for bring your own vnet
Microsoft.Network/virtualNetworks/read
Microsoft.Network/virtualNetworks/subnets/delete
Microsoft.Network/virtualNetworks/subnets/join/action
Microsoft.Network/virtualNetworks/subnets/read
Microsoft.Network/virtualNetworks/subnets/write
Microsoft.Network/virtualNetworks/write # not required for bring your own vnet
&lt;/code>&lt;/pre>&lt;h2 id="microsoftresources">&lt;code>Microsoft.Resources&lt;/code>&lt;/h2>
&lt;pre tabindex="0">&lt;code># Required to let Gardener maintain the basic infrastructure of the Shoot cluster.
Microsoft.Resources/subscriptions/resourceGroups/delete
Microsoft.Resources/subscriptions/resourceGroups/read
Microsoft.Resources/subscriptions/resourceGroups/write
&lt;/code>&lt;/pre>&lt;h2 id="microsoftstorage">&lt;code>Microsoft.Storage&lt;/code>&lt;/h2>
&lt;pre tabindex="0">&lt;code># Required if Azure File should be used and/or if the Shoot should act as Seed.
Microsoft.Storage/operations/read
Microsoft.Storage/storageAccounts/blobServices/containers/delete
Microsoft.Storage/storageAccounts/blobServices/containers/read
Microsoft.Storage/storageAccounts/blobServices/containers/write
Microsoft.Storage/storageAccounts/blobServices/read
Microsoft.Storage/storageAccounts/delete
Microsoft.Storage/storageAccounts/listkeys/action
Microsoft.Storage/storageAccounts/read
Microsoft.Storage/storageAccounts/write
&lt;/code>&lt;/pre></description></item><item><title>Docs: Backup and Restore</title><link>https://gardener.cloud/docs/gardener/concepts/backup-restore/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/backup-restore/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Kubernetes uses &lt;a href="https://etcd.io/">etcd&lt;/a> as the key-value store for its resource definitions. Gardener supports the backup and restore of etcd. It is the responsibility of the shoot owners to backup the workload data.&lt;/p>
&lt;p>Gardener uses an &lt;a href="https://github.com/gardener/etcd-backup-restore">etcd-backup-restore&lt;/a> component to backup the etcd backing the Shoot cluster regularly and restore it in case of disaster. It is deployed as sidecar via &lt;a href="https://github.com/gardener/etcd-druid">etcd-druid&lt;/a>. This doc mainly focuses on the backup and restore configuration used by Gardener when deploying these components. For more details on the design and internal implementation details, please refer to &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/06-etcd-druid.md">GEP-06&lt;/a> and the documentation on individual repositories.&lt;/p>
&lt;h2 id="bucket-provisioning">Bucket Provisioning&lt;/h2>
&lt;p>Refer to the &lt;a href="https://gardener.cloud/docs/gardener/extensions/backupbucket/">backup bucket extension document&lt;/a> to find out details about configuring the backup bucket.&lt;/p>
&lt;h2 id="backup-policy">Backup Policy&lt;/h2>
&lt;p>etcd-backup-restore supports full snapshot and delta snapshots over full snapshot. In Gardener, this configuration is currently hard-coded to the following parameters:&lt;/p>
&lt;ul>
&lt;li>Full Snapshot schedule:
&lt;ul>
&lt;li>Daily, &lt;code>24hr&lt;/code> interval.&lt;/li>
&lt;li>For each Shoot, the schedule time in a day is randomized based on the configured Shoot maintenance window.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Delta Snapshot schedule:
&lt;ul>
&lt;li>At &lt;code>5min&lt;/code> interval.&lt;/li>
&lt;li>If aggregated events size since last snapshot goes beyond &lt;code>100Mib&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup History / Garbage backup deletion policy:
&lt;ul>
&lt;li>Gardener configures backup restore to have &lt;code>Exponential&lt;/code> garbage collection policy.&lt;/li>
&lt;li>As per policy, the following backups are retained:
&lt;ul>
&lt;li>All full backups and delta backups for the previous hour.&lt;/li>
&lt;li>Latest full snapshot of each previous hour for the day.&lt;/li>
&lt;li>Latest full snapshot of each previous day for 7 days.&lt;/li>
&lt;li>Latest full snapshot of the previous 4 weeks.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Garbage Collection is configured at &lt;code>12hr&lt;/code> interval.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Listing:
&lt;ul>
&lt;li>Gardener doesn&amp;rsquo;t have any API to list out the backups.&lt;/li>
&lt;li>To find the backups list, an admin can checkout the &lt;code>BackupEntry&lt;/code> resource associated with the Shoot which holds the bucket and prefix details on the object store.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="restoration">Restoration&lt;/h2>
&lt;p>The restoration process of etcd is automated through the etcd-backup-restore component from the latest snapshot. Gardener doesn&amp;rsquo;t support Point-In-Time-Recovery (PITR) of etcd. In case of an etcd disaster, the etcd is recovered from the latest backup automatically. For further details, please refer the &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/master/docs/proposals/restoration.md">Restoration&lt;/a> topic. Post restoration of etcd, the Shoot reconciliation loop brings the cluster back to its previous state.&lt;/p>
&lt;p>Again, the Shoot owner is responsible for maintaining the backup/restore of his workload. Gardener only takes care of the cluster&amp;rsquo;s etcd.&lt;/p></description></item><item><title>Docs: BackupBucket</title><link>https://gardener.cloud/docs/gardener/extensions/backupbucket/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/extensions/backupbucket/</guid><description>
&lt;h1 id="contract-backupbucket-resource">Contract: &lt;code>BackupBucket&lt;/code> Resource&lt;/h1>
&lt;p>The Gardener project features a sub-project called &lt;a href="https://github.com/gardener/etcd-backup-restore">etcd-backup-restore&lt;/a> to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) to be created and configured externally with appropriate credentials. The &lt;code>BackupBucket&lt;/code> resource takes this responsibility in Gardener.&lt;/p>
&lt;p>Before introducing the &lt;code>BackupBucket&lt;/code> extension resource, Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see &lt;a href="https://github.com/gardener/gardener/tree/0.27.0/charts/seed-terraformer/charts/aws-backup">AWS Backup&lt;/a>).
Now, Gardener commissions an external, provider-specific controller to take over this task. You can also refer to &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md">backupInfra proposal documentation&lt;/a> to get an idea about how the transition was done and understand the resource in a broader scope.&lt;/p>
&lt;h2 id="what-is-the-scope-of-a-bucket">What Is the Scope of a Bucket?&lt;/h2>
&lt;p>A bucket will be provisioned per &lt;code>Seed&lt;/code>. So, a backup of every &lt;code>Shoot&lt;/code> created on that &lt;code>Seed&lt;/code> will be stored under a different shoot specific prefix under the bucket.
For the backup of the &lt;code>Shoot&lt;/code> rescheduled on different &lt;code>Seed&lt;/code>, it will continue to use the same bucket.&lt;/p>
&lt;h2 id="what-is-the-lifespan-of-a-backupbucket">What Is the Lifespan of a &lt;code>BackupBucket&lt;/code>?&lt;/h2>
&lt;p>The bucket associated with &lt;code>BackupBucket&lt;/code> will be created at the creation of the &lt;code>Seed&lt;/code>. And as per current implementation, it will also be deleted on deletion of the &lt;code>Seed&lt;/code>, if there isn&amp;rsquo;t any &lt;code>BackupEntry&lt;/code> resource associated with it.&lt;/p>
&lt;p>In the future, we plan to introduce a schedule for &lt;code>BackupBucket&lt;/code> - the deletion logic for the &lt;code>BackupBucket&lt;/code> resource, which will reschedule it on different available &lt;code>Seed&lt;/code>s on deletion or failure of a health check for the currently associated &lt;code>seed&lt;/code>. In that case, the &lt;code>BackupBucket&lt;/code> will be deleted only if there isn&amp;rsquo;t any schedulable &lt;code>Seed&lt;/code> available and there isn&amp;rsquo;t any associated &lt;code>BackupEntry&lt;/code> resource.&lt;/p>
&lt;h2 id="what-needs-to-be-implemented-to-support-a-new-infrastructure-provider">What Needs to Be Implemented to Support a New Infrastructure Provider?&lt;/h2>
&lt;p>As part of the seed flow, Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: BackupBucket
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: foo
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: azure
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;some-optional-provider-specific-backupbucket-configuration&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> region: eu-west-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: backupprovider
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: shoot--foo--bar
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>.spec.secretRef&lt;/code> contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. This provider secret will be configured by the Gardener operator in the &lt;code>Seed&lt;/code> resource and propagated over there by the seed controller.&lt;/p>
&lt;p>After your controller has created the required bucket, if required, it generates the secret to access the objects in the bucket and put a reference to it in &lt;code>status&lt;/code>. This secret is supposed to be used by Gardener, or eventually a &lt;code>BackupEntry&lt;/code> resource and etcd-backup-restore component, to backup the etcd.&lt;/p>
&lt;p>In order to support a new infrastructure provider, you need to write a controller that watches all &lt;code>BackupBucket&lt;/code>s with &lt;code>.spec.type=&amp;lt;my-provider-name&amp;gt;&lt;/code>. You can take a look at the below referenced example implementation for the Azure provider.&lt;/p>
&lt;h2 id="references-and-additional-resources">References and Additional Resources&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/extensions/#backupbucket">&lt;code>BackupBucket&lt;/code> API Reference&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener-extension-provider-azure/tree/master/pkg/controller/backupbucket">Exemplary Implementation for the Azure Provider&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/extensions/backupentry/">&lt;code>BackupEntry&lt;/code> Resource Documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md">Shared Bucket Proposal&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: BackupEntry</title><link>https://gardener.cloud/docs/gardener/extensions/backupentry/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/extensions/backupentry/</guid><description>
&lt;h1 id="contract-backupentry-resource">Contract: &lt;code>BackupEntry&lt;/code> Resource&lt;/h1>
&lt;p>The Gardener project features a sub-project called &lt;a href="https://github.com/gardener/etcd-backup-restore">etcd-backup-restore&lt;/a> to take periodic backups of etcd backing Shoot clusters. It demands the bucket (or its equivalent in different object store providers) access credentials to be created and configured externally with appropriate credentials. The &lt;code>BackupEntry&lt;/code> resource takes this responsibility in Gardener to provide this information by creating a secret specific to the component.&lt;/p>
&lt;p>That being said, the core motivation for introducing this resource was to support retention of backups post deletion of &lt;code>Shoot&lt;/code>. The etcd-backup-restore components take responsibility of garbage collecting old backups out of the defined period. Once a shoot is deleted, we need to persist the backups for few days. Hence, Gardener uses the &lt;code>BackupEntry&lt;/code> resource for this housekeeping work post deletion of a &lt;code>Shoot&lt;/code>. The &lt;code>BackupEntry&lt;/code> resource is responsible for shoot specific prefix under referred bucket.&lt;/p>
&lt;p>Before introducing the &lt;code>BackupEntry&lt;/code> extension resource, Gardener was using Terraform in order to create and manage these provider-specific resources (e.g., see &lt;a href="https://github.com/gardener/gardener/tree/0.27.0/charts/seed-terraformer/charts/aws-backup">AWS Backup&lt;/a>).
Now, Gardener commissions an external, provider-specific controller to take over this task. You can also refer to &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md">backupInfra proposal documentation&lt;/a> to get idea about how the transition was done and understand the resource in broader scope.&lt;/p>
&lt;h2 id="what-is-the-lifespan-of-a-backupentry">What Is the Lifespan of a &lt;code>BackupEntry&lt;/code>?&lt;/h2>
&lt;p>The bucket associated with &lt;code>BackupEntry&lt;/code> will be created by using a &lt;code>BackupBucket&lt;/code> resource. The &lt;code>BackupEntry&lt;/code> resource will be created as a part of the &lt;code>Shoot&lt;/code> creation. But resources might continue to exist post deletion of a &lt;code>Shoot&lt;/code> (see &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/#backupentry-controller">gardenlet&lt;/a> for more details).&lt;/p>
&lt;h2 id="what-needs-to-be-implemented-to-support-a-new-infrastructure-provider">What Needs to be Implemented to Support a New Infrastructure Provider?&lt;/h2>
&lt;p>As part of the shoot flow, Gardener will create a special CRD in the seed cluster that needs to be reconciled by an extension controller, for example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: BackupEntry
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: shoot--foo--bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: azure
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;some-optional-provider-specific-backup-bucket-configuration&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backupBucketProviderStatus:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;some-optional-provider-specific-backup-bucket-status&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> region: eu-west-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> bucketName: foo
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: backupprovider
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: shoot--foo--bar
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>.spec.secretRef&lt;/code> contains a reference to the provider secret pointing to the account that shall be used to create the needed resources. This provider secret will be propagated from the &lt;code>BackupBucket&lt;/code> resource by the shoot controller.&lt;/p>
&lt;p>Your controller is supposed to create the &lt;code>etcd-backup&lt;/code> secret in the control plane namespace of a shoot. This secret is supposed to be used by Gardener or eventually by the etcd-backup-restore component to backup the etcd. The controller implementation should clean up the objects created under the shoot specific prefix in the bucket equivalent to the name of the &lt;code>BackupEntry&lt;/code> resource.&lt;/p>
&lt;p>In order to support a new infrastructure provider, you need to write a controller that watches all the &lt;code>BackupBucket&lt;/code>s with &lt;code>.spec.type=&amp;lt;my-provider-name&amp;gt;&lt;/code>. You can take a look at the below referenced example implementation for the Azure provider.&lt;/p>
&lt;h2 id="references-and-additional-resources">References and Additional Resources&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/extensions/#backupbucket">&lt;code>BackupEntry&lt;/code> API Reference&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener-extension-provider-azure/tree/master/pkg/controller/backupentry">Exemplary Implementation for the Azure Provider&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/extensions/backupbucket/">&lt;code>BackupBucket&lt;/code> Resource Documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/02-backupinfra.md">Shared Bucket Proposal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener/blob/master/pkg/controllermanager/apis/config/types.go#L101-%23L107">Gardener-controller-manager-component-config API Specification&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>