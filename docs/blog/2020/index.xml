<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener – 2020</title><link>https://gardener.cloud/blog/2020/</link><description>Recent content in 2020 on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Thu, 03 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://gardener.cloud/blog/2020/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: STACKIT Kubernetes Engine with Gardener</title><link>https://gardener.cloud/blog/2020/12.03-stackit-kubernetes-engine-with-gardener/</link><pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2020/12.03-stackit-kubernetes-engine-with-gardener/</guid><description>
&lt;p>&lt;a href="https://stackit.de/en/">STACKIT&lt;/a> is a digital brand of Europe’s biggest retailer, the Schwarz Group, which consists of Lidl, Kaufland, as well as production and recycling companies. Following the industry trend, the Schwarz Group is in the process of a digital transformation. STACKIT enables this transformation by helping to modernize the internal IT of the company branches.&lt;/p>
&lt;h2 id="what-is-stackit-and-the-stackit-kubernetes-engine-ske">What is STACKIT and the STACKIT Kubernetes Engine (SKE)?&lt;/h2>
&lt;p>STACKIT started with colocation solutions for internal and external customers in Europe-based data centers, which was then expanded to a full cloud platform stack providing an IaaS layer with VMs, storage and network, as well as a PaaS layer including Cloud Foundry and a growing set of cloud services, like databases, messaging, etc.&lt;/p>
&lt;p>With containers and Kubernetes becoming the lingua franca of the cloud, we are happy to announce the &lt;em>STACKIT Kubernetes Engine (SKE)&lt;/em>, which has been released as Beta in November this year. We decided to use Gardener as the cluster management engine underneath SKE - for good reasons as you will see – and we would like to share our experiences with Gardener when working on the SKE Beta release, and serve as a testimonial for this technology.&lt;/p>
&lt;img title="Figure 1: STACKIT Component Diagram" src="https://gardener.cloud/__resources/00_9a27e4.png" style="width:90%; height:auto"/>
&lt;figcaption style="text-align:center;margin-top: 0px;margin-bottom: 30px;font-size: 90%;">Figure 1: STACKIT Component Diagram&lt;/figcaption>
&lt;h2 id="why-we-chose-gardener-as-a-cluster-management-tool">Why We Chose Gardener as a Cluster Management Tool&lt;/h2>
&lt;p>We started with the Kubernetes endeavor in the beginning of 2020 with a newly formed agile team that consisted of software engineers, highly experienced in IT operations and development. After some exploration and a short conceptual phase, we had a clear-cut opinion on how the cluster management for STACKIT should look like: we were looking for a highly customizable tool that could be adapted to the specific needs of STACKIT and the Schwarz Group, e.g. in terms of network setup or the infrastructure layer it should be running on. Moreover, the tool should be scalable to a high number of managed Kubernetes clusters and should therefore provide a fully automated operation experience. As an open source project, contributing and influencing the tool, as well as collaborating with a larger community were important aspects that motivated us. Furthermore, we aimed to offer cluster management as a self-service in combination with an excellent user experience. Our objective was to have the managed clusters come with enterprise-grade SLAs – i.e. with “batteries included”, as some say.&lt;/p>
&lt;p>With this mission, we started our quest through the world of Kubernetes and soon found Gardener to be a hot candidate of cluster management tools that seemed to fulfill our demands. We quickly got in contact and received a warm welcome from the Gardener community. As an interested potential adopter, but in the early days of the COVID-19 lockdown, we managed to organize an online workshop during which we got an introduction and deep dive into Gardener and discussed the STACKIT use cases. We learned that Gardener is extensible in many dimensions, and that contributions are always welcome and encouraged. Once we understood the basic Gardener concepts of Garden, Shoot and Seed clusters, its inception design and how this extends Kubernetes concepts in a natural way, we were eager to evaluate this tool in more detail.&lt;/p>
&lt;p>After this evaluation, we were convinced that this tool fulfilled all our requirements - a decision was made and off we went.&lt;/p>
&lt;h2 id="how-gardener-was-adapted-and-extended-by-ske">How Gardener was Adapted and Extended by SKE&lt;/h2>
&lt;p>After becoming familiar with Gardener, we started to look into its code base to adapt it to the specific needs of the STACKIT OpenStack environment. Changes and extensions were made in order to get it integrated into the STACKIT environment, and whenever reasonable, we contributed those changes back:&lt;/p>
&lt;ul>
&lt;li>To run smoothly with the STACKIT OpenStack layer, the Gardener configuration was adapted in different places, e.g. to support CSI driver or to configure the domains of a shoot API server or ingress.&lt;/li>
&lt;li>Gardener was extended to support shoots and shooted seeds in dual stack and dual home setup. This is used in SKE for the communication between shooted seeds and the Garden cluster.&lt;/li>
&lt;li>SKE uses a private image registry for the Gardener installation in order to resolve dependencies to public image registries and to have more control over the used Gardener versions. To install and run Gardener with the private image registry, some new configurations need to be introduced into Gardener.&lt;/li>
&lt;li>Gardener is a first-class API based service what allowed us to smoothly integrate it into the STACKIT User Interface. We were also able to jump-start and utilize the Gardener Dashboard for our Beta release by merely adjusting the look-&amp;amp;-feel, i.e. colors, labels and icons.&lt;/li>
&lt;/ul>
&lt;img title="Figure 2: Gardener Dashboard adapted to STACKIT UI style" src="https://gardener.cloud/__resources/01_613c49.png" style="width:90%; height:auto"/>
&lt;figcaption style="text-align:center;margin-top: 0px;margin-bottom: 30px;font-size: 90%;">Figure 2: Gardener Dashboard adapted to STACKIT UI style&lt;/figcaption>
&lt;h2 id="experience-with-gardener-operations">Experience with Gardener Operations&lt;/h2>
&lt;p>As no OpenStack installation is identical to one another, getting Gardener to run stable on the STACKIT IaaS layer revealed some operational challenges. For instance, it was challenging to find the right configuration for Cinder CSI.&lt;/p>
&lt;p>To test for its resilience, we tried to break the managed clusters with a Chaos Monkey test, e.g. by deleting services or components needed by Kubernetes and Gardener to work properly. The reconciliation feature of Gardener fixed all those problems automatically, so that damaged Shoot clusters became operational again after a short period of time. Thus, we were not able to break Shoot clusters from an end user perspective permanently, despite our efforts. Which again speaks for Gardener’s first-class cloud native design.&lt;/p>
&lt;p>We also participated in a fruitful community support: For several challenges we contacted the community channel and help was provided in a timely manner. A lesson learned was that raising an issue in the community early on, before getting stuck too long on your own with an unresolved problem, is essential and efficient.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Gardener is used by SKE to provide a managed Kubernetes offering for internal use cases of the Schwarz Group as well as for the public cloud offering of STACKIT. Thanks to Gardener, it was possible to get from zero to a Beta release in only about half a year’s time – this speaks for itself. Within this period, we were able to integrate Gardener into the STACKIT environment, i.e. in its OpenStack IaaS layer, its management tools and its identity provisioning solution.&lt;/p>
&lt;p>Gardener has become a vital building block in STACKIT&amp;rsquo;s cloud native platform offering. For the future, the possibility to manage clusters also on other infrastructures and hyperscalers is seen as another great opportunity for extended use cases. The open co-innovation exchange with the Gardener community member companies has also opened the door to commercial co-operation.&lt;/p></description></item><item><title>Blog: Gardener v1.13 Released</title><link>https://gardener.cloud/blog/2020/11.23-gardener-v1.13-released/</link><pubDate>Mon, 23 Nov 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2020/11.23-gardener-v1.13-released/</guid><description>
&lt;p>Dear community, we&amp;rsquo;re happy to announce a new minor release of Gardener, in fact, the 16th in 2020!
v1.13 came out just today after a couple of weeks of code improvements and feature implementations.
As usual, this blog post provides brief summaries for the most notable changes that we introduce with this version.
Behind the scenes (and not explicitly highlighted below) we are progressing on internal code restructurings and refactorings to ease further extensions and to enhance development productivity.
Speaking of those: You might be interested in watching &lt;a href="https://www.youtube.com/watch?v=4sQs_Hj6xpY">the recording of the last Gardener Community Meeting&lt;/a> which includes a detailed session for &lt;a href="https://github.com/gardener/terraformer/releases/tag/v2.0.0-rc.0">v2 of Terraformer&lt;/a>, a complete rewrite in Golang, and improved state handling.&lt;/p>
&lt;h2 id="notable-changes-in-v113">Notable Changes in v1.13&lt;/h2>
&lt;p>The main themes of Gardener&amp;rsquo;s v1.13 release are increments for feature gate promotions, scalability and robustness, and cleanups and refactorings.
The community plans to continue on those and wants to deliver at least one more release in 2020.&lt;/p>
&lt;h3 id="automatic-quotas-for-gardener-resources-gardenergardener3072httpsgithubcomgardenergardenerpull3072">Automatic Quotas for Gardener Resources (&lt;a href="https://github.com/gardener/gardener/pull/3072">gardener/gardener#3072&lt;/a>)&lt;/h3>
&lt;p>Gardener already supports &lt;code>ResourceQuota&lt;/code>s since the last release, however, it was still up to operators/administrators to create these objects in project namespaces.
Obviously, in large Gardener installations with thousands of projects, this is a quite challenging task.
With this release, we are shipping an improvement in the &lt;code>Project&lt;/code> controller in the gardener-controller-manager that allows operators to automatically create &lt;code>ResourceQuota&lt;/code>s based on configuration.
Operators can distinguish via project label selectors which default quotas shall be defined for various projects.
Please find more details at &lt;a href="https://github.com/gardener/gardener/blob/v1.13.0/docs/concepts/controller-manager.md#main-reconciler">Gardener Controller Manager&lt;/a>!&lt;/p>
&lt;h3 id="resource-capacity-and-reservations-for-seeds-gardenergardener3075httpsgithubcomgardenergardenerpull3075">Resource Capacity and Reservations for Seeds (&lt;a href="https://github.com/gardener/gardener/pull/3075">gardener/gardener#3075&lt;/a>)&lt;/h3>
&lt;p>The larger the Gardener landscape, the more seed clusters you require.
Naturally, they have limits of how many shoots they can accommodate (based on constraints of the underlying infrastructure provider and/or seed cluster configuration).
Until this release, there were no means to prevent a seed cluster from becoming overloaded (and potentially die due to this load).
Now you define resource capacity and reservations in the &lt;a href="https://github.com/gardener/gardener/blob/v1.13.0/example/20-componentconfig-gardenlet.yaml#L68-L70">gardenlet&amp;rsquo;s component configuration&lt;/a>, similar to how the kubelet announces allocatable resources for &lt;code>Node&lt;/code> objects.
We are &lt;a href="https://github.com/gardener/gardener/blob/v1.13.0/charts/gardener/gardenlet/values.yaml#L100-L102">defaulting this to 250 shoots&lt;/a>, but you might want to adapt this value for your own environment.&lt;/p>
&lt;h3 id="distributed-gardenlet-rollout-for-shooted-seeds-gardenergardener3135httpsgithubcomgardenergardenerpull3135">Distributed Gardenlet Rollout for Shooted Seeds (&lt;a href="https://github.com/gardener/gardener/pull/3135">gardener/gardener#3135&lt;/a>)&lt;/h3>
&lt;p>With the same motivation, i.e., to improve catering with large landscapes, we allow operators to configure distributed rollouts of gardenlets for shooted seeds.
When a new Gardener version is being deployed in landscapes with a high number of shooted seeds, gardenlets of earlier versions were immediately re-deploying copies of themselves into the shooted seeds they manage.
This leads to a large number of new gardenlet pods that all roughly start at the same time.
Depending on the size of the landscape, this may trouble the gardener-apiservers as all of them are starting to fill their caches and create watches at the same time.
By default, this rollout is now randomized &lt;a href="https://github.com/gardener/gardener/blob/v1.13.0/example/20-componentconfig-gardenlet.yaml#L63-L64">within a &lt;code>5m&lt;/code> time window&lt;/a>, i.e., it may take up to &lt;code>5m&lt;/code> until all gardenlets in all seeds have been updated.&lt;/p>
&lt;h3 id="progressing-on-beta-promotion-for-apiserversni-feature-gate-gardenergardener3082httpsgithubcomgardenergardenerpull3082-gardenergardener3143httpsgithubcomgardenergardenerpull3143">Progressing on Beta-Promotion for &lt;code>APIServerSNI&lt;/code> Feature Gate (&lt;a href="https://github.com/gardener/gardener/pull/3082">gardener/gardener#3082&lt;/a>, &lt;a href="https://github.com/gardener/gardener/pull/3143">gardener/gardener#3143&lt;/a>)&lt;/h3>
&lt;p>The alpha &lt;code>APIServerSNI&lt;/code> feature will drastically reduce the costs for load balancers in the seed clusters, thus, it is effectively contributing to Gardener&amp;rsquo;s &amp;ldquo;minimal TCO&amp;rdquo; goal.
In this release we are introducing an important improvement that optimizes the connectivity when pods talk to their control plane by avoiding an extra network hop.
This is realized by a &lt;code>MutatingWebhookConfiguration&lt;/code> whose server runs as a sidecar container in the kube-apiserver pod in the seed (only when the &lt;code>APIServerSNI&lt;/code> feature gate is enabled).
The webhook injects a &lt;code>KUBERNETES_SERVICE_HOST&lt;/code> environment variable into pods in the shoot which prevents the additional network hop to the &lt;code>apiserver-proxy&lt;/code> on all worker nodes.
You can read more about it in &lt;a href="https://github.com/gardener/gardener/blob/v1.13.0/docs/usage/apiserver-sni-injection.md">APIServerSNI environment variable injection&lt;/a>.&lt;/p>
&lt;h3 id="more-control-plane-configurability-gardenergardener3141httpsgithubcomgardenergardenerpull3141-gardenergardener3139httpsgithubcomgardenergardenerpull3139">More Control Plane Configurability (&lt;a href="https://github.com/gardener/gardener/pull/3141">gardener/gardener#3141&lt;/a>, &lt;a href="https://github.com/gardener/gardener/pull/3139">gardener/gardener#3139&lt;/a>)&lt;/h3>
&lt;p>A main capability beloved by Gardener users is its openness when it comes to configurability and fine-tuning of the Kubernetes control plane components.
Most managed Kubernetes offerings are not exposing options of the master components, but Gardener&amp;rsquo;s &lt;a href="https://github.com/gardener/gardener/blob/v1.13.0/example/90-shoot.yaml">&lt;code>Shoot&lt;/code> API&lt;/a> offers a selected set of settings.
With this release we are allowing to change the maximum number of (non-)mutating requests for the kube-apiserver of shoot clusters.
Similarly, the grace period before deleting pods on failed nodes can now be fine-grained for the kube-controller-manager.&lt;/p>
&lt;h3 id="improved-project-resource-handling-gardenergardener3137httpsgithubcomgardenergardenerpull3137-gardenergardener3136httpsgithubcomgardenergardenerpull3136-gardenergardener3179httpsgithubcomgardenergardenerpull3179">Improved &lt;code>Project&lt;/code> Resource Handling (&lt;a href="https://github.com/gardener/gardener/pull/3137">gardener/gardener#3137&lt;/a>, &lt;a href="https://github.com/gardener/gardener/pull/3136">gardener/gardener#3136&lt;/a>, &lt;a href="https://github.com/gardener/gardener/pull/3179">gardener/gardener#3179&lt;/a>)&lt;/h3>
&lt;p>&lt;code>Project&lt;/code>s are an important resource in the Gardener ecosystem as they enable collaboration with team members.
A couple of improvements have landed into this release.
Firstly, duplicates in the member list were not validated so far.
With this release, the gardener-apiserver is automatically merging them, and in future releases requests with duplicates will be denied.
Secondly, specific &lt;code>Project&lt;/code>s may now be excluded from the &lt;a href="https://github.com/gardener/gardener/blob/v1.13.0/docs/concepts/controller-manager.md#stale-projects-reconciler">stale checks&lt;/a> if desired.
Lastly, namespaces for &lt;code>Project&lt;/code>s that were adopted (i.e., those that exist before the &lt;code>Project&lt;/code> already) will now no longer be deleted when the &lt;code>Project&lt;/code> is being deleted.
Please note that this only applies for newly created &lt;code>Project&lt;/code>s.&lt;/p>
&lt;h3 id="removal-of-deprecated-labels-and-annotations-gardenergardener3094httpsgithubcomgardenergardenerpull3094">Removal of Deprecated Labels and Annotations (&lt;a href="https://github.com/gardener/gardener/pull/3094">gardener/gardener#3094&lt;/a>)&lt;/h3>
&lt;p>The &lt;code>core.gardener.cloud&lt;/code> API group succeeded the old &lt;code>garden.sapcloud.io&lt;/code> API group in the beginning of 2020, however, a lot of labels and annotations with the old API group name were still supported.
We have continued with the process of removing those deprecated (but replaced with the new API group name) names.
Concretely, the project labels &lt;code>garden.sapcloud.io/role=project&lt;/code> and &lt;code>project.garden.sapcloud.io/name=&amp;lt;project-name&amp;gt;&lt;/code> are no longer supported now.
Similarly, the &lt;code>shoot.garden.sapcloud.io/use-as-seed&lt;/code> and &lt;code>shoot.garden.sapcloud.io/ignore-alerts&lt;/code> annotations got deleted.
We are not finished yet, but we do small increments and plan to progress on the topic until we finally get rid of all artifacts with the old API group name.&lt;/p>
&lt;h3 id="nodelocaldns-network-policy-rules-adapted-gardenergardener3184httpsgithubcomgardenergardenerpull3184">&lt;code>NodeLocalDNS&lt;/code> Network Policy Rules Adapted (&lt;a href="https://github.com/gardener/gardener/pull/3184">gardener/gardener#3184&lt;/a>)&lt;/h3>
&lt;p>The alpha &lt;code>NodeLocalDNS&lt;/code> feature was already &lt;a href="https://gardener.cloud/blog/2020/08.06-gardener-v1.8.0-released/">introduced and explained with Gardener v1.8&lt;/a> with the motivation to overcome certain bottlenecks with the horizontally auto-scaled CoreDNS in all shoot clusters.
Unfortunately, due to a bug in the network policy rules, it was not working in all environments.
We have fixed this one now, so it should be ready for further tests and investigations.
Come give it a try!&lt;/p>
&lt;p>Please bear in mind that this blog post only highlights the most noticeable changes and improvements, but there is a whole bunch more, including a ton of bug fixes in older versions! Come check out the &lt;a href="https://github.com/gardener/gardener/releases/tag/v1.13.0">full release notes&lt;/a> and share your feedback in our &lt;a href="https://kubernetes.slack.com/archives/CB57N0BFG">#gardener&lt;/a> Slack channel!&lt;/p></description></item><item><title>Blog: Case Study: Migrating ETCD Volumes in Production</title><link>https://gardener.cloud/blog/2020/11.20-case-study-migrating-etcd-volumes-in-production/</link><pubDate>Fri, 20 Nov 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2020/11.20-case-study-migrating-etcd-volumes-in-production/</guid><description>
&lt;div class="alert alert-info" role="alert">
This is a guest commentary from &lt;span style="white-space:nowrap">&lt;a href="https://metal-stack.io/">metal-stack&lt;/a>&lt;/span>.&lt;br>&lt;br>metal-stack is a software that provides an API for provisioning and managing physical servers in the data center. To categorize this product, the terms &amp;ldquo;Metal-as-a-Service&amp;rdquo; (MaaS) or &amp;ldquo;bare metal cloud&amp;rdquo; are commonly used.
&lt;/div>
&lt;p>One reason that you stumbled upon this blog post could be that you saw errors like the following in your ETCD instances:&lt;/p>
&lt;pre tabindex="0">&lt;code>etcd-main-0 etcd 2020-09-03 06:00:07.556157 W | etcdserver: read-only range request &amp;#34;key:\&amp;#34;/registry/deployments/shoot--pwhhcd--devcluster2/kube-apiserver\&amp;#34; &amp;#34; with result &amp;#34;range_response_count:1 size:9566&amp;#34; took too long (13.95374909s) to execute
&lt;/code>&lt;/pre>&lt;p>As it turns out, 14 seconds are way too slow for running Kubernetes API servers. It makes them go into a crash loop (leader election fails). Even worse, this whole thing is self-amplifying: The longer a response takes, the more requests queue up, leading to response times increasing further and further. The system is very unlikely to recover. 😞&lt;/p>
&lt;p>On Github, you can easily find the reason for this problem. Most probably your disks are too slow (see &lt;a href="https://github.com/etcd-io/etcd/issues/10860">etcd-io/etcd#10860&lt;/a>). So, when you are (like in our case) on GKE and run your ETCD on their default persistent volumes, consider moving from standard disks to SSDs and the error messages should disappear. A guide on how to use SSD volumes on GKE can be found at &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/ssd-pd">Using SSD persistent disks&lt;/a>.&lt;/p>
&lt;p>Case closed? Well. For some people it might be. But when you are seeing this in your Gardener infrastructure, it&amp;rsquo;s likely that there is something going wrong. The entire ETCD management is fully managed by Gardener, which makes the problem a bit more interesting to look at. This blog post strives to cover topics such as:&lt;/p>
&lt;ul>
&lt;li>Gardener operating principles&lt;/li>
&lt;li>Gardener architecture and ETCD management&lt;/li>
&lt;li>Pitfalls with multi-cloud environments&lt;/li>
&lt;li>Migrating GCP volumes to a new storage class&lt;/li>
&lt;/ul>
&lt;p>We from metal-stack learned quite a lot about the capabilities of Gardener through this problem. We are happy to share this experience with a broader audience. Gardener adopters and operators read on.&lt;/p>
&lt;h2 id="how-gardener-manages-etcds">How Gardener Manages ETCDs&lt;/h2>
&lt;p>In our infrastructure, we use Gardener to provision Kubernetes clusters on bare metal machines in our own data centers using &lt;span style="white-space:nowrap">&lt;a href="https://metal-stack.io/">metal-stack&lt;/a>&lt;/span>. Even if the entire stack could be running on-premise, our initial &lt;a href="https://gardener.cloud/docs/gardener/concepts/apiserver/#seeds">seed cluster&lt;/a> and the &lt;a href="https://docs.metal-stack.io/stable/overview/architecture/#Metal-Control-Plane">metal control plane&lt;/a> are hosted on GKE. This way, we do not need to manage a single Kubernetes cluster in our entire landscape manually. As soon as we have Gardener deployed on this initial cluster, we can spin up further Seeds in our own data centers through the concept of &lt;a href="https://gardener.cloud/docs/gardener/operations/managed_seed/">ManagedSeeds&lt;/a>.&lt;/p>
&lt;p>To make this easier to understand, let us give you a simplified picture of how our Gardener production setup looks like:&lt;/p>
&lt;img title="Production Setup" src="https://gardener.cloud/__resources/01-001_364b2e.svg" style="width:80vw; height:auto">
&lt;figcaption style="text-align:center;margin-top: -25px;margin-bottom: 30px;font-size: 90%;">Figure 1: Simplified View on Our Production Setup&lt;/figcaption>
&lt;p>For every &lt;a href="https://gardener.cloud/docs/gardener/concepts/apiserver/#shoots">shoot cluster&lt;/a>, Gardener deploys an individual, standalone ETCD as a stateful set into a &lt;em>shoot namespace&lt;/em>. The deployment of the ETCD stateful set is managed by a controller called &lt;a href="https://github.com/gardener/etcd-druid">etcd-druid&lt;/a>, which reconciles a special resource of the kind &lt;code>etcds.druid.gardener.cloud&lt;/code>. This &lt;code>Etcd&lt;/code> resource is getting deployed during the shoot provisioning flow in the &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/">gardenlet&lt;/a>.&lt;/p>
&lt;p>For failure-safety, the etcd-druid deploys the official ETCD container image along with a sidecar project called &lt;a href="https://github.com/gardener/etcd-backup-restore">etcd-backup-restore&lt;/a>. The sidecar automatically takes backups of the ETCD and stores them at a cloud provider, e.g. in S3 Buckets, Google Buckets, or similar. In case the ETCD comes up without or with corrupted data, the sidecar looks into the backup buckets and automatically restores the latest backup before ETCD starts up. This entire approach basically takes away the pain for operators to manually have to restore data in the event of data loss.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
We found the etcd-backup-restore project very intriguing. It was the inspiration for us to come up with a similar sidecar for the databases we use with metal-stack. This project is called &lt;a href="https://github.com/metal-stack/backup-restore-sidecar">backup-restore-sidecar&lt;/a>. We can cope with postgres and rethinkdb database at the moment and more to come. Feel free to check it out when you are interested.
&lt;/div>
&lt;p>As it&amp;rsquo;s the nature for multi-cloud applications to act upon a variety of cloud providers, with a single installation of Gardener, it is easily possible to spin up new Kubernetes clusters not only on GCP, but on other supported cloud platforms, too.&lt;/p>
&lt;p>When the Gardenlet deploys a resource like the &lt;code>Etcd&lt;/code> resource into a shoot namespace, a provider-specific extension-controller has the chance to manipulate it through a &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook">mutating webhook&lt;/a>. This way, a cloud provider can adjust the generic Gardener resource to fit the provider-specific needs. For every cloud that Gardener supports, there is such an extension-controller. For metal-stack, we also maintain one, called &lt;a href="https://github.com/metal-stack/gardener-extension-provider-metal">gardener-extension-provider-metal&lt;/a>.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
A side note for cloud providers: Meanwhile, new cloud providers can be added &lt;em>fully&lt;/em> out-of-tree, i.e. without touching any of Gardener&amp;rsquo;s sources. This works through &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">API extensions&lt;/a> and &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">CRDs&lt;/a>. Gardener handles generic resources and backpacks provider-specific configuration through raw extensions. When you are a cloud provider on your own, this is really encouraging because you can integrate with Gardener without any burdens. You can find documentation on how to integrate your cloud into Gardener at &lt;a href="https://gardener.cloud/docs/gardener/development/new-cloud-provider/">Adding Cloud Providers&lt;/a> and &lt;a href="https://gardener.cloud/docs/gardener/extensions/overview/">Extensibility Overview&lt;/a>.
&lt;/div>
&lt;h2 id="the-mistake-is-in-the-deployment">The Mistake Is in the Deployment&lt;/h2>
&lt;div class="alert alert-info" role="alert">
This section contains code examples from Gardener v1.8.
&lt;/div>
&lt;p>Now that we know how the ETCDs are managed by Gardener, we can come back to the original problem from the beginning of this article. It turned out that the real problem was a misconfiguration in our deployment. Gardener actually &lt;em>does&lt;/em> use SSD-backed storage on GCP for ETCDs by default. During reconciliation, the &lt;a href="https://github.com/gardener/gardener-extension-provider-gcp">gardener-extension-controller-gcp&lt;/a> deploys a storage class called &lt;code>gardener.cloud-fast&lt;/code> that enables accessing SSDs on GCP.&lt;/p>
&lt;p>But for some reason, in our cluster we did not find such a storage class. And even more interesting, we did not use the &lt;code>gardener-extension-provider-gcp&lt;/code> for any shoot reconciliation, only for ETCD backup purposes. And that was the big mistake we made: We reconciled the shoot control plane completely with &lt;code>gardener-extension-provider-metal&lt;/code> even though our initial &lt;code>Seed&lt;/code> actually runs on GKE and specific parts of the shoot control plane should be reconciled by the GCP extension-controller instead!&lt;/p>
&lt;p>This is how the initial &lt;code>Seed&lt;/code> resource looked like:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: core.gardener.cloud/v1beta1
kind: Seed
metadata:
name: initial-seed
spec:
...
provider:
region: gke
type: metal
...
...
&lt;/code>&lt;/pre>&lt;p>Surprisingly, this configuration was working pretty well for a long time. The initial seed properly produced the Kubernetes control planes of our managed seeds that looked like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl get controlplanes.extensions.gardener.cloud
NAME TYPE PURPOSE STATUS AGE
fra-equ01 metal Succeeded 85d
fra-equ01-exposure metal exposure Succeeded 85d
&lt;/code>&lt;/pre>&lt;p>And this is another interesting observation: There are two &lt;code>ControlPlane&lt;/code> resources. One regular resource and one with an &lt;code>exposure&lt;/code> purpose. Gardener distinguishes between two types for this exact reason: Environments where the shoot control plane runs on a different cloud provider than the Kubernetes worker nodes. The regular &lt;code>ControlPlane&lt;/code> resource gets reconciled by the provider configured in the &lt;code>Shoot&lt;/code> resource, and the &lt;code>exposure&lt;/code> type &lt;code>ControlPlane&lt;/code> by the provider configured in the &lt;code>Seed&lt;/code> resource.&lt;/p>
&lt;p>With the existing configuration the &lt;code>gardener-extension-provider-gcp&lt;/code> does not kick in and hence, it neither deploys the &lt;code>gardener.cloud-fast&lt;/code> storage class nor does it mutate the &lt;code>Etcd&lt;/code> resource to point to it. And in the end, we are left with ETCD volumes using the default storage class (which is what we do for ETCD stateful sets in the metal-stack seeds, because our default storage class uses &lt;a href="https://github.com/metal-stack/csi-lvm">csi-lvm&lt;/a> that writes into logical volumes on the SSD disks in our physical servers).&lt;/p>
&lt;p>The correction we had to make was a one-liner: Setting the provider type of the initial &lt;code>Seed&lt;/code> resource to &lt;code>gcp&lt;/code>.&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl get seed initial-seed -o yaml
apiVersion: core.gardener.cloud/v1beta1
kind: Seed
metadata:
name: initial-seed
spec:
...
provider:
region: gke
type: gcp # &amp;lt;-- here
...
...
&lt;/code>&lt;/pre>&lt;p>This change moved over the control plane exposure reconciliation to the &lt;code>gardener-extension-provider-gcp&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl get -n &amp;lt;shoot-namespace&amp;gt; controlplanes.extensions.gardener.cloud
NAME TYPE PURPOSE STATUS AGE
fra-equ01 metal Succeeded 85d
fra-equ01-exposure gcp exposure Succeeded 85d
&lt;/code>&lt;/pre>&lt;p>And boom, after some time of waiting for all sorts of magic reconciliations taking place in the background, the missing storage class suddenly appeared:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl get sc
NAME PROVISIONER
gardener.cloud-fast kubernetes.io/gce-pd
standard (default) kubernetes.io/gce-pd
&lt;/code>&lt;/pre>&lt;p>Also, the &lt;code>Etcd&lt;/code> resource was now configured properly to point to the new storage class:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl get -n &amp;lt;shoot-namespace&amp;gt; etcd etcd-main -o yaml
apiVersion: druid.gardener.cloud/v1alpha1
kind: Etcd
metadata:
...
name: etcd-main
spec:
...
storageClass: gardener.cloud-fast # &amp;lt;-- was pointing to default storage class before!
volumeClaimTemplate: main-etcd
...
&lt;/code>&lt;/pre>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Only the &lt;code>etcd-main&lt;/code> storage class gets changed to &lt;code>gardener.cloud-fast&lt;/code>. The &lt;code>etcd-events&lt;/code> configuration will still point to standard disk storage because this ETCD is much less occupied as compared to the &lt;code>etcd-main&lt;/code> stateful set.
&lt;/div>
&lt;h2 id="the-migration">The Migration&lt;/h2>
&lt;p>Now that the deployment was in place such that this mistake would not repeat in the future, we still had the ETCDs running on the default storage class. The reconciliation does not delete the existing persistent volumes (PVs) on its own.&lt;/p>
&lt;p>To bring production back up quickly, we temporarily moved the ETCD pods to other nodes in the GKE cluster. These were nodes which were less occupied, such that the disk throughput was a little higher than before. But surely that was not a final solution.&lt;/p>
&lt;p>For a proper solution we had to move the ETCD data out of the standard disk PV into a SSD-based PV.&lt;/p>
&lt;p>Even though we had the etcd-backup-restore sidecar, we did not want to fully rely on the restore mechanism to do the migration. The backup should only be there for emergency situations when something goes wrong. Thus, we came up with another approach to introduce the SSD volume: GCP disk snapshots. This is how we did the migration:&lt;/p>
&lt;ol>
&lt;li>Scale down etcd-druid to zero in order to prevent it from disturbing your migration&lt;/li>
&lt;li>Scale down the kube-apiservers deployment to zero, then wait for the ETCD stateful to take another clean snapshot&lt;/li>
&lt;li>Scale down the ETCD stateful set to zero as well&lt;/li>
&lt;li>(in order to prevent Gardener from trying to bring up the downscaled resources, we used small shell constructs like &lt;code>while true; do kubectl scale deploy etcd-druid --replicas 0 -n garden; sleep 1; done&lt;/code>)&lt;/li>
&lt;li>Take a drive snapshot in GCP from the volume that is referenced by the ETCD PVC&lt;/li>
&lt;li>Create a new disk in GCP from the snapshot on a SSD disk&lt;/li>
&lt;li>Delete the existing PVC and PV of the ETCD (oops, data is now gone!)&lt;/li>
&lt;li>Manually deploy a PV into your Kubernetes cluster that references this new SSD disk&lt;/li>
&lt;li>Manually deploy a PVC with the name of the original PVC and let it reference the PV that you have just created&lt;/li>
&lt;li>Scale up the ETCD stateful set and check that ETCD is running properly&lt;/li>
&lt;li>(if something went terribly wrong, you still have the backup from the etcd-backup-restore sidecar, delete the PVC and PV again and let the sidecar bring up ETCD instead)&lt;/li>
&lt;li>Scale up the kube-apiserver deployment again&lt;/li>
&lt;li>Scale up etcd-druid again&lt;/li>
&lt;li>(stop your shell hacks ;D)&lt;/li>
&lt;/ol>
&lt;p>This approach worked very well for us and we were able to fix our production deployment issue. And what happened: We have never seen any crashing kube-apiservers again. 🎉&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>As bad as problems in production are, they are the best way for learning from your mistakes. For new users of Gardener it can be pretty overwhelming to understand the rich configuration possibilities that Gardener brings. However, once you get a hang of how Gardener works, the application offers an exceptional versatility that makes it very much suitable for production use-cases like ours.&lt;/p>
&lt;p>This example has shown how Gardener:&lt;/p>
&lt;ul>
&lt;li>Can handle arbitrary layers of infrastructure hosted by different cloud providers.&lt;/li>
&lt;li>Allows provider-specific tweaks to gain ideal performance for every cloud you want to support.&lt;/li>
&lt;li>Leverages Kubernetes core principles across the entire project architecture, making it vastly extensible and resilient.&lt;/li>
&lt;li>Brings useful disaster recovery mechanisms to your infrastructure (e.g. with etcd-backup-restore).&lt;/li>
&lt;/ul>
&lt;p>We hope that you could take away something new through this blog post. With this article we also want to thank the SAP Gardener team for helping us to integrate Gardener with metal-stack. It&amp;rsquo;s been a great experience so far. 😄 😍&lt;/p></description></item><item><title>Blog: Gardener v1.11 and v1.12 Released</title><link>https://gardener.cloud/blog/2020/11.04-gardener-v1.11-and-v1.12-released/</link><pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2020/11.04-gardener-v1.11-and-v1.12-released/</guid><description>
&lt;p>Two months after our last Gardener release update, we are happy again to present release v1.11 and v1.12 in this blog post. Control plane migration, load balancer consolidation, and new security features are just a few topics we progressed with. As always, a detailed list of features, improvements, and bug fixes can be found in the &lt;a href="https://github.com/gardener/gardener/releases">release notes&lt;/a> of each release. If you are going to update from a previous Gardener version, please take the time to go through the action items in the release notes.&lt;/p>
&lt;h2 id="notable-changes-in-v112">Notable Changes in v1.12&lt;/h2>
&lt;p>Release v1.12, fresh from the oven, is shipped with plenty of improvements, features, and some API changes we want to pick up in the next sections.&lt;/p>
&lt;h3 id="drop-functionless-dns-providers-gardenergardener3036httpsgithubcomgardenergardenerpull3036">Drop Functionless DNS Providers (&lt;a href="https://github.com/gardener/gardener/pull/3036">gardener/gardener#3036&lt;/a>)&lt;/h3>
&lt;p>This release drops the support for the so-called functionless DNS providers. Those are providers in a shoot’s specification (&lt;code>.spec.dns.providers&lt;/code>) which don’t serve the shoot’s domain (&lt;code>.spec.dns.domain&lt;/code>), but are created by Gardener in the seed cluster to serve DNS requests coming from the shoot cluster. If such providers don’t specify a &lt;code>type&lt;/code> or &lt;code>secretName&lt;/code>, the creation or update request for the corresponding shoot is denied.&lt;/p>
&lt;h3 id="seed-taints-gardenergardener2955httpsgithubcomgardenergardenerpull2955">Seed Taints (&lt;a href="https://github.com/gardener/gardener/pull/2955">gardener/gardener#2955&lt;/a>)&lt;/h3>
&lt;p>In an earlier release, we reserved a dedicated section in &lt;code>seed.spec.settings&lt;/code> as a replacement for &lt;code>disable-capacity-reservation, disable-dns, invisible&lt;/code> taints. These already deprecated taints were still considered and synced, which gave operators enough time to switch their integration to the new &lt;code>settings&lt;/code> field. As of version v1.12, support for them has been discontinued and they are automatically removed from seed objects. You may use the actual taint names in a future release of Gardener again.&lt;/p>
&lt;h3 id="load-balancer-events-during-shoot-reconciliation-gardenergardener3028httpsgithubcomgardenergardenerpull3028">Load Balancer Events During Shoot Reconciliation (&lt;a href="https://github.com/gardener/gardener/pull/3028">gardener/gardener#3028&lt;/a>)&lt;/h3>
&lt;p>As Gardener is capable of managing thousands of clusters, it is crucial to keep operation efforts at a minimum. This release demonstrates this endeavor by further improving error reporting to the end user. During a shoot’s reconciliation, Gardener creates &lt;code>Services&lt;/code> of type &lt;code>LoadBalancer&lt;/code> in the shoot cluster, e.g. for VPN or Nginx-Ingress addon, and waits for a successful creation. However, in the past we experienced that occurring issues caused by the party creating the load balancer (typically &lt;a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/">Cloud-Controller-Manager&lt;/a>) are only exposed in the logs or as events. Gardener now fetches these event messages and propagates them to the shoot status in case of a failure. Users can then often fix the problem themselves, if for example the failure discloses an exhausted quota on the cloud provider.&lt;/p>
&lt;h3 id="konnectivitytunnel-feature-per-shootgardenergardener3007httpsgithubcomgardenergardenerpull3007">KonnectivityTunnel Feature per Shoot(&lt;a href="https://github.com/gardener/gardener/pull/3007">gardener/gardener#3007&lt;/a>)&lt;/h3>
&lt;p>Since release &lt;code>v1.6&lt;/code>, Gardener has been capable of &lt;a href="https://gardener.cloud/docs/gardener/usage/reversed-vpn-tunnel/">reversing the tunnel direction&lt;/a> from the seed to the shoot via the &lt;code>KonnectivityTunnel&lt;/code> feature gate. With this release we make it possible to control the feature per shoot. We recommend to selectively enable the &lt;code>KonnectivityTunnel&lt;/code>, as it is still in &lt;code>alpha&lt;/code> state.&lt;/p>
&lt;h3 id="reference-protection-gardenergardener2771httpsgithubcomgardenergardenerpull2771-gardenergardener-1708419httpsgithubcomgardenergardenercommit17084191c752c206537b9506b54828f4d723d9b7">Reference Protection (&lt;a href="https://github.com/gardener/gardener/pull/2771">gardener/gardener#2771&lt;/a>, &lt;a href="https://github.com/gardener/gardener/commit/17084191c752c206537b9506b54828f4d723d9b7">gardener/gardener 1708419&lt;/a>)&lt;/h3>
&lt;p>Shoot clusters may refer to external objects, like &lt;code>Secrets&lt;/code> for specified DNS providers or they have a reference to an audit policy &lt;code>ConfigMap&lt;/code>. Deleting those objects while any shoot still references them causes server errors, often only recoverable by an immense amount of manual operations effort. To prevent such scenarios, Gardener now adds a new finalizer &lt;code>gardener.cloud/reference-protection&lt;/code> to these objects and removes it as soon as the object itself becomes releasable. Due to compatibility reasons, we decided that the handling for the audit policy &lt;code>ConfigMaps&lt;/code> is delivered as an opt-in feature first, so please familiarize yourself with the necessary settings in the Gardener Controller Manager &lt;a href="https://github.com/gardener/gardener/blob/3db1c41726dc5f669e015f294b690d330b55bbf1/example/20-componentconfig-gardener-controller-manager.yaml#L28">component config&lt;/a> if you already plan to enable it.&lt;/p>
&lt;h3 id="support-for-resource-quotas-gardenergardener2627httpsgithubcomgardenergardenerpull2627">Support for Resource Quotas (&lt;a href="https://github.com/gardener/gardener/pull/2627">gardener/gardener#2627&lt;/a>)&lt;/h3>
&lt;p>After the Kubernetes upstream change (&lt;a href="https://github.com/kubernetes/kubernetes/pull/93537">kubernetes/kubernetes#93537&lt;/a>) for externalizing the backing admission plugin has been accepted, we are happy to announce the support of &lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">ResourceQuotas&lt;/a> for Gardener offered resource kinds. &lt;a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">ResourceQuotas&lt;/a> allow you to specify a maximum number of objects per namespace, especially for end-user objects like &lt;code>Shoots&lt;/code> or &lt;code>SecretBindings&lt;/code> in a project namespace. Even though the admission plugin is enabled by default in the Gardener API Server, make sure the Kube Controller Manager runs the &lt;code>resourcequota&lt;/code> controller as well.&lt;/p>
&lt;h3 id="watch-out-developers-terraformer-v2-is-coming-gardenergardener3034httpsgithubcomgardenergardenerpull3034">Watch Out Developers, Terraformer v2 is Coming! (&lt;a href="https://github.com/gardener/gardener/pull/3034">gardener/gardener#3034&lt;/a>)&lt;/h3>
&lt;p>Although not related only to Gardener core, the preparation towards &lt;a href="https://github.com/gardener/terraformer/pull/48">Terraformer v2&lt;/a> in the &lt;a href="https://github.com/gardener/gardener/tree/master/extensions">extensions library&lt;/a> is still an important milestone to mention. With Terraformer v2, Gardener extensions using Terraform scripts will benefit from great consistency improvements. Please check out &lt;a href="https://github.com/gardener/gardener/pull/3034">PR #3034&lt;/a>), which demonstrates necessary steps to transition to Terraformer v2 as soon as it’s released.&lt;/p>
&lt;h2 id="notable-changes-in-v111">Notable Changes in v1.11&lt;/h2>
&lt;p>The Gardener community worked eagerly to deliver plenty of improvements with version v1.11. Those help us to further progress with topics like &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md">control plane migration&lt;/a>, which is actively being worked on, or to harden our load balancer consolidation (&lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md">APIServerSNI&lt;/a>) feature.
Besides improvements and fixes (full list available in release notes), this release contains major features as well, and we don’t want to miss a chance to walk you through them.&lt;/p>
&lt;h3 id="gardener-admission-controller-gardenergardener2832httpsgithubcomgardenergardenerpull2832-gardenergardener2781httpsgithubcomgardenergardenerpull2781">Gardener Admission Controller (&lt;a href="https://github.com/gardener/gardener/pull/2832">gardener/gardener#2832&lt;/a>), (&lt;a href="https://github.com/gardener/gardener/pull/2781">gardener/gardener#2781&lt;/a>)&lt;/h3>
&lt;p>In this release, all admission related HTTP handlers moved from the Gardener Controller Manager (GCM) to the new component &lt;a href="https://gardener.cloud/docs/gardener/concepts/admission-controller/">Gardener Admission Controller&lt;/a>. The admission controller is rather a small component as opposed to GCM with regards to memory footprint and CPU consumption, and thus allows you to run multiple replicas of it much cheaper than it was before. We certainly recommend specifying the admission controller deployment with more than one replica, since it reduces the odds of a system-wide outage and increases the performance of your Gardener service.&lt;/p>
&lt;p>Besides the already known &lt;code>Namespace&lt;/code> and Kubeconfig &lt;code>Secret&lt;/code> validation, a new admission handler &lt;code>Resource-Size-Validator&lt;/code> was added to the admission controller. It allows operators to restrict the size for all kinds of Kubernetes objects, especially sent by end-users to the Kubernetes or Gardener API Server. We address a security concern with this feature to prevent denial of service attacks in which an attacker artificially increases the size of objects to exhaust your object store, API server caches, or to let Gardener and Kubernetes controllers run out-of-memory. The &lt;a href="https://gardener.cloud/docs/gardener/concepts/admission-controller/#resource-size-validator">documentation&lt;/a> reveals an approach of finding the right resource size for your setup and why you should create exceptions for technical users and operators.&lt;/p>
&lt;h3 id="deferring-shoot-progress-reporting-gardenergardener2909httpsgithubcomgardenergardenerpull2909">Deferring Shoot Progress Reporting (&lt;a href="https://github.com/gardener/gardener/pull/2909">gardener/gardener#2909&lt;/a>),&lt;/h3>
&lt;p>Shoot progress reporting is the continuous update process of a shoot’s &lt;code>.status.lastOperation&lt;/code> field while the shoot is being reconciled by Gardener. Many steps are involved during reconciliation and depending on the size of your setup, the updates might become an issue for the Gardener API Server, which will refrain from processing further requests for a certain period.
With &lt;code>.controllers.shoot.progressReportPeriod&lt;/code> in Gardenlet’s component configuration, you can now delay these updates for the specified period.&lt;/p>
&lt;h3 id="new-policy-for-controller-registrations-gardenergardener2896httpsgithubcomgardenergardenerpull2896">New Policy for Controller Registrations (&lt;a href="https://github.com/gardener/gardener/pull/2896">gardener/gardener#2896&lt;/a>),&lt;/h3>
&lt;p>A while ago, we added support for different policies in &lt;code>ControllerRegistrations&lt;/code> which determine under which circumstances the deployments of registration controllers happen in affected seed clusters. If you specify the new policy &lt;code>AlwaysExceptNoShoots&lt;/code>, the respective extension controller will be deployed to all seed cluster hosting at least one shoot cluster. After all shoot clusters from a seed are gone, the extension deployment will be deleted again.
A full list of supported policies can be found at &lt;a href="https://gardener.cloud/docs/gardener/extensions/controllerregistration/#deployment-configuration-options">Registering Extension Controllers&lt;/a>.&lt;/p></description></item><item><title>Blog: Gardener Integrates with KubeVirt</title><link>https://gardener.cloud/blog/2020/10.19-gardener-integrates-with-kubevirt/</link><pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2020/10.19-gardener-integrates-with-kubevirt/</guid><description>
&lt;p>The Gardener team is happy to announce that &lt;a href="https://gardener.cloud">Gardener&lt;/a> now offers support for an additional, often requested, infrastructure/virtualization technology, namely &lt;a href="https://kubevirt.io/">KubeVirt&lt;/a>! Gardener can now provide &lt;a href="https://github.com/cncf/k8s-conformance">Kubernetes-conformant&lt;/a> clusters using KubeVirt managed Virtual Machines in the environment of your choice. This integration has been tested and works with any qualified Kubernetes (provider) cluster that is compatibly configured to host the required KubeVirt components, in particular for example &lt;a href="https://www.openshift.com/blog/openshift-virtualization-containers-kvm-and-your-vms">Red Hat OpenShift Virtualization&lt;/a>.&lt;/p>
&lt;p>Gardener enables Kubernetes consumers to centralize and operate efficiently homogenous Kubernetes clusters across different IaaS providers and even private environments. This way the same cloud-based application version can be hosted and operated by its vendor or consumer on a variety of infrastructures. When a new customer or your development team demands for a new infrastructure provider, Gardener helps you to quickly and easily on-board your workload. Furthermore, on this new infrastructure, Gardener keeps the seamless Kubernetes management experience for your Kubernetes operators, while upholding the consistency of the CI/CD pipeline of your software development team.&lt;/p>
&lt;h2 id="architecture-and-workflow">Architecture and Workflow&lt;/h2>
&lt;p>Gardener is based on the idea of three types of clusters – &lt;em>Garden cluster&lt;/em>, &lt;em>Seed cluster&lt;/em> and &lt;em>Shoot cluster&lt;/em> (see &lt;strong>Figure 1&lt;/strong>). The Garden cluster is used to control the entire Kubernetes environment centrally in a highly scalable design. The highly available seed clusters are used to host the end users (shoot) clusters’ control planes. Finally, the shoot clusters consist only of worker nodes to host the cloud native applications.&lt;/p>
&lt;img title="Gardener Architecture" src="https://gardener.cloud/__resources/00-001_7d61c5.png" style="width:90%; height:auto">
&lt;figcaption style="text-align:center;margin-top: -25px;margin-bottom: 30px;font-size: 90%;">Figure 1: Gardener Architecture&lt;/figcaption>
&lt;p>An integration of the Gardener open source project with a new cloud provider follows a standard &lt;a href="https://gardener.cloud/docs/gardener/extensions/overview/">Gardener extensibility&lt;/a> approach. The integration requires two new components: a &lt;a href="https://gardener.cloud/docs/gardener/extensions/overview/">provider extension&lt;/a> and a &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/development/cp_support_new/">Machine Controller Manager (MCM) extension&lt;/a>. Both components together enable Gardener to instruct the new cloud provider. They run in the Gardener seed clusters that host the control planes of the shoots based on that cloud provider. The role of the provider extension is to manage the provider-specific aspects of the shoot clusters’ lifecycle, including infrastructure, control plane, worker nodes, and others. It works in cooperation with the MCM extension, which in particular is responsible to handle machines that are provisioned as worker nodes for the shoot clusters. To get this job done, the MCM extension leverages the VM management/API capabilities available with the respective cloud provider.&lt;/p>
&lt;p>Setting up a Kubernetes cluster always involves a flow of interdependent steps (see &lt;strong>Figure 2&lt;/strong>), beginning with the generation of certificates and preparation of the infrastructure, continuing with the provisioning of the control plane and the worker nodes, and ending with the deployment of system components. Gardener can be configured to utilize the KubeVirt extensions in its generic workflow at the right extension points, and deliver the desired outcome of a KubeVirt backed cluster.&lt;/p>
&lt;img title="Gardener Architecture" src="https://gardener.cloud/__resources/00-002_989581.png" style="width:90%; height:auto">
&lt;figcaption style="text-align:center;margin-top: -25px;margin-bottom: 30px;font-size: 90%;">Figure 2: Generic cluster reconciliation flow with extension points&lt;/figcaption>
&lt;h3 id="gardener-integration-with-kubevirt-in-detail">Gardener Integration with KubeVirt in Detail&lt;/h3>
&lt;p>Integration with KubeVirt follows the Gardener extensibility concept and introduces the two new components mentioned above: the &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt">KubeVirt Provider Extension&lt;/a> and the &lt;a href="https://github.com/gardener/machine-controller-manager-provider-kubevirt">KubeVirt Machine Controller Manager (MCM) Extension&lt;/a>.&lt;/p>
&lt;img title="Gardener integration with KubeVirt" src="https://gardener.cloud/__resources/00-003_24951c.png" style="width:80%; height:auto">
&lt;figcaption style="text-align:center;margin-top: -25px;margin-bottom: 30px;font-size: 90%;">Figure 3: Gardener integration with KubeVirt&lt;/figcaption>
&lt;p>The KubeVirt Provider Extension consists of three separate controllers that handle respectively the infrastructure, the control plane, and the worker nodes of the shoot cluster.&lt;/p>
&lt;p>The &lt;strong>Infrastructure Controller&lt;/strong> configures the network communication between the shoot worker nodes. By default, shoot worker nodes only use the provider cluster’s pod network. To achieve higher level of network isolation and better performance, it is possible to add more networks and replace the default pod network with a different network using container network interface (CNI) plugins available in the provider cluster. This is currently based on &lt;a href="https://github.com/intel/multus-cni/blob/master/README.md">Multus CNI&lt;/a> and &lt;a href="https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/quickstart.md">NetworkAttachmentDefinitions&lt;/a>.&lt;/p>
&lt;p>Example infrastructure configuration in a shoot definition:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: kubevirt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> infrastructureConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: InfrastructureConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networks:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tenantNetworks:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: network-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config: |&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;cniVersion&amp;#34;: &amp;#34;0.4.0&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;name&amp;#34;: &amp;#34;bridge-firewall&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;plugins&amp;#34;: [
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;type&amp;#34;: &amp;#34;bridge&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;isGateway&amp;#34;: true,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;isDefaultGateway&amp;#34;: true,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;ipMasq&amp;#34;: true,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;ipam&amp;#34;: {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;type&amp;#34;: &amp;#34;host-local&amp;#34;,
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;subnet&amp;#34;: &amp;#34;10.100.0.0/16&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> },
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &amp;#34;type&amp;#34;: &amp;#34;firewall&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> }
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> ]
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> }&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> default: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;strong>Control Plane Controller&lt;/strong> deploys a &lt;em>Cloud Controller Manager (CCM)&lt;/em>. This is a Kubernetes control plane component that embeds cloud-specific control logic. As any other CCM, it runs the Node controller that is responsible for initializing Node objects, annotating and labeling them with cloud-specific information, obtaining the node’s hostname and IP addresses, and verifying the node’s health. It also runs the Service controller that is responsible for setting up load balancers and other infrastructure components for Service resources that require them.&lt;/p>
&lt;p>Finally, the &lt;strong>Worker Controller&lt;/strong> is responsible for managing the worker nodes of the Gardener shoot clusters.&lt;/p>
&lt;p>Example worker configuration in a shoot definition:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: kubevirt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: cpu-worker
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> minimum: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maximum: 2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machine:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: standard-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> image:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: ubuntu
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: &lt;span style="color:#a31515">&amp;#34;18.04&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volume:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> size: 20Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> zones:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - europe-west1-c
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For more information about configuring the KubeVirt Provider Extension as an end-user, see &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/usage-as-end-user.md">Using the KubeVirt provider extension with Gardener as end-user&lt;/a>.&lt;/p>
&lt;h3 id="enabling-your-gardener-setup-to-leverage-a-kubevirt-compatible-environment">Enabling Your Gardener Setup to Leverage a KubeVirt Compatible Environment&lt;/h3>
&lt;p>The very first step required is to define the machine types (VM types) for VMs that will be available. This is achieved via the &lt;code>CloudProfile&lt;/code> custom resource. The machine types configuration includes details such as CPU, GPU, memory, OS image, and more.&lt;/p>
&lt;p>Example &lt;code>CloudProfile&lt;/code> custom resource:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: CloudProfile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: kubevirt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: kubevirt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: CloudProfileConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineImages:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: ubuntu
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: &lt;span style="color:#a31515">&amp;#34;18.04&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sourceURL: &lt;span style="color:#a31515">&amp;#34;https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: &lt;span style="color:#a31515">&amp;#34;1.18.5&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineImages:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: ubuntu
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: &lt;span style="color:#a31515">&amp;#34;18.04&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineTypes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: standard-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpu: &lt;span style="color:#a31515">&amp;#34;1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> gpu: &lt;span style="color:#a31515">&amp;#34;0&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memory: 4Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumeTypes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> class: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> regions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: europe-west1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> zones:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: europe-west1-b
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: europe-west1-c
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: europe-west1-d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once a machine type is defined, it can be referenced in shoot definitions. This information is used by the KubeVirt Provider Extension to generate &lt;code>MachineDeployment&lt;/code> and &lt;code>MachineClass&lt;/code> custom resources required by the KubeVirt MCM extension for managing the worker nodes of the shoot clusters during the reconciliation process.&lt;/p>
&lt;p>For more information about configuring the KubeVirt Provider Extension as an operator, see &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/usage-as-operator.md">Using the KubeVirt provider extension with Gardener as operator&lt;/a>.&lt;/p>
&lt;h3 id="kubevirt-machine-controller-manager-mcm-extension">KubeVirt Machine Controller Manager (MCM) Extension&lt;/h3>
&lt;p>The &lt;a href="https://github.com/gardener/machine-controller-manager-provider-kubevirt">KubeVirt MCM Extension&lt;/a> is responsible for managing the VMs that are used as worker nodes of the Gardener shoot clusters using the virtualization capabilities of KubeVirt. This extension handles all necessary lifecycle management activities, such as machines creation, fetching, updating, listing, and deletion.&lt;/p>
&lt;p>The KubeVirt MCM Extension implements the Gardener’s common &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/driver/driver.go">driver interface&lt;/a> for managing VMs in different cloud providers. As already mentioned, the KubeVirt MCM Extension is using the &lt;code>MachineDeployments&lt;/code> and &lt;code>MachineClasses&lt;/code> – an abstraction layer that follows the Kubernetes native declarative approach - to get instructions from the KubeVirt Provider Extension about the required machines for the shoot worker nodes. Also, the cluster austoscaler integrates with the &lt;code>scale&lt;/code> subresource of the &lt;code>MachineDeployment&lt;/code> resource. This way, Gardener offers a homogeneous autoscaling experience across all supported providers.&lt;/p>
&lt;p>When a new shoot cluster is created or when a new worker node is needed for an existing shoot cluster, a new &lt;a href="https://github.com/gardener/machine-controller-manager-provider-kubevirt/blob/master/kubernetes/machine.yaml">Machine&lt;/a> will be created, and at that time, the KubeVirt MCM extension will create a new KubeVirt &lt;code>VirtualMachine&lt;/code> in the provider cluster. This &lt;code>VirtualMachine&lt;/code> will be created based on a set of configurations in the &lt;a href="https://github.com/gardener/machine-controller-manager-provider-kubevirt/blob/master/kubernetes/machine-class.yaml">MachineClass&lt;/a> that follows the &lt;a href="https://github.com/gardener/machine-controller-manager-provider-kubevirt/blob/master/pkg/kubevirt/apis/provider_spec.go">specification&lt;/a> of the KubeVirt provider.&lt;/p>
&lt;p>The KubeVirt MCM Extension has two main components. The &lt;strong>MachinePlugin&lt;/strong> is responsible for handling the machine objects, and the &lt;strong>PluginSPI&lt;/strong> is in charge of making calls to the cloud provider interface, to manage its resources.&lt;/p>
&lt;img title="KubeVirt MCM extension workflow and architecture" src="https://gardener.cloud/__resources/00-004_4235ad.png" style="width:60%; height:auto">
&lt;figcaption style="text-align:center;margin-top: -25px;margin-bottom: 30px;font-size: 90%;">Figure 4: KubeVirt MCM extension workflow and architecture&lt;/figcaption>
&lt;p>As shown in &lt;strong>Figure 4&lt;/strong>, the MachinePlugin receives a machine request from the MCM and starts its processing by decoding the request, doing partial validation, extracting the relevant information, and sending it to the PluginSPI.&lt;/p>
&lt;p>The PluginSPI then creates, gets, or deletes &lt;code>VirtualMachines&lt;/code> depending on the method called by the MachinePlugin. It extracts the kubeconfig of the provider cluster and handles all other required KubeVirt resources such as the secret that holds the &lt;code>cloud-init&lt;/code> configurations, and &lt;code>DataVolumes&lt;/code> that are mounted as disks to the VMs.&lt;/p>
&lt;h3 id="supported-environments">Supported Environments&lt;/h3>
&lt;p>The Gardener KubeVirt support is currently qualified on:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubevirt.io/2020/changelog-v0.32.0.html">KubeVirt v0.32.0&lt;/a> (and later)&lt;/li>
&lt;li>&lt;a href="https://docs.openshift.com/container-platform/4.4/welcome/index.html">Red Hat OpenShift Container Platform 4.4&lt;/a> (and later)&lt;/li>
&lt;/ul>
&lt;p>There are also plans for further improvements and new features, for example integration with CSI drivers for storage management. Details about the implementation progress can be found in the &lt;a href="https://github.com/gardener/gardener-extension-provider-kubevirt/issues">Gardener project on GitHub&lt;/a>.&lt;/p>
&lt;p>You can find further resources about the open source project Gardener at &lt;a href="https://gardener.cloud">https://gardener.cloud&lt;/a>.&lt;/p></description></item><item><title>Blog: Shoot Reconciliation Details</title><link>https://gardener.cloud/blog/2020/10.19-shoot-reconciliation-details/</link><pubDate>Mon, 19 Oct 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2020/10.19-shoot-reconciliation-details/</guid><description>
&lt;p>Do you want to understand how Gardener creates and updates Kubernetes clusters (Shoots)?
Well, it&amp;rsquo;s complicated, but if you are not afraid of large diagrams and are a visual learner like me, this might be useful to you.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In this blog post I will share a technical diagram which attempts to tie together the various components involved when Gardener creates a Kubernetes cluster.
I have created and curated the diagram, which visualizes the Shoot reconciliation flow since I started developing on Gardener.
Aside from serving as a memory aid for myself, I created it in hopes that it may potentially help contributors to understand a core piece of the complex Gardener machinery.
Please be advised that the diagram and components involved are large.
Although it can be easily divided into multiple diagrams, I want to show all the components and connections in a single diagram to create an overview of the reconciliation flow.&lt;/p>
&lt;p>The goal is to visualize the interactions of the components involved in the Shoot creation.
It is not intended to serve as a documentation of every component involved.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Taking a step back, the Gardener &lt;a href="https://gardener.cloud/docs/gardener/">README&lt;/a> states:&lt;/p>
&lt;blockquote>
&lt;p>In essence, Gardener is an &lt;a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/">extension API server&lt;/a>
that comes along with a bundle of custom controllers.
It introduces new API objects in an existing Kubernetes cluster (which is called a &lt;strong>garden&lt;/strong> cluster) in order to use them for the
management of end-user Kubernetes clusters (which are called &lt;strong>shoot&lt;/strong> clusters).
These shoot clusters are described via &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml">declarative cluster specifications&lt;/a> which are observed by the controllers.
They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.&lt;/p>
&lt;/blockquote>
&lt;p>This means that Gardener, just like any Kubernetes controller, creates Kubernetes clusters (Shoots) using a reconciliation loop.&lt;/p>
&lt;p>The &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/">Gardenlet&lt;/a> contains the controller and reconciliation loop responsible for the creation, update, deletion, and migration of Shoot clusters (there are more, but we spare them in this article).
In addition, the &lt;a href="https://gardener.cloud/docs/gardener/concepts/controller-manager/">Gardener Controller Manager&lt;/a> also reconciles Shoot resources, but only for seed-independent functionality such as Shoot hibernation, Shoot maintenance or quota control.&lt;/p>
&lt;p>This blog post is about the reconciliation loop in the Gardenlet responsible for creating and updating Shoot clusters.
The code can be found in the &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/shoot/reconciler_reconcile.go">gardener/gardener repository&lt;/a>.
The reconciliation loops of the extension controllers can be found in their individual repositories.&lt;/p>
&lt;h2 id="shoot-reconciliation-flow-diagram">Shoot Reconciliation Flow Diagram&lt;/h2>
&lt;p>When Gardner creates a Shoot cluster, there are three conceptual layers involved: the Garden cluster, the Seed cluster and the Shoot cluster.
Each layer represents a top-level section in the diagram (similar to a lane in a BPMN diagram).&lt;/p>
&lt;p>It might seem confusing that the Shoot cluster itself is a layer, because the whole flow in the first place is about creating the Shoot cluster.
I decided to introduce this separate layer to make a clear distinction between which resources exist in the Seed API server (managed by Gardener) and which in the Shoot API server (accessible by the Shoot owner).&lt;/p>
&lt;p>Each section contains several components.
Components are mostly Kubernetes resources in a Gardener installation (e.g. the gardenlet deployment in the Seed cluster).&lt;/p>
&lt;p>This is the list of components:&lt;/p>
&lt;p>&lt;strong>(Virtual) Garden Cluster&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Gardener Extension API server&lt;/li>
&lt;li>Validating Provider Webhooks&lt;/li>
&lt;li>Project Namespace&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Seed Cluster&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Gardenlet&lt;/li>
&lt;li>Seed API server
&lt;ul>
&lt;li>every Shoot Control Plane has a dedicated namespace in the Seed.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Cloud Provider (owned by Stakeholder).
&lt;ul>
&lt;li>Arguably part of the Shoot cluster but used by components in the Seed cluster to create the infrastructure for the Shoot.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/external-dns-management">Gardener DNS extension&lt;/a>&lt;/li>
&lt;li>Provider Extension (such as &lt;a href="https://github.com/gardener/gardener-extension-provider-aws">gardener-extension-provider-aws&lt;/a>)&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/etcd-druid">Gardener Extension ETCD Druid&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener-resource-manager">Gardener Resource Manager&lt;/a>&lt;/li>
&lt;li>Operating System Extension (such as &lt;a href="https://github.com/gardener/gardener-extension-os-gardenlinux">gardener-extension-os-gardenlinux&lt;/a>)&lt;/li>
&lt;li>Networking Extension (such as &lt;a href="https://github.com/gardener/gardener-extension-networking-cilium">gardener-extension-networking-cilium&lt;/a>)&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/machine-controller-manager">Machine Controller Manager&lt;/a>&lt;/li>
&lt;li>ContainerRuntime extension (such as &lt;a href="https://github.com/gardener/gardener-extension-runtime-gvisor">gardener-extension-runtime-gvisor&lt;/a>)&lt;/li>
&lt;li>Shoot API server (in the Shoot Namespace in the Seed cluster)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Shoot Cluster&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Cloud Provider Compute API (owned by Stakeholder) - for VM/Node creation.&lt;/li>
&lt;li>VM / Bare metal node hosted by Cloud Provider (in Stakeholder owned account).&lt;/li>
&lt;/ul>
&lt;h3 id="how-to-use-the-diagram">How to Use the Diagram&lt;/h3>
&lt;p>The diagram:&lt;/p>
&lt;ul>
&lt;li>should be read from top to bottom - starting in the top left corner with the creation of the Shoot resource via the Gardener Extension API server.&lt;/li>
&lt;li>should not require an encompassing documentation / description.
More detailed documentation on the components itself can usually be found in the respective repository.&lt;/li>
&lt;li>does not show which activities execute in parallel (many) and also does not describe the exact dependencies between the steps.
This can be found out by &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/shoot/reconciler_reconcile.go">looking at the source code&lt;/a>.
It however tries to put the activities in a logical order of execution during the reconciliation flow.&lt;/li>
&lt;/ul>
&lt;p>Occasionally, there is an info box with additional information next to parts in the diagram that in my point of view require further explanation.
Large example resource for the Gardener CRDs (e.g Worker CRD, Infrastructure CRD) are placed on the left side and are referenced by a dotted line (&amp;mdash;&amp;ndash;).&lt;/p>
&lt;p>Be aware that Gardener is an evolving project, so the diagram will most likely be already outdated by the time you are reading this.
Nevertheless, it should give a solid starting point for further explorations into the details of Gardener.&lt;/p>
&lt;h3 id="flow-diagram">Flow Diagram&lt;/h3>
&lt;p>The diagram can be found below and on &lt;a href="https://github.com/danielfoehrKn/diagrams/tree/master/gardener/shoot-reconciliation">GitHub&lt;/a>.
There are multiple formats available (svg, vsdx, draw.io, html).&lt;/p>
&lt;p>Please open an issue or open a PR in the repository if information is missing or is incorrect.
Thanks!&lt;/p>
&lt;p>&lt;img style="width:300px; height: auto; margin: 0;auto" src="https://raw.githubusercontent.com/danielfoehrKn/diagrams/master/gardener/shoot-reconciliation/gardener_reconcile_with_grid.png" target="_blank">&lt;/a>&lt;/p></description></item><item><title>Blog: Gardener v1.9 and v1.10 Released</title><link>https://gardener.cloud/blog/2020/09.11-gardener-v1.9-and-v1.10-released/</link><pubDate>Fri, 11 Sep 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2020/09.11-gardener-v1.9-and-v1.10-released/</guid><description>
&lt;p>Summer holidays aren&amp;rsquo;t over yet, still, the Gardener community was able to release two new minor versions in the past weeks.
Despite being limited in capacity these days, we were able to reach some major milestones, like adding Kubernetes v1.19 support and the long-delayed automated gardenlet certificate rotation.
Whilst we continue to work on topics related to scalability, robustness, and better observability, we agreed to adjust our focus a little more into the areas of development productivity, code quality and unit/integration testing for the upcoming releases.&lt;/p>
&lt;h2 id="notable-changes-in-v110">Notable Changes in v1.10&lt;/h2>
&lt;p>&lt;a href="https://github.com/gardener/gardener/releases/tag/v1.10.0">Gardener v1.10&lt;/a> was a comparatively small release (measured by the number of changes) but it comes with some major features!&lt;/p>
&lt;h3 id="kubernetes-119-support-gardenergardener2799httpsgithubcomgardenergardenerpull2799">Kubernetes 1.19 Support (&lt;a href="https://github.com/gardener/gardener/pull/2799">gardener/gardener#2799&lt;/a>)&lt;/h3>
&lt;p>The newest minor release of Kubernetes is now supported by Gardener (and all the maintained provider extensions)!
Predominantly, we have enabled CSI migration for OpenStack now that it got promoted to beta, i.e. 1.19 shoots will no longer use the in-tree Cinder volume provisioner.
The CSI migration enablement for Azure got postponed (to at least 1.20) due to some issues that the Kubernetes community is trying to fix in the 1.20 release cycle.
As usual, the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md">1.19 release notes&lt;/a> should be considered before upgrading your shoot clusters.&lt;/p>
&lt;h3 id="automated-certificate-rotation-for-gardenlet-gardenergardener2542httpsgithubcomgardenergardenerpull2542">Automated Certificate Rotation for gardenlet (&lt;a href="https://github.com/gardener/gardener/pull/2542">gardener/gardener#2542&lt;/a>)&lt;/h3>
&lt;p>Similar to the kubelet, the gardenlet supports TLS bootstrapping when deployed into a new seed cluster.
It will request a client certificate for the garden cluster using the &lt;code>CertificateSigningRequest&lt;/code> API of Kubernetes and store the generated results in a &lt;code>Secret&lt;/code> object in the &lt;code>garden&lt;/code> namespace of its seed.
These certificates are usually valid for one year.
We have now added support for automatic renewals if the expiration dates are approaching.&lt;/p>
&lt;h3 id="improved-monitoring-alerts-gardenergardener2776httpsgithubcomgardenergardenerpull2776">Improved Monitoring Alerts (&lt;a href="https://github.com/gardener/gardener/pull/2776">gardener/gardener#2776&lt;/a>)&lt;/h3>
&lt;p>We have worked on a larger refactoring to improve reliability and accuracy of our monitoring alerts for both shoot control planes in the seed, as well as shoot system components running on worker nodes.
The improvements are primarily for operators and should result in less false positive alerts.
Also, the alerts should fire less frequently and are better grouped in order to reduce to overall amount of alerts.&lt;/p>
&lt;h3 id="seed-deletion-protection-gardenergardener2732httpsgithubcomgardenergardenerpull2732">Seed Deletion Protection (&lt;a href="https://github.com/gardener/gardener/pull/2732">gardener/gardener#2732&lt;/a>)&lt;/h3>
&lt;p>Our validation to improve robustness and countermeasures against accidental mistakes has been improved.
Earlier, it was possible to remove the &lt;code>use-as-seed&lt;/code> annotation for shooted seeds or directly set the &lt;code>deletionTimestamp&lt;/code> on &lt;code>Seed&lt;/code> objects, despite of the fact that they might still run shoot control planes.
Seed deletion would not start in these cases, although, it would disrupt the system unnecessarily, and result in some unexpected behaviour.
The Gardener API server is now forbidding such requests if the seeds are not completely empty yet.&lt;/p>
&lt;h3 id="logging-improvements-for-loki-multiple-prs">Logging Improvements for Loki (multiple PRs)&lt;/h3>
&lt;p>After we released our large logging stack refactoring (from EFK to Loki) with &lt;a href="https://gardener.cloud/blog/2020/08.06-gardener-v1.8.0-released/">Gardener v1.8&lt;/a>, we have continued to work on reliability, quality and user feedback in general.
We aren&amp;rsquo;t done yet, though, Gardener v1.10 includes a bunch of improvements which will help to graduate the &lt;code>Logging&lt;/code> feature gate to beta and GA, eventually.&lt;/p>
&lt;h2 id="notable-changes-in-v19">Notable Changes in v1.9&lt;/h2>
&lt;p>The &lt;a href="https://github.com/gardener/gardener/releases/tag/v1.9.0">v1.9 release&lt;/a> contained tons of small improvements and adjustments in various areas of the code base and a little less new major features.
However, we don&amp;rsquo;t want to miss the opportunity to highlight a few of them.&lt;/p>
&lt;h3 id="cri-validation-in-cloudprofiles-gardenergardener2137httpsgithubcomgardenergardenerpull2137">CRI Validation in &lt;code>CloudProfile&lt;/code>s (&lt;a href="https://github.com/gardener/gardener/pull/2137">gardener/gardener#2137&lt;/a>)&lt;/h3>
&lt;p>A couple of releases back we have introduced support for &lt;code>containerd&lt;/code> and the &lt;code>ContainerRuntime&lt;/code> extension API.
The supported container runtimes are operating system specific, and until now it wasn&amp;rsquo;t possible for end-users to easily figure out whether they can enable &lt;code>containerd&lt;/code> or other &lt;code>ContainerRuntime&lt;/code> extensions for their shoots.
With this change, Gardener administrators/operators can now provide that information in the &lt;code>.spec.machineImages&lt;/code> section in the &lt;code>CloudProfile&lt;/code> resource.
This also allows for enhanced validation and prevents misconfigurations.&lt;/p>
&lt;h3 id="new-shoot-event-controller-gardenergardener2649httpsgithubcomgardenergardenerpull2649">New Shoot Event Controller (&lt;a href="https://github.com/gardener/gardener/pull/2649">gardener/gardener#2649&lt;/a>)&lt;/h3>
&lt;p>The shoot controllers in both the &lt;code>gardener-controller-manager&lt;/code> and &lt;code>gardenlet&lt;/code> fire several &lt;code>Event&lt;/code>s for some important operations (e.g., automated hibernation/wake-up due to hibernation schedule, automated Kubernetes/machine image version update during maintenance, etc.).
Earlier, the only way to prolong the lifetime of these events was to modify the &lt;code>--event-ttl&lt;/code> command line parameter of the garden cluster&amp;rsquo;s &lt;code>kube-apiserver&lt;/code>.
This came with the disadvantage that &lt;em>all&lt;/em> events were kept for a longer time (not only those related to &lt;code>Shoot&lt;/code>s that an operator is usually interested in and ideally wants to store for a couple of days).
The new shoot event controller allows to achieve this by deleting non-shoot events.
This helps operators and end-users to better understand which changes were applied to their shoots by Gardener.&lt;/p>
&lt;h3 id="early-deployment-of-the-logging-stack-for-new-shoots-gardenergardener2750httpsgithubcomgardenergardenerpull2750">Early Deployment of the Logging Stack for New Shoots (&lt;a href="https://github.com/gardener/gardener/pull/2750">gardener/gardener#2750&lt;/a>)&lt;/h3>
&lt;p>Since the first introduction of the &lt;code>Logging&lt;/code> feature gate two years back, the logging stack was only deployed at the very end of the shoot creation.
This had the disadvantage that control plane pod logs were not kept in case the shoot creation flow is interrupted before the logging stack could be deployed.
In some situations, this was preventing fetching relevant information about why a certain control plane component crashed.
We now deploy the logging stack very early in the shoot creation flow to always have access to such information.&lt;/p></description></item><item><title>Blog: Gardener v1.8.0 Released</title><link>https://gardener.cloud/blog/2020/08.06-gardener-v1.8.0-released/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2020/08.06-gardener-v1.8.0-released/</guid><description>
&lt;p>Even if we are in the midst of the summer holidays, a new Gardener release came out yesterday: v1.8.0! It&amp;rsquo;s main themes are the large change of our logging stack to Loki (which was already explained in detail on a &lt;a href="https://grafana.com/blog/2020/07/15/gardener-saps-kubernetes-as-a-service-open-source-project-is-moving-its-logging-stack-to-loki/">blog post on grafana.com&lt;/a>), more configuration options to optimize the utilization of a shoot, node-local DNS, new project roles, and significant improvements for the Kubernetes client that Gardener uses to interact with the many different clusters.&lt;/p>
&lt;h2 id="notable-changes">Notable Changes&lt;/h2>
&lt;h3 id="logging-20-efk-stack-replaced-by-loki-gardenergardener2515httpsgithubcomgardenergardenerpull2515">Logging 2.0: EFK Stack Replaced by Loki (&lt;a href="https://github.com/gardener/gardener/pull/2515">gardener/gardener#2515&lt;/a>)&lt;/h3>
&lt;p>Since two years or so, Gardener could optionally provision a dedicated logging stack per seed and per shoot which was based on fluent-bit, fluentd, ElasticSearch and Kibana. This feature was still hidden behind an alpha-level feature gate and never got promoted to beta so far. Due to various limitations of this solution, we decided to replace the EFK stack with Loki. As we already have Prometheus and Grafana deployments for both users and operators by default for all clusters, the choice was just natural.
Please find out more on this topic at &lt;a href="https://grafana.com/blog/2020/07/15/gardener-saps-kubernetes-as-a-service-open-source-project-is-moving-its-logging-stack-to-loki/">this dedicated blog post&lt;/a>.&lt;/p>
&lt;h3 id="cluster-identities-and-dnsowner-objects-gardenergardener2471httpsgithubcomgardenergardenerpull2471-gardenergardener2576httpsgithubcomgardenergardenerpull2576">Cluster Identities and &lt;code>DNSOwner&lt;/code> Objects (&lt;a href="https://github.com/gardener/gardener/pull/2471">gardener/gardener#2471&lt;/a>, &lt;a href="https://github.com/gardener/gardener/pull/2576">gardener/gardener#2576&lt;/a>)&lt;/h3>
&lt;p>The shoot control plane migration topic is ongoing since a few months already, and we are very much progressing with it. A first alpha version will probably make it out soon. As part of these endeavors, we introduced cluster identities and the usage of &lt;code>DNSOwner&lt;/code> objects in this release. Both are needed to gracefully migrate the &lt;code>DNSEntry&lt;/code> extension objects from the old seed to the new seed as part of the control plane migration process.
Please find out more on this topic at &lt;a href="https://kubernetes.io/blog/2019/12/02/gardener-project-update/#control-plane-migration-between-seed-clusters">this blog post&lt;/a>.&lt;/p>
&lt;h3 id="new-uam-role-for-project-members-to-limit-user-access-management-privileges-gardenergardener2611httpsgithubcomgardenergardenerpull2611">New &lt;code>uam&lt;/code> Role for &lt;code>Project&lt;/code> Members to Limit User Access Management Privileges (&lt;a href="https://github.com/gardener/gardener/pull/2611">gardener/gardener#2611&lt;/a>)&lt;/h3>
&lt;p>In order to allow external user access management system to integrate with Gardener and to fulfil certain compliance aspects, we have introduced a new role called &lt;code>uam&lt;/code> for &lt;code>Project&lt;/code> members (next to &lt;code>admin&lt;/code> and &lt;code>viewer&lt;/code>). Only if a user has this role, then he/she is allowed to add/remove other human users to the respective &lt;code>Project&lt;/code>. By default, all newly created &lt;code>Project&lt;/code>s assign this role only to the owner while, for backwards-compatibility reasons, it will be assigned for all members for existing projects. Project owners can steadily revoke this access as desired.
Interestingly, the &lt;code>uam&lt;/code> role is backed by a custom RBAC verb called &lt;code>manage-members&lt;/code>, i.e., the Gardener API server is only admitting changes to the human &lt;code>Project&lt;/code> members if the respective user is bound to this RBAC verb.&lt;/p>
&lt;h3 id="new-node-local-dns-feature-for-shoots-gardenergardener2528httpsgithubcomgardenergardenerpull2528">New Node-Local DNS Feature for Shoots (&lt;a href="https://github.com/gardener/gardener/pull/2528">gardener/gardener#2528&lt;/a>)&lt;/h3>
&lt;p>By default, we are using CoreDNS as DNS plugin in shoot clusters which we auto-scale horizontally using HPA. However, in some situations we are discovering certain bottlenecks with it, e.g., unreliable UDP connections, unnecessary node hopping, inefficient load balancing, etc.
To further optimize the DNS performance for shoot clusters, it is now possible to enable a new alpha-level feature gate in the gardenlet&amp;rsquo;s componentconfig: &lt;code>NodeLocalDNS&lt;/code>. If enabled, all shoots will get a new &lt;code>DaemonSet&lt;/code> to run a DNS server on each node.&lt;/p>
&lt;h3 id="more-kubelet-and-api-server-configurability-gardenergardener2574httpsgithubcomgardenergardenerpull2574-gardenergardener2668httpsgithubcomgardenergardenerpull2668">More kubelet and API Server Configurability (&lt;a href="https://github.com/gardener/gardener/pull/2574">gardener/gardener#2574&lt;/a>, &lt;a href="https://github.com/gardener/gardener/pull/2668">gardener/gardener#2668&lt;/a>)&lt;/h3>
&lt;p>One large benefit of Gardener is that it allows you to optimize the usage of your control plane as well as worker nodes by exposing relevant configuration parameters in the &lt;code>Shoot&lt;/code> API.
In this version, we are adding support to configure kubelet&amp;rsquo;s values for &lt;code>systemReserved&lt;/code> and &lt;code>kubeReserved&lt;/code> resources as well as the kube-apiserver&amp;rsquo;s watch cache sizes.
This allows end-users to get to better node utilization and/or performance for their shoot clusters.&lt;/p>
&lt;h3 id="configurable-timeout-settings-for-machine-controller-manager-gardenergardener2563httpsgithubcomgardenergardenerpull2563">Configurable Timeout Settings for machine-controller-manager (&lt;a href="https://github.com/gardener/gardener/pull/2563">gardener/gardener#2563&lt;/a>)&lt;/h3>
&lt;p>One very central component in Project Gardener is the &lt;a href="https://github.com/gardener/machine-controller-manager">machine-controller-manager&lt;/a> for managing the worker nodes of shoot clusters. It has extensive qualities with respect to node lifecycle management and rolling updates. As such, it uses certain timeout values, e.g. when creating or draining nodes, or when checking their health.
Earlier, those were not customizable by end-users, but we are adding this possibility now. You can fine-grain these settings per worker pool in the &lt;code>Shoot&lt;/code> API such that you can optimize the lifecycle management of your worker nodes even more!&lt;/p>
&lt;h3 id="improved-usage-of-cached-client-to-reduce-network-io-gardenergardener2635httpsgithubcomgardenergardenerpull2635-gardenergardener2637httpsgithubcomgardenergardenerpull2637">Improved Usage of Cached Client to Reduce Network I/O (&lt;a href="https://github.com/gardener/gardener/pull/2635">gardener/gardener#2635&lt;/a>, &lt;a href="https://github.com/gardener/gardener/pull/2637">gardener/gardener#2637&lt;/a>)&lt;/h3>
&lt;p>In the last Gardener release v1.7 we have introduced a huge refactoring the clients that we use to interact with the many different Kubernetes clusters. This is to further optimize the network I/O performed by leveraging watches and caches as good as possible. It&amp;rsquo;s still an alpha-level feature that must be explicitly enabled in the Gardenlet&amp;rsquo;s component configuration, though, with this release we have improved certain things in order to pave the way for beta promotion. For example, we were initially also using a cached client when interacting with shoots. However, as the gardenlet runs in the seed as well (and thus can communicate cluster-internally with the kube-apiservers of the respective shoots) this cache is not necessary and just memory overhead. We have removed it again and saw the memory usage getting lower again. More to come!&lt;/p>
&lt;h3 id="aws-ebs-volume-encryption-by-default-gardenergardener-extension-provider-aws147httpsgithubcomgardenergardener-extension-provider-awspull147">AWS EBS Volume Encryption by Default (&lt;a href="https://github.com/gardener/gardener-extension-provider-aws/pull/147">gardener/gardener-extension-provider-aws#147&lt;/a>)&lt;/h3>
&lt;p>The &lt;code>Shoot&lt;/code> API already exposed the possibility to encrypt the root disks of worker nodes since quite a while, but it was disabled by default (for backwards-compatibility reasons). With this release we have change this default, so new shoot worker nodes will be provisioned with encrypted root disks out-of-the-box. However, the &lt;code>g4dn&lt;/code> instance types of AWS don&amp;rsquo;t support this encryption, so when you use them you have to explicitly disable the encryption in the worker pool configuration.&lt;/p>
&lt;h3 id="liveness-probe-for-gardener-api-server-deployment-gardenergardener2647httpsgithubcomgardenergardenerpull2647">Liveness Probe for Gardener API Server Deployment (&lt;a href="https://github.com/gardener/gardener/pull/2647">gardener/gardener#2647&lt;/a>)&lt;/h3>
&lt;p>A small, but very valuable improvement is the introduction of a liveness probe for our Gardener API server. As it&amp;rsquo;s built with the same library like the Kubernetes API server, it exposes two endpoints at &lt;code>/livez&lt;/code> and &lt;code>/readyz&lt;/code> which were created exactly for the purpose of live- and readiness probes.
With Gardener v1.8, the Helm chart contains a liveness probe configuration by default, and we are awaiting an upstream fix (&lt;a href="https://github.com/kubernetes/kubernetes/issues/93599">kubernetes/kubernetes#93599&lt;/a>) to also enable the readiness probe. This will help in a smoother rolling update of the Gardener API server pods, i.e., preventing clients from talking to a not yet initialized or already terminating API server instance.&lt;/p>
&lt;h3 id="webhook-ports-changed-to-enable-openshift-gardenergardener2660httpsgithubcomgardenergardenerpull2660">Webhook Ports Changed to Enable OpenShift (&lt;a href="https://github.com/gardener/gardener/pull/2660">gardener/gardener#2660&lt;/a>)&lt;/h3>
&lt;p>In order to make it possible to run Gardener on OpenShift clusters as well, we had to make a change in the port configuration for the webhooks we are using in both Gardener and the extension controllers. Earlier, all the webhook servers directly exposed port &lt;code>443&lt;/code>, i.e., a system port which is a security concern and disallowed in OpenShift. We have changed this port now across all places and also adapted our network policies accordingly. This is most likely not the last necessary change to enable this scenario, however, it&amp;rsquo;s a great improvement to push the project forward.&lt;/p>
&lt;p>If you&amp;rsquo;re interested in more details and even more improvements, you can read all the &lt;a href="https://github.com/gardener/gardener/releases/tag/v1.8.0">release notes for Gardener v1.8.0&lt;/a>.&lt;/p></description></item><item><title>Blog: PingCAP’s Experience in Implementing Their Managed TiDB Service with Gardener</title><link>https://gardener.cloud/blog/2020/05.27-pingcaps-experience/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2020/05.27-pingcaps-experience/</guid><description>
&lt;p>Gardener is showing successful collaboration with its growing community of contributors and adopters. With this come some success stories, including PingCAP using Gardener to implement its managed service.&lt;/p>
&lt;h3 id="about-pingcap-and-its-tidb-cloud">About PingCAP and Its TiDB Cloud&lt;/h3>
&lt;p>&lt;a href="https://pingcap.com/about/">PingCAP&lt;/a> started in 2015, when three seasoned infrastructure engineers working at leading Internet companies got sick and tired of the way databases were managed, scaled and maintained. Seeing no good solution on the market, they decided to build their own - the open-source way. With the help of a first-class team and hundreds of contributors from around the globe, PingCAP is building a distributed NewSQL, hybrid transactional and analytical processing (HTAP) database.&lt;/p>
&lt;p>Its flagship project, &lt;a href="https://en.wikipedia.org/wiki/TiDB">TiDB&lt;/a>, is a cloud-native distributed SQL database with MySQL compatibility, and one of the &lt;a href="https://github.com/pingcap/tidb">most popular&lt;/a> open-source database projects - with 23.5K+ stars and 400+ contributors. Its sister project &lt;a href="https://github.com/tikv/tikv">TiKV&lt;/a> is a &lt;a href="https://landscape.cncf.io/card-mode">Cloud Native Interactive Landscape project&lt;/a>.&lt;/p>
&lt;p>PingCAP envisioned their managed TiDB service, known as &lt;a href="https://pingcap.com/tidb-cloud/sign-up/">TiDB Cloud&lt;/a>, to be multi-tenant, secure, cost-efficient, and to be compatible with different cloud providers. As a result, the company turned to Gardener to build their managed TiDB cloud service offering.&lt;/p>
&lt;img title="TiDB Cloud Beta Preview" src="https://gardener.cloud/__resources/00-001_7d61c5.png" style="width:90%; height:auto">
&lt;figcaption style="text-align:center;margin-top: -25px;margin-bottom: 30px;font-size: 90%;">TiDB Cloud Beta Preview&lt;/figcaption>
&lt;h3 id="limitations-with-other-public-managed-kubernetes-services">Limitations with Other Public Managed Kubernetes Services&lt;/h3>
&lt;p>Previously, PingCAP encountered issues while using other public managed K8s cluster services, to develop the first version of its TiDB Cloud. Their worst pain point was that they felt helpless when encountering certain malfunctions. PingCAP wasn’t able to do much to resolve these issues, except waiting for the providers’ help. More specifically, they experienced problems due to cloud-provider specific Kubernetes system upgrades, delays in the support response (which could be avoided in exchange of a costly support fee), and no control over when things got fixed.&lt;/p>
&lt;p>There was also a lot of cloud-specific integration work needed to follow a multi-cloud strategy, which proved to be expensive both to produce and maintain. With one of these managed K8s services, you would have to integrate the instance API, as opposed to a solution like Gardener, which provides a unified API for all clouds. Such a unified API eliminates the need to worry about cloud specific-integration work altogether.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/00-002_989581.png" style="width:60%; height:auto">
&lt;h3 id="why-pingcap-chose-gardener-to-build-tidb-cloud">Why PingCAP Chose Gardener to Build TiDB Cloud&lt;/h3>
&lt;blockquote>
&lt;p>“Gardener has similar concepts to Kubernetes. Each Kubernetes cluster is just like a Kubernetes pod, so the similar concepts apply, and the controller pattern makes Gardener easy to manage. It was also easy to extend, as the team was already very familiar with Kubernetes, so it wasn’t hard for us to extend Gardener. We also saw that Gardener has a very active community, which is always a plus!”&lt;/p>
&lt;p>- Aylei Wu, (Cloud Engineer) at PingCAP&lt;/p>
&lt;/blockquote>
&lt;p>At first glance, PingCAP had initial reservations about using Gardener - mainly due to its adoption level (still at the beginning) and an apparent complexity of use. However, these were soon eliminated as they learned more about the solution. As Aylei Wu mentioned during the last &lt;a href="https://www.youtube.com/watch?v=nqkzUylfIbU&amp;amp;feature=youtu.be">Gardener community meeting&lt;/a>, &lt;em>&lt;strong>“a good product speaks for itself”&lt;/strong>&lt;/em>, and once the company got familiar with Gardener, they quickly noticed that the concepts were very similar to Kubernetes, which they were already familiar with.&lt;/p>
&lt;p>They recognized that Gardener would be their best option, as it is highly extensible and provides a unified abstraction API layer. In essence, the machines can be managed via a machine controller manager for different cloud providers - without having to worry about the individual cloud APIs.&lt;/p>
&lt;p>They agreed that Gardener’s solution, although complex, was definitely worth it. Even though it is a relatively new solution, meaning they didn’t have access to other user testimonials, they decided to go with the service since it checked all the boxes (and as SAP was running it productively with a huge fleet). PingCAP also came to the conclusion that building a managed Kubernetes service themselves would not be easy. Even if they were to build a managed K8s service, they would have to heavily invest in development and would still end up with an even more complex platform than Gardener’s. For all these reasons combined, PingCAP decided to go with Gardener to build its TiDB Cloud.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/00-003_24951c.png" style="width:70%; height:auto">
&lt;p>Here are certain features of Gardener that PingCAP found appealing:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Cloud agnostic:&lt;/strong> Gardener’s abstractions for cloud-specific integrations dramatically reduce the investment in supporting more than one cloud infrastructure. Once the integration with Amazon Web Services was done, moving on to Google Cloud Platform proved to be relatively easy. (At the moment, TiDB Cloud has subscription plans available for both GCP and AWS, and they are planning to support Alibaba Cloud in the future.)&lt;/li>
&lt;li>&lt;strong>Familiar concepts:&lt;/strong> Gardener is K8s native; its concepts are easily related to core Kubernetes concepts. As such, it was easy to onboard for a K8s experienced team like PingCAP’s SRE team.&lt;/li>
&lt;li>&lt;strong>Easy to manage and extend:&lt;/strong> Gardener’s API and extensibility are easy to implement, which has a positive impact on the implementation, maintenance costs and time-to-market.&lt;/li>
&lt;li>&lt;strong>Active community:&lt;/strong> Prompt and quality responses on Slack from the Gardener team tremendously helped to quickly onboard and produce an efficient solution.&lt;/li>
&lt;/ul>
&lt;h3 id="how-pingcap-built-tidb-cloud-with-gardener">How PingCAP Built TiDB Cloud with Gardener&lt;/h3>
&lt;p>On a technical level, PingCAP’s set-up overview includes the following:&lt;/p>
&lt;ul>
&lt;li>A Base Cluster globally, which is the top-level control plane of TiDB Cloud&lt;/li>
&lt;li>A Seed Cluster per cloud provider per region, which makes up the fundamental data plane of TiDB Cloud&lt;/li>
&lt;li>A Shoot Cluster is dynamically provisioned per tenant per cloud provider per region when requested&lt;/li>
&lt;li>A tenant may create one or more TiDB clusters in a Shoot Cluster&lt;/li>
&lt;/ul>
&lt;p>As a real world example, PingCAP sets up the Base Cluster and Seed Clusters in advance. When a tenant creates its first TiDB cluster under the us-west-2 region of AWS, a Shoot Cluster will be dynamically provisioned in this region, and will host all the TiDB clusters of this tenant under us-west-2. Nevertheless, if another tenant requests a TiDB cluster in the same region, a &lt;strong>new&lt;/strong> Shoot Cluster will be provisioned. Since different Shoot Clusters are located in different VPCs and can even be hosted under different AWS accounts, TiDB Cloud is able to achieve hard isolation between tenants and meet the critical security requirements for our customers.&lt;/p>
&lt;p>To automate these processes, PingCAP creates a service in the Base Cluster, known as the TiDB Cloud “Central” service. The Central is responsible for managing shoots and the TiDB clusters in the Shoot Clusters. As shown in the following diagram, user operations go to the Central, being authenticated, authorized, validated, stored and then applied asynchronously in a controller manner. The Central will talk to the Gardener API Server to create and scale Shoot clusters. The Central will also access the Shoot API Service to deploy and reconcile components in the Shoot cluster, including control components (&lt;a href="https://github.com/pingcap/tidb-operator">TiDB Operator&lt;/a>, API Proxy, Usage Reporter for billing, etc.) and the TiDB clusters.&lt;/p>
&lt;img title="TiDB Cloud on Gardener Architecture Overview" src="https://gardener.cloud/__resources/00-004_4235ad.png" style="width:90%; height:auto">
&lt;figcaption style="text-align:center;margin-top: -25px;margin-bottom: 30px;font-size: 90%;">TiDB Cloud on Gardener Architecture Overview&lt;/figcaption>
&lt;h3 id="whats-next-for-pingcap-and-gardener">What’s Next for PingCAP and Gardener&lt;/h3>
&lt;p>With the initial success of using the project to build TiDB Cloud, PingCAP is now working heavily on the stability and day-to-day operations of TiDB Cloud on Gardener. This includes writing Infrastructure-as-Code scripts/controllers with it to achieve GitOps, building tools to help diagnose problems across regions and clusters, as well as running chaos tests to identify and eliminate potential risks. After benefiting greatly from the community, PingCAP will continue to contribute back to Gardener.&lt;/p>
&lt;p>In the future, PingCAP also plans to support more cloud providers like AliCloud and Azure. Moreover, PingCAP may explore the opportunity of running TiDB Cloud in on-premise data centers with the constantly expanding support this project provides. Engineers at PingCAP enjoy the ease of learning from Gardener’s Kubernetes-like concepts and being able to apply them everywhere. Gone are the days of heavy integrations with different clouds and worrying about vendor stability. With this project, PingCAP now sees broader opportunities to land TiDB Cloud on various infrastructures to meet the needs of their global user group.&lt;/p>
&lt;p>Stay tuned, more &lt;a href="https://gardener.cloud/blog/">blog posts&lt;/a> to come on how Gardener is collaborating with its contributors and adopters to bring fully-managed clusters at scale everywhere! If you want to join in on the fun, &lt;a href="https://gardener.cloud/community/">connect with our community&lt;/a>.&lt;/p></description></item><item><title>Blog: New Website, Same Green Flower</title><link>https://gardener.cloud/blog/2020/05.11-new-website-same-green-flower/</link><pubDate>Mon, 11 May 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2020/05.11-new-website-same-green-flower/</guid><description>
&lt;p>The &lt;a href="https://gardener.cloud">Gardener project website&lt;/a> just received a serious facelift. Here are some of the highlights:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>A completely new landing page&lt;/strong>, emphasizing both on Gardener&amp;rsquo;s value proposition and the open community behind it.&lt;/li>
&lt;li>&lt;strong>The Community page&lt;/strong> was reconstructed for quick access to the various community channels and will soon merge the Adopters page. It will provide a better insight into success stories from the communty.&lt;/li>
&lt;li>&lt;strong>Improved blogs&lt;/strong> layout. One-click sharing options are available starting with simple URL copy link and twitter button and others will closely follow up. While we are at it, give it a try. Spread the word.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Website builds&lt;/strong> also got to a new level with:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Containerization&lt;/strong>. The whole build environment is containerized now, eliminating differences between local and CI/CD setup and reducing content developers focus only to the &lt;code>/documentation&lt;/code> repository. Running a local server for live preview of changes as you make them when developing content for the website, is now as easy as runing &lt;code>make serve&lt;/code> in your local &lt;code>/documentation&lt;/code> clone.&lt;/li>
&lt;li>&lt;strong>Numerous improvements to the buld scripts&lt;/strong>. More &lt;a href="https://github.com/gardener/website-generator#build-configuration">configuration options&lt;/a>, authenticated requests, fault tolerance and performance.&lt;/li>
&lt;li>&lt;strong>Good news for Windows WSL users&lt;/strong> who will now enjoy a significantly support. See the updated &lt;a href="https://github.com/gardener/website-generator#windows-10-users">README&lt;/a> for details on that.&lt;/li>
&lt;li>&lt;strong>A number of improvements&lt;/strong> in layouts styles, site assets and hugo site-building techniques.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>But hey, THAT&amp;rsquo;S NOT ALL!&lt;/strong>&lt;/p>
&lt;p>Stay tuned for more improvements around the corner. The biggest ones are aligning the documentation with the new theme and restructuring it along, more emphasis on community success stories all around, more sharing options and more than a handful of shortcodes for content development and &amp;hellip; let&amp;rsquo;s cut the spoilers here.&lt;/p>
&lt;p>I hope you will like it. Let us know what you think about it. Feel free to leave comments and discuss on &lt;img src="https://gardener.cloud/__resources/twitter-logo-green_0b6bc3.svg" class="icon inline">&lt;a href="https://twitter.com/GardenerProject">Twitter&lt;/a> and &lt;img src="https://gardener.cloud/__resources/slack-logo-green_ed055f.svg" class="icon inline">&lt;a href="https://kubernetes.slack.com/archives/CB57N0BFG">Slack&lt;/a>, or in case of issues - on &lt;img src="https://gardener.cloud/__resources/github-mark-logo-green_650f96.svg" class="icon inline">&lt;a href="https://github.com/gardener/documentation/issues">GitHub&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Go ahead and help us spread the word: &lt;a href="https://gardener.cloud">&lt;a href="https://gardener.cloud">https://gardener.cloud&lt;/a>&lt;/a>&lt;/strong>&lt;/p>
&lt;img src="https://gardener.cloud/__resources/website-screen-L_70f11a.png"/>
&lt;br></description></item></channel></rss>