<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>2025 on Gardener</title><link>https://gardener.cloud/blog/2025/</link><description>Recent content in 2025 on Gardener</description><generator>Hugo</generator><language>en-US</language><lastBuildDate>Wed, 21 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://gardener.cloud/blog/2025/index.xml" rel="self" type="application/rss+xml"/><item><title>Fine-Tuning kube-proxy Readiness: Ensuring Accurate Health Checks During Node Scale-Down</title><link>https://gardener.cloud/blog/2025/05-21-fine-tuning-kube-proxy-readiness-ensuring-accurate-health-checks-during-node-scale-down/</link><pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2025/05-21-fine-tuning-kube-proxy-readiness-ensuring-accurate-health-checks-during-node-scale-down/</guid><description>&lt;p>Gardener has recently refined how it determines the readiness of &lt;code>kube-proxy&lt;/code> components within managed Kubernetes clusters. This adjustment leads to more accurate system health reporting, especially during node scale-down operations orchestrated by &lt;code>cluster-autoscaler&lt;/code>.&lt;/p>
&lt;h3 id="the-challenge-kube-proxy-readiness-during-node-scale-down">The Challenge: kube-proxy Readiness During Node Scale-Down&lt;a class="td-heading-self-link" href="#the-challenge-kube-proxy-readiness-during-node-scale-down" aria-label="Heading self-link">&lt;/a>&lt;/h3>&lt;p>Previously, Gardener utilized &lt;code>kube-proxy&lt;/code>&amp;rsquo;s &lt;code>/healthz&lt;/code> endpoint for its readiness probe. While generally effective, this endpoint&amp;rsquo;s behavior changed in Kubernetes 1.28 (as part of &lt;a href="https://github.com/alexanderConstantinescu/kubernetes-enhancements/blob/e3d8adae9cf79338add2149db0900e47a4c64338/keps/sig-network/3836-kube-proxy-improved-ingress-connectivity-reliability/README.md?plain=1#L105-L107">KEP-3836&lt;/a> and implemented in &lt;a href="https://github.com/kubernetes/kubernetes/pull/116470">kubernetes/kubernetes#116470&lt;/a>). The &lt;code>/healthz&lt;/code> endpoint now reports &lt;code>kube-proxy&lt;/code> as unhealthy if its node is marked for deletion by &lt;code>cluster-autoscaler&lt;/code> (e.g., via a specific taint) or has a deletion timestamp.&lt;/p></description></item><item><title>New in Gardener: Forceful Redeployment of gardenlets for Enhanced Operational Control</title><link>https://gardener.cloud/blog/2025/05-21-new-in-gardener-forceful-redeployment-of-gardenlets-for-enhanced-operational-control/</link><pubDate>Wed, 21 May 2025 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2025/05-21-new-in-gardener-forceful-redeployment-of-gardenlets-for-enhanced-operational-control/</guid><description>&lt;p>Gardener continues to enhance its operational capabilities, and a recent improvement introduces a much-requested feature for managing gardenlets: the ability to forcefully trigger their redeployment. This provides operators with greater control and a streamlined recovery path for specific scenarios.&lt;/p>
&lt;h3 id="the-standard-gardenlet-lifecycle">The Standard gardenlet Lifecycle&lt;a class="td-heading-self-link" href="#the-standard-gardenlet-lifecycle" aria-label="Heading self-link">&lt;/a>&lt;/h3>&lt;p>gardenlets, crucial components in the Gardener architecture, are typically deployed into seed clusters. For setups utilizing the &lt;code>seedmanagement.gardener.cloud/v1alpha1.Gardenlet&lt;/code> resource, particularly in unmanaged seeds (those not backed by a shoot cluster and &lt;code>ManagedSeed&lt;/code> resource), the &lt;code>gardener-operator&lt;/code> handles the initial deployment of the gardenlet.&lt;/p></description></item><item><title>Enhanced Node Management: Introducing In-Place Updates in Gardener</title><link>https://gardener.cloud/blog/2025/05-19-enhanced-node-management-introducing-in-place-updates-in-gardener/</link><pubDate>Mon, 19 May 2025 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2025/05-19-enhanced-node-management-introducing-in-place-updates-in-gardener/</guid><description>&lt;p>Gardener is committed to providing efficient and flexible Kubernetes cluster management. Traditionally, updates to worker pool configurations, such as machine image or Kubernetes minor version changes, trigger a rolling update. This process involves replacing existing nodes with new ones, which is a robust approach for many scenarios. However, for environments with physical or bare-metal nodes, or stateful workloads sensitive to node replacement, or if the virtual machine type is scarce, this can introduce challenges like extended update times and potential disruptions.&lt;/p></description></item></channel></rss>