<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://gardener.cloud/blog/2025/06/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/blog/2025/06/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>June | Gardener</title><meta name=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:url" content="https://gardener.cloud/blog/2025/06/"><meta property="og:site_name" content="Gardener"><meta property="og:title" content="June"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:locale" content="en_US"><meta property="og:type" content="website"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="June"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta itemprop=datePublished content="2025-06-30T00:00:00+00:00"><meta itemprop=dateModified content="2025-06-30T00:00:00+00:00"><meta itemprop=image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="June"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.64d56283aba037cc3a217d684edadfb4e3c57ca54122947d2f030f74bcd28a27.css as=style integrity="sha256-ZNVig6ugN8w6IX1oTtrftOPFfKVBIpR9LwMPdLzSiic=" crossorigin=anonymous><link href=/scss/main.min.64d56283aba037cc3a217d684edadfb4e3c57ca54122947d2f030f74bcd28a27.css rel=stylesheet integrity="sha256-ZNVig6ugN8w6IX1oTtrftOPFfKVBIpR9LwMPdLzSiic=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class="td-section td-blog"><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg width="90" height="90" viewBox="0 0 90 90" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009F76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=navbar-brand__name>Gardener</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://demo.gardener.cloud target=_blank rel=noopener><span>Demo</span></a></li><li class=nav-item><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class=nav-item><a class=nav-link href=/docs><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/blog><span>Blogs</span></a></li><li class=nav-item><a class=nav-link href=/community><span>Community</span></a></li><li class=nav-item><a class=nav-link href=https://join.slack.com/t/gardener-cloud/shared_invite/zt-33c9daems-3oOorhnqOSnldZPWqGmIBw target=_blank rel=noopener><span>Join us on</span></a></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.aeb4cb130742b2ddcafa7a319a723e89.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 ps-md-5 pe-md-4" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/blog/2025/06/>Return to the regular view of this page</a>.</p></div><h1 class=title>June</h1><div class=content></div></div><div class=td-content><h1 id=pg-05ec71ab6a25b8bc2595adea5fbe1dc7>Getting Started with OpenTelemetry on a Gardener Shoot Cluster</h1><div class="td-byline mb-4"><time datetime=2025-06-30 class=text-body-secondary>Monday, June 30, 2025</time></div><p>In this blog post, we will explore how to set up an <a href=https://opentelemetry.io/>OpenTelemetry</a> based observability stack on a Gardener shoot cluster. OpenTelemetry is an open-source observability framework that provides a set of APIs, SDKs, agents, and instrumentation to collect telemetry data from applications and systems.
It provides a unified approach for collecting, processing, and exporting telemetry data such as traces, metrics, and logs. In addition, it gives flexibility in designing observability stacks, helping avoid vendor lock-in and allowing users to choose the most suitable tools for their use cases.</p><p>Here we will focus on setting up OpenTelemetry for a Gardener shoot cluster, collecting both logs and metrics and exporting them to various backends. We will use the <a href=https://github.com/open-telemetry/opentelemetry-operator>OpenTelemetry Operator</a> to simplify the deployment and management of OpenTelemetry collectors on Kubernetes and demonstrate some best practices for configuration including security and performance considerations.</p><h2 id=prerequisites>Prerequisites<a class=td-heading-self-link href=#prerequisites aria-label="Heading self-link"></a></h2><p>To follow along with this guide, you will need:</p><ul><li>A Gardener Shoot Cluster.</li><li><code>kubectl</code> configured to access the cluster.</li><li><code>shoot-cert-service</code> enabled on the shoot cluster, to manage TLS certificates for the OpenTelemetry Collectors and backends.</li></ul><h2 id=component-overview-of-the-sample-opentelemetry-stack>Component Overview of the Sample OpenTelemetry Stack<a class=td-heading-self-link href=#component-overview-of-the-sample-opentelemetry-stack aria-label="Heading self-link"></a></h2><p><img src=/blog/2025/06/images/otel-gardener-shoot.png alt="OpenTelemetry Stack"></p><h2 id=setting-up-a-gardener-shoot-for-mtls-certificate-management>Setting Up a Gardener Shoot for mTLS Certificate Management<a class=td-heading-self-link href=#setting-up-a-gardener-shoot-for-mtls-certificate-management aria-label="Heading self-link"></a></h2><p>Here we use a self managed mTLS architecture with an illustration purpose. In a production environment, you would typically use a managed certificate authority (CA) or a service mesh to handle mTLS certificates and encryption. However, there might be cases where you want to have flexibility in authentication and authorization mechanisms, for example, by leveraging Kubernetes RBAC to determine whether a service is authorized to connect to a backend or not. In our illustration, we will use a <code>kube-rbac-proxy</code> as a sidecar to the backends, to enforce the mTLS authentication and authorization. The <code>kube-rbac-proxy</code> is a reverse proxy that uses Kubernetes RBAC to control access to services, allowing us to define fine-grained access control policies.</p><p><img src=/blog/2025/06/images/otel-mtls-kube-rbac-proxy.png alt=otel-mtls></p><p>The <code>kube-rbac-proxy</code> extracts the identity of the client (OpenTelemetry collector) from the CommonName (CN) field of the TLS certificate and uses it to perform authorization checks against the Kubernetes API server. This enables fine-grained access control policies based on client identity, ensuring that only authorized clients can connect to the backends.</p><p>First, set up the <code>Issuer</code> certificate in the Gardener shoot cluster, allowing you to later issue and manage TLS certificates for the OpenTelemetry collectors and the backends. To allow a custom issuer, the shoot cluster shall be configured with the <code>shoot-cert-service</code> extension.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-shoot
</span></span><span style=display:flex><span>  namespace: my-project
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-cert-service
</span></span><span style=display:flex><span>      providerConfig:
</span></span><span style=display:flex><span>        apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>        kind: CertConfig
</span></span><span style=display:flex><span>        shootIssuers:
</span></span><span style=display:flex><span>          enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>Once the shoot is reconciled, the <code>Issuer.cert.gardener.cloud</code> resources will be available.
We can use <code>openssl</code> to create a self-signed CA certificate that will be used to sign the TLS certificates for the OpenTelemetry Collector and backends.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>openssl genrsa -out ./ca.key 4096
</span></span><span style=display:flex><span>openssl req -x509 -new -nodes -key ./ca.key -sha256 -days 365 -out ./ca.crt -subj <span style=color:#a31515>&#34;/CN=ca&#34;</span>
</span></span><span style=display:flex><span><span style=color:green># Create namespace and apply the CA secret and issuer</span>
</span></span><span style=display:flex><span>kubectl create namespace certs <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --dry-run=client -o yaml | kubectl apply -f -
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Create the CA secret in the certs namespace</span>
</span></span><span style=display:flex><span>kubectl create secret tls ca --namespace certs <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --key=./ca.key --cert=./ca.crt <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --dry-run=client -o yaml | kubectl apply -f -
</span></span></code></pre></div><p>Next, we will create the cluster <code>Issuer</code> resource, referencing the CA secret we just created.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Issuer
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: issuer-selfsigned
</span></span><span style=display:flex><span>  namespace: certs
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ca:
</span></span><span style=display:flex><span>    privateKeySecretRef:
</span></span><span style=display:flex><span>      name: ca
</span></span><span style=display:flex><span>      namespace: certs
</span></span></code></pre></div><p>Later, we can create <code>Certificate</code> resources to securely connect the OpenTelemetry collectors to the backends.</p><h2 id=setting-up-the-opentelemetry-operator>Setting Up the OpenTelemetry Operator<a class=td-heading-self-link href=#setting-up-the-opentelemetry-operator aria-label="Heading self-link"></a></h2><p>To deploy the OpenTelemetry Operator on your Gardener Shoot Cluster, we can use the <a href=https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-operator>project helm chart</a> with a minimum configuration. The important part is to set the <code>collector</code> image to the latest <code>contrib</code> distribution image which determines the set of receivers, processors, and exporters plugins that will be available in the OpenTelemetry collector instance. There are several <a href=https://github.com/open-telemetry/opentelemetry-collector-releases>pre-built distributions</a> available such as: <code>otelcol</code>, <code>otelcol-contrib</code>, <code>otelcol-k8s</code>, <code>otelcol-otlp</code>, and <code>otelcol-ebpf-profiler</code>. For the purpose of this guide, we will use the <code>otelcol-contrib</code> distribution, which includes a wide range of plugins for various backends and data sources.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>manager:
</span></span><span style=display:flex><span>  collectorImage:
</span></span><span style=display:flex><span>    repository: <span style=color:#a31515>&#34;otel/opentelemetry-collector-contrib&#34;</span>
</span></span></code></pre></div><h2 id=setting-up-the-backends-prometheus-victoria-logs>Setting Up the Backends (prometheus, victoria-logs)<a class=td-heading-self-link href=#setting-up-the-backends-prometheus-victoria-logs aria-label="Heading self-link"></a></h2><p>Setting up the backends is a straightforward process. We will use plain resource manifests for illustration purposes, outlining the important parts allowing OpenTelemetry collectors to connect securely to the backends using mTLS. An important part is enabling the respective <code>OTLP</code> ingestion endpoints on the backends, which will be used by the OpenTelemetry collectors to send telemetry data. In a production environment, the lifecycle of the backends will be probably managed by the respective component&rsquo;s operators</p><h3 id=setting-up-prometheus-metrics-backend>Setting Up Prometheus (Metrics Backend)<a class=td-heading-self-link href=#setting-up-prometheus-metrics-backend aria-label="Heading self-link"></a></h3><p>Here is the complete list of manifests for deploying a single prometheus instance with the <code>OTLP</code> ingestion endpoint and a <code>kube-rbac-proxy</code> sidecar for mTLS authentication and authorization:</p><ul><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-prometheus/certificate.yaml>Prometheus Certificate</a>
That is the serving certificate of the <code>kube-rbac-proxy</code> sidecar. The OpenTelemetry collector needs to trust the signing CA, hence we use the same <code>Issuer</code> we created earlier.</p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-prometheus/prometheus.yaml>Prometheus</a>
The prometheus needs to be configured to allow <code>OTLP</code> ingestion endpoint: <a href=https://prometheus.io/docs/guides/opentelemetry/#enable-the-OTLP-receiver><code>--web.enable-OTLP-receiver</code></a>. That allows the OpenTelemetry collector to push metrics to the Prometheus instance (via the <code>kube-rbac-proxy</code> sidecar).</p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-prometheus/prometheus-config.yml>Prometheus Configuration</a>
In Prometheus&rsquo; case, the <code>OpenTelemetry</code> resource attributes usually set by the collectors can be used to determine labels for the metrics. This is illustrated in the collector&rsquo;s <code>prometheus</code> receiver configuration. A common and unified set of labels across all metrics collected by the OpenTelemetry collector is a fundamental requirement for sharing and understanding the data across different teams and systems. This common set is defined by the <a href=https://opentelemetry.io/docs/specs/semconv/>OpenTelemetry Semantic Conventions</a> specification. For example ,<code>k8s.pod.name</code>, <code>k8s.namespace.name</code>, <code>k8s.node.name</code>, etc. are some of the common labels that can be used to identify the source of the observability signals. Those are also common across the different types of telemetry data (traces, metrics, logs), serving correlation and analysis use cases.</p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-prometheus/mtls-rbac.yaml>mTLS Proxy rbac</a>
This example defines a <code>Role</code> allowing requests to the prometheus backend to pass the kube-rbac-proxy.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>rules:
</span></span><span style=display:flex><span>- apiGroups: [<span style=color:#a31515>&#34;authorization.kubernetes.io&#34;</span>]
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>    - observabilityapps/prometheus
</span></span><span style=display:flex><span>  verbs: [<span style=color:#a31515>&#34;get&#34;</span>, <span style=color:#a31515>&#34;create&#34;</span>] <span style=color:green># GET, POST</span>
</span></span></code></pre></div><p>In this example, we allow <code>GET</code> and <code>POST</code> requests to reach the prometheus upstream service, if the request is authenticated with a valid mTLS certificate and the identified user is allowed to access the Prometheus service by the corresponding <code>RoleBinding</code>. <code>PATCH</code> and <code>DELETE</code> requests are not allowed. The mapping between the http request methods and the Kubernetes RBAC verbs is seen at <a href=https://github.com/brancz/kube-rbac-proxy/blob/8d0b850862fc4dac71a0e2cbd8a3c8a1c5fdc61a/pkg/proxy/proxy.go#L47>kube-rbac-proxy/proxy.go</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>subjects:
</span></span><span style=display:flex><span>- apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>  kind: User
</span></span><span style=display:flex><span>  name: client
</span></span></code></pre></div></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-prometheus/mtls-ra.yml>mTLS Proxy resource-attributes</a>
<code>kube-rbac-proxy</code> creates Kubernetes <code>SubjectAccessReview</code> to determine if the request is allowed to pass. The <code>SubjectAccessReview</code> is created with the <code>resourceAttributes</code> set to the upstream service, in this case the Prometheus service.</p></li></ul><h3 id=setting-up-victoria-logs-logs-backend>Setting Up victoria-logs (Logs Backend)<a class=td-heading-self-link href=#setting-up-victoria-logs-logs-backend aria-label="Heading self-link"></a></h3><p>In our example, we will use <a href=https://docs.victoriametrics.com/victorialogs/>victoria-logs</a> as the logs backend. victoria-logs is a high-performance, cost-effective, and scalable log management solution. It is designed to work seamlessly with Kubernetes and provides powerful querying capabilities. It is important to note that any <code>OTLP</code> compatible backend can be used as a logs backend, allowing flexibility in choosing the best tool for the concrete needs.</p><p>Here is the complete manifests for deploying a single <code>victoria-logs</code> instance with the <code>OTLP</code> ingestion endpoint enabled and <code>kube-rbac-proxy</code> sidecar for mTLS authentication and authorization, using the upstream helm chart:</p><ul><li><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-VictoriaLogs/certificate.yaml>Victoria-Logs Certificate</a>
That is the serving certificate of the <code>kube-rbac-proxy</code> sidecar. The OpenTelemetry collector needs to trust the signing CA hence we use the same <code>Issuer</code> we created earlier.</li><li><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-victorialogs/values.yaml>Victoria-Logs chart values</a>
The certificate secret shall be mounted in the VictoriaLogs pod as a volume, as it is referenced by the <code>kube-rbac-proxy</code> sidecar.</li><li><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-victorialogs/mtls-rbac.yaml>Victoria-Logs mTLS Proxy rbac</a>
There is no fundamental difference compared to how we configured the Prometheus mTLS proxy. The <code>Role</code> allows requests to the VictoriaLogs backend to pass the kube-rbac-proxy.</li><li><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-victorialogs/mtls-ra.yml>Victoria-Logs mTLS Proxy resource-attributes</a></li></ul><p>By now we shall have a working Prometheus and victoria-logs backends, both secured with mTLS and ready to accept telemetry data from the OpenTelemetry collector.</p><h3 id=setting-up-the-opentelemetry-collectors>Setting Up the OpenTelemetry Collectors<a class=td-heading-self-link href=#setting-up-the-opentelemetry-collectors aria-label="Heading self-link"></a></h3><p>We are going to deploy two OpenTelemetry collectors: <code>k8s-events</code> and <code>shoot-metrics</code>. Both collectors will emit their own telemetry data in addition to the data collected from the respective receivers.</p><h4 id=k8s-events-collector><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-collectors/k8s-events-otel.yaml>k8s-events</a> collector<a class=td-heading-self-link href=#k8s-events-collector aria-label="Heading self-link"></a></h4><p>In this example, we use 2 receivers:</p><ul><li><a href=https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/k8seventsreceiver>k8s_events receiver</a> to collect Kubernetes events from the cluster.</li><li><a href=https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/k8sclusterreceiver>k8s_cluster receiver</a> to collect Kubernetes cluster metrics.</li></ul><p>Here is an example of Kubernetes events persited in the <code>victoria-logs</code> backend. It filters logs which represents events from <code>kube-system</code> namesapce related to a rollout restart of the target statefulset. Then it formats the UI to show the event reason and object name.
<img src=/blog/2025/06/images/otel-victoria-logs.png alt=otel-victoria-logs></p><p>The collector features few important configurations related to reliability and performance.
The collected metrics points are are sent in batches to the Prometheus backend using the corresponding OTLP exporter and the memory consumption of the collector is also limited. In general, it is always a good practice to set a memory limiter and batch processing in the collector pipeline.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>processors:
</span></span><span style=display:flex><span>  memory_limiter:
</span></span><span style=display:flex><span>    check_interval: 1s
</span></span><span style=display:flex><span>    limit_percentage: 80
</span></span><span style=display:flex><span>    spike_limit_percentage: 2
</span></span><span style=display:flex><span>  batch:
</span></span><span style=display:flex><span>    timeout: 5s
</span></span><span style=display:flex><span>    send_batch_size: 1000
</span></span></code></pre></div><p>Allowing the collector to emit its own telemetry data is configured in the service section of the collector configuration.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>service:
</span></span><span style=display:flex><span>  <span style=color:green># Configure the collector own telemetry</span>
</span></span><span style=display:flex><span>  telemetry:
</span></span><span style=display:flex><span>    <span style=color:green># Emit collector logs to stdout, you can also push them to a backend.</span>
</span></span><span style=display:flex><span>    logs:
</span></span><span style=display:flex><span>      level: info
</span></span><span style=display:flex><span>      encoding: console
</span></span><span style=display:flex><span>      output_paths: [stdout]
</span></span><span style=display:flex><span>      error_output_paths: [stderr]
</span></span><span style=display:flex><span>    <span style=color:green># Push collector internal metrics to Prometheus</span>
</span></span><span style=display:flex><span>    metrics:
</span></span><span style=display:flex><span>      level: detailed
</span></span><span style=display:flex><span>      readers:
</span></span><span style=display:flex><span>        - <span style=color:green># push metrics to Prometheus backend</span>
</span></span><span style=display:flex><span>          periodic:
</span></span><span style=display:flex><span>            interval: 30000
</span></span><span style=display:flex><span>            timeout: 10000
</span></span><span style=display:flex><span>            exporter:
</span></span><span style=display:flex><span>              OTLP:
</span></span><span style=display:flex><span>                protocol: http/protobuf
</span></span><span style=display:flex><span>                endpoint: <span style=color:#a31515>&#34;${env:PROMETHEUS_URL}/api/v1/OTLP/v1/metrics&#34;</span>
</span></span><span style=display:flex><span>                insecure: <span style=color:#00f>false</span> <span style=color:green># Ensure server certificate is validated against the CA</span>
</span></span><span style=display:flex><span>                certificate: /etc/cert/ca.crt
</span></span><span style=display:flex><span>                client_certificate: /etc/cert/tls.crt
</span></span><span style=display:flex><span>                client_key: /etc/cert/tls.key
</span></span></code></pre></div><p>The majority of the samples use an prometheus receiver to scrape the collector metrics endpoint, however that is not a clean solution because it puts the metrics via the pipeline, thus consuming resources and potentially causing performance issues. Instead, we use the <code>periodic</code> reader to push the metrics directly to the Prometheus backend.</p><p>Since the <code>k8s-events</code> collector obtains telemetry data from the kube-apiserver, it requires a corresponding set of permissions defined at <a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-collectors/k8s-events-rbac.yaml>k8s-events rbac</a> manifests.</p><h4 id=shoot-metrics-collector><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-collectors/shoot-metrics-otel.yaml>shoot-metrics</a> collector<a class=td-heading-self-link href=#shoot-metrics-collector aria-label="Heading self-link"></a></h4><p>In this example, we have a single receiver:</p><ul><li><a href=https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver>prometheus receiver</a> scraping metrics from Gardener managed exporters present in the shoot cluster, including the <code>kubelet</code> system service metrics. This receiver accepts standard Prometheus scrape configurations using <code>kubernetes_sd_configs</code> to discover the targets dynamically. The <code>kubernetes_sd_configs</code> allows the receiver to discover Kubernetes resources such as pods, nodes, and services, and scrape their metrics endpoints.</li></ul><p>Here, the example illustrates the <code>prometheus</code> receiver scraping metrics from the <code>kubelet</code> service, adding node kubernetes labels as labels to the scraped metrics and filtering the metrics to keep only the relevant ones. Since the kubelet metrics endpoint is secured, it needs the corresponding bearer token to be provided in the scrape configuration. The bearer token is automatically mounted in the pod by Kubernetes, allowing the OpenTelemetry collector to authenticate with the kubelet service.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>- job_name: shoot-kube-kubelet
</span></span><span style=display:flex><span>  honor_labels: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  scheme: https
</span></span><span style=display:flex><span>  tls_config:
</span></span><span style=display:flex><span>    insecure_skip_verify: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  metrics_path: /metrics
</span></span><span style=display:flex><span>  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
</span></span><span style=display:flex><span>  kubernetes_sd_configs:
</span></span><span style=display:flex><span>    - role: node
</span></span><span style=display:flex><span>  relabel_configs:
</span></span><span style=display:flex><span>    - source_labels:
</span></span><span style=display:flex><span>        - job
</span></span><span style=display:flex><span>      target_label: __tmp_prometheus_job_name
</span></span><span style=display:flex><span>    - target_label: job
</span></span><span style=display:flex><span>      replacement: kube-kubelet
</span></span><span style=display:flex><span>      action: replace
</span></span><span style=display:flex><span>    - target_label: type
</span></span><span style=display:flex><span>      replacement: shoot
</span></span><span style=display:flex><span>      action: replace
</span></span><span style=display:flex><span>    - source_labels:
</span></span><span style=display:flex><span>        - __meta_kubernetes_node_address_InternalIP
</span></span><span style=display:flex><span>      target_label: instance
</span></span><span style=display:flex><span>      action: replace
</span></span><span style=display:flex><span>    - regex: __meta_kubernetes_node_label_(.+)
</span></span><span style=display:flex><span>      action: labelmap
</span></span><span style=display:flex><span>      replacement: <span style=color:#a31515>&#34;k8s_node_label_$${1}&#34;</span>
</span></span><span style=display:flex><span>  metric_relabel_configs:
</span></span><span style=display:flex><span>    - source_labels:
</span></span><span style=display:flex><span>        - __name__
</span></span><span style=display:flex><span>      regex: ^(kubelet_running_pods|process_max_fds|process_open_fds|kubelet_volume_stats_available_bytes|kubelet_volume_stats_capacity_bytes|kubelet_volume_stats_used_bytes|kubelet_image_pull_duration_seconds_bucket|kubelet_image_pull_duration_seconds_sum|kubelet_image_pull_duration_seconds_count)$
</span></span><span style=display:flex><span>      action: keep
</span></span><span style=display:flex><span>    - source_labels:
</span></span><span style=display:flex><span>        - namespace
</span></span><span style=display:flex><span>      regex: (^$|^kube-system$)
</span></span><span style=display:flex><span>      action: keep
</span></span></code></pre></div><p>The collector also illustrates collecting metrics from <code>cadvisor</code> endpoints and Gardener specific exporters such as
<code>shoot-apiserver-proxy</code>, <code>shoot-coredns</code>, etc. The exporters usually reside in the <code>kube-system</code> namespace and are configured to expose metrics on a specific port.</p><p>Since we aimed at unified set of resources attribues accross all telemetry data, we can translate exporters metrics which do not conform the conventions in OpenTelemetry.
Here is an example of translating the metrics, produced by the kubelet, to the OpenTelemetry conventions using the <code>transform/metrics</code> processor:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green># Convert Prometheus metrics names to OpenTelemetry metrics names</span>
</span></span><span style=display:flex><span>transform/metrics:
</span></span><span style=display:flex><span>  error_mode: ignore
</span></span><span style=display:flex><span>  metric_statements:
</span></span><span style=display:flex><span>    - context: datapoint
</span></span><span style=display:flex><span>      statements:
</span></span><span style=display:flex><span>        - set(attributes[&#34;k8s.container.name&#34;], attributes[&#34;container&#34;]) where attributes[&#34;container&#34;] != nil
</span></span><span style=display:flex><span>        - delete_key(attributes, &#34;container&#34;) where attributes[&#34;container&#34;] != nil
</span></span><span style=display:flex><span>        - set(attributes[&#34;k8s.pod.name&#34;], attributes[&#34;pod&#34;]) where attributes[&#34;pod&#34;] != nil
</span></span><span style=display:flex><span>        - delete_key(attributes, &#34;pod&#34;) where attributes[&#34;pod&#34;] != nil
</span></span><span style=display:flex><span>        - set(attributes[&#34;k8s.namespace.name&#34;], attributes[&#34;namespace&#34;]) where attributes[&#34;namespace&#34;] != nil
</span></span><span style=display:flex><span>        - delete_key(attributes, &#34;namespace&#34;) where attributes[&#34;namespace&#34;] != nil
</span></span></code></pre></div><p>Here is a visualization of <code>container_network_transmit_bytes_total</code> metric collected from the <code>cadvisor</code> endpoint of the <code>kubelet</code> service, showing the network traffic in bytes transmitted by the <code>vpn-shoot</code> containers.
<img src=/blog/2025/06/images/otel-prometheus.png alt=otel-prometheus></p><p>Similarly to the <code>k8s-events</code> collector, the <code>shoot-metrics</code> collector also emits its own telemetry data, including metrics and logs. The collector is configured to push its own metrics to the Prometheus backend using the <code>periodic</code> reader, avoiding the need for a separate Prometheus scrape configuration. It requires a corresponding set of permissions defined at <a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-collectors/shoot-metrics-rbac.yaml>shoot-metrics rbac</a> manifest.</p><h2 id=summary>Summary<a class=td-heading-self-link href=#summary aria-label="Heading self-link"></a></h2><p>In this blog post, we have explored how to set up an OpenTelemetry based observability stack on a Gardener Shoot Cluster. We have demonstrated how to deploy the OpenTelemetry Operator, configure the backends prometheus and victoria-logs), and deploy OpenTelemetry collectors to obtain telemetry data from the cluster. We have also discussed best practices for configuration, including security and performance considerations. In this blog we have shown the unified set of resource attributes that can be used to identify the source of the telemetry data, allowing correlation and analysis across different teams and systems. We have demonstrated how to transform metrics labels which do not conform to the OpenTelemetry conventions, achieving a unified set of labels across all telemetry data. Finally, we have illustrated how to securely connect the OpenTelemetry collectors to the backends using mTLS and <code>kube-rbac-proxy</code> for authentication and authorization.</p><p>We hope this guide will inspire you to get started with OpenTelemetry on a Gardener managed shoot cluster and equip you with ideas and best practices for building a powerful observability stack that meets your needs. For more information, please refer to the <a href=https://opentelemetry.io/docs/>OpenTelemetry documentation</a> and the <a href=https://gardener.cloud/docs/>Gardener documentation</a>.</p><h2 id=manifests-list>Manifests List<a class=td-heading-self-link href=#manifests-list aria-label="Heading self-link"></a></h2><ul><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-prometheus/certificate.yaml>Prometheus certificate</a></p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-prometheus/prometheus.yaml>Prometheus workload</a></p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-prometheus/prometheus-config.yml>Prometheus configuration</a></p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-prometheus/mtls-rbac.yaml>Prometheus mTLS proxy rbac</a></p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-prometheus/mtls-ra.yml>Prometheus mTLS proxy resource-attributes</a></p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-victorialogs/certificate.yaml>Victoria-Logs certificate</a></p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-victorialogs/values.yaml>Victoria-Logs helm chart values</a></p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-victorialogs/mtls-rbac.yaml>Victoria-Logs mTLS proxy rbac</a></p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-victorialogs/mtls-ra.yml>Victoria-Logs mTLS proxy resource-attributes</a></p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-collectors/otel-certificate.yaml>OpenTelemetry collectors client certificate</a></p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-collectors/k8s-events-otel.yaml>K8S-Events collector</a></p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-collectors/k8s-events-rbac.yaml>K8S-Events rbac</a></p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-collectors/shoot-metrics-otel.yaml>Shoot-Metrics collector</a></p></li><li><p><a href=https://github.com/gardener/documentation/blob/master/website/blog/2025/06/manifests/otel-collectors/shoot-metrics-rbac.yaml>Shoot-Metrics rbac</a></p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-972a525e60e64cc80b18520614b6e451>Taking Gardener to the Next Level: Highlights from the 7th Gardener Community Hackathon in Schelklingen</h1><div class="td-byline mb-4"><time datetime=2025-06-17 class=text-body-secondary>Tuesday, June 17, 2025</time></div><h1 id=taking-gardener-to-the-next-level-highlights-from-the-7th-gardener-community-hackathon-in-schelklingen>Taking Gardener to the Next Level: Highlights from the 7th Gardener Community Hackathon in Schelklingen<a class=td-heading-self-link href=#taking-gardener-to-the-next-level-highlights-from-the-7th-gardener-community-hackathon-in-schelklingen aria-label="Heading self-link"></a></h1><p>The latest &ldquo;Hack The Garden&rdquo; event, held in June 2025 at <a href=https://schlosshof-info.de/>Schlosshof in Schelklingen</a>, brought together members of the Gardener community for an intensive week of collaboration, coding, and problem-solving.
The hackathon focused on a wide array of topics aimed at enhancing Gardener&rsquo;s capabilities, modernizing its stack, and improving user experience.
You can find a <a href=https://github.com/gardener-community/hackathon/blob/main/2025-06_Schelklingen/README.md>full summary of all topics on GitHub</a> and watch the <a href=https://youtu.be/TCLXovw43HA>wrap-up presentations on YouTube</a>.</p><p><img src=/blog/2025/06/images/group-picture.png alt="Group picture of the 7th Hack the Garden event in Schelklingen"></p><p>Here&rsquo;s a look at some of the key achievements and ongoing efforts:</p><h2 id=-modernizing-core-infrastructure-and-networking>🚀 Modernizing Core Infrastructure and Networking<a class=td-heading-self-link href=#-modernizing-core-infrastructure-and-networking aria-label="Heading self-link"></a></h2><p>A significant focus was on upgrading and refining Gardener&rsquo;s foundational components.</p><p>One major undertaking was the <strong>replacement of <a href=https://openvpn.net/>OpenVPN</a> with <a href=https://www.wireguard.com/>Wireguard</a></strong> (<a href="https://youtu.be/TCLXovw43HA?t=104s">watch presentation</a>).
The goal is to modernize the VPN stack for communication between control and data planes, leveraging Wireguard&rsquo;s reputed performance and simplicity.
OpenVPN, while established, presents challenges like TCP-in-TCP. The team developed a Proof of Concept (POC) for a Wireguard-based VPN connection for a single shoot in a local setup, utilizing a reverse proxy like <a href=https://github.com/apernet/mwgp>mwgp</a> to manage connections without needing a load balancer per control plane.
A <a href=https://github.com/axel7born/vpn2/blob/wireguard/docs/wireguard.md>document describing the approach</a> is available.
Next steps involve thorough testing of resilience and throughput, aggregating secrets for MWGP configuration, and exploring ways to update MWGP configuration without restarts.
Code contributions can be found in forks of <a href=https://github.com/axel7born/gardener/tree/wireguard>gardener</a>, <a href=https://github.com/axel7born/vpn2/tree/wireguard>vpn2</a>, and <a href=https://github.com/majst01/mwgp>mwgp</a>.</p><p>Another critical area is <strong>overcoming the 450 <code>Node</code> limit on Azure</strong> (<a href="https://youtu.be/TCLXovw43HA?t=3076s">watch presentation</a>).
Current Azure networking for Gardener relies on route tables, which have size limitations.
The team analyzed the hurdles and discussed a potential solution involving a combination of route tables and virtual networks.
Progress here depends on an upcoming Azure preview feature.</p><p>The hackathon also saw progress on <strong>cluster-internal L7 Load-Balancing for <code>kube-apiserver</code>s</strong>.
Building on previous work for external endpoints, this initiative aims to provide L7 load-balancing for internal traffic from components like <code>gardener-resource-manager</code>.
Achievements include an implementation leveraging generic token kubeconfig and a dedicated ClusterIP service for Istio ingress gateway pods.
The <a href=https://github.com/gardener/gardener/pull/12260>PR #12260</a> is awaiting review to merge this improvement, addressing <a href=https://github.com/gardener/gardener/issues/8810>issue #8810</a>.</p><h2 id=-enhancing-observability-and-operations>🔭 Enhancing Observability and Operations<a class=td-heading-self-link href=#-enhancing-observability-and-operations aria-label="Heading self-link"></a></h2><p>Improving how users monitor and manage Gardener clusters was another key theme.</p><p>A significant step towards Gardener&rsquo;s <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/34-observability2.0-opentelemtry-operator-and-collectors.md>Observability 2.0 initiative</a> was made with the <strong>OpenTelemetry Transport for <code>Shoot</code> Metrics</strong> (<a href="https://youtu.be/TCLXovw43HA?t=808s">watch presentation</a>).
The current method of collecting shoot metrics via the Kubernetes API server <code>/proxy</code> endpoint lacks fine-tuning capabilities.
The hackathon proved the viability of collecting and filtering shoot metrics via OpenTelemetry collector instances on shoots, transporting them to Prometheus OTLP ingestion endpoints on seeds. This allows for more flexible and modern metrics collection.</p><p>For deeper network insights, the <strong>Cluster Network Observability</strong> project (<a href="https://youtu.be/TCLXovw43HA?t=1342s">watch presentation</a>) enhanced the <a href=https://github.com/microsoft/retina>Retina tool</a> by Microsoft.
The team successfully added labeling for source and destination availability zones to Retina&rsquo;s traffic metrics (<a href=https://github.com/microsoft/retina/issues/1654>see issue #1654</a> and <a href=https://github.com/microsoft/retina/pull/1657>PR #1657</a>).
This will help identify cross-AZ traffic, potentially reducing costs and latency.</p><p>To support lightweight deployments, efforts were made to <strong>make <code>gardener-operator</code> Single-Node Ready</strong> (<a href="https://youtu.be/TCLXovw43HA?t=511s">watch presentation</a>).
This involved making several components, including Prometheus deployments, configurable to reduce resource overhead in single-node or bare-metal scenarios.
Relevant PRs include those for <a href=https://github.com/gardener/gardener-extension-provider-gcp/pull/1052>gardener-extension-provider-gcp #1052</a>, <a href=https://github.com/gardener/gardener-extension-provider-openstack/pull/1042>gardener-extension-provider-openstack #1042</a>, <a href=https://github.com/fluent/fluent-operator/pull/1616>fluent-operator #1616</a>, and <a href=https://github.com/gardener/gardener/pull/12248>gardener #12248</a>, along with fixes in forked Cortex and Vali repositories.</p><p>Streamlining node management was the focus of the <strong>Worker Group Node Roll-out</strong> project (<a href="https://youtu.be/TCLXovw43HA?t=3806s">watch presentation</a>).
A PoC was created (see <a href=https://github.com/rrhubenov/gardener/tree/worker-pool-rollout>rrhubenov/gardener branch</a>) allowing users to trigger a node roll-out for specific worker groups via a shoot annotation (<code>gardener.cloud/operation=rollout-workers=&lt;pool-names></code>), which is particularly useful for scenarios like dual-stack migration.</p><p>Proactive workload management is the aim of the <strong>Instance Scheduled Events Watcher</strong> (<a href="https://youtu.be/TCLXovw43HA?t=4010s">watch presentation</a>).
This initiative seeks to create an agent that monitors cloud provider VM events (like reboots or retirements) and exposes them as node events or dashboard warnings.
A <a href=https://github.com/kubernetes-sigs/cloud-provider-azure/pull/9170>PR #9170 for cloud-provider-azure</a> was raised to enable this for Azure, allowing users to take timely action.</p><h2 id=-bolstering-security-and-resource-management>🛡️ Bolstering Security and Resource Management<a class=td-heading-self-link href=#-bolstering-security-and-resource-management aria-label="Heading self-link"></a></h2><p>Security and efficient resource handling remain paramount.</p><p>The <strong>Signing of <code>ManagedResource</code> Secrets</strong> project (<a href="https://youtu.be/TCLXovw43HA?t=1556s">watch presentation</a>) addressed a potential privilege escalation vector.
A PoC demonstrated that signing <code>ManagedResource</code> secrets with a key known only to the Gardener Resource Manager (GRM) is feasible, allowing GRM to verify secret integrity.
This work is captured in <a href=https://github.com/gardener/gardener/pull/12247>gardener PR #12247</a>.</p><p>Simplifying operations was the goal of <strong>Migrating Control Plane Reconciliation of Provider Extensions to <code>ManagedResource</code>s</strong> (<a href="https://youtu.be/TCLXovw43HA?t=1785s">watch presentation</a>). Instead of using the chart applier, this change wraps control-plane components in <code>ManagedResource</code>s, improving scalability and automation (e.g., scaling components imperatively).
<a href=https://github.com/gardener/gardener/pull/12251>Gardener PR #12251</a> was created for this, with a stretch goal related to <a href=https://github.com/gardener/gardener/issues/12250>issue #12250</a> explored in a <a href=https://github.com/gardener/gardener/compare/master...metal-stack:gardener:controlplane-objects-provider-interface>compare branch</a>.</p><p>A quick win, marked as a 🏎️ fast-track item, was to <strong>Expose EgressCIDRs in the shoot-info <code>ConfigMap</code></strong> (<a href="https://youtu.be/TCLXovw43HA?t=2961s">watch presentation</a>).
This makes egress CIDRs available to workloads within the shoot cluster, useful for controllers like <a href=https://www.crossplane.io/>Crossplane</a>.
This was implemented and merged during the hackathon via <a href=https://github.com/gardener/gardener/pull/12252>gardener PR #12252</a>.</p><h2 id=-improving-user-and-developer-experience>✨ Improving User and Developer Experience<a class=td-heading-self-link href=#-improving-user-and-developer-experience aria-label="Heading self-link"></a></h2><p>Enhancing the usability of Gardener tools is always a priority.</p><p>The <strong>Dashboard Usability Improvements</strong> project (<a href="https://youtu.be/TCLXovw43HA?t=2052s">watch presentation</a>) tackled several areas based on <a href=https://github.com/gardener/dashboard/issues/2469>dashboard issue #2469</a>. Achievements include:</p><ul><li>Allowing custom display names for projects via annotations (<a href=https://github.com/gardener/dashboard/pull/2470>dashboard PR #2470</a>).</li><li>Configurable default values for Shoot creation, like AutoScaler min/max replicas (<a href=https://github.com/gardener/dashboard/pull/2476>dashboard PR #2476</a>).</li><li>The ability to hide certain UI elements, such as Control Plane HA options (<a href=https://github.com/gardener/dashboard/pull/2478>dashboard PR #2478</a>).</li></ul><p>The <strong>Documentation Revamp</strong> (<a href="https://youtu.be/TCLXovw43HA?t=2551s">watch presentation</a>) aimed to improve the structure and discoverability of Gardener&rsquo;s documentation.
Metadata for pages was enhanced (<a href=https://github.com/gardener/documentation/pull/652>documentation PR #652</a>), the glossary was expanded (<a href=https://github.com/gardener/documentation/pull/653>documentation PR #653</a>), and a PoC for using VitePress as a more modern documentation generator was created.</p><h2 id=-advancing-versioning-and-deployment-strategies>🔄 Advancing Versioning and Deployment Strategies<a class=td-heading-self-link href=#-advancing-versioning-and-deployment-strategies aria-label="Heading self-link"></a></h2><p>Flexibility in managing Gardener versions and deployments was also explored.</p><p>The topic of <strong>Multiple Parallel Versions in a Gardener Landscape</strong> (formerly Canary Deployments) (<a href="https://youtu.be/TCLXovw43HA?t=3482s">watch presentation</a>) investigated ways to overcome tight versioning constraints.
It was discovered that the current implementation already allows rolling out different extension versions across different seeds using controller registration seat selectors.
Further discussion is needed on some caveats, particularly around the <code>primary</code> field in <code>ControllerRegistration</code> resources.</p><p>Progress was also made on <strong>GEP-32 – Version Classification Lifecycles</strong> (🏎️ fast-track).
This initiative, started in a previous hackathon, aims to automate version lifecycle management.
The previous PR (<a href=https://github.com/metal-stack/gardener/pull/9>metal-stack/gardener #9</a>) was rebased and broken into smaller, more reviewable PRs.</p><h2 id=-conclusion>🌱 Conclusion<a class=td-heading-self-link href=#-conclusion aria-label="Heading self-link"></a></h2><p>The Hack The Garden event in Schelklingen was a testament to the community&rsquo;s dedication and collaborative spirit.
Numerous projects saw significant progress, from PoCs for major architectural changes to practical improvements in daily operations and user experience.
Many of these efforts are now moving into further development, testing, and review, promising exciting enhancements for the Gardener ecosystem.</p><p>Stay tuned for more updates as these projects mature and become integrated into Gardener!</p><p>The next hackathon takes place in early December 2025.
If you&rsquo;d like to join, head over to the <a href=https://join.slack.com/t/gardener-cloud/shared_invite/zt-33c9daems-3oOorhnqOSnldZPWqGmIBw>Gardener Slack</a>.
Happy to meet you there! ✌️</p></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://demo.gardener.cloud>Demo</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://gardener-cloud.slack.com/><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://x.com/GardenerProject><img src=/images/branding/x-logo-white.svg class=media-icon><div class=media-text>X</div></a></li></ul><span class=copyright>Copyright 2019-2025 Gardener project authors<div style=margin-top:2%><a href=https://www.sap.com/about/legal/terms-of-use.html>Terms of Use
<i class="fa fa-external-link" aria-hidden=true></i>
</a>|
<a href=https://www.sap.com/about/legal/privacy.html>Privacy Statement
<i class="fa fa-external-link" aria-hidden=true></i>
</a>|
<a href=https://www.sap.com/about/legal/impressum.html>Legal Disclosure
<i class="fa fa-external-link" aria-hidden=true></i></a></div></span></div></footer></div><script src=/js/main.min.403ff095218c662472dab60ed98ecbb19431682de5ac7c6159891241cd366af5.js integrity="sha256-QD/wlSGMZiRy2rYO2Y7LsZQxaC3lrHxhWYkSQc02avU=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script><script src=/js/navbar.js></script><script src=/js/filtering.js></script><script src=/js/page-content.js></script><script src=/js/community-index.js></script></body></html>