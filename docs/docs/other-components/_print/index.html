<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://gardener.cloud/docs/other-components/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/other-components/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Other Components | Gardener</title>
<meta name=description content="Other components included in the Gardener project"><meta property="og:url" content="https://gardener.cloud/docs/other-components/"><meta property="og:site_name" content="Gardener"><meta property="og:title" content="Other Components"><meta property="og:description" content="Other components included in the Gardener project"><meta property="og:locale" content="en_US"><meta property="og:type" content="website"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Other Components"><meta itemprop=description content="Other components included in the Gardener project"><meta itemprop=image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Other Components"><meta name=twitter:description content="Other components included in the Gardener project"><link rel=preload href=/scss/main.min.52d93bcd446cd63373e35bf3f68b78c63f08370d066ccb303bdd0e25f237661c.css as=style integrity="sha256-Utk7zURs1jNz41vz9ot4xj8INw0GbMswO90OJfI3Zhw=" crossorigin=anonymous><link href=/scss/main.min.52d93bcd446cd63373e35bf3f68b78c63f08370d066ccb303bdd0e25f237661c.css rel=stylesheet integrity="sha256-Utk7zURs1jNz41vz9ot4xj8INw0GbMswO90OJfI3Zhw=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><style>.nav-link:hover{text-decoration:none}.ml-md-auto{margin-left:auto!important}.td-search__icon{color:#fff!important}.td-search__input.form-control::placeholder{color:#fff;border:1px;border-radius:20px}.td-search__input.form-control{border:1px;border-radius:20px}.td-search__input{max-width:90%}.td-search:not(:focus-within){color:#fff!important}</style><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://demo.gardener.cloud target=_blank><span>Demo</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><div class=dropdown><a href=/docs class=nav-link>Documentation</a><div class=dropdown-content><a class=taxonomy-term href=/docs>Users</a>
<a class=taxonomy-term href=/docs>Operators</a>
<a class=taxonomy-term href=/docs>Developers</a>
<a class=taxonomy-term href=/docs>All</a></div></div></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.dad36e389dc8a051272c170f0795a20c.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/other-components/>Return to the regular view of this page</a>.</p></div><h1 class=title>Other Components</h1><div class=lead>Other components included in the Gardener project</div><div class=content></div></div><div class=td-content><h1 id=pg-7c8e5176ea4b0efeb882ff3fe9745e14>1 - Dependency Watchdog</h1><div class=lead>A watchdog which actively looks out for disruption and recovery of critical services</div><h1 id=dependency-watchdog>Dependency Watchdog</h1><img src=/__resources/gardener-dwd_833554.png style=width:200px><p><a href=https://api.reuse.software/info/github.com/gardener/dependency-watchdog><img src=https://api.reuse.software/badge/github.com/gardener/dependency-watchdog alt="REUSE status"></a>
<a href=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/dependency-watchdog-master/jobs/master-head-update-job/><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/dependency-watchdog-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://testgrid.k8s.io/q/summary/gardener-dependency-watchdog/ci-dependency-watchdog-unit/tests_status><img src="https://testgrid.k8s.io/q/summary/gardener-dependency-watchdog/ci-dependency-watchdog-unit/tests_status?style=svg" alt="Unit Tests"></a>
<a href=https://goreportcard.com/report/github.com/gardener/dependency-watchdog><img src=https://goreportcard.com/badge/github.com/gardener/dependency-watchdog alt="Go Report Card"></a>
<a href=https://pkg.go.dev/github.com/gardener/dependency-watchdog><img src=https://godoc.org/github.com/gardener/dependency-watchdog?status.svg alt=GoDoc></a></p><h2 id=overview>Overview</h2><p>A watchdog which actively looks out for disruption and recovery of critical services. If there is a disruption then it will prevent cascading failure by conservatively scaling down dependent configured resources and if a critical service has just recovered then it will expedite the recovery of dependent services/pods.</p><p>Avoiding cascading failure is handled by <a href=/docs/other-components/dependency-watchdog/concepts/prober/>Prober</a> component and expediting recovery of dependent services/pods is handled by <a href=/docs/other-components/dependency-watchdog/concepts/weeder/>Weeder</a> component. These are separately deployed as individual pods.</p><h3 id=current-limitation--future-scope>Current Limitation & Future Scope</h3><p>Although in the current offering the <code>Prober</code> is tailored to handle one such use case of <code>kube-apiserver</code> connectivity, but the usage of prober can be extended to solve similar needs for other scenarios where the components involved might be different.</p><h2 id=start-using-or-developing-the-dependency-watchdog>Start using or developing the Dependency Watchdog</h2><p>See our documentation in the /docs repository, please <a href=https://github.com/gardener/dependency-watchdog/blob/master/docs/README.md>find the index here</a>.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>We always look forward to active community engagement.</p><p>Please report bugs or suggestions on how we can enhance <code>dependency-watchdog</code> to address additional recovery scenarios on <a href=https://github.com/gardener/dependency-watchdog/issues>GitHub issues</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-75a0184996815280264f682a2b501338>1.1 - Concepts</h1></div><div class=td-content><h1 id=pg-37f31312dc8095d109349c3174ff97f5>1.1.1 - Prober</h1><h1 id=prober>Prober</h1><h2 id=overview>Overview</h2><p>Prober starts asynchronous and periodic probes for every shoot cluster. The first probe is the api-server probe which checks the reachability of the API Server from the control plane. The second probe is the lease probe which is done after the api server probe is successful and checks if the number of expired node leases is below a certain threshold.
If the lease probe fails, it will scale down the dependent kubernetes resources. Once the connectivity to <code>kube-apiserver</code> is reestablished and the number of expired node leases are within the accepted threshold, the prober will then proactively scale up the dependent kubernetes resources it had scaled down earlier. The failure threshold fraction for lease probe
and dependent kubernetes resources are defined in <a href=https://github.com/gardener/dependency-watchdog/blob/master/example/01-dwd-prober-configmap.yaml>configuration</a> that is passed to the prober.</p><h3 id=origin>Origin</h3><p>In a shoot cluster (a.k.a data plane) each node runs a kubelet which periodically renewes its lease. Leases serve as heartbeats informing Kube Controller Manager that the node is alive. The connectivity between the kubelet and the Kube ApiServer can break for different reasons and not recover in time.</p><p>As an example, consider a large shoot cluster with several hundred nodes. There is an issue with a NAT gateway on the shoot cluster which prevents the Kubelet from any node in the shoot cluster to reach its control plane Kube ApiServer. As a consequence, Kube Controller Manager transitioned the nodes of this shoot cluster to <code>Unknown</code> state.</p><p><a href=https://github.com/gardener/machine-controller-manager>Machine Controller Manager</a> which also runs in the shoot control plane reacts to any changes to the Node status and then takes action to recover backing VMs/machine(s). It waits for a grace period and then it will begin to replace the unhealthy machine(s) with new ones.</p><p>This replacement of healthy machines due to a broken connectivity between the worker nodes and the control plane Kube ApiServer results in undesired downtimes for customer workloads that were running on these otherwise healthy nodes. It is therefore required that there be an actor which detects the connectivity loss between the the kubelet and shoot cluster&rsquo;s Kube ApiServer and proactively scales down components in the shoot control namespace which could exacerbate the availability of nodes in the shoot cluster.</p><h2 id=dependency-watchdog-prober-in-gardener>Dependency Watchdog Prober in Gardener</h2><p>Prober is a central component which is deployed in the <code>garden</code> namespace in the seed cluster. Control plane components for a shoot are deployed in a dedicated shoot namespace for the shoot within the seed cluster.</p><img src=/__resources/prober-components.excalidraw_dd1d01.png><blockquote><p>NOTE: If you are not familiar with what gardener components like seed, shoot then please see the <a href=/docs/other-components/dependency-watchdog/concepts/prober/#appendix>appendix</a> for links.</p></blockquote><p>Prober periodically probes Kube ApiServer via two separate probes:</p><ol><li>API Server Probe: Local cluster DNS name which resolves to the ClusterIP of the Kube Apiserver</li><li>Lease Probe: Checks for number of expired leases to be within the specified threshold. The threshold defines the limit after which DWD can say that the kubelets are not able to reach the API server.</li></ol><h2 id=behind-the-scene>Behind the scene</h2><p>For all active shoot clusters (which have not been hibernated or deleted or moved to another seed via control-plane-migration), prober will schedule a probe to run periodically. During each run of a probe it will do the following:</p><ol><li>Checks if the Kube ApiServer is reachable via local cluster DNS. This should always succeed and will fail only when the Kube ApiServer has gone down. If the Kube ApiServer is down then there can be no further damage to the existing shoot cluster (barring new requests to the Kube Api Server).</li><li>Only if the probe is able to reach the Kube ApiServer via local cluster DNS, will it attempt to check the number of expired node leases in the shoot. The node lease renewal is done by the Kubelet, and so we can say that the lease probe is checking if the kubelet is able to reach the API server. If the number of expired node leases reaches
the threshold, then the probe fails.</li><li>If and when a lease probe fails, then it will initiate a scale-down operation for dependent resources as defined in the prober configuration.</li><li>In subsequent runs it will keep performing the lease probe. If it is successful, then it will start the scale-up operation for dependent resources as defined in the configuration.</li></ol><h3 id=prober-lifecycle>Prober lifecycle</h3><p>A reconciler is registered to listen to all events for <a href=/docs/gardener/api-reference/extensions/#extensions.gardener.cloud/v1alpha1.Cluster>Cluster</a> resource.</p><p>When a <code>Reconciler</code> receives a request for a <code>Cluster</code> change, it will query the extension kube-api server to get the <code>Cluster</code> resource.</p><p>In the following cases it will either remove an existing probe for this cluster or skip creating a new probe:</p><ol><li>Cluster is marked for deletion.</li><li>Hibernation has been enabled for the cluster.</li><li>There is an ongoing seed migration for this cluster.</li><li>If a new cluster is created with no workers.</li><li>If an update is made to the cluster by removing all workers (in other words making it worker-less).</li></ol><p>If none of the above conditions are true and there is no existing probe for this cluster then a new probe will be created, registered and started.</p><h3 id=probe-failure-identification>Probe failure identification</h3><p>DWD probe can either be a success or it could return an error. If the API server probe fails, the lease probe is not done and the probes will be retried. If the error is a <code>TooManyRequests</code> error due to requests to the Kube-API-Server being throttled,
then the probes are retried after a backOff of <code>backOffDurationForThrottledRequests</code>.</p><p>If the lease probe fails, then the error could be due to failure in listing the leases. In this case, no scaling operations are performed. If the error in listing the leases is a <code>TooManyRequests</code> error due to requests to the Kube-API-Server being throttled,
then the probes are retried after a backOff of <code>backOffDurationForThrottledRequests</code>.</p><p>If there is no error in listing the leases, then the Lease probe fails if the number of expired leases reaches the threshold fraction specified in the <a href=https://github.com/gardener/dependency-watchdog/blob/master/example/01-dwd-prober-configmap.yaml>configuration</a>.
A lease is considered expired in the following scenario:-</p><pre tabindex=0><code>	time.Now() &gt;= lease.Spec.RenewTime + (p.config.KCMNodeMonitorGraceDuration.Duration * expiryBufferFraction)
</code></pre><p>Here, <code>lease.Spec.RenewTime</code> is the time when current holder of a lease has last updated the lease. <code>config</code> is the probe config generated from the <a href=https://github.com/gardener/dependency-watchdog/blob/master/example/01-dwd-prober-configmap.yaml>configuration</a> and
<code>KCMNodeMonitorGraceDuration</code> is amount of time which KCM allows a running Node to be unresponsive before marking it unhealthy (See <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/#:~:text=Amount%20of%20time%20which%20we%20allow%20running%20Node%20to%20be%20unresponsive%20before%20marking%20it%20unhealthy.%20Must%20be%20N%20times%20more%20than%20kubelet%27s%20nodeStatusUpdateFrequency%2C%20where%20N%20means%20number%20of%20retries%20allowed%20for%20kubelet%20to%20post%20node%20status.">ref</a>)
. <code>expiryBufferFraction</code> is a hard coded value of <code>0.75</code>. Using this fraction allows the prober to intervene before KCM marks a node as unknown, but at the same time allowing kubelet sufficient retries to renew the node lease (Kubelet renews the lease every <code>10s</code> See <a href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#:~:text=The%20lease%20is%20currently%20renewed%20every%2010s%2C%20per%20KEP%2D0009.">ref</a>).</p><h2 id=appendix>Appendix</h2><ul><li><a href=https://github.com/gardener/gardener/blob/master/docs>Gardener</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/14-reversed-cluster-vpn.md>Reverse Cluster VPN</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-68d260224b4548925f479aa69121626d>1.1.2 - Weeder</h1><h1 id=weeder>Weeder</h1><h2 id=overview>Overview</h2><p>Weeder watches for an update to service endpoints and on receiving such an event it will create a time-bound watch for all configured dependent pods that need to be actively recovered in case they have not yet recovered from <code>CrashLoopBackoff</code> state. In a nutshell it accelerates recovery of pods when an upstream service recovers.</p><p>An interference in automatic recovery for dependent pods is required because kubernetes pod restarts a container with an exponential backoff when the pod is in <code>CrashLoopBackOff</code> state. This backoff could become quite large if the service stays down for long. Presence of weeder would not let that happen as it&rsquo;ll restart the pod.</p><h2 id=prerequisites>Prerequisites</h2><p>Before we understand how Weeder works, we need to be familiar with kubernetes <a href=https://kubernetes.io/docs/concepts/services-networking/service/>services & endpoints</a>.</p><blockquote><p>NOTE: If a kubernetes service is created with selectors then kubernetes will create corresponding endpoint resource which will have the same name as that of the service. In weeder implementation service and endpoint name is used interchangeably.</p></blockquote><h2 id=config>Config</h2><p>Weeder can be configured via command line arguments and a weeder configuration. See <a href=/docs/other-components/dependency-watchdog/deployment/configure/#weeder>configure weeder</a>.</p><h2 id=internals>Internals</h2><p>Weeder keeps a watch on the events for the specified endpoints in the config. For every endpoints a list of <code>podSelectors</code> can be specified. It cretes a weeder object per endpoints resource when it receives a satisfactory <code>Create</code> or <code>Update</code> event. Then for every podSelector it creates a goroutine. This goroutine keeps a watch on the pods with labels as per the podSelector and kills any pod which turn into <code>CrashLoopBackOff</code>. Each weeder lives for <code>watchDuration</code> interval which has a default value of 5 mins if not explicitly set.</p><p>To understand the actions taken by the weeder lets use the following diagram as a reference.
<img src=/__resources/weeder-components.excalidraw_931119.png>
Let us also assume the following configuration for the weeder:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>watchDuration: 2m0s
</span></span><span style=display:flex><span>servicesAndDependantSelectors:
</span></span><span style=display:flex><span>  etcd-main-client: <span style=color:green># name of the service/endpoint for etcd statefulset that weeder will receive events for.</span>
</span></span><span style=display:flex><span>    podSelectors: <span style=color:green># all pods matching the label selector are direct dependencies for etcd service</span>
</span></span><span style=display:flex><span>      - matchExpressions:
</span></span><span style=display:flex><span>          - key: gardener.cloud/role
</span></span><span style=display:flex><span>            operator: In
</span></span><span style=display:flex><span>            values:
</span></span><span style=display:flex><span>              - controlplane
</span></span><span style=display:flex><span>          - key: role
</span></span><span style=display:flex><span>            operator: In
</span></span><span style=display:flex><span>            values:
</span></span><span style=display:flex><span>              - apiserver
</span></span><span style=display:flex><span>  kube-apiserver: <span style=color:green># name of the service/endpoint for kube-api-server pods that weeder will receive events for. </span>
</span></span><span style=display:flex><span>    podSelectors: <span style=color:green># all pods matching the label selector are direct dependencies for kube-api-server service</span>
</span></span><span style=display:flex><span>      - matchExpressions:
</span></span><span style=display:flex><span>          - key: gardener.cloud/role
</span></span><span style=display:flex><span>            operator: In
</span></span><span style=display:flex><span>            values:
</span></span><span style=display:flex><span>              - controlplane
</span></span><span style=display:flex><span>          - key: role
</span></span><span style=display:flex><span>            operator: NotIn
</span></span><span style=display:flex><span>            values:
</span></span><span style=display:flex><span>              - main
</span></span><span style=display:flex><span>              - apiserver
</span></span></code></pre></div><p>Only for the sake of demonstration lets pick the first service -> dependent pods tuple (<code>etcd-main-client</code> as the service endpoint).</p><blockquote><ol><li>Assume that there are 3 replicas for etcd statefulset.</li><li>Time here is just for showing the series of events</li></ol></blockquote><ul><li><code>t=0</code> -> all etcd pods go down</li><li><code>t=10</code> -> kube-api-server pods transition to CrashLoopBackOff</li><li><code>t=100</code> -> all etcd pods recover together</li><li><code>t=101</code> -> Weeder sees <code>Update</code> event for <code>etcd-main-client</code> endpoints resource</li><li><code>t=102</code> -> go routine created to keep watch on kube-api-server pods</li><li><code>t=103</code> -> Since kube-api-server pods are still in CrashLoopBackOff, weeder deletes the pods to accelerate the recovery.</li><li><code>t=104</code> -> new kube-api-server pod created by replica-set controller in kube-controller-manager</li></ul><h3 id=points-to-note>Points to Note</h3><ul><li>Weeder only respond on <code>Update</code> events where a <code>notReady</code> endpoints resource turn to <code>Ready</code>. Thats why there was no weeder action at time <code>t=10</code> in the example above.<ul><li><code>notReady</code> -> no backing pod is Ready</li><li><code>Ready</code> -> atleast one backing pod is Ready</li></ul></li><li>Weeder doesn&rsquo;t respond on <code>Delete</code> events</li><li>Weeder will always wait for the entire <code>watchDuration</code>. If the dependent pods transition to CrashLoopBackOff after the watch duration or even after repeated deletion of these pods they do not recover then weeder will exit. Quality of service offered via a weeder is only Best-Effort.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7c43b00ba717958d1829ad1c780cc625>1.2 - Deployment</h1></div><div class=td-content><h1 id=pg-922ffeaf5b813bbead3438c478d3ffa4>1.2.1 - Configure</h1><h1 id=configure-dependency-watchdog-components>Configure Dependency Watchdog Components</h1><h2 id=prober>Prober</h2><p>Dependency watchdog prober command takes command-line-flags which are meant to fine-tune the prober. In addition a <code>ConfigMap</code> is also mounted to the container which provides tuning knobs for the all probes that the prober starts.</p><h3 id=command-line-arguments>Command line arguments</h3><p>Prober can be configured via the following flags:</p><table><thead><tr><th>Flag Name</th><th>Type</th><th>Required</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>kube-api-burst</td><td>int</td><td>No</td><td>10</td><td>Burst to use while talking with kubernetes API server. The number must be >= 0. If it is 0 then a default value of 10 will be used</td></tr><tr><td>kube-api-qps</td><td>float</td><td>No</td><td>5.0</td><td>Maximum QPS (queries per second) allowed when talking with kubernetes API server. The number must be >= 0. If it is 0 then a default value of 5.0 will be used</td></tr><tr><td>concurrent-reconciles</td><td>int</td><td>No</td><td>1</td><td>Maximum number of concurrent reconciles</td></tr><tr><td>config-file</td><td>string</td><td>Yes</td><td>NA</td><td>Path of the config file containing the configuration to be used for all probes</td></tr><tr><td>metrics-bind-addr</td><td>string</td><td>No</td><td>&ldquo;:9643&rdquo;</td><td>The TCP address that the controller should bind to for serving prometheus metrics</td></tr><tr><td>health-bind-addr</td><td>string</td><td>No</td><td>&ldquo;:9644&rdquo;</td><td>The TCP address that the controller should bind to for serving health probes</td></tr><tr><td>enable-leader-election</td><td>bool</td><td>No</td><td>false</td><td>In case prober deployment has more than 1 replica for high availability, then it will be setup in a active-passive mode. Out of many replicas one will become the leader and the rest will be passive followers waiting to acquire leadership in case the leader dies.</td></tr><tr><td>leader-election-namespace</td><td>string</td><td>No</td><td>&ldquo;garden&rdquo;</td><td>Namespace in which leader election resource will be created. It should be the same namespace where DWD pods are deployed</td></tr><tr><td>leader-elect-lease-duration</td><td>time.Duration</td><td>No</td><td>15s</td><td>The duration that non-leader candidates will wait after observing a leadership renewal until attempting to acquire leadership of a led but unrenewed leader slot. This is effectively the maximum duration that a leader can be stopped before it is replaced by another candidate. This is only applicable if leader election is enabled.</td></tr><tr><td>leader-elect-renew-deadline</td><td>time.Duration</td><td>No</td><td>10s</td><td>The interval between attempts by the acting master to renew a leadership slot before it stops leading. This must be less than or equal to the lease duration. This is only applicable if leader election is enabled.</td></tr><tr><td>leader-elect-retry-period</td><td>time.Duration</td><td>No</td><td>2s</td><td>The duration the clients should wait between attempting acquisition and renewal of a leadership. This is only applicable if leader election is enabled.</td></tr></tbody></table><p>You can view an example kubernetes prober <a href=https://github.com/gardener/dependency-watchdog/blob/master/example/03-dwd-prober-deployment.yaml>deployment</a> YAML to see how these command line args are configured.</p><h3 id=prober-configuration>Prober Configuration</h3><p>A probe configuration is mounted as <code>ConfigMap</code> to the container. The path to the config file is configured via <code>config-file</code> command line argument as mentioned above. Prober will start one probe per Shoot control plane hosted within the Seed cluster. Each such probe will run asynchronously and will periodically connect to the Kube ApiServer of the Shoot. Configuration below will influence each such probe.</p><p>You can view an example YAML configuration provided as <code>data</code> in a <code>ConfigMap</code> <a href=https://github.com/gardener/dependency-watchdog/blob/master/example/01-dwd-prober-configmap.yaml>here</a>.</p><table><thead><tr><th>Name</th><th>Type</th><th>Required</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>kubeConfigSecretName</td><td>string</td><td>Yes</td><td>NA</td><td>Name of the kubernetes Secret which has the encoded KubeConfig required to connect to the Shoot control plane Kube ApiServer via an internal domain. This typically uses the local cluster DNS.</td></tr><tr><td>probeInterval</td><td>metav1.Duration</td><td>No</td><td>10s</td><td>Interval with which each probe will run.</td></tr><tr><td>initialDelay</td><td>metav1.Duration</td><td>No</td><td>30s</td><td>Initial delay for the probe to become active. Only applicable when the probe is created for the first time.</td></tr><tr><td>probeTimeout</td><td>metav1.Duration</td><td>No</td><td>30s</td><td>In each run of the probe it will attempt to connect to the Shoot Kube ApiServer. probeTimeout defines the timeout after which a single run of the probe will fail.</td></tr><tr><td>backoffJitterFactor</td><td>float64</td><td>No</td><td>0.2</td><td>Jitter with which a probe is run.</td></tr><tr><td>dependentResourceInfos</td><td>[]prober.DependentResourceInfo</td><td>Yes</td><td>NA</td><td>Detailed below.</td></tr><tr><td>kcmNodeMonitorGraceDuration</td><td>metav1.Duration</td><td>Yes</td><td>NA</td><td>It is the node-monitor-grace-period set in the kcm flags. Used to determine whether a node lease can be considered expired.</td></tr><tr><td>nodeLeaseFailureFraction</td><td>float64</td><td>No</td><td>0.6</td><td>is used to determine the maximum number of leases that can be expired for a lease probe to succeed.</td></tr></tbody></table><h3 id=dependentresourceinfo>DependentResourceInfo</h3><p>If a lease probe fails, then it scales down the dependent resources defined by this property. Similarly, if the lease probe is now successful, then it scales up the dependent resources defined by this property.</p><p>Each dependent resource info has the following properties:</p><table><thead><tr><th>Name</th><th>Type</th><th>Required</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>ref</td><td>autoscalingv1.CrossVersionObjectReference</td><td>Yes</td><td>NA</td><td>It is a collection of ApiVersion, Kind and Name for a kubernetes resource thus serving as an identifier.</td></tr><tr><td>optional</td><td>bool</td><td>Yes</td><td>NA</td><td>It is possible that a dependent resource is optional for a Shoot control plane. This property enables a probe to determine the correct behavior in case it is unable to find the resource identified via <code>ref</code>.</td></tr><tr><td>scaleUp</td><td>prober.ScaleInfo</td><td>No</td><td></td><td>Captures the configuration to scale up this resource. Detailed below.</td></tr><tr><td>scaleDown</td><td>prober.ScaleInfo</td><td>No</td><td></td><td>Captures the configuration to scale down this resource. Detailed below.</td></tr></tbody></table><blockquote><p>NOTE: Since each dependent resource is a target for scale up/down, therefore it is mandatory that the resource reference points a kubernetes resource which has a <code>scale</code> subresource.</p></blockquote><h3 id=scaleinfo>ScaleInfo</h3><p>How to scale a <code>DependentResourceInfo</code> is captured in <code>ScaleInfo</code>. It has the following properties:</p><table><thead><tr><th>Name</th><th>Type</th><th>Required</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>level</td><td>int</td><td>Yes</td><td>NA</td><td>Detailed below.</td></tr><tr><td>initialDelay</td><td>metav1.Duration</td><td>No</td><td>0s (No initial delay)</td><td>Once a decision is taken to scale a resource then via this property a delay can be induced before triggering the scale of the dependent resource.</td></tr><tr><td>timeout</td><td>metav1.Duration</td><td>No</td><td>30s</td><td>Defines the timeout for the scale operation to finish for a dependent resource.</td></tr></tbody></table><p><strong>Determining target replicas</strong></p><p>Prober cannot assume any target replicas during a scale-up operation for the following reasons:</p><ol><li>Kubernetes resources could be set to provide highly availability and the number of replicas could wary from one shoot control plane to the other. In gardener the number of replicas of pods in shoot namespace are controlled by the <a href=https://github.com/gardener/gardener/blob/master/docs/usage/shoot_high_availability.md>shoot control plane configuration</a>.</li><li>If Horizontal Pod Autoscaler has been configured for a kubernetes dependent resource then it could potentially change the <code>spec.replicas</code> for a deployment/statefulset.</li></ol><p>Given the above constraint lets look at how prober determines the target replicas during scale-down or scale-up operations.</p><ol><li><p><code>Scale-Up</code>: Primary responsibility of a probe while performing a scale-up is to restore the replicas of a kubernetes dependent resource prior to scale-down. In order to do that it updates the following for each dependent resource that requires a scale-up:</p><ol><li><code>spec.replicas</code>: Checks if <code>dependency-watchdog.gardener.cloud/replicas</code> is set. If it is, then it will take the value stored against this key as the target replicas. To be a valid value it should always be greater than 0.</li><li>If <code>dependency-watchdog.gardener.cloud/replicas</code> annotation is not present then it falls back to the hard coded default value for scale-up which is set to 1.</li><li>Removes the annotation <code>dependency-watchdog.gardener.cloud/replicas</code> if it exists.</li></ol></li><li><p><code>Scale-Down</code>: To scale down a dependent kubernetes resource it does the following:</p><ol><li>Adds an annotation <code>dependency-watchdog.gardener.cloud/replicas</code> and sets its value to the current value of <code>spec.replicas</code>.</li><li>Updates <code>spec.replicas</code> to 0.</li></ol></li></ol><p><strong>Level</strong></p><p>Each dependent resource that should be scaled up or down is associated to a level. Levels are ordered and processed in ascending order (starting with 0 assigning it the highest priority). Consider the following configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>dependentResourceInfos:
</span></span><span style=display:flex><span>  - ref: 
</span></span><span style=display:flex><span>      kind: <span style=color:#a31515>&#34;Deployment&#34;</span>
</span></span><span style=display:flex><span>      name: <span style=color:#a31515>&#34;kube-controller-manager&#34;</span>
</span></span><span style=display:flex><span>      apiVersion: <span style=color:#a31515>&#34;apps/v1&#34;</span>
</span></span><span style=display:flex><span>    scaleUp: 
</span></span><span style=display:flex><span>      level: 1 
</span></span><span style=display:flex><span>    scaleDown: 
</span></span><span style=display:flex><span>      level: 0 
</span></span><span style=display:flex><span>  - ref:
</span></span><span style=display:flex><span>      kind: <span style=color:#a31515>&#34;Deployment&#34;</span>
</span></span><span style=display:flex><span>      name: <span style=color:#a31515>&#34;machine-controller-manager&#34;</span>
</span></span><span style=display:flex><span>      apiVersion: <span style=color:#a31515>&#34;apps/v1&#34;</span>
</span></span><span style=display:flex><span>    scaleUp:
</span></span><span style=display:flex><span>      level: 1
</span></span><span style=display:flex><span>    scaleDown:
</span></span><span style=display:flex><span>      level: 1
</span></span><span style=display:flex><span>  - ref:
</span></span><span style=display:flex><span>      kind: <span style=color:#a31515>&#34;Deployment&#34;</span>
</span></span><span style=display:flex><span>      name: <span style=color:#a31515>&#34;cluster-autoscaler&#34;</span>
</span></span><span style=display:flex><span>      apiVersion: <span style=color:#a31515>&#34;apps/v1&#34;</span>
</span></span><span style=display:flex><span>    scaleUp:
</span></span><span style=display:flex><span>      level: 0
</span></span><span style=display:flex><span>    scaleDown:
</span></span><span style=display:flex><span>      level: 2
</span></span></code></pre></div><p>Let us order the dependent resources by their respective levels for both scale-up and scale-down. We get the following order:</p><p><em>Scale Up Operation</em></p><p>Order of scale up will be:</p><ol><li>cluster-autoscaler</li><li>kube-controller-manager and machine-controller-manager will be scaled up concurrently after cluster-autoscaler has been scaled up.</li></ol><p><em>Scale Down Operation</em></p><p>Order of scale down will be:</p><ol><li>kube-controller-manager</li><li>machine-controller-manager after (1) has been scaled down.</li><li>cluster-autoscaler after (2) has been scaled down.</li></ol><h3 id=disableignore-scaling>Disable/Ignore Scaling</h3><p>A probe can be configured to ignore scaling of configured dependent kubernetes resources.
To do that one must set <code>dependency-watchdog.gardener.cloud/ignore-scaling</code> annotation to <code>true</code> on the scalable resource for which scaling should be ignored.</p><h2 id=weeder>Weeder</h2><p>Dependency watchdog weeder command also (just like the prober command) takes command-line-flags which are meant to fine-tune the weeder. In addition a <code>ConfigMap</code> is also mounted to the container which helps in defining the dependency of pods on endpoints.</p><h3 id=command-line-arguments-1>Command Line Arguments</h3><p>Weeder can be configured with the same flags as that for prober described under <a href=/docs/other-components/dependency-watchdog/deployment/configure/#command-line-arguments>command-line-arguments</a> section
You can find an example weeder <a href=https://github.com/gardener/dependency-watchdog/blob/master/example/04-dwd-weeder-deployment.yaml>deployment</a> YAML to see how these command line args are configured.</p><h3 id=weeder-configuration>Weeder Configuration</h3><p>Weeder configuration is mounted as <code>ConfigMap</code> to the container. The path to the config file is configured via <code>config-file</code> command line argument as mentioned above. Weeder will start one go routine per podSelector per endpoint on an endpoint event as described in <a href=/docs/other-components/dependency-watchdog/concepts/weeder/#internals>weeder internal concepts</a>.</p><p>You can view the example YAML configuration provided as <code>data</code> in a <code>ConfigMap</code> <a href=https://github.com/gardener/dependency-watchdog/blob/master/example/02-dwd-weeder-configmap.yaml>here</a>.</p><table><thead><tr><th>Name</th><th>Type</th><th>Required</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>watchDuration</td><td>*metav1.Duration</td><td>No</td><td>5m0s</td><td>The time duration for which watch is kept on dependent pods to see if anyone turns to <code>CrashLoopBackoff</code></td></tr><tr><td>servicesAndDependantSelectors</td><td>map[string]DependantSelectors</td><td>Yes</td><td>NA</td><td>Endpoint name and its corresponding dependent pods. More info below.</td></tr></tbody></table><h3 id=dependantselectors>DependantSelectors</h3><p>If the service recovers from downtime, then weeder starts to watch for CrashLoopBackOff pods. These pods are identified by info stored in this property.</p><table><thead><tr><th>Name</th><th>Type</th><th>Required</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>podSelectors</td><td>[]*metav1.LabelSelector</td><td>Yes</td><td>NA</td><td>This is a list of <a href=https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1@v0.24.3#LabelSelector>Label selector</a></td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-f28b0a628f77e282807d7fe76d71d2a7>1.2.2 - Monitor</h1><h1 id=monitoring>Monitoring</h1><h2 id=work-in-progress><em>Work In Progress</em></h2><p>We will be introducing metrics for <code>Dependency-Watchdog-Prober</code> and <code>Dependency-Watchdog-Weeder</code>. These metrics will be pushed to prometheus. Once that is completed we will provide details on all the metrics that will be supported here.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9ca8f65686a5d016a3852ad7e840d061>1.3 - Contribution</h1><h1 id=how-to-contribute>How to contribute?</h1><p>Contributions are always welcome!</p><p>In order to contribute ensure that you have the development environment setup and you familiarize yourself with required steps to build, verify-quality and test.</p><h2 id=setting-up-development-environment>Setting up development environment</h2><h3 id=installing-go>Installing Go</h3><p>Minimum Golang version required: <code>1.18</code>.
On MacOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install go
</span></span></code></pre></div><p>For other OS, follow the <a href=https://go.dev/doc/install>installation instructions</a>.</p><h3 id=installing-git>Installing Git</h3><p>Git is used as version control for dependency-watchdog. On MacOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install git
</span></span></code></pre></div><p>If you do not have git installed already then please follow the <a href=https://git-scm.com/downloads>installation instructions</a>.</p><h3 id=installing-docker>Installing Docker</h3><p>In order to test dependency-watchdog containers you will need a local kubernetes setup. Easiest way is to first install Docker. This becomes a pre-requisite to setting up either a vanilla KIND/minikube cluster or a local Gardener cluster.</p><p>On MacOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install -cash docker
</span></span></code></pre></div><p>For other OS, follow the <a href=https://docs.docker.com/get-docker/>installation instructions</a>.</p><h3 id=installing-kubectl>Installing Kubectl</h3><p>To interact with the local Kubernetes cluster you will need kubectl. On MacOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install kubernetes-cli
</span></span></code></pre></div><p>For other IS, follow the <a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/>installation instructions</a>.</p><h2 id=get-the-sources>Get the sources</h2><p>Clone the repository from Github:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/gardener/dependency-watchdog.git
</span></span></code></pre></div><h2 id=using-makefile>Using Makefile</h2><p>For every change following make targets are recommended to run.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># build the code changes</span>
</span></span><span style=display:flex><span>&gt; make build
</span></span><span style=display:flex><span><span style=color:green># ensure that all required checks pass</span>
</span></span><span style=display:flex><span>&gt; make verify <span style=color:green># this will check formatting, linting and will run unit tests</span>
</span></span><span style=display:flex><span><span style=color:green># if you do not wish to run tests then you can use the following make target.</span>
</span></span><span style=display:flex><span>&gt; make check
</span></span></code></pre></div><p>All tests should be run and the test coverage should ideally not reduce.
Please ensure that you have read <a href=/docs/other-components/dependency-watchdog/testing/>testing guidelines</a>.</p><p>Before raising a pull request ensure that if you are introducing any new file then you must add licesence header to all new files. To add license header you can run this make target:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&gt; make add-license-headers
</span></span><span style=display:flex><span><span style=color:green># This will add license headers to any file which does not already have it.</span>
</span></span></code></pre></div><blockquote><p>NOTE: Also have a look at the Makefile as it has other targets that are not mentioned here.</p></blockquote><h2 id=raising-a-pull-request>Raising a Pull Request</h2><p>To raise a pull request do the following:</p><ol><li>Create a fork of <a href=https://github.com/gardener/dependency-watchdog>dependency-watchdog</a></li><li>Add <a href=https://github.com/gardener/dependency-watchdog>dependency-watchdog</a> as upstream remote via</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>   git remote add upstream https://github.com/gardener/dependency-watchdog
</span></span></code></pre></div><ol start=3><li>It is recommended that you create a git branch and push all your changes for the pull-request.</li><li>Ensure that while you work on your pull-request, you continue to rebase the changes from upstream to your branch. To do that execute the following command:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>   git pull --rebase upstream master
</span></span></code></pre></div><ol start=5><li>We prefer clean commits. If you have multiple commits in the pull-request, then squash the commits to a single commit. You can do this via <code>interactive git rebase</code> command. For example if your PR branch is ahead of remote origin HEAD by 5 commits then you can execute the following command and pick the first commit and squash the remaining commits.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>   git rebase -i HEAD~5 <span style=color:green>#actual number from the head will depend upon how many commits your branch is ahead of remote origin master</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-aaeabd99918759e49fd4224faf002bcd>1.4 - Dwd Using Local Garden</h1><h1 id=dependency-watchdog-with-local-garden-cluster>Dependency Watchdog with Local Garden Cluster</h1><h2 id=setting-up-local-garden-cluster>Setting up Local Garden cluster</h2><p>A convenient way to test local dependency-watchdog changes is to use a local garden cluster.
To setup a local garden cluster you can follow the <a href=/docs/gardener/deployment/getting_started_locally/>setup-guide</a>.</p><h2 id=dependency-watchdog-resources>Dependency Watchdog resources</h2><p>As part of the local garden installation, a <code>local</code> seed will be available.</p><h3 id=dependency-watchdog-resources-created-in-the-seed>Dependency Watchdog resources created in the seed</h3><h4 id=namespaced-resources>Namespaced resources</h4><p>In the <code>garden</code> namespace of the seed cluster, following resources will be created:</p><table><thead><tr><th>Resource (GVK)</th><th>Name</th></tr></thead><tbody><tr><td>{apiVersion: v1, Kind: ServiceAccount}</td><td>dependency-watchdog-prober</td></tr><tr><td>{apiVersion: v1, Kind: ServiceAccount}</td><td>dependency-watchdog-weeder</td></tr><tr><td>{apiVersion: apps/v1, Kind: Deployment}</td><td>dependency-watchdog-prober</td></tr><tr><td>{apiVersion: apps/v1, Kind: Deployment}</td><td>dependency-watchdog-weeder</td></tr><tr><td>{apiVersion: v1, Kind: ConfigMap}</td><td>dependency-watchdog-prober-*</td></tr><tr><td>{apiVersion: v1, Kind: ConfigMap}</td><td>dependency-watchdog-weeder-*</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: Role}</td><td>gardener.cloud:dependency-watchdog-prober:role</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: Role}</td><td>gardener.cloud:dependency-watchdog-weeder:role</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: RoleBinding}</td><td>gardener.cloud:dependency-watchdog-prober:role-binding</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: RoleBinding}</td><td>gardener.cloud:dependency-watchdog-weeder:role-binding</td></tr><tr><td>{apiVersion: resources.gardener.cloud/v1alpha1, Kind: ManagedResource}</td><td>dependency-watchdog-prober</td></tr><tr><td>{apiVersion: resources.gardener.cloud/v1alpha1, Kind: ManagedResource}</td><td>dependency-watchdog-weeder</td></tr><tr><td>{apiVersion: v1, Kind: Secret}</td><td>managedresource-dependency-watchdog-weeder</td></tr><tr><td>{apiVersion: v1, Kind: Secret}</td><td>managedresource-dependency-watchdog-prober</td></tr></tbody></table><h4 id=cluster-resources>Cluster resources</h4><table><thead><tr><th>Resource (GVK)</th><th>Name</th></tr></thead><tbody><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: ClusterRole}</td><td>gardener.cloud:dependency-watchdog-prober:cluster-role</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: ClusterRole}</td><td>gardener.cloud:dependency-watchdog-weeder:cluster-role</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: ClusterRoleBinding}</td><td>gardener.cloud:dependency-watchdog-prober:cluster-role-binding</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: ClusterRoleBinding}</td><td>gardener.cloud:dependency-watchdog-weeder:cluster-role-binding</td></tr></tbody></table><h3 id=dependency-watchdog-resources-created-in-shoot-control-namespace>Dependency Watchdog resources created in Shoot control namespace</h3><table><thead><tr><th>Resource (GVK)</th><th>Name</th></tr></thead><tbody><tr><td>{apiVersion: v1, Kind: Secret}</td><td>dependency-watchdog-prober</td></tr><tr><td>{apiVersion: resources.gardener.cloud/v1alpha1, Kind: ManagedResource}</td><td>shoot-core-dependency-watchdog</td></tr></tbody></table><h3 id=dependency-watchdog-resources-created-in-the-kube-node-lease-namespace-of-the-shoot>Dependency Watchdog resources created in the kube-node-lease namespace of the shoot</h3><table><thead><tr><th>Resource (GVK)</th><th>Name</th></tr></thead><tbody><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: Role}</td><td>gardener.cloud:target:dependency-watchdog</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: RoleBinding}</td><td>gardener.cloud:target:dependency-watchdog</td></tr></tbody></table><p>These will be created by the GRM and will have a managed resource named <code>shoot-core-dependency-watchdog</code> in the shoot namespace in the seed.</p><h2 id=update-gardener-with-custom-dependency-watchdog-docker-images>Update Gardener with custom Dependency Watchdog Docker images</h2><h3 id=build-tag-and-push-docker-images>Build, Tag and Push docker images</h3><p>To build dependency watchdog docker images run the following make target:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&gt; make docker-build
</span></span></code></pre></div><p>Local gardener hosts a docker registry which can be access at <code>localhost:5001</code>. To enable local gardener to be able to access the custom docker images you need to tag and push these images to the embedded docker registry. To do that execute the following commands:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&gt; docker images
</span></span><span style=display:flex><span><span style=color:green># Get the IMAGE ID of the dependency watchdog images that were built using docker-build make target.</span>
</span></span><span style=display:flex><span>&gt; docker tag &lt;IMAGE-ID&gt; localhost:5001/europe-docker.pkg.dev/gardener-project/public/gardener/dependency-watchdog-prober:&lt;TAGNAME&gt;
</span></span><span style=display:flex><span>&gt; docker push localhost:5001/europe-docker.pkg.dev/gardener-project/public/gardener/dependency-watchdog-prober:&lt;TAGNAME&gt;
</span></span></code></pre></div><h3 id=update-managedresource>Update ManagedResource</h3><p>Garden resource manager will revert back any changes that are done to the kubernetes deployment for dependency watchdog. This is quite useful in live landscapes where only tested and qualified images are used for all gardener managed components. Any change therefore is automatically reverted.</p><p>However, during development and testing you will need to use custom docker images. To prevent garden resource manager from reverting the changes done to the kubernetes deployment for dependency watchdog components you must update the respective managed resources first.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># List the managed resources</span>
</span></span><span style=display:flex><span>&gt; kubectl get mr -n garden | grep dependency
</span></span><span style=display:flex><span><span style=color:green># Sample response</span>
</span></span><span style=display:flex><span>dependency-watchdog-weeder            seed    True      True      False         26h
</span></span><span style=display:flex><span>dependency-watchdog-prober            seed    True      True      False         26h
</span></span><span style=display:flex><span><span style=color:green># Lets assume that you are currently testing prober and would like to use a custom docker image</span>
</span></span><span style=display:flex><span>&gt; kubectl edit mr dependency-watchdog-prober -n garden
</span></span><span style=display:flex><span><span style=color:green># This will open the resource YAML for editing. Add the annotation resources.gardener.cloud/ignore=true</span>
</span></span><span style=display:flex><span><span style=color:green># Reference: https://github.com/gardener/gardener/blob/master/docs/concepts/resource-manager.md</span>
</span></span><span style=display:flex><span><span style=color:green># Save the YAML file.</span>
</span></span></code></pre></div><p>When you are done with your testing then you can again edit the ManagedResource and remove the annotation. Garden resource manager will revert back to the image with which gardener was initially built and started.</p><h3 id=update-kubernetes-deployment>Update Kubernetes Deployment</h3><p>Find and update the kubernetes deployment for dependency watchdog.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&gt; kubectl get deploy -n garden | grep dependency
</span></span><span style=display:flex><span><span style=color:green># Sample response</span>
</span></span><span style=display:flex><span>dependency-watchdog-weeder            1/1     1            1           26h
</span></span><span style=display:flex><span>dependency-watchdog-prober            1/1     1            1           26h
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Lets assume that you are currently testing prober and would like to use a custom docker image</span>
</span></span><span style=display:flex><span>&gt; kubectl edit deploy dependency-watchdog-prober -n garden
</span></span><span style=display:flex><span><span style=color:green># This will open the resource YAML for editing. Change the image or any other changes and save.</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-65baffe8a57fa26fd40648819522e688>1.5 - Testing</h1><h1 id=testing-strategy-and-developer-guideline>Testing Strategy and Developer Guideline</h1><p>Intent of this document is to introduce you (the developer) to the following:</p><ul><li>Category of tests that exists.</li><li>Libraries that are used to write tests.</li><li>Best practices to write tests that are correct, stable, fast and maintainable.</li><li>How to run each category of tests.</li></ul><p>For any new contributions <strong>tests are a strict requirement</strong>. <code>Boy Scouts Rule</code> is followed: If you touch a code for which either no tests exist or coverage is insufficient then it is expected that you will add relevant tests.</p><h2 id=tools-used-for-writing-tests>Tools Used for Writing Tests</h2><p>These are the following tools that were used to write all the tests (unit + envtest + vanilla kind cluster tests), it is preferred not to introduce any additional tools / test frameworks for writing tests:</p><h3 id=gomega>Gomega</h3><p>We use gomega as our matcher or assertion library. Refer to Gomega&rsquo;s <a href=https://onsi.github.io/gomega/>official documentation</a> for details regarding its installation and application in tests.</p><h3 id=testing-package-from-standard-library><code>Testing</code> Package from Standard Library</h3><p>We use the <code>Testing</code> package provided by the standard library in golang for writing all our tests. Refer to its <a href=https://pkg.go.dev/testing>official documentation</a> to learn how to write tests using <code>Testing</code> package. You can also refer to <a href=https://go.dev/doc/tutorial/add-a-test>this</a> example.</p><h2 id=writing-tests>Writing Tests</h2><h3 id=common-for-all-kinds>Common for All Kinds</h3><ul><li>For naming the individual tests (<code>TestXxx</code> and <code>testXxx</code> methods) and helper methods, make sure that the name describes the implementation of the method. For eg: <code>testScalingWhenMandatoryResourceNotFound</code> tests the behaviour of the <code>scaler</code> when a mandatory resource (KCM deployment) is not present.</li><li>Maintain proper logging in tests. Use <code>t.log()</code> method to add appropriate messages wherever necessary to describe the flow of the test. See <a href=https://github.com/gardener/dependency-watchdog/blob/master/controllers/endpoint/endpoints_controller_test.go>this</a> for examples.</li><li>Make use of the <code>testdata</code> directory for storing arbitrary sample data needed by tests (YAML manifests, etc.). See <a href=https://github.com/gardener/dependency-watchdog/tree/master/controllers>this</a> package for examples.<ul><li>From <a href=https://pkg.go.dev/cmd/go/internal/test>https://pkg.go.dev/cmd/go/internal/test</a>:<blockquote><p>The go tool will ignore a directory named &ldquo;testdata&rdquo;, making it available to hold ancillary data needed by the tests.</p></blockquote></li></ul></li></ul><h3 id=table-driven-tests>Table-driven tests</h3><p>We need a tabular structure in two cases:</p><ul><li><strong>When we have multiple tests which require the same kind of setup</strong>:- In this case we have a <code>TestXxxSuite</code> method which will do the setup and run all the tests. We have a slice of <code>test</code> struct which holds all the tests (typically a <code>title</code> and <code>run</code> method). We use a <code>for</code> loop to run all the tests one by one. See <a href=https://github.com/gardener/dependency-watchdog/blob/master/controllers/cluster/cluster_controller_test.go>this</a> for examples.</li><li><strong>When we have the same code path and multiple possible values to check</strong>:- In this case we have the arguments and expectations in a struct. We iterate through the slice of all such structs, passing the arguments to appropriate methods and checking if the expectation is met. See <a href=https://github.com/gardener/dependency-watchdog/blob/master/internal/prober/scaler/scaler_test.go>this</a> for examples.</li></ul><h3 id=env-tests>Env Tests</h3><p>Env tests in Dependency Watchdog use the <code>sigs.k8s.io/controller-runtime/pkg/envtest</code> package. It sets up a temporary control plane (etcd + kube-apiserver) and runs the test against it. The code to set up and teardown the environment can be checked out <a href=https://github.com/gardener/dependency-watchdog/blob/master/internal/test/testenv.go>here</a>.</p><p>These are the points to be followed while writing tests that use <code>envtest</code> setup:</p><ul><li><p>All tests should be divided into two top level partitions:</p><ol><li>tests with common environment (<code>testXxxCommonEnvTests</code>)</li><li>tests which need a dedicated environment for each one. (<code>testXxxDedicatedEnvTests</code>)</li></ol><p>They should be contained within the <code>TestXxxSuite</code> method. See <a href=https://github.com/gardener/dependency-watchdog/blob/master/controllers/cluster/cluster_controller_test.go>this</a> for examples. If all tests are of one kind then this is not needed.</p></li><li><p>Create a method named <code>setUpXxxTest</code> for performing setup tasks before all/each test. It should either return a method or have a separate method to perform teardown tasks. See <a href=https://github.com/gardener/dependency-watchdog/blob/master/controllers/cluster/cluster_controller_test.go>this</a> for examples.</p></li><li><p>The tests run by the suite can be table-driven as well.</p></li><li><p>Use the <code>envtest</code> setup when there is a need of an environment close to an actual setup. Eg: start controllers against a real Kubernetes control plane to catch bugs that can only happen when talking to a real API server.</p></li></ul><blockquote><p>NOTE: It is currently not possible to bring up more than one envtest environments. See <a href=https://github.com/kubernetes-sigs/controller-runtime/issues/1363>issue#1363</a>. We enforce running serial execution of test suites each of which uses a different envtest environments. See <a href=https://github.com/gardener/dependency-watchdog/blob/master/hack/test.sh>hack/test.sh</a>.</p></blockquote><h3 id=vanilla-kind-cluster-tests>Vanilla Kind Cluster Tests</h3><p>There are some tests where we need a vanilla kind cluster setup, for eg:- The <code>scaler.go</code> code in the <code>prober</code> package uses the <code>scale</code> subresource to scale the deployments mentioned in the prober config. But the <code>envtest</code> setup does not support the <code>scale</code> subresource as of now. So we need this setup to test if the deployments are scaled as per the config or not.
You can check out the code for this setup <a href=https://github.com/gardener/dependency-watchdog/blob/master/internal/test/kind.go>here</a>. You can add utility methods for different kubernetes and custom resources in there.</p><p>These are the points to be followed while writing tests that use <code>Vanilla Kind Cluster</code> setup:</p><ul><li>Use this setup only if there is a need of an actual Kubernetes cluster(api server + control plane + etcd) to write the tests. (Because this is slower than your normal <code>envTest</code> setup)</li><li>Create <code>setUpXxxTest</code> similar to the one in <code>envTest</code>. Follow the same structural pattern used in <code>envTest</code> for writing these tests. See <a href=https://github.com/gardener/dependency-watchdog/blob/master/internal/prober/scaler/scaler_test.go>this</a> for examples.</li></ul><h2 id=run-tests>Run Tests</h2><p>To run unit tests, use the following Makefile target</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make test
</span></span></code></pre></div><p>To run KIND cluster based tests, use the following Makefile target</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make kind-tests <span style=color:green># these tests will be slower as it brings up a vanilla KIND cluster</span>
</span></span></code></pre></div><p>To view coverage after running the tests, run :</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>go tool cover -html=cover.out
</span></span></code></pre></div><h2 id=flaky-tests>Flaky tests</h2><p>If you see that a test is flaky then you can use <code>make stress</code> target which internally uses <a href=https://pkg.go.dev/golang.org/x/tools/cmd/stress>stress tool</a></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make stress test-package=&lt;test-package&gt; test-func=&lt;test-func&gt; tool-params=<span style=color:#a31515>&#34;&lt;tool-params&gt;&#34;</span>
</span></span></code></pre></div><p>An example invocation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make stress test-package=./internal/util test-func=TestRetryUntilPredicateWithBackgroundContext tool-params=<span style=color:#a31515>&#34;-p 10&#34;</span>
</span></span></code></pre></div><p>The make target will do the following:</p><ol><li>It will create a test binary for the package specified via <code>test-package</code> at <code>/tmp/pkg-stress.test</code> directory.</li><li>It will run <code>stress</code> tool passing the <code>tool-params</code> and targets the function <code>test-func</code>.</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-4c5520f0940e52e6166ce8932f4dd864>2 - Machine Controller Manager</h1><div class=lead>Declarative way of managing machines for Kubernetes cluster</div><h1 id=machine-controller-manager>machine-controller-manager</h1><p><a href=https://api.reuse.software/info/github.com/gardener/machine-controller-manager><img src=https://api.reuse.software/badge/github.com/gardener/machine-controller-manager alt="REUSE status"></a>
<a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/machine-controller-manager-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/machine-controller-manager-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/machine-controller-manager><img src=https://goreportcard.com/badge/github.com/gardener/machine-controller-manager alt="Go Report Card"></a></p><p><strong>Note</strong>
One can add support for a new cloud provider by following <a href=/docs/other-components/machine-controller-manager/cp_support_new/>Adding support for new provider</a>.</p><h1 id=overview>Overview</h1><p>Machine Controller Manager aka MCM is a group of cooperative controllers that manage the lifecycle of the worker machines. It is inspired by the design of Kube Controller Manager in which various sub controllers manage their respective Kubernetes Clients. MCM gives you the following benefits:</p><ul><li>seamlessly manage machines/nodes with a declarative API (of course, across different cloud providers)</li><li>integrate generically with the cluster autoscaler</li><li>plugin with tools such as the node-problem-detector</li><li>transport the immutability design principle to machine/nodes</li><li>implement e.g. rolling upgrades of machines/nodes</li></ul><p>MCM supports following providers. These provider code is maintained externally (out-of-tree), and the links for the same are linked below:</p><ul><li><a href=https://github.com/gardener/machine-controller-manager-provider-alicloud>Alicloud</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-aws>AWS</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-azure>Azure</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-equinix-metal>Equinix Metal</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-gcp>GCP</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-kubevirt>KubeVirt</a></li><li><a href=https://github.com/metal-stack/machine-controller-manager-provider-metal>Metal Stack</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-openstack>Openstack</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-vsphere>V Sphere</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-yandex>Yandex</a></li></ul><p>It can easily be extended to support other cloud providers as well.</p><p>Example of managing machine:</p><pre tabindex=0><code>kubectl create/get/delete machine vm1
</code></pre><h2 id=key-terminologies>Key terminologies</h2><p>Nodes/Machines/VMs are different terminologies used to represent similar things. We use these terms in the following way</p><ol><li>VM: A virtual machine running on any cloud provider. It could also refer to a physical machine (PM) in case of a bare metal setup.</li><li>Node: Native kubernetes node objects. The objects you get to see when you do a <em>&ldquo;kubectl get nodes&rdquo;</em>. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM.</li><li>Machine: A VM that is provisioned/managed by the Machine Controller Manager.</li></ol><h1 id=design-of-machine-controller-manager>Design of Machine Controller Manager</h1><p>The design of the Machine Controller Manager is influenced by the Kube Controller Manager, where-in multiple sub-controllers are used to manage the Kubernetes clients.</p><h2 id=design-principles>Design Principles</h2><p>It&rsquo;s designed to run in the master plane of a Kubernetes cluster. It follows the best principles and practices of writing controllers, including, but not limited to:</p><ul><li>Reusing code from kube-controller-manager</li><li>leader election to allow HA deployments of the controller</li><li><code>workqueues</code> and multiple thread-workers</li><li><code>SharedInformers</code> that limit to minimum network calls, de-serialization and provide helpful create/update/delete events for resources</li><li>rate-limiting to allow back-off in case of network outages and general instability of other cluster components</li><li>sending events to respected resources for easy debugging and overview</li><li>Prometheus metrics, health and (optional) profiling endpoints</li></ul><h2 id=objects-of-machine-controller-manager>Objects of Machine Controller Manager</h2><p>Machine Controller Manager reconciles a set of Custom Resources namely <code>MachineDeployment</code>, <code>MachineSet</code> and <code>Machines</code> which are managed & monitored by their controllers MachineDeployment Controller, MachineSet Controller, Machine Controller respectively along with another cooperative controller called the Safety Controller.</p><p>Machine Controller Manager makes use of 4 CRD objects and 1 Kubernetes secret object to manage machines. They are as follows:</p><table><thead><tr><th>Custom ResourceObject</th><th>Description</th></tr></thead><tbody><tr><td><code>MachineClass</code></td><td>A <code>MachineClass</code> represents a template that contains cloud provider specific details used to create machines.</td></tr><tr><td><code>Machine</code></td><td>A <code>Machine</code> represents a VM which is backed by the cloud provider.</td></tr><tr><td><code>MachineSet</code></td><td>A <code>MachineSet</code> ensures that the specified number of <code>Machine</code> replicas are running at a given point of time.</td></tr><tr><td><code>MachineDeployment</code></td><td>A <code>MachineDeployment</code> provides a declarative update for <code>MachineSet</code> and <code>Machines</code>.</td></tr><tr><td><code>Secret</code></td><td>A <code>Secret</code> here is a Kubernetes secret that stores cloudconfig (initialization scripts used to create VMs) and cloud specific credentials.</td></tr></tbody></table><p>See <a href=/docs/other-components/machine-controller-manager/documents/apis/>here</a> for CRD API Documentation</p><h2 id=components-of-machine-controller-manager>Components of Machine Controller Manager</h2><table><thead><tr><th>Controller</th><th>Description</th></tr></thead><tbody><tr><td>MachineDeployment controller</td><td>Machine Deployment controller reconciles the <code>MachineDeployment</code> objects and manages the lifecycle of <code>MachineSet</code> objects. <code>MachineDeployment</code> consumes provider specific <code>MachineClass</code> in its <code>spec.template.spec</code> which is the template of the VM spec that would be spawned on the cloud by MCM.</td></tr><tr><td>MachineSet controller</td><td>MachineSet controller reconciles the <code>MachineSet</code> objects and manages the lifecycle of <code>Machine</code> objects.</td></tr><tr><td>Safety controller</td><td>There is a Safety Controller responsible for handling the unidentified or unknown behaviours from the cloud providers. Safety Controller:<ul><li>freezes the MachineDeployment controller and MachineSet controller if the number of <code>Machine</code> objects goes beyond a certain threshold on top of <code>Spec.replicas</code>. It can be configured by the flag <code>--safety-up</code> or <code>--safety-down</code> and also <code>--machine-safety-overshooting-period`</code>.</li><li>freezes the functionality of the MCM if either of the <code>target-apiserver</code> or the <code>control-apiserver</code> is not reachable.</li><li>unfreezes the MCM automatically once situation is resolved to normal. A <code>freeze</code> label is applied on <code>MachineDeployment</code>/<code>MachineSet</code> to enforce the freeze condition.</li></ul></td></tr></tbody></table><p>Along with the above Custom Controllers and Resources, MCM requires the <code>MachineClass</code> to use K8s <code>Secret</code> that stores cloudconfig (initialization scripts used to create VMs) and cloud specific credentials. All these controllers work in an co-operative manner. They form a parent-child relationship with <code>MachineDeployment</code> Controller being the grandparent, <code>MachineSet</code> Controller being the parent, and <code>Machine</code> Controller being the child.</p><h2 id=development>Development</h2><p>To start using or developing the Machine Controller Manager, see the documentation in the <code>/docs</code> repository.</p><h2 id=faq>FAQ</h2><p>An FAQ is available <a href=/docs/other-components/machine-controller-manager/faq/>here</a>.</p><h2 id=cluster-api-implementation>cluster-api Implementation</h2><ul><li><code>cluster-api</code> branch of machine-controller-manager implements the machine-api aspect of the <a href=https://github.com/kubernetes-sigs/cluster-api>cluster-api project</a>.</li><li>Link: <a href=https://github.com/gardener/machine-controller-manager/tree/cluster-api>https://github.com/gardener/machine-controller-manager/tree/cluster-api</a></li><li>Once cluster-api project gets stable, we may make <code>master</code> branch of MCM as well cluster-api compliant, with well-defined migration notes.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-9143519cb73c14a3f4ae463f9b24457d>2.1 - Documents</h1></div><div class=td-content><h1 id=pg-e2f257b15e56d71d58253c3abb85f408>2.1.1 - Apis</h1><h2 id=specification>Specification</h2><h3 id=providerspec-schema>ProviderSpec Schema</h3><br><h3 id=machine.sapcloud.io/v1alpha1.Machine><b>Machine</b></h3><p><p>Machine is the representation of a physical or virtual machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io/v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>Machine</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><p>ObjectMeta for machine object</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSpec>MachineSpec</a></em></td><td><p>Spec contains the specification of the machine</p><br><br><table><tr><td><code>class</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em><p>Class contains the machineclass attributes of a machine</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>ProviderID represents the provider’s unique ID given to a machine</p></td></tr><tr><td><code>nodeTemplate</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.NodeTemplateSpec>NodeTemplateSpec</a></em></td><td><em>(Optional)</em><p>NodeTemplateSpec describes the data a node should have when created from a template</p></td></tr><tr><td><code>MachineConfiguration</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineConfiguration>MachineConfiguration</a></em></td><td><p>(Members of <code>MachineConfiguration</code> are embedded into this type.)</p><em>(Optional)</em><p>Configuration for the machine-controller.</p></td></tr></table></td></tr><tr><td><code>status</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineStatus>MachineStatus</a></em></td><td><p>Status contains fields depicting the status</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineClass><b>MachineClass</b></h3><p><p>MachineClass can be used to templatize and re-use provider configuration
across multiple Machines / MachineSets / MachineDeployments.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io/v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>MachineClass</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>nodeTemplate</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.NodeTemplate>NodeTemplate</a></em></td><td><em>(Optional)</em><p>NodeTemplate contains subfields to track all node resources and other node info required to scale nodegroup from zero</p></td></tr><tr><td><code>credentialsSecretRef</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>CredentialsSecretRef can optionally store the credentials (in this case the SecretRef does not need to store them).
This might be useful if multiple machine classes with the same credentials but different user-datas are used.</p></td></tr><tr><td><code>providerSpec</code></td><td><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/runtime#RawExtension>k8s.io/apimachinery/pkg/runtime.RawExtension</a></em></td><td><p>Provider-specific configuration to use during node creation.</p></td></tr><tr><td><code>provider</code></td><td><em>string</em></td><td><p>Provider is the combination of name and location of cloud-specific drivers.</p></td></tr><tr><td><code>secretRef</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>SecretRef stores the necessary secrets such as credentials or userdata.</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineDeployment><b>MachineDeployment</b></h3><p><p>MachineDeployment enables declarative updates for machines and MachineSets.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io/v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>MachineDeployment</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object metadata.</p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentSpec>MachineDeploymentSpec</a></em></td><td><em>(Optional)</em><p>Specification of the desired behavior of the MachineDeployment.</p><br><br><table><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Number of desired machines. This is a pointer to distinguish between explicit
zero and not specified. Defaults to 0.</p></td></tr><tr><td><code>selector</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em><p>Label selector for machines. Existing MachineSets whose machines are
selected by this will be the ones affected by this MachineDeployment.</p></td></tr><tr><td><code>template</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineTemplateSpec>MachineTemplateSpec</a></em></td><td><p>Template describes the machines that will be created.</p></td></tr><tr><td><code>strategy</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentStrategy>MachineDeploymentStrategy</a></em></td><td><em>(Optional)</em><p>The MachineDeployment strategy to use to replace existing machines with new ones.</p></td></tr><tr><td><code>minReadySeconds</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Minimum number of seconds for which a newly created machine should be ready
without any of its container crashing, for it to be considered available.
Defaults to 0 (machine will be considered available as soon as it is ready)</p></td></tr><tr><td><code>revisionHistoryLimit</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>The number of old MachineSets to retain to allow rollback.
This is a pointer to distinguish between explicit zero and not specified.</p></td></tr><tr><td><code>paused</code></td><td><em>bool</em></td><td><em>(Optional)</em><p>Indicates that the MachineDeployment is paused and will not be processed by the
MachineDeployment controller.</p></td></tr><tr><td><code>rollbackTo</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.RollbackConfig>RollbackConfig</a></em></td><td><em>(Optional)</em><p>DEPRECATED.
The config this MachineDeployment is rolling back to. Will be cleared after rollback is done.</p></td></tr><tr><td><code>progressDeadlineSeconds</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>The maximum time in seconds for a MachineDeployment to make progress before it
is considered to be failed. The MachineDeployment controller will continue to
process failed MachineDeployments and a condition with a ProgressDeadlineExceeded
reason will be surfaced in the MachineDeployment status. Note that progress will
not be estimated during the time a MachineDeployment is paused. This is not set
by default, which is treated as infinite deadline.</p></td></tr></table></td></tr><tr><td><code>status</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentStatus>MachineDeploymentStatus</a></em></td><td><em>(Optional)</em><p>Most recently observed status of the MachineDeployment.</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineSet><b>MachineSet</b></h3><p><p>MachineSet TODO</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiVersion</code></td><td>string</td><td><code>machine.sapcloud.io/v1alpha1</code></td></tr><tr><td><code>kind</code></td><td>string</td><td><code>MachineSet</code></td></tr><tr><td><code>metadata</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSetSpec>MachineSetSpec</a></em></td><td><em>(Optional)</em><br><br><table><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em></td></tr><tr><td><code>selector</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>machineClass</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>template</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineTemplateSpec>MachineTemplateSpec</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>minReadySeconds</code></td><td><em>int32</em></td><td><em>(Optional)</em></td></tr></table></td></tr><tr><td><code>status</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSetStatus>MachineSetStatus</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.ClassSpec><b>ClassSpec</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSetSpec>MachineSetSpec</a>,
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSpec>MachineSpec</a>)</p><p><p>ClassSpec is the class specification of machine</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>apiGroup</code></td><td><em>string</em></td><td><p>API group to which it belongs</p></td></tr><tr><td><code>kind</code></td><td><em>string</em></td><td><p>Kind for machine class</p></td></tr><tr><td><code>name</code></td><td><em>string</em></td><td><p>Name of machine class</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.ConditionStatus><b>ConditionStatus</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentCondition>MachineDeploymentCondition</a>,
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSetCondition>MachineSetCondition</a>)</p><p><p>ConditionStatus are valid condition statuses</p></p><br><h3 id=machine.sapcloud.io/v1alpha1.CurrentStatus><b>CurrentStatus</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineStatus>MachineStatus</a>)</p><p><p>CurrentStatus contains information about the current status of Machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>phase</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachinePhase>MachinePhase</a></em></td><td></td></tr><tr><td><code>timeoutActive</code></td><td><em>bool</em></td><td></td></tr><tr><td><code>lastUpdateTime</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last update time of current status</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.LastOperation><b>LastOperation</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSetStatus>MachineSetStatus</a>,
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineStatus>MachineStatus</a>,
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSummary>MachineSummary</a>)</p><p><p>LastOperation suggests the last operation performed on the object</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>description</code></td><td><em>string</em></td><td><p>Description of the current operation</p></td></tr><tr><td><code>errorCode</code></td><td><em>string</em></td><td><em>(Optional)</em><p>ErrorCode of the current operation if any</p></td></tr><tr><td><code>lastUpdateTime</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last update time of current operation</p></td></tr><tr><td><code>state</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineState>MachineState</a></em></td><td><p>State of operation</p></td></tr><tr><td><code>type</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineOperationType>MachineOperationType</a></em></td><td><p>Type of operation</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineConfiguration><b>MachineConfiguration</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSpec>MachineSpec</a>)</p><p><p>MachineConfiguration describes the configurations useful for the machine-controller.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>drainTimeout</code></td><td><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MachineDraintimeout is the timeout after which machine is forcefully deleted.</p></td></tr><tr><td><code>healthTimeout</code></td><td><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MachineHealthTimeout is the timeout after which machine is declared unhealhty/failed.</p></td></tr><tr><td><code>creationTimeout</code></td><td><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1#Duration>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>MachineCreationTimeout is the timeout after which machinie creation is declared failed.</p></td></tr><tr><td><code>maxEvictRetries</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>MaxEvictRetries is the number of retries that will be attempted while draining the node.</p></td></tr><tr><td><code>nodeConditions</code></td><td><em>*string</em></td><td><em>(Optional)</em><p>NodeConditions are the set of conditions if set to true for MachineHealthTimeOut, machine will be declared failed.</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineDeploymentCondition><b>MachineDeploymentCondition</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentStatus>MachineDeploymentStatus</a>)</p><p><p>MachineDeploymentCondition describes the state of a MachineDeployment at a certain point.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentConditionType>MachineDeploymentConditionType</a></em></td><td><p>Type of MachineDeployment condition.</p></td></tr><tr><td><code>status</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.ConditionStatus>ConditionStatus</a></em></td><td><p>Status of the condition, one of True, False, Unknown.</p></td></tr><tr><td><code>lastUpdateTime</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>The last time this condition was updated.</p></td></tr><tr><td><code>lastTransitionTime</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last time the condition transitioned from one status to another.</p></td></tr><tr><td><code>reason</code></td><td><em>string</em></td><td><p>The reason for the condition’s last transition.</p></td></tr><tr><td><code>message</code></td><td><em>string</em></td><td><p>A human readable message indicating details about the transition.</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineDeploymentConditionType><b>MachineDeploymentConditionType</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentCondition>MachineDeploymentCondition</a>)</p><p><p>MachineDeploymentConditionType are valid conditions of MachineDeployments</p></p><br><h3 id=machine.sapcloud.io/v1alpha1.MachineDeploymentSpec><b>MachineDeploymentSpec</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeployment>MachineDeployment</a>)</p><p><p>MachineDeploymentSpec is the specification of the desired behavior of the MachineDeployment.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Number of desired machines. This is a pointer to distinguish between explicit
zero and not specified. Defaults to 0.</p></td></tr><tr><td><code>selector</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em><p>Label selector for machines. Existing MachineSets whose machines are
selected by this will be the ones affected by this MachineDeployment.</p></td></tr><tr><td><code>template</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineTemplateSpec>MachineTemplateSpec</a></em></td><td><p>Template describes the machines that will be created.</p></td></tr><tr><td><code>strategy</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentStrategy>MachineDeploymentStrategy</a></em></td><td><em>(Optional)</em><p>The MachineDeployment strategy to use to replace existing machines with new ones.</p></td></tr><tr><td><code>minReadySeconds</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Minimum number of seconds for which a newly created machine should be ready
without any of its container crashing, for it to be considered available.
Defaults to 0 (machine will be considered available as soon as it is ready)</p></td></tr><tr><td><code>revisionHistoryLimit</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>The number of old MachineSets to retain to allow rollback.
This is a pointer to distinguish between explicit zero and not specified.</p></td></tr><tr><td><code>paused</code></td><td><em>bool</em></td><td><em>(Optional)</em><p>Indicates that the MachineDeployment is paused and will not be processed by the
MachineDeployment controller.</p></td></tr><tr><td><code>rollbackTo</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.RollbackConfig>RollbackConfig</a></em></td><td><em>(Optional)</em><p>DEPRECATED.
The config this MachineDeployment is rolling back to. Will be cleared after rollback is done.</p></td></tr><tr><td><code>progressDeadlineSeconds</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>The maximum time in seconds for a MachineDeployment to make progress before it
is considered to be failed. The MachineDeployment controller will continue to
process failed MachineDeployments and a condition with a ProgressDeadlineExceeded
reason will be surfaced in the MachineDeployment status. Note that progress will
not be estimated during the time a MachineDeployment is paused. This is not set
by default, which is treated as infinite deadline.</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineDeploymentStatus><b>MachineDeploymentStatus</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeployment>MachineDeployment</a>)</p><p><p>MachineDeploymentStatus is the most recently observed status of the MachineDeployment.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>observedGeneration</code></td><td><em>int64</em></td><td><em>(Optional)</em><p>The generation observed by the MachineDeployment controller.</p></td></tr><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of non-terminated machines targeted by this MachineDeployment (their labels match the selector).</p></td></tr><tr><td><code>updatedReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of non-terminated machines targeted by this MachineDeployment that have the desired template spec.</p></td></tr><tr><td><code>readyReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of ready machines targeted by this MachineDeployment.</p></td></tr><tr><td><code>availableReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of available machines (ready for at least minReadySeconds) targeted by this MachineDeployment.</p></td></tr><tr><td><code>unavailableReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>Total number of unavailable machines targeted by this MachineDeployment. This is the total number of
machines that are still required for the MachineDeployment to have 100% available capacity. They may
either be machines that are running but not yet available or machines that still have not been created.</p></td></tr><tr><td><code>conditions</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentCondition>[]MachineDeploymentCondition</a></em></td><td><p>Represents the latest available observations of a MachineDeployment’s current state.</p></td></tr><tr><td><code>collisionCount</code></td><td><em>*int32</em></td><td><em>(Optional)</em><p>Count of hash collisions for the MachineDeployment. The MachineDeployment controller uses this
field as a collision avoidance mechanism when it needs to create the name for the
newest MachineSet.</p></td></tr><tr><td><code>failedMachines</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineSummary>[]*github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineSummary</a></em></td><td><em>(Optional)</em><p>FailedMachines has summary of machines on which lastOperation Failed</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineDeploymentStrategy><b>MachineDeploymentStrategy</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentSpec>MachineDeploymentSpec</a>)</p><p><p>MachineDeploymentStrategy describes how to replace existing machines with new ones.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentStrategyType>MachineDeploymentStrategyType</a></em></td><td><em>(Optional)</em><p>Type of MachineDeployment. Can be “Recreate” or “RollingUpdate”. Default is RollingUpdate.</p></td></tr><tr><td><code>rollingUpdate</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.RollingUpdateMachineDeployment>RollingUpdateMachineDeployment</a></em></td><td><em>(Optional)</em><p>Rolling update config params. Present only if MachineDeploymentStrategyType =</p><h2>RollingUpdate.</h2><p>TODO: Update this to follow our convention for oneOf, whatever we decide it
to be.</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineDeploymentStrategyType><b>MachineDeploymentStrategyType</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentStrategy>MachineDeploymentStrategy</a>)</p><p><p>MachineDeploymentStrategyType are valid strategy types for rolling MachineDeployments</p></p><br><h3 id=machine.sapcloud.io/v1alpha1.MachineOperationType><b>MachineOperationType</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.LastOperation>LastOperation</a>)</p><p><p>MachineOperationType is a label for the operation performed on a machine object.</p></p><br><h3 id=machine.sapcloud.io/v1alpha1.MachinePhase><b>MachinePhase</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.CurrentStatus>CurrentStatus</a>)</p><p><p>MachinePhase is a label for the condition of a machine at the current time.</p></p><br><h3 id=machine.sapcloud.io/v1alpha1.MachineSetCondition><b>MachineSetCondition</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSetStatus>MachineSetStatus</a>)</p><p><p>MachineSetCondition describes the state of a machine set at a certain point.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSetConditionType>MachineSetConditionType</a></em></td><td><p>Type of machine set condition.</p></td></tr><tr><td><code>status</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.ConditionStatus>ConditionStatus</a></em></td><td><p>Status of the condition, one of True, False, Unknown.</p></td></tr><tr><td><code>lastTransitionTime</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><em>(Optional)</em><p>The last time the condition transitioned from one status to another.</p></td></tr><tr><td><code>reason</code></td><td><em>string</em></td><td><em>(Optional)</em><p>The reason for the condition’s last transition.</p></td></tr><tr><td><code>message</code></td><td><em>string</em></td><td><em>(Optional)</em><p>A human readable message indicating details about the transition.</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineSetConditionType><b>MachineSetConditionType</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSetCondition>MachineSetCondition</a>)</p><p><p>MachineSetConditionType is the condition on machineset object</p></p><br><h3 id=machine.sapcloud.io/v1alpha1.MachineSetSpec><b>MachineSetSpec</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSet>MachineSet</a>)</p><p><p>MachineSetSpec is the specification of a MachineSet.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>replicas</code></td><td><em>int32</em></td><td><em>(Optional)</em></td></tr><tr><td><code>selector</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>machineClass</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>template</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineTemplateSpec>MachineTemplateSpec</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>minReadySeconds</code></td><td><em>int32</em></td><td><em>(Optional)</em></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineSetStatus><b>MachineSetStatus</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSet>MachineSet</a>)</p><p><p>MachineSetStatus holds the most recently observed status of MachineSet.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>replicas</code></td><td><em>int32</em></td><td><p>Replicas is the number of actual replicas.</p></td></tr><tr><td><code>fullyLabeledReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>The number of pods that have labels matching the labels of the pod template of the replicaset.</p></td></tr><tr><td><code>readyReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>The number of ready replicas for this replica set.</p></td></tr><tr><td><code>availableReplicas</code></td><td><em>int32</em></td><td><em>(Optional)</em><p>The number of available replicas (ready for at least minReadySeconds) for this replica set.</p></td></tr><tr><td><code>observedGeneration</code></td><td><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed by the controller.</p></td></tr><tr><td><code>machineSetCondition</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSetCondition>[]MachineSetCondition</a></em></td><td><em>(Optional)</em><p>Represents the latest available observations of a replica set’s current state.</p></td></tr><tr><td><code>lastOperation</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.LastOperation>LastOperation</a></em></td><td><p>LastOperation performed</p></td></tr><tr><td><code>failedMachines</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.[]github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineSummary>[]github.com/gardener/machine-controller-manager/pkg/apis/machine/v1alpha1.MachineSummary</a></em></td><td><em>(Optional)</em><p>FailedMachines has summary of machines on which lastOperation Failed</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineSpec><b>MachineSpec</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.Machine>Machine</a>,
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineTemplateSpec>MachineTemplateSpec</a>)</p><p><p>MachineSpec is the specification of a Machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>class</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em><p>Class contains the machineclass attributes of a machine</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>ProviderID represents the provider’s unique ID given to a machine</p></td></tr><tr><td><code>nodeTemplate</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.NodeTemplateSpec>NodeTemplateSpec</a></em></td><td><em>(Optional)</em><p>NodeTemplateSpec describes the data a node should have when created from a template</p></td></tr><tr><td><code>MachineConfiguration</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineConfiguration>MachineConfiguration</a></em></td><td><p>(Members of <code>MachineConfiguration</code> are embedded into this type.)</p><em>(Optional)</em><p>Configuration for the machine-controller.</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineState><b>MachineState</b>
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.LastOperation>LastOperation</a>)</p><p><p>MachineState is a current state of the operation.</p></p><br><h3 id=machine.sapcloud.io/v1alpha1.MachineStatus><b>MachineStatus</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.Machine>Machine</a>)</p><p><p>MachineStatus holds the most recently observed status of Machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#nodecondition-v1-core>[]Kubernetes core/v1.NodeCondition</a></em></td><td><p>Conditions of this machine, same as node</p></td></tr><tr><td><code>lastOperation</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.LastOperation>LastOperation</a></em></td><td><p>Last operation refers to the status of the last operation performed</p></td></tr><tr><td><code>currentStatus</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.CurrentStatus>CurrentStatus</a></em></td><td><p>Current status of the machine object</p></td></tr><tr><td><code>lastKnownState</code></td><td><em>string</em></td><td><em>(Optional)</em><p>LastKnownState can store details of the last known state of the VM by the plugins.
It can be used by future operation calls to determine current infrastucture state</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineSummary><b>MachineSummary</b></h3><p><p>MachineSummary store the summary of machine.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></td><td><em>string</em></td><td><p>Name of the machine object</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><p>ProviderID represents the provider’s unique ID given to a machine</p></td></tr><tr><td><code>lastOperation</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.LastOperation>LastOperation</a></em></td><td><p>Last operation refers to the status of the last operation performed</p></td></tr><tr><td><code>ownerRef</code></td><td><em>string</em></td><td><p>OwnerRef</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.MachineTemplateSpec><b>MachineTemplateSpec</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentSpec>MachineDeploymentSpec</a>,
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSetSpec>MachineSetSpec</a>)</p><p><p>MachineTemplateSpec describes the data a machine should have when created from a template</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em><p>Standard object’s metadata.
More info: <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata>https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</a></p>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSpec>MachineSpec</a></em></td><td><em>(Optional)</em><p>Specification of the desired behavior of the machine.
More info: <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status</a></p><br><br><table><tr><td><code>class</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.ClassSpec>ClassSpec</a></em></td><td><em>(Optional)</em><p>Class contains the machineclass attributes of a machine</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>ProviderID represents the provider’s unique ID given to a machine</p></td></tr><tr><td><code>nodeTemplate</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.NodeTemplateSpec>NodeTemplateSpec</a></em></td><td><em>(Optional)</em><p>NodeTemplateSpec describes the data a node should have when created from a template</p></td></tr><tr><td><code>MachineConfiguration</code></td><td><em><a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineConfiguration>MachineConfiguration</a></em></td><td><p>(Members of <code>MachineConfiguration</code> are embedded into this type.)</p><em>(Optional)</em><p>Configuration for the machine-controller.</p></td></tr></table></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.NodeTemplate><b>NodeTemplate</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineClass>MachineClass</a>)</p><p><p>NodeTemplate contains subfields to track all node resources and other node info required to scale nodegroup from zero</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>capacity</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#resourcelist-v1-core>Kubernetes core/v1.ResourceList</a></em></td><td><p>Capacity contains subfields to track all node resources required to scale nodegroup from zero</p></td></tr><tr><td><code>instanceType</code></td><td><em>string</em></td><td><p>Instance type of the node belonging to nodeGroup</p></td></tr><tr><td><code>region</code></td><td><em>string</em></td><td><p>Region of the expected node belonging to nodeGroup</p></td></tr><tr><td><code>zone</code></td><td><em>string</em></td><td><p>Zone of the expected node belonging to nodeGroup</p></td></tr><tr><td><code>architecture</code></td><td><em>*string</em></td><td><em>(Optional)</em><p>CPU Architecture of the node belonging to nodeGroup</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.NodeTemplateSpec><b>NodeTemplateSpec</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineSpec>MachineSpec</a>)</p><p><p>NodeTemplateSpec describes the data a node should have when created from a template</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td><em>(Optional)</em>
Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#nodespec-v1-core>Kubernetes core/v1.NodeSpec</a></em></td><td><em>(Optional)</em><p>NodeSpec describes the attributes that a node is created with.</p><br><br><table><tr><td><code>podCIDR</code></td><td><em>string</em></td><td><em>(Optional)</em><p>PodCIDR represents the pod IP range assigned to the node.</p></td></tr><tr><td><code>podCIDRs</code></td><td><em>[]string</em></td><td><em>(Optional)</em><p>podCIDRs represents the IP ranges assigned to the node for usage by Pods on that node. If this
field is specified, the 0th entry must match the podCIDR field. It may contain at most 1 value for
each of IPv4 and IPv6.</p></td></tr><tr><td><code>providerID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>ID of the node assigned by the cloud provider in the format: <providername>://<providerspecificnodeid></p></td></tr><tr><td><code>unschedulable</code></td><td><em>bool</em></td><td><em>(Optional)</em><p>Unschedulable controls node schedulability of new pods. By default, node is schedulable.
More info: <a href=https://kubernetes.io/docs/concepts/nodes/node/#manual-node-administration>https://kubernetes.io/docs/concepts/nodes/node/#manual-node-administration</a></p></td></tr><tr><td><code>taints</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#taint-v1-core>[]Kubernetes core/v1.Taint</a></em></td><td><em>(Optional)</em><p>If specified, the node’s taints.</p></td></tr><tr><td><code>configSource</code></td><td><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#nodeconfigsource-v1-core>Kubernetes core/v1.NodeConfigSource</a></em></td><td><em>(Optional)</em><p>Deprecated: Previously used to specify the source of the node’s configuration for the DynamicKubeletConfig feature. This feature is removed.</p></td></tr><tr><td><code>externalID</code></td><td><em>string</em></td><td><em>(Optional)</em><p>Deprecated. Not all kubelets will set this field. Remove field after 1.13.
see: <a href=https://issues.k8s.io/61966>https://issues.k8s.io/61966</a></p></td></tr></table></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.RollbackConfig><b>RollbackConfig</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentSpec>MachineDeploymentSpec</a>)</p><p><p>RollbackConfig is the config to rollback a MachineDeployment</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>revision</code></td><td><em>int64</em></td><td><em>(Optional)</em><p>The revision to rollback to. If set to 0, rollback to the last revision.</p></td></tr></tbody></table><br><h3 id=machine.sapcloud.io/v1alpha1.RollingUpdateMachineDeployment><b>RollingUpdateMachineDeployment</b></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/machine-controller-manager/documents/apis/#machine.sapcloud.io/v1alpha1.MachineDeploymentStrategy>MachineDeploymentStrategy</a>)</p><p><p>RollingUpdateMachineDeployment is the spec to control the desired behavior of rolling update.</p></p><table><thead><tr><th>Field</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>maxUnavailable</code></td><td><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/util/intstr#IntOrString>k8s.io/apimachinery/pkg/util/intstr.IntOrString</a></em></td><td><em>(Optional)</em><p>The maximum number of machines that can be unavailable during the update.
Value can be an absolute number (ex: 5) or a percentage of desired machines (ex: 10%).
Absolute number is calculated from percentage by rounding down.
This can not be 0 if MaxSurge is 0.
By default, a fixed value of 1 is used.
Example: when this is set to 30%, the old MC can be scaled down to 70% of desired machines
immediately when the rolling update starts. Once new machines are ready, old MC
can be scaled down further, followed by scaling up the new MC, ensuring
that the total number of machines available at all times during the update is at
least 70% of desired machines.</p></td></tr><tr><td><code>maxSurge</code></td><td><em><a href=https://godoc.org/k8s.io/apimachinery/pkg/util/intstr#IntOrString>k8s.io/apimachinery/pkg/util/intstr.IntOrString</a></em></td><td><em>(Optional)</em><p>The maximum number of machines that can be scheduled above the desired number of
machines.
Value can be an absolute number (ex: 5) or a percentage of desired machines (ex: 10%).
This can not be 0 if MaxUnavailable is 0.
Absolute number is calculated from percentage by rounding up.
By default, a value of 1 is used.
Example: when this is set to 30%, the new MC can be scaled up immediately when
the rolling update starts, such that the total number of old and new machines do not exceed
130% of desired machines. Once old machines have been killed,
new MC can be scaled up further, ensuring that total number of machines running
at any time during the update is atmost 130% of desired machines.</p></td></tr></tbody></table><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-9c506ae4839d6c04412db7f05a0e4d2a>2.2 - Proposals</h1></div><div class=td-content><h1 id=pg-62e1d100b3bccabcb90bdb26a65462a9>2.2.1 - Excess Reserve Capacity</h1><h1 id=excess-reserve-capacity>Excess Reserve Capacity</h1><ul><li><a href=/docs/other-components/machine-controller-manager/proposals/excess_reserve_capacity/#excess-reserve-capacity>Excess Reserve Capacity</a><ul><li><a href=/docs/other-components/machine-controller-manager/proposals/excess_reserve_capacity/#goal>Goal</a></li><li><a href=/docs/other-components/machine-controller-manager/proposals/excess_reserve_capacity/#note>Note</a></li><li><a href=/docs/other-components/machine-controller-manager/proposals/excess_reserve_capacity/#possible-approaches>Possible Approaches</a><ul><li><a href=/docs/other-components/machine-controller-manager/proposals/excess_reserve_capacity/#approach-1-enhance-machine-controller-manager-to-also-entertain-the-excess-machines>Approach 1: Enhance Machine-controller-manager to also entertain the excess machines</a></li><li><a href=/docs/other-components/machine-controller-manager/proposals/excess_reserve_capacity/#approach-2-enhance-cluster-autoscaler-by-simulating-fake-pods-in-it>Approach 2: Enhance Cluster-autoscaler by simulating fake pods in it</a></li><li><a href=/docs/other-components/machine-controller-manager/proposals/excess_reserve_capacity/#approach-3-enhance-cluster-autoscaler-to-support-pluggable-scaling-events>Approach 3: Enhance cluster-autoscaler to support pluggable scaling-events</a></li><li><a href=/docs/other-components/machine-controller-manager/proposals/excess_reserve_capacity/#approach-4-make-intelligent-use-of-low-priority-pods>Approach 4: Make intelligent use of Low-priority pods</a></li></ul></li></ul></li></ul><h2 id=goal>Goal</h2><p>Currently, autoscaler optimizes the number of machines for a given application-workload. Along with effective resource utilization, this feature brings concern where, many times, when new application instances are created - they don&rsquo;t find space in existing cluster. This leads the cluster-autoscaler to create new machines via MachineDeployment, which can take from 3-4 minutes to ~10 minutes, for the machine to really come-up and join the cluster. In turn, application-instances have to wait till new machines join the cluster.</p><p>One of the promising solutions to this issue is Excess Reserve Capacity. Idea is to keep a certain number of machines or percent of resources[cpu/memory] always available, so that new workload, in general, can be scheduled immediately unless huge spike in the workload. Also, the user should be given enough flexibility to choose how many resources or how many machines should be kept alive and non-utilized as this affects the Cost directly.</p><h2 id=note>Note</h2><ul><li>We decided to go with Approach-4 which is based on low priority pods. Please find more details here: <a href=https://github.com/gardener/gardener/issues/254>https://github.com/gardener/gardener/issues/254</a></li><li>Approach-3 looks more promising in long term, we may decide to adopt that in future based on developments/contributions in autoscaler-community.</li></ul><h2 id=possible-approaches>Possible Approaches</h2><p>Following are the possible approaches, we could think of so far.</p><h3 id=approach-1-enhance-machine-controller-manager-to-also-entertain-the-excess-machines>Approach 1: Enhance Machine-controller-manager to also entertain the excess machines</h3><ul><li><p>Machine-controller-manager currently takes care of the machines in the shoot cluster starting from creation-deletion-health check to efficient rolling-update of the machines. From the architecture point of view, MachineSet makes sure that X number of machines are always <strong>running and healthy</strong>. MachineDeployment controller smartly uses this facility to perform rolling-updates.</p></li><li><p>We can expand the scope of MachineDeployment controller to maintain excess number of machines by introducing new parallel independent controller named <em>MachineTaint</em> controller. This will result in MCM to include Machine, MachineSet, MachineDeployment, MachineSafety, MachineTaint controllers. MachineTaint controller does not need to introduce any new CRD - analogy fits where taint-controller also resides into kube-controller-manager.</p></li><li><p>Only Job of MachineTaint controller will be:</p><ul><li>List all the Machines under each MachineDeployment.</li><li>Maintain taints of <a href=https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/><em>noSchedule</em> and <em>noExecute</em></a> on <code>X</code> latest MachineObjects.</li><li>There should be an event-based informer mechanism where MachineTaintController gets to know about any Update/Delete/Create event of MachineObjects - in turn, maintains the <em>noSchedule</em> and <em>noExecute</em> taints on all the <em>latest</em> machines.
- Why latest machines?
- Whenever autoscaler decides to add new machines - essentially ScaleUp event - taints from the older machines are removed and newer machines get the taints. This way X number of Machines immediately becomes free for new pods to be scheduled.
- While ScaleDown event, autoscaler specifically mentions which machines should be deleted, and that should not bring any concerns. Though we will have to put proper label/annotation defined by autoscaler on taintedMachines, so that autoscaler does not consider the taintedMachines for deletion while scale-down.
* Annotation on tainted node: <code>"cluster-autoscaler.kubernetes.io/scale-down-disabled": "true"</code></li></ul></li><li><p>Implementation Details:</p><ul><li>Expect new <strong>optional field</strong> <em>ExcessReplicas</em> in <code>MachineDeployment.Spec</code>. MachineDeployment controller now adds both <code>Spec.Replicas</code> and <code>Spec.ExcessReplicas</code>[if provided], and considers that as a standard desiredReplicas.
- Current working of MCM will not be affected if ExcessReplicas field is kept nil.</li><li>MachineController currently reads the <em>NodeObject</em> and sets the MachineConditions in MachineObject. Machine-controller will now also read the taints/labels from the MachineObject - and maintains it on the <em>NodeObject</em>.</li></ul></li><li><p>We expect cluster-autoscaler to intelligently make use of the provided feature from MCM.</p><ul><li>CA gets the input of <em>min:max:excess</em> from Gardener. CA continues to set the <code>MachineDeployment.Spec.Replicas</code> as usual based on the application-workload.</li><li>In addition, CA also sets the <code>MachieDeployment.Spec.ExcessReplicas</code> .</li><li>Corner-case:
* CA should decrement the excessReplicas field accordingly when <em>desiredReplicas+excessReplicas</em> on MachineDeployment goes beyond <em>max</em>.</li></ul></li></ul><h3 id=approach-2-enhance-cluster-autoscaler-by-simulating-fake-pods-in-it>Approach 2: Enhance Cluster-autoscaler by simulating fake pods in it</h3><ul><li>There was already an attempt by community to support this feature.<ul><li>Refer for details to: <a href=https://github.com/kubernetes/autoscaler/pull/77/files>https://github.com/kubernetes/autoscaler/pull/77/files</a></li></ul></li></ul><h3 id=approach-3-enhance-cluster-autoscaler-to-support-pluggable-scaling-events>Approach 3: Enhance cluster-autoscaler to support pluggable scaling-events</h3><ul><li>Forked version of cluster-autoscaler could be improved to plug-in the algorithm for excess-reserve capacity.</li><li>Needs further discussion around upstream support.</li><li>Create golang channel to separate the algorithms to trigger scaling (hard-coded in cluster-autoscaler, currently) from the algorithms about how to to achieve the scaling (already pluggable in cluster-autoscaler). This kind of separation can help us introduce/plug-in new algorithms (such as based node resource utilisation) without affecting existing code-base too much while almost completely re-using the code-base for the actual scaling.</li><li>Also this approach is not specific to our fork of cluster-autoscaler. It can be made upstream eventually as well.</li></ul><h3 id=approach-4-make-intelligent-use-of-low-priority-pods>Approach 4: Make intelligent use of Low-priority pods</h3><ul><li>Refer to: <a href=https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/>pod-priority-preemption</a></li><li>TL; DR:<ul><li>High priority pods can preempt the low-priority pods which are already scheduled.</li><li>Pre-create bunch[equivivalent of X shoot-control-planes] of low-priority pods with priority of zero, then start creating the workload pods with better priority which will reschedule the low-priority pods or otherwise keep them in pending state if the limit for max-machines has reached.</li><li>This is still alpha feature.</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-099b6b715491def6df5afea44f6779f6>2.2.2 - GRPC Based Implementation of Cloud Providers</h1><h1 id=grpc-based-implementation-of-cloud-providers---wip>GRPC based implementation of Cloud Providers - WIP</h1><h2 id=goal>Goal:</h2><p>Currently the Cloud Providers&rsquo; (CP) functionalities ( Create(), Delete(), List() ) are part of the Machine Controller Manager&rsquo;s (MCM)repository. Because of this, adding support for new CPs into MCM requires merging code into MCM which may not be required for core functionalities of MCM itself. Also, for various reasons it may not be feasible for all CPs to merge their code with MCM which is an Open Source project.</p><p>Because of these reasons, it was decided that the CP&rsquo;s code will be moved out in separate repositories so that they can be maintained separately by the respective teams. Idea is to make MCM act as a GRPC server, and CPs as GRPC clients. The CP can register themselves with the MCM using a GRPC service exposed by the MCM. Details of this approach is discussed below.</p><h2 id=how-it-works>How it works:</h2><p>MCM acts as GRPC server and listens on a pre-defined port 5000. It implements below GRPC services. Details of each of these services are mentioned in next section.</p><ul><li><code>Register()</code></li><li><code>GetMachineClass()</code></li><li><code>GetSecret()</code></li></ul><h2 id=grpc-services-exposed-by-mcm>GRPC services exposed by MCM:</h2><h3 id=register>Register()</h3><p><code>rpc Register(stream DriverSide) returns (stream MCMside) {}</code></p><p>The CP GRPC client calls this service to register itself with the MCM. The CP passes the <code>kind</code> and the <code>APIVersion</code> which it implements, and MCM maintains an internal map for all the registered clients. A GRPC stream is returned in response which is kept open througout the life of both the processes. MCM uses this stream to communicate with the client for machine operations: <code>Create()</code>, <code>Delete()</code> or <code>List()</code>.
The CP client is responsible for reading the incoming messages continuously, and based on the <code>operationType</code> parameter embedded in the message, it is supposed to take the required action. This part is already handled in the package <code>grpc/infraclient</code>.
To add a new CP client, import the package, and implement the <code>ExternalDriverProvider</code> interface:</p><pre tabindex=0><code>type ExternalDriverProvider interface {
	Create(machineclass *MachineClassMeta, credentials, machineID, machineName string) (string, string, error)
	Delete(machineclass *MachineClassMeta, credentials, machineID string) error
	List(machineclass *MachineClassMeta, credentials, machineID string) (map[string]string, error)
}
</code></pre><h3 id=getmachineclass>GetMachineClass()</h3><p><code>rpc GetMachineClass(MachineClassMeta) returns (MachineClass) {}</code></p><p>As part of the message from MCM for various machine operations, the name of the machine class is sent instead of the full machine class spec. The CP client is expected to use this GRPC service to get the full spec of the machine class. This optionally enables the client to cache the machine class spec, and make the call only if the machine calass spec is not already cached.</p><h3 id=getsecret>GetSecret()</h3><p><code>rpc GetSecret(SecretMeta) returns (Secret) {}</code></p><p>As part of the message from MCM for various machine operations, the Cloud Config (CC) and CP credentials are not sent. The CP client is expected to use this GRPC service to get the secret which has CC and CP&rsquo;s credentials from MCM. This enables the client to cache the CC and credentials, and to make the call only if the data is not already cached.</p><h2 id=how-to-add-a-new-cloud-providers-support>How to add a new Cloud Provider&rsquo;s support</h2><p>Import the package <code>grpc/infraclient</code> and <code>grpc/infrapb</code> from MCM (currently in MCM&rsquo;s &ldquo;grpc-driver&rdquo; branch)</p><ul><li>Implement the interface <code>ExternalDriverProvider</code><ul><li><code>Create()</code>: Creates a new machine</li><li><code>Delete()</code>: Deletes a machine</li><li><code>List()</code>: Lists machines</li></ul></li><li>Use the interface <code>MachineClassDataProvider</code><ul><li><code>GetMachineClass()</code>: Makes the call to MCM to get machine class spec</li><li><code>GetSecret()</code>: Makes the call to MCM to get secret containing Cloud Config and CP&rsquo;s credentials</li></ul></li></ul><h3 id=example-implementation>Example implementation:</h3><p>Refer GRPC based implementation for AWS client:
<a href=https://github.com/ggaurav10/aws-driver-grpc>https://github.com/ggaurav10/aws-driver-grpc</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-9146991d7ac0a6e2b783c8a7932c0c52>2.2.3 - Hotupdate Instances</h1><h1 id=hot-update-virtualmachine-tags-without-triggering-a-rolling-update>Hot-Update VirtualMachine tags without triggering a rolling-update</h1><ul><li><a href=/docs/other-components/machine-controller-manager/proposals/hotupdate-instances/#hot-update-virtualmachine-tags-without-triggering-a-rolling-update>Hot-Update VirtualMachine tags without triggering a rolling-update</a><ul><li><a href=/docs/other-components/machine-controller-manager/proposals/hotupdate-instances/#motivation>Motivation</a></li><li><a href=/docs/other-components/machine-controller-manager/proposals/hotupdate-instances/#boundary-condition>Boundary Condition</a></li><li><a href=/docs/other-components/machine-controller-manager/proposals/hotupdate-instances/#what-is-available-today>What is available today?</a></li><li><a href=/docs/other-components/machine-controller-manager/proposals/hotupdate-instances/#what-are-the-problems-with-the-current-approach>What are the problems with the current approach?</a><ul><li><a href=/docs/other-components/machine-controller-manager/proposals/hotupdate-instances/#machineclass-update-and-its-impact>MachineClass Update and its impact</a></li></ul></li><li><a href=/docs/other-components/machine-controller-manager/proposals/hotupdate-instances/#proposal>Proposal</a><ul><li><a href=/docs/other-components/machine-controller-manager/proposals/hotupdate-instances/#shoot-yaml-changes>Shoot YAML changes</a></li><li><a href=/docs/other-components/machine-controller-manager/proposals/hotupdate-instances/#provider-specific-workerconfig-api-changes>Provider specific WorkerConfig API changes</a></li><li><a href=/docs/other-components/machine-controller-manager/proposals/hotupdate-instances/#gardener-provider-extension-changes>Gardener provider extension changes</a></li><li><a href=/docs/other-components/machine-controller-manager/proposals/hotupdate-instances/#driver-interface-changes>Driver interface changes</a></li><li><a href=/docs/other-components/machine-controller-manager/proposals/hotupdate-instances/#machine-class-reconciliation>Machine Class reconciliation</a><ul><li><a href=/docs/other-components/machine-controller-manager/proposals/hotupdate-instances/#reconciliation-changes>Reconciliation Changes</a></li></ul></li></ul></li></ul></li></ul><h2 id=motivation>Motivation</h2><ul><li><p><a href=https://github.com/gardener/machine-controller-manager/issues/750>MCM Issue#750</a> There is a requirement to provide a way for consumers to add tags which can be hot-updated onto VMs. This requirement can be generalized to also offer a convenient way to specify tags which can be applied to VMs, NICs, Devices etc.</p></li><li><p><a href=https://github.com/gardener/machine-controller-manager/issues/635>MCM Issue#635</a> which in turn points to <a href=https://github.com/gardener/machine-controller-manager-provider-aws/issues/36#issuecomment-677530395>MCM-Provider-AWS Issue#36</a> - The issue hints at other fields like enable/disable <a href=https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html#EIP_Disable_SrcDestCheck>source/destination checks for NAT instances</a> which needs to be hot-updated on network interfaces.</p></li><li><p>In GCP provider - <code>instance.ServiceAccounts</code> can be updated without the need to roll-over the instance. <a href=https://cloud.google.com/compute/docs/access/service-accounts>See</a></p></li></ul><h2 id=boundary-condition>Boundary Condition</h2><p>All tags that are added via means other than MachineClass.ProviderSpec should be preserved as-is. Only updates done to tags in <code>MachineClass.ProviderSpec</code> should be applied to the infra resources (VM/NIC/Disk).</p><h2 id=what-is-available-today>What is available today?</h2><p>WorkerPool configuration inside <a href=https://github.com/gardener/gardener/blob/fb29d38e6615ed17d409a8271a285254d9dd00ad/example/90-shoot.yaml#L61-L62>shootYaml</a> provides a way to set labels. As per the <a href=https://gardener.cloud/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.Worker>definition</a> these labels will be applied on <code>Node</code> resources. Currently these labels are also passed to the VMs as tags. There is no distinction made between <code>Node</code> labels and <code>VM</code> tags.</p><p><code>MachineClass</code> has a field which holds <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/v1alpha1/machineclass_types.go#L54>provider specific configuration</a> and one such configuration is <code>tags</code>. Gardener provider extensions updates the tags in <code>MachineClass</code>.</p><ul><li>AWS provider extension directly passes the labels to the <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/0a740eeca301320275d77d1c48d3c32d4ebcd7dd/pkg/controller/worker/machines.go#L158-L164>tag section</a> of machineClass.</li><li>Azure provider extension <a href=https://github.com/gardener/gardener-extension-provider-azure/blob/b6424f0122e174863e783555aa0ad68700edd87b/pkg/controller/worker/machines.go#L371-L373>sanitizes</a> the woker pool labels and adds them as <a href=https://github.com/gardener/gardener-extension-provider-azure/blob/b6424f0122e174863e783555aa0ad68700edd87b/pkg/controller/worker/machines.go#L187>tags in MachineClass</a>.</li><li>GCP provider extension <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/eb851f716e45336b486f3aaf46268859de2adecb/pkg/controller/worker/machines.go#L312-L315>sanitizes</a> them, and then sets them as <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/eb851f716e45336b486f3aaf46268859de2adecb/pkg/controller/worker/machines.go#L169>labels in the MachineClass</a>. In GCP tags only have keys and are currently <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/eb851f716e45336b486f3aaf46268859de2adecb/pkg/controller/worker/machines.go#L204-L207>hard coded</a>.</li></ul><p>Let us look at an example of <code>MachineClass.ProviderSpec</code> in AWS:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>providerSpec:
</span></span><span style=display:flex><span>  ami: ami-02fe00c0afb75bbd3
</span></span><span style=display:flex><span>  tags:
</span></span><span style=display:flex><span>    <span style=color:green>#[section-1] pool lables added by gardener extension</span>
</span></span><span style=display:flex><span>    <span style=color:green>#########################################################</span>
</span></span><span style=display:flex><span>    kubernetes.io/arch: amd64
</span></span><span style=display:flex><span>    networking.gardener.cloud/node-local-dns-enabled: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>    node.kubernetes.io/role: node
</span></span><span style=display:flex><span>    worker.garden.sapcloud.io/group: worker-ser234
</span></span><span style=display:flex><span>    worker.gardener.cloud/cri-name: containerd
</span></span><span style=display:flex><span>    worker.gardener.cloud/pool: worker-ser234
</span></span><span style=display:flex><span>    worker.gardener.cloud/system-components: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:green>#[section-2] Tags defined in the gardener-extension-provider-aws</span>
</span></span><span style=display:flex><span>    <span style=color:green>###########################################################</span>
</span></span><span style=display:flex><span>    kubernetes.io/cluster/cluster-full-name: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>    kubernetes.io/role/node: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:green>#[section-3]</span>
</span></span><span style=display:flex><span>    <span style=color:green>###########################################################</span>
</span></span><span style=display:flex><span>    user-defined-key1: user-defined-val1
</span></span><span style=display:flex><span>    user-defined-key2: user-defined-val2
</span></span></code></pre></div><blockquote><p>Refer <a href=https://github.com/gardener/gardener/blob/c11c86ae07d8ea784f5c41362cd41800f06bb3ed/pkg/operation/botanist/component/extensions/worker/worker.go#L171-L197>src</a> for tags defined in <code>section-1</code>.
Refer <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/0a740eeca301320275d77d1c48d3c32d4ebcd7dd/pkg/controller/worker/machines.go#L158-L164>src</a> for tags defined in <code>section-2</code>.
Tags in <code>section-3</code> are defined by the user.</p></blockquote><p>Out of the above three tag categories, MCM depends <code>section-2</code> tags (<code>mandatory-tags</code>) for its <code>orphan collection</code> and Driver&rsquo;s <code>DeleteMachine</code>and <code>GetMachineStatus</code> to work.</p><p><code>ProviderSpec.Tags</code> are transported to the provider specific resources as follows:</p><table><thead><tr><th>Provider</th><th>Resources Tags are set on</th><th>Code Reference</th><th>Comment</th></tr></thead><tbody><tr><td>AWS</td><td>Instance(VM), Volume, Network-Interface</td><td><a href=https://github.com/gardener/machine-controller-manager-provider-aws/blob/v0.17.0/pkg/aws/core.go#L116-L129>aws-VM-Vol-NIC</a></td><td>No distinction is made between tags set on VM, NIC or Volume</td></tr><tr><td>Azure</td><td>Instance(VM), Network-Interface</td><td><a href=https://github.com/gardener/machine-controller-manager-provider-azure/blob/v0.10.0/pkg/azure/utils.go#L234>azure-VM-parameters</a> & <a href=https://github.com/gardener/machine-controller-manager-provider-azure/blob/v0.10.0/pkg/azure/utils.go#L116>azureNIC-Parameters</a></td><td></td></tr><tr><td>GCP</td><td>Instance(VM), 1 tag: <code>name</code> (denoting the name of the worker) is added to Disk</td><td><a href=https://github.com/gardener/machine-controller-manager-provider-gcp/blob/v0.14.0/pkg/gcp/machine_controller_util.go#L78-L80>gcp-VM</a> & <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/v1.28.1/pkg/controller/worker/machines.go#L291-L293>gcp-Disk</a></td><td>In GCP key-value pairs are called <code>labels</code> while <code>network tags</code> have only keys</td></tr><tr><td>AliCloud</td><td>Instance(VM)</td><td><a href=https://github.com/gardener/machine-controller-manager-provider-alicloud/blob/master/pkg/spi/spi.go#L125-L129>aliCloud-VM</a></td><td></td></tr></tbody></table><h2 id=what-are-the-problems-with-the-current-approach>What are the problems with the current approach?</h2><p>There are a few shortcomings in the way tags/labels are handled:</p><ul><li>Tags can only be set at the time a machine is created.</li><li>There is no distinction made amongst tags/labels that are added to VM&rsquo;s, disks or network interfaces. As stated above for AWS same set of tags are added to all. There is a limit defined on the number of tags/labels that can be associated to the devices (disks, VMs, NICs etc). Example: In AWS a max of <a href=https://docs.aws.amazon.com/general/latest/gr/aws_tagging.html>50 user created tags are allowed</a>. Similar restrictions are applied on different resources across providers. Therefore adding all tags to all devices even if the subset of tags are not meant for that resource exhausts the total allowed tags/labels for that resource.</li><li>The only placeholder in shoot yaml as mentioned above is meant to only hold labels that should be applied on primarily on the <a href=https://github.com/gardener/gardener/blob/v1.66.1/pkg/apis/core/v1beta1/types_shoot.go#L1315-L1317>Node</a> objects. So while you could use the node labels for <a href=https://github.com/gardener/machine-controller-manager/issues/727>extended resources</a>, using it also for tags is not clean.</li><li>There is no provision in the shoot YAML today to add tags only to a subset of resources.</li></ul><h3 id=machineclass-update-and-its-impact>MachineClass Update and its impact</h3><p>When <a href=https://github.com/gardener/gardener/blob/v1.66.1/pkg/apis/core/types_shoot.go#L1042-L1043>Worker.ProviderConfig</a> is changed then a <a href=https://github.com/gardener/gardener/blob/v1.66.1/extensions/pkg/controller/worker/machines.go#L146-L148>worker-hash</a> is computed which includes the raw <code>ProviderConfig</code>. This hash value is then used as a suffix when constructing the name for a <code>MachineClass</code>. See <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/master/pkg/controller/worker/machines.go#L190>aws-extension-provider</a> as an example. A change in the name of the <code>MachineClass</code> will then in-turn trigger a rolling update of machines. Since <code>tags</code> are provider specific and therefore will be part of <code>ProviderConfig</code>, any update to them will result in a rolling-update of machines.</p><h2 id=proposal>Proposal</h2><h3 id=shoot-yaml-changes>Shoot YAML changes</h3><p>Provider specific configuration is set via <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L57-L58>providerConfig</a> section for each worker pool.</p><p>Example worker provider config (current):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>providerConfig:
</span></span><span style=display:flex><span>   apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>   kind: WorkerConfig
</span></span><span style=display:flex><span>   volume:
</span></span><span style=display:flex><span>     iops: 10000
</span></span><span style=display:flex><span>   dataVolumes:
</span></span><span style=display:flex><span>   - name: kubelet-dir
</span></span><span style=display:flex><span>     snapshotID: snap-13234
</span></span><span style=display:flex><span>   iamInstanceProfile: <span style=color:green># (specify either ARN or name)</span>
</span></span><span style=display:flex><span>     name: my-profile
</span></span><span style=display:flex><span>     arn: my-instance-profile-arn
</span></span></code></pre></div><p>It is proposed that an additional field be added for <code>tags</code> under <code>providerConfig</code>. Proposed changed YAML:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>providerConfig:
</span></span><span style=display:flex><span>   apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>   kind: WorkerConfig
</span></span><span style=display:flex><span>   volume:
</span></span><span style=display:flex><span>     iops: 10000
</span></span><span style=display:flex><span>   dataVolumes:
</span></span><span style=display:flex><span>   - name: kubelet-dir
</span></span><span style=display:flex><span>     snapshotID: snap-13234
</span></span><span style=display:flex><span>   iamInstanceProfile: <span style=color:green># (specify either ARN or name)</span>
</span></span><span style=display:flex><span>     name: my-profile
</span></span><span style=display:flex><span>     arn: my-instance-profile-arn
</span></span><span style=display:flex><span>   tags:
</span></span><span style=display:flex><span>     vm:
</span></span><span style=display:flex><span>       key1: val1
</span></span><span style=display:flex><span>       key2: val2
</span></span><span style=display:flex><span>       ..
</span></span><span style=display:flex><span>     <span style=color:green># for GCP network tags are just keys (there is no value associated to them). </span>
</span></span><span style=display:flex><span>     <span style=color:green># What is shown below will work for AWS provider.</span>
</span></span><span style=display:flex><span>     network:
</span></span><span style=display:flex><span>       key3: val3
</span></span><span style=display:flex><span>       key4: val4
</span></span></code></pre></div><p>Under <code>tags</code> clear distinction is made between tags for VMs, Disks, network interface etc. Each provider has a different allowed-set of characters that it accepts as key names, has different limits on the tags that can be set on a resource (disk, NIC, VM etc.) and also has a different format (GCP network tags are only keys).</p><blockquote><p>TODO:</p><ul><li><p>Check if worker.labels are getting added as tags on infra resources. We should continue to support it and double check that these should only be added to VMs and not to other resources.</p></li><li><p>Should we support users adding VM tags as node labels?</p></li></ul></blockquote><h3 id=provider-specific-workerconfig-api-changes>Provider specific WorkerConfig API changes</h3><blockquote><p>Taking <code>AWS</code> provider extension as an example to show the changes.</p></blockquote><p><a href=https://github.com/gardener/gardener-extension-provider-aws/blob/master/pkg/apis/aws/types_worker.go#L27-L38>WorkerConfig</a> will now have the following changes:</p><ol><li>A new field for tags will be introduced.</li><li>Additional metadata for struct fields will now be added via <code>struct tags</code>.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>type</span> WorkerConfig <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>    metav1.TypeMeta
</span></span><span style=display:flex><span>    Volume *Volume
</span></span><span style=display:flex><span>    <span style=color:green>// .. all fields are not mentioned here.
</span></span></span><span style=display:flex><span><span style=color:green></span>    <span style=color:green>// Tags are a collection of tags to be set on provider resources (e.g. VMs, Disks, Network Interfaces etc.)
</span></span></span><span style=display:flex><span><span style=color:green></span>    Tags *Tags <span style=color:#a31515>`hotupdatable:true`</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// Tags is a placeholder for all tags that can be set/updated on VMs, Disks and Network Interfaces.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> Tags <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>    <span style=color:green>// VM tags set on the VM instances.
</span></span></span><span style=display:flex><span><span style=color:green></span>    VM <span style=color:#00f>map</span>[<span style=color:#2b91af>string</span>]<span style=color:#2b91af>string</span>
</span></span><span style=display:flex><span>    <span style=color:green>// Network tags set on the network interfaces.
</span></span></span><span style=display:flex><span><span style=color:green></span>    Network <span style=color:#00f>map</span>[<span style=color:#2b91af>string</span>]<span style=color:#2b91af>string</span>
</span></span><span style=display:flex><span>    <span style=color:green>// Disk tags set on the volumes/disks.
</span></span></span><span style=display:flex><span><span style=color:green></span>    Disk <span style=color:#00f>map</span>[<span style=color:#2b91af>string</span>]<span style=color:#2b91af>string</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>There is a need to distinguish fields within <code>ProviderSpec</code> (which is then mapped to the above <code>WorkerConfig</code>) which can be updated without the need to change the hash suffix for <code>MachineClass</code> and thus trigger a rolling update on machines.</p><p>To achieve that we propose to use <strong>struct tag</strong> <code>hotupdatable</code> whose value indicates if the field can be updated without the need to do a rolling update. To ensure backward compatibility, all fields which do not have this tag or have <code>hotupdatable</code> set to <code>false</code> will be considered as immutable and will require a rolling update to take affect.</p><h3 id=gardener-provider-extension-changes>Gardener provider extension changes</h3><blockquote><p>Taking AWS provider extension as an example. Following changes should be made to all gardener provider extensions</p></blockquote><p>AWS Gardener Extension <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/v1.42.1/pkg/controller/worker/machines.go#L104-L107>generates machine config</a> using worker pool configuration. As part of that it also computes the <code>workerPoolHash</code> which is then used to create the <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/master/pkg/controller/worker/machines.go#L193>name of the MachineClass</a>.</p><p>Currently <code>WorkerPoolHash</code> function uses the <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/47d6bb34a538f3dfeedcf99361696de72d1eeae2/vendor/github.com/gardener/gardener/extensions/pkg/controller/worker/machines.go#L146-L148>entire providerConfig</a> to compute the hash. Proposal is to do the following:</p><ol><li>Remove the <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/47d6bb34a538f3dfeedcf99361696de72d1eeae2/vendor/github.com/gardener/gardener/extensions/pkg/controller/worker/machines.go#L146-L148>code</a> from function <code>WorkerPoolHash</code>.</li><li>Add another function to compute hash using all immutable fields in the provider config struct and then pass that to <code>worker.WorkerPoolHash</code> as <code>additionalData</code>.</li></ol><p>The above will ensure that tags and any other field in <code>WorkerConfig</code> which is marked with <code>updatable:true</code> is not considered for hash computation and will therefore not contribute to changing the name of <code>MachineClass</code> object thus preventing a rolling update.</p><p><code>WorkerConfig</code> and therefore the contained tags will be set as <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/v1alpha1/machineclass_types.go#L54>ProviderSpec</a> in <code>MachineClass</code>.</p><p>If only fields which have <code>updatable:true</code> are changed then it should result in update/patch of <code>MachineClass</code> and not creation.</p><h3 id=driver-interface-changes>Driver interface changes</h3><p><a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/driver/driver.go#L28>Driver</a> interface which is a facade to provider specific API implementations will have one additional method.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-golang data-lang=golang><span style=display:flex><span><span style=color:#00f>type</span> Driver <span style=color:#00f>interface</span> {
</span></span><span style=display:flex><span>    <span style=color:green>// .. existing methods are not mentioned here for brevity.
</span></span></span><span style=display:flex><span><span style=color:green></span>    UpdateMachine(context.Context, *UpdateMachineRequest) <span style=color:#2b91af>error</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// UpdateMachineRequest is the request to update machine tags. 
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> UpdateMachineRequest <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>    ProviderID <span style=color:#2b91af>string</span>
</span></span><span style=display:flex><span>    LastAppliedProviderSpec raw.Extension
</span></span><span style=display:flex><span>    MachineClass *v1alpha1.MachineClass
</span></span><span style=display:flex><span>    Secret *corev1.Secret
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><blockquote><p>If any <code>machine-controller-manager-provider-&lt;providername></code> has not implemented <code>UpdateMachine</code> then updates of tags on Instances/NICs/Disks will not be done. An error message will be logged instead.</p></blockquote><blockquote></blockquote><h3 id=machine-class-reconciliation>Machine Class reconciliation</h3><p>Current <a href=https://github.com/gardener/machine-controller-manager/blob/v0.48.1/pkg/util/provider/machinecontroller/machineclass.go#L140-L194>MachineClass reconciliation</a> does not reconcile <code>MachineClass</code> resource updates but it only enqueues associated machines. The reason is that it is assumed that anything that is changed in a MachineClass will result in a creation of a new MachineClass with a different name. This will result in a rolling update of all machines using the MachineClass as a template.</p><p>However, it is possible that there is data that all machines in a <code>MachineSet</code> share which do not require a rolling update (e.g. tags), therefore there is a need to reconcile the MachineClass as well.</p><h4 id=reconciliation-changes>Reconciliation Changes</h4><p>In order to ensure that machines get updated eventually with changes to the <code>hot-updatable</code> fields defined in the <code>MachineClass.ProviderConfig</code> as <code>raw.Extension</code>.</p><p>We should only fix <a href=https://github.com/gardener/machine-controller-manager/issues/751>MCM Issue#751</a> in the MachineClass reconciliation and let it enqueue the machines as it does today. We additionally propose the following two things:</p><ol><li><p>Introduce a new annotation <code>last-applied-providerspec</code> on every machine resource. This will capture the last successfully applied <code>MachineClass.ProviderSpec</code> on this instance.</p></li><li><p>Enhance the machine reconciliation to include code to hot-update machine.</p></li></ol><p>In <a href=https://github.com/gardener/machine-controller-manager/blob/v0.48.1/pkg/util/provider/machinecontroller/machine.go#L114>machine-reconciliation</a> there are currently two flows <code>triggerDeletionFlow</code> and <code>triggerCreationFlow</code>. When a machine gets enqueued due to changes in MachineClass then in this method following changes needs to be introduced:</p><p>Check if the machine has <code>last-applied-providerspec</code> annotation.</p><p><em>Case 1.1</em></p><p>If the annotation is not present then there can be just 2 possibilities:</p><ul><li><p>It is a fresh/new machine and no backing resources (VM/NIC/Disk) exist yet. The current flow checks if the providerID is empty and <code>Status.CurrenStatus.Phase</code> is empty then it enters into the <code>triggerCreationFlow</code>.</p></li><li><p>It is an existing machine which does not yet have this annotation. In this case call <code>Driver.UpdateMachine</code>. If the driver returns no error then add <code>last-applied-providerspec</code> annotation with the value of <code>MachineClass.ProviderSpec</code> to this machine.</p></li></ul><p><em>Case 1.2</em></p><p>If the annotation is present then compare the last applied provider-spec with the current provider-spec. If there are changes (check their hash values) then call <code>Driver.UpdateMachine</code>. If the driver returns no error then add <code>last-applied-providerspec</code> annotation with the value of <code>MachineClass.ProviderSpec</code> to this machine.</p><blockquote><p>NOTE: It is assumed that if there are changes to the fields which are not marked as <code>hotupdatable</code> then it will result in the change of name for MachineClass resulting in a rolling update of machines. If the name has not changed + machine is enqueued + there is a change in machine-class then it will be change to a hotupdatable fields in the spec.</p></blockquote><p>Trigger update flow can be done after <code>reconcileMachineHealth</code> and <code>syncMachineNodeTemplates</code> in <a href=https://github.com/gardener/machine-controller-manager/blob/v0.48.1/pkg/util/provider/machinecontroller/machine.go#L164-L175>machine-reconciliation</a>.</p><p>There are 2 edge cases that needs attention and special handling:</p><blockquote><p>Premise: It is identified that there is an update done to one or more hotupdatable fields in the MachineClass.ProviderSpec.</p></blockquote><p><em>Edge-Case-1</em></p><p>In the machine reconciliation, an update-machine-flow is triggered which in-turn calls <code>Driver.UpdateMachine</code>. Consider the case where the hot update needs to be done to all VM, NIC and Disk resources. The driver returns an error which indicates a <code>partial-failure</code>. As we have mentioned above only when <code>Driver.UpdateMachine</code> returns no error will <code>last-applied-providerspec</code> be updated. In case of partial failure the annotation will not be updated. This event will be re-queued for a re-attempt. However consider a case where before the item is re-queued, another update is done to MachineClass reverting back the changes to the original spec.</p><table><thead><tr><th>At T1</th><th>At T2 (T2 > T1)</th><th>At T3 (T3> T2)</th></tr></thead><tbody><tr><td>last-applied-providerspec=S1<br>MachineClass.ProviderSpec = S1</td><td>last-applied-providerspec=S1<br>MachineClass.ProviderSpec = S2<br> Another update to MachineClass.ProviderConfig = S3 is enqueue (S3 == S1)</td><td>last-applied-providerspec=S1<br>Driver.UpdateMachine for S1-S2 update - returns partial failure<br>Machine-Key is requeued</td></tr></tbody></table><p>At T4 (T4> T3) when a machine is reconciled then it checks that <code>last-applied-providerspec</code> is S1 and current MachineClass.ProviderSpec = S3 and since S3 is same as S1, no update is done. At T2 Driver.UpdateMachine was called to update the machine with <code>S2</code> but it partially failed. So now you will have resources which are partially updated with S2 and no further updates will be attempted.</p><p><em>Edge-Case-2</em></p><p>The above situation can also happen when <code>Driver.UpdateMachine</code> is in the process of updating resources. It has hot-updated lets say 1 resource. But now MCM crashes. By the time it comes up another update to MachineClass.ProviderSpec is done essentially reverting back the previous change (same case as above). In this case reconciliation loop never got a chance to get any response from the driver.</p><p>To handle the above edge cases there are 2 options:</p><p><em>Option #1</em></p><p>Introduce a new annotation <code>inflight-providerspec-hash</code> . The value of this annotation will be the hash value of the <code>MachineClass.ProviderSpec</code> that is in the process of getting applied on this machine. The machine will be updated with this annotation just before calling <code>Driver.UpdateMachine</code> (in the trigger-update-machine-flow). If the driver returns no error then (in a single update):</p><ol><li><p><code>last-applied-providerspec</code> will be updated</p></li><li><p><code>inflight-providerspec-hash</code> annotation will be removed.</p></li></ol><p><em>Option #2</em> - Preferred</p><p>Leverage <code>Machine.Status.LastOperation</code> with <code>Type</code> set to <code>MachineOperationUpdate</code> and <code>State</code> set to <code>MachineStateProcessing</code> This status will be updated just before calling <code>Driver.UpdateMachine</code>.</p><p>Semantically <code>LastOperation</code> captures the details of the operation post-operation and not pre-operation. So this solution would be a divergence from the norm.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-52cc8311c573a80bf6dbfdc71d466074>2.2.4 - Initialize Machine</h1><h1 id=post-create-initialization-of-machine-instance>Post-Create Initialization of Machine Instance</h1><h2 id=background>Background</h2><p>Today the <a href=https://github.com/gardener/machine-controller-manager/blob/rel-v0.49/pkg/util/provider/driver/driver.go#L28>driver.Driver</a> facade represents the boundary between the the <code>machine-controller</code> and its various provider specific implementations.</p><p>We have abstract operations for creation/deletion and listing of machines (actually compute instances) but we do not correctly handle post-creation initialization logic. Nor do we provide an abstract operation to represent the hot update of an instance after creation.</p><p>We have found this to be necessary for several use cases. Today in the MCM AWS Provider, we already misuse <code>driver.GetMachineStatus</code> which is supposed to be a read-only operation obtaining the status of an instance.</p><ol><li><p>Each AWS EC2 instance performs source/destination checks by default.
For <a href=https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html#EIP_Disable_SrcDestCheck>EC2 NAT</a> instances
these should be disabled. This is done by issuing
a <a href=https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_ModifyInstanceAttribute.html>ModifyInstanceAttribute</a> request with the <code>SourceDestCheck</code> set to <code>false</code>. The MCM AWS Provider, decodes the <a href=https://github.com/gardener/machine-controller-manager-provider-aws/blob/39318bb2b5b4a573fdc77eaf400839d12c4abf59/pkg/aws/apis/aws_provider_spec.go#L63>AWSProviderSpec</a>, reads <code>providerSpec.SrcAndDstChecksEnabled</code> and correspondingly issues the call to modify the already launched instance. However, this should be done as an action after creating the instance and should not be part of the VM status retrieval.</p></li><li><p>Similarly, there is a <a href=https://github.com/gardener/machine-controller-manager-provider-aws/pull/128>pending PR</a> to add the <code>Ipv6AddessCount</code> and <code>Ipv6PrefixCount</code> to enable the assignment of an ipv6 address and an ipv6 prefix to instances. This requires constructing and issuing an <a href=https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_AssignIpv6Addresses.html>AssignIpv6Addresses</a> request after the EC2 instance is available.</p></li><li><p>We have other uses-cases such as <a href=https://github.com/gardener/machine-controller-manager/issues/750>MCM Issue#750</a> where there is a requirement to provide a way for consumers to add tags which can be hot-updated onto instances. This requirement can be generalized to also offer a convenient way to specify tags which can be applied to VMs, NICs, Devices etc.</p></li><li><p>We have a need for &ldquo;machine-instance-not-ready&rdquo; taint as described in <a href=https://github.com/gardener/machine-controller-manager/issues/740>MCM#740</a> which should only get removed once the post creation updates are finished.</p></li></ol><h2 id=objectives>Objectives</h2><p>We will split the fulfilment of this overall need into 2 stages of implementation.</p><ol><li><p><strong>Stage-A</strong>: Support post-VM creation initialization logic of the instance suing a proposed <code>Driver.InitializeMachine</code> by permitting provider implementors to add initialization logic after VM creation, return with special new error code <code>codes.Initialization</code> for initialization errors and correspondingly support a new machine operation stage <code>InstanceInitialization</code> which will be updated in the machine <code>LastOperation</code>. The <a href=https://github.com/gardener/machine-controller-manager/blob/rel-v0.50/pkg/util/provider/machinecontroller/machine.go#L310>triggerCreationFlow</a> - a reconciliation sub-flow of the MCM responsible for orchestrating instance creation and updating machine status will be changed to support this behaviour.</p></li><li><p><strong>Stage-B</strong>: Introduction of <code>Driver.UpdateMachine</code> and enhancing the MCM, MCM providers and gardener extension providers to support hot update of instances through <code>Driver.UpdateMachine</code>. The MCM <a href=https://github.com/gardener/machine-controller-manager/blob/v0.50.1/pkg/util/provider/machinecontroller/machine.go#L531>triggerUpdationFlow</a> - a reconciliation sub-flow of the MCM which is supposed to be responsible for orchestrating instance update - but currently not used, will be updated to invoke the provider <code>Driver.UpdateMachine</code> on hot-updates to to the <code>Machine</code> object</p></li></ol><h2 id=stage-a-proposal>Stage-A Proposal</h2><h3 id=current-mcm-triggercreationflow>Current MCM triggerCreationFlow</h3><p>Today, <a href=https://github.com/gardener/machine-controller-manager/blob/v0.50.1/pkg/util/provider/machinecontroller/machine.go#L89>reconcileClusterMachine</a> which is the main routine for the <code>Machine</code> object reconciliation invokes <a href=https://github.com/gardener/machine-controller-manager/blob/rel-v0.50/pkg/util/provider/machinecontroller/machine.go#L310>triggerCreationFlow</a> at the end when the <code>machine.Spec.ProviderID</code> is empty or if the <code>machine.Status.CurrentStatus.Phase</code> is empty or in <code>CrashLoopBackOff</code></p><pre class=mermaid>%%{ init: {
    &#39;themeVariables&#39;:
        { &#39;fontSize&#39;: &#39;12px&#39;}
} }%%
flowchart LR

other[&#34;...&#34;]
--&gt;chk{&#34;machine ProviderID empty
OR
Phase empty or CrashLoopBackOff ?
&#34;}--yes--&gt;triggerCreationFlow
chk--noo--&gt;LongRetry[&#34;return machineutils.LongRetry&#34;]</pre><p>Today, the <code>triggerCreationFlow</code> is illustrated below with some minor details omitted/compressed for brevity</p><p><em>NOTES</em></p><ul><li>The <code>lastop</code> below is an abbreviation for <code>machine.Status.LastOperation</code>. This, along with the machine phase is generally updated on the <code>Machine</code> object just before returning from the method.</li><li>regarding <code>phase=CrashLoopBackOff|Failed</code>. the machine phase may either be <code>CrashLoopBackOff</code> or move to <code>Failed</code> if the difference between current time and the <code>machine.CreationTimestamp</code> has exceeded the configured <code>MachineCreationTimeout</code>.</li></ul><pre class=mermaid>%%{ init: {
    &#39;themeVariables&#39;:
        { &#39;fontSize&#39;: &#39;12px&#39;}
} }%%
flowchart TD


end1((&#34;end&#34;))
begin((&#34; &#34;))
medretry[&#34;return MediumRetry, err&#34;]
shortretry[&#34;return ShortRetry, err&#34;]
medretry--&gt;end1
shortretry--&gt;end1

begin--&gt;AddBootstrapTokenToUserData
--&gt;gms[&#34;statusResp,statusErr=driver.GetMachineStatus(...)&#34;]
--&gt;chkstatuserr{&#34;Check statusErr&#34;}
chkstatuserr--notFound--&gt;chknodelbl{&#34;Chk Node Label&#34;}
chkstatuserr--else--&gt;createFailed[&#34;lastop.Type=Create,lastop.state=Failed,phase=CrashLoopBackOff|Failed&#34;]--&gt;medretry
chkstatuserr--nil--&gt;initnodename[&#34;nodeName = statusResp.NodeName&#34;]--&gt;setnodename


chknodelbl--notset--&gt;createmachine[&#34;createResp, createErr=driver.CreateMachine(...)&#34;]--&gt;chkCreateErr{&#34;Check createErr&#34;}

chkCreateErr--notnil--&gt;createFailed

chkCreateErr--nil--&gt;getnodename[&#34;nodeName = createResp.NodeName&#34;]
--&gt;chkstalenode{&#34;nodeName != machine.Name\n//chk stale node&#34;}
chkstalenode--false--&gt;setnodename[&#34;if unset machine.Labels[&#39;node&#39;]= nodeName&#34;]
--&gt;machinepending[&#34;if empty/crashloopbackoff lastop.type=Create,lastop.State=Processing,phase=Pending&#34;]
--&gt;shortretry

chkstalenode--true--&gt;delmachine[&#34;driver.DeleteMachine(...)&#34;]
--&gt;permafail[&#34;lastop.type=Create,lastop.state=Failed,Phase=Failed&#34;]
--&gt;shortretry

subgraph noteA [&#34; &#34;]
    permafail -.- note1([&#34;VM was referring to stale node obj&#34;])
end
style noteA opacity:0


subgraph noteB [&#34; &#34;]
    setnodename-.- note2([&#34;Proposal: Introduce Driver.InitializeMachine after this&#34;])
end</pre><h3 id=enhancement-of-mcm-triggercreationflow>Enhancement of MCM triggerCreationFlow</h3><h4 id=relevant-observations-on-current-flow>Relevant Observations on Current Flow</h4><ol><li>Observe that we always perform a call to <code>Driver.GetMachineStatus</code> and only then conditionally perform a call to <code>Driver.CreateMachine</code> if there was was no machine found.</li><li>Observe that after the call to a successful <code>Driver.CreateMachine</code>, the machine phase is set to <code>Pending</code>, the <code>LastOperation.Type</code> is currently set to <code>Create</code> and the <code>LastOperation.State</code> set to <code>Processing</code> before returning with a <code>ShortRetry</code>. The <code>LastOperation.Description</code> is (unfortunately) set to the fixed message: <code>Creating machine on cloud provider</code>.</li><li>Observe that after an erroneous call to <code>Driver.CreateMachine</code>, the machine phase is set to <code>CrashLoopBackOff</code> or <code>Failed</code> (in case of creation timeout).</li></ol><p>The following changes are proposed with a view towards minimal impact on current code and no introduction of a new Machine Phase.</p><h4 id=mcm-changes>MCM Changes</h4><ol><li>We propose introducing a new machine operation <code>Driver.InitializeMachine</code> with the following signature<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>type</span> Driver <span style=color:#00f>interface</span> {
</span></span><span style=display:flex><span>    <span style=color:green>// .. existing methods are omitted for brevity.
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span>    <span style=color:green>// InitializeMachine call is responsible for post-create initialization of the provider instance.
</span></span></span><span style=display:flex><span><span style=color:green></span>    InitializeMachine(context.Context, *InitializeMachineRequest) <span style=color:#2b91af>error</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// InitializeMachineRequest is the initialization request for machine instance initialization
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> InitializeMachineRequest <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>    <span style=color:green>// Machine object whose VM instance should be initialized 
</span></span></span><span style=display:flex><span><span style=color:green></span>    Machine *v1alpha1.Machine
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:green>// MachineClass backing the machine object
</span></span></span><span style=display:flex><span><span style=color:green></span>    MachineClass *v1alpha1.MachineClass
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:green>// Secret backing the machineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>    Secret *corev1.Secret
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></li><li>We propose introducing a new MC error code <code>codes.Initialization</code> indicating that the VM Instance was created but there was an error in initialization after VM creation. The implementor of <code>Driver.InitializeMachine</code> can return this error code, indicating that <code>InitializeMachine</code> needs to be called again. The Machine Controller will change the phase to <code>CrashLoopBackOff</code> as usual when encountering a <code>codes.Initialization</code> error.</li><li>We will introduce a new <em>machine operation</em> stage <code>InstanceInitialization</code>. In case of an <code>codes.Initialization</code> error<ol><li>the <code>machine.Status.LastOperation.Description</code> will be set to <code>InstanceInitialization</code>,</li><li><code>machine.Status.LastOperation.ErrorCode</code> will be set to <code>codes.Initialization</code></li><li>the <code>LastOperation.Type</code> will be set to <code>Create</code></li><li>the <code>LastOperation.State</code> set to <code>Failed</code> before returning with a <code>ShortRetry</code></li></ol></li><li>The semantics of <code>Driver.GetMachineStatus</code> will be changed. If the instance associated with machine exists, but the instance was not initialized as expected, the provider implementations of <code>GetMachineStatus</code> should return an error: <code>status.Error(codes.Initialization)</code>.</li><li>If <code>Driver.GetMachineStatus</code> returned an error encapsulating <code>codes.Initialization</code> then <code>Driver.InitializeMachine</code> will be invoked again in the <code>triggerCreationFlow</code>.</li><li>As according to the usual logic, the main machine controller reconciliation loop will now re-invoke the <code>triggerCreationFlow</code> again if the machine phase is <code>CrashLoopBackOff</code>.</li></ol><h4 id=illustration>Illustration</h4><p><img src=/__resources/initialize-machine-triggercreationflow_9b7945.svg alt="Enhanced triggerCreationFlow"></p><h4 id=aws-provider-changes>AWS Provider Changes</h4><h5 id=driverinitializemachine>Driver.InitializeMachine</h5><p>The implementation for the AWS Provider will look something like:</p><ol><li>After the VM instance is available, check <code>providerSpec.SrcAndDstChecksEnabled</code>, construct <code>ModifyInstanceAttributeInput</code> and call <code>ModifyInstanceAttribute</code>. In case of an error return <code>codes.Initialization</code> instead of the current <code>codes.Internal</code></li><li>Check <code>providerSpec.NetworkInterfaces</code> and if <code>Ipv6PrefixCount</code> is not <code>nil</code>, then construct <code>AssignIpv6AddressesInput</code> and call <code>AssignIpv6Addresses</code>. In case of an error return <code>codes.Initialization</code>. Don&rsquo;t use the generic <code>codes.Internal</code></li></ol><p>The <a href=https://github.com/gardener/machine-controller-manager-provider-aws/pull/128>existing Ipv6 PR</a> will need modifications.</p><h5 id=drivergetmachinestatus>Driver.GetMachineStatus</h5><ol><li>If <code>providerSpec.SrcAndDstChecksEnabled</code> is <code>false</code>, check <code>ec2.Instance.SourceDestCheck</code>. If it does not match then return <code>status.Error(codes.Initialization)</code></li><li>Check <code>providerSpec.NetworkInterfaces</code> and if <code>Ipv6PrefixCount</code> is not <code>nil</code>, check <code>ec2.Instance.NetworkInterfaces</code> and check if <code>InstanceNetworkInterface.Ipv6Addresses</code> has a non-nil slice. If this is not the case then return <code>status.Error(codes.Initialization)</code></li></ol><h3 id=instance-not-ready-taint>Instance Not Ready Taint</h3><ul><li>Due to the fact that creation flow for machines will now be enhanced to correctly support post-creation startup logic, we should not scheduled workload until this startup logic is complete. Even without this feature we have a need for such a taint as described in <a href=https://github.com/gardener/machine-controller-manager/issues/740>MCM#740</a></li><li>We propose a new taint <code>node.machine.sapcloud.io/instance-not-ready</code> which will be added as a node startup taint in gardener core <a href=https://github.com/gardener/gardener/blob/v1.83.1/pkg/component/extensions/operatingsystemconfig/original/components/kubelet/config.go#L101>KubeletConfiguration.RegisterWithTaints</a></li><li>The will will then removed by MCM in health check reconciliation, once the machine becomes fully ready. (when moving to <code>Running</code> phase)</li><li>We will add this taint as part of <code>--ignore-taint</code> in CA</li><li>We will introduce a disclaimer / prerequisite in the MCM FAQ, to add this taint as part of kubelet config under <code>--register-with-taints</code>, otherwise workload could get scheduled , before machine beomes <code>Running</code></li></ul><h2 id=stage-b-proposal>Stage-B Proposal</h2><h3 id=enhancement-of-driver-interface-for-hot-updation>Enhancement of Driver Interface for Hot Updation</h3><p>Kindly refer to the <a href=/docs/other-components/machine-controller-manager/proposals/hotupdate-instances/>Hot-Update Instances</a> design which provides elaborate detail.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4c81f3bece299b069a468ca5e8928e05>2.3 - ToDo</h1></div><div class=td-content><h1 id=pg-1ccd85a54ff93a5c31b2f99d1f444eb1>2.3.1 - Outline</h1><h1 id=machine-controller-manager>Machine Controller Manager</h1><p>CORE &ndash; ./machine-controller-manager(provider independent)
Out of tree : Machine controller (provider specific)
MCM is a set controllers:</p><ul><li><p>Machine Deployment Controller</p></li><li><p>Machine Set Controller</p></li><li><p>Machine Controller</p></li><li><p>Machine Safety Controller</p></li></ul><h2 id=questions-and-refactoring-suggestions>Questions and refactoring Suggestions</h2><h3 id=refactoring>Refactoring</h3><table><thead><tr><th>Statement</th><th>FilePath</th><th>Status</th></tr></thead><tbody><tr><td>ConcurrentNodeSyncs” bad name - nothing to do with node syncs actually.<br>If its value is ’10’ then it will start 10 goroutines (workers) per resource type (machine, machinist, machinedeployment, provider-specific-class, node - study the different resource types.</td><td>cmd/machine-controller-manager/app/options/options.go</td><td>pending</td></tr><tr><td>LeaderElectionConfiguration is very similar to the one present in “client-go/tools/leaderelection/leaderelection.go” - can we simply used the one in client-go instead of defining again?</td><td>pkg/options/types.go - MachineControllerManagerConfiguration</td><td>pending</td></tr><tr><td>Have all userAgents as constant. Right now there is just one.</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>Shouldn’t run function be defined on MCMServer struct itself?</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>clientcmd.BuildConfigFromFlags fallsback to inClusterConfig which will surely not work as that is not the target. Should it not check and exit early?</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>A more direct way to create an in cluster config is using <code>k8s.io/client-go/rest</code> -> rest.InClusterConfig instead of using clientcmd.BuildConfigFromFlags passing empty arguments and depending upon the implementation to fallback to creating a inClusterConfig. If they change the implementation that you get affected.</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>Introduce a method on MCMServer which gets a target KubeConfig and controlKubeConfig or alternatively which creates respective clients.</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>Why can’t we use Kubernetes.NewConfigOrDie also for kubeClientControl?</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>I do not see any benefit of client builders actually. All you need to do is pass in a config and then directly use client-go functions to create a client.</td><td>cmd/app/controllermanager.go - run Function</td><td>pending</td></tr><tr><td>Function: getAvailableResources - rename this to getApiServerResources</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>Move the method which waits for API server to up and ready to a separate method which returns a discoveryClient when the API server is ready.</td><td>cmd/app/controllermanager.go - getAvailableResources function</td><td>pending</td></tr><tr><td>Many methods in client-go used are now deprecated. Switch to the ones that are now recommended to be used instead.</td><td>cmd/app/controllermanager.go - startControllers</td><td>pending</td></tr><tr><td>This method needs a general overhaul</td><td>cmd/app/controllermanager.go - startControllers</td><td>pending</td></tr><tr><td>If the design is influenced/copied from KCM then its very different. There are different controller structs defined for deployment, replicaset etc which makes the code much more clearer. You can see “kubernetes/cmd/kube-controller-manager/apps.go” and then follow the trail from there. - agreed needs to be changed in future (if time permits)</td><td>pkg/controller/controller.go</td><td>pending</td></tr><tr><td>I am not sure why “MachineSetControlInterface”, “RevisionControlInterface”, “MachineControlInterface”, “FakeMachineControl” are defined in this file?</td><td>pkg/controller/controller_util.go</td><td>pending</td></tr><tr><td><code>IsMachineActive</code> - combine the first 2 conditions into one with OR.</td><td>pkg/controller/controller_util.go</td><td>pending</td></tr><tr><td>Minor change - correct the comment, first word should always be the method name. Currently none of the comments have correct names.</td><td>pkg/controller/controller_util.go</td><td>pending</td></tr><tr><td>There are too many deep copies made. What is the need to make another deep copy in this method? You are not really changing anything here.</td><td>pkg/controller/deployment.go - updateMachineDeploymentFinalizers</td><td>pending</td></tr><tr><td>Why can&rsquo;t these validations be done as part of a validating webhook?</td><td>pkg/controller/machineset.go - reconcileClusterMachineSet</td><td>pending</td></tr><tr><td>Small change to the following <code>if</code> condition. <code>else if</code> is not required a simple <code>else</code> is sufficient. <a href=/docs/other-components/machine-controller-manager/todo/outline/#1.1-code1>Code1</a></td><td></td><td></td></tr><tr><td>pkg/controller/machineset.go - reconcileClusterMachineSet</td><td>pending</td><td></td></tr><tr><td>Why call these <code>inactiveMachines</code>, these are live and running and therefore active.</td><td>pkg/controller/machineset.go - terminateMachines</td><td>pending</td></tr></tbody></table><h3 id=clarification>Clarification</h3><table><thead><tr><th>Statement</th><th>FilePath</th><th>Status</th></tr></thead><tbody><tr><td>Why are there 2 versions - internal and external versions?</td><td>General</td><td>pending</td></tr><tr><td>Safety controller freezes MCM controllers in the following cases:<br>* Num replicas go beyond a threshold (above the defined replicas)<br>* Target API service is not reachable<br>There seems to be an overlap between DWD and MCM Safety controller. In the meltdown scenario why is MCM being added to DWD, you could have used Safety controller for that.</td><td>General</td><td>pending</td></tr><tr><td>All machine resources are v1alpha1 - should we not promote it to beta. V1alpha1 has a different semantic and does not give any confidence to the consumers.</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>Shouldn’t controller manager use context.Context instead of creating a stop channel? - Check if signals (<code>os.Interrupt</code> and <code>SIGTERM</code> are handled properly. Do not see code where this is handled currently.)</td><td>cmd/app/controllermanager.go</td><td>pending</td></tr><tr><td>What is the rationale behind a timeout of 10s? If the API server is not up, should this not just block as it can anyways not do anything. Also, if there is an error returned then you exit the MCM which does not make much sense actually as it will be started again and you will again do the poll for the API server to come back up. Forcing an exit of MCM will not have any impact on the reachability of the API server in anyway so why exit?</td><td>cmd/app/controllermanager.go - getAvailableResources</td><td>pending</td></tr><tr><td>There is a very weird check - <code>availableResources[machineGVR] || availableResources[machineSetGVR] || availableResources[machineDeploymentGVR]</code><br>Shouldn’t this be conjunction instead of disjunction?<br>* What happens if you do not find one or all of these resources?<br>Currently an error log is printed and nothing else is done. MCM can be used outside gardener context where consumers can directly create MachineClass and Machine and not create MachineSet / Maching Deployment. There is no distinction made between context (gardener or outside-gardener).<br></td><td>cmd/app/controllermanager.go - StartControllers</td><td>pending</td></tr><tr><td>Instead of having an empty select {} to block forever, isn’t it better to wait on the stop channel?</td><td>cmd/app/controllermanager.go - StartControllers</td><td>pending</td></tr><tr><td>Do we need provider specific queues and syncs and listers</td><td>pkg/controller/controller.go</td><td>pending</td></tr><tr><td>Why are resource types prefixed with “Cluster”? - not sure , check PR</td><td>pkg/controller/controller.go</td><td>pending</td></tr><tr><td>When will forgetAfterSuccess be false and why? - as per the current code this is never the case. - Himanshu will check</td><td>cmd/app/controllermanager.go - createWorker</td><td>pending</td></tr><tr><td>What is the use of “ExpectationsInterface” and “UIDTrackingContExpectations”?<br>* All expectations related code should be in its own file “expectations.go” and not in this file.</td><td>pkg/controller/controller_util.go</td><td>pending</td></tr><tr><td>Why do we not use lister but directly use the controlMachingClient to get the deployment? Is it because you want to avoid any potential delays caused by update of the local cache held by the informer and accessed by the lister? What is the load on API server due to this?</td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>Why is this conversion needed? <a href=/docs/other-components/machine-controller-manager/todo/outline/#1.2-code2>code2</a></td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>A deep copy of <code>machineDeployment</code> is already passed and within the function another deepCopy is made. Any reason for it?</td><td>pkg/controller/deployment.go - addMachineDeploymentFinalizers</td><td>pending</td></tr><tr><td>What is an <code>Status.ObservedGeneration</code>?<br>*<em>Read more about generations and observedGeneration at:<br><a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata>https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata</a><br></em><a href=https://alenkacz.medium.com/kubernetes-operator-best-practices-implementing-observedgeneration-250728868792>https://alenkacz.medium.com/kubernetes-operator-best-practices-implementing-observedgeneration-250728868792</a><br>Ideally the update to the <code>ObservedGeneration</code> should only be made after successful reconciliation and not before. I see that this is just copied from <code>deployment_controller.go</code> as is</td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>Why and when will a <code>MachineDeployment</code> be marked as frozen and when will it be un-frozen?</td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>Shoudn&rsquo;t the validation of the machine deployment be done during the creation via a validating webhook instead of allowing it to be stored in etcd and then failing the validation during sync? I saw the checks and these can be done via validation webhook.</td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>RollbackTo has been marked as deprecated. What is the replacement? <a href=/docs/other-components/machine-controller-manager/todo/outline/#1.3-code3>code3</a></td><td>pkg/controller/deployment.go - reconcileClusterMachineDeployment</td><td>pending</td></tr><tr><td>What is the max machineSet deletions that you could process in a single run? The reason for asking this question is that for every machineSetDeletion a new goroutine spawned.<br>* Is the <code>Delete</code> call a synchrounous call? Which means it blocks till the machineset deletion is triggered which then also deletes the machines (due to cascade-delete and blockOwnerDeletion= true)?</td><td>pkg/controller/deployment.go - terminateMachineSets</td><td>pending</td></tr><tr><td>If there are validation errors or error when creating label selector then a nil is returned. In the worker reconcile loop if the return value is nil then it will remove it from the queue (forget + done). What is the way to see any errors? Typically when we describe a resource the errors are displayed. Will these be displayed when we discribe a <code>MachineDeployment</code>?</td><td>pkg/controller/deployment.go - reconcileClusterMachineSet</td><td>pending</td></tr><tr><td>If an error is returned by <code>updateMachineSetStatus</code> and it is <code>IsNotFound</code> error then returning an error will again queue the <code>MachineSet</code>. Is this desired as <code>IsNotFound</code> indicates the <code>MachineSet</code> has been deleted and is no longer there?</td><td>pkg/controller/deployment.go - reconcileClusterMachineSet</td><td>pending</td></tr><tr><td>is <code>machineControl.DeleteMachine</code> a synchronous operation which will wait till the machine has been deleted? Also where is the <code>DeletionTimestamp</code> set on the <code>Machine</code>? Will it be automatically done by the API server?</td><td>pkg/controller/deployment.go - prepareMachineForDeletion</td><td>pending</td></tr></tbody></table><h3 id=bugsenhancements>Bugs/Enhancements</h3><table><thead><tr><th>Statement + TODO</th><th>FilePath</th><th>Status</th></tr></thead><tbody><tr><td>This defines QPS and Burst for its requests to the KAPI. Check if it would make sense to explicitly define a FlowSchema and PriorityLevelConfiguration to ensure that the requests from this controller are given a well-defined preference. What is the rational behind deciding these values?</td><td>pkg/options/types.go - MachineControllerManagerConfiguration</td><td>pending</td></tr><tr><td>In function “validateMachineSpec” fldPath func parameter is never used.</td><td>pkg/apis/machine/validation/machine.go</td><td>pending</td></tr><tr><td>If there is an update failure then this method recursively calls itself without any sort of delays which could lead to a LOT of load on the API server. (opened: <a href=https://github.com/gardener/machine-controller-manager/issues/686>https://github.com/gardener/machine-controller-manager/issues/686</a>)</td><td>pkg/controller/deployment.go - updateMachineDeploymentFinalizers</td><td>pending</td></tr><tr><td>We are updating <code>filteredMachines</code> by invoking <code>syncMachinesNodeTemplates</code>, <code>syncMachinesConfig</code> and <code>syncMachinesClassKind</code> but we do not create any deepCopy here. Everywhere else the general principle is when you mutate always make a deepCopy and then mutate the copy instead of the original as a lister is used and that changes the cached copy.<br><code>Fix</code>: <code>SatisfiedExpectations</code> check has been commented and there is a TODO there to fix it. Is there a PR for this?</td><td>pkg/controller/machineset.go - reconcileClusterMachineSet</td><td>pending</td></tr></tbody></table><p>Code references</p><h1 id=11-code1>1.1 code1</h1><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>       <span style=color:#00f>if</span> machineSet.DeletionTimestamp == <span style=color:#00f>nil</span> {
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        		<span style=color:green>// manageReplicas is the core machineSet method where scale up/down occurs
</span></span></span><span style=display:flex><span><span style=color:green></span>        
</span></span><span style=display:flex><span>        		<span style=color:green>// It is not called when deletion timestamp is set
</span></span></span><span style=display:flex><span><span style=color:green></span>        
</span></span><span style=display:flex><span>        		manageReplicasErr = c.manageReplicas(ctx, filteredMachines, machineSet)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span>​</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        	} <span style=color:#00f>else</span> <span style=color:#00f>if</span> machineSet.DeletionTimestamp != <span style=color:#00f>nil</span> { 
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>            <span style=color:green>//FIX: change this to simple else without the if
</span></span></span></code></pre></div><h1 id=12-code2>1.2 code2</h1><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>    <span style=color:#00f>defer</span> dc.enqueueMachineDeploymentAfter(deployment, 10*time.Minute)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    *  <span style=color:#a31515>`Clarification`</span>:  Why  is  this  conversion  needed<span>?</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    err = v1alpha1.Convert_v1alpha1_MachineDeployment_To_machine_MachineDeployment(deployment, internalMachineDeployment, <span style=color:#00f>nil</span>)
</span></span></code></pre></div><h1 id=13-code3>1.3 code3</h1><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// rollback is not re-entrant in case the underlying machine sets are updated with a new
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span>	<span style=color:green>// revision so we should ensure that we won&#39;t proceed to update machine sets until we
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span>	<span style=color:green>// make sure that the deployment has cleaned up its rollback spec in subsequent enqueues.
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span>	<span style=color:#00f>if</span> d.Spec.RollbackTo != <span style=color:#00f>nil</span> {
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#00f>return</span> dc.rollback(ctx, d, machineSets, machineMap)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	}
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-a021045e0a5544e5e8c6993bca77c7bf>2.4 - FAQ</h1><div class=lead>Frequently Asked Questions</div><h1 id=frequently-asked-questions>Frequently Asked Questions</h1><p>The answers in this FAQ apply to the newest (HEAD) version of Machine Controller Manager. If
you&rsquo;re using an older version of MCM please refer to corresponding version of
this document. Few of the answers assume that the MCM being used is in conjuction with <a href=https://github.com/gardener/autoscaler>cluster-autoscaler</a>:</p><h1 id=table-of-contents>Table of Contents:</h1><ul><li><a href=/docs/other-components/machine-controller-manager/faq/#frequently-asked-questions>Frequently Asked Questions</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#table-of-contents>Table of Contents:</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#basics>Basics</a><ul><li><a href=/docs/other-components/machine-controller-manager/faq/#what-is-machine-controller-manager>What is Machine Controller Manager?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#why-is-my-machine-deleted>Why is my machine deleted?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#what-are-the-different-sub-controllers-in-mcm>What are the different sub-controllers in MCM?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#what-is-safety-controller-in-mcm>What is Safety Controller in MCM?</a></li></ul></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-to>How to?</a><ul><li><a href=/docs/other-components/machine-controller-manager/faq/#how-to-install-mcm-in-a-kubernetes-cluster>How to install MCM in a Kubernetes cluster?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-to-better-control-the-rollout-process-of-the-worker-nodes>How to better control the rollout process of the worker nodes?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-to-scale-down-machinedeployment-by-selective-deletion-of-machines>How to scale down MachineDeployment by selective deletion of machines?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-to-force-delete-a-machine>How to force delete a machine?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-to-pause-the-ongoing-rolling-update-of-the-machinedeployment>How to pause the ongoing rolling-update of the machinedeployment?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-to-delete-machine-object-immedietly-if-i-dont-have-access-to-it>How to delete machine object immedietly if I don&rsquo;t have access to it?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-to-avoid-garbage-collection-of-your-node>How to avoid garbage collection of your node?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-to-trigger-rolling-update-of-a-machinedeployment>How to trigger rolling update of a machinedeployment?</a></li></ul></li><li><a href=/docs/other-components/machine-controller-manager/faq/#internals>Internals</a><ul><li><a href=/docs/other-components/machine-controller-manager/faq/#what-is-the-high-level-design-of-mcm>What is the high level design of MCM?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#what-are-the-different-configuration-options-in-mcm>What are the different configuration options in MCM?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#what-are-the-different-timeoutsconfigurations-in-a-machines-lifecycle>What are the different timeouts/configurations in a machine&rsquo;s lifecycle?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-is-the-drain-of-a-machine-implemented>How is the drain of a machine implemented?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-are-the-stateful-applications-drained-during-machine-deletion>How are the stateful applications drained during machine deletion?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-does-maxevictretries-configuration-work-with-draintimeout-configuration>How does <code>maxEvictRetries</code> configuration work with <code>drainTimeout</code> configuration?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#what-are-the-different-phases-of-a-machine>What are the different phases of a machine?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#what-health-checks-are-performed-on-a-machine>What health checks are performed on a machine?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection>How does rate limiting replacement of machine work in MCM? How is it related to meltdown protection?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-mcm-responds-when-scale-outscale-in-is-done-during-rolling-update-of-a-machinedeployment>How MCM responds when scale-out/scale-in is done during rolling update of a machinedeployment?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-does-mcm-prioritize-the-machines-for-deletion-on-scale-down-of-machinedeployment>How does MCM prioritize the machines for deletion on scale-down of machinedeployment?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-some-unhealthy-machines-are-drained-quickly>How some unhealthy machines are drained quickly?</a></li></ul></li><li><a href=/docs/other-components/machine-controller-manager/faq/#troubleshooting>Troubleshooting</a><ul><li><a href=/docs/other-components/machine-controller-manager/faq/#my-machine-is-stuck-in-deletion-for-1-hr-why>My machine is stuck in deletion for 1 hr, why?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#my-machine-is-not-joining-the-cluster-why>My machine is not joining the cluster, why?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#my-rolling-update-is-stuck-why>My rolling update is stuck, why?</a></li></ul></li><li><a href=/docs/other-components/machine-controller-manager/faq/#developer>Developer</a><ul><li><a href=/docs/other-components/machine-controller-manager/faq/#how-should-i-test-my-code-before-submitting-a-pr>How should I test my code before submitting a PR?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#i-need-to-change-the-apis-what-are-the-recommended-steps>I need to change the APIs, what are the recommended steps?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-can-i-update-the-dependencies-of-mcm>How can I update the dependencies of MCM?</a></li></ul></li><li><a href=/docs/other-components/machine-controller-manager/faq/#in-the-context-of-gardener>In the context of Gardener</a><ul><li><a href=/docs/other-components/machine-controller-manager/faq/#how-can-i-configure-mcm-using-shoot-resource>How can I configure MCM using Shoot resource?</a></li><li><a href=/docs/other-components/machine-controller-manager/faq/#how-is-my-worker-pool-spread-across-zones>How is my worker-pool spread across zones?</a></li></ul></li></ul><h1 id=basics>Basics</h1><h3 id=what-is-machine-controller-manager>What is Machine Controller Manager?</h3><p>Machine Controller Manager aka MCM is a bunch of controllers used for the lifecycle management of the worker machines. It reconciles a set of CRDs such as <code>Machine</code>, <code>MachineSet</code>, <code>MachineDeployment</code> which depicts the functionality of <code>Pod</code>, <code>Replicaset</code>, <code>Deployment</code> of the core Kubernetes respectively. Read more about it at <a href=https://github.com/gardener/machine-controller-manager/tree/master/docs>README</a>.</p><ul><li>Gardener uses MCM to manage its Kubernetes nodes of the shoot cluster. However, by design, MCM can be used independent of Gardener.</li></ul><h3 id=why-is-my-machine-deleted>Why is my machine deleted?</h3><p>A machine is deleted by MCM generally for 2 reasons-</p><ul><li>Machine is unhealthy for at least <code>MachineHealthTimeout</code> period. The default <code>MachineHealthTimeout</code> is 10 minutes.<ul><li>By default, a machine is considered unhealthy if any of the following node conditions - <code>DiskPressure</code>, <code>KernelDeadlock</code>, <code>FileSystem</code>, <code>Readonly</code> is set to <code>true</code>, or <code>KubeletReady</code> is set to <code>false</code>. However, this is something that is configurable using the following <a href=https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/deployment/out-of-tree/deployment.yaml#L30>flag</a>.</li></ul></li><li>Machine is scaled down by the <code>MachineDeployment</code> resource.<ul><li>This is very usual when an external controller cluster-autoscaler (aka CA) is used with MCM. CA deletes the under-utilized machines by scaling down the <code>MachineDeployment</code>. Read more about cluster-autoscaler&rsquo;s scale down behavior <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#how-does-scale-down-work>here</a>.</li></ul></li></ul><h3 id=what-are-the-different-sub-controllers-in-mcm>What are the different sub-controllers in MCM?</h3><p>MCM mainly contains the following sub-controllers:</p><ul><li><code>MachineDeployment Controller</code>: Responsible for reconciling the <code>MachineDeployment</code> objects. It manages the lifecycle of the <code>MachineSet</code> objects.</li><li><code>MachineSet Controller</code>: Responsible for reconciling the <code>MachineSet</code> objects. It manages the lifecycle of the <code>Machine</code> objects.</li><li><code>Machine Controller</code>: responsible for reconciling the <code>Machine</code> objects. It manages the lifecycle of the actual VMs/machines created in cloud/on-prem. This controller has been moved out of tree. Please refer an AWS machine controller for more info - <a href=https://github.com/gardener/machine-controller-manager-provider-gcp>link</a>.</li><li>Safety-controller: Responsible for handling the unidentified/unknown behaviors from the cloud providers. Please read more about its functionality <a href=/docs/other-components/machine-controller-manager/faq/#what-is-safety-controller-in-mcm>below</a>.</li></ul><h3 id=what-is-safety-controller-in-mcm>What is Safety Controller in MCM?</h3><p><code>Safety Controller</code> contains following functions:</p><ul><li>Orphan VM handler:<ul><li>It lists all the VMs in the cloud matching the <code>tag</code> of given cluster name and maps the VMs with the <code>machine</code> objects using the <code>ProviderID</code> field. VMs without any backing <code>machine</code> objects are logged and deleted after confirmation.</li><li>This handler runs every 30 minutes and is configurable via <a href=https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L112>machine-safety-orphan-vms-period</a> flag.</li></ul></li><li>Freeze mechanism:<ul><li><code>Safety Controller</code> freezes the <code>MachineDeployment</code> and <code>MachineSet</code> controller if the number of <code>machine</code> objects goes beyond a certain threshold on top of <code>Spec.Replicas</code>. It can be configured by the flag <a href=https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L102-L103>&ndash;safety-up or &ndash;safety-down</a> and also <a href=https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L113>machine-safety-overshooting-period</a>.</li><li><code>Safety Controller</code> freezes the functionality of the MCM if either of the <code>target-apiserver</code> or the <code>control-apiserver</code> is not reachable.</li><li><code>Safety Controller</code> unfreezes the MCM automatically once situation is resolved to normal. A <code>freeze</code> label is applied on <code>MachineDeployment</code>/<code>MachineSet</code> to enforce the freeze condition.</li></ul></li></ul><h1 id=how-to>How to?</h1><h3 id=how-to-install-mcm-in-a-kubernetes-cluster>How to install MCM in a Kubernetes cluster?</h3><p>MCM can be installed in a cluster with following steps:</p><ul><li>Apply all the CRDs from <a href=https://github.com/gardener/machine-controller-manager/tree/master/kubernetes/crds>here</a></li><li>Apply all the deployment, role-related objects from <a href=https://github.com/gardener/machine-controller-manager/tree/master/kubernetes/deployment/out-of-tree>here</a>.<ul><li>Control cluster is the one where the <code>machine-*</code> objects are stored. Target cluster is where all the node objects are registered.</li></ul></li></ul><h3 id=how-to-better-control-the-rollout-process-of-the-worker-nodes>How to better control the rollout process of the worker nodes?</h3><p>MCM allows configuring the rollout of the worker machines using <code>maxSurge</code> and <code>maxUnavailable</code> fields. These fields are applicable only during the rollout process and means nothing in general scale up/down scenarios.
The overall process is very similar to how the <code>Deployment Controller</code> manages pods during <code>RollingUpdate</code>.</p><ul><li><code>maxSurge</code> refers to the number of additional machines that can be added on top of the <code>Spec.Replicas</code> of MachineDeployment <em>during rollout process</em>.</li><li><code>maxUnavailable</code> refers to the number of machines that can be deleted from <code>Spec.Replicas</code> field of the MachineDeployment <em>during rollout process</em>.</li></ul><h3 id=how-to-scale-down-machinedeployment-by-selective-deletion-of-machines>How to scale down MachineDeployment by selective deletion of machines?</h3><p>During scale down, triggered via <code>MachineDeployment</code>/<code>MachineSet</code>, MCM prefers to delete the <code>machine/s</code> which have the least priority set.
Each <code>machine</code> object has an annotation <code>machinepriority.machine.sapcloud.io</code> set to <code>3</code> by default. Admin can reduce the priority of the given machines by changing the annotation value to <code>1</code>. The next scale down by <code>MachineDeployment</code> shall delete the machines with the least priority first.</p><h3 id=how-to-force-delete-a-machine>How to force delete a machine?</h3><p>A machine can be force deleted by adding the label <code>force-deletion: "True"</code> on the <code>machine</code> object before executing the actual delete command. During force deletion, MCM skips the drain function and simply triggers the deletion of the machine. This label should be used with caution as it can violate the PDBs for pods running on the machine.</p><h3 id=how-to-pause-the-ongoing-rolling-update-of-the-machinedeployment>How to pause the ongoing rolling-update of the machinedeployment?</h3><p>An ongoing rolling-update of the machine-deployment can be paused by using <code>spec.paused</code> field. See the example below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: machine.sapcloud.io/v1alpha1
</span></span><span style=display:flex><span>kind: MachineDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: test-machine-deployment
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  paused: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>It can be unpaused again by removing the <code>Paused</code> field from the machine-deployment.</p><h3 id=how-to-delete-machine-object-immedietly-if-i-dont-have-access-to-it>How to delete machine object immedietly if I don&rsquo;t have access to it?</h3><p>If the user doesn&rsquo;t have access to the machine objects (like in case of Gardener clusters) and they would like to replace a node immedietly then they can place the annotation <code>node.machine.sapcloud.io/trigger-deletion-by-mcm: "true"</code> on their node. This will start the replacement of the machine with a new node.</p><p>On the other hand if the user deletes the node object immedietly then replacement will start only after <code>MachineHealthTimeout</code>.</p><p>This annotation can also be used if the user wants to expedite the <a href=/docs/other-components/machine-controller-manager/faq/#how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection>replacement of unhealthy nodes</a></p><p><code>NOTE</code>:</p><ul><li><code>node.machine.sapcloud.io/trigger-deletion-by-mcm: "false"</code> annotation is NOT acted upon by MCM , neither does it mean that MCM will not replace this machine.</li><li>this annotation would delete the desired machine but another machine would be created to maintain <code>desired replicas</code> specified for the machineDeployment/machineSet. Currently if the user doesn&rsquo;t have access to machineDeployment/machineSet then they cannot remove a machine without replacement.</li></ul><h3 id=how-to-avoid-garbage-collection-of-your-node>How to avoid garbage collection of your node?</h3><p>MCM provides an in-built safety mechanism to garbage collect VMs which have no corresponding machine object. This is done to save costs and is one of the key features of MCM.
However, sometimes users might like to add nodes directly to the cluster without the help of MCM and would prefer MCM to not garbage collect such VMs.
To do so they should remove/not-use tags on their VMs containing the following strings:</p><ol><li><code>kubernetes.io/cluster/</code></li><li><code>kubernetes.io/role/</code></li><li><code>kubernetes-io-cluster-</code></li><li><code>kubernetes-io-role-</code></li></ol><h3 id=how-to-trigger-rolling-update-of-a-machinedeployment>How to trigger rolling update of a machinedeployment?</h3><p>Rolling update can be triggered for a machineDeployment by updating one of the following:</p><ul><li><code>.spec.template.annotations</code></li><li><code>.spec.template.spec.class.name</code></li></ul><h1 id=internals>Internals</h1><h3 id=what-is-the-high-level-design-of-mcm>What is the high level design of MCM?</h3><p>Please refer the following <a href=/docs/other-components/machine-controller-manager/#design-of-machine-controller-manager>document</a>.</p><h3 id=what-are-the-different-configuration-options-in-mcm>What are the different configuration options in MCM?</h3><p>MCM allows configuring many knobs to fine-tune its behavior according to the user&rsquo;s need.
Please refer to the <a href=https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go>link</a> to check the exact configuration options.</p><h3 id=what-are-the-different-timeoutsconfigurations-in-a-machines-lifecycle>What are the different timeouts/configurations in a machine&rsquo;s lifecycle?</h3><p>A machine&rsquo;s lifecycle is governed by mainly following timeouts, which can be configured <a href=https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/machine_objects/machine-deployment.yaml#L30-L34>here</a>.</p><ul><li><code>MachineDrainTimeout</code>: Amount of time after which drain times out and the machine is force deleted. Default ~2 hours.</li><li><code>MachineHealthTimeout</code>: Amount of time after which an unhealthy machine is declared <code>Failed</code> and the machine is replaced by <code>MachineSet</code> controller.</li><li><code>MachineCreationTimeout</code>: Amount of time after which a machine creation is declared <code>Failed</code> and the machine is replaced by the <code>MachineSet</code> controller.</li><li><code>NodeConditions</code>: List of node conditions which if set to true for <code>MachineHealthTimeout</code> period, the machine is declared <code>Failed</code> and replaced by <code>MachineSet</code> controller.</li><li><code>MaxEvictRetries</code>: An integer number depicting the number of times a failed <em>eviction</em> should be retried on a pod during drain process. A pod is <em>deleted</em> after <code>max-retries</code>.</li></ul><h3 id=how-is-the-drain-of-a-machine-implemented>How is the drain of a machine implemented?</h3><p>MCM imports the functionality from the upstream Kubernetes-drain library. Although, few parts have been modified to make it work best in the context of MCM. Drain is executed before machine deletion for graceful migration of the applications.
Drain internally uses the <code>EvictionAPI</code> to evict the pods and triggers the <code>Deletion</code> of pods after <code>MachineDrainTimeout</code>. Please note:</p><ul><li>Stateless pods are evicted in parallel.</li><li>Stateful applications (with PVCs) are serially evicted. Please find more info in this <a href=/docs/other-components/machine-controller-manager/faq/#how-are-the-stateful-applications-drained-during-machine-deletion>answer below</a>.</li></ul><h3 id=how-are-the-stateful-applications-drained-during-machine-deletion>How are the stateful applications drained during machine deletion?</h3><p>Drain function serially evicts the stateful-pods. It is observed that serial eviction of stateful pods yields better overall availability of pods as the underlying cloud in most cases detaches and reattaches disks serially anyways.
It is implemented in the following manner:</p><ul><li>Drain lists all the pods with attached volumes. It evicts very first stateful-pod and waits for its related entry in Node object&rsquo;s <code>.status.volumesAttached</code> to be removed by KCM. It does the same for all the stateful-pods.</li><li>It waits for <code>PvDetachTimeout</code> (default 2 minutes) for a given pod&rsquo;s PVC to be removed, else moves forward.</li></ul><h3 id=how-does-maxevictretries-configuration-work-with-draintimeout-configuration>How does <code>maxEvictRetries</code> configuration work with <code>drainTimeout</code> configuration?</h3><p>It is recommended to only set <code>MachineDrainTimeout</code>. It satisfies the related requirements. <code>MaxEvictRetries</code> is auto-calculated based on <code>MachineDrainTimeout</code>, if <code>maxEvictRetries</code> is not provided. Following will be the overall behavior of both configurations together:</p><ul><li>If <code>maxEvictRetries</code> isn&rsquo;t set and only <code>maxDrainTimeout</code> is set:<ul><li>MCM auto calculates the <code>maxEvictRetries</code> based on the <code>drainTimeout</code>.</li></ul></li><li>If <code>drainTimeout</code> isn&rsquo;t set and only <code>maxEvictRetries</code> is set:<ul><li>Default <code>drainTimeout</code> and user provided <code>maxEvictRetries</code> for each pod is considered.</li></ul></li><li>If both <code>maxEvictRetries</code> and <code>drainTimoeut</code> are set:<ul><li>Then both will be respected.</li></ul></li><li>If none are set:<ul><li>Defaults are respected.</li></ul></li></ul><h3 id=what-are-the-different-phases-of-a-machine>What are the different phases of a machine?</h3><p>A phase of a <code>machine</code> can be identified with <code>Machine.Status.CurrentStatus.Phase</code>. Following are the possible phases of a <code>machine</code> object:</p><ul><li><p><code>Pending</code>: Machine creation call has succeeded. MCM is waiting for machine to join the cluster.</p></li><li><p><code>CrashLoopBackOff</code>: Machine creation call has failed. MCM will retry the operation after a minor delay.</p></li><li><p><code>Running</code>: Machine creation call has succeeded. Machine has joined the cluster successfully and corresponding node doesn&rsquo;t have <code>node.gardener.cloud/critical-components-not-ready</code> taint.</p></li><li><p><code>Unknown</code>: Machine <a href=/docs/other-components/machine-controller-manager/faq/#what-health-checks-are-performed-on-a-machine>health checks</a> are failing, e.g., <code>kubelet</code> has stopped posting the status.</p></li><li><p><code>Failed</code>: Machine health checks have failed for a prolonged time. Hence it is declared failed by <code>Machine</code> controller in a <a href=/docs/other-components/machine-controller-manager/faq/#how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection>rate limited fashion</a>. <code>Failed</code> machines get replaced immediately.</p></li><li><p><code>Terminating</code>: Machine is being terminated. Terminating state is set immediately when the deletion is triggered for the <code>machine</code> object. It also includes time when it&rsquo;s being drained.</p></li></ul><p><code>NOTE</code>: No phase means the machine is being created on the cloud-provider.</p><p>Below is a simple phase transition diagram:
<img src=/__resources/machine_phase_transition_1435f5.png alt=image></p><h3 id=what-health-checks-are-performed-on-a-machine>What health checks are performed on a machine?</h3><p>Health check performed on a machine are:</p><ul><li>Existense of corresponding node obj</li><li>Status of certain user-configurable node conditions.<ul><li>These conditions can be specified using the flag <code>--node-conditions</code> for OOT MCM provider or can be specified per machine object.</li><li>The default user configurable node conditions can be found <a href=https://github.com/gardener/machine-controller-manager/blob/91eec24516b8339767db5a40e82698f9fe0daacd/pkg/util/provider/app/options/options.go#L60>here</a></li></ul></li><li><code>True</code> status of <code>NodeReady</code> condition . This condition shows kubelet&rsquo;s status</li></ul><p>If any of the above checks fails , the machine turns to <code>Unknown</code> phase.</p><h3 id=how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection>How does rate limiting replacement of machine work in MCM? How is it related to meltdown protection?</h3><p>Currently MCM replaces only <code>1</code> <code>Unknown</code> machine at a time per machinedeployment. This means until the particular <code>Unknown</code> machine get terminated and its replacement joins, no other <code>Unknown</code> machine would be removed.</p><p>The above is achieved by enabling <code>Machine</code> controller to turn machine from <code>Unknown</code> -> <code>Failed</code> only if the above condition is met. <code>MachineSet</code> controller on the other hand marks <code>Failed</code> machine as <code>Terminating</code> immediately.</p><p>One reason for this rate limited replacement was to ensure that in case of network failures , where node&rsquo;s kubelet can&rsquo;t reach out to kube-apiserver , all nodes are not removed together i.e. <code>meltdown protection</code>.
In gardener context however, <a href=/docs/other-components/dependency-watchdog/concepts/prober/#origin>DWD</a> is deployed to deal with this scenario, but to stay protected from corner cases, this mechanism has been introduced in MCM.</p><p><code>NOTE</code>: Rate limiting replacement is not yet configurable</p><h3 id=how-mcm-responds-when-scale-outscale-in-is-done-during-rolling-update-of-a-machinedeployment>How MCM responds when scale-out/scale-in is done during rolling update of a machinedeployment?</h3><p><code>Machinedeployment</code> controller executes the logic of <code>scaling</code> BEFORE logic of <code>rollout</code>. It identifies <code>scaling</code> by comparing the <code>deployment.kubernetes.io/desired-replicas</code> of each machineset under the machinedeployment with machinedeployment&rsquo;s <code>.spec.replicas</code>. If the difference is found for any machineSet, a scaling event is detected.</p><ul><li>Case <code>scale-out</code> -> ONLY New machineSet is scaled out</li><li>Case <code>scale-in</code> -> ALL machineSets(new or old) are scaled in , in proportion to their replica count , any leftover is adjusted in the largest machineSet.</li></ul><p>During update for scaling event, a machineSet is updated if any of the below is true for it:</p><ul><li><code>.spec.Replicas</code> needs update</li><li><code>deployment.kubernetes.io/desired-replicas</code> needs update</li></ul><p>Once scaling is achieved, rollout continues.</p><h3 id=how-does-mcm-prioritize-the-machines-for-deletion-on-scale-down-of-machinedeployment>How does MCM prioritize the machines for deletion on scale-down of machinedeployment?</h3><p>There could be many machines under a machinedeployment with different phases, creationTimestamp. When a scale down is triggered, MCM decides to remove the machine using the following logic:</p><ul><li>Machine with least value of <code>machinepriority.machine.sapcloud.io</code> annotation is picked up.</li><li>If all machines have equal priorities, then following precedence is followed:<ul><li>Terminating > Failed > CrashloopBackoff > Unknown > Pending > Available > Running</li></ul></li><li>If still there is no match, the machine with oldest creation time (.i.e. creationTimestamp) is picked up.</li></ul><h2 id=how-some-unhealthy-machines-are-drained-quickly>How some unhealthy machines are drained quickly?</h2><p>If a node is unhealthy for more than the <code>machine-health-timeout</code> specified for the <code>machine-controller</code>, the controller
health-check moves the machine phase to <code>Failed</code>. By default, the <code>machine-health-timeout</code> is 10` minutes.</p><p><code>Failed</code> machines have their deletion timestamp set and the machine then moves to the <code>Terminating</code> phase. The node
drain process is initiated. The drain process is invoked either <em>gracefully</em> or <em>forcefully</em>.</p><p>The usual drain process is graceful. Pods are evicted from the node and the drain process waits until any existing
attached volumes are mounted on new node. However, if the node <code>Ready</code> is <code>False</code> or the <code>ReadonlyFilesystem</code> is <code>True</code>
for greater than <code>5</code> minutes (non-configurable), then a forceful drain is initiated. In a forceful drain, pods are deleted
and <code>VolumeAttachment</code> objects associated with the old node are also marked for deletion. This is followed by the deletion of the
cloud provider VM associated with the <code>Machine</code> and then finally ending with the <code>Node</code> object deletion.</p><p>During the deletion of the VM we only delete the local data disks and boot disks associated with the VM. The disks associated
with persistent volumes are left un-touched as their attach/de-detach, mount/unmount processes are handled by k8s
attach-detach controller in conjunction with the CSI driver.</p><h1 id=troubleshooting>Troubleshooting</h1><h3 id=my-machine-is-stuck-in-deletion-for-1-hr-why>My machine is stuck in deletion for 1 hr, why?</h3><p>In most cases, the <code>Machine.Status.LastOperation</code> provides information around why a machine can&rsquo;t be deleted.
Though following could be the reasons but not limited to:</p><ul><li>Pod/s with misconfigured PDBs block the drain operation. PDBs with <code>maxUnavailable</code> set to 0, doesn&rsquo;t allow the eviction of the pods. Hence, drain/eviction is retried till <code>MachineDrainTimeout</code>. Default <code>MachineDrainTimeout</code> could be as large as ~2hours. Hence, blocking the machine deletion.<ul><li>Short term: User can manually delete the pod in the question, <em>with caution</em>.</li><li>Long term: Please set more appropriate PDBs which allow disruption of at least one pod.</li></ul></li><li>Expired cloud credentials can block the deletion of the machine from infrastructure.</li><li>Cloud provider can&rsquo;t delete the machine due to internal errors. Such situations are best debugged by using cloud provider specific CLI or cloud console.</li></ul><h3 id=my-machine-is-not-joining-the-cluster-why>My machine is not joining the cluster, why?</h3><p>In most cases, the <code>Machine.Status.LastOperation</code> provides information around why a machine can&rsquo;t be created.
It could possibly be debugged with following steps:</p><ul><li>Firstly make sure all the relevant controllers like <code>kube-controller-manager</code> , <code>cloud-controller-manager</code> are running.</li><li>Verify if the machine is actually created in the cloud. User can use the <code>Machine.Spec.ProviderId</code> to query the machine in cloud.</li><li>A Kubernetes node is generally bootstrapped with the cloud-config. Please verify, if <code>MachineDeployment</code> is pointing the correct <code>MachineClass</code>, and <code>MachineClass</code> is pointing to the correct <code>Secret</code>. The secret object contains the actual cloud-config in <code>base64</code> format which will be used to boot the machine.</li><li>User must also check the logs of the MCM pod to understand any broken logical flow of reconciliation.</li></ul><h3 id=my-rolling-update-is-stuck-why>My rolling update is stuck, why?</h3><p>The following can be the reason:</p><ul><li>Insufficient capacity for the new instance type the machineClass mentions.</li><li><a href=/docs/other-components/machine-controller-manager/faq/#my-machine-is-stuck-in-deletion-for-1-hr-why>Old machines are stuck in deletion</a></li><li>If you are using Gardener for setting up kubernetes cluster, then machine object won&rsquo;t turn to <code>Running</code> state until <code>node-critical-components</code> are ready. Refer <a href=/docs/gardener/advanced/node-readiness/>this</a> for more details.</li></ul><h1 id=developer>Developer</h1><h3 id=how-should-i-test-my-code-before-submitting-a-pr>How should I test my code before submitting a PR?</h3><ul><li><p>Developer can locally setup the MCM using following <a href=/docs/other-components/machine-controller-manager/local_setup/>guide</a></p></li><li><p>Developer must also enhance the unit tests related to the incoming changes.</p></li><li><p>Developer can run the unit test locally by executing:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>make test-unit
</span></span></code></pre></div></li><li><p>Developer can locally run <a href=/docs/other-components/machine-controller-manager/integration_tests/>integration tests</a> to ensure basic functionality of MCM is not altered.</p></li></ul><h3 id=i-need-to-change-the-apis-what-are-the-recommended-steps>I need to change the APIs, what are the recommended steps?</h3><p>Developer should add/update the API fields at both of the following places:</p><ul><li><a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/types.go>types.go</a></li><li><a href=https://github.com/gardener/machine-controller-manager/tree/master/pkg/apis/machine/v1alpha1>v1alpha1</a></li></ul><p>Once API changes are done, auto-generate the code using following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>make generate
</span></span></code></pre></div><p>Please ignore the API-violation errors for now.</p><h3 id=how-can-i-update-the-dependencies-of-mcm>How can I update the dependencies of MCM?</h3><p>MCM uses <code>gomod</code> for depedency management.
Developer should add/udpate depedency in the go.mod file. Please run following command to automatically tidy the dependencies.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>make tidy
</span></span></code></pre></div><h1 id=in-the-context-of-gardener>In the context of Gardener</h1><h3 id=how-can-i-configure-mcm-using-shoot-resource>How can I configure MCM using Shoot resource?</h3><p>All of the knobs of MCM can be configured by the <code>workers</code> <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L29-L126>section</a> of the shoot resource.</p><ul><li>Gardener creates a <code>MachineDeployment</code> per zone for each worker-pool under <code>workers</code> section.</li><li><code>workers.dataVolumes</code> allows to attach multiple disks to a machine during creation. Refer the <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L29-L126>link</a>.</li><li><code>workers.machineControllerManager</code> allows configuration of multiple knobs of the <code>MachineDeployment</code> from the shoot resource.</li></ul><h3 id=how-is-my-worker-pool-spread-across-zones>How is my worker-pool spread across zones?</h3><p>Shoot resource allows the worker-pool to spread across multiple zones using the field <code>workers.zones</code>. Refer <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L115>link</a>.</p><ul><li><p>Gardener creates one <code>MachineDeployment</code> per zone. Each <code>MachineDeployment</code> is initiated with the following replica:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>MachineDeployment.Spec.Replicas = (Workers.Minimum)/(Number of availability zones)
</span></span></code></pre></div></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-577b0c8dc631907de420f1bca5b1b5ee>2.5 - Adding Support for a Cloud Provider</h1><h1 id=adding-support-for-a-new-provider>Adding support for a new provider</h1><p>Steps to be followed while implementing a new (hyperscale) provider are mentioned below. This is the easiest way to add new provider support using a blueprint code.</p><p>However, you may also develop your machine controller from scratch, which would provide you with more flexibility. First, however, make sure that your custom machine controller adheres to the <code>Machine.Status</code> struct defined in the <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/types.go>MachineAPIs</a>. This will make sure the MCM can act with higher-level controllers like MachineSet and MachineDeployment controller. The key is the <code>Machine.Status.CurrentStatus.Phase</code> key that indicates the status of the machine object.</p><p>Our strong recommendation would be to follow the steps below. This provides the most flexibility required to support machine management for adding new providers. And if you feel to extend the functionality, feel free to update our <a href=https://github.com/gardener/machine-controller-manager/tree/master/pkg/util/provider>machine controller libraries</a>.</p><h2 id=setting-up-your-repository>Setting up your repository</h2><ol><li>Create a new empty repository named <code>machine-controller-manager-provider-{provider-name}</code> on GitHub username/project. Do not initialize this repository with a README.</li><li>Copy the remote repository <code>URL</code> (HTTPS/SSH) to this repository displayed once you create this repository.</li><li>Now, on your local system, create directories as required. {your-github-username} given below could also be {github-project} depending on where you have created the new repository.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir -p $GOPATH/src/github.com/{your-github-username}
</span></span></code></pre></div></li><li>Navigate to this created directory.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd $GOPATH/src/github.com/{your-github-username}
</span></span></code></pre></div></li><li>Clone <a href=https://github.com/gardener/machine-controller-manager-provider-sampleprovider>this repository</a> on your local machine.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone git@github.com:gardener/machine-controller-manager-provider-sampleprovider.git
</span></span></code></pre></div></li><li>Rename the directory from <code>machine-controller-manager-provider-sampleprovider</code> to <code>machine-controller-manager-provider-{provider-name}</code>.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mv machine-controller-manager-provider-sampleprovider machine-controller-manager-provider-{provider-name}
</span></span></code></pre></div></li><li>Navigate into the newly-created directory.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd machine-controller-manager-provider-{provider-name}
</span></span></code></pre></div></li><li>Update the remote <code>origin</code> URL to the newly created repository&rsquo;s URL you had copied above.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git remote set-url origin git@github.com:{your-github-username}/machine-controller-manager-provider-{provider-name}.git
</span></span></code></pre></div></li><li>Rename GitHub project from <code>gardener</code> to <code>{github-org/your-github-username}</code> wherever you have cloned the repository above. Also, edit all occurrences of the word <code>sampleprovider</code> to <code>{provider-name}</code> in the code. Then, use the hack script given below to do the same.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make rename-project PROJECT_NAME={github-org/your-github-username} PROVIDER_NAME={provider-name}
</span></span><span style=display:flex><span>eg:
</span></span><span style=display:flex><span>    make rename-project PROJECT_NAME=gardener PROVIDER_NAME=AmazonWebServices (or)
</span></span><span style=display:flex><span>    make rename-project PROJECT_NAME=githubusername PROVIDER_NAME=AWS
</span></span></code></pre></div></li><li>Now, commit your changes and push them upstream.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git add -A
</span></span><span style=display:flex><span>git commit -m <span style=color:#a31515>&#34;Renamed SampleProvide to {provider-name}&#34;</span>
</span></span><span style=display:flex><span>git push origin master
</span></span></code></pre></div></li></ol><h2 id=code-changes-required>Code changes required</h2><p>The contract between the Machine Controller Manager (MCM) and the Machine Controller (MC) AKA driver has been <a href=/docs/other-components/machine-controller-manager/machine_error_codes/>documented here</a> and the <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/machinecodes/codes/codes.go>machine error codes can be found here</a>. You may refer to them for any queries.</p><p>&#9888;&#xfe0f;</p><ul><li>Keep in mind that <strong>there should be a unique way to map between machine objects and VMs</strong>. This can be done by mapping machine object names with VM-Name/ tags/ other metadata.</li><li>Optionally, there should also be a unique way to map a VM to its machine class object. This can be done by tagging VM objects with tags/resource groups associated with the machine class.</li></ul><h4 id=steps-to-integrate>Steps to integrate</h4><ol><li>Update the <code>pkg/provider/apis/provider_spec.go</code> specification file to reflect the structure of the <code>ProviderSpec</code> blob. It typically contains the machine template details in the <code>MachineClass</code> object. Follow the sample spec provided already in the file. A sample provider specification can be found <a href=https://github.com/gardener/machine-controller-manager-provider-aws/blob/master/pkg/aws/apis/aws_provider_spec.go>here</a>.</li><li>Fill in the methods described at <code>pkg/provider/core.go</code> to manage VMs on your cloud provider. Comments are provided above each method to help you fill them up with desired <code>REQUEST</code> and <code>RESPONSE</code> parameters.<ul><li>A sample provider implementation for these methods can be found <a href=https://github.com/gardener/machine-controller-manager-provider-aws/blob/master/pkg/aws/core.go>here</a>.</li><li>Fill in the required methods <code>CreateMachine()</code>, and <code>DeleteMachine()</code> methods.</li><li>Optionally fill in methods like <code>GetMachineStatus()</code>, <code>InitializeMachine</code>, <code>ListMachines()</code>, and <code>GetVolumeIDs()</code>. You may choose to fill these once the working of the required methods seems to be working.</li><li><code>GetVolumeIDs()</code> expects VolumeIDs to be decoded from the volumeSpec based on the cloud provider.</li><li>There is also an OPTIONAL method <code>GenerateMachineClassForMigration()</code> that helps in migration of <code>{ProviderSpecific}MachineClass</code> to <code>MachineClass</code> CR (custom resource). This only makes sense if you have an existing implementation (in-tree) acting on different CRD types. You would like to migrate this. If not, you MUST return an error (machine error UNIMPLEMENTED) to avoid processing this step.</li></ul></li><li>Perform validation of APIs that you have described and make it a part of your methods as required at each request.</li><li>Write unit tests to make it work with your implementation by running <code>make test</code>.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make test
</span></span></code></pre></div></li><li>Tidy the go dependencies.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make tidy
</span></span></code></pre></div></li><li>Update the sample YAML files on the <code>kubernetes/</code> directory to provide sample files through which the working of the machine controller can be tested.</li><li>Update <code>README.md</code> to reflect any additional changes</li></ol><h2 id=testing-your-code-changes>Testing your code changes</h2><p>Make sure <code>$TARGET_KUBECONFIG</code> points to the cluster where you wish to manage machines. Likewise, <code>$CONTROL_NAMESPACE</code> represents the namespaces where MCM is looking for machine CR objects, and <code>$CONTROL_KUBECONFIG</code> points to the cluster that holds these machine CRs.</p><ol><li>On the first terminal running at <code>$GOPATH/src/github.com/{github-org/your-github-username}/machine-controller-manager-provider-{provider-name}</code>,<ul><li>Run the machine controller (driver) using the command below.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start
</span></span></code></pre></div></li></ul></li><li>On the second terminal pointing to <code>$GOPATH/src/github.com/gardener</code>,<ul><li>Clone the <a href=https://github.com/gardener/machine-controller-manager>latest MCM code</a><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone git@github.com:gardener/machine-controller-manager.git
</span></span></code></pre></div></li><li>Navigate to the newly-created directory.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd machine-controller-manager
</span></span></code></pre></div></li><li>Deploy the required CRDs from the machine-controller-manager repo,<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/crds
</span></span></code></pre></div></li><li>Run the machine-controller-manager in the <code>master</code> branch<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start
</span></span></code></pre></div></li></ul></li><li>On the third terminal pointing to <code>$GOPATH/src/github.com/{github-org/your-github-username}/machine-controller-manager-provider-{provider-name}</code><ul><li>Fill in the object files given below and deploy them as described below.</li><li>Deploy the <code>machine-class</code><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/machine-class.yaml
</span></span></code></pre></div></li><li>Deploy the <code>kubernetes secret</code> if required.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/secret.yaml
</span></span></code></pre></div></li><li>Deploy the <code>machine</code> object and make sure it joins the cluster successfully.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/machine.yaml
</span></span></code></pre></div></li><li>Once the machine joins, you can test by deploying a machine-deployment.</li><li>Deploy the <code>machine-deployment</code> object and make sure it joins the cluster successfully.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/machine-deployment.yaml
</span></span></code></pre></div></li><li>Make sure to delete both the <code>machine</code> and <code>machine-deployment</code> objects after use.<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete -f kubernetes/machine.yaml
</span></span><span style=display:flex><span>kubectl delete -f kubernetes/machine-deployment.yaml
</span></span></code></pre></div></li></ul></li></ol><h2 id=releasing-your-docker-image>Releasing your docker image</h2><ol><li>Make sure you have logged into gcloud/docker using the CLI.</li><li>To release your docker image, run the following.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>    make release IMAGE_REPOSITORY=&lt;link-to-image-repo&gt;
</span></span></code></pre></div><ol start=3><li>A sample kubernetes deploy file can be found at <code>kubernetes/deployment.yaml</code>. Update the same (with your desired MCM and MC images) to deploy your MCM pod.</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-8bb33f6412faf0154bfd64577052d094>2.6 - Deployment</h1><h1 id=deploying-the-machine-controller-manager-into-a-kubernetes-cluster>Deploying the Machine Controller Manager into a Kubernetes cluster</h1><ul><li><a href=/docs/other-components/machine-controller-manager/deployment/#deploying-the-machine-controller-manager-into-a-kubernetes-cluster>Deploying the Machine Controller Manager into a Kubernetes cluster</a><ul><li><a href=/docs/other-components/machine-controller-manager/deployment/#prepare-the-cluster>Prepare the cluster</a></li><li><a href=/docs/other-components/machine-controller-manager/deployment/#build-the-docker-image>Build the Docker image</a></li><li><a href=/docs/other-components/machine-controller-manager/deployment/#configuring-optional-parameters-while-deploying>Configuring optional parameters while deploying</a></li><li><a href=/docs/other-components/machine-controller-manager/deployment/#usage>Usage</a></li></ul></li></ul><p>As already mentioned, the Machine Controller Manager is designed to run as controller in a Kubernetes cluster. The existing source code can be compiled and tested on a local machine as described in <a href=/docs/other-components/machine-controller-manager/local_setup/>Setting up a local development environment</a>. You can deploy the Machine Controller Manager using the steps described below.</p><h2 id=prepare-the-cluster>Prepare the cluster</h2><ul><li>Connect to the remote kubernetes cluster where you plan to deploy the Machine Controller Manager using the kubectl. Set the environment variable KUBECONFIG to the path of the yaml file containing the cluster info.</li><li>Now, create the required CRDs on the remote cluster using the following command,</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/crds
</span></span></code></pre></div><h2 id=build-the-docker-image>Build the Docker image</h2><blockquote><p>&#9888;&#xfe0f; Modify the <code>Makefile</code> to refer to your own registry.</p></blockquote><ul><li>Run the build which generates the binary to <code>bin/machine-controller-manager</code></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make build
</span></span></code></pre></div><ul><li>Build docker image from latest compiled binary</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make docker-image
</span></span></code></pre></div><ul><li>Push the last created docker image onto the online docker registry.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make push
</span></span></code></pre></div><ul><li>Now you can deploy this docker image to your cluster. A <a href=https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/deployment/out-of-tree/deployment.yaml>sample development file</a> is provided. By default, the deployment manages the cluster it is running in. Optionally, the kubeconfig could also be passed as a flag as described in <code>/kubernetes/deployment/out-of-tree/deployment.yaml</code>. This is done when you want your controller running outside the cluster to be managed from.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/deployment/out-of-tree/deployment.yaml
</span></span></code></pre></div><ul><li>Also deploy the required clusterRole and clusterRoleBindings</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/deployment/out-of-tree/clusterrole.yaml
</span></span><span style=display:flex><span>$ kubectl apply -f kubernetes/deployment/out-of-tree/clusterrolebinding.yaml
</span></span></code></pre></div><h2 id=configuring-optional-parameters-while-deploying>Configuring optional parameters while deploying</h2><p>Machine-controller-manager supports several configurable parameters while deploying. Refer to <a href=https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/deployment/out-of-tree/deployment.yaml#L21-L30>the following lines</a>, to know how each parameter can be configured, and what it&rsquo;s purpose is for.</p><h2 id=usage>Usage</h2><p>To start using Machine Controller Manager, follow the links given at <a href=/docs/other-components/machine-controller-manager/>usage here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-928a64df86ba3dcfa43a809e2bbbcd94>2.7 - Integration Tests</h1><h1 id=integration-tests>Integration tests</h1><h2 id=usage>Usage</h2><h2 id=general-setup--configurations>General setup & configurations</h2><p>Integration tests for <code>machine-controller-manager-provider-{provider-name}</code> can be executed manually by following below steps.</p><ol><li>Clone the repository <code>machine-controller-manager-provider-{provider-name}</code> on the local system.</li><li>Navigate to <code>machine-controller-manager-provider-{provider-name}</code> directory and create a <code>dev</code> sub-directory in it.</li><li>If the tags on instances & associated resources on the provider are of <code>String</code> type (for example, GCP tags on its instances are of type <code>String</code> and not key-value pair) then add <code>TAGS_ARE_STRINGS := true</code> in the <code>Makefile</code> and export it. For GCP this has already been hard coded in the <code>Makefile</code>.</li></ol><h2 id=running-the-tests>Running the tests</h2><ol><li>There is a rule <code>test-integration</code> in the <code>Makefile</code> of the provider repository, which can be used to start the integration test:<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make test-integration 
</span></span></code></pre></div></li><li>This will ask for additional inputs. Most of them are self explanatory except:</li></ol><ul><li>The script assumes that both the control and target clusters are already being created.</li><li>In case of non-gardener setup (control cluster is not a gardener seed), the name of the machineclass must be <code>test-mc-v1</code> and the value of <code>providerSpec.secretRef.name</code> should be <code>test-mc-secret</code>.</li><li>In case of azure, <code>TARGET_CLUSTER_NAME</code> must be same as the name of the Azure ResourceGroup for the cluster.</li><li>If you are deploying the secret manually, a <code>Secret</code> named <code>test-mc-secret</code> (that contains the provider secret and cloud-config) in the <code>default</code> namespace of the Control Cluster should be created.</li></ul><ol start=3><li>The controllers log files (<code>mcm_process.log</code> and <code>mc_process.log</code>) are stored in <code>.ci/controllers-test/logs</code> repo and can be used later.</li></ol><h2 id=adding-integration-tests-for-new-providers>Adding Integration Tests for new providers</h2><p>For a new provider, Running Integration tests works with no changes. But for the orphan resource test cases to work correctly, the provider-specific API calls and the Resource Tracker Interface (RTI) should be implemented. Please check <a href=https://github.com/gardener/machine-controller-manager-provider-aws/blob/master/test/integration/provider/><code>machine-controller-manager-provider-aws</code></a> for reference.</p><h2 id=extending-integration-tests>Extending integration tests</h2><ul><li>Update <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/test/integration/common/framework.go#L481>ControllerTests</a> to be extend the testcases for all providers. Common testcases for machine|machineDeployment creation|deletion|scaling are packaged into <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/test/integration/common/framework.go#L481>ControllerTests</a>.</li><li>To extend the provider specfic test cases, the changes should be done in the <code>machine-controller-manager-provider-{provider-name}</code> repository. For example, to extended the testcases for <code>machine-controller-manager-provider-aws</code>, make changes to <code>test/integration/controller/controller_test.go</code> inside the <code>machine-controller-manager-provider-aws</code> repository. <code>commons</code> contains the <code>Cluster</code> and <code>Clientset</code> objects that makes it easy to extend the tests.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a03c9f5ae366bec0f55865d19e5686a2>2.8 - Local Setup</h1><h1 id=preparing-the-local-development-setup-mac-os-x>Preparing the Local Development Setup (Mac OS X)</h1><ul><li><a href=/docs/other-components/machine-controller-manager/local_setup/#preparing-the-local-development-setup-mac-os-x>Preparing the Local Development Setup (Mac OS X)</a><ul><li><a href=/docs/other-components/machine-controller-manager/local_setup/#installing-golang-environment>Installing Golang environment</a></li><li><a href=/docs/other-components/machine-controller-manager/local_setup/#installing-docker-optional>Installing <code>Docker</code> (Optional)</a></li><li><a href=/docs/other-components/machine-controller-manager/local_setup/#setup-docker-hub-account-optional>Setup <code>Docker Hub</code> account (Optional)</a></li><li><a href=/docs/other-components/machine-controller-manager/local_setup/#local-development>Local development</a><ul><li><a href=/docs/other-components/machine-controller-manager/local_setup/#installing-the-machine-controller-manager-locally>Installing the Machine Controller Manager locally</a></li></ul></li><li><a href=/docs/other-components/machine-controller-manager/local_setup/#prepare-the-cluster>Prepare the cluster</a></li><li><a href=/docs/other-components/machine-controller-manager/local_setup/#getting-started>Getting started</a></li><li><a href=/docs/other-components/machine-controller-manager/local_setup/#testing-machine-classes>Testing Machine Classes</a></li><li><a href=/docs/other-components/machine-controller-manager/local_setup/#usage>Usage</a></li></ul></li></ul><p>Conceptionally, the Machine Controller Manager is designed to run in a container within a Pod inside a Kubernetes cluster. For development purposes, you can run the Machine Controller Manager as a Go process on your local machine. This process connects to your remote cluster to manage VMs for that cluster. That means that the Machine Controller Manager runs outside a Kubernetes cluster which requires providing a <a href=https://kubernetes.io/docs/tasks/access-application-cluster/authenticate-across-clusters-kubeconfig/>Kubeconfig</a> in your local filesystem and point the Machine Controller Manager to it when running it (see below).</p><p>Although the following installation instructions are for Mac OS X, similar alternate commands could be found for any Linux distribution.</p><h2 id=installing-golang-environment>Installing Golang environment</h2><p>Install the latest version of Golang (at least <code>v1.8.3</code> is required) by using <a href=https://brew.sh/>Homebrew</a>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ brew install golang
</span></span></code></pre></div><p>In order to perform linting on the Go source code, install <a href=https://github.com/golang/lint>Golint</a>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ go get -u golang.org/x/lint/golint
</span></span></code></pre></div><h2 id=installing-docker-optional>Installing <code>Docker</code> (Optional)</h2><p>In case you want to build Docker images for the Machine Controller Manager you have to install Docker itself. We recommend using <a href=https://docs.docker.com/docker-for-mac/>Docker for Mac OS X</a> which can be downloaded from <a href=https://download.docker.com/mac/stable/Docker.dmg>here</a>.</p><h2 id=setup-docker-hub-account-optional>Setup <code>Docker Hub</code> account (Optional)</h2><p>Create a Docker hub account at <a href=https://hub.docker.com/>Docker Hub</a> if you don&rsquo;t already have one.</p><h2 id=local-development>Local development</h2><p>&#9888;&#xfe0f; Before you start developing, please ensure to comply with the following requirements:</p><ol><li>You have understood the <a href=https://kubernetes.io/docs/concepts/>principles of Kubernetes</a>, and its <a href=https://kubernetes.io/docs/concepts/overview/components/>components</a>, what their purpose is and how they interact with each other.</li><li>You have understood the <a href=/docs/other-components/machine-controller-manager/#design-of-machine-controller-manager>architecture of the Machine Controller Manager</a></li></ol><p>The development of the Machine Controller Manager could happen by targeting any cluster. You basically need a Kubernetes cluster running on a set of machines. You just need the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/authenticate-across-clusters-kubeconfig/>Kubeconfig</a> file with the required access permissions attached to it.</p><h3 id=installing-the-machine-controller-manager-locally>Installing the Machine Controller Manager locally</h3><p>Clone the repository from GitHub.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ git clone git@github.com:gardener/machine-controller-manager.git
</span></span><span style=display:flex><span>$ cd machine-controller-manager
</span></span></code></pre></div><h2 id=prepare-the-cluster>Prepare the cluster</h2><ul><li>Connect to the remote kubernetes cluster where you plan to deploy the Machine Controller Manager using kubectl. Set the environment variable KUBECONFIG to the path of the yaml file containing your cluster info</li><li>Now, create the required CRDs on the remote cluster using the following command,</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/crds.yaml
</span></span></code></pre></div><h2 id=getting-started>Getting started</h2><p><strong>Setup and Restore with Gardener</strong></p><p><em>Setup</em></p><p>In gardener access to static kubeconfig files is no longer supported due to security reasons. One needs to generate short-lived (max TTL = 1 day) admin kube configs for target and control clusters.
A convenience script/Makefile target has been provided to do the required initial setup which includes:</p><ul><li>Creating a temporary directory where target and control kubeconfigs will be stored.</li><li>Create a request to generate the short lived admin kubeconfigs. These are downloaded and stored in the temporary folder created above.</li><li>In gardener clusters <code>DWD (Dependency Watchdog)</code> runs as an additional component which can interfere when MCM/CA is scaled down. To prevent that an annotation <code>dependency-watchdog.gardener.cloud/ignore-scaling</code> is added to <code>machine-controller-manager</code> deployment which prevents <code>DWD</code> from scaling up the deployment replicas.</li><li>Scales down <code>machine-controller-manager</code> deployment in the control cluster to 0 replica.</li><li>Creates the required <code>.env</code> file and populates required environment variables which are then used by the <code>Makefile</code> in both <code>machine-controller-manager</code> and in <code>machine-controller-manager-provider-&lt;provider-name></code> projects.</li><li>Copies the generated and downloaded kubeconfig files for the target and control clusters to <code>machine-controller-manager-provider-&lt;provider-name></code> project as well.</li></ul><p>To do the above you can either invoke <code>make gardener-setup</code> or you can directly invoke the script <code>./hack/gardener_local_setup.sh</code>. If you invoke the script with <code>-h or --help</code> option then it will give you all CLI options that one can pass.</p><p><em>Restore</em></p><p>Once the testing is over you can invoke a convenience script/Makefile target which does the following:</p><ul><li>Removes all generated admin kubeconfig files from both <code>machine-controller-manager</code> and in <code>machine-controller-manager-provider-&lt;provider-name></code> projects.</li><li>Removes the <code>.env</code> file that was generated as part of the setup from both <code>machine-controller-manager</code> and in <code>machine-controller-manager-provider-&lt;provider-name></code> projects.</li><li>Scales up <code>machine-controller-manager</code> deployment in the control cluster back to 1 replica.</li><li>Removes the annotation <code>dependency-watchdog.gardener.cloud/ignore-scaling</code> that was added to prevent <code>DWD</code> to scale up MCM.</li></ul><p>To do the above you can either invoke <code>make gardener-restore</code> or you can directly invoke the script <code>./hack/gardener_local_restore.sh</code>. If you invoke the script with <code>-h or --help</code> option then it will give you all CLI options that one can pass.</p><p><strong>Setup and Restore without Gardener</strong></p><p><em>Setup</em></p><p>If you are not running MCM components in a gardener cluster, then it is assumed that there is not going to be any <code>DWD (Dependency Watchdog)</code> component.
A convenience script/Makefile target has been provided to the required initial setup which includes:</p><ul><li>Copies the provided control and target kubeconfig files to <code>machine-controller-manager-provider-&lt;provider-name></code> project.</li><li>Scales down <code>machine-controller-manager</code> deployment in the control cluster to 0 replica.</li><li>Creates the required <code>.env</code> file and populates required environment variables which are then used by the <code>Makefile</code> in both <code>machine-controller-manager</code> and in <code>machine-controller-manager-provider-&lt;provider-name></code> projects.</li></ul><p>To do the above you can either invoke <code>make non-gardener-setup</code> or you can directly invoke the script <code>./hack/non_gardener_local_setup.sh</code>. If you invoke the script with <code>-h or --help</code> option then it will give you all CLI options that one can pass.</p><p><em>Restore</em></p><p>Once the testing is over you can invoke a convenience script/Makefile target which does the following:</p><ul><li>Removes all provided kubeconfig files from both <code>machine-controller-manager</code> and in <code>machine-controller-manager-provider-&lt;provider-name></code> projects.</li><li>Removes the <code>.env</code> file that was generated as part of the setup from both <code>machine-controller-manager</code> and in <code>machine-controller-manager-provider-&lt;provider-name></code> projects.</li><li>Scales up <code>machine-controller-manager</code> deployment in the control cluster back to 1 replica.</li></ul><p>To do the above you can either invoke <code>make non-gardener-restore</code> or you can directly invoke the script <code>./hack/non_gardener_local_restore.sh</code>. If you invoke the script with <code>-h or --help</code> option then it will give you all CLI options that one can pass.</p><p>Once the setup is done then you can start the <code>machine-controller-manager</code> as a local process using the following <code>Makefile</code> target:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make start
</span></span><span style=display:flex><span>I1227 11:08:19.963638   55523 controllermanager.go:204] Starting shared informers
</span></span><span style=display:flex><span>I1227 11:08:20.766085   55523 controller.go:247] Starting machine-controller-manager
</span></span></code></pre></div><p>&#9888;&#xfe0f; The file <code>dev/target-kubeconfig.yaml</code> points to the cluster whose nodes you want to manage. <code>dev/control-kubeconfig.yaml</code> points to the cluster from where you want to manage the nodes from. However, <code>dev/control-kubeconfig.yaml</code> is optional.</p><p>The Machine Controller Manager should now be ready to manage the VMs in your kubernetes cluster.</p><p>&#9888;&#xfe0f; This is assuming that your MCM is built to manage machines for any in-tree supported providers. There is a new way to deploy and manage out of tree (external) support for providers whose development can be <a href=/docs/other-components/machine-controller-manager/cp_support_new/>found here</a></p><h2 id=testing-machine-classes>Testing Machine Classes</h2><p>To test the creation/deletion of a single instance for one particular machine class you can use the <code>managevm</code> cli. The corresponding <code>INFRASTRUCTURE-machine-class.yaml</code> and the <code>INFRASTRUCTURE-secret.yaml</code> need to be defined upfront. To build and run it</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>GO111MODULE=on go build -o managevm cmd/machine-controller-manager-cli/main.go
</span></span><span style=display:flex><span><span style=color:green># create machine</span>
</span></span><span style=display:flex><span>./managevm --secret PATH_TO/INFRASTRUCTURE-secret.yaml --machineclass PATH_TO/INFRASTRUCTURE-machine-class.yaml --classkind INFRASTRUCTURE --machinename test
</span></span><span style=display:flex><span><span style=color:green># delete machine</span>
</span></span><span style=display:flex><span>./managevm --secret PATH_TO/INFRASTRUCTURE-secret.yaml --machineclass PATH_TO/INFRASTRUCTURE-machine-class.yaml --classkind INFRASTRUCTURE --machinename test --machineid INFRASTRUCTURE:///REGION/INSTANCE_ID
</span></span></code></pre></div><h2 id=usage>Usage</h2><p>To start using Machine Controller Manager, follow the links given at <a href=/docs/other-components/machine-controller-manager/>usage here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f69f6015d7782c64761f2ba8dbf994ee>2.9 - Machine</h1><h1 id=creatingdeleting-machines-vm>Creating/Deleting machines (VM)</h1><ul><li><a href=/docs/other-components/machine-controller-manager/machine/#creatingdeleting-machines-vm>Creating/Deleting machines (VM)</a><ul><li><a href=/docs/other-components/machine-controller-manager/machine/#setting-up-your-usage-environment>Setting up your usage environment</a></li><li><a href=/docs/other-components/machine-controller-manager/machine/#important>Important :</a></li><li><a href=/docs/other-components/machine-controller-manager/machine/#creating-machine>Creating machine</a></li><li><a href=/docs/other-components/machine-controller-manager/machine/#inspect-status-of-machine>Inspect status of machine</a></li><li><a href=/docs/other-components/machine-controller-manager/machine/#delete-machine>Delete machine</a></li></ul></li></ul><h2 id=setting-up-your-usage-environment>Setting up your usage environment</h2><ul><li>Follow the <a href=/docs/other-components/machine-controller-manager/prerequisite/>steps described here</a></li></ul><h2 id=important->Important :</h2><blockquote><p>Make sure that the <code>kubernetes/machine_objects/machine.yaml</code> points to the same class name as the <code>kubernetes/machine_classes/aws-machine-class.yaml</code>.</p></blockquote><blockquote><p>Similarly <code>kubernetes/machine_objects/aws-machine-class.yaml</code> secret name and namespace should be same as that mentioned in <code>kubernetes/secrets/aws-secret.yaml</code></p></blockquote><h2 id=creating-machine>Creating machine</h2><ul><li>Modify <code>kubernetes/machine_objects/machine.yaml</code> as per your requirement and create the VM as shown below:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/machine_objects/machine.yaml
</span></span></code></pre></div><p>You should notice that the Machine Controller Manager has immediately picked up your manifest and started to create a new machine by talking to the cloud provider.</p><ul><li>Check Machine Controller Manager machines in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine
</span></span><span style=display:flex><span>NAME           STATUS    AGE
</span></span><span style=display:flex><span>test-machine   Running   5m
</span></span></code></pre></div><p>A new machine is created with the name provided in the <code>kubernetes/machine_objects/machine.yaml</code> file.</p><ul><li>After a few minutes (~3 minutes for AWS), you should notice a new node joining the cluster. You can verify this by running:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span><span style=display:flex><span>NAME                                         STATUS     AGE     VERSION
</span></span><span style=display:flex><span>ip-10-250-14-52.eu-east-1.compute.internal.  Ready      1m      v1.8.0
</span></span></code></pre></div><p>This shows that a new node has successfully joined the cluster.</p><h2 id=inspect-status-of-machine>Inspect status of machine</h2><p>To inspect the status of any created machine, run the command given below.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine test-machine -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: machine.sapcloud.io/v1alpha1
</span></span><span style=display:flex><span>kind: Machine
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    kubectl.kubernetes.io/last-applied-configuration: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      {&#34;apiVersion&#34;:&#34;machine.sapcloud.io/v1alpha1&#34;,&#34;kind&#34;:&#34;Machine&#34;,&#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;labels&#34;:{&#34;test-label&#34;:&#34;test-label&#34;},&#34;name&#34;:&#34;test-machine&#34;,&#34;namespace&#34;:&#34;&#34;},&#34;spec&#34;:{&#34;class&#34;:{&#34;kind&#34;:&#34;AWSMachineClass&#34;,&#34;name&#34;:&#34;test-aws&#34;}}}</span>      
</span></span><span style=display:flex><span>  clusterName: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  creationTimestamp: 2017-12-27T06:58:21Z
</span></span><span style=display:flex><span>  finalizers:
</span></span><span style=display:flex><span>  - machine.sapcloud.io/operator
</span></span><span style=display:flex><span>  generation: 0
</span></span><span style=display:flex><span>  initializers: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    node: ip-10-250-14-52.eu-east-1.compute.internal
</span></span><span style=display:flex><span>    test-label: test-label
</span></span><span style=display:flex><span>  name: test-machine
</span></span><span style=display:flex><span>  namespace: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  resourceVersion: <span style=color:#a31515>&#34;12616948&#34;</span>
</span></span><span style=display:flex><span>  selfLink: /apis/machine.sapcloud.io/v1alpha1/test-machine
</span></span><span style=display:flex><span>  uid: 535e596c-ead3-11e7-a6c0-828f843e4186
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  class:
</span></span><span style=display:flex><span>    kind: AWSMachineClass
</span></span><span style=display:flex><span>    name: test-aws
</span></span><span style=display:flex><span>  providerID: aws:///eu-east-1/i-00bef3f2618ffef23
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - lastHeartbeatTime: 2017-12-27T07:00:46Z
</span></span><span style=display:flex><span>    lastTransitionTime: 2017-12-27T06:59:16Z
</span></span><span style=display:flex><span>    message: kubelet has sufficient disk space available
</span></span><span style=display:flex><span>    reason: KubeletHasSufficientDisk
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;False&#34;</span>
</span></span><span style=display:flex><span>    type: OutOfDisk
</span></span><span style=display:flex><span>  - lastHeartbeatTime: 2017-12-27T07:00:46Z
</span></span><span style=display:flex><span>    lastTransitionTime: 2017-12-27T06:59:16Z
</span></span><span style=display:flex><span>    message: kubelet has sufficient memory available
</span></span><span style=display:flex><span>    reason: KubeletHasSufficientMemory
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;False&#34;</span>
</span></span><span style=display:flex><span>    type: MemoryPressure
</span></span><span style=display:flex><span>  - lastHeartbeatTime: 2017-12-27T07:00:46Z
</span></span><span style=display:flex><span>    lastTransitionTime: 2017-12-27T06:59:16Z
</span></span><span style=display:flex><span>    message: kubelet has no disk pressure
</span></span><span style=display:flex><span>    reason: KubeletHasNoDiskPressure
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;False&#34;</span>
</span></span><span style=display:flex><span>    type: DiskPressure
</span></span><span style=display:flex><span>  - lastHeartbeatTime: 2017-12-27T07:00:46Z
</span></span><span style=display:flex><span>    lastTransitionTime: 2017-12-27T07:00:06Z
</span></span><span style=display:flex><span>    message: kubelet is posting ready status
</span></span><span style=display:flex><span>    reason: KubeletReady
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    type: Ready
</span></span><span style=display:flex><span>  currentStatus:
</span></span><span style=display:flex><span>    lastUpdateTime: 2017-12-27T07:00:06Z
</span></span><span style=display:flex><span>    phase: Running
</span></span><span style=display:flex><span>  lastOperation:
</span></span><span style=display:flex><span>    description: Machine is now ready
</span></span><span style=display:flex><span>    lastUpdateTime: 2017-12-27T07:00:06Z
</span></span><span style=display:flex><span>    state: Successful
</span></span><span style=display:flex><span>    type: Create
</span></span><span style=display:flex><span>  node: ip-10-250-14-52.eu-west-1.compute.internal
</span></span></code></pre></div><h2 id=delete-machine>Delete machine</h2><p>To delete the VM using the <code>kubernetes/machine_objects/machine.yaml</code> as shown below</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl delete -f kubernetes/machine_objects/machine.yaml
</span></span></code></pre></div><p>Now the Machine Controller Manager picks up the manifest immediately and starts to delete the existing VM by talking to the cloud provider. The node should be detached from the cluster in a few minutes (~1min for AWS).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d307931a633a4ff197e34fa848fd815b>2.10 - Machine Deployment</h1><h1 id=maintaining-machine-replicas-using-machines-deployments>Maintaining machine replicas using machines-deployments</h1><ul><li><a href=/docs/other-components/machine-controller-manager/machine_deployment/#maintaining-machine-replicas-using-machines-deployments>Maintaining machine replicas using machines-deployments</a><ul><li><a href=/docs/other-components/machine-controller-manager/machine_deployment/#setting-up-your-usage-environment>Setting up your usage environment</a><ul><li><a href=/docs/other-components/machine-controller-manager/machine_deployment/#important-warning>Important &#9888;&#xfe0f;</a></li></ul></li><li><a href=/docs/other-components/machine-controller-manager/machine_deployment/#creating-machine-deployment>Creating machine-deployment</a></li><li><a href=/docs/other-components/machine-controller-manager/machine_deployment/#inspect-status-of-machine-deployment>Inspect status of machine-deployment</a></li><li><a href=/docs/other-components/machine-controller-manager/machine_deployment/#health-monitoring>Health monitoring</a></li><li><a href=/docs/other-components/machine-controller-manager/machine_deployment/#update-your-machines>Update your machines</a><ul><li><a href=/docs/other-components/machine-controller-manager/machine_deployment/#inspect-existing-cluster-configuration>Inspect existing cluster configuration</a></li><li><a href=/docs/other-components/machine-controller-manager/machine_deployment/#perform-a-rolling-update>Perform a rolling update</a></li><li><a href=/docs/other-components/machine-controller-manager/machine_deployment/#re-check-cluster-configuration>Re-check cluster configuration</a></li><li><a href=/docs/other-components/machine-controller-manager/machine_deployment/#more-variants-of-updates>More variants of updates</a></li></ul></li><li><a href=/docs/other-components/machine-controller-manager/machine_deployment/#undo-an-update>Undo an update</a></li><li><a href=/docs/other-components/machine-controller-manager/machine_deployment/#pause-an-update>Pause an update</a></li><li><a href=/docs/other-components/machine-controller-manager/machine_deployment/#delete-machine-deployment>Delete machine-deployment</a></li></ul></li></ul><h2 id=setting-up-your-usage-environment>Setting up your usage environment</h2><p>Follow the <a href=/docs/other-components/machine-controller-manager/prerequisite/>steps described here</a></p><h3 id=important-warning>Important &#9888;&#xfe0f;</h3><blockquote><p>Make sure that the <code>kubernetes/machine_objects/machine-deployment.yaml</code> points to the same class name as the <code>kubernetes/machine_classes/aws-machine-class.yaml</code>.</p></blockquote><blockquote><p>Similarly <code>kubernetes/machine_classes/aws-machine-class.yaml</code> secret name and namespace should be same as that mentioned in <code>kubernetes/secrets/aws-secret.yaml</code></p></blockquote><h2 id=creating-machine-deployment>Creating machine-deployment</h2><ul><li>Modify <code>kubernetes/machine_objects/machine-deployment.yaml</code> as per your requirement. Modify the number of replicas to the desired number of machines. Then, create an machine-deployment.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/machine_objects/machine-deployment.yaml
</span></span></code></pre></div><p>Now the Machine Controller Manager picks up the manifest immediately and starts to create a new machines based on the number of replicas you have provided in the manifest.</p><ul><li>Check Machine Controller Manager machine-deployments in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machinedeployment
</span></span><span style=display:flex><span>NAME                      READY   DESIRED   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>test-machine-deployment   3       3         3            0           10m
</span></span></code></pre></div><p>You will notice a new machine-deployment with your given name</p><ul><li>Check Machine Controller Manager machine-sets in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>NAME                                 DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f   3         3         0       10m
</span></span></code></pre></div><p>You will notice a new machine-set backing your machine-deployment</p><ul><li>Check Machine Controller Manager machines in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine
</span></span><span style=display:flex><span>NAME                                       STATUS    AGE
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f-5d24b   Pending   5m
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f-6mpn4   Pending   5m
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f-dpt2q   Pending   5m
</span></span></code></pre></div><p>Now you will notice N (number of replicas specified in the manifest) new machines whose name are prefixed with the machine-deployment object name that you created.</p><ul><li>After a few minutes (~3 minutes for AWS), you would see that new nodes have joined the cluster. You can see this using</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$  kubectl get nodes
</span></span><span style=display:flex><span>NAME                                          STATUS    AGE       VERSION
</span></span><span style=display:flex><span>ip-10-250-20-19.eu-west-1.compute.internal    Ready     1m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-27-123.eu-west-1.compute.internal   Ready     1m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-31-80.eu-west-1.compute.internal    Ready     1m        v1.8.0
</span></span></code></pre></div><p>This shows how new nodes have joined your cluster</p><h2 id=inspect-status-of-machine-deployment>Inspect status of machine-deployment</h2><p>To inspect the status of any created machine-deployment run the command below,</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machinedeployment test-machine-deployment -o yaml
</span></span></code></pre></div><p>You should get the following output.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: machine.sapcloud.io/v1alpha1
</span></span><span style=display:flex><span>kind: MachineDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    deployment.kubernetes.io/revision: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>    kubectl.kubernetes.io/last-applied-configuration: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      {&#34;apiVersion&#34;:&#34;machine.sapcloud.io/v1alpha1&#34;,&#34;kind&#34;:&#34;MachineDeployment&#34;,&#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;name&#34;:&#34;test-machine-deployment&#34;,&#34;namespace&#34;:&#34;&#34;},&#34;spec&#34;:{&#34;minReadySeconds&#34;:200,&#34;replicas&#34;:3,&#34;selector&#34;:{&#34;matchLabels&#34;:{&#34;test-label&#34;:&#34;test-label&#34;}},&#34;strategy&#34;:{&#34;rollingUpdate&#34;:{&#34;maxSurge&#34;:1,&#34;maxUnavailable&#34;:1},&#34;type&#34;:&#34;RollingUpdate&#34;},&#34;template&#34;:{&#34;metadata&#34;:{&#34;labels&#34;:{&#34;test-label&#34;:&#34;test-label&#34;}},&#34;spec&#34;:{&#34;class&#34;:{&#34;kind&#34;:&#34;AWSMachineClass&#34;,&#34;name&#34;:&#34;test-aws&#34;}}}}}</span>      
</span></span><span style=display:flex><span>  clusterName: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  creationTimestamp: 2017-12-27T08:55:56Z
</span></span><span style=display:flex><span>  generation: 0
</span></span><span style=display:flex><span>  initializers: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  name: test-machine-deployment
</span></span><span style=display:flex><span>  namespace: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  resourceVersion: <span style=color:#a31515>&#34;12634168&#34;</span>
</span></span><span style=display:flex><span>  selfLink: /apis/machine.sapcloud.io/v1alpha1/test-machine-deployment
</span></span><span style=display:flex><span>  uid: c0b488f7-eae3-11e7-a6c0-828f843e4186
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  minReadySeconds: 200
</span></span><span style=display:flex><span>  replicas: 3
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      test-label: test-label
</span></span><span style=display:flex><span>  strategy:
</span></span><span style=display:flex><span>    rollingUpdate:
</span></span><span style=display:flex><span>      maxSurge: 1
</span></span><span style=display:flex><span>      maxUnavailable: 1
</span></span><span style=display:flex><span>    type: RollingUpdate
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      creationTimestamp: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        test-label: test-label
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      class:
</span></span><span style=display:flex><span>        kind: AWSMachineClass
</span></span><span style=display:flex><span>        name: test-aws
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  availableReplicas: 3
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - lastTransitionTime: 2017-12-27T08:57:22Z
</span></span><span style=display:flex><span>    lastUpdateTime: 2017-12-27T08:57:22Z
</span></span><span style=display:flex><span>    message: Deployment has minimum availability.
</span></span><span style=display:flex><span>    reason: MinimumReplicasAvailable
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>    type: Available
</span></span><span style=display:flex><span>  readyReplicas: 3
</span></span><span style=display:flex><span>  replicas: 3
</span></span><span style=display:flex><span>  updatedReplicas: 3
</span></span></code></pre></div><h2 id=health-monitoring>Health monitoring</h2><p>Health monitor is also applied similar to how it&rsquo;s described for <a href=/docs/other-components/machine-controller-manager/machine_set/>machine-sets</a></p><h2 id=update-your-machines>Update your machines</h2><p>Let us consider the scenario where you wish to update all nodes of your cluster from t2.xlarge machines to m5.xlarge machines. Assume that your current <em>test-aws</em> has its <strong>spec.machineType: t2.xlarge</strong> and your deployment <em>test-machine-deployment</em> points to this AWSMachineClass.</p><h4 id=inspect-existing-cluster-configuration>Inspect existing cluster configuration</h4><ul><li>Check Nodes present in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span><span style=display:flex><span>NAME                                          STATUS    AGE       VERSION
</span></span><span style=display:flex><span>ip-10-250-20-19.eu-west-1.compute.internal    Ready     1m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-27-123.eu-west-1.compute.internal   Ready     1m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-31-80.eu-west-1.compute.internal    Ready     1m        v1.8.0
</span></span></code></pre></div><ul><li>Check Machine Controller Manager machine-sets in the cluster. You will notice one machine-set backing your machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>NAME                                 DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f   3         3         3       10m
</span></span></code></pre></div><ul><li>Login to your cloud provider (AWS). In the VM management console, you will find N VMs created of type t2.xlarge.</li></ul><h4 id=perform-a-rolling-update>Perform a rolling update</h4><p>To update this machine-deployment VMs to <code>m5.xlarge</code>, we would do the following:</p><ul><li>Copy your existing aws-machine-class.yaml</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cp kubernetes/machine_classes/aws-machine-class.yaml kubernetes/machine_classes/aws-machine-class-new.yaml
</span></span></code></pre></div><ul><li>Modify aws-machine-class-new.yaml, and update its <em>metadata.name: test-aws2</em> and <em>spec.machineType: m5.xlarge</em></li><li>Now create this modified MachineClass</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f kubernetes/machine_classes/aws-machine-class-new.yaml
</span></span></code></pre></div><ul><li>Edit your existing machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl edit machinedeployment test-machine-deployment
</span></span></code></pre></div><ul><li>Update from <em>spec.template.spec.class.name: test-aws</em> to <em>spec.template.spec.class.name: test-aws2</em></li></ul><h4 id=re-check-cluster-configuration>Re-check cluster configuration</h4><p>After a few minutes (~3mins)</p><ul><li>Check nodes present in cluster now. They are different nodes.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span><span style=display:flex><span>NAME                                          STATUS    AGE       VERSION
</span></span><span style=display:flex><span>ip-10-250-11-171.eu-west-1.compute.internal   Ready     4m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-17-213.eu-west-1.compute.internal   Ready     5m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-31-81.eu-west-1.compute.internal    Ready     5m        v1.8.0
</span></span></code></pre></div><ul><li>Check Machine Controller Manager machine-sets in the cluster. You will notice two machine-sets backing your machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>NAME                                 DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>test-machine-deployment-5bc6dd7c8f   0         0         0       1h
</span></span><span style=display:flex><span>test-machine-deployment-86ff45cc5    3         3         3       20m
</span></span></code></pre></div><ul><li>Login to your cloud provider (AWS). In the VM management console, you will find N VMs created of type t2.xlarge in terminated state, and N new VMs of type m5.xlarge in running state.</li></ul><p>This shows how a rolling update of a cluster from nodes with t2.xlarge to m5.xlarge went through.</p><h4 id=more-variants-of-updates>More variants of updates</h4><ul><li>The above demonstration was a simple use case. This could be more complex like - updating the system disk image versions/ kubelet versions/ security patches etc.</li><li>You can also play around with the maxSurge and maxUnavailable fields in machine-deployment.yaml</li><li>You can also change the update strategy from rollingupdate to recreate</li></ul><h2 id=undo-an-update>Undo an update</h2><ul><li>Edit the existing machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl edit machinedeployment test-machine-deployment
</span></span></code></pre></div><ul><li>Edit the deployment to have this new field of <em>spec.rollbackTo.revision: 0</em> as shown as comments in <code>kubernetes/machine_objects/machine-deployment.yaml</code></li><li>This will undo your update to the previous version.</li></ul><h2 id=pause-an-update>Pause an update</h2><ul><li>You can also pause the update while update is going on by editing the existing machine-deployment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl edit machinedeployment test-machine-deployment
</span></span></code></pre></div><ul><li><p>Edit the deployment to have this new field of <em>spec.paused: true</em> as shown as comments in <code>kubernetes/machine_objects/machine-deployment.yaml</code></p></li><li><p>This will pause the rollingUpdate if it&rsquo;s in process</p></li><li><p>To resume the update, edit the deployment as mentioned above and remove the field <em>spec.paused: true</em> updated earlier</p></li></ul><h2 id=delete-machine-deployment>Delete machine-deployment</h2><ul><li>To delete the VM using the <code>kubernetes/machine_objects/machine-deployment.yaml</code></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl delete -f kubernetes/machine_objects/machine-deployment.yaml
</span></span></code></pre></div><p>The Machine Controller Manager picks up the manifest and starts to delete the existing VMs by talking to the cloud provider. The nodes should be detached from the cluster in a few minutes (~1min for AWS).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9ab479aa51285e6bf2c3ce853d58a520>2.11 - Machine Error Codes</h1><h1 id=machine-error-code-handling>Machine Error code handling</h1><h2 id=notational-conventions>Notational Conventions</h2><p>The keywords &ldquo;MUST&rdquo;, &ldquo;MUST NOT&rdquo;, &ldquo;REQUIRED&rdquo;, &ldquo;SHALL&rdquo;, &ldquo;SHALL NOT&rdquo;, &ldquo;SHOULD&rdquo;, &ldquo;SHOULD NOT&rdquo;, &ldquo;RECOMMENDED&rdquo;, &ldquo;NOT RECOMMENDED&rdquo;, &ldquo;MAY&rdquo;, and &ldquo;OPTIONAL&rdquo; are to be interpreted as described in <a href=https://datatracker.ietf.org/doc/html/rfc2119>RFC 2119</a> (Bradner, S., &ldquo;Key words for use in RFCs to Indicate Requirement Levels&rdquo;, BCP 14, RFC 2119, March 1997).</p><p>The key words &ldquo;unspecified&rdquo;, &ldquo;undefined&rdquo;, and &ldquo;implementation-defined&rdquo; are to be interpreted as described in the <a href="https://www.open-std.org/jtc1/sc22/wg14/www/C99RationaleV5.10.pdf#page=18">rationale for the C99 standard</a>.</p><p>An implementation is not compliant if it fails to satisfy one or more of the MUST, REQUIRED, or SHALL requirements for the protocols it implements.
An implementation is compliant if it satisfies all the MUST, REQUIRED, and SHALL requirements for the protocols it implements.</p><h2 id=terminology>Terminology</h2><table><thead><tr><th>Term</th><th>Definition</th></tr></thead><tbody><tr><td>CR</td><td>Custom Resource (CR) is defined by a cluster admin using the Kubernetes Custom Resource Definition primitive.</td></tr><tr><td>VM</td><td>A Virtual Machine (VM) provisioned and managed by a provider. It could also refer to a physical machine in case of a bare metal provider.</td></tr><tr><td>Machine</td><td>Machine refers to a VM that is provisioned/managed by MCM. It typically describes the metadata used to store/represent a Virtual Machine</td></tr><tr><td>Node</td><td>Native kubernetes <code>Node</code> object. The objects you get to see when you do a &ldquo;kubectl get nodes&rdquo;. Although nodes can be either physical/virtual machines, for the purposes of our discussions it refers to a VM.</td></tr><tr><td>MCM</td><td><a href=https://github.com/gardener/machine-controller-manager>Machine Controller Manager (MCM)</a> is the controller used to manage higher level Machine Custom Resource (CR) such as machine-set and machine-deployment CRs.</td></tr><tr><td>Provider/Driver/MC</td><td><code>Provider</code> (or) <code>Driver</code> (or) <code>Machine Controller (MC)</code> is the driver responsible for managing machine objects present in the cluster from whom it manages these machines. A simple example could be creation/deletion of VM on the provider.</td></tr></tbody></table><h2 id=pre-requisite>Pre-requisite</h2><h3 id=machineclass-resources>MachineClass Resources</h3><p>MCM introduces the CRD <code>MachineClass</code>. This is a blueprint for creating machines that join a certain cluster as nodes in a certain role. The provider only works with <code>MachineClass</code> resources that have the structure described here.</p><h4 id=providerspec>ProviderSpec</h4><p>The <code>MachineClass</code> resource contains a <code>providerSpec</code> field that is passed in the <code>ProviderSpec</code> request field to CMI methods such as <a href=/docs/other-components/machine-controller-manager/machine_error_codes/#createmachine>CreateMachine</a>. The <code>ProviderSpec</code> can be thought of as a machine template from which the VM specification must be adopted. It can contain key-value pairs of these specs. An example for these key-value pairs are given below.</p><table><thead><tr><th>Parameter</th><th>Mandatory</th><th>Type</th><th>Description</th></tr></thead><tbody><tr><td><code>vmPool</code></td><td>Yes</td><td><code>string</code></td><td>VM pool name, e.g. <code>TEST-WOKER-POOL</code></td></tr><tr><td><code>size</code></td><td>Yes</td><td><code>string</code></td><td>VM size, e.g. <code>xsmall</code>, <code>small</code>, etc. Each size maps to a number of CPUs and memory size.</td></tr><tr><td><code>rootFsSize</code></td><td>No</td><td><code>int</code></td><td>Root (<code>/</code>) filesystem size in GB</td></tr><tr><td><code>tags</code></td><td>Yes</td><td><code>map</code></td><td>Tags to be put on the created VM</td></tr></tbody></table><p>Most of the <code>ProviderSpec</code> fields are not mandatory. If not specified, the provider passes an empty value in the respective <code>Create VM</code> parameter.</p><p>The <code>tags</code> can be used to map a VM to its corresponding machine object&rsquo;s Name</p><p>The <code>ProviderSpec</code> is validated by methods that receive it as a request field for presence of all mandatory parameters and tags, and for validity of all parameters.</p><h4 id=secrets>Secrets</h4><p>The <code>MachineClass</code> resource also contains a <code>secretRef</code> field that contains a reference to a secret. The keys of this secret are passed in the <code>Secrets</code> request field to CMI methods.</p><p>The secret can contain sensitive data such as</p><ul><li><code>cloud-credentials</code> secret data used to authenticate at the provider</li><li><code>cloud-init</code> scripts used to initialize a new VM. The cloud-init script is expected to contain scripts to initialize the Kubelet and make it join the cluster.</li></ul><h4 id=identifying-cluster-machines>Identifying Cluster Machines</h4><p>To implement certain methods, the provider should be able to identify all machines associated with a particular Kubernetes cluster. This can be achieved using one/more of the below mentioned ways:</p><ul><li>Names of VMs created by the provider are prefixed by the cluster ID specified in the ProviderSpec.</li><li>VMs created by the provider are tagged with the special tags like <code>kubernetes.io/cluster</code> (for the cluster ID) and <code>kubernetes.io/role</code> (for the role), specified in the ProviderSpec.</li><li>Mapping <code>Resource Groups</code> to individual cluster.</li></ul><h3 id=error-scheme>Error Scheme</h3><p>All provider API calls defined in this spec MUST return a <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/machinecodes/codes/codes.go>machine error status</a>, which is very similar to <a href=https://github.com/grpc/grpc/blob/master/src/proto/grpc/status/status.proto>standard machine status</a>.</p><h3 id=machine-provider-interface>Machine Provider Interface</h3><ul><li>The provider MUST have a unique way to map a <code>machine object</code> to a <code>VM</code> which triggers the deletion for the corresponding VM backing the machine object.</li><li>The provider SHOULD have a unique way to map the <code>ProviderSpec</code> of a machine-class to a unique <code>Cluster</code>. This avoids deletion of other machines, not backed by the MCM.</li></ul><h4 id=createmachine><code>CreateMachine</code></h4><p>A Provider is REQUIRED to implement this interface method.
This interface method will be called by the MCM to provision a new VM on behalf of the requesting machine object.</p><ul><li><p>This call requests the provider to create a VM backing the machine-object.</p></li><li><p>If VM backing the <code>Machine.Name</code> already exists, and is compatible with the specified <code>Machine</code> object in the <code>CreateMachineRequest</code>, the Provider MUST reply <code>0 OK</code> with the corresponding <code>CreateMachineResponse</code>.</p></li><li><p>The provider can OPTIONALLY make use of the MachineClass supplied in the <code>MachineClass</code> in the <code>CreateMachineRequest</code> to communicate with the provider.</p></li><li><p>The provider can OPTIONALLY make use of the secrets supplied in the <code>Secret</code> in the <code>CreateMachineRequest</code> to communicate with the provider.</p></li><li><p>The provider can OPTIONALLY make use of the <code>Status.LastKnownState</code> in the <code>Machine</code> object to decode the state of the VM operation based on the last known state of the VM. This can be useful to restart/continue an operations which are mean&rsquo;t to be atomic.</p></li><li><p>The provider MUST have a unique way to map a <code>machine object</code> to a <code>VM</code>. This could be implicitly provided by the provider by letting you set VM-names (or) could be explicitly specified by the provider using appropriate tags to map the same.</p></li><li><p>This operation SHOULD be idempotent.</p></li><li><p>The <code>CreateMachineResponse</code> returned by this method is expected to return</p><ul><li><code>ProviderID</code> that uniquely identifys the VM at the provider. This is expected to match with the node.Spec.ProviderID on the node object.</li><li><code>NodeName</code> that is the expected name of the machine when it joins the cluster. It must match with the node name.</li><li><code>LastKnownState</code> is an OPTIONAL field that can store details of the last known state of the VM. It can be used by future operation calls to determine current infrastucture state. This state is saved on the machine object.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// CreateMachine call is responsible for VM creation on the provider
</span></span></span><span style=display:flex><span><span style=color:green></span>CreateMachine(context.Context, <span>*</span>CreateMachineRequest) (<span>*</span>CreateMachineResponse, error)<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// CreateMachineRequest is the create request for VM creation
</span></span></span><span style=display:flex><span><span style=color:green></span>type CreateMachineRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Machine object from whom VM is to be created
</span></span></span><span style=display:flex><span><span style=color:green></span>	Machine <span>*</span>v1alpha1.Machine<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass backing the machine object
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>//  Secret backing the machineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	Secret <span>*</span>corev1.Secret<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// CreateMachineResponse is the create response for VM creation
</span></span></span><span style=display:flex><span><span style=color:green></span>type CreateMachineResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// ProviderID is the unique identification of the VM at the cloud provider.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// ProviderID typically matches with the node.Spec.ProviderID on the node object.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// Eg: gce://project-name/region/vm-ID
</span></span></span><span style=display:flex><span><span style=color:green></span>	ProviderID <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// NodeName is the name of the node-object registered to kubernetes.
</span></span></span><span style=display:flex><span><span style=color:green></span>	NodeName <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// LastKnownState represents the last state of the VM during an creation/deletion error
</span></span></span><span style=display:flex><span><span style=color:green></span>	LastKnownState <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=createmachine-errors>CreateMachine Errors</h5><p>If the provider is unable to complete the CreateMachine call successfully, it MUST return a non-ok ginterface method code in the machine status.
If the conditions defined below are encountered, the provider MUST return the specified machine error code.
The MCM MUST implement the specified error recovery behavior when it encounters the machine error code.</p><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call was successful in creating/adopting a VM that matches supplied creation request. The <code>CreateMachineResponse</code> is returned with desired values</td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>Machine.Name</code> and <code>ProviderSpec</code>. Make sure all parameters are in permitted range of values. Exact issue to be given in <code>.message</code></td><td>Update providerSpec to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>6 ALREADY_EXISTS</td><td>Already exists but desired parameters doesn&rsquo;t match</td><td>Parameters of the existing VM don&rsquo;t match the ProviderSpec</td><td>Create machine with a different name</td><td>N</td></tr><tr><td>7 PERMISSION_DENIED</td><td>Insufficent permissions</td><td>The requestor doesn&rsquo;t have enough permissions to create an VM and it&rsquo;s required dependencies</td><td>Update requestor permissions to grant the same</td><td>N</td></tr><tr><td>8 RESOURCE_EXHAUSTED</td><td>Resource limits have been reached</td><td>The requestor doesn&rsquo;t have enough resource limits to process this creation request</td><td>Enhance resource limits associated with the user/account to process this</td><td>N</td></tr><tr><td>9 PRECONDITION_FAILED</td><td>VM is in inconsistent state</td><td>The VM is in a state that is invalid for this operation</td><td>Manual intervention might be needed to fix the state of the VM</td><td>N</td></tr><tr><td>10 ABORTED</td><td>Operation is pending</td><td>Indicates that there is already an operation pending for the specified machine</td><td>Wait until previous pending operation is processed</td><td>Y</td></tr><tr><td>11 OUT_OF_RANGE</td><td>Resources were out of range</td><td>The requested number of CPUs, memory size, of FS size in ProviderSpec falls outside of the corresponding valid range</td><td>Update request paramaters to request valid resource requests</td><td>N</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>16 UNAUTHENTICATED</td><td>Missing provider credentials</td><td>Request does not have valid authentication credentials for the operation</td><td>Fix the provider credentials</td><td>N</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=initializemachine><code>InitializeMachine</code></h4><p>Provider can OPTIONALLY implement this driver call. Else should return a <code>UNIMPLEMENTED</code> status in error.<br>This interface method will be called by the MCM to initialize a new VM just after creation. This can be used to configure network configuration etc.</p><ul><li>This call requests the provider to initialize a newly created VM backing the machine-object.</li><li>The <code>InitializeMachineResponse</code> returned by this method is expected to return<ul><li><code>ProviderID</code> that uniquely identifys the VM at the provider. This is expected to match with the <code>node.Spec.ProviderID</code> on the node object.</li><li><code>NodeName</code> that is the expected name of the machine when it joins the cluster. It must match with the node name.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// InitializeMachine call is responsible for VM initialization on the provider.
</span></span></span><span style=display:flex><span><span style=color:green></span>InitializeMachine(context.Context, <span>*</span>InitializeMachineRequest) (<span>*</span>InitializeMachineResponse, error)<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// InitializeMachineRequest encapsulates params for the VM Initialization operation (Driver.InitializeMachine).
</span></span></span><span style=display:flex><span><span style=color:green></span>type InitializeMachineRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Machine object representing VM that must be initialized
</span></span></span><span style=display:flex><span><span style=color:green></span>	Machine <span>*</span>v1alpha1.Machine<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass backing the machine object
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Secret backing the machineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	Secret <span>*</span>corev1.Secret<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// InitializeMachineResponse is the response for VM instance initialization (Driver.InitializeMachine).
</span></span></span><span style=display:flex><span><span style=color:green></span>type InitializeMachineResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// ProviderID is the unique identification of the VM at the cloud provider.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// ProviderID typically matches with the node.Spec.ProviderID on the node object.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// Eg: gce://project-name/region/vm-ID
</span></span></span><span style=display:flex><span><span style=color:green></span>	ProviderID <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// NodeName is the name of the node-object registered to kubernetes.
</span></span></span><span style=display:flex><span><span style=color:green></span>	NodeName <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=initializemachine-errors>InitializeMachine Errors</h5><p>If the provider is unable to complete the <code>InitializeMachine</code> call successfully, it MUST return a non-ok machine code in the machine status.</p><p>If the conditions defined below are encountered, the provider MUST return the specified machine error code.
The MCM MUST implement the specified error recovery behavior when it encounters the machine error code.</p><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call was successful in initializing a VM that matches supplied initialization request. The <code>InitializeMachineResponse</code> is returned with desired values</td><td></td><td>N</td></tr><tr><td>5 NOT_FOUND</td><td>Timeout</td><td>VM Instance for Machine isn&rsquo;t found at provider</td><td>Skip Initialization and Continue</td><td>N</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Skip Initialization and continue</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken.</td><td>Needs investigation and possible intervention to fix this</td><td>Y</td></tr><tr><td>17 UNINITIALIZED</td><td>Failed Initialization</td><td>VM Instance could not be initializaed</td><td>Initialization is reattempted in next reconcile cycle</td><td>Y</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=deletemachine><code>DeleteMachine</code></h4><p>A Provider is REQUIRED to implement this driver call.
This driver call will be called by the MCM to deprovision/delete/terminate a VM backed by the requesting machine object.</p><ul><li><p>If a VM corresponding to the specified machine-object&rsquo;s name does not exist or the artifacts associated with the VM do not exist anymore (after deletion), the Provider MUST reply <code>0 OK</code>.</p></li><li><p>The provider SHALL only act on machines belonging to the cluster-id/cluster-name obtained from the <code>ProviderSpec</code>.</p></li><li><p>The provider can OPTIONALY make use of the secrets supplied in the <code>Secrets</code> map in the <code>DeleteMachineRequest</code> to communicate with the provider.</p></li><li><p>The provider can OPTIONALY make use of the <code>Spec.ProviderID</code> map in the <code>Machine</code> object.</p></li><li><p>The provider can OPTIONALLY make use of the <code>Status.LastKnownState</code> in the <code>Machine</code> object to decode the state of the VM operation based on the last known state of the VM. This can be useful to restart/continue an operations which are mean&rsquo;t to be atomic.</p></li><li><p>This operation SHOULD be idempotent.</p></li><li><p>The provider must have a unique way to map a <code>machine object</code> to a <code>VM</code> which triggers the deletion for the corresponding VM backing the machine object.</p></li><li><p>The <code>DeleteMachineResponse</code> returned by this method is expected to return</p><ul><li><code>LastKnownState</code> is an OPTIONAL field that can store details of the last known state of the VM. It can be used by future operation calls to determine current infrastucture state. This state is saved on the machine object.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// DeleteMachine call is responsible for VM deletion/termination on the provider
</span></span></span><span style=display:flex><span><span style=color:green></span>DeleteMachine(context.Context, <span>*</span>DeleteMachineRequest) (<span>*</span>DeleteMachineResponse, error)<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// DeleteMachineRequest is the delete request for VM deletion
</span></span></span><span style=display:flex><span><span style=color:green></span>type DeleteMachineRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Machine object from whom VM is to be deleted
</span></span></span><span style=display:flex><span><span style=color:green></span>	Machine <span>*</span>v1alpha1.Machine<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass backing the machine object
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Secret backing the machineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	Secret <span>*</span>corev1.Secret<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// DeleteMachineResponse is the delete response for VM deletion
</span></span></span><span style=display:flex><span><span style=color:green></span>type DeleteMachineResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// LastKnownState represents the last state of the VM during an creation/deletion error
</span></span></span><span style=display:flex><span><span style=color:green></span>	LastKnownState <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=deletemachine-errors>DeleteMachine Errors</h5><p>If the provider is unable to complete the DeleteMachine call successfully, it MUST return a non-ok machine code in the machine status.
If the conditions defined below are encountered, the provider MUST return the specified machine error code.</p><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call was successful in deleting a VM that matches supplied deletion request.</td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>Machine.Name</code> and make sure that it is in the desired format and not a blank value. Exact issue to be given in <code>.message</code></td><td>Update <code>Machine.Name</code> to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>7 PERMISSION_DENIED</td><td>Insufficent permissions</td><td>The requestor doesn&rsquo;t have enough permissions to delete an VM and it&rsquo;s required dependencies</td><td>Update requestor permissions to grant the same</td><td>N</td></tr><tr><td>9 PRECONDITION_FAILED</td><td>VM is in inconsistent state</td><td>The VM is in a state that is invalid for this operation</td><td>Manual intervention might be needed to fix the state of the VM</td><td>N</td></tr><tr><td>10 ABORTED</td><td>Operation is pending</td><td>Indicates that there is already an operation pending for the specified machine</td><td>Wait until previous pending operation is processed</td><td>Y</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>16 UNAUTHENTICATED</td><td>Missing provider credentials</td><td>Request does not have valid authentication credentials for the operation</td><td>Fix the provider credentials</td><td>N</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=getmachinestatus><code>GetMachineStatus</code></h4><p>A Provider can OPTIONALLY implement this driver call. Else should return a <code>UNIMPLEMENTED</code> status in error.
This call will be invoked by the MC to get the status of a machine.
This optional driver call helps in optimizing the working of the provider by avoiding unwanted calls to <code>CreateMachine()</code> and <code>DeleteMachine()</code>.</p><ul><li>If a VM corresponding to the specified machine object&rsquo;s <code>Machine.Name</code> exists on provider the <code>GetMachineStatusResponse</code> fields are to be filled similar to the <code>CreateMachineResponse</code>.</li><li>The provider SHALL only act on machines belonging to the cluster-id/cluster-name obtained from the <code>ProviderSpec</code>.</li><li>The provider can OPTIONALY make use of the secrets supplied in the <code>Secrets</code> map in the <code>GetMachineStatusRequest</code> to communicate with the provider.</li><li>The provider can OPTIONALY make use of the VM unique ID (returned by the provider on machine creation) passed in the <code>ProviderID</code> map in the <code>GetMachineStatusRequest</code>.</li><li>This operation MUST be idempotent.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// GetMachineStatus call get&#39;s the status of the VM backing the machine object on the provider
</span></span></span><span style=display:flex><span><span style=color:green></span>GetMachineStatus(context.Context, <span>*</span>GetMachineStatusRequest) (<span>*</span>GetMachineStatusResponse, error)<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// GetMachineStatusRequest is the get request for VM info
</span></span></span><span style=display:flex><span><span style=color:green></span>type GetMachineStatusRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Machine object from whom VM status is to be fetched
</span></span></span><span style=display:flex><span><span style=color:green></span>	Machine <span>*</span>v1alpha1.Machine<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass backing the machine object
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>//  Secret backing the machineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	Secret <span>*</span>corev1.Secret<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// GetMachineStatusResponse is the get response for VM info
</span></span></span><span style=display:flex><span><span style=color:green></span>type GetMachineStatusResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// ProviderID is the unique identification of the VM at the cloud provider.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// ProviderID typically matches with the node.Spec.ProviderID on the node object.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// Eg: gce://project-name/region/vm-ID
</span></span></span><span style=display:flex><span><span style=color:green></span>	ProviderID <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// NodeName is the name of the node-object registered to kubernetes.
</span></span></span><span style=display:flex><span><span style=color:green></span>	NodeName <span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=getmachinestatus-errors>GetMachineStatus Errors</h5><p>If the provider is unable to complete the GetMachineStatus call successfully, it MUST return a non-ok machine code in the machine status.
If the conditions defined below are encountered, the provider MUST return the specified machine error code.</p><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call was successful in getting machine details for given machine <code>Machine.Name</code></td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>Machine.Name</code> and make sure that it is in the desired format and not a blank value. Exact issue to be given in <code>.message</code></td><td>Update <code>Machine.Name</code> to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>5 NOT_FOUND</td><td>Machine isn&rsquo;t found at provider</td><td>The machine could not be found at provider</td><td>Not required</td><td>N</td></tr><tr><td>7 PERMISSION_DENIED</td><td>Insufficent permissions</td><td>The requestor doesn&rsquo;t have enough permissions to get details for the VM and it&rsquo;s required dependencies</td><td>Update requestor permissions to grant the same</td><td>N</td></tr><tr><td>9 PRECONDITION_FAILED</td><td>VM is in inconsistent state</td><td>The VM is in a state that is invalid for this operation</td><td>Manual intervention might be needed to fix the state of the VM</td><td>N</td></tr><tr><td>11 OUT_OF_RANGE</td><td>Multiple VMs found</td><td>Multiple VMs found with matching machine object names</td><td>Orphan VM handler to cleanup orphan VMs / Manual intervention maybe required if orphan VM handler isn&rsquo;t enabled.</td><td>Y</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>16 UNAUTHENTICATED</td><td>Missing provider credentials</td><td>Request does not have valid authentication credentials for the operation</td><td>Fix the provider credentials</td><td>N</td></tr><tr><td>17 UNINITIALIZED</td><td>Failed Initialization</td><td>VM Instance could not be initializaed</td><td>Initialization is reattempted in next reconcile cycle</td><td>N</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=listmachines><code>ListMachines</code></h4><p>A Provider can OPTIONALLY implement this driver call. Else should return a <code>UNIMPLEMENTED</code> status in error.
The Provider SHALL return the information about all the machines associated with the <code>MachineClass</code>.
Make sure to use appropriate filters to achieve the same to avoid data transfer overheads.
This optional driver call helps in cleaning up orphan VMs present in the cluster. If not implemented, any orphan VM that might have been created incorrectly by the MCM/Provider (due to bugs in code/infra) might require manual clean up.</p><ul><li>If the Provider succeeded in returning a list of <code>Machine.Name</code> with their corresponding <code>ProviderID</code>, then return <code>0 OK</code>.</li><li>The <code>ListMachineResponse</code> contains a map of <code>MachineList</code> whose<ul><li>Key is expected to contain the <code>ProviderID</code> &</li><li>Value is expected to contain the <code>Machine.Name</code> corresponding to it&rsquo;s kubernetes machine CR object</li></ul></li><li>The provider can OPTIONALY make use of the secrets supplied in the <code>Secrets</code> map in the <code>ListMachinesRequest</code> to communicate with the provider.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// ListMachines lists all the machines that might have been created by the supplied machineClass
</span></span></span><span style=display:flex><span><span style=color:green></span>ListMachines(context.Context, <span>*</span>ListMachinesRequest) (<span>*</span>ListMachinesResponse, error)<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// ListMachinesRequest is the request object to get a list of VMs belonging to a machineClass
</span></span></span><span style=display:flex><span><span style=color:green></span>type ListMachinesRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// Secret backing the machineClass object
</span></span></span><span style=display:flex><span><span style=color:green></span>	Secret <span>*</span>corev1.Secret<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// ListMachinesResponse is the response object of the list of VMs belonging to a machineClass
</span></span></span><span style=display:flex><span><span style=color:green></span>type ListMachinesResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineList is the map of list of machines. Format for the map should be &lt;ProviderID, MachineName&gt;.
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineList map[<span style=color:#2b91af>string</span>]<span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=listmachines-errors>ListMachines Errors</h5><p>If the provider is unable to complete the ListMachines call successfully, it MUST return a non-ok machine code in the machine status.
If the conditions defined below are encountered, the provider MUST return the specified machine error code.
The MCM MUST implement the specified error recovery behavior when it encounters the machine error code.</p><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call for listing all VMs associated with <code>ProviderSpec</code> was successful.</td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>ProviderSpec</code> and make sure that all required fields are present in their desired value format. Exact issue to be given in <code>.message</code></td><td>Update <code>ProviderSpec</code> to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>7 PERMISSION_DENIED</td><td>Insufficent permissions</td><td>The requestor doesn&rsquo;t have enough permissions to list VMs and it&rsquo;s required dependencies</td><td>Update requestor permissions to grant the same</td><td>N</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>16 UNAUTHENTICATED</td><td>Missing provider credentials</td><td>Request does not have valid authentication credentials for the operation</td><td>Fix the provider credentials</td><td>N</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=getvolumeids><code>GetVolumeIDs</code></h4><p>A Provider can OPTIONALLY implement this driver call. Else should return a <code>UNIMPLEMENTED</code> status in error.
This driver call will be called by the MCM to get the <code>VolumeIDs</code> for the list of <code>PersistentVolumes (PVs)</code> supplied.
This OPTIONAL (but recommended) driver call helps in serailzied eviction of pods with PVs while draining of machines. This implies applications backed by PVs would be evicted one by one, leading to shorter application downtimes.</p><ul><li>On succesful returnal of a list of <code>Volume-IDs</code> for all supplied <code>PVSpecs</code>, the Provider MUST reply <code>0 OK</code>.</li><li>The <code>GetVolumeIDsResponse</code> is expected to return a repeated list of <code>strings</code> consisting of the <code>VolumeIDs</code> for <code>PVSpec</code> that could be extracted.</li><li>If for any <code>PV</code> the Provider wasn&rsquo;t able to identify the <code>Volume-ID</code>, the provider MAY chose to ignore it and return the <code>Volume-IDs</code> for the rest of the <code>PVs</code> for whom the <code>Volume-ID</code> was found.</li><li>Getting the <code>VolumeID</code> from the <code>PVSpec</code> depends on the Cloud-provider. You can extract this information by parsing the <code>PVSpec</code> based on the <code>ProviderType</code><ul><li><a href=https://github.com/kubernetes/api/blob/release-1.15/core/v1/types.go#L297-L339>https://github.com/kubernetes/api/blob/release-1.15/core/v1/types.go#L297-L339</a></li><li><a href=https://github.com/kubernetes/api/blob/release-1.15//core/v1/types.go#L175-L257>https://github.com/kubernetes/api/blob/release-1.15//core/v1/types.go#L175-L257</a></li></ul></li><li>This operation MUST be idempotent.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// GetVolumeIDsRequest is the request object to get a list of VolumeIDs for a PVSpec
</span></span></span><span style=display:flex><span><span style=color:green></span>type GetVolumeIDsRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// PVSpecsList is a list of PV specs for whom volume-IDs are required
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// Plugin should parse this raw data into pre-defined list of PVSpecs
</span></span></span><span style=display:flex><span><span style=color:green></span>	PVSpecs []<span>*</span>corev1.PersistentVolumeSpec<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// GetVolumeIDsResponse is the response object of the list of VolumeIDs for a PVSpec
</span></span></span><span style=display:flex><span><span style=color:green></span>type GetVolumeIDsResponse struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// VolumeIDs is a list of VolumeIDs.
</span></span></span><span style=display:flex><span><span style=color:green></span>	VolumeIDs []<span style=color:#2b91af>string</span><span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span></code></pre></div><h5 id=getvolumeids-errors>GetVolumeIDs Errors</h5><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>The call getting list of <code>VolumeIDs</code> for the list of <code>PersistentVolumes</code> was successful.</td><td></td><td>N</td></tr><tr><td>1 CANCELED</td><td>Cancelled</td><td>Call was cancelled. Perform any pending clean-up tasks and return the call</td><td></td><td>N</td></tr><tr><td>2 UNKNOWN</td><td>Something went wrong</td><td>Not enough information on what went wrong</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>3 INVALID_ARGUMENT</td><td>Re-check supplied parameters</td><td>Re-check the supplied <code>PVSpecList</code> and make sure that it is in the desired format. Exact issue to be given in <code>.message</code></td><td>Update <code>PVSpecList</code> to fix issues.</td><td>N</td></tr><tr><td>4 DEADLINE_EXCEEDED</td><td>Timeout</td><td>The call processing exceeded supplied deadline</td><td>Retry operation after sometime</td><td>Y</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this service.</td><td>Retry with an alternate logic or implement this method at the provider. Most methods by default are in this state</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Needs manual intervension to fix this</td><td>N</td></tr><tr><td>14 UNAVAILABLE</td><td>Not Available</td><td>Unavailable indicates the service is currently unavailable.</td><td>Retry operation after sometime</td><td>Y</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h4 id=generatemachineclassformigration><code>GenerateMachineClassForMigration</code></h4><p>A Provider SHOULD implement this driver call, else it MUST return a <code>UNIMPLEMENTED</code> status in error.
This driver call will be called by the Machine Controller to try to perform a machineClass migration for an unknown machineClass Kind. This helps in migration of one kind of machineClass to another kind. For instance an machineClass custom resource of <code>AWSMachineClass</code> to <code>MachineClass</code>.</p><ul><li>On successful generation of machine class the Provider MUST reply <code>0 OK</code> (or) <code>nil</code> error.</li><li><code>GenerateMachineClassForMigrationRequest</code> expects the provider-specific machine class (eg. AWSMachineClass)
to be supplied as the <code>ProviderSpecificMachineClass</code>. The provider is responsible for unmarshalling the golang struct. It also passes a reference to an existing <code>MachineClass</code> object.</li><li>The provider is expected to fill in this<code>MachineClass</code> object based on the conversions.</li><li>An optional <code>ClassSpec</code> containing the <code>type ClassSpec struct</code> is also provided to decode the provider info.</li><li><code>GenerateMachineClassForMigration</code> is only responsible for filling up the passed <code>MachineClass</code> object.</li><li>The task of creating the new <code>CR</code> of the new kind (MachineClass) with the same name as the previous one and also annotating the old machineClass CR with a migrated annotation and migrating existing references is done by the calling library implicitly.</li><li>This operation MUST be idempotent.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=color:green>// GenerateMachineClassForMigrationRequest is the request for generating the generic machineClass
</span></span></span><span style=display:flex><span><span style=color:green>// for the provider specific machine class
</span></span></span><span style=display:flex><span><span style=color:green></span>type GenerateMachineClassForMigrationRequest struct {<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// ProviderSpecificMachineClass is provider specfic machine class object.
</span></span></span><span style=display:flex><span><span style=color:green></span>	<span style=color:green>// E.g. AWSMachineClass
</span></span></span><span style=display:flex><span><span style=color:green></span>	ProviderSpecificMachineClass interface{}<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// MachineClass is the machine class object generated that is to be filled up
</span></span></span><span style=display:flex><span><span style=color:green></span>	MachineClass <span>*</span>v1alpha1.MachineClass<span>
</span></span></span><span style=display:flex><span><span></span>	<span style=color:green>// ClassSpec contains the class spec object to determine the machineClass kind
</span></span></span><span style=display:flex><span><span style=color:green></span>	ClassSpec <span>*</span>v1alpha1.ClassSpec<span>
</span></span></span><span style=display:flex><span><span></span>}<span>
</span></span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:green>// GenerateMachineClassForMigrationResponse is the response for generating the generic machineClass
</span></span></span><span style=display:flex><span><span style=color:green>// for the provider specific machine class
</span></span></span><span style=display:flex><span><span style=color:green></span>type GenerateMachineClassForMigrationResponse struct{}<span>
</span></span></span></code></pre></div><h5 id=migratemachineclass-errors>MigrateMachineClass Errors</h5><table><thead><tr><th>machine Code</th><th>Condition</th><th>Description</th><th>Recovery Behavior</th><th>Auto Retry Required</th></tr></thead><tbody><tr><td>0 OK</td><td>Successful</td><td>Migration of provider specific machine class was successful</td><td>Machine reconcilation is retried once the new class has been created</td><td>Y</td></tr><tr><td>12 UNIMPLEMENTED</td><td>Not implemented</td><td>Unimplemented indicates operation is not implemented or not supported/enabled in this provider.</td><td>None</td><td>N</td></tr><tr><td>13 INTERNAL</td><td>Major error</td><td>Means some invariants expected by underlying system has been broken. If you see one of these errors, something is very broken.</td><td>Might need manual intervension to fix this</td><td>Y</td></tr></tbody></table><p>The status <code>message</code> MUST contain a human readable description of error, if the status <code>code</code> is not <code>OK</code>.
This string MAY be surfaced by MCM to end users.</p><h2 id=configuration-and-operation>Configuration and Operation</h2><h3 id=supervised-lifecycle-management>Supervised Lifecycle Management</h3><ul><li>For Providers packaged in software form:<ul><li>Provider Packages SHOULD use a well-documented container image format (e.g., Docker, OCI).</li><li>The chosen package image format MAY expose configurable Provider properties as environment variables, unless otherwise indicated in the section below.
Variables so exposed SHOULD be assigned default values in the image manifest.</li><li>A Provider Supervisor MAY programmatically evaluate or otherwise scan a Provider Package’s image manifest in order to discover configurable environment variables.</li><li>A Provider SHALL NOT assume that an operator or Provider Supervisor will scan an image manifest for environment variables.</li></ul></li></ul><h4 id=environment-variables>Environment Variables</h4><ul><li>Variables defined by this specification SHALL be identifiable by their <code>MC_</code> name prefix.</li><li>Configuration properties not defined by the MC specification SHALL NOT use the same <code>MC_</code> name prefix; this prefix is reserved for common configuration properties defined by the MC specification.</li><li>The Provider Supervisor SHOULD supply all RECOMMENDED MC environment variables to a Provider.</li><li>The Provider Supervisor SHALL supply all REQUIRED MC environment variables to a Provider.</li></ul><h5 id=logging>Logging</h5><ul><li>Providers SHOULD generate log messages to ONLY standard output and/or standard error.<ul><li>In this case the Provider Supervisor SHALL assume responsibility for all log lifecycle management.</li></ul></li><li>Provider implementations that deviate from the above recommendation SHALL clearly and unambiguously document the following:<ul><li>Logging configuration flags and/or variables, including working sample configurations.</li><li>Default log destination(s) (where do the logs go if no configuration is specified?)</li><li>Log lifecycle management ownership and related guidance (size limits, rate limits, rolling, archiving, expunging, etc.) applicable to the logging mechanism embedded within the Provider.</li></ul></li><li>Providers SHOULD NOT write potentially sensitive data to logs (e.g. secrets).</li></ul><h5 id=available-services>Available Services</h5><ul><li>Provider Packages MAY support all or a subset of CMI services; service combinations MAY be configurable at runtime by the Provider Supervisor.<ul><li>This specification does not dictate the mechanism by which mode of operation MUST be discovered, and instead places that burden upon the VM Provider.</li></ul></li><li>Misconfigured provider software SHOULD fail-fast with an OS-appropriate error code.</li></ul><h5 id=linux-capabilities>Linux Capabilities</h5><ul><li>Providers SHOULD clearly document any additionally required capabilities and/or security context.</li></ul><h5 id=cgroup-isolation>Cgroup Isolation</h5><ul><li>A Provider MAY be constrained by cgroups.</li></ul><h5 id=resource-requirements>Resource Requirements</h5><ul><li>VM Providers SHOULD unambiguously document all of a Provider’s resource requirements.</li></ul><h3 id=deploying>Deploying</h3><ul><li><strong>Recommended:</strong> The MCM and Provider are typically expected to run as two containers inside a common <code>Pod</code>.</li><li>However, for the security reasons they could execute on seperate Pods provided they have a secure way to exchange data between them.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1dbd9c09f455939438c6b6f19af0c7fc>2.12 - Machine Set</h1><h1 id=maintaining-machine-replicas-using-machines-sets>Maintaining machine replicas using machines-sets</h1><ul><li><a href=/docs/other-components/machine-controller-manager/machine_set/#maintaining-machine-replicas-using-machines-sets>Maintaining machine replicas using machines-sets</a><ul><li><a href=/docs/other-components/machine-controller-manager/machine_set/#setting-up-your-usage-environment>Setting up your usage environment</a></li><li><a href=/docs/other-components/machine-controller-manager/machine_set/#important-warning>Important &#9888;&#xfe0f;</a></li><li><a href=/docs/other-components/machine-controller-manager/machine_set/#creating-machine-set>Creating machine-set</a></li><li><a href=/docs/other-components/machine-controller-manager/machine_set/#inspect-status-of-machine-set>Inspect status of machine-set</a></li><li><a href=/docs/other-components/machine-controller-manager/machine_set/#health-monitoring>Health monitoring</a></li><li><a href=/docs/other-components/machine-controller-manager/machine_set/#delete-machine-set>Delete machine-set</a></li></ul></li></ul><h2 id=setting-up-your-usage-environment>Setting up your usage environment</h2><ul><li>Follow the <a href=/docs/other-components/machine-controller-manager/prerequisite/>steps described here</a></li></ul><h2 id=important-warning>Important &#9888;&#xfe0f;</h2><blockquote><p>Make sure that the <code>kubernetes/machines_objects/machine-set.yaml</code> points to the same class name as the <code>kubernetes/machine_classes/aws-machine-class.yaml</code>.</p></blockquote><blockquote><p>Similarly <code>kubernetes/machine_classes/aws-machine-class.yaml</code> secret name and namespace should be same as that mentioned in <code>kubernetes/secrets/aws-secret.yaml</code></p></blockquote><h2 id=creating-machine-set>Creating machine-set</h2><ul><li>Modify <code>kubernetes/machine_objects/machine-set.yaml</code> as per your requirement. You can modify the number of replicas to the desired number of machines. Then, create an machine-set:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/machine_objects/machine-set.yaml
</span></span></code></pre></div><p>You should notice that the Machine Controller Manager has immediately picked up your manifest and started to create a new machines based on the number of replicas you have provided in the manifest.</p><ul><li>Check Machine Controller Manager machine-sets in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>NAME               DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>test-machine-set   3         3         0       1m
</span></span></code></pre></div><p>You will see a new machine-set with your given name</p><ul><li>Check Machine Controller Manager machines in the cluster:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine
</span></span><span style=display:flex><span>NAME                     STATUS    AGE
</span></span><span style=display:flex><span>test-machine-set-b57zs   Pending   5m
</span></span><span style=display:flex><span>test-machine-set-c4bg8   Pending   5m
</span></span><span style=display:flex><span>test-machine-set-kvskg   Pending   5m
</span></span></code></pre></div><p>Now you will see N (number of replicas specified in the manifest) new machines whose names are prefixed with the machine-set object name that you created.</p><ul><li>After a few minutes (~3 minutes for AWS), you should notice new nodes joining the cluster. You can verify this by running:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span><span style=display:flex><span>NAME                                         STATUS    AGE       VERSION
</span></span><span style=display:flex><span>ip-10-250-0-234.eu-west-1.compute.internal   Ready     3m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-15-98.eu-west-1.compute.internal   Ready     3m        v1.8.0
</span></span><span style=display:flex><span>ip-10-250-6-21.eu-west-1.compute.internal    Ready     2m        v1.8.0
</span></span></code></pre></div><p>This shows how new nodes have joined your cluster</p><h2 id=inspect-status-of-machine-set>Inspect status of machine-set</h2><ul><li>To inspect the status of any created machine-set run the following command:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset test-machine-set -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: machine.sapcloud.io/v1alpha1
</span></span><span style=display:flex><span>kind: MachineSet
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    kubectl.kubernetes.io/last-applied-configuration: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      {&#34;apiVersion&#34;:&#34;machine.sapcloud.io/v1alpha1&#34;,&#34;kind&#34;:&#34;MachineSet&#34;,&#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;name&#34;:&#34;test-machine-set&#34;,&#34;namespace&#34;:&#34;&#34;,&#34;test-label&#34;:&#34;test-label&#34;},&#34;spec&#34;:{&#34;minReadySeconds&#34;:200,&#34;replicas&#34;:3,&#34;selector&#34;:{&#34;matchLabels&#34;:{&#34;test-label&#34;:&#34;test-label&#34;}},&#34;template&#34;:{&#34;metadata&#34;:{&#34;labels&#34;:{&#34;test-label&#34;:&#34;test-label&#34;}},&#34;spec&#34;:{&#34;class&#34;:{&#34;kind&#34;:&#34;AWSMachineClass&#34;,&#34;name&#34;:&#34;test-aws&#34;}}}}}</span>      
</span></span><span style=display:flex><span>  clusterName: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  creationTimestamp: 2017-12-27T08:37:42Z
</span></span><span style=display:flex><span>  finalizers:
</span></span><span style=display:flex><span>  - machine.sapcloud.io/operator
</span></span><span style=display:flex><span>  generation: 0
</span></span><span style=display:flex><span>  initializers: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  name: test-machine-set
</span></span><span style=display:flex><span>  namespace: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>  resourceVersion: <span style=color:#a31515>&#34;12630893&#34;</span>
</span></span><span style=display:flex><span>  selfLink: /apis/machine.sapcloud.io/v1alpha1/test-machine-set
</span></span><span style=display:flex><span>  uid: 3469faaa-eae1-11e7-a6c0-828f843e4186
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  machineClass: {}
</span></span><span style=display:flex><span>  minReadySeconds: 200
</span></span><span style=display:flex><span>  replicas: 3
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      test-label: test-label
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      creationTimestamp: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        test-label: test-label
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      class:
</span></span><span style=display:flex><span>        kind: AWSMachineClass
</span></span><span style=display:flex><span>        name: test-aws
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  availableReplicas: 3
</span></span><span style=display:flex><span>  fullyLabeledReplicas: 3
</span></span><span style=display:flex><span>  machineSetCondition: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  lastOperation:
</span></span><span style=display:flex><span>    lastUpdateTime: <span style=color:#00f>null</span>
</span></span><span style=display:flex><span>  observedGeneration: 0
</span></span><span style=display:flex><span>  readyReplicas: 3
</span></span><span style=display:flex><span>  replicas: 3
</span></span></code></pre></div><h2 id=health-monitoring>Health monitoring</h2><ul><li>If you try to delete/terminate any of the machines backing the machine-set by either talking to the Machine Controller Manager or from the cloud provider, the Machine Controller Manager recreates a matching healthy machine to replace the deleted machine.</li><li>Similarly, if any of your machines are unreachable or in an unhealthy state (kubelet not ready / disk pressure) for longer than the configured timeout (~ 5mins), the Machine Controller Manager recreates the nodes to replace the unhealthy nodes.</li></ul><h2 id=delete-machine-set>Delete machine-set</h2><ul><li>To delete the VM using the <code>kubernetes/machine_objects/machine-set.yaml</code>:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl delete -f kubernetes/machine-set.yaml
</span></span></code></pre></div><p>Now the Machine Controller Manager has immediately picked up your manifest and started to delete the existing VMs by talking to the cloud provider. Your nodes should be detached from the cluster in a few minutes (~1min for AWS).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f2f35b1645215d45337b7c1d319cc43a>2.13 - Prerequisite</h1><h1 id=setting-up-the-usage-environment>Setting up the usage environment</h1><ul><li><a href=/docs/other-components/machine-controller-manager/prerequisite/#setting-up-the-usage-environment>Setting up the usage environment</a><ul><li><a href=/docs/other-components/machine-controller-manager/prerequisite/#important-warning>Important &#9888;&#xfe0f;</a></li><li><a href=/docs/other-components/machine-controller-manager/prerequisite/#set-kubeconfig>Set KUBECONFIG</a></li><li><a href=/docs/other-components/machine-controller-manager/prerequisite/#replace-provider-credentials-and-desired-vm-configurations>Replace provider credentials and desired VM configurations</a></li><li><a href=/docs/other-components/machine-controller-manager/prerequisite/#deploy-required-crds-and-objects>Deploy required CRDs and Objects</a></li><li><a href=/docs/other-components/machine-controller-manager/prerequisite/#check-current-cluster-state>Check current cluster state</a></li></ul></li></ul><h2 id=important-warning>Important &#9888;&#xfe0f;</h2><blockquote><p>All paths are relative to the root location of this project repository.</p></blockquote><blockquote><p>Run the Machine Controller Manager either as described in <a href=/docs/other-components/machine-controller-manager/local_setup/>Setting up a local development environment</a> or <a href=/docs/other-components/machine-controller-manager/deployment/>Deploying the Machine Controller Manager into a Kubernetes cluster</a>.</p></blockquote><blockquote><p>Make sure that the following steps are run before managing machines/ machine-sets/ machine-deploys.</p></blockquote><h2 id=set-kubeconfig>Set KUBECONFIG</h2><p>Using the existing <a href=https://kubernetes.io/docs/tasks/access-application-cluster/authenticate-across-clusters-kubeconfig/>Kubeconfig</a>, open another Terminal panel/window with the <code>KUBECONFIG</code> environment variable pointing to this Kubeconfig file as shown below,</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ export KUBECONFIG=&lt;PATH_TO_REPO&gt;/dev/kubeconfig.yaml
</span></span></code></pre></div><h2 id=replace-provider-credentials-and-desired-vm-configurations>Replace provider credentials and desired VM configurations</h2><p>Open <code>kubernetes/machine_classes/aws-machine-class.yaml</code> and replace required values there with the desired VM configurations.</p><p>Similarily open <code>kubernetes/secrets/aws-secret.yaml</code> and replace - <em>userData, providerAccessKeyId, providerSecretAccessKey</em> with base64 encoded values of cloudconfig file, AWS access key id, and AWS secret access key respectively. Use the following command to get the base64 encoded value of your details</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ echo <span style=color:#a31515>&#34;sample-cloud-config&#34;</span> | base64
</span></span><span style=display:flex><span>base64-encoded-cloud-config
</span></span></code></pre></div><p>Do the same for your access key id and secret access key.</p><h2 id=deploy-required-crds-and-objects>Deploy required CRDs and Objects</h2><p>Create all the required CRDs in the cluster using <code>kubernetes/crds.yaml</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/crds.yaml
</span></span></code></pre></div><p>Create the class template that will be used as an machine template to create VMs using <code>kubernetes/machine_classes/aws-machine-class.yaml</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/machine_classes/aws-machine-class.yaml
</span></span></code></pre></div><p>Create the secret used for the cloud credentials and cloudconfig using <code>kubernetes/secrets/aws-secret.yaml</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f kubernetes/secrets/aws-secret.yaml
</span></span></code></pre></div><h2 id=check-current-cluster-state>Check current cluster state</h2><p>Get to know the current cluster state using the following commands,</p><ul><li>Checking aws-machine-class in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get awsmachineclass
</span></span><span style=display:flex><span>NAME       MACHINE TYPE   AMI          AGE
</span></span><span style=display:flex><span>test-aws   t2.large       ami-123456   5m
</span></span></code></pre></div><ul><li>Checking kubernetes secrets in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get secret
</span></span><span style=display:flex><span>NAME                  TYPE                                  DATA      AGE
</span></span><span style=display:flex><span>test-secret           Opaque                                3         21h
</span></span></code></pre></div><ul><li>Checking kubernetes nodes in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get nodes
</span></span></code></pre></div><p>Lists the default set of nodes attached to your cluster</p><ul><li>Checking Machine Controller Manager machines in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machine
</span></span><span style=display:flex><span>No resources found.
</span></span></code></pre></div><ul><li>Checking Machine Controller Manager machine-sets in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machineset
</span></span><span style=display:flex><span>No resources found.
</span></span></code></pre></div><ul><li>Checking Machine Controller Manager machine-deploys in the cluster</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get machinedeployment
</span></span><span style=display:flex><span>No resources found.
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-8b77049d5ac3cdd7c4372044ce177c64>2.14 - Testing And Dependencies</h1><h2 id=dependency-management>Dependency management</h2><p>We use golang modules to manage golang dependencies. In order to add a new package dependency to the project, you can perform <code>go get &lt;PACKAGE>@&lt;VERSION></code> or edit the <code>go.mod</code> file and append the package along with the version you want to use.</p><h3 id=updating-dependencies>Updating dependencies</h3><p>The <code>Makefile</code> contains a rule called <code>tidy</code> which performs <code>go mod tidy</code>.</p><p><code>go mod tidy</code> makes sure go.mod matches the source code in the module. It adds any missing modules necessary to build the current module&rsquo;s packages and dependencies, and it removes unused modules that don&rsquo;t provide any relevant packages.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ make tidy
</span></span></code></pre></div><p>The dependencies are installed into the go mod cache folder.</p><p>&#9888;&#xfe0f; Make sure you test the code after you have updated the dependencies!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f95024613e3fc900eec5f206fcaf61c5>3 - Etcd Druid</h1><div class=lead>A druid for etcd management in Gardener</div><p><img src=/__resources/etcd-druid-with-tagline_d6d07f.png style=width:120%></img></p><p><a href=https://api.reuse.software/info/github.com/gardener/etcd-druid><img src=https://api.reuse.software/badge/github.com/gardener/etcd-druid alt="REUSE status"></a> <a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/etcd-druid-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/etcd-druid-master/jobs/master-head-update-job/badge alt="CI Build status"></a> <a href=https://goreportcard.com/report/github.com/gardener/etcd-druid><img src=https://goreportcard.com/badge/github.com/gardener/etcd-druid alt="Go Report Card"></a> <a href=https://github.com/gardener/etcd-druid/blob/master/LICENSE><img src=https://img.shields.io/badge/License-Apache--2.0-blue.svg alt="License: Apache-2.0"></a> <a href=https://github.com/gardener/etcd-druid><img src="https://img.shields.io/github/v/release/gardener/etcd-druid.svg?style=flat" alt=Release></a> <a href=https://pkg.go.dev/github.com/gardener/etcd-druid><img src=https://pkg.go.dev/badge/github.com/gardener/etcd-druid.svg alt="Go Reference"></a> <a href=https://gardener.github.io/etcd-druid/index.html><img src=https://img.shields.io/badge/Docs-reference-orange alt=Docs></a></p><p><code>etcd-druid</code> is an <a href=https://github.com/etcd-io/etcd>etcd</a> <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/operator/>operator</a> which makes it easy to configure, provision, reconcile, monitor and delete etcd clusters. It enables management of etcd clusters through <a href=https://github.com/gardener/etcd-druid/blob/master/config/crd/bases/crd-druid.gardener.cloud_etcds.yaml>declarative Kubernetes API model</a>.</p><p>In every etcd cluster managed by <code>etcd-druid</code>, each etcd member is a two container <code>Pod</code> which consists of:</p><ul><li><a href=https://github.com/gardener/etcd-wrapper>etcd-wrapper</a> which manages the lifecycle (validation & initialization) of an etcd.</li><li><a href=https://github.com/gardener/etcd-backup-restore>etcd-backup-restore</a> sidecar which currently provides the following capabilities (the list is not comprehensive):<ul><li><a href=https://github.com/etcd-io/etcd>etcd</a> DB validation.</li><li>Scheduled <a href=https://github.com/etcd-io/etcd>etcd</a> DB defragmentation.</li><li>Backup - <a href=https://github.com/etcd-io/etcd>etcd</a> DB snapshots are taken regularly and backed in an object store if one is configured.</li><li>Restoration - In case of a DB corruption for a single-member cluster it helps in restoring from latest set of snapshots (full & delta).</li><li>Member control operations.</li></ul></li></ul><p><code>etcd-druid</code> additionally provides the following capabilities:</p><ul><li>Facilitates declarative scale-out of <a href=https://github.com/etcd-io/etcd>etcd</a> clusters.</li><li>Provides protection against accidental deletion/mutation of resources provisioned as part of an etcd cluster.</li><li>Offers an asynchronous and threshold based capability to process backed up snapshots to:<ul><li>Potentially minimize the recovery time by leveraging restoration from backups followed by <a href=https://etcd.io/docs/v3.4/op-guide/maintenance/>etcd&rsquo;s compaction and defragmentation</a>.</li><li>Indirectly assert integrity of the backed up snaphots.</li></ul></li><li>Allows seamless copy of backups between any two object store buckets.</li></ul><h2 id=start-using-or-developing-etcd-druid-locally>Start using or developing <code>etcd-druid</code> locally</h2><p>If you are looking to try out druid then you can use a <a href=https://kind.sigs.k8s.io/>Kind</a> cluster based setup.</p><p><a href=https://github.com/user-attachments/assets/cfe0d891-f709-4d7f-b975-4300c6de67e4>https://github.com/user-attachments/assets/cfe0d891-f709-4d7f-b975-4300c6de67e4</a></p><p>For detailed documentation, see our <a href=https://gardener.github.io/etcd-druid/index.html>docs</a>.</p><h2 id=contributions>Contributions</h2><p>If you wish to contribute then please see our <a href=/docs/other-components/etcd-druid/contribution/>contributor guidelines</a>.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>We always look forward to active community engagement. Please report bugs or suggestions on how we can enhance <code>etcd-druid</code> on <a href=https://github.com/gardener/etcd-druid/issues>GitHub Issues</a>.</p><h2 id=license>License</h2><p>Release under <a href=https://github.com/gardener/etcd-druid/blob/master/LICENSE>Apache-2.0</a> license.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0fd23e3f8def1f60476fbcc7a230dc6f>3.1 - API Reference</h1><p>Packages:</p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1>druid.gardener.cloud/v1alpha1</a></li></ul><h2 id=druid.gardener.cloud/v1alpha1>druid.gardener.cloud/v1alpha1</h2><p><p>Package v1alpha1 is the v1alpha1 version of the etcd-druid API.</p></p>Resource Types:<ul></ul><h3 id=druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a>)</p><p><p>BackupSpec defines parameters associated with the full and delta snapshots of etcd.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>port</code></br><em>int32</em></td><td><em>(Optional)</em><p>Port define the port on which etcd-backup-restore server will be exposed.</p></td></tr><tr><td><code>tls</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>image</code></br><em>string</em></td><td><em>(Optional)</em><p>Image defines the etcd container image and tag</p></td></tr><tr><td><code>store</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><em>(Optional)</em><p>Store defines the specification of object store provider for storing backups.</p></td></tr><tr><td><code>resources</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core>Kubernetes core/v1.ResourceRequirements</a></em></td><td><em>(Optional)</em><p>Resources defines compute Resources required by backup-restore container.
More info: <a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/</a></p></td></tr><tr><td><code>compactionResources</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core>Kubernetes core/v1.ResourceRequirements</a></em></td><td><em>(Optional)</em><p>CompactionResources defines compute Resources required by compaction job.
More info: <a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/</a></p></td></tr><tr><td><code>fullSnapshotSchedule</code></br><em>string</em></td><td><em>(Optional)</em><p>FullSnapshotSchedule defines the cron standard schedule for full snapshots.</p></td></tr><tr><td><code>garbageCollectionPolicy</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.GarbageCollectionPolicy>GarbageCollectionPolicy</a></em></td><td><em>(Optional)</em><p>GarbageCollectionPolicy defines the policy for garbage collecting old backups</p></td></tr><tr><td><code>garbageCollectionPeriod</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>GarbageCollectionPeriod defines the period for garbage collecting old backups</p></td></tr><tr><td><code>deltaSnapshotPeriod</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>DeltaSnapshotPeriod defines the period after which delta snapshots will be taken</p></td></tr><tr><td><code>deltaSnapshotMemoryLimit</code></br><em>k8s.io/apimachinery/pkg/api/resource.Quantity</em></td><td><em>(Optional)</em><p>DeltaSnapshotMemoryLimit defines the memory limit after which delta snapshots will be taken</p></td></tr><tr><td><code>compression</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompressionSpec>CompressionSpec</a></em></td><td><em>(Optional)</em><p>SnapshotCompression defines the specification for compression of Snapshots.</p></td></tr><tr><td><code>enableProfiling</code></br><em>bool</em></td><td><em>(Optional)</em><p>EnableProfiling defines if profiling should be enabled for the etcd-backup-restore-sidecar</p></td></tr><tr><td><code>etcdSnapshotTimeout</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>EtcdSnapshotTimeout defines the timeout duration for etcd FullSnapshot operation</p></td></tr><tr><td><code>leaderElection</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.LeaderElectionSpec>LeaderElectionSpec</a></em></td><td><em>(Optional)</em><p>LeaderElection defines parameters related to the LeaderElection configuration.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.ClientService>ClientService</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a>)</p><p><p>ClientService defines the parameters of the client service that a user can specify</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Annotations specify the annotations that should be added to the client service</p></td></tr><tr><td><code>labels</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Labels specify the labels that should be added to the client service</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.CompactionMode>CompactionMode
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SharedConfig>SharedConfig</a>)</p><p><p>CompactionMode defines the auto-compaction-mode: ‘periodic’ or ‘revision’.
‘periodic’ for duration based retention and ‘revision’ for revision number based retention.</p></p><h3 id=druid.gardener.cloud/v1alpha1.CompressionPolicy>CompressionPolicy
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompressionSpec>CompressionSpec</a>)</p><p><p>CompressionPolicy defines the type of policy for compression of snapshots.</p></p><h3 id=druid.gardener.cloud/v1alpha1.CompressionSpec>CompressionSpec</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>)</p><p><p>CompressionSpec defines parameters related to compression of Snapshots(full as well as delta).</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><em>(Optional)</em></td></tr><tr><td><code>policy</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompressionPolicy>CompressionPolicy</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.Condition>Condition</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus>EtcdCopyBackupsTaskStatus</a>,
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</a>)</p><p><p>Condition holds the information about the state of a resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.ConditionType>ConditionType</a></em></td><td><p>Type of the Etcd condition.</p></td></tr><tr><td><code>status</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.ConditionStatus>ConditionStatus</a></em></td><td><p>Status of the condition, one of True, False, Unknown.</p></td></tr><tr><td><code>lastTransitionTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last time the condition transitioned from one status to another.</p></td></tr><tr><td><code>lastUpdateTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last time the condition was updated.</p></td></tr><tr><td><code>reason</code></br><em>string</em></td><td><p>The reason for the condition’s last transition.</p></td></tr><tr><td><code>message</code></br><em>string</em></td><td><p>A human-readable message indicating details about the transition.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.ConditionStatus>ConditionStatus
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition>Condition</a>)</p><p><p>ConditionStatus is the status of a condition.</p></p><h3 id=druid.gardener.cloud/v1alpha1.ConditionType>ConditionType
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition>Condition</a>)</p><p><p>ConditionType is the type of condition.</p></p><h3 id=druid.gardener.cloud/v1alpha1.CrossVersionObjectReference>CrossVersionObjectReference</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</a>)</p><p><p>CrossVersionObjectReference contains enough information to let you identify the referred resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>kind</code></br><em>string</em></td><td><p>Kind of the referent</p></td></tr><tr><td><code>name</code></br><em>string</em></td><td><p>Name of the referent</p></td></tr><tr><td><code>apiVersion</code></br><em>string</em></td><td><em>(Optional)</em><p>API version of the referent</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.Etcd>Etcd</h3><p><p>Etcd is the Schema for the etcds API</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a></em></td><td><br><br><table><tr><td><code>selector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><p>selector is a label query over pods that should match the replica count.
It must match the pod template’s labels.
More info: <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors>https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors</a></p></td></tr><tr><td><code>labels</code></br><em>map[string]string</em></td><td></td></tr><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em></td></tr><tr><td><code>etcd</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a></em></td><td></td></tr><tr><td><code>backup</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a></em></td><td></td></tr><tr><td><code>sharedConfig</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SharedConfig>SharedConfig</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>schedulingConstraints</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SchedulingConstraints>SchedulingConstraints</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>replicas</code></br><em>int32</em></td><td></td></tr><tr><td><code>priorityClassName</code></br><em>string</em></td><td><em>(Optional)</em><p>PriorityClassName is the name of a priority class that shall be used for the etcd pods.</p></td></tr><tr><td><code>storageClass</code></br><em>string</em></td><td><em>(Optional)</em><p>StorageClass defines the name of the StorageClass required by the claim.
More info: <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1>https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1</a></p></td></tr><tr><td><code>storageCapacity</code></br><em>k8s.io/apimachinery/pkg/api/resource.Quantity</em></td><td><em>(Optional)</em><p>StorageCapacity defines the size of persistent volume.</p></td></tr><tr><td><code>volumeClaimTemplate</code></br><em>string</em></td><td><em>(Optional)</em><p>VolumeClaimTemplate defines the volume claim template to be created</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</a></em></td><td></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a>)</p><p><p>EtcdConfig defines parameters associated etcd deployed</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>quota</code></br><em>k8s.io/apimachinery/pkg/api/resource.Quantity</em></td><td><em>(Optional)</em><p>Quota defines the etcd DB quota.</p></td></tr><tr><td><code>defragmentationSchedule</code></br><em>string</em></td><td><em>(Optional)</em><p>DefragmentationSchedule defines the cron standard schedule for defragmentation of etcd.</p></td></tr><tr><td><code>serverPort</code></br><em>int32</em></td><td><em>(Optional)</em></td></tr><tr><td><code>clientPort</code></br><em>int32</em></td><td><em>(Optional)</em></td></tr><tr><td><code>image</code></br><em>string</em></td><td><em>(Optional)</em><p>Image defines the etcd container image and tag</p></td></tr><tr><td><code>authSecretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>metrics</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.MetricsLevel>MetricsLevel</a></em></td><td><em>(Optional)</em><p>Metrics defines the level of detail for exported metrics of etcd, specify ‘extensive’ to include histogram metrics.</p></td></tr><tr><td><code>resources</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core>Kubernetes core/v1.ResourceRequirements</a></em></td><td><em>(Optional)</em><p>Resources defines the compute Resources required by etcd container.
More info: <a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/</a></p></td></tr><tr><td><code>clientUrlTls</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</a></em></td><td><em>(Optional)</em><p>ClientUrlTLS contains the ca, server TLS and client TLS secrets for client communication to ETCD cluster</p></td></tr><tr><td><code>peerUrlTls</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</a></em></td><td><em>(Optional)</em><p>PeerUrlTLS contains the ca and server TLS secrets for peer communication within ETCD cluster
Currently, PeerUrlTLS does not require client TLS secrets for gardener implementation of ETCD cluster.</p></td></tr><tr><td><code>etcdDefragTimeout</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>EtcdDefragTimeout defines the timeout duration for etcd defrag call</p></td></tr><tr><td><code>heartbeatDuration</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>HeartbeatDuration defines the duration for members to send heartbeats. The default value is 10s.</p></td></tr><tr><td><code>clientService</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.ClientService>ClientService</a></em></td><td><em>(Optional)</em><p>ClientService defines the parameters of the client service that a user can specify</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask>EtcdCopyBackupsTask</h3><p><p>EtcdCopyBackupsTask is a task for copying etcd backups from a source to a target store.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec>EtcdCopyBackupsTaskSpec</a></em></td><td><br><br><table><tr><td><code>sourceStore</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><p>SourceStore defines the specification of the source object store provider for storing backups.</p></td></tr><tr><td><code>targetStore</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><p>TargetStore defines the specification of the target object store provider for storing backups.</p></td></tr><tr><td><code>maxBackupAge</code></br><em>uint32</em></td><td><em>(Optional)</em><p>MaxBackupAge is the maximum age in days that a backup must have in order to be copied.
By default all backups will be copied.</p></td></tr><tr><td><code>maxBackups</code></br><em>uint32</em></td><td><em>(Optional)</em><p>MaxBackups is the maximum number of backups that will be copied starting with the most recent ones.</p></td></tr><tr><td><code>waitForFinalSnapshot</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec>WaitForFinalSnapshotSpec</a></em></td><td><em>(Optional)</em><p>WaitForFinalSnapshot defines the parameters for waiting for a final full snapshot before copying backups.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus>EtcdCopyBackupsTaskStatus</a></em></td><td></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec>EtcdCopyBackupsTaskSpec</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask>EtcdCopyBackupsTask</a>)</p><p><p>EtcdCopyBackupsTaskSpec defines the parameters for the copy backups task.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>sourceStore</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><p>SourceStore defines the specification of the source object store provider for storing backups.</p></td></tr><tr><td><code>targetStore</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><p>TargetStore defines the specification of the target object store provider for storing backups.</p></td></tr><tr><td><code>maxBackupAge</code></br><em>uint32</em></td><td><em>(Optional)</em><p>MaxBackupAge is the maximum age in days that a backup must have in order to be copied.
By default all backups will be copied.</p></td></tr><tr><td><code>maxBackups</code></br><em>uint32</em></td><td><em>(Optional)</em><p>MaxBackups is the maximum number of backups that will be copied starting with the most recent ones.</p></td></tr><tr><td><code>waitForFinalSnapshot</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec>WaitForFinalSnapshotSpec</a></em></td><td><em>(Optional)</em><p>WaitForFinalSnapshot defines the parameters for waiting for a final full snapshot before copying backups.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus>EtcdCopyBackupsTaskStatus</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask>EtcdCopyBackupsTask</a>)</p><p><p>EtcdCopyBackupsTaskStatus defines the observed state of the copy backups task.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition>[]Condition</a></em></td><td><em>(Optional)</em><p>Conditions represents the latest available observations of an object’s current state.</p></td></tr><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed for this resource.</p></td></tr><tr><td><code>lastError</code></br><em>string</em></td><td><em>(Optional)</em><p>LastError represents the last occurred error.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdMemberConditionStatus>EtcdMemberConditionStatus
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberStatus>EtcdMemberStatus</a>)</p><p><p>EtcdMemberConditionStatus is the status of an etcd cluster member.</p></p><h3 id=druid.gardener.cloud/v1alpha1.EtcdMemberStatus>EtcdMemberStatus</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</a>)</p><p><p>EtcdMemberStatus holds information about a etcd cluster membership.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the etcd member. It is the name of the backing <code>Pod</code>.</p></td></tr><tr><td><code>id</code></br><em>string</em></td><td><em>(Optional)</em><p>ID is the ID of the etcd member.</p></td></tr><tr><td><code>role</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdRole>EtcdRole</a></em></td><td><em>(Optional)</em><p>Role is the role in the etcd cluster, either <code>Leader</code> or <code>Member</code>.</p></td></tr><tr><td><code>status</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberConditionStatus>EtcdMemberConditionStatus</a></em></td><td><p>Status of the condition, one of True, False, Unknown.</p></td></tr><tr><td><code>reason</code></br><em>string</em></td><td><p>The reason for the condition’s last transition.</p></td></tr><tr><td><code>lastTransitionTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>LastTransitionTime is the last time the condition’s status changed.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdRole>EtcdRole
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberStatus>EtcdMemberStatus</a>)</p><p><p>EtcdRole is the role of an etcd cluster member.</p></p><h3 id=druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Etcd>Etcd</a>)</p><p><p>EtcdSpec defines the desired state of Etcd</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>selector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><p>selector is a label query over pods that should match the replica count.
It must match the pod template’s labels.
More info: <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors>https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors</a></p></td></tr><tr><td><code>labels</code></br><em>map[string]string</em></td><td></td></tr><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em></td></tr><tr><td><code>etcd</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a></em></td><td></td></tr><tr><td><code>backup</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a></em></td><td></td></tr><tr><td><code>sharedConfig</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SharedConfig>SharedConfig</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>schedulingConstraints</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SchedulingConstraints>SchedulingConstraints</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>replicas</code></br><em>int32</em></td><td></td></tr><tr><td><code>priorityClassName</code></br><em>string</em></td><td><em>(Optional)</em><p>PriorityClassName is the name of a priority class that shall be used for the etcd pods.</p></td></tr><tr><td><code>storageClass</code></br><em>string</em></td><td><em>(Optional)</em><p>StorageClass defines the name of the StorageClass required by the claim.
More info: <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1>https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1</a></p></td></tr><tr><td><code>storageCapacity</code></br><em>k8s.io/apimachinery/pkg/api/resource.Quantity</em></td><td><em>(Optional)</em><p>StorageCapacity defines the size of persistent volume.</p></td></tr><tr><td><code>volumeClaimTemplate</code></br><em>string</em></td><td><em>(Optional)</em><p>VolumeClaimTemplate defines the volume claim template to be created</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Etcd>Etcd</a>)</p><p><p>EtcdStatus defines the observed state of Etcd.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed for this resource.</p></td></tr><tr><td><code>etcd</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CrossVersionObjectReference>CrossVersionObjectReference</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>conditions</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition>[]Condition</a></em></td><td><em>(Optional)</em><p>Conditions represents the latest available observations of an etcd’s current state.</p></td></tr><tr><td><code>serviceName</code></br><em>string</em></td><td><em>(Optional)</em><p>ServiceName is the name of the etcd service.</p></td></tr><tr><td><code>lastError</code></br><em>string</em></td><td><em>(Optional)</em><p>LastError represents the last occurred error.</p></td></tr><tr><td><code>clusterSize</code></br><em>int32</em></td><td><em>(Optional)</em><p>Cluster size is the size of the etcd cluster.</p></td></tr><tr><td><code>currentReplicas</code></br><em>int32</em></td><td><em>(Optional)</em><p>CurrentReplicas is the current replica count for the etcd cluster.</p></td></tr><tr><td><code>replicas</code></br><em>int32</em></td><td><em>(Optional)</em><p>Replicas is the replica count of the etcd resource.</p></td></tr><tr><td><code>readyReplicas</code></br><em>int32</em></td><td><em>(Optional)</em><p>ReadyReplicas is the count of replicas being ready in the etcd cluster.</p></td></tr><tr><td><code>ready</code></br><em>bool</em></td><td><em>(Optional)</em><p>Ready is <code>true</code> if all etcd replicas are ready.</p></td></tr><tr><td><code>updatedReplicas</code></br><em>int32</em></td><td><em>(Optional)</em><p>UpdatedReplicas is the count of updated replicas in the etcd cluster.</p></td></tr><tr><td><code>labelSelector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em><p>LabelSelector is a label query over pods that should match the replica count.
It must match the pod template’s labels.</p></td></tr><tr><td><code>members</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberStatus>[]EtcdMemberStatus</a></em></td><td><em>(Optional)</em><p>Members represents the members of the etcd cluster</p></td></tr><tr><td><code>peerUrlTLSEnabled</code></br><em>bool</em></td><td><em>(Optional)</em><p>PeerUrlTLSEnabled captures the state of peer url TLS being enabled for the etcd member(s)</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.GarbageCollectionPolicy>GarbageCollectionPolicy
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>)</p><p><p>GarbageCollectionPolicy defines the type of policy for snapshot garbage collection.</p></p><h3 id=druid.gardener.cloud/v1alpha1.LeaderElectionSpec>LeaderElectionSpec</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>)</p><p><p>LeaderElectionSpec defines parameters related to the LeaderElection configuration.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>reelectionPeriod</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>ReelectionPeriod defines the Period after which leadership status of corresponding etcd is checked.</p></td></tr><tr><td><code>etcdConnectionTimeout</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>EtcdConnectionTimeout defines the timeout duration for etcd client connection during leader election.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.MetricsLevel>MetricsLevel
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a>)</p><p><p>MetricsLevel defines the level ‘basic’ or ‘extensive’.</p></p><h3 id=druid.gardener.cloud/v1alpha1.SchedulingConstraints>SchedulingConstraints</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a>)</p><p><p>SchedulingConstraints defines the different scheduling constraints that must be applied to the
pod spec in the etcd statefulset.
Currently supported constraints are Affinity and TopologySpreadConstraints.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>affinity</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#affinity-v1-core>Kubernetes core/v1.Affinity</a></em></td><td><em>(Optional)</em><p>Affinity defines the various affinity and anti-affinity rules for a pod
that are honoured by the kube-scheduler.</p></td></tr><tr><td><code>topologySpreadConstraints</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#topologyspreadconstraint-v1-core>[]Kubernetes core/v1.TopologySpreadConstraint</a></em></td><td><em>(Optional)</em><p>TopologySpreadConstraints describes how a group of pods ought to spread across topology domains,
that are honoured by the kube-scheduler.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.SecretReference>SecretReference</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</a>)</p><p><p>SecretReference defines a reference to a secret.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>SecretReference</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>(Members of <code>SecretReference</code> are embedded into this type.)</p></td></tr><tr><td><code>dataKey</code></br><em>string</em></td><td><em>(Optional)</em><p>DataKey is the name of the key in the data map containing the credentials.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.SharedConfig>SharedConfig</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a>)</p><p><p>SharedConfig defines parameters shared and used by Etcd as well as backup-restore sidecar.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>autoCompactionMode</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompactionMode>CompactionMode</a></em></td><td><em>(Optional)</em><p>AutoCompactionMode defines the auto-compaction-mode:‘periodic’ mode or ‘revision’ mode for etcd and embedded-Etcd of backup-restore sidecar.</p></td></tr><tr><td><code>autoCompactionRetention</code></br><em>string</em></td><td><em>(Optional)</em><p>AutoCompactionRetention defines the auto-compaction-retention length for etcd as well as for embedded-Etcd of backup-restore sidecar.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.StorageProvider>StorageProvider
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a>)</p><p><p>StorageProvider defines the type of object store provider for storing backups.</p></p><h3 id=druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>,
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec>EtcdCopyBackupsTaskSpec</a>)</p><p><p>StoreSpec defines parameters related to ObjectStore persisting backups</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>container</code></br><em>string</em></td><td><em>(Optional)</em><p>Container is the name of the container the backup is stored at.</p></td></tr><tr><td><code>prefix</code></br><em>string</em></td><td><p>Prefix is the prefix used for the store.</p></td></tr><tr><td><code>provider</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StorageProvider>StorageProvider</a></em></td><td><em>(Optional)</em><p>Provider is the name of the backup provider.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><em>(Optional)</em><p>SecretRef is the reference to the secret which used to connect to the backup store.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>,
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a>)</p><p><p>TLSConfig hold the TLS configuration details.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>tlsCASecretRef</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SecretReference>SecretReference</a></em></td><td></td></tr><tr><td><code>serverTLSSecretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>clientTLSSecretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec>WaitForFinalSnapshotSpec</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec>EtcdCopyBackupsTaskSpec</a>)</p><p><p>WaitForFinalSnapshotSpec defines the parameters for waiting for a final full snapshot before copying backups.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><p>Enabled specifies whether to wait for a final full snapshot before copying backups.</p></td></tr><tr><td><code>timeout</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>Timeout is the timeout for waiting for a final full snapshot. When this timeout expires, the copying of backups
will be performed anyway. No timeout or 0 means wait forever.</p></td></tr></tbody></table><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-26b0cf1c7a98444327254b2dca32b345>3.2 - 01 Multi Node Etcd Clusters</h1><h1 id=dep-01-multi-node-etcd-cluster-instances-via-etcd-druid>DEP-01: Multi-node etcd cluster instances via etcd-druid</h1><p>This document proposes an approach (along with some alternatives) to support provisioning and management of multi-node etcd cluster instances via <a href=https://github.com/gardener/etcd-druid>etcd-druid</a> and <a href=https://github.com/gardener/etcd-backup-restore>etcd-backup-restore</a>.</p><h2 id=goal>Goal</h2><ul><li>Enhance etcd-druid and etcd-backup-restore to support provisioning and management of multi-node etcd cluster instances within a single Kubernetes cluster.</li><li>The etcd CRD interface should be simple to use. It should preferably work with just setting the <code>spec.replicas</code> field to the desired value and should not require any more configuration in the CRD than currently required for the single-node etcd instances. The <code>spec.replicas</code> field is part of the <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource><code>scale</code> sub-resource</a> <a href=https://github.com/gardener/etcd-druid/blob/eaf04a2d0e6c7a4f2c8c220182b7a141aabfc70b/api/v1alpha1/etcd_types.go#L299>implementation</a> in <code>Etcd</code> CRD.</li><li>The single-node and multi-node scenarios must be automatically identified and managed by <code>etcd-druid</code> and <code>etcd-backup-restore</code>.</li><li>The etcd clusters (single-node or multi-node) managed by <code>etcd-druid</code> and <code>etcd-backup-restore</code> must automatically recover from failures (even quorum loss) and disaster (e.g. etcd member persistence/data loss) as much as possible.</li><li>It must be possible to dynamically scale an etcd cluster horizontally (even between single-node and multi-node scenarios) by simply scaling the <code>Etcd</code> scale sub-resource.</li><li>It must be possible to (optionally) schedule the individual members of an etcd clusters on different nodes or even infrastructure availability zones (within the hosting Kubernetes cluster).</li></ul><p>Though this proposal tries to cover most aspects related to single-node and multi-node etcd clusters, there are some more points that are not goals for this document but are still in the scope of either etcd-druid/etcd-backup-restore and/or gardener.
In such cases, a high-level description of how they can be <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#future-work>addressed in the future</a> are mentioned at the end of the document.</p><h2 id=background-and-motivation>Background and Motivation</h2><h3 id=single-node-etcd-cluster>Single-node etcd cluster</h3><p>At present, <code>etcd-druid</code> supports only single-node etcd cluster instances.
The advantages of this approach are given below.</p><ul><li>The problem domain is smaller.
There are no leader election and quorum related issues to be handled.
It is simpler to setup and manage a single-node etcd cluster.</li><li>Single-node etcd clusters instances have <a href=https://etcd.io/docs/v2/admin_guide/#optimal-cluster-size>less request latency</a> than multi-node etcd clusters because there is no requirement to replicate the changes to the other members before committing the changes.</li><li><code>etcd-druid</code> provisions etcd cluster instances as pods (actually as <code>statefulsets</code>) in a Kubernetes cluster and Kubernetes is quick (&lt;<code>20s</code>) to restart container/pods if they go down.</li><li>Also, <code>etcd-druid</code> is currently only used by gardener to provision etcd clusters to act as back-ends for Kubernetes control-planes and Kubernetes control-plane components (<code>kube-apiserver</code>, <code>kubelet</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code> etc.) can tolerate etcd going down and recover when it comes back up.</li><li>Single-node etcd clusters incur less cost (CPU, memory and storage)</li><li>It is easy to cut-off client requests if backups fail by using <a href=https://github.com/gardener/etcd-druid/blob/eaf04a2d0e6c7a4f2c8c220182b7a141aabfc70b/charts/etcd/templates/etcd-statefulset.yaml#L54-L62><code>readinessProbe</code> on the <code>etcd-backup-restore</code> healthz endpoint</a> to minimize the gap between the latest revision and the backup revision.</li></ul><p>The disadvantages of using single-node etcd clusters are given below.</p><ul><li>The <a href=https://github.com/gardener/etcd-backup-restore/blob/master/docs/proposals/design.md#workflow>database verification</a> step by <code>etcd-backup-restore</code> can introduce additional delays whenever etcd container/pod restarts (in total ~<code>20-25s</code>).
This can be much longer if a database restoration is required.
Especially, if there are incremental snapshots that need to be replayed (this can be mitigated by <a href=https://github.com/gardener/etcd-druid/issues/88>compacting the incremental snapshots in the background</a>).</li><li>Kubernetes control-plane components can go into <code>CrashloopBackoff</code> if etcd is down for some time. This is mitigated by the <a href=https://github.com/gardener/gardener/blob/9e4a809008fb122a6d02045adc08b9c98b5cd564/charts/seed-bootstrap/charts/dependency-watchdog/templates/endpoint-configmap.yaml#L29-L41>dependency-watchdog</a>.
But Kubernetes control-plane components require a lot of resources and create a lot of load on the etcd cluster and the apiserver when they come out of <code>CrashloopBackoff</code>.
Especially, in medium or large sized clusters (> <code>20</code> nodes).</li><li>Maintenance operations such as updates to etcd (and updates to <code>etcd-druid</code> of <code>etcd-backup-restore</code>), rolling updates to the nodes of the underlying Kubernetes cluster and vertical scaling of etcd pods are disruptive because they cause etcd pods to be restarted.
The vertical scaling of etcd pods is somewhat mitigated during scale down by doing it only during the target clusters&rsquo; <a href=https://github.com/gardener/gardener/blob/86aa30dfd095f7960ae50a81d2cee27c0d18408b/charts/seed-controlplane/charts/etcd/templates/etcd-hvpa.yaml#L53>maintenance window</a>.
But scale up is still disruptive.</li><li>We currently use some form of elastic storage (via <code>persistentvolumeclaims</code>) for storing which have some upper-bounds on the I/O latency and throughput. This can be potentially be a problem for large clusters (> <code>220</code> nodes).
Also, some cloud providers (e.g. Azure) take a long time to attach/detach volumes to and from machines which increases the down time to the Kubernetes components that depend on etcd.
It is difficult to use ephemeral/local storage (to achieve better latency/throughput as well as to circumvent volume attachment/detachment) for single-node etcd cluster instances.</li></ul><h3 id=multi-node-etcd-cluster>Multi-node etcd-cluster</h3><p>The advantages of introducing support for multi-node etcd clusters via <code>etcd-druid</code> are below.</p><ul><li>Multi-node etcd cluster is highly-available. It can tolerate disruption to individual etcd pods as long as the quorum is not lost (i.e. more than half the etcd member pods are healthy and ready).</li><li>Maintenance operations such as updates to etcd (and updates to <code>etcd-druid</code> of <code>etcd-backup-restore</code>), rolling updates to the nodes of the underlying Kubernetes cluster and vertical scaling of etcd pods can be done non-disruptively by <a href=https://kubernetes.io/docs/concepts/workloads/pods/disruptions/>respecting <code>poddisruptionbudgets</code></a> for the various multi-node etcd cluster instances hosted on that cluster.</li><li>Kubernetes control-plane components do not see any etcd cluster downtime unless quorum is lost (which is expected to be lot less frequent than current frequency of etcd container/pod restarts).</li><li>We can consider using ephemeral/local storage for multi-node etcd cluster instances because individual member restarts can afford to take time to restore from backup before (re)joining the etcd cluster because the remaining members serve the requests in the meantime.</li><li>High-availability across availability zones is also possible by specifying (anti)affinity for the etcd pods (possibly via <a href=https://github.com/gardener/kupid><code>kupid</code></a>).</li></ul><p>Some disadvantages of using multi-node etcd clusters due to which it might still be desirable, in some cases, to continue to use single-node etcd cluster instances in the gardener context are given below.</p><ul><li>Multi-node etcd cluster instances are more complex to manage.
The problem domain is larger including the following.<ul><li>Leader election</li><li>Quorum loss</li><li>Managing rolling changes</li><li>Backups to be taken from only the leading member.</li><li>More complex to cut-off client requests if backups fail to minimize the gap between the latest revision and the backup revision is under control.</li></ul></li><li>Multi-node etcd cluster instances incur more cost (CPU, memory and storage).</li></ul><h3 id=dynamic-multi-node-etcd-cluster>Dynamic multi-node etcd cluster</h3><p>Though it is <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#non-goal>not part of this proposal</a>, it is conceivable to convert a single-node etcd cluster into a multi-node etcd cluster temporarily to perform some disruptive operation (etcd, <code>etcd-backup-restore</code> or <code>etcd-druid</code> updates, etcd cluster vertical scaling and perhaps even node rollout) and convert it back to a single-node etcd cluster once the disruptive operation has been completed. This will necessarily still involve a down-time because scaling from a single-node etcd cluster to a three-node etcd cluster will involve etcd pod restarts, it is still probable that it can be managed with a shorter down time than we see at present for single-node etcd clusters (on the other hand, converting a three-node etcd cluster to five node etcd cluster can be non-disruptive).</p><p>This is <em>definitely not</em> to argue in favour of such a dynamic approach in all cases (eventually, if/when dynamic multi-node etcd clusters are supported). On the contrary, it makes sense to make use of <em>static</em> (fixed in size) multi-node etcd clusters for production scenarios because of the high-availability.</p><h2 id=prior-art>Prior Art</h2><h3 id=etcd-operator-from-coreos>ETCD Operator from CoreOS</h3><blockquote><p><a href=https://github.com/coreos/etcd-operator#etcd-operator>etcd operator</a></p><p><a href=https://github.com/coreos/etcd-operator#project-status-archived>Project status: archived</a></p><p>This project is no longer actively developed or maintained. The project exists here for historical reference. If you are interested in the future of the project and taking over stewardship, please contact <a href=mailto:etcd-dev@googlegroups.com>etcd-dev@googlegroups.com</a>.</p></blockquote><h3 id=etcdadm-from-kubernetes-sigs>etcdadm from kubernetes-sigs</h3><blockquote><p><a href=https://github.com/kubernetes-sigs/etcdadm#etcdadm>etcdadm</a> is a command-line tool for operating an etcd cluster. It makes it easy to create a new cluster, add a member to, or remove a member from an existing cluster. Its user experience is inspired by kubeadm.</p></blockquote><p>It is a tool more tailored for manual command-line based management of etcd clusters with no API&rsquo;s.
It also makes no assumptions about the underlying platform on which the etcd clusters are provisioned and hence, doesn&rsquo;t leverage any capabilities of Kubernetes.</p><h3 id=etcd-cluster-operator-from-improbable-engineering>Etcd Cluster Operator from Improbable-Engineering</h3><blockquote><p><a href=https://github.com/improbable-eng/etcd-cluster-operator>Etcd Cluster Operator</a></p><p>Etcd Cluster Operator is an Operator for automating the creation and management of etcd inside of Kubernetes. It provides a custom resource definition (CRD) based API to define etcd clusters with Kubernetes resources, and enable management with native Kubernetes tooling._</p></blockquote><p>Out of all the alternatives listed here, this one seems to be the only possible viable alternative.
Parts of its design/implementations are similar to some of the approaches mentioned in this proposal. However, we still don&rsquo;t propose to use it as -</p><ol><li>The project is still in early phase and is not mature enough to be consumed as is in productive scenarios of ours.</li><li>The resotration part is completely different which makes it difficult to adopt as-is and requries lot of re-work with the current restoration semantics with etcd-backup-restore making the usage counter-productive.</li></ol><h2 id=general-approach-to-etcd-cluster-management>General Approach to ETCD Cluster Management</h2><h3 id=bootstrapping>Bootstrapping</h3><p>There are three ways to bootstrap an etcd cluster which are <a href=https://etcd.io/docs/v3.4.0/op-guide/clustering/#static>static</a>, <a href=https://etcd.io/docs/v3.4.0/op-guide/clustering/#etcd-discovery>etcd discovery</a> and <a href=https://etcd.io/docs/v3.4.0/op-guide/clustering/#dns-discovery>DNS discovery</a>.
Out of these, the static way is the simplest (and probably faster to bootstrap the cluster) and has the least external dependencies.
Hence, it is preferred in this proposal.
But it requires that the initial (during bootstrapping) etcd cluster size (number of members) is already known before bootstrapping and that all of the members are already addressable (DNS,IP,TLS etc.).
Such information needs to be passed to the individual members during startup using the following static configuration.</p><ul><li>ETCD_INITIAL_CLUSTER<ul><li>The list of peer URLs including all the members. This must be the same as the advertised peer URLs configuration. This can also be passed as <code>initial-cluster</code> flag to etcd.</li></ul></li><li>ETCD_INITIAL_CLUSTER_STATE<ul><li>This should be set to <code>new</code> while bootstrapping an etcd cluster.</li></ul></li><li>ETCD_INITIAL_CLUSTER_TOKEN<ul><li>This is a token to distinguish the etcd cluster from any other etcd cluster in the same network.</li></ul></li></ul><h4 id=assumptions>Assumptions</h4><ul><li>ETCD_INITIAL_CLUSTER can use DNS instead of IP addresses. We need to verify this by deleting a pod (as against scaling down the statefulset) to ensure that the pod IP changes and see if the recreated pod (by the statefulset controller) re-joins the cluster automatically.</li><li>DNS for the individual members is known or computable. This is true in the case of etcd-druid setting up an etcd cluster using a single statefulset. But it may not necessarily be true in other cases (multiple statefulset per etcd cluster or deployments instead of statefulsets or in the case of etcd cluster with members distributed across more than one Kubernetes cluster.</li></ul><h3 id=adding-a-new-member-to-an-etcd-cluster>Adding a new member to an etcd cluster</h3><p>A <a href=https://etcd.io/docs/v3.4.0/op-guide/runtime-configuration/#add-a-new-member>new member can be added</a> to an existing etcd cluster instance using the following steps.</p><ol><li>If the latest backup snapshot exists, restore the member&rsquo;s etcd data to the latest backup snapshot. This can reduce the load on the leader to bring the new member up to date when it joins the cluster.<ol><li>If the latest backup snapshot doesn&rsquo;t exist or if the latest backup snapshot is not accessible (please see <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure>backup failure</a>) and if the cluster itself is quorate, then the new member can be started with an empty data. But this will will be suboptimal because the new member will fetch all the data from the leading member to get up-to-date.</li></ol></li><li>The cluster is informed that a new member is being added using the <a href=https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L40><code>MemberAdd</code> API</a> including information like the member name and its advertised peer URLs.</li><li>The new etcd member is then started with <code>ETCD_INITIAL_CLUSTER_STATE=existing</code> apart from other required configuration.</li></ol><p>This proposal recommends this approach.</p><h4 id=note>Note</h4><ul><li>If there are incremental snapshots (taken by <code>etcd-backup-restore</code>), they cannot be applied because that requires the member to be started in isolation without joining the cluster which is not possible.
This is acceptable if the amount of incremental snapshots are managed to be relatively small.
This adds one more reason to increase the priority of the issue of <a href=https://github.com/gardener/etcd-druid/issues/88>incremental snapshot compaction</a>.</li><li>There is a time window, between the <code>MemberAdd</code> call and the new member joining the cluster and getting up to date, where the cluster is <a href=https://etcd.io/docs/v3.3.12/learning/learner/#background>vulnerable to leader elections which could be disruptive</a>.</li></ul><h4 id=alternative>Alternative</h4><p>With <code>v3.4</code>, the new <a href=https://etcd.io/docs/v3.3.12/learning/learner/#raft-learner>raft learner approach</a> can be used to mitigate some of the possible disruptions mentioned <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note>above</a>.
Then the steps will be as follows.</p><ol><li>If the latest backup snapshot exists, restore the member&rsquo;s etcd data to the latest backup snapshot. This can reduce the load on the leader to bring the new member up to date when it joins the cluster.</li><li>The cluster is informed that a new member is being added using the <a href=https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L43><code>MemberAddAsLearner</code> API</a> including information like the member name and its advertised peer URLs.</li><li>The new etcd member is then started with <code>ETCD_INITIAL_CLUSTER_STATE=existing</code> apart from other required configuration.</li><li>Once the new member (learner) is up to date, it can be promoted to a full voting member by using the <a href=https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L52><code>MemberPromote</code> API</a></li></ol><p>This approach is new and involves more steps and is not recommended in this proposal.
It can be considered in future enhancements.</p><h3 id=managing-failures>Managing Failures</h3><p>A multi-node etcd cluster may face failures of <a href=https://etcd.io/docs/v3.1.12/op-guide/failures/>diffent kinds</a> during its life-cycle.
The actions that need to be taken to manage these failures depend on the failure mode.</p><h4 id=removing-an-existing-member-from-an-etcd-cluster>Removing an existing member from an etcd cluster</h4><p>If a member of an etcd cluster becomes unhealthy, it must be explicitly removed from the etcd cluster, as soon as possible.
This can be done by using the <a href=https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L46><code>MemberRemove</code> API</a>.
This ensures that only healthy members participate as voting members.</p><p>A member of an etcd cluster may be removed not just for managing failures but also for other reasons such as -</p><ul><li>The etcd cluster is being scaled down. I.e. the cluster size is being reduced</li><li>An existing member is being replaced by a new one for some reason (e.g. upgrades)</li></ul><p>If the majority of the members of the etcd cluster are healthy and the member that is unhealthy/being removed happens to be the <a href=https://etcd.io/docs/v3.1.12/op-guide/failures/#leader-failure>leader</a> at that moment then the etcd cluster will automatically elect a new leader.
But if only a minority of etcd clusters are healthy after removing the member then the the cluster will no longer be <a href=https://etcd.io/docs/v3.1.12/op-guide/failures/#majority-failure>quorate</a> and will stop accepting write requests.
Such an etcd cluster needs to be recovered via some kind of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>disaster-recovery</a>.</p><h4 id=restarting-an-existing-member-of-an-etcd-cluster>Restarting an existing member of an etcd cluster</h4><p>If the existing member of an etcd cluster restarts and retains an uncorrupted data directory after the restart, then it can simply re-join the cluster as an existing member without any API calls or configuration changes.
This is because the relevant metadata (including member ID and cluster ID) are <a href=https://etcd.io/docs/v2/admin_guide/#lifecycle>maintained in the write ahead logs</a>.
However, if it doesn&rsquo;t retain an uncorrupted data directory after the restart, then it must first be <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster>removed</a> and <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>added</a> as a new member.</p><h4 id=recovering-an-etcd-cluster-from-failure-of-majority-of-members>Recovering an etcd cluster from failure of majority of members</h4><p>If a majority of members of an etcd cluster fail but if they retain their uncorrupted data directory then they can be simply restarted and they will re-form the existing etcd cluster when they come up.
However, if they do not retain their uncorrupted data directory, then the etcd cluster must be <a href=https://etcd.io/docs/v3.4.0/op-guide/recovery/#restoring-a-cluster>recovered from latest snapshot in the backup</a>.
This is very similar to <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping>bootstrapping</a> with the additional initial step of restoring the latest snapshot in each of the members.
However, the same <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note>limitation</a> about incremental snapshots, as in the case of adding a new member, applies here.
But unlike in the case of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>adding a new member</a>, not applying incremental snapshots is not acceptable in the case of etcd cluster recovery.
Hence, if incremental snapshots are required to be applied, the etcd cluster must be <a href=https://etcd.io/docs/v3.4.0/op-guide/runtime-configuration/#restart-cluster-from-majority-failure>recovered</a> in the following steps.</p><ol><li>Restore a new single-member cluster using the latest snapshot.</li><li>Apply incremental snapshots on the single-member cluster.</li><li>Take a full snapshot which can now be used while adding the remaining members.</li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>Add</a> new members using the latest snapshot created in the step above.</li></ol><h2 id=kubernetes-context>Kubernetes Context</h2><ul><li>Users will provision an etcd cluster in a Kubernetes cluster by creating an etcd CRD resource instance.</li><li>A multi-node etcd cluster is indicated if the <code>spec.replicas</code> field is set to any value greater than 1. The etcd-druid will add validation to ensure that the <code>spec.replicas</code> value is an odd number according to the requirements of etcd.</li><li>The etcd-druid controller will provision a statefulset with the etcd main container and the etcd-backup-restore sidecar container. It will pass on the <code>spec.replicas</code> field from the etcd resource to the statefulset. It will also supply the right pre-computed configuration to both the containers.</li><li>The statefulset controller will create the pods based on the pod template in the statefulset spec and these individual pods will be the members that form the etcd cluster.</li></ul><p><img src=/__resources/01-multi-node-etcd_1afcbd.png alt="Component diagram"></p><p>This approach makes it possible to satisfy the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#assumption>assumption</a> that the DNS for the individual members of the etcd cluster must be known/computable.
This can be achieved by using a <code>headless</code> service (along with the statefulset) for each etcd cluster instance.
Then we can address individual pods/etcd members via the predictable DNS name of <code>&lt;statefulset_name>-{0|1|2|3|…|n}.&lt;headless_service_name></code> from within the Kubernetes namespace (or from outside the Kubernetes namespace by appending <code>.&lt;namespace>.svc.&lt;cluster_domain> suffix)</code>.
The etcd-druid controller can compute the above configurations automatically based on the <code>spec.replicas</code> in the etcd resource.</p><p>This proposal recommends this approach.</p><h4 id=alternative-1>Alternative</h4><p>One statefulset is used for each member (instead of one statefulset for all members).
While this approach gives a flexibility to have different pod specifications for the individual members, it makes managing the individual members (e.g. rolling updates) more complicated.
Hence, this approach is not recommended.</p><h2 id=etcd-configuration>ETCD Configuration</h2><p>As mentioned in the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#general-approach-to-etcd-cluster-management>general approach section</a>, there are differences in the configuration that needs to be passed to individual members of an etcd cluster in different scenarios such as <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping>bootstrapping</a>, <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>adding</a> a new member, <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster>removing</a> a member, <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>restarting</a> an existing member etc.
Managing such differences in configuration for individual pods of a statefulset is tricky in the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#kubernetes-context>recommended approach</a> of using a single statefulset to manage all the member pods of an etcd cluster.
This is because statefulset uses the same pod template for all its pods.</p><p>The recommendation is for <code>etcd-druid</code> to provision the base configuration template in a <code>ConfigMap</code> which is passed to all the pods via the pod template in the <code>StatefulSet</code>.
The <code>initialization</code> flow of <code>etcd-backup-restore</code> (which is invoked every time the etcd container is (re)started) is then enhanced to generate the customized etcd configuration for the corresponding member pod (in a shared <em>volume</em> between etcd and the backup-restore containers) based on the supplied template configuration.
This will require that <code>etcd-backup-restore</code> will have to have a mechanism to detect which scenario listed <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-configuration>above</a> applies during any given member container/pod restart.</p><h3 id=alternative-2>Alternative</h3><p>As mentioned <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-1>above</a>, one statefulset is used for each member of the etcd cluster.
Then different configuration (generated directly by <code>etcd-druid</code>) can be passed in the pod templates of the different statefulsets.
Though this approach is advantageous in the context of managing the different configuration, it is not recommended in this proposal because it makes the rest of the management (e.g. rolling updates) more complicated.</p><h2 id=data-persistence>Data Persistence</h2><p>The type of persistence used to store etcd data (including the member ID and cluster ID) has an impact on the steps that are needed to be taken when the member pods or containers (<a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster>minority</a> of them or <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>majority</a>) need to be recovered.</p><h3 id=persistent>Persistent</h3><p>Like the single-node case, <code>persistentvolumes</code> can be used to persist ETCD data for all the member pods. The individual member pods then get their own <code>persistentvolumes</code>.
The advantage is that individual members retain their member ID across pod restarts and even pod deletion/recreation across Kubernetes nodes.
This means that member pods that crash (or are unhealthy) can be <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>restarted</a> automatically (by configuring <code>livenessProbe</code>) and they will re-join the etcd cluster using their existing member ID without any need for explicit etcd cluster management).</p><p>The disadvantages of this approach are as follows.</p><ul><li>The number of persistentvolumes increases linearly with the cluster size which is a cost-related concern.</li><li>Network-mounted persistentvolumes might eventually become a performance bottleneck under heavy load for a latency-sensitive component like ETCD.</li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#single-node-etcd-cluster>Volume attach/detach issues</a> when associated with etcd cluster instances cause downtimes to the target shoot clusters that are backed by those etcd cluster instances.</li></ul><h3 id=ephemeral>Ephemeral</h3><p>The ephemeral volumes use-case is considered as an optimization and may be planned as a follow-up action.</p><h4 id=disk>Disk</h4><p>Ephemeral persistence can be achieved in Kubernetes by using either <a href=https://kubernetes.io/docs/concepts/storage/volumes/#emptydir><code>emptyDir</code></a> volumes or <a href=https://kubernetes.io/docs/concepts/storage/volumes/#local><code>local</code> persistentvolumes</a> to persist ETCD data.
The advantages of this approach are as follows.</p><ul><li>Potentially faster disk I/O.</li><li>The number of persistent volumes does not increase linearly with the cluster size (at least not technically).</li><li>Issues related volume attachment/detachment can be avoided.</li></ul><p>The main disadvantage of using ephemeral persistence is that the individual members may retain their identity and data across container restarts but not across pod deletion/recreation across Kubernetes nodes. If the data is lost then on restart of the member pod, the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>older member (represented by the container) has to be removed and a new member has to be added</a>.</p><p>Using <code>emptyDir</code> ephemeral persistence has the disadvantage that the volume doesn&rsquo;t have its own identity.
So, if the member pod is recreated but scheduled on the same node as before then it will not retain the identity as the persistence is lost.
But it has the advantage that scheduling of pods is unencumbered especially during pod recreation as they are free to be scheduled anywhere.</p><p>Using <code>local</code> persistentvolumes has the advantage that the volume has its own indentity and hence, a recreated member pod will retain its identity if scheduled on the same node.
But it has the disadvantage of tying down the member pod to a node which is a problem if the node becomes unhealthy requiring etcd druid to take additional actions (such as deleting the local persistent volume).</p><p>Based on these constraints, if ephemeral persistence is opted for, it is recommended to use <code>emptyDir</code> ephemeral persistence.</p><h4 id=in-memory>In-memory</h4><p>In-memory ephemeral persistence can be achieved in Kubernetes by using <code>emptyDir</code> with <a href=https://kubernetes.io/docs/concepts/storage/volumes/#emptydir><code>medium: Memory</code></a>.
In this case, a <code>tmpfs</code> (RAM-backed file-system) volume will be used.
In addition to the advantages of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral persistence</a>, this approach can achieve the fastest possible <em>disk I/O</em>.
Similarly, in addition to the disadvantages of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral persistence</a>, in-memory persistence has the following additional disadvantages.</p><ul><li>More memory required for the individual member pods.</li><li>Individual members may not at all retain their data and identity across container restarts let alone across pod restarts/deletion/recreation across Kubernetes nodes.
I.e. every time an etcd container restarts, <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>the old member (represented by the container) will have to be removed and a new member has to be added</a>.</li></ul><h3 id=how-to-detect-if-valid-metadata-exists-in-an-etcd-member>How to detect if valid metadata exists in an etcd member</h3><p>Since the likelyhood of a member not having valid metadata in the WAL files is much more likely in the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral</a> persistence scenario, one option is to pass the information that ephemeral persistence is being used to the <code>etcd-backup-restore</code> sidecar (say, via command-line flags or environment variables).</p><p>But in principle, it might be better to determine this from the WAL files directly so that the possibility of corrupted WAL files also gets handled correctly.
To do this, the <a href=https://github.com/etcd-io/etcd/tree/main/server/storage/wal>wal</a> package has <a href=https://github.com/etcd-io/etcd/blob/57a092b45d0eae6c9e600e62513ffcd2f1f25a92/server/wal/wal.go#L324-L326>some</a> <a href=https://github.com/etcd-io/etcd/blob/57a092b45d0eae6c9e600e62513ffcd2f1f25a92/server/wal/wal.go#L429-L548>functions</a> that might be useful.</p><h4 id=recommendation>Recommendation</h4><p>It might be possible that using the <a href=https://github.com/etcd-io/etcd/tree/main/server/storage/wal>wal</a> package for verifying if valid metadata exists might be performance intensive.
So, the performance impact needs to be measured.
If the performance impact is acceptable (both in terms of resource usage and time), it is recommended to use this way to verify if the member contains valid metadata.
Otherwise, alternatives such as a simple check that WAL folder exists coupled with the static information about use of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent>persistent</a> or <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral</a> storage might be considered.</p><h3 id=how-to-detect-if-valid-data-exists-in-an-etcd-member>How to detect if valid data exists in an etcd member</h3><p>The <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#decision-table-for-etcd-backup-restore-during-initialization>initialization sequence</a> in <code>etcd-backup-restore</code> already includes <a href=https://github.com/gardener/etcd-backup-restore/blob/c98f76c7c55f7d1039687cc293536d7caf893ba5/pkg/initializer/validator/datavalidator.go#L78-L94>database verification</a>.
This would suffice to determine if the member has valid data.</p><h3 id=recommendation-1>Recommendation</h3><p>Though <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral</a> persistence has performance and logistics advantages,
it is recommended to start with <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent>persistent</a> data for the member pods.
In addition to the reasons and concerns listed above, there is also the additional concern that in case of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure>backup failure</a>, the risk of additional data loss is a bit higher if ephemeral persistence is used (simultaneous quoram loss is sufficient) when compared to persistent storage (simultaenous quorum loss with majority persistence loss is needed).
The risk might still be acceptable but the idea is to gain experience about how frequently member containers/pods get restarted/recreated, how frequently leader election happens among members of an etcd cluster and how frequently etcd clusters lose quorum.
Based on this experience, we can move towards using <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral</a> (perhaps even <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#in-memory>in-memory</a>) persistence for the member pods.</p><h2 id=separating-peer-and-client-traffic>Separating peer and client traffic</h2><p>The current single-node ETCD cluster implementation in <code>etcd-druid</code> and <code>etcd-backup-restore</code> uses a single <code>service</code> object to act as the entry point for the client traffic.
There is no separation or distinction between the client and peer traffic because there is not much benefit to be had by making that distinction.</p><p>In the multi-node ETCD cluster scenario, it makes sense to distinguish between and separate the peer and client traffic.
This can be done by using two <code>services</code>.</p><ul><li>peer<ul><li>To be used for peer communication. This could be a <code>headless</code> service.</li></ul></li><li>client<ul><li>To be used for client communication. This could be a normal <code>ClusterIP</code> service like it is in the single-node case.</li></ul></li></ul><p>The main advantage of this approach is that it makes it possible (if needed) to allow only peer to peer communication while blocking client communication. Such a thing might be required during some phases of some maintenance tasks (manual or automated).</p><h3 id=cutting-off-client-requests>Cutting off client requests</h3><p>At present, in the single-node ETCD instances, etcd-druid configures the readinessProbe of the etcd main container to probe the healthz endpoint of the etcd-backup-restore sidecar which considers the status of the latest backup upload in addition to the regular checks about etcd and the side car being up and healthy. This has the effect of setting the etcd main container (and hence the etcd pod) as not ready if the latest backup upload failed. This results in the endpoints controller removing the pod IP address from the endpoints list for the service which eventually cuts off ingress traffic coming into the etcd pod via the etcd client service. The rationale for this is to fail early when the backup upload fails rather than continuing to serve requests while the gap between the last backup and the current data increases which might lead to unacceptably large amount of data loss if disaster strikes.</p><p>This approach will not work in the multi-node scenario because we need the individual member pods to be able to talk to each other to maintain the cluster quorum when backup upload fails but need to cut off only client ingress traffic.</p><p>It is recommended to separate the backup health condition tracking taking appropriate remedial actions.
With that, the backup health condition tracking is now separated to the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions><code>BackupReady</code> condition</a> in the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status><code>Etcd</code> resource <code>status</code></a> and the cutting off of client traffic (which could now be done for more reasons than failed backups) can be achieved in a different way described <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector>below</a>.</p><h4 id=manipulating-client-service-podselector>Manipulating Client Service podSelector</h4><p>The client traffic can be cut off by updating (manually or automatically by some component) the <code>podSelector</code> of the client service to add an additional label (say, unhealthy or disabled) such that the <code>podSelector</code> no longer matches the member pods created by the statefulset.
This will result in the client ingress traffic being cut off.
The peer service is left unmodified so that peer communication is always possible.</p><h2 id=health-check>Health Check</h2><p>The etcd main container and the etcd-backup-restore sidecar containers will be configured with livenessProbe and readinessProbe which will indicate the health of the containers and effectively the corresponding ETCD cluster member pod.</p><h3 id=backup-failure>Backup Failure</h3><p>As described <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#cutting-off-client-requests>above</a> using <code>readinessProbe</code> failures based on latest backup failure is not viable in the multi-node ETCD scenario.</p><p>Though cutting off traffic by <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector>manipulating client <code>service</code> <code>podSelector</code></a> is workable, it may not be desirable.</p><p>It is recommended that on backup failure, the leading <code>etcd-backup-restore</code> sidecar (the one that is responsible for taking backups at that point in time, as explained in the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup>backup section below</a>, updates the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions><code>BackupReady</code> condition</a> in the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status><code>Etcd</code> status</a> and raises a high priority alert to the landscape operators but <em><em>does not</em></em> cut off the client traffic.</p><p>The reasoning behind this decision to not cut off the client traffic on backup failure is to allow the Kubernetes cluster&rsquo;s control plane (which relies on the ETCD cluster) to keep functioning as long as possible and to avoid bringing down the control-plane due to a missed backup.</p><p>The risk of this approach is that with a cascaded sequence of failures (on top of the backup failure), there is a chance of more data loss than the frequency of backup would otherwise indicate.</p><p>To be precise, the risk of such an additional data loss manifests only when backup failure as well as a special case of quorum loss (majority of the members are not ready) happen in such a way that the ETCD cluster needs to be re-bootstrapped from the backup.
As described <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>here</a>, re-bootstrapping the ETCD cluster requires restoration from the latest backup only when a majority of members no longer have uncorrupted data persistence.</p><p>If <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent>persistent storage</a> is used, this will happen only when backup failure as well as a majority of the disks/volumes backing the ETCD cluster members fail simultaneously.
This would indeed be rare and might be an acceptable risk.</p><p>If <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral storage</a> is used (especially, <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#in-memory>in-memory</a>), the data loss will happen if a majority of the ETCD cluster members become <code>NotReady</code> (requiring a pod restart) at the same time as the backup failure.
This may not be as rare as majority members&rsquo; disk/volume failure.
The risk can be somewhat mitigated at least for planned maintenance operations by postponing potentially disruptive maintenance operations when <code>BackupReady</code> condition is <code>false</code> (vertical scaling, rolling updates, evictions due to node roll-outs).</p><p>But in practice (when <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral storage</a> is used), the current proposal suggests restoring from the latest full backup even when a minority of ETCD members (even a single pod) <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>restart</a> both to speed up the process of the new member catching up to the latest revision but also to avoid load on the leading member which needs to supply the data to bring the new member up-to-date.
But as described <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>here</a>, in case of a minority member failure while using ephemeral storage, it is possible to restart the new member with empty data and let it fetch all the data from the leading member (only if backup is not accessible).
Though this is suboptimal, it is workable given the constraints and conditions.
With this, the risk of additional data loss in the case of ephemeral storage is only if backup failure as well as quorum loss happens.
While this is still less rare than the risk of additional data loss in case of persistent storage, the risk might be tolerable. Provided the risk of quorum loss is not too high. This needs to be monitored/evaluated before opting for ephemeral storage.</p><p>Given these constraints, it is better to dynamically avoid/postpone some potentially disruptive operations when <code>BackupReady</code> condition is <code>false</code>.
This has the effect of allowing <code>n/2</code> members to be evicted when the backups are healthy and completely disabling evictions when backups are not healthy.</p><ol><li>Skip/postpone potentially disruptive maintenance operations (listed below) when the <code>BackupReady</code> condition is <code>false</code>.</li><li>Vertical scaling.</li><li>Rolling updates, Basically, any updates to the <code>StatefulSet</code> spec which includes vertical scaling.</li><li>Dynamically toggle the <code>minAvailable</code> field of the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#poddisruptionbudget><code>PodDisruptionBudget</code></a> between <code>n/2 + 1</code> and <code>n</code> (where <code>n</code> is the ETCD desired cluster size) whenever the <code>BackupReady</code> condition toggles between <code>true</code> and <code>false</code>.</li></ol><p>This will mean that <code>etcd-backup-restore</code> becomes Kubernetes-aware. But there might be reasons for making <code>etcd-backup-restore</code> Kubernetes-aware anyway (e.g. to update the <code>etcd</code> resource <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status>status</a> with latest full snapshot details).
This enhancement should keep <code>etcd-backup-restore</code> backward compatible.
I.e. it should be possible to use <code>etcd-backup-restore</code> Kubernetes-unaware as before this proposal.
This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as <code>--enable-client-service-updates</code> which can be defaulted to <code>false</code> for backward compatibility).</p><h5 id=alternative-3>Alternative</h5><p>The alternative is for <code>etcd-druid</code> to implement the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector>above functionality</a>.</p><p>But <code>etcd-druid</code> is centrally deployed in the host Kubernetes cluster and cannot scale well horizontally.
So, it can potentially be a bottleneck if it is involved in regular health check mechanism for all the etcd clusters it manages.
Also, the recommended approach above is more robust because it can work even if <code>etcd-druid</code> is down when the backup upload of a particular etcd cluster fails.</p><h2 id=status>Status</h2><p>It is desirable (for the <code>etcd-druid</code> and landscape administrators/operators) to maintain/expose status of the etcd cluster instances in the <code>status</code> sub-resource of the <code>Etcd</code> CRD.
The proposed structure for maintaining the status is as shown in the example below.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: druid.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Etcd
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: etcd-main
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 3
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - type: Ready                 <span style=color:green># Condition type for the readiness of the ETCD cluster</span>
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>              <span style=color:green># Indicates of the ETCD Cluster is ready or not</span>
</span></span><span style=display:flex><span>    lastHeartbeatTime:          <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    lastTransitionTime:         <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    reason: Quorate             <span style=color:green># Quorate|QuorumLost</span>
</span></span><span style=display:flex><span>  - type: AllMembersReady       <span style=color:green># Condition type for the readiness of all the member of the ETCD cluster</span>
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>              <span style=color:green># Indicates if all the members of the ETCD Cluster are ready</span>
</span></span><span style=display:flex><span>    lastHeartbeatTime:          <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    lastTransitionTime:         <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    reason: AllMembersReady     <span style=color:green># AllMembersReady|NotAllMembersReady</span>
</span></span><span style=display:flex><span>  - type: BackupReady           <span style=color:green># Condition type for the readiness of the backup of the ETCD cluster</span>
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>              <span style=color:green># Indicates if the backup of the ETCD cluster is ready</span>
</span></span><span style=display:flex><span>    lastHeartbeatTime:          <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    lastTransitionTime:         <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    reason: FullBackupSucceeded <span style=color:green># FullBackupSucceeded|IncrementalBackupSucceeded|FullBackupFailed|IncrementalBackupFailed</span>
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  clusterSize: 3
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  replicas: 3
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  members:
</span></span><span style=display:flex><span>  - name: etcd-main-0          <span style=color:green># member pod name</span>
</span></span><span style=display:flex><span>    id: 272e204152             <span style=color:green># member Id</span>
</span></span><span style=display:flex><span>    role: Leader               <span style=color:green># Member|Leader</span>
</span></span><span style=display:flex><span>    status: Ready              <span style=color:green># Ready|NotReady|Unknown</span>
</span></span><span style=display:flex><span>    lastTransitionTime:        <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    reason: LeaseSucceeded     <span style=color:green># LeaseSucceeded|LeaseExpired|UnknownGracePeriodExceeded|PodNotRead</span>
</span></span><span style=display:flex><span>  - name: etcd-main-1          <span style=color:green># member pod name</span>
</span></span><span style=display:flex><span>    id: 272e204153             <span style=color:green># member Id</span>
</span></span><span style=display:flex><span>    role: Member               <span style=color:green># Member|Leader</span>
</span></span><span style=display:flex><span>    status: Ready              <span style=color:green># Ready|NotReady|Unknown</span>
</span></span><span style=display:flex><span>    lastTransitionTime:        <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    reason: LeaseSucceeded     <span style=color:green># LeaseSucceeded|LeaseExpired|UnknownGracePeriodExceeded|PodNotRead</span>
</span></span></code></pre></div><p>This proposal recommends that <code>etcd-druid</code> (preferrably, the <code>custodian</code> controller in <code>etcd-druid</code>) maintains most of the information in the <code>status</code> of the <code>Etcd</code> resources described above.</p><p>One exception to this is the <code>BackupReady</code> condition which is recommended to be maintained by the <em>leading</em> <code>etcd-backup-restore</code> sidecar container.
This will mean that <code>etcd-backup-restore</code> becomes Kubernetes-aware. But there are other reasons for making <code>etcd-backup-restore</code> Kubernetes-aware anyway (e.g. to <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#health-check>maintain health conditions</a>).
This enhancement should keep <code>etcd-backup-restore</code> backward compatible.
But it should be possible to use <code>etcd-backup-restore</code> Kubernetes-unaware as before this proposal. This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as <code>--enable-etcd-status-updates</code> which can be defaulted to <code>false</code> for backward compatibility).</p><h3 id=members>Members</h3><p>The <code>members</code> section of the status is intended to be maintained by <code>etcd-druid</code> (preferraby, the <code>custodian</code> controller of <code>etcd-druid</code>) based on the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases><code>leases</code> of the individual members</a>.</p><h4 id=note-1>Note</h4><p>An earlier design in this proposal was for the individual <code>etcd-backup-restore</code> sidecars to update the corresponding <code>status.members</code> entries themselves. But this was redesigned to use <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>member <code>leases</code></a> to avoid conflicts rising from frequent updates and the limitations in the support for <a href=https://kubernetes.io/docs/reference/using-api/server-side-apply/>Server-Side Apply</a> in some versions of Kubernetes.</p><p>The <code>spec.holderIdentity</code> field in the <code>leases</code> is used to communicate the ETCD member <code>id</code> and <code>role</code> between the <code>etcd-backup-restore</code> sidecars and <code>etcd-druid</code>.</p><h4 id=member-name-as-the-key>Member name as the key</h4><p>In an ETCD cluster, the member <code>id</code> is the <a href=https://etcd.io/docs/v3.4/dev-guide/api_reference_v3/#message-member-etcdserveretcdserverpbrpcproto>unique identifier for a member</a>.
However, this proposal recommends using a <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#kubernetes-context>single <code>StatefulSet</code></a> whose pods form the members of the ETCD cluster and <code>Pods</code> of a <code>StatefulSet</code> have <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#ordinal-index>uniquely indexed names</a> as well as <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-network-id>uniquely addressible DNS</a>.</p><p>This proposal recommends that the <code>name</code> of the member (which is the same as the name of the member <code>Pod</code>) be used as the unique key to identify a member in the <code>members</code> array.
This can minimise the need to cleanup superfluous entries in the <code>members</code> array after the member pods are gone to some extent because the replacement pods for any member will share the same <code>name</code> and will overwrite the entry with a <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>possibly new</a> member <code>id</code>.</p><p>There is still the possibility of not only <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status>superfluous entries in the <code>members</code> array</a> but also <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member>superfluous <code>members</code> in the ETCD cluster</a> for which there is no corresponding pod in the <code>StatefulSet</code> anymore.</p><p>For example, if an ETCD cluster is scaled up from <code>3</code> to <code>5</code> and the new members were failing constantly due to insufficient resources and then if the ETCD client is scaled back down to <code>3</code> and failing member pods may not have the chance to clean up their <code>member</code> entries (from the <code>members</code> array as well as from the ETCD cluster) leading to superfluous members in the cluster that may have adverse effect on quorum of the cluster.</p><p>Hence, the superfluous entries in both <code>members</code> array as well as the ETCD cluster need to be <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12>cleaned up</a> <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member>as appropriate</a>.</p><h4 id=member-leases>Member Leases</h4><p>One <a href=https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/>Kubernetes <code>lease</code> object</a> per desired ETCD member is maintained by <code>etcd-druid</code> (preferrably, the <code>custodian</code> controller in <code>etcd-druid</code>).
The <code>lease</code> objects will be created in the same <code>namespace</code> as their owning <code>Etcd</code> object and will have the same <code>name</code> as the member to which they correspond (which, in turn would be the same as <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-name-as-the-key>the <code>pod</code> name in which the member ETCD process runs</a>).</p><p>The <code>lease</code> objects are created and deleted only by <code>etcd-druid</code> but are continually renewed within the <code>leaseDurationSeconds</code> by the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-independent-of-leader-election-in-all-members>individual <code>etcd-backup-restore</code> sidecars</a> (corresponding to their members) if the the corresponding ETCD member is ready and is part of the ETCD cluster.</p><p>This will mean that <code>etcd-backup-restore</code> becomes Kubernetes-aware. But there are other reasons for making <code>etcd-backup-restore</code> Kubernetes-aware anyway (e.g. to <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#health-check>maintain health conditions</a>).
This enhancement should keep <code>etcd-backup-restore</code> backward compatible.
But it should be possible to use <code>etcd-backup-restore</code> Kubernetes-unaware as before this proposal. This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as <code>--enable-etcd-lease-renewal</code> which can be defaulted to <code>false</code> for backward compatibility).</p><p>A <code>member</code> entry in the <code>Etcd</code> resource <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status><code>status</code></a> would be marked as <code>Ready</code> (with <code>reason: LeaseSucceeded</code>) if the corresponding <code>pod</code> is ready and the corresponding <code>lease</code> has not yet expired.
The <code>member</code> entry would be marked as <code>NotReady</code> if the corresponding <code>pod</code> is not ready (with reason <code>PodNotReady</code>) or as <code>Unknown</code> if the corresponding <code>lease</code> has expired (with <code>reason: LeaseExpired</code>).</p><p>While renewing the lease, the <code>etcd-backup-restore</code> sidecars also maintain the ETCD member <code>id</code> and their <code>role</code> (<code>Leader</code> or <code>Member</code>) separated by <code>:</code> in the <code>spec.holderIdentity</code> field of the corresponding <code>lease</code> object since this information is only available to the <code>ETCD</code> member processes and the <code>etcd-backup-restore</code> sidecars (e.g. <code>272e204152:Leader</code> or <code>272e204153:Member</code>).
When the <code>lease</code> objects are created by <code>etcd-druid</code>, the <code>spec.holderIdentity</code> field would be empty.</p><p>The value in <code>spec.holderIdentity</code> in the <code>leases</code> is parsed and copied onto the <code>id</code> and <code>role</code> fields of the corresponding <code>status.members</code> by <code>etcd-druid</code>.</p><h3 id=conditions>Conditions</h3><p>The <code>conditions</code> section in the status describe the overall condition of the ETCD cluster.
The condition type <code>Ready</code> indicates if the ETCD cluster as a whole is ready to serve requests (i.e. the cluster is quorate) even though some minority of the members are not ready.
The condition type <code>AllMembersReady</code> indicates of all the members of the ETCD cluster are ready.
The distinction between these conditions could be significant for both external consumers of the status as well as <code>etcd-druid</code> itself.
Some maintenance operations might be safe to do (e.g. rolling updates) only when all members of the cluster are ready.
The condition type <code>BackupReady</code> indicates of the most recent backup upload (full or incremental) succeeded.
This information also might be significant because some maintenance operations might be safe to do (e.g. anything that involves re-bootstrapping the ETCD cluster) only when backup is ready.</p><p>The <code>Ready</code> and <code>AllMembersReady</code> conditions can be maintained by <code>etcd-druid</code> based on the status in the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#members><code>members</code> section</a>.
The <code>BackupReady</code> condition will be maintained by the leading <code>etcd-backup-restore</code> sidecar that is in charge of taking backups.</p><p>More condition types could be introduced in the future if specific purposes arise.</p><h3 id=clustersize>ClusterSize</h3><p>The <code>clusterSize</code> field contains the current size of the ETCD cluster. It will be actively kept up-to-date by <code>etcd-druid</code> in all scenarios.</p><ul><li>Before <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping>bootstrapping</a> the ETCD cluster (during cluster creation or later bootstrapping because of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-9>quorum failure</a>), <code>etcd-druid</code> will clear the <code>status.members</code> array and set <code>status.clusterSize</code> to be equal to <code>spec.replicas</code>.</li><li>While the ETCD cluster is quorate, <code>etcd-druid</code> will actively set <code>status.clusterSize</code> to be equal to length of the <code>status.members</code> whenever the length of the array changes (say, due to scaling of the ETCD cluster).</li></ul><p>Given that <code>clusterSize</code> reliably represents the size of the ETCD cluster, it can be used to calculate the <code>Ready</code> <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions>condition</a>.</p><h3 id=alternative-4>Alternative</h3><p>The alternative is for <code>etcd-druid</code> to maintain the status in the <code>Etcd</code> status sub-resource.
But <code>etcd-druid</code> is centrally deployed in the host Kubernetes cluster and cannot scale well horizontally.
So, it can potentially be a bottleneck if it is involved in regular health check mechanism for all the etcd clusters it manages.
Also, the recommended approach above is more robust because it can work even if <code>etcd-druid</code> is down when the backup upload of a particular etcd cluster fails.</p><h2 id=decision-table-for-etcd-druid-based-on-the-status>Decision table for etcd-druid based on the status</h2><p>The following decision table describes the various criteria <code>etcd-druid</code> takes into consideration to determine the different etcd cluster management scenarios and the corresponding reconciliation actions it must take.
The general principle is to detect the scenario and take the minimum action to move the cluster along the path to good health.
The path from any one scenario to a state of good health will typically involve going through multiple reconciliation actions which probably take the cluster through many other cluster management scenarios.
Especially, it is proposed that individual members auto-heal where possible, even in the case of the failure of a majority of members of the etcd cluster and that <code>etcd-druid</code> takes action only if the auto-healing doesn&rsquo;t happen for a configured period of time.</p><h3 id=1-pink-of-health>1. Pink of health</h3><h4 id=observed-state>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: <code>n</code></li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: <code>n</code></li><li>Ready: <code>n</code></li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>0</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>0</code></li><li>Members with expired <code>lease</code>: <code>0</code></li></ul></li><li>conditions:<ul><li>Ready: <code>true</code></li><li>AllMembersReady: <code>true</code></li><li>BackupReady: <code>true</code></li></ul></li></ul></li></ul><h4 id=recommended-action>Recommended Action</h4><p>Nothing to do</p><h3 id=2-member-status-is-out-of-sync-with-their-leases>2. Member status is out of sync with their leases</h3><h4 id=observed-state-1>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: <code>n</code></li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: <code>n</code></li><li>Ready: <code>n</code></li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>r</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>0</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>0</code></li><li>Members with expired <code>lease</code>: <code>l</code></li></ul></li><li>conditions:<ul><li>Ready: <code>true</code></li><li>AllMembersReady: <code>true</code></li><li>BackupReady: <code>true</code></li></ul></li></ul></li></ul><h4 id=recommended-action-1>Recommended Action</h4><p>Mark the <code>l</code> members corresponding to the expired <code>leases</code> as <code>Unknown</code> with reason <code>LeaseExpired</code> and with <code>id</code> populated from <code>spec.holderIdentity</code> of the <code>lease</code> if they are not already updated so.</p><p>Mark the <code>n - l</code> members corresponding to the active <code>leases</code> as <code>Ready</code> with reason <code>LeaseSucceeded</code> and with <code>id</code> populated from <code>spec.holderIdentity</code> of the <code>lease</code> if they are not already updated so.</p><p>Please refer <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>here</a> for more details.</p><h3 id=3-all-members-are-ready-but-allmembersready-condition-is-stale>3. All members are <code>Ready</code> but <code>AllMembersReady</code> condition is stale</h3><h4 id=observed-state-2>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: N/A</li><li>Current: N/A</li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: <code>n</code></li><li>Ready: N/A</li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>0</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>0</code></li><li>Members with expired <code>lease</code>: <code>0</code></li></ul></li><li>conditions:<ul><li>Ready: N/A</li><li>AllMembersReady: false</li><li>BackupReady: N/A</li></ul></li></ul></li></ul><h4 id=recommended-action-2>Recommended Action</h4><p>Mark the status condition type <code>AllMembersReady</code> to <code>true</code>.</p><h3 id=4-not-all-members-are-ready-but-allmembersready-condition-is-stale>4. Not all members are <code>Ready</code> but <code>AllMembersReady</code> condition is stale</h3><h4 id=observed-state-3>Observed state</h4><ul><li><p>Cluster Size</p><ul><li>Desired: N/A</li><li>Current: N/A</li></ul></li><li><p><code>StatefulSet</code> replicas</p><ul><li>Desired: <code>n</code></li><li>Ready: N/A</li></ul></li><li><p><code>Etcd</code> status</p><ul><li>members<ul><li>Total: N/A</li><li>Ready: <code>r</code> where <code>0 &lt;= r &lt; n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>nr</code> where <code>0 &lt; nr &lt; n</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>u</code> where <code>0 &lt; u &lt; n</code></li><li>Members with expired <code>lease</code>: <code>h</code> where <code>0 &lt; h &lt; n</code></li></ul></li><li>conditions:<ul><li>Ready: N/A</li><li>AllMembersReady: true</li><li>BackupReady: N/A</li></ul></li></ul><p>where <code>(nr + u + h) > 0</code> or <code>r &lt; n</code></p></li></ul><h4 id=recommended-action-3>Recommended Action</h4><p>Mark the status condition type <code>AllMembersReady</code> to <code>false</code>.</p><h3 id=5-majority-members-are-ready-but-ready-condition-is-stale>5. Majority members are <code>Ready</code> but <code>Ready</code> condition is stale</h3><h4 id=observed-state-4>Observed state</h4><ul><li><p>Cluster Size</p><ul><li>Desired: N/A</li><li>Current: N/A</li></ul></li><li><p><code>StatefulSet</code> replicas</p><ul><li>Desired: <code>n</code></li><li>Ready: N/A</li></ul></li><li><p><code>Etcd</code> status</p><ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>r</code> where <code>r > n/2</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>nr</code> where <code>0 &lt; nr &lt; n/2</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>u</code> where <code>0 &lt; u &lt; n/2</code></li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions:<ul><li>Ready: <code>false</code></li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul><p>where <code>0 &lt; (nr + u + h) &lt; n/2</code></p></li></ul><h4 id=recommended-action-4>Recommended Action</h4><p>Mark the status condition type <code>Ready</code> to <code>true</code>.</p><h3 id=6-majority-members-are-notready-but-ready-condition-is-stale>6. Majority members are <code>NotReady</code> but <code>Ready</code> condition is stale</h3><h4 id=observed-state-5>Observed state</h4><ul><li><p>Cluster Size</p><ul><li>Desired: N/A</li><li>Current: N/A</li></ul></li><li><p><code>StatefulSet</code> replicas</p><ul><li>Desired: <code>n</code></li><li>Ready: N/A</li></ul></li><li><p><code>Etcd</code> status</p><ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>r</code> where <code>0 &lt; r &lt; n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>nr</code> where <code>0 &lt; nr &lt; n</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>u</code> where <code>0 &lt; u &lt; n</code></li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions:<ul><li>Ready: <code>true</code></li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul><p>where <code>(nr + u + h) > n/2</code> or <code>r &lt; n/2</code></p></li></ul><h4 id=recommended-action-5>Recommended Action</h4><p>Mark the status condition type <code>Ready</code> to <code>false</code>.</p><h3 id=7-some-members-have-been-in-unknown-status-for-a-while>7. Some members have been in <code>Unknown</code> status for a while</h3><h4 id=observed-state-6>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: N/A</li><li>Ready: N/A</li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: N/A</li><li>Ready: N/A</li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: N/A</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>u</code> where <code>u &lt;= n</code></li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions:<ul><li>Ready: N/A</li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul></li></ul><h4 id=recommended-action-6>Recommended Action</h4><p>Mark the <code>u</code> members as <code>NotReady</code> in <code>Etcd</code> status with <code>reason: UnknownGracePeriodExceeded</code>.</p><h3 id=8-some-member-pods-are-not-ready-but-have-not-had-the-chance-to-update-their-status>8. Some member pods are not <code>Ready</code> but have not had the chance to update their status</h3><h4 id=observed-state-7>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: <code>n</code></li><li>Ready: <code>s</code> where <code>s &lt; n</code></li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: N/A</li><li>Ready: N/A</li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: N/A</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: N/A</li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions:<ul><li>Ready: N/A</li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul></li></ul><h4 id=recommended-action-7>Recommended Action</h4><p>Mark the <code>n - s</code> members (corresponding to the pods that are not <code>Ready</code>) as <code>NotReady</code> in <code>Etcd</code> status with <code>reason: PodNotReady</code></p><h3 id=9-quorate-cluster-with-a-minority-of-members-notready>9. Quorate cluster with a minority of members <code>NotReady</code></h3><h4 id=observed-state-8>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: N/A</li><li>Ready: N/A</li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>n - f</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>f</code> where <code>f &lt; n/2</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>0</code></li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions:<ul><li>Ready: true</li><li>AllMembersReady: false</li><li>BackupReady: true</li></ul></li></ul></li></ul><h4 id=recommended-action-8>Recommended Action</h4><p>Delete the <code>f</code> <code>NotReady</code> member pods to force restart of the pods if they do not automatically restart via failed <code>livenessProbe</code>. The expectation is that they will either re-join the cluster as an existing member or remove themselves and join as new members on restart of the container or pod and <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>renew their <code>leases</code></a>.</p><h3 id=10-quorum-lost-with-a-majority-of-members-notready>10. Quorum lost with a majority of members <code>NotReady</code></h3><h4 id=observed-state-9>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: N/A</li><li>Ready: N/A</li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>n - f</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>f</code> where <code>f >= n/2</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: N/A</li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions:<ul><li>Ready: false</li><li>AllMembersReady: false</li><li>BackupReady: true</li></ul></li></ul></li></ul><h4 id=recommended-action-9>Recommended Action</h4><p>Scale down the <code>StatefulSet</code> to <code>replicas: 0</code>. Ensure that all member pods are deleted. Ensure that all the members are removed from <code>Etcd</code> status. Delete and recreate all the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>member <code>leases</code></a>. Recover the cluster from loss of quorum as discussed <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>here</a>.</p><h3 id=11-scale-up-of-a-healthy-cluster>11. Scale up of a healthy cluster</h3><h4 id=observed-state-10>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: <code>d</code></li><li>Current: <code>n</code> where <code>d > n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: N/A</li><li>Ready: <code>n</code></li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: 0</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: 0</li><li>Members with expired <code>lease</code>: 0</li></ul></li><li>conditions:<ul><li>Ready: true</li><li>AllMembersReady: true</li><li>BackupReady: true</li></ul></li></ul></li></ul><h4 id=recommended-action-10>Recommended Action</h4><p>Add <code>d - n</code> new members by scaling the <code>StatefulSet</code> to <code>replicas: d</code>. The rest of the <code>StatefulSet</code> spec need not be updated until the next cluster bootstrapping (alternatively, the rest of the <code>StatefulSet</code> spec can be updated pro-actively once the new members join the cluster. This will trigger a rolling update).</p><p>Also, create the additional <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>member <code>leases</code></a> for the <code>d - n</code> new members.</p><h3 id=12-scale-down-of-a-healthy-cluster>12. Scale down of a healthy cluster</h3><h4 id=observed-state-11>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: <code>d</code></li><li>Current: <code>n</code> where <code>d &lt; n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: <code>n</code></li><li>Ready: <code>n</code></li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: 0</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: 0</li><li>Members with expired <code>lease</code>: 0</li></ul></li><li>conditions:<ul><li>Ready: true</li><li>AllMembersReady: true</li><li>BackupReady: true</li></ul></li></ul></li></ul><h4 id=recommended-action-11>Recommended Action</h4><p>Remove <code>d - n</code> existing members (numbered <code>d</code>, <code>d + 1</code> &mldr; <code>n</code>) by scaling the <code>StatefulSet</code> to <code>replicas: d</code>. The <code>StatefulSet</code> spec need not be updated until the next cluster bootstrapping (alternatively, the <code>StatefulSet</code> spec can be updated pro-actively once the superfluous members exit the cluster. This will trigger a rolling update).</p><p>Also, delete the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>member <code>leases</code></a> for the <code>d - n</code> members being removed.</p><p>The <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status>superfluous entries in the <code>members</code> array</a> will be cleaned up as explained <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12>here</a>.
The superfluous members in the ETCD cluster will be cleaned up by the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member>leading <code>etcd-backup-restore</code> sidecar</a>.</p><h3 id=13-superfluous-member-entries-in-etcd-status>13. Superfluous member entries in <code>Etcd</code> status</h3><h4 id=observed-state-12>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: n</li><li>Ready: n</li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>m</code> where <code>m > n</code></li><li>Ready: N/A</li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: N/A</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: N/A</li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions:<ul><li>Ready: N/A</li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul></li></ul><h4 id=recommended-action-12>Recommended Action</h4><p>Remove the superfluous <code>m - n</code> member entries from <code>Etcd</code> status (numbered <code>n</code>, <code>n+1</code> &mldr; <code>m</code>).
Remove the superfluous <code>m - n</code> <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>member <code>leases</code></a> if they exist.
The superfluous members in the ETCD cluster will be cleaned up by the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member>leading <code>etcd-backup-restore</code> sidecar</a>.</p><h2 id=decision-table-for-etcd-backup-restore-during-initialization>Decision table for etcd-backup-restore during initialization</h2><p>As discussed above, the initialization sequence of <code>etcd-backup-restore</code> in a member pod needs to <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-configuration>generate suitable etcd configuration</a> for its etcd container.
It also might have to handle the etcd database verification and restoration functionality differently in <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>different</a> <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>scenarios</a>.</p><p>The initialization sequence itself is proposed to be as follows.
It is an enhancement of the <a href=https://github.com/gardener/etcd-backup-restore/blob/master/docs/proposals/design.md#workflow>existing</a> initialization sequence.
<img src=/__resources/01-etcd-member-initialization-sequence_364f5e.png alt="etcd member initialization sequence"></p><p>The details of the decisions to be taken during the initialization are given below.</p><h3 id=1-first-member-during-bootstrap-of-a-fresh-etcd-cluster>1. First member during bootstrap of a fresh etcd cluster</h3><h4 id=observed-state-13>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>0</code></li><li>Ready: <code>0</code></li><li>Status contains own member: <code>false</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: <code>false</code></li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup<ul><li>Backup exists: <code>false</code></li><li>Backup has incremental snapshots: <code>false</code></li></ul></li></ul><h4 id=recommended-action-13>Recommended Action</h4><p>Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state new and return success.</p><h3 id=2-addition-of-a-new-following-member-during-bootstrap-of-a-fresh-etcd-cluster>2. Addition of a new following member during bootstrap of a fresh etcd cluster</h3><h4 id=observed-state-14>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>m</code> where <code>0 &lt; m &lt; n</code></li><li>Ready: <code>m</code></li><li>Status contains own member: <code>false</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: <code>false</code></li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup<ul><li>Backup exists: <code>false</code></li><li>Backup has incremental snapshots: <code>false</code></li></ul></li></ul><h4 id=recommended-action-14>Recommended Action</h4><p>Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state new and return success.</p><h3 id=3-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-and-data>3. Restart of an existing member of a quorate cluster with valid metadata and data</h3><h4 id=observed-state-15>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>m</code> where <code>m > n/2</code></li><li>Ready: <code>r</code> where <code>r > n/2</code></li><li>Status contains own member: <code>true</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: <code>true</code></li><li>Data directory is valid and up-to-date: <code>true</code></li></ul></li><li>Backup<ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id=recommended-action-15>Recommended Action</h4><p>Re-use previously generated etcd configuration and return success.</p><h3 id=4-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-but-without-valid-data>4. Restart of an existing member of a quorate cluster with valid metadata but without valid data</h3><h4 id=observed-state-16>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>m</code> where <code>m > n/2</code></li><li>Ready: <code>r</code> where <code>r > n/2</code></li><li>Status contains own member: <code>true</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: <code>true</code></li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup<ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id=recommended-action-16>Recommended Action</h4><p><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster>Remove</a> self as a member (old member ID) from the etcd cluster as well as <code>Etcd</code> status. <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>Add</a> self as a new member of the etcd cluster as well as in the <code>Etcd</code> status. If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>here</a> for the reason for not restoring incremental snapshots). Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state <code>existing</code> and return success.</p><h3 id=5-restart-of-an-existing-member-of-a-quorate-cluster-without-valid-metadata>5. Restart of an existing member of a quorate cluster without valid metadata</h3><h4 id=observed-state-17>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>m</code> where <code>m > n/2</code></li><li>Ready: <code>r</code> where <code>r > n/2</code></li><li>Status contains own member: <code>true</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: <code>false</code></li><li>Data directory is valid and up-to-date: N/A</li></ul></li><li>Backup<ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id=recommended-action-17>Recommended Action</h4><p><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster>Remove</a> self as a member (old member ID) from the etcd cluster as well as <code>Etcd</code> status. <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>Add</a> self as a new member of the etcd cluster as well as in the <code>Etcd</code> status. If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>here</a> for the reason for not restoring incremental snapshots). Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state <code>existing</code> and return success.</p><h3 id=6-restart-of-an-existing-member-of-a-non-quorate-cluster-with-valid-metadata-and-data>6. Restart of an existing member of a non-quorate cluster with valid metadata and data</h3><h4 id=observed-state-18>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>m</code> where <code>m &lt; n/2</code></li><li>Ready: <code>r</code> where <code>r &lt; n/2</code></li><li>Status contains own member: <code>true</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: <code>true</code></li><li>Data directory is valid and up-to-date: <code>true</code></li></ul></li><li>Backup<ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id=recommended-action-18>Recommended Action</h4><p>Re-use previously generated etcd configuration and return success.</p><h3 id=7-restart-of-the-first-member-of-a-non-quorate-cluster-without-valid-data>7. Restart of the first member of a non-quorate cluster without valid data</h3><h4 id=observed-state-19>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>0</code></li><li>Ready: <code>0</code></li><li>Status contains own member: <code>false</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: N/A</li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup<ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id=recommended-action-19>Recommended Action</h4><p>If backups do not exist, create an empty data and WAL directory. If backups exist, restore the latest full snapshot. Start a single-node embedded etcd with initial cluster peer URLs containing only own peer URL and initial cluster state <code>new</code>. If incremental snapshots exist, apply them serially (honouring source transactions). Take and upload a full snapshot after incremental snapshots are applied successfully (please see <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>here</a> for more reasons why). Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state <code>new</code> and return success.</p><h3 id=8-restart-of-a-following-member-of-a-non-quorate-cluster-without-valid-data>8. Restart of a following member of a non-quorate cluster without valid data</h3><h4 id=observed-state-20>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>m</code> where <code>1 &lt; m &lt; n</code></li><li>Ready: <code>r</code> where <code>1 &lt; r &lt; n</code></li><li>Status contains own member: <code>false</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: N/A</li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup<ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id=recommended-action-20>Recommended Action</h4><p>If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>here</a> for the reason for not restoring incremental snapshots). Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state <code>existing</code> and return success.</p><h2 id=backup>Backup</h2><p>Only one of the etcd-backup-restore sidecars among the members are required to take the backup for a given ETCD cluster. This can be called a <code>backup leader</code>. There are two possibilities to ensure this.</p><h3 id=leading-etcd-main-containers-sidecar-is-the-backup-leader>Leading ETCD main container’s sidecar is the backup leader</h3><p>The backup-restore sidecar could poll the etcd cluster and/or its own etcd main container to see if it is the leading member in the etcd cluster.
This information can be used by the backup-restore sidecars to decide that sidecar of the leading etcd main container is the backup leader (i.e. responsible to for taking/uploading backups regularly).</p><p>The advantages of this approach are as follows.</p><ul><li>The approach is operationally and conceptually simple. The leading etcd container and backup-restore sidecar are always located in the same pod.</li><li>Network traffic between the backup container and the etcd cluster will always be local.</li></ul><p>The disadvantage is that this approach may not age well in the future if we think about moving the backup-restore container as a separate pod rather than a sidecar container.</p><h3 id=independent-leader-election-between-backup-restore-sidecars>Independent leader election between backup-restore sidecars</h3><p>We could use the etcd <code>lease</code> mechanism to perform leader election among the backup-restore sidecars. For example, using something like <a href=https://pkg.go.dev/go.etcd.io/etcd/clientv3/concurrency#Election.Campaign><code>go.etcd.io/etcd/clientv3/concurrency</code></a>.</p><p>The advantage and disadvantages are pretty much the opposite of the approach <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#leading-etcd-main-containers-sidecar-is-the-backup-leader>above</a>.
The advantage being that this approach may age well in the future if we think about moving the backup-restore container as a separate pod rather than a sidecar container.</p><p>The disadvantages are as follows.</p><ul><li>The approach is operationally and conceptually a bit complex. The leading etcd container and backup-restore sidecar might potentially belong to different pods.</li><li>Network traffic between the backup container and the etcd cluster might potentially be across nodes.</li></ul><h2 id=history-compaction>History Compaction</h2><p>This proposal recommends to configure <a href=https://etcd.io/docs/v3.2.17/op-guide/maintenance/#history-compaction>automatic history compaction</a> on the individual members.</p><h2 id=defragmentation>Defragmentation</h2><p>Defragmentation is already <a href=https://github.com/gardener/etcd-backup-restore/blob/0dfdd50fbfc5ebc88238be3bc79c3ac3fc242c08/cmd/options.go#L209>triggered periodically</a> by <code>etcd-backup-restore</code>.
This proposal recommends to enhance this functionality to be performed only by the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup>leading</a> backup-restore container.
The defragmentation must be performed only when etcd cluster is in full health and must be done in a rolling manner for each members to <a href=https://etcd.io/docs/v3.2.17/op-guide/maintenance/#defragmentation>avoid disruption</a>.
The leading member should be defragmented last after all the rest of the members have been defragmented to minimise potential leadership changes caused by defragmentation.
If the etcd cluster is unhealthy when it is time to trigger scheduled defragmentation, the defragmentation must be postponed until the cluster becomes healthy. This check must be done before triggering defragmentation for each member.</p><h2 id=work-flows-in-etcd-backup-restore>Work-flows in etcd-backup-restore</h2><p>There are different work-flows in etcd-backup-restore.
Some existing flows like initialization, scheduled backups and defragmentation have been enhanced or modified.
Some new work-flows like status updates have been introduced.
Some of these work-flows are sensitive to which <code>etcd-backup-restore</code> container is <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup>leading</a> and some are not.</p><p>The life-cycle of these work-flows is shown below.
<img src=/__resources/01-etcd-backup-restore-work-flows-life-cycle_eec586.png alt="etcd-backup-restore work-flows life-cycle"></p><h3 id=work-flows-independent-of-leader-election-in-all-members>Work-flows independent of leader election in all members</h3><ul><li>Serve the <a href=https://github.com/gardener/etcd-backup-restore/blob/master/pkg/server/httpAPI.go#L101-L107>HTTP API</a> that all members are expected to support currently but some HTTP API call which are used to take <a href=https://github.com/gardener/etcd-backup-restore/blob/5dfcc1f848a9f325d41a24eae4defb70d997c215/pkg/server/httpAPI.go#L103-L105>out-of-sync delta or full snapshot</a> should delegate the incoming HTTP requests to the <code>leading-sidecar</code> and one of the possible approach to achieve this is via an <a href=https://pkg.go.dev/net/http/httputil#ReverseProxy.ServeHTTP>HTTP reverse proxy</a>.</li><li>Check the health of the respective etcd member and renew the corresponding <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>member <code>lease</code></a>.</li></ul><h3 id=work-flows-only-on-the-leading-member>Work-flows only on the leading member</h3><ul><li>Take <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup>backups</a> (full and incremental) at configured regular intervals</li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#defragmentation>Defragment</a> all the members sequentially at configured regular intervals</li><li>Cleanup superflous members from the ETCD cluster for which there is no corresponding pod (the <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#ordinal-index>ordinal</a> in the pod name is greater than the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#clustersize>cluster size</a>) at regular intervals (or whenever the <code>Etcd</code> resource <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status>status</a> changes by watching it)<ul><li>The cleanup of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status>superfluous entries in <code>status.members</code> array</a> is already covered <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12>here</a></li></ul></li></ul><h2 id=high-availability>High Availability</h2><p>Considering that high-availability is the primary reason for using a multi-node etcd cluster, it makes sense to distribute the individual member pods of the etcd cluster across different physical nodes.
If the underlying Kubernetes cluster has nodes from multiple availability zones, it makes sense to also distribute the member pods across nodes from different availability zones.</p><p>One possibility to do this is via <a href=https://kubernetes.io/docs/reference/scheduling/policies/#priorities><code>SelectorSpreadPriority</code></a> of <code>kube-scheduler</code> but this is only <a href=https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone>best-effort</a> and may not always be enforced strictly.</p><p>It is better to use <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>pod anti-affinity</a> to enforce such distribution of member pods.</p><h3 id=zonal-cluster---single-availability-zone>Zonal Cluster - Single Availability Zone</h3><p>A zonal cluster is configured to consist of nodes belonging to only a single availability zone in a region of the cloud provider.
In such a case, we can at best distribute the member pods of a multi-node etcd cluster instance only across different nodes in the configured availability zone.</p><p>This can be done by specifying <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>pod anti-affinity</a> in the specification of the member pods using <a href=https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#kubernetes-io-hostname><code>kubernetes.io/hostname</code></a> as the topology key.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: StatefulSet
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>      affinity:
</span></span><span style=display:flex><span>        podAntiAffinity:
</span></span><span style=display:flex><span>          requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span style=display:flex><span>          - labelSelector: {} <span style=color:green># podSelector that matches the member pods of the given etcd cluster instance</span>
</span></span><span style=display:flex><span>            topologyKey: <span style=color:#a31515>&#34;kubernetes.io/hostname&#34;</span>
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>The recommendation is to keep <code>etcd-druid</code> agnostic of such topics related scheduling and cluster-topology and to use <a href=https://github.com/gardener/kupid>kupid</a> to <a href=https://github.com/gardener/kupid#mutating-higher-order-controllers>orthogonally inject</a> the desired <a href=https://github.com/gardener/kupid/blob/master/config/samples/cpsp-pod-affinity-anti-affinity.yaml>pod anti-affinity</a>.</p><h4 id=alternative-5>Alternative</h4><p>Another option is to build the functionality into <code>etcd-druid</code> to include the required pod anti-affinity when it provisions the <code>StatefulSet</code> that manages the member pods.
While this has the advantage of avoiding a dependency on an external component like <a href=https://github.com/gardener/kupid>kupid</a>, the disadvantage is that we might need to address development or testing use-cases where it might be desirable to avoid distributing member pods and schedule them on as less number of nodes as possible.
Also, as mentioned <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#regional-cluster---multiple-availability-zones>below</a>, <a href=https://github.com/gardener/kupid>kupid</a> can be used to distribute member pods of an etcd cluster instance across nodes in a single availability zone as well as across nodes in multiple availability zones with very minor variation.
This keeps the solution uniform regardless of the topology of the underlying Kubernetes cluster.</p><h3 id=regional-cluster---multiple-availability-zones>Regional Cluster - Multiple Availability Zones</h3><p>A regional cluster is configured to consist of nodes belonging to multiple availability zones (typically, three) in a region of the cloud provider.
In such a case, we can distribute the member pods of a multi-node etcd cluster instance across nodes belonging to different availability zones.</p><p>This can be done by specifying <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>pod anti-affinity</a> in the specification of the member pods using <a href=https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone><code>topology.kubernetes.io/zone</code></a> as the topology key.
In Kubernetes clusters using Kubernetes release older than <code>1.17</code>, the older (and now deprecated) <a href=https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#failure-domainbetakubernetesiozone><code>failure-domain.beta.kubernetes.io/zone</code></a> might have to be used as the topology key.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: StatefulSet
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>      affinity:
</span></span><span style=display:flex><span>        podAntiAffinity:
</span></span><span style=display:flex><span>          requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span style=display:flex><span>          - labelSelector: {} <span style=color:green># podSelector that matches the member pods of the given etcd cluster instance</span>
</span></span><span style=display:flex><span>            topologyKey: &#34;topology.kubernetes.io/zone
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>The recommendation is to keep <code>etcd-druid</code> agnostic of such topics related scheduling and cluster-topology and to use <a href=https://github.com/gardener/kupid>kupid</a> to <a href=https://github.com/gardener/kupid#mutating-higher-order-controllers>orthogonally inject</a> the desired <a href=https://github.com/gardener/kupid/blob/master/config/samples/cpsp-pod-affinity-anti-affinity.yaml>pod anti-affinity</a>.</p><h4 id=alternative-6>Alternative</h4><p>Another option is to build the functionality into <code>etcd-druid</code> to include the required pod anti-affinity when it provisions the <code>StatefulSet</code> that manages the member pods.
While this has the advantage of avoiding a dependency on an external component like <a href=https://github.com/gardener/kupid>kupid</a>, the disadvantage is that such built-in support necessarily limits what kind of topologies of the underlying cluster will be supported.
Hence, it is better to keep <code>etcd-druid</code> altogether agnostic of issues related to scheduling and cluster-topology.</p><h3 id=poddisruptionbudget>PodDisruptionBudget</h3><p>This proposal recommends that <code>etcd-druid</code> should deploy <a href=https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets><code>PodDisruptionBudget</code></a> (<code>minAvailable</code> set to <code>floor(&lt;cluster size>/2) + 1</code>) for multi-node etcd clusters (if <code>AllMembersReady</code> <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions>condition</a> is <code>true</code>) to ensure that any planned disruptive operation can try and honour the disruption budget to ensure high availability of the etcd cluster while making potentially disrupting maintenance operations.</p><p>Also, it is recommended to toggle the <code>minAvailable</code> field between <code>floor(&lt;cluster size>/2)</code> and <code>&lt;number of members with status Ready true></code> whenever the <code>AllMembersReady</code> condition toggles between <code>true</code> and <code>false</code>.
This is to disable eviction of any member pods when not all members are <code>Ready</code>.</p><p>In case of a conflict, the recommendation is to use the highest of the applicable values for <code>minAvailable</code>.</p><h2 id=rolling-updates-to-etcd-members>Rolling updates to etcd members</h2><p>Any changes to the <code>Etcd</code> resource spec that might result in a change to <code>StatefulSet</code> spec or otherwise result in a rolling update of member pods should be applied/propagated by <code>etcd-druid</code> only when the etcd cluster is fully healthy to reduce the risk of quorum loss during the updates.
This would include vertical autoscaling changes (via, <a href=https://github.com/gardener/hvpa-controller>HVPA</a>).
If the cluster <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status>status</a> unhealthy (i.e. if either <code>AllMembersReady</code> or <code>BackupReady</code> <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions>conditions</a> are <code>false</code>), <code>etcd-druid</code> must restore it to full health <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure>before proceeding</a> with such operations that lead to rolling updates.
This can be further optimized in the future to handle the cases where rolling updates can still be performed on an etcd cluster that is not fully healthy.</p><h2 id=follow-up>Follow Up</h2><h3 id=ephemeral-volumes>Ephemeral Volumes</h3><p>See section <em><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#Ephemeral_Volumes>Ephemeral Volumes</a></em>.</p><h3 id=shoot-control-plane-migration>Shoot Control-Plane Migration</h3><p>This proposal adds support for multi-node etcd clusters but it should not have significant impact on <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md>shoot control-plane migration</a> any more than what already present in the single-node etcd cluster scenario.
But to be sure, this needs to be discussed further.</p><h3 id=performance-impact-of-multi-node-etcd-clusters>Performance impact of multi-node etcd clusters</h3><p>Multi-node etcd clusters incur a cost on <a href=https://etcd.io/docs/v2/admin_guide/#optimal-cluster-size>write performance</a> as compared to single-node etcd clusters.
This performance impact needs to be measured and documented.
Here, we should compare different persistence option for the multi-nodeetcd clusters so that we have all the information necessary to take the decision balancing the high-availability, performance and costs.</p><h3 id=metrics-dashboards-and-alerts>Metrics, Dashboards and Alerts</h3><p>There are already metrics exported by etcd and <code>etcd-backup-restore</code> which are visualized in monitoring dashboards and also used in triggering alerts.
These might have hidden assumptions about single-node etcd clusters.
These might need to be enhanced and potentially new metrics, dashboards and alerts configured to cover the multi-node etcd cluster scenario.</p><p>Especially, a high priority alert must be raised if <code>BackupReady</code> <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#condition>condition</a> becomes <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure><code>false</code></a>.</p><h3 id=costs>Costs</h3><p>Multi-node etcd clusters will clearly involve higher cost (when compared with single-node etcd clusters) just going by the CPU and memory usage for the additional members.
Also, the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#data-persistence>different options</a> for persistence for etcd data for the members will have different cost implications.
Such cost impact needs to be assessed and documented to help navigate the trade offs between high availability, performance and costs.</p><h2 id=future-work>Future Work</h2><h3 id=gardener-ring>Gardener Ring</h3><p><a href=https://github.com/gardener/gardener/issues/233>Gardener Ring</a>, requires provisioning and management of an etcd cluster with the members distributed across more than one Kubernetes cluster.
This cannot be achieved by etcd-druid alone which has only the view of a single Kubernetes cluster.
An additional component that has the view of all the Kubernetes clusters involved in setting up the gardener ring will be required to achieve this.
However, etcd-druid can be used by such a higher-level component/controller (for example, by supplying the initial cluster configuration) such that individual etcd-druid instances in the individual Kubernetes clusters can manage the corresponding etcd cluster members.</p><h3 id=autonomous-shoot-clusters>Autonomous Shoot Clusters</h3><p><a href=https://github.com/gardener/gardener/issues/2906>Autonomous Shoot Clusters</a> also will require a highly availble etcd cluster to back its control-plane and the multi-node support proposed here can be leveraged in that context.
However, the current proposal will not meet all the needs of a autonomous shoot cluster.
Some additional components will be required that have the overall view of the autonomous shoot cluster and they can use etcd-druid to manage the multi-node etcd cluster. But this scenario may be different from that of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#gardener-ring>Gardener Ring</a> in that the individual etcd members of the cluster may not be hosted on different Kubernetes clusters.</p><h3 id=optimization-of-recovery-from-non-quorate-cluster-with-some-member-containing-valid-data>Optimization of recovery from non-quorate cluster with some member containing valid data</h3><p>It might be possible to optimize the actions during the recovery of a non-quorate cluster where some of the members contain valid data and some other don&rsquo;t.
The optimization involves verifying the data of the valid members to determine the data of which member is the most recent (even considering the latest backup) so that the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>full snapshot</a> can be taken from it before recovering the etcd cluster.
Such an optimization can be attempted in the future.</p><h3 id=optimization-of-rolling-updates-to-unhealthy-etcd-clusters>Optimization of rolling updates to unhealthy etcd clusters</h3><p>As mentioned <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#rolling-updates-to-etcd-members>above</a>, optimizations to proceed with rolling updates to unhealthy etcd clusters (without first restoring the cluster to full health) can be pursued in future work.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-68692e55d72dded65ed0a27990d8ff03>3.3 - 02 Snapshot Compaction</h1><h1 id=dep-02-snapshot-compaction-for-etcd>DEP-02: Snapshot Compaction for Etcd</h1><h2 id=current-problem>Current Problem</h2><p>To ensure recoverability of Etcd, backups of the database are taken at regular interval.
Backups are of two types: Full Snapshots and Incremental Snapshots.</p><h3 id=full-snapshots>Full Snapshots</h3><p>Full snapshot is a snapshot of the complete database at given point in time.The size of the database keeps changing with time and typically the size is relatively large (measured in 100s of megabytes or even in gigabytes. For this reason, full snapshots are taken after some large intervals.</p><h3 id=incremental-snapshots>Incremental Snapshots</h3><p>Incremental Snapshots are collection of events on Etcd database, obtained through running WATCH API Call on Etcd. After some short intervals, all the events that are accumulated through WATCH API Call are saved in a file and named as Incremental Snapshots at relatively short time intervals.</p><h3 id=recovery-from-the-snapshots>Recovery from the Snapshots</h3><h4 id=recovery-from-full-snapshots>Recovery from Full Snapshots</h4><p>As the full snapshots are snapshots of the complete database, the whole database can be recovered from a full snapshot in one go. Etcd provides API Call to restore the database from a full snapshot file.</p><h4 id=recovery-from-incremental-snapshots>Recovery from Incremental Snapshots</h4><p>Delta snapshots are collection of retrospective Etcd events. So, to restore from Incremental snapshot file, the events from the file are needed to be applied sequentially on Etcd database through Etcd Put/Delete API calls. As it is heavily dependent on Etcd calls sequentially, restoring from Incremental Snapshot files can take long if there are numerous commands captured in Incremental Snapshot files.</p><p>Delta snapshots are applied on top of running Etcd database. So, if there is inconsistency between the state of database at the point of applying and the state of the database when the delta snapshot commands were captured, restoration will fail.</p><p>Currently, in Gardener setup, Etcd is restored from the last full snapshot and then the delta snapshots, which were captured after the last full snapshot.</p><p>The main problem with this is that the complete restoration time can be unacceptably large if the rate of change coming into the etcd database is quite high because there are large number of events in the delta snapshots to be applied sequentially.
A secondary problem is that, though auto-compaction is enabled for etcd, it is not quick enough to compact all the changes from the incremental snapshots being re-applied during the relatively short period of time of restoration (as compared to the actual period of time when the incremental snapshots were accumulated). This may lead to the etcd pod (the backup-restore sidecar container, to be precise) to run out of memory and/or storage space even if it is sufficient for normal operations.</p><h2 id=solution>Solution</h2><h3 id=compaction-command>Compaction command</h3><p>To help with the problem mentioned earlier, our proposal is to introduce <code>compact</code> subcommand with <code>etcdbrctl</code>. On execution of <code>compact</code> command, A separate embedded Etcd process will be started where the Etcd data will be restored from the snapstore (exactly as in the restoration scenario today). Then the new Etcd database will be compacted and defragmented using Etcd API calls. The compaction will strip off the Etcd database of old revisions as per the Etcd auto-compaction configuration. The defragmentation will free up the unused fragment memory space released after compaction. Then a full snapshot of the compacted database will be saved in snapstore which then can be used as the base snapshot during any subsequent restoration (or backup compaction).</p><h3 id=how-the-solution-works>How the solution works</h3><p>The newly introduced compact command does not disturb the running Etcd while compacting the backup snapshots. The command is designed to run potentially separately (from the main Etcd process/container/pod). Etcd Druid can be configured to run the newly introduced compact command as a separate job (scheduled periodically) based on total number of Etcd events accumulated after the most recent full snapshot.</p><h3 id=etcd-druid-flags>Etcd-druid flags:</h3><p>Etcd-druid introduces the following flags to configure the compaction job:</p><ul><li><code>--enable-backup-compaction</code> (default <code>false</code>): Set this flag to <code>true</code> to enable the automatic compaction of etcd backups when the threshold value denoted by CLI flag <code>--etcd-events-threshold</code> is exceeded.</li><li><code>--compaction-workers</code> (default <code>3</code>): Number of worker threads of the CompactionJob controller. The controller creates a backup compaction job if a certain etcd event threshold is reached. If compaction is enabled, the value for this flag must be greater than zero.</li><li><code>--etcd-events-threshold</code> (default <code>1000000</code>): Total number of etcd events that can be allowed before a backup compaction job is triggered.</li><li><code>--active-deadline-duration</code> (default <code>3h</code>): Duration after which a running backup compaction job will be terminated.</li><li><code>--metrics-scrape-wait-duration</code> (default <code>0s</code>): Duration to wait for after compaction job is completed, to allow Prometheus metrics to be scraped.</li></ul><h3 id=points-to-take-care-while-saving-the-compacted-snapshot><strong>Points to take care while saving the compacted snapshot:</strong></h3><p>As compacted snapshot and the existing periodic full snapshots are taken by different processes running in different pods but accessing same store to save the snapshots, some problems may arise:</p><ol><li>When uploading the compacted snapshot to the snapstore, there is the problem of how does the restorer know when to start using the newly compacted snapshot. This communication needs to be atomic.</li><li>With a regular schedule for compaction that happens potentially separately from the main etcd pod, is there a need for regular scheduled full snapshots anymore?</li><li>We are planning to introduce new directory structure, under v2 prefix, for saving the snapshots (compacted and full), as mentioned in details below. But for backward compatibility, we also need to consider the older directory, which is currently under v1 prefix, during accessing snapshots.</li></ol><h4 id=how-to-swap-full-snapshot-with-compacted-snapshot-atomically><strong>How to swap full snapshot with compacted snapshot atomically</strong></h4><p>Currently, full snapshots and the subsequent delta snapshots are grouped under same prefix path in the snapstore. When a full snapshot is created, it is placed under a prefix/directory with the name comprising of timestamp. Then subsequent delta snapshots are also pushed into the same directory. Thus each prefix/directory contains a single full snapshot and the subsequent delta snapshots. So far, it is the job of ETCDBR to start main Etcd process and snapshotter process which takes full snapshot and delta snapshot periodically. But as per our proposal, compaction will be running as parallel process to main Etcd process and snapshotter process. So we can&rsquo;t reliably co-ordinate between the processes to achieve switching to the compacted snapshot as the base snapshot atomically.</p><h5 id=current-directory-structure><strong>Current Directory Structure</strong></h5><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>- Backup-192345
</span></span><span style=display:flex><span>    - Full-Snapshot-0-1-192345
</span></span><span style=display:flex><span>    - Incremental-Snapshot-1-100-192355
</span></span><span style=display:flex><span>    - Incremental-Snapshot-100-200-192365
</span></span><span style=display:flex><span>    - Incremental-Snapshot-200-300-192375
</span></span><span style=display:flex><span>- Backup-192789
</span></span><span style=display:flex><span>    - Full-Snapshot-0-300-192789
</span></span><span style=display:flex><span>    - Incremental-Snapshot-300-400-192799
</span></span><span style=display:flex><span>    - Incremental-Snapshot-400-500-192809
</span></span><span style=display:flex><span>    - Incremental-Snapshot-500-600-192819
</span></span></code></pre></div><p>To solve the problem, proposal is:</p><ol><li>ETCDBR will take the first full snapshot after it starts main Etcd Process and snapshotter process. After taking the first full snapshot, snapshotter will continue taking full snapshots. On the other hand, ETCDBR compactor command will be run as periodic job in a separate pod and use the existing full or compacted snapshots to produce further compacted snapshots. Full snapshots and compacted snapshots will be named after same fashion. So, there is no need of any mechanism to choose which snapshots(among full and compacted snapshot) to consider as base snapshots.</li><li>Flatten the directory structure of backup folder. Save all the full snapshots, delta snapshots and compacted snapshots under same directory/prefix. Restorer will restore from full/compacted snapshots and delta snapshots sorted based on the revision numbers in name (or timestamp if the revision numbers are equal).</li></ol><h5 id=proposed-directory-structure><strong>Proposed Directory Structure</strong></h5><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>Backup :
</span></span><span style=display:flex><span>    - Full-Snapshot-0-1-192355 (Taken by snapshotter)
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-1-100-192365
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-100-200-192375
</span></span><span style=display:flex><span>    - Full-Snapshot-revision-0-200-192379 (Taken by snapshotter)
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-200-300-192385
</span></span><span style=display:flex><span>    - Full-Snapshot-revision-0-300-192386 (Taken by compaction job)
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-300-400-192396
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-400-500-192406
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-500-600-192416
</span></span><span style=display:flex><span>    - Full-Snapshot-revision-0-600-192419 (Taken by snapshotter)
</span></span><span style=display:flex><span>    - Full-Snapshot-revision-0-600-192420 (Taken by compaction job)
</span></span></code></pre></div><h5 id=what-happens-to-the-delta-snapshots-that-were-compacted>What happens to the delta snapshots that were compacted?</h5><p>The proposed <code>compaction</code> sub-command in <code>etcdbrctl</code> (and hence, the <code>CronJob</code> provisioned by <code>etcd-druid</code> that will schedule it at a regular interval) would only upload the compacted full snapshot.
It will not delete the snapshots (delta or full snapshots) that were compacted.
These snapshots which were superseded by a freshly uploaded compacted snapshot would follow the same life-cycle as other older snapshots.
I.e. they will be garbage collected according to the configured backup snapshot retention policy.
For example, if an <code>exponential</code> retention policy is configured and if compaction is done every <code>30m</code> then there might be at most <code>48</code> additional (compacted) full snapshots (<code>24h * 2</code>) in the backup for the latest day. As time rolls forward to the next day, these additional compacted snapshots (along with the delta snapshots that were compacted into them) will get garbage collected retaining only one full snapshot for the day before according to the retention policy.</p><h5 id=future-work><strong>Future work</strong></h5><p>In the future, we have plan to stop the snapshotter just after taking the first full snapshot. Then, the compaction job will be solely responsible for taking subsequent full snapshots. The directory structure would be looking like following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>Backup :
</span></span><span style=display:flex><span>    - Full-Snapshot-0-1-192355 (Taken by snapshotter)
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-1-100-192365
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-100-200-192375
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-200-300-192385
</span></span><span style=display:flex><span>    - Full-Snapshot-revision-0-300-192386 (Taken by compaction job)
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-300-400-192396
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-400-500-192406
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-500-600-192416
</span></span><span style=display:flex><span>    - Full-Snapshot-revision-0-600-192420 (Taken by compaction job)
</span></span></code></pre></div><h4 id=backward-compatibility>Backward Compatibility</h4><ol><li><strong>Restoration</strong> : The changes to handle the newly proposed backup directory structure must be backward compatible with older structures at least for restoration because we need have to restore from backups in the older structure. This includes the support for restoring from a backup without a metadata file if that is used in the actual implementation.</li><li><strong>Backup</strong> : For new snapshots (even on a backup containing the older structure), the new structure may be used. The new structure must be setup automatically including creating the base full snapshot.</li><li><strong>Garbage collection</strong> : The existing functionality of garbage collection of snapshots (full and incremental) according to the backup retention policy must be compatible with both old and new backup folder structure. I.e. the snapshots in the older backup structure must be retained in their own structure and the snapshots in the proposed backup structure should be retained in the proposed structure. Once all the snapshots in the older backup structure go out of the retention policy and are garbage collected, we can think of removing the support for older backup folder structure.</li></ol><p><strong>Note:</strong> Compactor will run parallel to current snapshotter process and work only if there is any full snapshot already present in the store. By current design, a full snapshot will be taken if there is already no full snapshot or the existing full snapshot is older than 24 hours. It is not limitation but a design choice. As per proposed design, the backup storage will contain both periodic full snapshots as well as periodic compacted snapshot. Restorer will pickup the base snapshot whichever is latest one.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4b238c4f12151eab9f3e4bd2376dc879>3.4 - 03 Scaling Up An Etcd Cluster</h1><h1 id=dep-03-scaling-up-a-single-node-to-multi-node-etcd-cluster-deployed-by-etcd-druid>DEP-03: Scaling-up a single-node to multi-node etcd cluster deployed by etcd-druid</h1><p>To mark a cluster for scale-up from single node to multi-node etcd, just patch the etcd custom resource&rsquo;s <code>.spec.replicas</code> from <code>1</code> to <code>3</code> (for example).</p><h2 id=challenges-for-scale-up>Challenges for scale-up</h2><ol><li>Etcd cluster with single replica don&rsquo;t have any peers, so no peer communication is required hence peer URL may or may not be TLS enabled. However, while scaling up from single node etcd to multi-node etcd, there will be a requirement to have peer communication between members of the etcd cluster. Peer communication is required for various reasons, for instance for members to sync up cluster state, data, and to perform leader election or any cluster wide operation like removal or addition of a member etc. Hence in a multi-node etcd cluster we need to have TLS enable peer URL for peer communication.</li><li>Providing the correct configuration to start new etcd members as it is different from boostrapping a cluster since these new etcd members will join an existing cluster.</li></ol><h2 id=approach>Approach</h2><p>We first went through the etcd doc of <a href=https://etcd.io/docs/v3.4/op-guide/runtime-configuration/#update-advertise-peer-urls>update-advertise-peer-urls</a> to find out information regarding peer URL updation. Interestingly, etcd doc has mentioned the following:</p><pre tabindex=0><code>To update the advertise peer URLs of a member, first update it explicitly via member command and then restart the member.
</code></pre><p>But we can&rsquo;t assume peer URL is not TLS enabled for single-node cluster as it depends on end-user. A user may or may not enable the TLS for peer URL for a single node etcd cluster. So, How do we detect whether peer URL was enabled or not when cluster is marked for scale-up?</p><h2 id=detecting-if-peerurl-tls-is-enabled-or-not>Detecting if peerURL TLS is enabled or not</h2><p>For this we use an annotation in member lease object <code>member.etcd.gardener.cloud/tls-enabled</code> set by backup-restore sidecar of etcd. As etcd configuration is provided by backup-restore, so it can find out whether TLS is enabled or not and accordingly set this annotation <code>member.etcd.gardener.cloud/tls-enabled</code> to either <code>true</code> or <code>false</code> in member lease object.
And with the help of this annotation and config-map values etcd-druid is able to detect whether there is a change in a peer URL or not.</p><h2 id=etcd-druid-helps-in-scaling-up-etcd-cluster>Etcd-Druid helps in scaling up etcd cluster</h2><p>Now, it is detected whether peer URL was TLS enabled or not for single node etcd cluster. Etcd-druid can now use this information to take action:</p><ul><li>If peer URL was already TLS enabled then no action is required from etcd-druid side. Etcd-druid can proceed with scaling up the cluster.</li><li>If peer URL was not TLS enabled then etcd-druid has to intervene and make sure peer URL should be TLS enabled first for the single node before marking the cluster for scale-up.</li></ul><h2 id=action-taken-by-etcd-druid-to-enable-the-peerurl-tls>Action taken by etcd-druid to enable the peerURL TLS</h2><ol><li>Etcd-druid will update the <code>{etcd.Name}-config</code> config-map with new config like initial-cluster,initial-advertise-peer-urls etc. Backup-restore will detect this change and update the member lease annotation to <code>member.etcd.gardener.cloud/tls-enabled: "true"</code>.</li><li>In case the peer URL TLS has been changed to <code>enabled</code>: Etcd-druid will add tasks to the deployment flow:<ul><li>Check if peer TLS has been enabled for existing StatefulSet pods, by checking the member leases for the annotation <code>member.etcd.gardener.cloud/tls-enabled</code>.</li><li>If peer TLS enablement is pending for any of the members, then check and patch the StatefulSet with the peer TLS volume mounts, if not already patched. This will cause a rolling update of the existing StatefulSet pods, which allows etcd-backup-restore to update the member peer URL in the etcd cluster.</li><li>Requeue this reconciliation flow until peer TLS has been enabled for all the existing etcd members.</li></ul></li></ol><h2 id=after-peerurl-is-tls-enabled>After PeerURL is TLS enabled</h2><p>After peer URL TLS enablement for single node etcd cluster, now etcd-druid adds a scale-up annotation: <code>gardener.cloud/scaled-to-multi-node</code> to the etcd statefulset and etcd-druid will patch the statefulsets <code>.spec.replicas</code> to <code>3</code>(for example). The statefulset controller will then bring up new pods(etcd with backup-restore as a sidecar). Now etcd&rsquo;s sidecar i.e backup-restore will check whether this member is already a part of a cluster or not and incase it is unable to check (may be due to some network issues) then backup-restore checks presence of this annotation: <code>gardener.cloud/scaled-to-multi-node</code> in etcd statefulset to detect scale-up. If it finds out it is the scale-up case then backup-restore adds new etcd member as a <a href=https://etcd.io/docs/v3.3/learning/learner/>learner</a> first and then starts the etcd learner by providing the correct configuration. Once learner gets in sync with the etcd cluster leader, it will get promoted to a voting member.</p><h2 id=providing-the-correct-etcd-config>Providing the correct etcd config</h2><p>As backup-restore detects that it&rsquo;s a scale-up scenario, backup-restore sets <code>initial-cluster-state</code> to <code>existing</code> as this member will join an existing cluster and it calculates the rest of the config from the updated config-map provided by etcd-druid.</p><p><img src=/__resources/03-scale-up-sequenceDiagram_76558b.png alt="Sequence diagram"></p><h2 id=future-improvements>Future improvements:</h2><p>The need of restarting etcd pods twice will change in the future. please refer: <a href=https://github.com/gardener/etcd-backup-restore/issues/538>https://github.com/gardener/etcd-backup-restore/issues/538</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-29c5d418724237fdcb014bcbfa83ad7b>3.5 - Add New Etcd Cluster Component</h1><h1 id=add-a-new-etcd-cluster-component>Add A New Etcd Cluster Component</h1><p><code>etcd-druid</code> defines an <a href=https://github.com/gardener/etcd-druid/blob/3383e0219a6c21c6ef1d5610db964cc3524807c8/internal/component/types.go#L42>Operator</a> which is responsible for creation, deletion and update of a resource that is created for an <code>Etcd</code> cluster. If you want to introduce a new resource for an <code>Etcd</code> cluster then you must do the following:</p><ul><li><p>Add a dedicated <code>package</code> for the resource under <a href=https://github.com/gardener/etcd-druid/tree/3383e0219a6c21c6ef1d5610db964cc3524807c8/internal/component>component</a>.</p></li><li><p>Implement <code>Operator</code> interface.</p></li><li><p>Define a new <a href=https://github.com/gardener/etcd-druid/blob/3383e0219a6c21c6ef1d5610db964cc3524807c8/internal/component/registry.go#L19>Kind</a> for this resource in the operator <a href=https://github.com/gardener/etcd-druid/blob/3383e0219a6c21c6ef1d5610db964cc3524807c8/internal/component/registry.go#L8>Registry</a>.</p></li><li><p>Every resource a.k.a <code>Component</code> needs to have the following set of default labels:</p><ul><li><code>app.kubernetes.io/name</code> - value of this label is the name of this component. Helper functions are defined <a href=https://github.com/gardener/etcd-druid/blob/master/api/v1alpha1/helper.go>here</a> to create the name of each component using the parent <code>Etcd</code> resource. Please define a new helper function to generate the name of your resource using the parent <code>Etcd</code> resource.</li><li><code>app.kubernetes.io/component</code> - value of this label is the type of the component. All component type label values are defined <a href=https://github.com/gardener/etcd-druid/blob/3383e0219a6c21c6ef1d5610db964cc3524807c8/internal/common/constants.go>here</a> where you can add an entry for your component.</li><li>In addition to the above component specific labels, each resource/component should have default labels defined on the <code>Etcd</code> resource. You can use <a href=https://github.com/gardener/etcd-druid/blob/3383e0219a6c21c6ef1d5610db964cc3524807c8/api/v1alpha1/helper.go#L124>GetDefaultLabels</a> function.</li></ul><blockquote><p>These labels are also part of <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/>recommended labels</a> by kubernetes.
NOTE: Constants for the label keys are already defined <a href=https://github.com/gardener/etcd-druid/blob/3383e0219a6c21c6ef1d5610db964cc3524807c8/api/v1alpha1/constants.go>here</a>.</p></blockquote></li><li><p>Ensure that there is no <code>wait</code> introduced in any <code>Operator</code> method implementation in your component. In case there are multiple steps to be executed in a sequence then re-queue the event with a special <a href=https://github.com/gardener/etcd-druid/blob/3383e0219a6c21c6ef1d5610db964cc3524807c8/internal/errors/errors.go#L19>error code</a> in case there is an error or if the pre-conditions check to execute the next step are not yet satisfied.</p></li><li><p>All errors should be wrapped with a custom <a href=https://github.com/gardener/etcd-druid/blob/3383e0219a6c21c6ef1d5610db964cc3524807c8/internal/errors/errors.go#L24>DruidError</a>.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-df25f0addd125646df40c38cadbb3cc3>3.6 - Changing Api</h1><h1 id=change-the-api>Change the API</h1><p>This guide provides detailed information on what needs to be done when the API needs to be changed.</p><p><code>etcd-druid</code> API follows the same API conventions and guidelines which Kubernetes defines and adopts. The Kubernetes <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md>API Conventions</a> as well as <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md>Changing the API</a> topics already provide a good overview and general explanation of the basic concepts behind it. We adhere to the principles laid down by Kubernetes.</p><h2 id=etcd-druid-api>Etcd Druid API</h2><p>The etcd-druid API is defined <a href=https://github.com/gardener/etcd-druid/tree/3383e0219a6c21c6ef1d5610db964cc3524807c8/api>here</a>.</p><p>!!! info
The current version of the API is <code>v1alpha1</code>. We are currently working on migration to <code>v1beta1</code> API.</p><h3 id=changing-the-api>Changing the API</h3><p>If there is a need to make changes to the API, then one should do the following:</p><ul><li>If new fields are added then ensure that these are added as <code>optional</code> fields. They should have the <code>+optional</code> comment and an <code>omitempty</code> JSON tag should be added against the field.</li><li>Ensure that all new fields or changing the existing fields are well documented with doc-strings.</li><li>Care should be taken that incompatible API changes should not be made in the same version of the API. If there is a real necessity to introduce a backward incompatible change then a newer version of the API should be created and an <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#webhook-conversion>API conversion webhook</a> should be put in place to support more than one version of the API.</li><li>After the changes to the API are finalized, run <code>make generate</code> to ensure that the changes are also reflected in the CRD.</li><li>If necessary, implement or adapt the validation for the API.</li><li>If necessary, adapt the <a href=https://github.com/gardener/etcd-druid/tree/3383e0219a6c21c6ef1d5610db964cc3524807c8/config/samples>samples</a> YAML manifests.</li><li>When opening a pull-request, always add a release note informing the end-users of the changes that are coming in.</li></ul><h3 id=removing-a-field>Removing a Field</h3><p>If field(s) needs to be removed permanently from the API, then one should ensure the following:</p><ul><li>Field should not be directly removed, instead first a deprecation notice should be put which should follow a well-defined deprecation period. Ensure that the release note in the pull-request is properly categorized so that this is easily visible to the end-users and clearly mentiones which field(s) have been deprecated. Clearly suggest a way in which clients need to adapt.</li><li>To allow sufficient time to the end-users to adapt to the API changes, deprecated field(s) should only be removed once the deprecation period is over. It is generally recommended that this be done in 2 stages:<ul><li><em>First stage:</em> Remove the code that refers to the deprecated fields. This ensures that the code no longer has dependency on the deprecated field(s).</li><li><em>Second Stage:</em> Remove the field from the API.</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-15ddc945eb7e155d36b61817aa394b27>3.7 - Configure Etcd Druid</h1><h1 id=etcd-druid-cli-flags>etcd-druid CLI Flags</h1><p><code>etcd-druid</code> process can be started with the following command line flags.</p><h2 id=command-line-flags>Command line flags</h2><h3 id=leader-election>Leader election</h3><p>If you wish to setup <code>etcd-druid</code> in high-availability mode then leader election needs to be enabled to ensure that at a time only one replica services the incoming events and does the reconciliation.</p><table><thead><tr><th>Flag</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td>enable-leader-election</td><td>Leader election provides the capability to select one replica as a leader where active reconciliation will happen. The other replicas will keep waiting for leadership change and not do active reconciliations.</td><td>false</td></tr><tr><td>leader-election-id</td><td>Name of the k8s lease object that leader election will use for holding the leader lock. By default etcd-druid will use lease resource lock for leader election which is also a <a href=https://kubernetes.io/docs/concepts/architecture/leases/#leader-election>natural usecase</a> for leases and is also recommended by k8s.</td><td>&ldquo;druid-leader-election&rdquo;</td></tr><tr><td>leader-election-resource-lock</td><td><em><strong>Deprecated</strong></em>: This flag will be removed in later version of druid. By default <code>lease.coordination.k8s.io</code> resources will be used for leader election resource locking for the controller manager.</td><td>&ldquo;leases&rdquo;</td></tr></tbody></table><h3 id=metrics>Metrics</h3><p><code>etcd-druid</code> exposes a <code>/metrics</code> endpoint which can be scrapped by tools like <a href=https://prometheus.io/>Prometheus</a>. If the default metrics endpoint configuration is not suitable then consumers can change it via the following options.</p><table><thead><tr><th>Flag</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td>metrics-bind-address</td><td>The IP address that the metrics endpoint binds to</td><td>""</td></tr><tr><td>metrics-port</td><td>The port used for the metrics endpoint</td><td>8080</td></tr><tr><td>metrics-addr</td><td>Duration to wait for after compaction job is completed, to allow Prometheus metrics to be scraped.<br><strong>Deprecated:</strong> Please use <code>--metrics-bind-address</code> and <code>--metrics-port</code> instead</td><td>&ldquo;:8080&rdquo;</td></tr></tbody></table><p>Metrics bind-address is computed by joining the host and port. By default its value is computed as <code>:8080</code>.</p><p>!!! tip
Ensure that the <code>metrics-port</code> is also reflected in the <code>etcd-druid</code> deployment specification.</p><h3 id=webhook-server>Webhook Server</h3><p>etcd-druid provides the following CLI flags to configure <a href=https://github.com/gardener/etcd-druid/blob/master/docs/concepts/webhooks.md>webhook</a> server. These CLI flags are used to construct a new <a href=https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/webhook#Server>webhook.Server</a> by configuring <a href=https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/webhook#Options>Options</a>.</p><table><thead><tr><th>Flag</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td>webhook-server-bind-address</td><td>It is the address that the webhook server will listen on.</td><td>""</td></tr><tr><td>webhook-server-port</td><td>Port is the port number that the webhook server will serve.</td><td>9443</td></tr><tr><td>webhook-server-tls-server-cert-dir</td><td>The path to a directory containing the server&rsquo;s TLS certificate and key (the files must be named tls.crt and tls.key respectively).</td><td>/etc/webhook-server-tls</td></tr></tbody></table><h3 id=etcd-components-webhook>Etcd-Components Webhook</h3><p>etcd-druid provisions and manages several Kubernetes resources which we call <a href=/docs/other-components/etcd-druid/concepts/etcd-cluster-components/><code>Etcd</code>cluster components</a>. To ensure that there is no accidental changes done to these managed resources, a webhook is put in place to check manual changes done to any managed etcd-cluster Kubernetes resource. It rejects most of these changes except a few. The details on how to enable the <code>etcd-components</code> webhook, which resources are protected and in which scenarios is the change allowed is documented <a href=/docs/other-components/etcd-druid/concepts/etcd-cluster-resource-protection/>here</a>.</p><p>Following CLI flags are provided to configure the <code>etcd-components</code> webhook:</p><table><thead><tr><th>Flag</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td>enable-etcd-components-webhook</td><td>Enable EtcdComponents Webhook to prevent unintended changes to resources managed by etcd-druid.</td><td>false</td></tr><tr><td>reconciler-service-account</td><td>The fully qualified name of the service account used by etcd-druid for reconciling etcd resources. If unspecified, the default service account mounted for etcd-druid will be used</td><td>etcd-druid-service-account</td></tr><tr><td>etcd-components-exempt-service-accounts</td><td>In case there is a need to allow changes to <code>Etcd</code> resources from external controllers like <code>vertical-pod-autoscaler</code> then one must list the <code>ServiceAaccount</code> that is used by each such controller.</td><td>""</td></tr></tbody></table><h3 id=reconcilers>Reconcilers</h3><p>Following set of flags configures the reconcilers running within etcd-druid. To know more about different reconcilers read <a href=/docs/other-components/etcd-druid/controllers/>this</a> document.</p><h4 id=etcd-reconciler>Etcd Reconciler</h4><table><thead><tr><th>Flag</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td>etcd-workers</td><td>Number of workers spawned for concurrent reconciles of <code>Etcd</code> resources.</td><td>3</td></tr><tr><td>enable-etcd-spec-auto-reconcile</td><td>If true then automatically reconciles Etcd Spec. If false, waits for explicit annotation <code>gardener.cloud/operation: reconcile</code> to be placed on the Etcd resource to trigger reconcile.</td><td>false</td></tr><tr><td>disable-etcd-serviceaccount-automount</td><td>For each <code>Etcd</code> cluster a <code>ServiceAccount</code> is created which is used by the <code>StatefulSet</code> pods and tied to <code>Role</code> via <code>RoleBinding</code>. If <code>false</code> then pods running as this <code>ServiceAccount</code> will have the API token automatically mounted. You can consider disabling it if you wish to use <a href=https://kubernetes.io/docs/concepts/storage/projected-volumes/#serviceaccounttoken>Projected Volumes</a> allowing one to set an <code>expirationSeconds</code> on the mounted token for better security. To use projected volumes ensure that you have set relevant <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#serviceaccount-token-volume-projection>kube-apiserver flags</a>.<br><strong>Note:</strong> With Kubernetes version >=1.24 projected service account token is the default. This means that we no longer need this flag. <a href=https://github.com/gardener/etcd-druid/issues/872>Issue #872</a> has been raised to address this.</td><td>false</td></tr><tr><td>etcd-status-sync-period</td><td><code>Etcd.Status</code> is periodically updated. This interval defines the status sync frequency.</td><td>15s</td></tr><tr><td>etcd-member-notready-threshold</td><td>Threshold after which an etcd member is considered not ready if the status was unknown before. This is currently used to update <a href=https://github.com/gardener/etcd-druid/blob/55efca1c8f6c852b0a4e97f08488ffec2eed0e68/api/v1alpha1/etcd.go#L360>EtcdMemberConditionStatus</a>.</td><td>5m</td></tr><tr><td>etcd-member-unknown-threshold</td><td>Threshold after which an etcd member is considered unknown. This is currently used to update <a href=https://github.com/gardener/etcd-druid/blob/55efca1c8f6c852b0a4e97f08488ffec2eed0e68/api/v1alpha1/etcd.go#L360>EtcdMemberConditionStatus</a>.</td><td>1m</td></tr><tr><td>ignore-operation-annotation</td><td>Specifies whether to ignore or honour the annotation <code>gardener.cloud/operation: reconcile</code> on resources to be reconciled.<br><strong>Deprecated:</strong> please use <code>--enable-etcd-spec-auto-reconcile</code> instead.</td><td>false</td></tr></tbody></table><h4 id=compaction-reconciler>Compaction Reconciler</h4><table><thead><tr><th>Flag</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td>enable-backup-compaction</td><td>Enable automatic compaction of etcd backups</td><td>false</td></tr><tr><td>compaction-workers</td><td>Number of workers that can be spawned for concurrent reconciles for compaction Jobs. The controller creates a backup compaction job if a certain etcd event threshold is reached. If compaction is enabled, the value for this flag must be greater than zero.</td><td>3</td></tr><tr><td>etcd-events-threshold</td><td>Defines the threshold in terms of total number of etcd events before a backup compaction job is triggered.</td><td>1000000</td></tr><tr><td>active-deadline-duration</td><td>Duration after which a running backup compaction job will be terminated.</td><td>3h</td></tr><tr><td>metrics-scrape-wait-duration</td><td>Duration to wait for after compaction job is completed, to allow Prometheus metrics to be scraped.</td><td>0s</td></tr></tbody></table><h4 id=etcd-copy-backup-task--secret-reconcilers>Etcd Copy-Backup Task & Secret Reconcilers</h4><table><thead><tr><th>Flag</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td>etcd-copy-backups-task-workers</td><td>Number of workers spawned for concurrent reconciles for <code>EtcdCopyBackupTask</code> resources.</td><td>3</td></tr><tr><td>secret-workers</td><td>Number of workers spawned for concurrent reconciles for secrets.</td><td>10</td></tr></tbody></table><h4 id=miscellaneous>Miscellaneous</h4><table><thead><tr><th>Flag</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td>feature-gates</td><td>A set of key=value pairs that describe feature gates for alpha/experimental features. Please check <a href=/docs/other-components/etcd-druid/deployment/feature-gates/>feature-gates</a> for more information.</td><td>""</td></tr><tr><td>disable-lease-cache</td><td>Disable cache for lease.coordination.k8s.io resources.</td><td>false</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-acd3d770acb4e69810b104c8d1534381>3.8 - Contribution</h1><h1 id=contributors-guide>Contributors Guide</h1><p><code>etcd-druid</code> is an actively maintained project which has organically evolved to be a mature and stable etcd operator. We welcome active participation from the community and to this end this guide serves as a good starting point.</p><h2 id=code-of-conduct>Code of Conduct</h2><p>All maintainers and contributors must abide by <a href=https://www.contributor-covenant.org/version/2/1/code_of_conduct/>Contributor Covenant</a>. Real progress can only happen in a collaborative environment which fosters mutual respect, openeness and disruptive innovation.</p><h2 id=developer-certificate-of-origin>Developer Certificate of Origin</h2><p>Due to legal reasons, contributors will be asked to accept a Developer Certificate of Origin (DCO) before they submit the first pull request to the IronCore project, this happens in an automated fashion during the submission process. We use <a href=https://developercertificate.org/>the standard DCO text of the Linux Foundation</a>.</p><h2 id=license>License</h2><p>Your contributions to <code>etcd-druid</code> must be licensed properly:</p><ul><li>Code contributions must be licensed under the <a href=http://www.apache.org/licenses/LICENSE-2.0>Apache 2.0 License</a>.</li><li>Documentation contributions must be licensed under the <a href=https://creativecommons.org/licenses/by/4.0/legalcode>Creative Commons Attribution 4.0 International License</a>.</li></ul><h2 id=contributing>Contributing</h2><p><code>etcd-druid</code> use Github to manage reviews of pull requests.</p><ul><li>If you are looking to make your first contribution, follow <a href=/docs/other-components/etcd-druid/contribution/#steps-to-contribute>Steps to Contribute</a>.</li><li>If you have a trivial fix or improvement, go ahead and create an issue first followed by a pull request.</li><li>If you plan to do something more involved, first discuss your ideas by creating an <a href=https://github.com/gardener/etcd-druid/issues>issue</a>. This will avoid unnecessary work and surely give you and us a good deal of inspiration.</li></ul><h2 id=steps-to-contribute>Steps to Contribute</h2><ul><li>If you wish to contribute and have not done that in the past, then first try and filter the list of issues with label <code>exp/beginner</code>. Once you find the issue that interests you, add a comment stating that you would like to work on it. This is to prevent duplicated efforts from contributors on the same issue.</li><li>If you have questions about one of the issues please comment on them and one of the maintainers will clarify it.</li></ul><p>We kindly ask you to follow the <a href=/docs/other-components/etcd-druid/raising-a-pr/>Pull Request Checklist</a> to ensure reviews can happen accordingly.</p><h2 id=issues-and-planning>Issues and Planning</h2><p>We use GitHub <a href=https://github.com/gardener/etcd-druid/issues>issues</a> to track bugs and enhancement requests. Please provide as much context as possible when you open an issue. The information you provide must be comprehensive enough to understand, reproduce the behavior and find related reports of that issue for the assignee. Therefore, contributors may use but aren&rsquo;t restricted to the issue template provided by the etcd-druid maintainers.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0baf9c7df729ed61a365e99fbc9ce823>3.9 - Controllers</h1><h1 id=controllers>Controllers</h1><p>etcd-druid is an operator to manage etcd clusters, and follows the <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/operator/><code>Operator</code></a> pattern for Kubernetes.
It makes use of the <a href=https://github.com/kubernetes-sigs/kubebuilder>Kubebuilder</a> framework which makes it quite easy to define Custom Resources (CRs) such as <code>Etcd</code>s and <code>EtcdCopyBackupTask</code>s through <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/><em>Custom Resource Definitions</em></a> (CRDs), and define controllers for these CRDs.
etcd-druid uses Kubebuilder to define the <code>Etcd</code> CR and its corresponding controllers.</p><p>All controllers that are a part of etcd-druid reside in package <code>internal/controller</code>, as sub-packages.</p><p>Etcd-druid currently consists of the following controllers, each having its own responsibility:</p><ul><li><em>etcd</em> : responsible for the reconciliation of the <code>Etcd</code> CR spec, which allows users to run etcd clusters within the specified Kubernetes cluster, and also responsible for periodically updating the <code>Etcd</code> CR status with the up-to-date state of the managed etcd cluster.</li><li><em>compaction</em> : responsible for <a href=/docs/other-components/etcd-druid/proposals/02-snapshot-compaction/>snapshot compaction</a>.</li><li><em>etcdcopybackupstask</em> : responsible for the reconciliation of the <code>EtcdCopyBackupsTask</code> CR, which helps perform the job of copying snapshot backups from one object store to another.</li><li><em>secret</em> : responsible in making sure <code>Secret</code>s being referenced by <code>Etcd</code> resources are not deleted while in use.</li></ul><h2 id=package-structure>Package Structure</h2><p>The typical package structure for the controllers that are part of etcd-druid is shown with the <em>compaction controller</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>internal/controller/compaction
</span></span><span style=display:flex><span>├── config.go
</span></span><span style=display:flex><span>├── reconciler.go
</span></span><span style=display:flex><span>└── register.go
</span></span></code></pre></div><ul><li><code>config.go</code>: contains all the logic for the configuration of the controller, including feature gate activations, CLI flag parsing and validations.</li><li><code>register.go</code>: contains the logic for registering the controller with the etcd-druid controller manager.</li><li><code>reconciler.go</code>: contains the controller reconciliation logic.</li></ul><p>Each controller package also contains auxiliary files which are relevant to that specific controller.</p><h2 id=controller-manager>Controller Manager</h2><p>A <em>manager</em> is first created for all controllers that are a part of etcd-druid.
The <em>controller manager</em> is responsible for all the controllers that are associated with CRDs.
Once the manager is <code>Start()</code>ed, all the controllers that are <em>registered</em> with it are started.</p><p>Each controller is built using a controller builder, configured with details such as the type of object being reconciled, owned objects whose owner object is reconciled, event filters (predicates), etc. <code>Predicates</code> are filters which allow controllers to filter which type of events the controller should respond to and which ones to ignore.</p><p>The logic relevant to the controller manager like the creation of the controller manager and registering each of the controllers with the manager, is contained in <a href=https://github.com/gardener/etcd-druid/blob/master/internal/manager/manager.go><code>internal/manager/manager.go</code></a>.</p><h2 id=etcd-controller>Etcd Controller</h2><p>The <em>etcd controller</em> is responsible for the reconciliation of the <code>Etcd</code> resource spec and status. It handles the provisioning and management of the etcd cluster. Different components that are required for the functioning of the cluster like <code>Leases</code>, <code>ConfigMap</code>s, and the <code>Statefulset</code> for the etcd cluster are all deployed and managed by the <em>etcd controller</em>.</p><p>Additionally, <em>etcd controller</em> also periodically updates the <code>Etcd</code> resource status with the latest available information from the etcd cluster, as well as results and errors from the recent-most reconciliation of the <code>Etcd</code> resource spec.</p><p>The <em>etcd controller</em> is essential to the functioning of the etcd cluster and etcd-druid, thus the minimum number of worker threads is 1 (default being 3), controlled by the CLI flag <code>--etcd-workers</code>.</p><h3 id=etcd-spec-reconciliation><code>Etcd</code> Spec Reconciliation</h3><p>While building the controller, an event filter is set such that the behavior of the controller, specifically for <code>Etcd</code> update operations, depends on the <code>gardener.cloud/operation: reconcile</code> <em>annotation</em>. This is controlled by the <code>--enable-etcd-spec-auto-reconcile</code> CLI flag, which, if set to <code>false</code>, tells the controller to perform reconciliation only when this annotation is present. If the flag is set to <code>true</code>, the controller will reconcile the etcd cluster anytime the <code>Etcd</code> spec, and thus <code>generation</code>, changes, and the next queued event for it is triggered.</p><p>!!! note
Creation and deletion of <code>Etcd</code> resources are not affected by the above flag or annotation.</p><p>The reason this filter is present is that any disruption in the <code>Etcd</code> resource due to reconciliation (due to changes in the <code>Etcd</code> spec, for example) while workloads are being run would cause unwanted downtimes to the etcd cluster. Hence, any user who wishes to avoid such disruptions, can choose to set the <code>--enable-etcd-spec-auto-reconcile</code> CLI flag to <code>false</code>. An example of this is Gardener&rsquo;s <a href=https://github.com/gardener/gardener/blob/676d1bd9e95d80b9f4bc9c56807806031da5d1ce/docs/concepts/gardenlet.md>gardenlet</a>, which reconciles the <code>Etcd</code> resource only during a shoot cluster&rsquo;s <a href=https://github.com/gardener/gardener/blob/676d1bd9e95d80b9f4bc9c56807806031da5d1ce/docs/usage/shoot/shoot_maintenance.md><em>maintenance window</em></a>.</p><p>The controller adds a finalizer to the <code>Etcd</code> resource in order to ensure that it does not get deleted until all dependent resources managed by etcd-druid, aka managed components, are properly cleaned up. Only the <em>etcd controller</em> can delete a resource once it adds finalizers to it. This ensures that the proper deletion flow steps are followed while deleting the resource. During deletion flow, managed components are deleted in parallel.</p><h3 id=etcd-status-updates><code>Etcd</code> Status Updates</h3><p>The <code>Etcd</code> resource status is updated periodically by <code>etcd controller</code>, the interval for which is determined by the CLI flag <code>--etcd-status-sync-period</code>.</p><p>Status fields of the <code>Etcd</code> resource such as <code>LastOperation</code>, <code>LastErrors</code> and <code>ObservedGeneration</code>, are updated to reflect the result of the recent reconciliation of the <code>Etcd</code> resource spec.</p><ul><li><code>LastOperation</code> holds information about the last operation performed on the etcd cluster, indicated by fields <code>Type</code>, <code>State</code>, <code>Description</code> and <code>LastUpdateTime</code>. Additionally, a field <code>RunID</code> indicates the unique ID assigned to the specific reconciliation run, to allow for better debugging of issues.</li><li><code>LastErrors</code> is a slice of errors encountered by the last reconciliation run. Each error consists of fields <code>Code</code> to indicate the custom etcd-druid error code for the error, a human-readable <code>Description</code>, and the <code>ObservedAt</code> time when the error was seen.</li><li><code>ObservedGeneration</code> indicates the latest <code>generation</code> of the <code>Etcd</code> resource that etcd-druid has &ldquo;observed&rdquo; and consequently reconciled. It helps identify whether a change in the <code>Etcd</code> resource spec was acted upon by druid or not.</li></ul><p>Status fields of the <code>Etcd</code> resource which correspond to the <code>StatefulSet</code> like <code>CurrentReplicas</code>, <code>ReadyReplicas</code> and <code>Replicas</code> are updated to reflect those of the <code>StatefulSet</code> by the controller.</p><p>Status fields related to the etcd cluster itself, such as <code>Members</code>, <code>PeerUrlTLSEnabled</code> and <code>Ready</code> are updated as follows:</p><ul><li>Cluster Membership: The controller updates the information about etcd cluster membership like <code>Role</code>, <code>Status</code>, <code>Reason</code>, <code>LastTransitionTime</code> and identifying information like the <code>Name</code> and <code>ID</code>. For the <code>Status</code> field, the member is checked for the <em>Ready</em> condition, where the member can be in <code>Ready</code>, <code>NotReady</code> and <code>Unknown</code> statuses.</li></ul><p><code>Etcd</code> resource conditions are indicated by status field <code>Conditions</code>. The condition checks that are currently performed are:</p><ul><li><code>AllMembersReady</code>: indicates readiness of all members of the etcd cluster.</li><li><code>Ready</code>: indicates overall readiness of the etcd cluster in serving traffic.</li><li><code>BackupReady</code>: indicates health of the etcd backups, i.e., whether etcd backups are being taken regularly as per schedule. This condition is applicable only when backups are enabled for the etcd cluster.</li><li><code>DataVolumesReady</code>: indicates health of the persistent volumes containing the etcd data.</li></ul><h2 id=compaction-controller>Compaction Controller</h2><p>The <em>compaction controller</em> deploys the snapshot compaction job whenever required. To understand the rationale behind this controller, please read <a href=/docs/other-components/etcd-druid/proposals/02-snapshot-compaction/>snapshot-compaction.md</a>.
The controller watches the number of events accumulated as part of delta snapshots in the etcd cluster&rsquo;s backups, and triggers a snapshot compaction when the number of delta events crosses the set threshold, which is configurable through the <code>--etcd-events-threshold</code> CLI flag (1M events by default).</p><p>The controller watches for changes in <em>snapshot</em> <code>Leases</code> associated with <code>Etcd</code> resources.
It checks the full and delta snapshot <code>Leases</code> and calculates the difference in events between the latest delta snapshot and the previous full snapshot, and initiates the compaction job if the event threshold is crossed.</p><p>The number of worker threads for the <em>compaction controller</em> needs to be greater than or equal to 0 (default 3), controlled by the CLI flag <code>--compaction-workers</code>.
This is unlike other controllers which need at least one worker thread for the proper functioning of etcd-druid as snapshot compaction is not a core functionality for the etcd clusters to be deployed.
The compaction controller should be explicitly enabled by the user, through the <code>--enable-backup-compaction</code> CLI flag.</p><h2 id=etcdcopybackupstask-controller>EtcdCopyBackupsTask Controller</h2><p>The <em>etcdcopybackupstask controller</em> is responsible for deploying the <a href=https://github.com/gardener/etcd-backup-restore/blob/master/cmd/copy.go><code>etcdbrctl copy</code></a> command as a job.
This controller reacts to create/update events arising from EtcdCopyBackupsTask resources, and deploys the <code>EtcdCopyBackupsTask</code> job with source and target backup storage providers as arguments, which are derived from source and target bucket secrets referenced by the <code>EtcdCopyBackupsTask</code> resource.</p><p>The number of worker threads for the <em>etcdcopybackupstask controller</em> needs to be greater than or equal to 0 (default being 3), controlled by the CLI flag <code>--etcd-copy-backups-task-workers</code>.
This is unlike other controllers who need at least one worker thread for the proper functioning of etcd-druid as <code>EtcdCopyBackupsTask</code> is not a core functionality for the etcd clusters to be deployed.</p><h2 id=secret-controller>Secret Controller</h2><p>The <em>secret controller</em>&rsquo;s primary responsibility is to add a finalizer on <code>Secret</code>s referenced by the <code>Etcd</code> resource.
The <em>secret controller</em> is registered for <code>Secret</code>s, and the controller keeps a watch on the <code>Etcd</code> CR.
This finalizer is added to ensure that <code>Secret</code>s which are referenced by the <code>Etcd</code> CR aren&rsquo;t deleted while still being used by the <code>Etcd</code> resource.</p><p>Events arising from the <code>Etcd</code> resource are mapped to a list of <code>Secret</code>s such as backup and TLS secrets that are referenced by the <code>Etcd</code> resource, and are enqueued into the request queue, which the reconciler then acts on.</p><p>The number of worker threads for the secret controller must be at least 1 (default being 10) for this core controller, controlled by the CLI flag <code>--secret-workers</code>, since the referenced TLS and infrastructure access secrets are essential to the proper functioning of the etcd cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-576e725c12b217f7eef3768da38537e7>3.10 - DEP Title</h1><h1 id=dep-nn-your-short-descriptive-title>DEP-NN: Your short, descriptive title</h1><h2 id=summary>Summary</h2><h2 id=motivation>Motivation</h2><h3 id=goals>Goals</h3><h3 id=non-goals>Non-Goals</h3><h2 id=proposal>Proposal</h2><h2 id=alternatives>Alternatives</h2></div><div class=td-content style=page-break-before:always><h1 id=pg-88b01cb90387569d89b21d0c4d4fadea>3.11 - Dependency Management</h1><h1 id=dependency-management>Dependency Management</h1><p>We use <a href=https://go.dev/wiki/Modules>Go Modules</a> for dependency management. In order to add a new package dependency to the project, you can perform <code>go get &lt;package@version></code> or edit the <code>go.mod</code> file and append the package along with the version you want to use.</p><h2 id=organize-dependencies>Organize Dependencies</h2><p>Unfortunately go does not differentiate between <code>dev</code> and <code>test</code> dependencies. It becomes cleaner to organize <code>dev</code> and <code>test</code> dependencies in their respective <code>require</code> clause which gives a clear view on existing set of dependencies. The goal is to keep the dependencies to a minimum and only add a dependency when absolutely required.</p><h2 id=updating-dependencies>Updating Dependencies</h2><p>The <code>Makefile</code> contains a rule called <code>tidy</code> which performs <a href=https://go.dev/ref/mod#go-mod-tidy>go mod tidy</a> which ensures that the <code>go.mod</code> file matches the source code in the module. It adds any missing module requirements necessary to build the current module’s packages and dependencies, and it removes requirements on modules that don’t provide any relevant packages. It also adds any missing entries to <code>go.sum</code> and removes unnecessary entries.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make tidy
</span></span></code></pre></div><p>!!! warning
Make sure that you test the code after you have updated the dependencies!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-aa26efc084acf438610aade3f996af59>3.12 - Etcd Cluster Components</h1><h1 id=etcd-cluster-components>Etcd Cluster Components</h1><p>For every <code>Etcd</code> cluster that is provisioned by <code>etcd-druid</code> it deploys a set of resources. Following sections provides information and code reference to each such resource.</p><h2 id=statefulset>StatefulSet</h2><p><a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a> is the primary kubernetes resource that gets provisioned for an etcd cluster.</p><ul><li><p>Replicas for the StatefulSet are derived from <code>Etcd.Spec.Replicas</code> in the custom resource.</p></li><li><p>Each pod comprises of two containers:</p><ul><li><p><code>etcd-wrapper</code> : This is the main container which runs an etcd process.</p></li><li><p><code>etcd-backup-restore</code> : This is a side-container which does the following:</p><ul><li>Orchestrates the initialization of etcd. This includes validation of any existing etcd data directory, restoration in case of corrupt etcd data directory files for a single-member etcd cluster.</li><li>Periodically renewes member lease.</li><li>Optionally takes schedule and thresold based delta and full snapshots and pushes them to a configured object store.</li><li>Orchestrates scheduled etcd-db defragmentation.</li></ul><blockquote><p>NOTE: This is not a complete list of functionalities offered out of <code>etcd-backup-restore</code>.</p></blockquote></li></ul></li></ul><p><strong>Code reference:</strong> <a href=https://github.com/gardener/etcd-druid/tree/480213808813c5282b19aff5f3fd6868529e779c/internal/component/statefulset>StatefulSet-Component</a></p><blockquote><p>For detailed information on each container you can visit <a href=https://github.com/gardener/etcd-wrapper>etcd-wrapper</a> and <a href=https://github.com/gardener/etcd-backup-restore>etcd-backup-restore</a> respositories.</p></blockquote><h2 id=configmap>ConfigMap</h2><p>Every <code>etcd</code> member requires <a href=https://etcd.io/docs/v3.4/op-guide/configuration/>configuration</a> with which it must be started. <code>etcd-druid</code> creates a <a href=https://kubernetes.io/docs/concepts/configuration/configmap/>ConfigMap</a> which gets mounted onto the <code>etcd-backup-restore</code> container. <code>etcd-backup-restore</code> container will modify the etcd configuration and serve it to the <code>etcd-wrapper</code> container upon request.</p><p><strong>Code reference:</strong> <a href=https://github.com/gardener/etcd-druid/tree/480213808813c5282b19aff5f3fd6868529e779c/internal/component/configmap>ConfigMap-Component</a></p><h2 id=poddisruptionbudget>PodDisruptionBudget</h2><p>An etcd cluster requires quorum for all write operations. Clients can additionally configure quorum based reads as well to ensure <a href=https://jepsen.io/consistency/models/linearizable>linearizable</a> reads (kube-apiserver&rsquo;s etcd client is configured for linearizable reads and writes). In a cluster of size 3, only 1 member failure is tolerated. <a href=https://etcd.io/docs/v3.3/faq/#what-is-failure-tolerance>Failure tolerance</a> for an etcd cluster with replicas <code>n</code> is computed as <code>(n-1)/2</code>.</p><p>To ensure that etcd pods are not evicted more than its failure tolerance, <code>etcd-druid</code> creates a <a href=https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets>PodDisruptionBudget</a>.</p><p>!!! note
For a single node etcd cluster a <code>PodDisruptionBudget</code> will be created, however <code>pdb.spec.minavailable</code> is set to 0 effectively disabling it.</p><p><strong>Code reference:</strong> <a href=https://github.com/gardener/etcd-druid/tree/480213808813c5282b19aff5f3fd6868529e779c/internal/component/poddistruptionbudget>PodDisruptionBudget-Component</a></p><h2 id=serviceaccount>ServiceAccount</h2><p><code>etch-backup-restore</code> container running as a side-car in every etcd-member, requires permissions to access resources like <code>Lease</code>, <code>StatefulSet</code> etc. A dedicated <a href=https://kubernetes.io/docs/concepts/security/service-accounts/>ServiceAccount</a> is created per <code>Etcd</code> cluster for this purpose.</p><p><strong>Code reference:</strong> <a href=https://github.com/gardener/etcd-druid/tree/3383e0219a6c21c6ef1d5610db964cc3524807c8/internal/component/serviceaccount>ServiceAccount-Component</a></p><h2 id=role--rolebinding>Role & RoleBinding</h2><p><code>etch-backup-restore</code> container running as a side-car in every etcd-member, requires permissions to access resources like <code>Lease</code>, <code>StatefulSet</code> etc. A dedicated <a href=/docs/other-components/etcd-druid/concepts/etcd-cluster-components/>Role</a> and <a href=/docs/other-components/etcd-druid/concepts/etcd-cluster-components/>RoleBinding</a> is created and linked to the <a href=https://kubernetes.io/docs/concepts/security/service-accounts/>ServiceAccount</a> created per <code>Etcd</code> cluster.</p><p><strong>Code reference:</strong> <a href=https://github.com/gardener/etcd-druid/tree/3383e0219a6c21c6ef1d5610db964cc3524807c8/internal/component/role>Role-Component</a> & <a href=https://github.com/gardener/etcd-druid/tree/master/internal/component/rolebinding>RoleBinding-Component</a></p><h2 id=client--peer-service>Client & Peer Service</h2><p>To enable clients to connect to an etcd cluster a ClusterIP <code>Client</code> <a href=https://kubernetes.io/docs/concepts/services-networking/service/>Service</a> is created. To enable <code>etcd</code> members to talk to each other(for discovery, leader-election, raft consensus etc.) <code>etcd-druid</code> also creates a <a href=https://kubernetes.io/docs/concepts/services-networking/service/#headless-services>Headless Service</a>.</p><p><strong>Code reference:</strong> <a href=https://github.com/gardener/etcd-druid/tree/480213808813c5282b19aff5f3fd6868529e779c/internal/component/clientservice>Client-Service-Component</a> & <a href=https://github.com/gardener/etcd-druid/tree/480213808813c5282b19aff5f3fd6868529e779c/internal/component/peerservice>Peer-Service-Component</a></p><h2 id=member-lease>Member Lease</h2><p>Every member in an <code>Etcd</code> cluster has a dedicated <a href=https://kubernetes.io/docs/concepts/architecture/leases/>Lease</a> that gets created which signifies that the member is alive. It is the responsibility of the <code>etcd-backup-store</code> side-car container to periodically renew the lease.</p><p>!!! note
Today the lease object is also used to indicate the member-ID and the role of the member in an etcd cluster. Possible roles are <code>Leader</code>, <code>Member</code>(which denotes that this is a member but not a leader). This will change in the future with <a href=https://github.com/gardener/etcd-druid/blob/3383e0219a6c21c6ef1d5610db964cc3524807c8/docs/proposals/04-etcd-member-custom-resource.md>EtcdMember resource</a>.</p><p><strong>Code reference:</strong> <a href=https://github.com/gardener/etcd-druid/tree/3383e0219a6c21c6ef1d5610db964cc3524807c8/internal/component/memberlease>Member-Lease-Component</a></p><h2 id=delta--full-snapshot-leases>Delta & Full Snapshot Leases</h2><p>One of the responsibilities of <code>etcd-backup-restore</code> container is to take periodic or threshold based snapshots (delta and full) of the etcd DB. Today <code>etcd-backup-restore</code> communicates the end-revision of the latest full/delta snapshots to <code>etcd-druid</code> operator via leases.</p><p><code>etcd-druid</code> creates two <a href=https://kubernetes.io/docs/concepts/architecture/leases/>Lease</a> resources one for delta and another for full snapshot. This information is used by the operator to trigger <a href=/docs/other-components/etcd-druid/proposals/02-snapshot-compaction/>snapshot-compaction</a> jobs. Snapshot leases are also used to derive the health of backups which gets updated in the <code>Status</code> subresource of every <code>Etcd</code> resource.</p><blockquote><p>In future these leases will be replaced by <a href=https://github.com/gardener/etcd-druid/blob/3383e0219a6c21c6ef1d5610db964cc3524807c8/docs/proposals/04-etcd-member-custom-resource.md>EtcdMember resource</a>.</p></blockquote><p><strong>Code reference:</strong> <a href=https://github.com/gardener/etcd-druid/tree/3383e0219a6c21c6ef1d5610db964cc3524807c8/internal/component/snapshotlease>Snapshot-Lease-Component</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-a5f59ce0cf46d8db6541b48cc05bb575>3.13 - Etcd Cluster Resource Protection</h1><h1 id=etcd-cluster-resource-protection>Etcd Cluster Resource Protection</h1><p><code>etcd-druid</code> provisions and manages <a href=/docs/other-components/etcd-druid/concepts/etcd-cluster-components/>kubernetes resources (a.k.a components)</a> for each <code>Etcd</code> cluster. To ensure that each component&rsquo;s specification is in line with the configured attributes defined in <code>Etcd</code> custom resource and to protect unintended changes done to any of these <em>managed components</em> a <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/>Validating Webhook</a> is employed.</p><p><a href=https://github.com/gardener/etcd-druid/tree/55efca1c8f6c852b0a4e97f08488ffec2eed0e68/internal/webhook/etcdcomponents>Etcd Components Webhook</a> is the <em>validating webhook</em> which prevents unintended <em>UPDATE</em> and <em>DELETE</em> operations on all managed resources. Following sections describe what is prohibited and in which specific conditions the changes are permitted.</p><h2 id=configure-etcd-components-webhook>Configure Etcd Components Webhook</h2><p>Prerequisite to enable the validation webhook is to <a href=/docs/other-components/etcd-druid/deployment/configure-etcd-druid/#webhook-server>configure the Webhook Server</a>. Additionally you need to enable the <code>Etcd Components</code> validating webhook and optionally configure other options. You can look at all the options <a href=/docs/other-components/etcd-druid/deployment/configure-etcd-druid/#etcd-components-webhook>here</a>.</p><h2 id=what-is-allowed>What is allowed?</h2><p>Modifications to managed resources under the following circumstances will be allowed:</p><ul><li><code>Create</code> and <code>Connect</code> operations are allowed and no validation is done.</li><li>Changes to a kubernetes resource (e.g. StatefulSet, ConfigMap etc) not managed by etcd-druid are allowed.</li><li>Changes to a resource whose Group-Kind is amongst the resources managed by etcd-druid but does not have a parent <code>Etcd</code> resource are allowed.</li><li>It is possible that an operator wishes to explicitly disable etcd-component protection. This can be done by setting <code>druid.gardener.cloud/disable-etcd-component-protection</code> annotation on an <code>Etcd</code> resource. If this annotation is present then changes to managed components will be allowed.</li><li>If <code>Etcd</code> resource has a deletion timestamp set indicating that it is marked for deletion and is awaiting etcd-druid to delete all managed resources then deletion requests for all managed resources for this etcd cluster will be allowed if:<ul><li>The deletion request has come from a <code>ServiceAccount</code> associated to etcd-druid. If not explicitly specified via <code>--reconciler-service-account</code> then a <a href=https://github.com/gardener/etcd-druid/blob/55efca1c8f6c852b0a4e97f08488ffec2eed0e68/internal/webhook/etcdcomponents/config.go#L23>default-reconciler-service-account</a> will be assumed.</li><li>The deletion request has come from a <code>ServiceAccount</code> configured via <code>--etcd-components-webhook-exempt-service-accounts</code>.</li></ul></li><li><code>Lease</code> objects are periodically updated by each etcd member pod. A single <code>ServiceAccount</code> is created for all members. <code>Update</code> operation on <code>Lease</code> objects from <a href=https://github.com/gardener/etcd-druid/blob/55efca1c8f6c852b0a4e97f08488ffec2eed0e68/api/v1alpha1/helper.go#L28>this ServiceAccount</a> is allowed.</li><li>If an active reconciliation is in-progress then only allow operations that are initiated by etcd-druid.</li><li>If no active reconciliation is currently in-progress, then allow updates to managed resource from <code>ServiceAccounts</code> configured via <code>--etcd-components-webhook-exempt-service-accounts</code>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f56c9ed1dc74841632b13995e1e1caea>3.14 - Etcd Druid Api</h1><h1 id=api-reference>API Reference</h1><h2 id=packages>Packages</h2><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#druidgardenercloudv1alpha1>druid.gardener.cloud/v1alpha1</a></li></ul><h2 id=druidgardenercloudv1alpha1>druid.gardener.cloud/v1alpha1</h2><p>Package v1alpha1 contains API Schema definitions for the druid v1alpha1 API group</p><h3 id=resource-types>Resource Types</h3><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcd>Etcd</a></li><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdcopybackupstask>EtcdCopyBackupsTask</a></li></ul><h4 id=backupspec>BackupSpec</h4><p>BackupSpec defines parameters associated with the full and delta snapshots of etcd.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdspec>EtcdSpec</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>port</code> <em>integer</em></td><td>Port define the port on which etcd-backup-restore server will be exposed.</td><td></td><td></td></tr><tr><td><code>tls</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#tlsconfig>TLSConfig</a></em></td><td></td><td></td><td></td></tr><tr><td><code>image</code> <em>string</em></td><td>Image defines the etcd container image and tag</td><td></td><td></td></tr><tr><td><code>store</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#storespec>StoreSpec</a></em></td><td>Store defines the specification of object store provider for storing backups.</td><td></td><td></td></tr><tr><td><code>resources</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#resourcerequirements-v1-core>ResourceRequirements</a></em></td><td>Resources defines compute Resources required by backup-restore container.<br>More info: <a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/</a></td><td></td><td></td></tr><tr><td><code>compactionResources</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#resourcerequirements-v1-core>ResourceRequirements</a></em></td><td>CompactionResources defines compute Resources required by compaction job.<br>More info: <a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/</a></td><td></td><td></td></tr><tr><td><code>fullSnapshotSchedule</code> <em>string</em></td><td>FullSnapshotSchedule defines the cron standard schedule for full snapshots.</td><td></td><td></td></tr><tr><td><code>garbageCollectionPolicy</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#garbagecollectionpolicy>GarbageCollectionPolicy</a></em></td><td>GarbageCollectionPolicy defines the policy for garbage collecting old backups</td><td></td><td>Enum: [Exponential LimitBased]<br></td></tr><tr><td><code>maxBackupsLimitBasedGC</code> <em>integer</em></td><td>MaxBackupsLimitBasedGC defines the maximum number of Full snapshots to retain in Limit Based GarbageCollectionPolicy<br>All full snapshots beyond this limit will be garbage collected.</td><td></td><td></td></tr><tr><td><code>garbageCollectionPeriod</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#duration-v1-meta>Duration</a></em></td><td>GarbageCollectionPeriod defines the period for garbage collecting old backups</td><td></td><td></td></tr><tr><td><code>deltaSnapshotPeriod</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#duration-v1-meta>Duration</a></em></td><td>DeltaSnapshotPeriod defines the period after which delta snapshots will be taken</td><td></td><td></td></tr><tr><td><code>deltaSnapshotMemoryLimit</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#quantity-resource-api>Quantity</a></em></td><td>DeltaSnapshotMemoryLimit defines the memory limit after which delta snapshots will be taken</td><td></td><td></td></tr><tr><td><code>deltaSnapshotRetentionPeriod</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#duration-v1-meta>Duration</a></em></td><td>DeltaSnapshotRetentionPeriod defines the duration for which delta snapshots will be retained, excluding the latest snapshot set.<br>The value should be a string formatted as a duration (e.g., &lsquo;1s&rsquo;, &lsquo;2m&rsquo;, &lsquo;3h&rsquo;, &lsquo;4d&rsquo;)</td><td></td><td>Pattern: <code>^([0-9][0-9]*([.][0-9]+)?(s|m|h|d))+$</code><br>Type: string<br></td></tr><tr><td><code>compression</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#compressionspec>CompressionSpec</a></em></td><td>SnapshotCompression defines the specification for compression of Snapshots.</td><td></td><td></td></tr><tr><td><code>enableProfiling</code> <em>boolean</em></td><td>EnableProfiling defines if profiling should be enabled for the etcd-backup-restore-sidecar</td><td></td><td></td></tr><tr><td><code>etcdSnapshotTimeout</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#duration-v1-meta>Duration</a></em></td><td>EtcdSnapshotTimeout defines the timeout duration for etcd FullSnapshot operation</td><td></td><td></td></tr><tr><td><code>leaderElection</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#leaderelectionspec>LeaderElectionSpec</a></em></td><td>LeaderElection defines parameters related to the LeaderElection configuration.</td><td></td><td></td></tr></tbody></table><h4 id=clientservice>ClientService</h4><p>ClientService defines the parameters of the client service that a user can specify</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdconfig>EtcdConfig</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>annotations</code> <em>object (keys:string, values:string)</em></td><td>Annotations specify the annotations that should be added to the client service</td><td></td><td></td></tr><tr><td><code>labels</code> <em>object (keys:string, values:string)</em></td><td>Labels specify the labels that should be added to the client service</td><td></td><td></td></tr></tbody></table><h4 id=compactionmode>CompactionMode</h4><p><em>Underlying type:</em> <em>string</em></p><p>CompactionMode defines the auto-compaction-mode: &lsquo;periodic&rsquo; or &lsquo;revision&rsquo;.
&lsquo;periodic&rsquo; for duration based retention and &lsquo;revision&rsquo; for revision number based retention.</p><p><em>Validation:</em></p><ul><li>Enum: [periodic revision]</li></ul><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#sharedconfig>SharedConfig</a></li></ul><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>periodic</code></td><td>Periodic is a constant to set auto-compaction-mode &lsquo;periodic&rsquo; for duration based retention.<br></td></tr><tr><td><code>revision</code></td><td>Revision is a constant to set auto-compaction-mode &lsquo;revision&rsquo; for revision number based retention.<br></td></tr></tbody></table><h4 id=compressionpolicy>CompressionPolicy</h4><p><em>Underlying type:</em> <em>string</em></p><p>CompressionPolicy defines the type of policy for compression of snapshots.</p><p><em>Validation:</em></p><ul><li>Enum: [gzip lzw zlib]</li></ul><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#compressionspec>CompressionSpec</a></li></ul><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>gzip</code></td><td>GzipCompression is constant for gzip compression policy.<br></td></tr><tr><td><code>lzw</code></td><td>LzwCompression is constant for lzw compression policy.<br></td></tr><tr><td><code>zlib</code></td><td>ZlibCompression is constant for zlib compression policy.<br></td></tr></tbody></table><h4 id=compressionspec>CompressionSpec</h4><p>CompressionSpec defines parameters related to compression of Snapshots(full as well as delta).</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#backupspec>BackupSpec</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>enabled</code> <em>boolean</em></td><td></td><td></td><td></td></tr><tr><td><code>policy</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#compressionpolicy>CompressionPolicy</a></em></td><td></td><td></td><td>Enum: [gzip lzw zlib]<br></td></tr></tbody></table><h4 id=condition>Condition</h4><p>Condition holds the information about the state of a resource.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdcopybackupstaskstatus>EtcdCopyBackupsTaskStatus</a></li><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdstatus>EtcdStatus</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>type</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#conditiontype>ConditionType</a></em></td><td>Type of the Etcd condition.</td><td></td><td></td></tr><tr><td><code>status</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#conditionstatus>ConditionStatus</a></em></td><td>Status of the condition, one of True, False, Unknown.</td><td></td><td></td></tr><tr><td><code>lastTransitionTime</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#time-v1-meta>Time</a></em></td><td>Last time the condition transitioned from one status to another.</td><td></td><td></td></tr><tr><td><code>lastUpdateTime</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#time-v1-meta>Time</a></em></td><td>Last time the condition was updated.</td><td></td><td></td></tr><tr><td><code>reason</code> <em>string</em></td><td>The reason for the condition&rsquo;s last transition.</td><td></td><td></td></tr><tr><td><code>message</code> <em>string</em></td><td>A human-readable message indicating details about the transition.</td><td></td><td></td></tr></tbody></table><h4 id=conditionstatus>ConditionStatus</h4><p><em>Underlying type:</em> <em>string</em></p><p>ConditionStatus is the status of a condition.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#condition>Condition</a></li></ul><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>True</code></td><td>ConditionTrue means a resource is in the condition.<br></td></tr><tr><td><code>False</code></td><td>ConditionFalse means a resource is not in the condition.<br></td></tr><tr><td><code>Unknown</code></td><td>ConditionUnknown means Gardener can&rsquo;t decide if a resource is in the condition or not.<br></td></tr><tr><td><code>Progressing</code></td><td>ConditionProgressing means the condition was seen true, failed but stayed within a predefined failure threshold.<br>In the future, we could add other intermediate conditions, e.g. ConditionDegraded.<br></td></tr><tr><td><code>ConditionCheckError</code></td><td>ConditionCheckError is a constant for a reason in condition.<br></td></tr></tbody></table><h4 id=conditiontype>ConditionType</h4><p><em>Underlying type:</em> <em>string</em></p><p>ConditionType is the type of condition.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#condition>Condition</a></li></ul><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>Ready</code></td><td>ConditionTypeReady is a constant for a condition type indicating that the etcd cluster is ready.<br></td></tr><tr><td><code>AllMembersReady</code></td><td>ConditionTypeAllMembersReady is a constant for a condition type indicating that all members of the etcd cluster are ready.<br></td></tr><tr><td><code>BackupReady</code></td><td>ConditionTypeBackupReady is a constant for a condition type indicating that the etcd backup is ready.<br></td></tr><tr><td><code>DataVolumesReady</code></td><td>ConditionTypeDataVolumesReady is a constant for a condition type indicating that the etcd data volumes are ready.<br></td></tr><tr><td><code>Succeeded</code></td><td>EtcdCopyBackupsTaskSucceeded is a condition type indicating that a EtcdCopyBackupsTask has succeeded.<br></td></tr><tr><td><code>Failed</code></td><td>EtcdCopyBackupsTaskFailed is a condition type indicating that a EtcdCopyBackupsTask has failed.<br></td></tr></tbody></table><h4 id=crossversionobjectreference>CrossVersionObjectReference</h4><p>CrossVersionObjectReference contains enough information to let you identify the referred resource.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdstatus>EtcdStatus</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>kind</code> <em>string</em></td><td>Kind of the referent</td><td></td><td></td></tr><tr><td><code>name</code> <em>string</em></td><td>Name of the referent</td><td></td><td></td></tr><tr><td><code>apiVersion</code> <em>string</em></td><td>API version of the referent</td><td></td><td></td></tr></tbody></table><h4 id=errorcode>ErrorCode</h4><p><em>Underlying type:</em> <em>string</em></p><p>ErrorCode is a string alias representing an error code that identifies an error.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#lasterror>LastError</a></li></ul><h4 id=etcd>Etcd</h4><p>Etcd is the Schema for the etcds API</p><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>apiVersion</code> <em>string</em></td><td><code>druid.gardener.cloud/v1alpha1</code></td><td></td><td></td></tr><tr><td><code>kind</code> <em>string</em></td><td><code>Etcd</code></td><td></td><td></td></tr><tr><td><code>metadata</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#objectmeta-v1-meta>ObjectMeta</a></em></td><td>Refer to Kubernetes API documentation for fields of <code>metadata</code>.</td><td></td><td></td></tr><tr><td><code>spec</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdspec>EtcdSpec</a></em></td><td></td><td></td><td></td></tr><tr><td><code>status</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdstatus>EtcdStatus</a></em></td><td></td><td></td><td></td></tr></tbody></table><h4 id=etcdconfig>EtcdConfig</h4><p>EtcdConfig defines the configuration for the etcd cluster to be deployed.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdspec>EtcdSpec</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>quota</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#quantity-resource-api>Quantity</a></em></td><td>Quota defines the etcd DB quota.</td><td></td><td></td></tr><tr><td><code>snapshotCount</code> <em>integer</em></td><td>SnapshotCount defines the number of applied Raft entries to hold in-memory before compaction.<br>More info: <a href=https://etcd.io/docs/v3.4/op-guide/maintenance/#raft-log-retention>https://etcd.io/docs/v3.4/op-guide/maintenance/#raft-log-retention</a></td><td></td><td></td></tr><tr><td><code>defragmentationSchedule</code> <em>string</em></td><td>DefragmentationSchedule defines the cron standard schedule for defragmentation of etcd.</td><td></td><td></td></tr><tr><td><code>serverPort</code> <em>integer</em></td><td></td><td></td><td></td></tr><tr><td><code>clientPort</code> <em>integer</em></td><td></td><td></td><td></td></tr><tr><td><code>image</code> <em>string</em></td><td>Image defines the etcd container image and tag</td><td></td><td></td></tr><tr><td><code>authSecretRef</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#secretreference-v1-core>SecretReference</a></em></td><td></td><td></td><td></td></tr><tr><td><code>metrics</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#metricslevel>MetricsLevel</a></em></td><td>Metrics defines the level of detail for exported metrics of etcd, specify &rsquo;extensive&rsquo; to include histogram metrics.</td><td></td><td>Enum: [basic extensive]<br></td></tr><tr><td><code>resources</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#resourcerequirements-v1-core>ResourceRequirements</a></em></td><td>Resources defines the compute Resources required by etcd container.<br>More info: <a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/</a></td><td></td><td></td></tr><tr><td><code>clientUrlTls</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#tlsconfig>TLSConfig</a></em></td><td>ClientUrlTLS contains the ca, server TLS and client TLS secrets for client communication to ETCD cluster</td><td></td><td></td></tr><tr><td><code>peerUrlTls</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#tlsconfig>TLSConfig</a></em></td><td>PeerUrlTLS contains the ca and server TLS secrets for peer communication within ETCD cluster<br>Currently, PeerUrlTLS does not require client TLS secrets for gardener implementation of ETCD cluster.</td><td></td><td></td></tr><tr><td><code>etcdDefragTimeout</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#duration-v1-meta>Duration</a></em></td><td>EtcdDefragTimeout defines the timeout duration for etcd defrag call</td><td></td><td></td></tr><tr><td><code>heartbeatDuration</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#duration-v1-meta>Duration</a></em></td><td>HeartbeatDuration defines the duration for members to send heartbeats. The default value is 10s.</td><td></td><td></td></tr><tr><td><code>clientService</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#clientservice>ClientService</a></em></td><td>ClientService defines the parameters of the client service that a user can specify</td><td></td><td></td></tr></tbody></table><h4 id=etcdcopybackupstask>EtcdCopyBackupsTask</h4><p>EtcdCopyBackupsTask is a task for copying etcd backups from a source to a target store.</p><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>apiVersion</code> <em>string</em></td><td><code>druid.gardener.cloud/v1alpha1</code></td><td></td><td></td></tr><tr><td><code>kind</code> <em>string</em></td><td><code>EtcdCopyBackupsTask</code></td><td></td><td></td></tr><tr><td><code>metadata</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#objectmeta-v1-meta>ObjectMeta</a></em></td><td>Refer to Kubernetes API documentation for fields of <code>metadata</code>.</td><td></td><td></td></tr><tr><td><code>spec</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdcopybackupstaskspec>EtcdCopyBackupsTaskSpec</a></em></td><td></td><td></td><td></td></tr><tr><td><code>status</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdcopybackupstaskstatus>EtcdCopyBackupsTaskStatus</a></em></td><td></td><td></td><td></td></tr></tbody></table><h4 id=etcdcopybackupstaskspec>EtcdCopyBackupsTaskSpec</h4><p>EtcdCopyBackupsTaskSpec defines the parameters for the copy backups task.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdcopybackupstask>EtcdCopyBackupsTask</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>sourceStore</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#storespec>StoreSpec</a></em></td><td>SourceStore defines the specification of the source object store provider for storing backups.</td><td></td><td></td></tr><tr><td><code>targetStore</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#storespec>StoreSpec</a></em></td><td>TargetStore defines the specification of the target object store provider for storing backups.</td><td></td><td></td></tr><tr><td><code>maxBackupAge</code> <em>integer</em></td><td>MaxBackupAge is the maximum age in days that a backup must have in order to be copied.<br>By default all backups will be copied.</td><td></td><td></td></tr><tr><td><code>maxBackups</code> <em>integer</em></td><td>MaxBackups is the maximum number of backups that will be copied starting with the most recent ones.</td><td></td><td></td></tr><tr><td><code>waitForFinalSnapshot</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#waitforfinalsnapshotspec>WaitForFinalSnapshotSpec</a></em></td><td>WaitForFinalSnapshot defines the parameters for waiting for a final full snapshot before copying backups.</td><td></td><td></td></tr></tbody></table><h4 id=etcdcopybackupstaskstatus>EtcdCopyBackupsTaskStatus</h4><p>EtcdCopyBackupsTaskStatus defines the observed state of the copy backups task.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdcopybackupstask>EtcdCopyBackupsTask</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>conditions</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#condition>Condition</a> array</em></td><td>Conditions represents the latest available observations of an object&rsquo;s current state.</td><td></td><td></td></tr><tr><td><code>observedGeneration</code> <em>integer</em></td><td>ObservedGeneration is the most recent generation observed for this resource.</td><td></td><td></td></tr><tr><td><code>lastError</code> <em>string</em></td><td>LastError represents the last occurred error.</td><td></td><td></td></tr></tbody></table><h4 id=etcdmemberconditionstatus>EtcdMemberConditionStatus</h4><p><em>Underlying type:</em> <em>string</em></p><p>EtcdMemberConditionStatus is the status of an etcd cluster member.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdmemberstatus>EtcdMemberStatus</a></li></ul><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>Ready</code></td><td>EtcdMemberStatusReady indicates that the etcd member is ready.<br></td></tr><tr><td><code>NotReady</code></td><td>EtcdMemberStatusNotReady indicates that the etcd member is not ready.<br></td></tr><tr><td><code>Unknown</code></td><td>EtcdMemberStatusUnknown indicates that the status of the etcd member is unknown.<br></td></tr></tbody></table><h4 id=etcdmemberstatus>EtcdMemberStatus</h4><p>EtcdMemberStatus holds information about etcd cluster membership.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdstatus>EtcdStatus</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>name</code> <em>string</em></td><td>Name is the name of the etcd member. It is the name of the backing <code>Pod</code>.</td><td></td><td></td></tr><tr><td><code>id</code> <em>string</em></td><td>ID is the ID of the etcd member.</td><td></td><td></td></tr><tr><td><code>role</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdrole>EtcdRole</a></em></td><td>Role is the role in the etcd cluster, either <code>Leader</code> or <code>Member</code>.</td><td></td><td></td></tr><tr><td><code>status</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdmemberconditionstatus>EtcdMemberConditionStatus</a></em></td><td>Status of the condition, one of True, False, Unknown.</td><td></td><td></td></tr><tr><td><code>reason</code> <em>string</em></td><td>The reason for the condition&rsquo;s last transition.</td><td></td><td></td></tr><tr><td><code>lastTransitionTime</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#time-v1-meta>Time</a></em></td><td>LastTransitionTime is the last time the condition&rsquo;s status changed.</td><td></td><td></td></tr></tbody></table><h4 id=etcdrole>EtcdRole</h4><p><em>Underlying type:</em> <em>string</em></p><p>EtcdRole is the role of an etcd cluster member.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdmemberstatus>EtcdMemberStatus</a></li></ul><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>Leader</code></td><td>EtcdRoleLeader describes the etcd role <code>Leader</code>.<br></td></tr><tr><td><code>Member</code></td><td>EtcdRoleMember describes the etcd role <code>Member</code>.<br></td></tr></tbody></table><h4 id=etcdspec>EtcdSpec</h4><p>EtcdSpec defines the desired state of Etcd</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcd>Etcd</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>selector</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#labelselector-v1-meta>LabelSelector</a></em></td><td>selector is a label query over pods that should match the replica count.<br>It must match the pod template&rsquo;s labels.<br>More info: <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors>https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors</a></td><td></td><td></td></tr><tr><td><code>labels</code> <em>object (keys:string, values:string)</em></td><td></td><td></td><td></td></tr><tr><td><code>annotations</code> <em>object (keys:string, values:string)</em></td><td></td><td></td><td></td></tr><tr><td><code>etcd</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdconfig>EtcdConfig</a></em></td><td></td><td></td><td></td></tr><tr><td><code>backup</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#backupspec>BackupSpec</a></em></td><td></td><td></td><td></td></tr><tr><td><code>sharedConfig</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#sharedconfig>SharedConfig</a></em></td><td></td><td></td><td></td></tr><tr><td><code>schedulingConstraints</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#schedulingconstraints>SchedulingConstraints</a></em></td><td></td><td></td><td></td></tr><tr><td><code>replicas</code> <em>integer</em></td><td></td><td></td><td></td></tr><tr><td><code>priorityClassName</code> <em>string</em></td><td>PriorityClassName is the name of a priority class that shall be used for the etcd pods.</td><td></td><td></td></tr><tr><td><code>storageClass</code> <em>string</em></td><td>StorageClass defines the name of the StorageClass required by the claim.<br>More info: <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1>https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1</a></td><td></td><td></td></tr><tr><td><code>storageCapacity</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#quantity-resource-api>Quantity</a></em></td><td>StorageCapacity defines the size of persistent volume.</td><td></td><td></td></tr><tr><td><code>volumeClaimTemplate</code> <em>string</em></td><td>VolumeClaimTemplate defines the volume claim template to be created</td><td></td><td></td></tr></tbody></table><h4 id=etcdstatus>EtcdStatus</h4><p>EtcdStatus defines the observed state of Etcd.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcd>Etcd</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>observedGeneration</code> <em>integer</em></td><td>ObservedGeneration is the most recent generation observed for this resource.</td><td></td><td></td></tr><tr><td><code>etcd</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#crossversionobjectreference>CrossVersionObjectReference</a></em></td><td></td><td></td><td></td></tr><tr><td><code>conditions</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#condition>Condition</a> array</em></td><td>Conditions represents the latest available observations of an etcd&rsquo;s current state.</td><td></td><td></td></tr><tr><td><code>serviceName</code> <em>string</em></td><td>ServiceName is the name of the etcd service.<br>Deprecated: this field will be removed in the future.</td><td></td><td></td></tr><tr><td><code>lastError</code> <em>string</em></td><td>LastError represents the last occurred error.<br>Deprecated: Use LastErrors instead.</td><td></td><td></td></tr><tr><td><code>lastErrors</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#lasterror>LastError</a> array</em></td><td>LastErrors captures errors that occurred during the last operation.</td><td></td><td></td></tr><tr><td><code>lastOperation</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#lastoperation>LastOperation</a></em></td><td>LastOperation indicates the last operation performed on this resource.</td><td></td><td></td></tr><tr><td><code>clusterSize</code> <em>integer</em></td><td>Cluster size is the current size of the etcd cluster.<br>Deprecated: this field will not be populated with any value and will be removed in the future.</td><td></td><td></td></tr><tr><td><code>currentReplicas</code> <em>integer</em></td><td>CurrentReplicas is the current replica count for the etcd cluster.</td><td></td><td></td></tr><tr><td><code>replicas</code> <em>integer</em></td><td>Replicas is the replica count of the etcd cluster.</td><td></td><td></td></tr><tr><td><code>readyReplicas</code> <em>integer</em></td><td>ReadyReplicas is the count of replicas being ready in the etcd cluster.</td><td></td><td></td></tr><tr><td><code>ready</code> <em>boolean</em></td><td>Ready is <code>true</code> if all etcd replicas are ready.</td><td></td><td></td></tr><tr><td><code>updatedReplicas</code> <em>integer</em></td><td>UpdatedReplicas is the count of updated replicas in the etcd cluster.<br>Deprecated: this field will be removed in the future.</td><td></td><td></td></tr><tr><td><code>labelSelector</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#labelselector-v1-meta>LabelSelector</a></em></td><td>LabelSelector is a label query over pods that should match the replica count.<br>It must match the pod template&rsquo;s labels.<br>Deprecated: this field will be removed in the future.</td><td></td><td></td></tr><tr><td><code>members</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdmemberstatus>EtcdMemberStatus</a> array</em></td><td>Members represents the members of the etcd cluster</td><td></td><td></td></tr><tr><td><code>peerUrlTLSEnabled</code> <em>boolean</em></td><td>PeerUrlTLSEnabled captures the state of peer url TLS being enabled for the etcd member(s)</td><td></td><td></td></tr></tbody></table><h4 id=garbagecollectionpolicy>GarbageCollectionPolicy</h4><p><em>Underlying type:</em> <em>string</em></p><p>GarbageCollectionPolicy defines the type of policy for snapshot garbage collection.</p><p><em>Validation:</em></p><ul><li>Enum: [Exponential LimitBased]</li></ul><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#backupspec>BackupSpec</a></li></ul><h4 id=lasterror>LastError</h4><p>LastError stores details of the most recent error encountered for a resource.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdstatus>EtcdStatus</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>code</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#errorcode>ErrorCode</a></em></td><td>Code is an error code that uniquely identifies an error.</td><td></td><td></td></tr><tr><td><code>description</code> <em>string</em></td><td>Description is a human-readable message indicating details of the error.</td><td></td><td></td></tr><tr><td><code>observedAt</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#time-v1-meta>Time</a></em></td><td>ObservedAt is the time the error was observed.</td><td></td><td></td></tr></tbody></table><h4 id=lastoperation>LastOperation</h4><p>LastOperation holds the information on the last operation done on the Etcd resource.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdstatus>EtcdStatus</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>type</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#lastoperationtype>LastOperationType</a></em></td><td>Type is the type of last operation.</td><td></td><td></td></tr><tr><td><code>state</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#lastoperationstate>LastOperationState</a></em></td><td>State is the state of the last operation.</td><td></td><td></td></tr><tr><td><code>description</code> <em>string</em></td><td>Description describes the last operation.</td><td></td><td></td></tr><tr><td><code>runID</code> <em>string</em></td><td>RunID correlates an operation with a reconciliation run.<br>Every time an Etcd resource is reconciled (barring status reconciliation which is periodic), a unique ID is<br>generated which can be used to correlate all actions done as part of a single reconcile run. Capturing this<br>as part of LastOperation aids in establishing this correlation. This further helps in also easily filtering<br>reconcile logs as all structured logs in a reconciliation run should have the <code>runID</code> referenced.</td><td></td><td></td></tr><tr><td><code>lastUpdateTime</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#time-v1-meta>Time</a></em></td><td>LastUpdateTime is the time at which the operation was last updated.</td><td></td><td></td></tr></tbody></table><h4 id=lastoperationstate>LastOperationState</h4><p><em>Underlying type:</em> <em>string</em></p><p>LastOperationState is a string alias representing the state of the last operation.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#lastoperation>LastOperation</a></li></ul><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>Processing</code></td><td>LastOperationStateProcessing indicates that an operation is in progress.<br></td></tr><tr><td><code>Succeeded</code></td><td>LastOperationStateSucceeded indicates that an operation has completed successfully.<br></td></tr><tr><td><code>Error</code></td><td>LastOperationStateError indicates that an operation is completed with errors and will be retried.<br></td></tr><tr><td><code>Requeue</code></td><td>LastOperationStateRequeue indicates that an operation is not completed and either due to an error or unfulfilled conditions will be retried.<br></td></tr></tbody></table><h4 id=lastoperationtype>LastOperationType</h4><p><em>Underlying type:</em> <em>string</em></p><p>LastOperationType is a string alias representing type of the last operation.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#lastoperation>LastOperation</a></li></ul><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>Create</code></td><td>LastOperationTypeCreate indicates that the last operation was a creation of a new Etcd resource.<br></td></tr><tr><td><code>Reconcile</code></td><td>LastOperationTypeReconcile indicates that the last operation was a reconciliation of the spec of an Etcd resource.<br></td></tr><tr><td><code>Delete</code></td><td>LastOperationTypeDelete indicates that the last operation was a deletion of an existing Etcd resource.<br></td></tr></tbody></table><h4 id=leaderelectionspec>LeaderElectionSpec</h4><p>LeaderElectionSpec defines parameters related to the LeaderElection configuration.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#backupspec>BackupSpec</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>reelectionPeriod</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#duration-v1-meta>Duration</a></em></td><td>ReelectionPeriod defines the Period after which leadership status of corresponding etcd is checked.</td><td></td><td></td></tr><tr><td><code>etcdConnectionTimeout</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#duration-v1-meta>Duration</a></em></td><td>EtcdConnectionTimeout defines the timeout duration for etcd client connection during leader election.</td><td></td><td></td></tr></tbody></table><h4 id=metricslevel>MetricsLevel</h4><p><em>Underlying type:</em> <em>string</em></p><p>MetricsLevel defines the level &lsquo;basic&rsquo; or &rsquo;extensive&rsquo;.</p><p><em>Validation:</em></p><ul><li>Enum: [basic extensive]</li></ul><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdconfig>EtcdConfig</a></li></ul><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>basic</code></td><td>Basic is a constant for metrics level basic.<br></td></tr><tr><td><code>extensive</code></td><td>Extensive is a constant for metrics level extensive.<br></td></tr></tbody></table><h4 id=schedulingconstraints>SchedulingConstraints</h4><p>SchedulingConstraints defines the different scheduling constraints that must be applied to the
pod spec in the etcd statefulset.
Currently supported constraints are Affinity and TopologySpreadConstraints.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdspec>EtcdSpec</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>affinity</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#affinity-v1-core>Affinity</a></em></td><td>Affinity defines the various affinity and anti-affinity rules for a pod<br>that are honoured by the kube-scheduler.</td><td></td><td></td></tr><tr><td><code>topologySpreadConstraints</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#topologyspreadconstraint-v1-core>TopologySpreadConstraint</a> array</em></td><td>TopologySpreadConstraints describes how a group of pods ought to spread across topology domains,<br>that are honoured by the kube-scheduler.</td><td></td><td></td></tr></tbody></table><h4 id=secretreference>SecretReference</h4><p>SecretReference defines a reference to a secret.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#tlsconfig>TLSConfig</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>name</code> <em>string</em></td><td>name is unique within a namespace to reference a secret resource.</td><td></td><td></td></tr><tr><td><code>namespace</code> <em>string</em></td><td>namespace defines the space within which the secret name must be unique.</td><td></td><td></td></tr><tr><td><code>dataKey</code> <em>string</em></td><td>DataKey is the name of the key in the data map containing the credentials.</td><td></td><td></td></tr></tbody></table><h4 id=sharedconfig>SharedConfig</h4><p>SharedConfig defines parameters shared and used by Etcd as well as backup-restore sidecar.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdspec>EtcdSpec</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>autoCompactionMode</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#compactionmode>CompactionMode</a></em></td><td>AutoCompactionMode defines the auto-compaction-mode:&lsquo;periodic&rsquo; mode or &lsquo;revision&rsquo; mode for etcd and embedded-etcd of backup-restore sidecar.</td><td></td><td>Enum: [periodic revision]<br></td></tr><tr><td><code>autoCompactionRetention</code> <em>string</em></td><td>AutoCompactionRetention defines the auto-compaction-retention length for etcd as well as for embedded-etcd of backup-restore sidecar.</td><td></td><td></td></tr></tbody></table><h4 id=storageprovider>StorageProvider</h4><p><em>Underlying type:</em> <em>string</em></p><p>StorageProvider defines the type of object store provider for storing backups.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#storespec>StoreSpec</a></li></ul><h4 id=storespec>StoreSpec</h4><p>StoreSpec defines parameters related to ObjectStore persisting backups</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#backupspec>BackupSpec</a></li><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdcopybackupstaskspec>EtcdCopyBackupsTaskSpec</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>container</code> <em>string</em></td><td>Container is the name of the container the backup is stored at.</td><td></td><td></td></tr><tr><td><code>prefix</code> <em>string</em></td><td>Prefix is the prefix used for the store.</td><td></td><td></td></tr><tr><td><code>provider</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#storageprovider>StorageProvider</a></em></td><td>Provider is the name of the backup provider.</td><td></td><td></td></tr><tr><td><code>secretRef</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#secretreference-v1-core>SecretReference</a></em></td><td>SecretRef is the reference to the secret which used to connect to the backup store.</td><td></td><td></td></tr></tbody></table><h4 id=tlsconfig>TLSConfig</h4><p>TLSConfig hold the TLS configuration details.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#backupspec>BackupSpec</a></li><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdconfig>EtcdConfig</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>tlsCASecretRef</code> <em><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#secretreference>SecretReference</a></em></td><td></td><td></td><td></td></tr><tr><td><code>serverTLSSecretRef</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#secretreference-v1-core>SecretReference</a></em></td><td></td><td></td><td></td></tr><tr><td><code>clientTLSSecretRef</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#secretreference-v1-core>SecretReference</a></em></td><td></td><td></td><td></td></tr></tbody></table><h4 id=waitforfinalsnapshotspec>WaitForFinalSnapshotSpec</h4><p>WaitForFinalSnapshotSpec defines the parameters for waiting for a final full snapshot before copying backups.</p><p><em>Appears in:</em></p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/etcd-druid-api/#etcdcopybackupstaskspec>EtcdCopyBackupsTaskSpec</a></li></ul><table><thead><tr><th>Field</th><th>Description</th><th>Default</th><th>Validation</th></tr></thead><tbody><tr><td><code>enabled</code> <em>boolean</em></td><td>Enabled specifies whether to wait for a final full snapshot before copying backups.</td><td></td><td></td></tr><tr><td><code>timeout</code> <em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.29/#duration-v1-meta>Duration</a></em></td><td>Timeout is the timeout for waiting for a final full snapshot. When this timeout expires, the copying of backups<br>will be performed anyway. No timeout or 0 means wait forever.</td><td></td><td></td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-28d547e6d88be739cbe597c4071b2470>3.15 - etcd Network Latency</h1><h1 id=network-latency-analysis-sn-etcd-sz-vs--mn-etcd-sz-vs-mn-etcd-mz>Network Latency analysis: <code>sn-etcd-sz</code> vs <code>mn-etcd-sz</code> vs <code>mn-etcd-mz</code></h1><p>This page captures the etcd cluster latency analysis for below scenarios using the benchmark tool (build from <a href=https://github.com/seshachalam-yv/etcd>etcd benchmark tool</a>).</p><p><code>sn-etcd-sz</code> -> single-node etcd single zone (Only single replica of etcd will be running)</p><p><code>mn-etcd-sz</code> -> multi-node etcd single zone (Multiple replicas of etcd pods will be running across nodes in a single zone)</p><p><code>mn-etcd-mz</code> -> multi-node etcd multi zone (Multiple replicas of etcd pods will be running across nodes in multiple zones)</p><h2 id=put-analysis>PUT Analysis</h2><h3 id=summary>Summary</h3><ul><li><code>sn-etcd-sz</code> latency is <strong>~20% less than</strong> <code>mn-etcd-sz</code> when benchmark tool with single client.</li><li><code>mn-etcd-sz</code> latency is less than <code>mn-etcd-mz</code> but the difference is <code>~+/-5%</code>.</li><li>Compared to <code>mn-etcd-sz</code>, <code>sn-etcd-sz</code> latency is higher and gradually grows with more clients and larger value size.</li><li>Compared to <code>mn-etcd-mz</code>, <code>mn-etcd-sz</code> latency is higher and gradually grows with more clients and larger value size.</li><li><em>Compared to <code>follower</code>, <code>leader</code> latency is less</em>, when benchmark tool with single client for all cases.</li><li><em>Compared to <code>follower</code>, <code>leader</code> latency is high</em>, when benchmark tool with multiple clients for all cases.</li></ul><p>Sample commands:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># write to leader</span>
</span></span><span style=display:flex><span>benchmark put --target-leader --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0 --val-size=256 --total=10000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># write to follower</span>
</span></span><span style=display:flex><span>benchmark put  --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0 --val-size=256 --total=10000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_FOLLOWER_HOST
</span></span></code></pre></div><h3 id=latency-analysis-during-put-requests-to-etcd>Latency analysis during PUT requests to etcd</h3><ul><li><details><summary>In this case benchmark tool tries to put key with random 256 bytes value.</summary><ul><li><p>Benchmark tool loads key/value to <code>leader</code> with single client .</p><ul><li><code>sn-etcd-sz</code> latency (~0.815ms) is <strong>~50% lesser than</strong> <code>mn-etcd-sz</code> (~1.74ms ).</li><li><ul><li><code>mn-etcd-sz</code> latency (~1.74ms ) is slightly lesser than <code>mn-etcd-mz</code> (~1.8ms) but the difference is negligible (within same ms).</li></ul></li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>1220.0520</td><td style=text-align:center>0.815ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>586.545</td><td style=text-align:center>1.74ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>554.0155654442634</td><td style=text-align:center>1.8ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>follower</code> with single client.</p><ul><li><code>mn-etcd-sz</code> latency(<code>~2.2ms</code>) is <strong>20% to 30% lesser than</strong> <code>mn-etcd-mz</code>(<code>~2.7ms</code>).</li><li><em>Compare to <code>follower</code>, <code>leader</code> has lower latency.</em></li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>445.743</td><td style=text-align:center>2.23ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>378.9366747610789</td><td style=text-align:center>2.63ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>457.967</td><td style=text-align:center>2.17ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>345.6586129825796</td><td style=text-align:center>2.89ms</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>leader</code> with multiple clients.</p><ul><li><code>sn-etcd-sz</code> latency(<code>~78.3ms</code>) is <strong>~10% greater than</strong> <code>mn-etcd-sz</code>(<code>~71.81ms</code>).</li><li><code>mn-etcd-sz</code> latency(<code>~71.81ms</code>) is less than <code>mn-etcd-mz</code>(<code>~72.5ms</code>) but the difference is negligible.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>leader</td><td style=text-align:center>12638.905</td><td style=text-align:center>78.32ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>leader</td><td style=text-align:center>13789.248</td><td style=text-align:center>71.81ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>leader</td><td style=text-align:center>13728.446436395223</td><td style=text-align:center>72.5ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>follower</code> with multiple clients.</p><ul><li><code>mn-etcd-sz</code> latency(<code>~69.8ms</code>) is <strong>~5% greater than</strong> <code>mn-etcd-mz</code>(<code>~72.6ms</code>).</li><li><em>Compare to <code>leader</code>, <code>follower</code> has lower latency</em>.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>follower-1</td><td style=text-align:center>14271.983</td><td style=text-align:center>69.80ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>follower-1</td><td style=text-align:center>13695.98</td><td style=text-align:center>72.62ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>follower-2</td><td style=text-align:center>14325.436</td><td style=text-align:center>69.47ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>follower-2</td><td style=text-align:center>15750.409490407475</td><td style=text-align:center>63.3ms</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li></ul></details></li><li><details><summary>In this case benchmark tool tries to put key with random 1 MB value.</summary><ul><li><p>Benchmark tool loads key/value to <code>leader</code> with single client.</p><ul><li><code>sn-etcd-sz</code> latency(<code>~16.35ms</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-sz</code>(<code>~20.64ms</code>).</li><li><code>mn-etcd-sz</code> latency(<code>~20.64ms</code>) is less than <code>mn-etcd-mz</code>(<code>~21.08ms</code>) but the difference is negligible..</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>61.117</td><td style=text-align:center>16.35ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>48.416</td><td style=text-align:center>20.64ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>45.7517341664802</td><td style=text-align:center>21.08ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value withto <code>follower</code> single client.</p><ul><li><code>mn-etcd-sz</code> latency(<code>~23.10ms</code>) is <strong>~10% greater than</strong> <code>mn-etcd-mz</code>(<code>~21.8ms</code>).</li><li><em>Compare to <code>follower</code>, <code>leader</code> has lower latency</em>.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>43.261</td><td style=text-align:center>23.10ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>45.7517341664802</td><td style=text-align:center>21.8ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>45.33</td><td style=text-align:center>22.05ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>40.0518</td><td style=text-align:center>24.95ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>43.28573155709838</td><td style=text-align:center>23.09ms</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>45.92</td><td style=text-align:center>21.76ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>35.5705</td><td style=text-align:center>28.1ms</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>leader</code> with multiple clients.</p><ul><li><code>sn-etcd-sz</code> latency(<code>~6.0375secs</code>) is <strong>~30% greater than</strong> <code>mn-etcd-sz``~4.000secs</code>).</li><li><code>mn-etcd-sz</code> latency(<code>~4.000secs</code>) is less than <code>mn-etcd-mz</code>(<code>~ 4.09secs</code>) but the difference is negligible.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>leader</td><td style=text-align:center>55.373</td><td style=text-align:center>6.0375secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>leader</td><td style=text-align:center>67.319</td><td style=text-align:center>4.000secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>leader</td><td style=text-align:center>65.91914167957594</td><td style=text-align:center>4.09secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>follower</code> with multiple clients.</p><ul><li><em><code>mn-etcd-sz</code> latency(<code>~4.04secs</code>) is <strong>~5% greater than</strong> <code>mn-etcd-mz</code>(<code>~ 3.90secs</code>).</em></li><li><em>Compare to <code>leader</code>, <code>follower</code> has lower latency</em>.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-1</td><td style=text-align:center>66.528</td><td style=text-align:center>4.0417secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-1</td><td style=text-align:center>70.6493461856332</td><td style=text-align:center>3.90secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-1</td><td style=text-align:center>71.95</td><td style=text-align:center>3.84secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-2</td><td style=text-align:center>66.447</td><td style=text-align:center>4.0164secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-2</td><td style=text-align:center>67.53038086369484</td><td style=text-align:center>3.87secs</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-2</td><td style=text-align:center>68.46</td><td style=text-align:center>3.92secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li></ul></details></li></ul><hr><br><h2 id=range-analysis>Range Analysis</h2><p>Sample commands are:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Single connection read request with sequential keys</span>
</span></span><span style=display:flex><span>benchmark range 0 --target-leader --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0  --total=10000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=l <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span><span style=display:flex><span><span style=color:green># --consistency=s [Serializable]</span>
</span></span><span style=display:flex><span>benchmark range 0 --target-leader --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0  --total=10000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=s <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span><span style=display:flex><span><span style=color:green># Each read request with range query matches key 0 9999 and repeats for total number of requests.  </span>
</span></span><span style=display:flex><span>benchmark range 0 9999 --target-leader --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --total=10 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=s <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=https://etcd-main-client:2379
</span></span><span style=display:flex><span><span style=color:green># Read requests with multiple connections</span>
</span></span><span style=display:flex><span>benchmark range 0 --target-leader --conns=100 --clients=1000 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0  --total=100000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=l <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span><span style=display:flex><span>benchmark range 0 --target-leader --conns=100 --clients=1000 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0  --total=100000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=s <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span></code></pre></div><h3 id=latency-analysis-during-range-requests-to-etcd>Latency analysis during Range requests to etcd</h3><ul><li><details><summary>In this case benchmark tool tries to get specific key with random 256 bytes value.</summary><ul><li><p>Benchmark tool range requests to <code>leader</code> with single client.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~1.24ms</code>) is <strong>~40% greater than</strong> <code>mn-etcd-sz</code>(<code>~0.67ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~0.67ms</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-mz</code>(<code>~0.85ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>800.272</td><td style=text-align:center>1.24ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>1173.9081</td><td style=text-align:center>0.67ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>999.3020189178693</td><td style=text-align:center>0.85ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~40% less</strong> for all cases</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>1411.229</td><td style=text-align:center>0.70ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>2033.131</td><td style=text-align:center>0.35ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>2100.2426362012025</td><td style=text-align:center>0.47ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> with single client .</p><ul><li><code>mn-etcd-sz</code> latency(<code>~1.3ms</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-mz</code>(<code>~1.6ms</code>).</li><li><em>Compare to <code>follower</code>, <code>leader</code> read request latency is <strong>~50% less</strong> for both <code>mn-etcd-sz</code>, <code>mn-etcd-mz</code></em></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>765.325</td><td style=text-align:center>1.3ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>596.1</td><td style=text-align:center>1.6ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~50% less</strong> for all cases</li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>1823.631</td><td style=text-align:center>0.54ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>1442.6</td><td style=text-align:center>0.69ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>1416.39</td><td style=text-align:center>0.70ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>2077.449</td><td style=text-align:center>0.47ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>leader</code> with multiple client.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~84.66ms</code>) is <strong>~20% greater than</strong> <code>mn-etcd-sz</code>(<code>~73.95ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~73.95ms</code>) is <strong>more or less equal to</strong> <code>mn-etcd-mz</code>(<code>~ 73.8ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>11775.721</td><td style=text-align:center>84.66ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>13446.9598</td><td style=text-align:center>73.95ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>13527.19810605353</td><td style=text-align:center>73.8ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~20% lesser</strong> for all cases</p></li><li><p><code>sn-etcd-sz</code> latency(<code>~69.37ms</code>) is <strong>more or less equal to</strong> <code>mn-etcd-sz</code>(<code>~69.89ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~69.89ms</code>) is <strong>slightly higher than</strong> <code>mn-etcd-mz</code>(<code>~67.63ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>14334.9027</td><td style=text-align:center>69.37ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>14270.008</td><td style=text-align:center>69.89ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>14715.287354023869</td><td style=text-align:center>67.63ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> with multiple client.</p><ul><li><p><code>mn-etcd-sz</code> latency(<code>~60.69ms</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-mz</code>(<code>~70.76ms</code>).</p></li><li><p>Compare to <code>leader</code>, <code>follower</code> has lower read request latency.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>11586.032</td><td style=text-align:center>60.69ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>14050.5</td><td style=text-align:center>70.76ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p><code>mn-etcd-sz</code> latency(<code>~86.09ms</code>) is <strong>~20 higher than</strong> <code>mn-etcd-mz</code>(<code>~64.6ms</code>).</p></li><li><ul><li>Compare to <code>mn-etcd-sz</code> consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~20% higher</strong>.*</li></ul></li><li><p>Compare to <code>mn-etcd-mz</code> consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~slightly less</strong>.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>11582.438</td><td style=text-align:center>86.09ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>15422.2</td><td style=text-align:center>64.6ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>leader</code> all keys.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~678.77ms</code>) is <strong>~5% slightly lesser than</strong> <code>mn-etcd-sz</code>(<code>~697.29ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~697.29ms</code>) is less than <code>mn-etcd-mz</code>(<code>~701ms</code>) but the difference is negligible.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>6.8875</td><td style=text-align:center>678.77ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>6.720</td><td style=text-align:center>697.29ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>6.7</td><td style=text-align:center>701ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><ul><li>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~5% slightly higher</strong> for all cases</li></ul></li><li><p><code>sn-etcd-sz</code> latency(<code>~687.36ms</code>) is less than <code>mn-etcd-sz</code>(<code>~692.68ms</code>) but the difference is negligible.</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~692.68ms</code>) is <strong>~5% slightly lesser than</strong> <code>mn-etcd-mz</code>(<code>~735.7ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>6.76</td><td style=text-align:center>687.36ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>6.635</td><td style=text-align:center>692.68ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>6.3</td><td style=text-align:center>735.7ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> all keys</p><ul><li><p><code>mn-etcd-sz</code>(<code>~737.68ms</code>) latency is <strong>~5% slightly higher than</strong> <code>mn-etcd-mz</code>(<code>~713.7ms</code>).</p></li><li><p>Compare to <code>leader</code> consistency <code>Linearizable</code>read request, <code>follower</code> is <em>~5% slightly higher</em>.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>6.163</td><td style=text-align:center>737.68ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>6.52</td><td style=text-align:center>713.7ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p><code>mn-etcd-sz</code> latency(<code>~757.73ms</code>) is <strong>~10% higher than</strong> <code>mn-etcd-mz</code>(<code>~690.4ms</code>).</p></li><li><p>Compare to <code>follower</code> consistency <code>Linearizable</code>read request, <code>follower</code> consistency <code>Serializable</code> is <em>~3% slightly higher</em> for <code>mn-etcd-sz</code>.</p></li><li><p><em>Compare to <code>follower</code> consistency <code>Linearizable</code>read request, <code>follower</code> consistency <code>Serializable</code> is <em>~5% less</em> for <code>mn-etcd-mz</code>.</em></p></li><li><p>*Compare to <code>leader</code> consistency <code>Serializable</code>read request, <code>follower</code> consistency <code>Serializable</code> is <em>~5% less</em> for <code>mn-etcd-mz</code>. *</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>6.0295</td><td style=text-align:center>757.73ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>6.87</td><td style=text-align:center>690.4ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li></ul><hr><br></details></li><li><details><summary>In this case benchmark tool tries to get specific key with random `1MB` value.</summary><ul><li><p>Benchmark tool range requests to <code>leader</code> with single client.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~5.96ms</code>) is <strong>~5% lesser than</strong> <code>mn-etcd-sz</code>(<code>~6.28ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~6.28ms</code>) is <strong>~10% higher than</strong> <code>mn-etcd-mz</code>(<code>~5.3ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>167.381</td><td style=text-align:center>5.96ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>158.822</td><td style=text-align:center>6.28ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>187.94</td><td style=text-align:center>5.3ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~15% less</strong> for <code>sn-etcd-sz</code>, <code>mn-etcd-sz</code>, <code>mn-etcd-mz</code></p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>184.95</td><td style=text-align:center>5.398ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>176.901</td><td style=text-align:center>5.64ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>209.99</td><td style=text-align:center>4.7ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> with single client.</p><ul><li><p><code>mn-etcd-sz</code> latency(<code>~6.66ms</code>) is <strong>~10% higher than</strong> <code>mn-etcd-mz</code>(<code>~6.16ms</code>).</p></li><li><p><em>Compare to <code>leader</code>, <code>follower</code> read request latency is <strong>~10% high</strong> for <code>mn-etcd-sz</code></em></p></li><li><p><em>Compare to <code>leader</code>, <code>follower</code> read request latency is <strong>~20% high</strong> for <code>mn-etcd-mz</code></em></p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>150.680</td><td style=text-align:center>6.66ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>162.072</td><td style=text-align:center>6.16ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~15% less</strong> for <code>mn-etcd-sz</code>(<code>~5.84ms</code>), <code>mn-etcd-mz</code>(<code>~5.01ms</code>).</p></li><li><p><em>Compare to <code>leader</code>, <code>follower</code> read request latency is <strong>~5% slightly high</strong> for <code>mn-etcd-sz</code>, <code>mn-etcd-mz</code></em></p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>170.918</td><td style=text-align:center>5.84ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>199.01</td><td style=text-align:center>5.01ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>leader</code> with multiple clients.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~1.593secs</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-sz</code>(<code>~1.974secs</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~1.974secs</code>) is <strong>~5% greater than</strong> <code>mn-etcd-mz</code>(<code>~1.81secs</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>252.149</td><td style=text-align:center>1.593secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>205.589</td><td style=text-align:center>1.974secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>230.42</td><td style=text-align:center>1.81secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p><em>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>more or less same</strong> for <code>sn-etcd-sz</code>(<code>~1.57961secs</code>), <code>mn-etcd-mz</code>(<code>~1.8secs</code>) not a big difference</em></p></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~10% high</strong> for <code>mn-etcd-sz</code>(<code>~ 2.277secs</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>252.406</td><td style=text-align:center>1.57961secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>181.905</td><td style=text-align:center>2.277secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>227.64</td><td style=text-align:center>1.8secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> with multiple client.</p><ul><li><p><code>mn-etcd-sz</code> latency is <strong>~20% less than</strong> <code>mn-etcd-mz</code>.</p></li><li><p>Compare to <code>leader</code> consistency <code>Linearizable</code>, <code>follower</code> read request latency is ~15 less for <code>mn-etcd-sz</code>(<code>~1.694secs</code>).</p></li><li><p>Compare to <code>leader</code> consistency <code>Linearizable</code>, <code>follower</code> read request latency is ~10% higher for <code>mn-etcd-sz</code>(<code>~1.977secs</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>248.489</td><td style=text-align:center>1.694secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>210.22</td><td style=text-align:center>1.977secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-2</td><td style=text-align:center>205.765</td><td style=text-align:center>1.967secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-2</td><td style=text-align:center>195.2</td><td style=text-align:center>2.159secs</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>231.458</td><td style=text-align:center>1.7413secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>214.80</td><td style=text-align:center>1.907secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-2</td><td style=text-align:center>183.320</td><td style=text-align:center>2.2810secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-2</td><td style=text-align:center>195.40</td><td style=text-align:center>2.164secs</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>leader</code> all keys.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~8.993secs</code>) is <strong>~3% slightly lower than</strong> <code>mn-etcd-sz</code>(<code>~9.236secs</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~9.236secs</code>) is <strong>~2% slightly lower than</strong> <code>mn-etcd-mz</code>(<code>~9.100secs</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>0.5139</td><td style=text-align:center>8.993secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>0.506</td><td style=text-align:center>9.236secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>0.508</td><td style=text-align:center>9.100secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>sn-etcd-sz</code>(<code>~9.secs</code>) is <strong>a slight difference <code>10ms</code></strong>.</p></li><li><p>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>mn-etcd-sz</code>(<code>~9.113secs</code>) is <strong>~1% less</strong>, not a big difference.</p></li><li><p>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>mn-etcd-mz</code>(<code>~8.799secs</code>) is <strong>~3% less</strong>, not a big difference.</p></li><li><p><code>sn-etcd-sz</code> latency(<code>~9.secs</code>) is <strong>~1% slightly less than</strong> <code>mn-etcd-sz</code>(<code>~9.113secs</code>).</p></li><li><p><em><code>mn-etcd-sz</code> latency(<code>~9.113secs</code>) is <strong>~3% slightly higher than</strong> <code>mn-etcd-mz</code>(<code>~8.799secs</code>)</em>.</p><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>0.51125</td><td style=text-align:center>9.0003secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>0.4993</td><td style=text-align:center>9.113secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>0.522</td><td style=text-align:center>8.799secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> all keys</p><ul><li><p><code>mn-etcd-sz</code> latency(<code>~9.065secs</code>) is <strong>~1% slightly higher than</strong> <code>mn-etcd-mz</code>(<code>~9.007secs</code>).</p></li><li><p>Compare to <code>leader</code> consistency <code>Linearizable</code>read request, <code>follower</code> is <em>~1% slightly higher</em> for both cases <code>mn-etcd-sz</code>, <code>mn-etcd-mz</code> .</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>0.512</td><td style=text-align:center>9.065secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>0.533</td><td style=text-align:center>9.007secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>mn-etcd-sz</code>(<code>~9.553secs</code>) is <strong>~5% high</strong>.</p></li><li><p><em>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>mn-etcd-mz</code>(<code>~7.7433secs</code>) is <strong>~15% less</strong></em>.</p></li><li><p><em><code>mn-etcd-sz</code>(<code>~9.553secs</code>) latency is <strong>~20% higher than</strong> <code>mn-etcd-mz</code>(<code>~7.7433secs</code>)</em>.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>0.4743</td><td style=text-align:center>9.553secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>0.5500</td><td style=text-align:center>7.7433secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li></ul><hr><br></details></li></ul><hr><br><br><blockquote><p>NOTE: This Network latency analysis is inspired by <a href=https://etcd.io/docs/v3.5/op-guide/performance/>etcd performance</a>.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-c2230bfa442d7dc5bd7c232f0253c256>3.16 - EtcdMember Custom Resource</h1><h1 id=dep-04-etcdmember-custom-resource>DEP-04: EtcdMember Custom Resource</h1><h2 id=summary>Summary</h2><p>Today, <a href=https://github.com/gardener/etcd-druid>etcd-druid</a> mainly acts as an etcd cluster provisioner, and seldom takes remediatory actions if the <a href=https://etcd.io/>etcd</a> cluster goes into an undesired state that needs to be resolved by a human operator. In other words, etcd-druid cannot perform day-2 operations on etcd clusters in its current form, and hence cannot carry out its full set of responsibilities as a true &ldquo;operator&rdquo; of etcd clusters. For etcd-druid to be fully capable of its responsibilities, it must know the latest state of the etcd clusters and their individual members at all times.</p><p>This proposal aims to bridge that gap by introducing <code>EtcdMember</code> custom resource allowing individual etcd cluster members to publish information/state (previously unknown to etcd-druid). This provides etcd-druid a handle to potentially take cluster-scoped remediatory actions.</p><h2 id=terminology>Terminology</h2><ul><li><p><strong>druid:</strong> <a href=https://github.com/gardener/etcd-druid>etcd-druid</a> - an operator for etcd clusters.</p></li><li><p><strong>etcd-member:</strong> A single etcd pod in an etcd cluster that is realised as a <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a>.</p></li><li><p><strong>backup-sidecar:</strong> It is the etcd-backup-restore sidecar container in each etcd-member pod.</p><blockquote><p><strong>NOTE:</strong> Term sidecar can now be confused with the latest definition in <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/753-sidecar-containers/README.md>KEP-73</a>. etcd-backup-restore container is currently not set as an <code>init-container</code> as proposed in the KEP but as a regular container in a multi-container [Pod](<a href=https://kubernetes.io/docs/concepts/workloads/pods/>Pods | Kubernetes</a>).</p></blockquote></li><li><p><strong>leading-backup-sidecar:</strong> A backup-sidecar that is associated to an etcd leader.</p></li><li><p><strong>restoration:</strong> It refers to an individual etcd-member restoring etcd data from an existing backup (comprising of full and delta snapshots). The authors have deliberately chosen to distinguish between restoration and learning. Learning refers to a process where a <a href=https://etcd.io/docs/v3.3/learning/learner/#raft-learner>learner</a> &ldquo;learns&rdquo; from an etcd-cluster leader.</p></li></ul><h2 id=motivation>Motivation</h2><p>Sharing state of an individual etcd-member with druid is essential for diagnostics, monitoring, cluster-wide-operations and potential remediation. At present, only a subset of etcd-member state is shared with druid using <a href=https://kubernetes.io/docs/concepts/architecture/leases/>leases</a>. It was always meant as a stopgap arrangement as mentioned in the <a href=https://github.com/gardener/etcd-druid/pull/207>corresponding issue</a> and is not the best use of leases.</p><p>There is a need to have a clear distinction between an etcd-member state and etcd cluster state since most of an etcd cluster state is often derived by looking at individual etcd-member states. In addition, actors which update each of these states should be clearly identified so as to prevent multiple actors updating a single resource holding the state of either an etcd cluster or an etcd-member. As a consequence, etcd-members should not directly update the <code>Etcd</code> resource status and would therefore need a new custom resource allowing each member to publish detailed information about its latest state.</p><h3 id=goals>Goals</h3><ul><li>Introduce <code>EtcdMember</code> custom resource via which each etcd-member can publish information about its state. This enables druid to deterministically orchestrate out-of-turn operations like compaction, defragmentation, volume management etc.</li><li>Define and capture states, sub-states and deterministic transitions amongst states of an etcd-member.</li><li>Today <a href=https://kubernetes.io/docs/concepts/architecture/leases/>leases</a> are <em>misused</em> to share member-specific information with druid. Their usage to share member state [leader, follower, learner], member-id, snapshot revisions etc should be removed.</li></ul><h3 id=non-goals>Non-Goals</h3><ul><li>Auto-recovery from quorum loss or cluster-split due to network partitioning.</li><li>Auto-recovery of an etcd-member due to volume mismatch.</li><li>Relooking at segregating responsiblities between <code>etcd</code> and <code>backup-sidecar </code>containers.</li></ul><h2 id=proposal>Proposal</h2><p>This proposal introduces a new custom resource <code>EtcdMember</code>, and in the following sections describes different sets of information that should be captured as part of the new resource.</p><h3 id=etcd-member-metadata>Etcd Member Metadata</h3><p>Every etcd-member has a unique <code>memberID</code> and it is part of an etcd cluster which has a unique <code>clusterID</code>. In a well-formed etcd cluster every member must have the same <code>clusterID</code>. Publishing this information to druid helps in identifying issues when one or more etcd-members form their own individual clusters, thus resulting in multiple clusters where only one was expected. Issues <a href=https://github.com/gardener/etcd-druid/issues/419>Issue#419</a>, Canary#4027, Canary#3973 are some such occurrences.</p><p>Today, this information is published by using a member <a href=https://kubernetes.io/docs/concepts/architecture/leases/>lease</a>. Both these fields are populated in the leases&rsquo; <code>Spec.HolderIdentity</code> by the backup-sidecar container.</p><p>The authors propose to publish member metadata information in <code>EtcdMember</code> resource.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>id: &lt;etcd-member id&gt;
</span></span><span style=display:flex><span>clusterID: &lt;etcd cluster id&gt;
</span></span></code></pre></div><blockquote><p><strong>NOTE:</strong> Druid would not do any auto-recovery when it finds out that there are more than one clusters being formed. Instead this information today will be used for diagnostic and alerting.</p></blockquote><h3 id=etcd-member-state-transitions>Etcd Member State Transitions</h3><p>Each etcd-member goes through different <code>States</code> during its lifetime. <code>State</code> is a derived high-level summary of where an etcd-member is in its lifecycle. A <code>SubState</code> gives additional information about the state. This proposal extends the concept of states with the notion of a <code>SubState</code>, since <code>State</code> indicates a top-level state of an <code>EtcdMember</code> resource, which can have one or more <code>SubState</code>s.</p><p>While <code>State</code> is sufficient for many human operators, the notion of a <code>SubState</code> provides operators with an insight about the discrete stage of an etcd-member in its lifecycle. For example, consider a top-level <code>State: Starting</code>, which indicates that an etcd-member is starting. <code>Starting</code> is meant to be a transient state for an etcd-member. If an etcd-member remains in this <code>State</code> longer than expected, then an operator would require additional insight, which the authors propose to provide via <code>SubState</code> (in this case, the possible <code>SubStates</code> could be <code>PendingLearner</code> and <code>Learner</code>, which are detailed in the following sections).</p><p>At present, these states are not captured and only the final state is known - i.e the etcd-member either fails to come up (all re-attempts to bring up the pod via the StatefulSet controller has exhausted) or it comes up. Getting an insight into all its state transitions would help in diagnostics.</p><p>The status of an etcd-member at any given point in time can be best categorized as a combination of a top-level <code>State</code> and a <code>SubState</code>. The authors propose to introduce the following states and sub-states:</p><h4 id=states-and-sub-states>States and Sub-States</h4><blockquote><p><strong>NOTE</strong>: Abbreviations have been used wherever possible, only to represent sub-states. These representations are chosen only for brevity and will have proper longer names.</p></blockquote><table><thead><tr><th>States</th><th>Sub-States</th><th>Description</th></tr></thead><tbody><tr><td>New</td><td>-</td><td>Every newly created etcd-member will start in this state and is termed as the initial state or the start state.</td></tr><tr><td>Initializing</td><td>DBV-S (DBValidationSanity)</td><td>This state denotes that backup-restore container in etcd-member pod has started initialization. Sub-State <code>DBV-S</code> which is an abbreviation for <code>DBValidationSanity</code> denotes that currently sanity etcd DB validation is in progress.</td></tr><tr><td>Initializing</td><td>DBV-F (DBValidationFull)</td><td>This state denotes that backup-restore container in etcd-member pod has started initialization. Sub-State <code>DBV-F</code> which is an abbreviation for <code>DBValidationFull</code> denotes that currently full etcd DB validation is in progress.</td></tr><tr><td>Initializing</td><td>R (Restoration)</td><td>This state denotes that backup-restore container in etcd-member pod has started initialization. Sub-State <code>R</code> which is an abbreviation for <code>Restoration</code> denotes that DB validation failed and now backup-restore has commenced restoration of etcd DB from the backup (comprising of full snapshot and delta-snapshots). An etcd-member will transition to this sub-state only when it is part of a single-node etcd-cluster.</td></tr><tr><td>Starting (SI)</td><td>PL (PendingLearner)</td><td>An etcd-member can transition from <code>Initializing</code> state to <code>PendingLearner</code> state. In this state backup-restore container will optionally delete any existing etcd data directory and then attempts to add its peer etcd-member process as a learner. Since there can be only one learner at a time in an etcd cluster, an etcd-member could be in this state for some time till its request to get added as a learner is accepted.</td></tr><tr><td>Starting (SI)</td><td>Learner</td><td>When backup-restore is successfully able to add its peer etcd-member process as a <code>Learner</code>. In this state the etcd-member process will start its DB sync from an etcd leader.</td></tr><tr><td>Started (Sd)</td><td>Follower</td><td>A follower is a voting raft member. A <code>Learner</code> etcd-member will get promoted to a <code>Follower</code> once its DB is in sync with the leader. It could also become a follower if during a re-election it loses leadership and transitions from being a <code>Leader</code> to <code>Follower</code>.</td></tr><tr><td>Started (Sd)</td><td>Leader</td><td>A leader is an etcd-member which will handle all client write requests and linearizable read requests. A member could transition to being a <code>Leader</code> from an existing <code>Follower</code> role due to winning a leader election or for a single node etcd cluster it directly transitions from <code>Initializing</code> state to <code>Leader</code> state as there is no other member.</td></tr></tbody></table><p>In the following sub-sections, the state transitions are categorized into several flows making it easier to grasp the different transitions.</p><h4 id=top-level-state-transitions>Top Level State Transitions</h4><p>Following DFA represents top level state transitions (without any representation of sub-states). As described in the table above there are 4 top level states:</p><ul><li><p><code>New</code>- this is a start state for all newly created etcd-members</p></li><li><p><code>Initializing</code> - In this state backup-restore will perform pre-requisite actions before it triggers the start of an etcd process. DB validation and optionally restoration is done in this state. Possible sub-states are: <code>DBValidationSanity</code>, <code>DBValidationFull</code> and <code>Restoration</code></p></li><li><p><code>Starting</code> - Once the optional initialization is done backup-restore will trigger the start of an etcd process. It can either directly go to <code>Learner</code> sub-state or wait for getting added as a learner and therefore be in <code>PendingLearner</code> sub-state.</p></li><li><p><code>Started</code> - In this state the etcd-member is a full voting member. It can either be in <code>Leader</code> or <code>Follower</code> sub-states.</p></li></ul><img title src=/__resources/04-top-level-state-transitions.excalidraw_0af3b5.png alt data-align=center><h4 id=starting-an-etcd-member-in-a-single-node-etcd-cluster>Starting an Etcd-Member in a Single-Node Etcd Cluster</h4><p>Following DFA represents the states, sub-states and transitions of a single etcd-member for a cluster that is bootstrapped from cluster size of 0 -> 1.</p><img title src=/__resources/04-state-transitions-bootstrap-0-1.excalidraw_5a2726.png alt data-align=center><h4 id=addition-of-a-new-etcd-member-in-a-multi-node-etcd-cluster>Addition of a New Etcd-Member in a Multi-Node Etcd Cluster</h4><p>Following DFA represents the states, sub-states and transitions of an etcd cluster which starts with having a single member (Leader) and then one or more new members are added which represents a scale-up of an etcd cluster from 1 -> n, where n is <a href=https://etcd.io/docs/v3.5/faq/#why-an-odd-number-of-cluster-members>odd</a>.</p><img title src=/__resources/04-state-transitions-scaleup-1-n.excalidraw_cfa4bf.png alt data-align=center><h4 id=restart-of-a-voting-etcd-member-in-a-multi-node-etcd-cluster>Restart of a Voting Etcd-Member in a Multi-Node Etcd Cluster</h4><p>Following DFA represents the states, sub-states and transitions when a voting etcd-member in a multi-node etcd cluster restarts.</p><img title src=/__resources/04-state-transitions-restart-member.excalidraw_d23a8d.png alt data-align=center><blockquote><p>NOTE: If the DB validation fails then data directory of the etcd-member is removed and etcd-member is removed from cluster membership, thus transitioning it to <code>New</code> state. The state transitions from <code>New</code> state are depicted by <a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#addition-of-a-new-etcd-member-in-a-multi-node-etcd-cluster>this section</a>.</p></blockquote><h3 id=deterministic-etcd-member-creationrestart-during-scale-up>Deterministic Etcd Member Creation/Restart During Scale-Up</h3><p><strong>Bootstrap information:</strong></p><p>When an etcd-member starts, then it needs to find out:</p><ul><li><p>If it should join an existing cluster or start a new cluster.</p></li><li><p>If it should add itself as a <code>Learner</code> or directly start as a voting member.</p></li></ul><p><strong>Issue with the current approach</strong>:</p><p><em>At present</em>, this is facilitated by three things:</p><ol><li><p>During scale-up, druid adds an annotation <code>gardener.cloud/scaled-to-multi-node</code> to the <code>StatefulSet</code>. Each etcd-members looks up this annotation.</p></li><li><p>backup-sidecar attempts to fetch etcd cluster member-list and checks if this etcd-member is already part of the cluster.</p></li><li><p>Size of the cluster by checking <code>initial-cluster</code> in the etcd config.</p></li></ol><p>Druid adds an annotation <code>gardener.cloud/scaled-to-multi-node</code> on the <code>StatefulSet</code> which is then shared by all etcd-members irrespective of the starting state of an etcd-member (as <code>Learner</code> or <code>Voting-Member</code>). This especially creates an issue for the current leader (often pod with index 0) during the scale-up of an etcd cluster as described in <a href=https://github.com/gardener/etcd-backup-restore/issues/646>this</a> issue.</p><p>It has been agreed that the current solution to <a href=https://github.com/gardener/etcd-backup-restore/issues/646>this issue</a> is a quick and dirty fix and needs to be revisited to be uniformly applied to all etcd-members. The authors propose to provide a more deterministic approach to scale-up using the <code>EtcdMember</code> resource.</p><p><strong>New approach</strong></p><p>Instead of adding an annotation <code>gardener.cloud/scaled-to-multi-node</code> on the <code>StatefulSet</code>, a new annotation <code>druid.gardener.cloud/create-as-learner</code> should be added by druid on an <code>EtcdMember</code> resource. This annotation will only be added to newly created members during scale-up.</p><p>Each etcd-member should look at the following to deterministically compute the <code>bootstrap information</code> specified above:</p><ul><li><p><code>druid.gardener.cloud/create-as-learner</code> annotation on its respective <code>EtcdMember</code> resource. This new annotation will be honored in the following cases:</p><ul><li><p>When an etcd-member is created for the very first time.</p></li><li><p>An etcd-member is restarted while it is in <code>Starting</code> state (<code>PendingLearner</code> and <code>Learner</code> sub-states).</p></li></ul></li><li><p>Etcd-cluster member list. to check if it is already part of the cluster.</p></li><li><p>Existing etcd data directory and its validity.</p></li></ul><blockquote><p><strong>NOTE:</strong> When the etcd-member gets promoted to a voting-member, then it should remove the annotation on its respective <code>EtcdMember</code> resource.</p></blockquote><h3 id=tls-enablement-for-peer-communication>TLS Enablement for Peer Communication</h3><p>Etcd-members in a cluster use <a href=https://etcd.io/docs/v3.4/op-guide/configuration/#--initial-advertise-peer-urls>peer URL(s)</a> to communicate amongst each other. If the advertised peer URL(s) for an etcd-member are updated then <a href=https://etcd.io/docs/v3.4/op-guide/runtime-configuration/#update-advertise-peer-urls>etcd mandates a restart</a> of the etcd-member.</p><p>Druid only supports toggling the transport level security for the advertised peer URL(s). To indicate that the etcd process within the etcd-member has the updated advertised peer URL(s), an annotation <code>member.etcd.gardener.cloud/tls-enabled</code> is added by backup-sidecar container to the member lease object.</p><p>During the reconciliation run for an <code>Etcd</code> resource in druid, if reconciler detects a change in advertised peer URL(s) TLS configuration then it will watch for the above mentioned annotation on the member lease. If the annotation has a value of <code>false</code> then it will trigger a restart of the etcd-member pod.</p><p>The authors propose to publish member metadata information in <code>EtcdMember</code> resource and not misuse member <a href=https://kubernetes.io/docs/concepts/architecture/leases/>lease</a>s.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>peerTLSEnabled: &lt;bool&gt;
</span></span></code></pre></div><h3 id=monitoring-backup-health>Monitoring Backup Health</h3><p>Backup-sidecar takes delta and full snapshot both periodically and threshold based. These backed-up snapshots are essential for restoration operations for bootstrapping an etcd cluster from 0 -> 1 replicas. It is essential that leading-backup-sidecar container which is responsible for taking delta/full snapshots and uploading these snapshots to the configured backup store, publishes this information for druid to consume.</p><p>At present, information about backed-up snapshot (<em>only</em> <code>latest-revision-number</code>) is published by leading-backup-sidecar container by updating <code>Spec.HolderIdentity</code> of the delta-snapshot and full-snapshot <a href=https://kubernetes.io/docs/concepts/architecture/leases/>lease</a>s.</p><p>Druid maintains <code>conditions</code> in the <code>Etcd</code> resource status, which include but are not limited to maintaining information on whether backups being taken for an etcd cluster are healthy (up-to-date) or stale (outdated in context to a configured schedule). Druid computes these conditions using information from full/delta snapshot leases.</p><p>In order to provide a holistic view of the health of backups to human operators, druid requires additional information about the snapshots that are being backed-up. The authors propose to not misuse <a href=https://kubernetes.io/docs/concepts/architecture/leases/>lease</a>s and instead publish the following snapshot information as part <code>EtcdMember</code> custom resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>snapshots:
</span></span><span style=display:flex><span>  lastFull:
</span></span><span style=display:flex><span>    timestamp: &lt;time of full snapshot&gt;
</span></span><span style=display:flex><span>    name: &lt;name of the file that is uploaded&gt;
</span></span><span style=display:flex><span>    size: &lt;size of the un-compressed snapshot file uploaded&gt;
</span></span><span style=display:flex><span>    startRevision: &lt;start revision of etcd db captured in the snapshot&gt;
</span></span><span style=display:flex><span>    endRevision: &lt;end revision of etcd db captured in the snapshot&gt;
</span></span><span style=display:flex><span>  lastDelta:
</span></span><span style=display:flex><span>    timestamp: &lt;time of delta snapshot&gt;
</span></span><span style=display:flex><span>    name: &lt;name of the file that is uploaded&gt;
</span></span><span style=display:flex><span>    size: &lt;size of the un-compressed snapshot file uploaded&gt;
</span></span><span style=display:flex><span>    startRevision: &lt;start revision of etcd db captured in the snapshot&gt;
</span></span><span style=display:flex><span>    endRevision: &lt;end revision of etcd db captured in the snapshot&gt;
</span></span></code></pre></div><p>While this information will primarily help druid compute accurate conditions regarding backup health from snapshot information and publish this to human operators, it could be further utilised by human operators to take remediatory actions (e.g. manually triggering a full or delta snapshot or further restarting the leader if the issue is still not resolved) if backup is unhealthy.</p><h3 id=enhanced-snapshot-compaction>Enhanced Snapshot Compaction</h3><p>Druid can be configured to perform regular snapshot compactions for etcd clusters, to reduce the total number of delta snapshots to be restored if and when a DB restoration for an etcd cluster is required. Druid triggers a snapshot compaction job when the accumulated etcd events in the latest set of delta snapshots (taken after the last full snapshot) crosses a specified threshold.</p><p>As described in <a href=https://github.com/gardener/etcd-druid/issues/591>Issue#591</a> scheduling compaction only based on number of accumulated etcd events is not sufficient to ensure a successful compaction. This is specifically targeted for kubernetes clusters where each etcd event is larger in size owing to large spec or status fields or respective resources.</p><p>Druid will now need information regarding snapshot sizes, and more importantly the total size of accumulated delta snapshots since the last full snapshot.</p><p>The authors propose to enhance the proposed <code>snapshots</code> field described in <a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#use-case-3-monitoring-backup-health>Use Case #3</a> with the following additional field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>snapshots:
</span></span><span style=display:flex><span>  accumulatedDeltaSize: &lt;total size of delta snapshots since last full snapshot&gt;
</span></span></code></pre></div><p>Druid can then use this information in addition to the existing revision information to decide to trigger an early snapshot compaction job. This effectively allows druid to be proactive in performing regular compactions for etcds receiving large events, reducing the probability of a failed snapshot compaction or restoration.</p><h3 id=enhanced-defragmentation>Enhanced Defragmentation</h3><p>Reader is recommended to read <a href=https://etcd.io/docs/v3.4/op-guide/maintenance/>Etcd Compaction & Defragmentation</a> in order to understand the following terminology:</p><p><a href=https://github.com/etcd-io/etcd/blob/a603c0798948941d453b158af794edab1a8230be/mvcc/backend/backend.go#L60-L64><code>dbSize</code></a> - total storage space used by the etcd database</p><p><a href=https://github.com/etcd-io/etcd/blob/a603c0798948941d453b158af794edab1a8230be/mvcc/backend/backend.go#L65-L68><code>dbSizeInUse</code></a> - logical storage space used by the etcd database, not accounting for free pages in the DB due to etcd history compaction</p><p>The leading-backup-sidecar performs periodic defragmentations of the DBs of all the etcd-members in the cluster, controlled via a defragmentation cron schedule provided to each backup-sidecar. Defragmentation is a costly maintenance operation and causes a brief downtime to the etcd-member being defragmented, due to which the leading-backup-sidecar defragments each etcd-member sequentially. This ensures that only one etcd-member would be unavailable at any given time, thus avoiding an accidental quorum loss in the etcd cluster.</p><p>The authors propose to move the responsibility of orchestrating these individual defragmentations to druid due to the following reasons:</p><ul><li>Since each backup-sidecar only has knowledge of the health of its own etcd, it can only determine whether its own etcd can be defragmented or not, based on etcd-member health. Trying to defragment a different healthy etcd-member while another etcd-member is unhealthy would lead to a transient quorum loss.</li><li>Each backup-sidecar is only a <code>sidecar</code> to its own etcd-member, and by good design principles, it must not be performing any cluster-wide maintenance operations, and this responsibility should remain with the etcd cluster operator.</li></ul><p>Additionally, defragmentation of an etcd DB becomes inevitable if the DB size exceeds the specified DB space quota, since the etcd DB then becomes read-only, ie no write operations on the etcd would be possible unless the etcd DB is defragmented and storage space is freed up. In order to automate this, druid will now need information about the etcd DB size from each member, specifically the leading etcd-member, so that a cluster-wide defragmentation can be triggered if the DB size reaches a certain threshold, as already described by <a href=https://github.com/gardener/etcd-backup-restore/issues/556>this issue</a>.</p><p>The authors propose to enhance each etcd-member to regularly publish information about the <code>dbSize</code> and <code>dbSizeInUse</code> so that druid may trigger defragmentation for the etcd cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>dbSize: &lt;db-size&gt; <span style=color:green># e.g 6Gi</span>
</span></span><span style=display:flex><span>dbSizeInUse: &lt;db-size-in-use&gt; <span style=color:green># e.g 3.5Gi</span>
</span></span></code></pre></div><p>Difference between <code>dbSize</code> and <code>dbSizeInUse</code> gives a clear indication of how much storage space would be freed up if a defragmentation is performed. If the difference is not significant (based on a configurable threshold provided to druid), then no defragmentation should be performed. This will ensure that druid does not perform frequent defragmentations that do not yield much benefit. Effectively it is to maximise the benefit of defragmentation since this operations involves transient downtime for each etcd-member.</p><h3 id=monitoring-defragmentations>Monitoring Defragmentations</h3><p>As discussed in the previous section, every etcd-member is defragmented periodically, and can also be defragmented based on the DB size reaching a certain threshold. It is beneficial for druid to have knowledge of this data from each etcd-member for the following reasons:</p><ul><li><p>[<strong>Diagnostics</strong>] It is expected that <code>backup-sidecar</code> will push releveant metrics and configure alerts on these metrics.</p></li><li><p>[<strong>Operational</strong>] Derive status of defragmentation at etcd cluster level. In case of partial failures for a subset of etcd-members druid can potentially re-trigger defragmentation only for those etcd-members.</p></li></ul><p>The authors propose to capture this information as part of <code>lastDefragmentation</code> section in the <code>EtcdMember</code> resource.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>lastDefragmentation:
</span></span><span style=display:flex><span>  startTime: &lt;start time of defragmentation&gt;
</span></span><span style=display:flex><span>  endTime: &lt;end time of defragmentation&gt;
</span></span><span style=display:flex><span>  status: &lt;Succeeded | Failed&gt;
</span></span><span style=display:flex><span>  message: &lt;success or failure message&gt;
</span></span><span style=display:flex><span>  initialDBSize: &lt;size of etcd DB prior to defragmentation&gt;
</span></span><span style=display:flex><span>  finalDBSize: &lt;size of etcd DB post defragmentation&gt;
</span></span></code></pre></div><blockquote><p><strong>NOTE</strong>: Defragmentation is a cluster-wide operation, and insights derived from aggregating defragmentation data from individual etcd-members would be captured in the <code>Etcd</code> resource status</p></blockquote><h3 id=monitoring-restorations>Monitoring Restorations</h3><p>Each etcd-member may perform restoration of data multiple times throughout its lifecycle, possibly owing to data corruptions. It would be useful to capture this information as part of an <code>EtcdMember</code> resource, for the following use cases:</p><ul><li><p>[<strong>Diagnostics</strong>] It is expected that <code>backup-sidecar</code> will push a metric indicating failure to restore.</p></li><li><p>[<strong>Operational</strong>] Restoration from backup-bucket only happens for a single node etcd cluster. If restoration is failing then druid cannot take any remediatory actions since there is no etcd quorum.</p></li></ul><p>The authors propose to capture this information under <code>lastRestoration</code> section in the <code>EtcdMember</code> resource.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>lastRestoration:
</span></span><span style=display:flex><span>  status: &lt;Failed | Success | In-Progress&gt;
</span></span><span style=display:flex><span>  reason: &lt;reason-code for status&gt;
</span></span><span style=display:flex><span>  message: &lt;human readable message for status&gt;
</span></span><span style=display:flex><span>  startTime: &lt;start time of restoration&gt;
</span></span><span style=display:flex><span>  endTime: &lt;end time of restoration&gt;
</span></span></code></pre></div><p>Authors have considered the following cases to better understand how errors during restoration will be handled:</p><p><strong>Case #1</strong> - <em>Failure to connect to Provider Object Store</em></p><p>At present full and delta snapshots are downloaded during restoration. If there is a failure then initialization status transitions to <code>Failed</code> followed by <code>New</code> which forces <code>etcd-wrapper</code> to trigger the initialization again. This in a way forces a retry and currently there is no limit on the number of attempts.</p><p>Authors propose to improve the retry logic but keep the overall behavior of not forcing a container restart the same.</p><p><strong>Case #2</strong> - <em>Read-Only Mounted volume</em></p><p>If a mounted volume which is used to create the etcd data directory turns <code>read-only</code> then authors propose to capture this state via <code>EtcdMember</code>.</p><p>Authors propose that <code>druid</code> should initiate recovery by deleting the PVC for this etcd-member and letting <code>StatefulSet</code> controller re-create the Pod and the PVC. Removing PVC and deleting the pod is considered safe because:</p><ul><li>Data directory is present and is the DB is corrupt resulting in an un-usasble etcd.</li><li>Data directory is not present but any attempt to create a directory structure fails due to <code>read-only</code> FS.</li></ul><p>In both these cases there is no side-effect of deleting the PVC and the Pod.</p><p><strong>Case #3</strong> - <em>Revision mismatch</em></p><p>There is currently an issue in <code>backup-sidecar</code> which results in a revision mismatch in the snapshots (full/delta) taken by leading the <code>backup-sidecar</code> container. This results in a restoration failure. One occurance of such issue has been captured in <a href=https://github.com/gardener/etcd-backup-restore/issues/583>Issue#583</a>. This occurence points to a bug which should be fixed however there is a rare possibility that these snapshots (full/delta) get corrupted. In this rare situation, <code>backup-sidecar</code> should only raise an alert.</p><p>Authors propose that <code>druid</code> should not take any remediatory actions as this involves:</p><ul><li>Inspecting snapshots</li><li>If the full snapshot is corrupt then a decision needs to be taken to recover from the last full snapshot as the base snapshot. This can result in data loss and therefore needs manual intervention.</li><li>If a delta snapshot is corrupt, then recovery can be done till the corrupt revision in the delta snapshot. Since this will also result in a loss of data therefore this decision needs to be take by an operator.</li></ul><h3 id=monitoring-volume-mismatches>Monitoring Volume Mismatches</h3><p>Each etcd-member checks for possible etcd data volume mismatches, based on which it decides whether to start the etcd process or not, but this information is not captured anywhere today. It would be beneficial to capture this information as part of the <code>EtcdMember</code> resource so that a human operator may check this and manually fix the underlying problem with the wrong volume being attached or mounted to an etcd-member pod.</p><p>The authors propose to capture this information under <code>volumeMismatches</code> section in the <code>EtcdMember</code> resource.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>volumeMismatches:
</span></span><span style=display:flex><span>- identifiedAt: &lt;time at which wrong volume mount was identified&gt;
</span></span><span style=display:flex><span>  fixedAt: &lt;time at which correct volume was mounted&gt;
</span></span><span style=display:flex><span>  volumeID: &lt;volume ID of wrong volume that got mounted&gt;
</span></span><span style=display:flex><span>  numRestarts: &lt;num of etcd-member restarts that were attempted&gt;
</span></span></code></pre></div><p>Each entry under <code>volumeMismatches</code> will be for a unique <code>volumeID</code>. If there is a pod restart and it results in yet another unexpected <code>volumeID</code> (different from the already captured volumeIDs) then a new entry will get created. <code>numRestarts</code> denotes the number of restarts seen by the etcd-member for a specific <code>volumeID</code>.</p><p>Based on information from the <code>volumeMismatches</code> section, druid <em>may</em> choose to perform rudimentary remediatory actions as simple as restarting the member pod to force a possible rescheduling of the pod to a different node which could potentially force the correct volume to be mounted to the member.</p><h3 id=custom-resource-api>Custom Resource API</h3><h4 id=spec-vs-status>Spec vs Status</h4><p>Information that is captured in the etcd-member custom resource could be represented either as <code>EtcdMember.Status</code> or <code>EtcdMemberState.Spec</code>.</p><p>Gardener has a similar need to capture a shoot state and they have taken the decision to represent it via <a href=https://github.com/gardener/gardener/blob/9e1a20aa9cc32fc806123003ba6079b284948767/pkg/apis/core/types_shootstate.go#L27-L33>ShootState</a> resource where the state or status of a shoot is captured as part of the <code>Spec</code> field in the <code>ShootState</code> custom resource.</p><p>The authors wish to instead align themselves with the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>K8S API conventions</a> and choose to use <code>EtcdMember</code> custom resource and capture the status of each member in <code>Status</code> field of this resource. This has the following advantages:</p><ul><li><p><code>Spec</code> represents a desired state of a resource and what is intended to be captured is the <code>As-Is</code> state of a resource which <code>Status</code> is meant to capture. Therefore, semantically using <code>Status</code> is the correct choice.</p></li><li><p>Not mis-using <code>Spec</code> now to represent <code>As-Is</code> state provides us with a choice to extend the custom resource with any future need for a <code>Spec</code> a.k.a desired state.</p></li></ul><h4 id=representing-state-transitions>Representing State Transitions</h4><p>The authors propose to use a custom representation for states, sub-states and transitions.</p><p>Consider the following representation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>transitions:
</span></span><span style=display:flex><span>- state: &lt;name of the state that the etcd-member has transitioned to&gt;
</span></span><span style=display:flex><span>  subState: &lt;name of the sub-state if any&gt;
</span></span><span style=display:flex><span>  reason: &lt;reason code for the transition&gt;
</span></span><span style=display:flex><span>  transitionTime: &lt;time of transition to this state&gt;
</span></span><span style=display:flex><span>  message: &lt;detailed message if any&gt;
</span></span></code></pre></div><p>As an example, consider the following transitions which represent addition of an etcd-member during scale-up of an etcd cluster, followed by a restart of the etcd-member which detects a corrupt DB:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  transitions:
</span></span><span style=display:flex><span>  - state: New
</span></span><span style=display:flex><span>    subState: New
</span></span><span style=display:flex><span>    reason: ClusterScaledUp
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T05:00:00Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;New member added due to etcd cluster scale-up&#34;</span>
</span></span><span style=display:flex><span>  - state: Starting
</span></span><span style=display:flex><span>    subState: PendingLearner
</span></span><span style=display:flex><span>    reason: WaitingToJoinAsLearner
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T05:00:30Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Waiting to join the cluster as a learner&#34;</span>
</span></span><span style=display:flex><span>  - state: Starting
</span></span><span style=display:flex><span>    subState: Learner
</span></span><span style=display:flex><span>    reason: JoinedAsLearner
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T05:01:20Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Joined the cluster as a learner&#34;</span>
</span></span><span style=display:flex><span>  - state: Started
</span></span><span style=display:flex><span>    subState: Follower
</span></span><span style=display:flex><span>    reason: PromotedAsVotingMember
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T05:02:00Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Now in sync with leader, promoted as voting member&#34;</span>
</span></span><span style=display:flex><span>  - state: Initializing
</span></span><span style=display:flex><span>    subState: DBValidationFull
</span></span><span style=display:flex><span>    reason: DetectedPreviousUncleanExit
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T08:00:00Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Detected previous unclean exit, requires full DB validation&#34;</span>
</span></span><span style=display:flex><span>  - state: New
</span></span><span style=display:flex><span>    subState: New
</span></span><span style=display:flex><span>    reason: DBCorruptionDetected
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T08:01:30Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Detected DB corruption during initialization, removing member from cluster&#34;</span>
</span></span><span style=display:flex><span>  - state: Starting
</span></span><span style=display:flex><span>    subState: PendingLearner
</span></span><span style=display:flex><span>    reason: WaitingToJoinAsLearner
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T08:02:10Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Waiting to join the cluster as a learner&#34;</span>
</span></span><span style=display:flex><span>  - state: Starting
</span></span><span style=display:flex><span>    subState: Learner
</span></span><span style=display:flex><span>    reason: JoinedAsLearner
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T08:02:20Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Joined the cluster as a learner&#34;</span>
</span></span><span style=display:flex><span>  - state: Started
</span></span><span style=display:flex><span>    subState: Follower
</span></span><span style=display:flex><span>    reason: PromotedAsVotingMember
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T08:04:00Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Now in sync with leader, promoted as voting member&#34;</span>
</span></span></code></pre></div><h5 id=reason-codes>Reason Codes</h5><p>The authors propose the following list of possible reason codes for transitions. This list is not exhaustive, and can be further enhanced to capture any new transitions in the future.</p><table><thead><tr><th>Reason</th><th>Transition From State (SubState)</th><th>Transition To State (SubState)</th></tr></thead><tbody><tr><td><code>ClusterScaledUp</code> | <code>NewSingleNodeClusterCreated</code></td><td>nil</td><td>New</td></tr><tr><td><code>DetectedPreviousCleanExit</code></td><td>New | Started (Leader) | Started (Follower)</td><td>Initializing (DBValidationSanity)</td></tr><tr><td><code>DetectedPreviousUncleanExit</code></td><td>New | Started (Leader) | Started (Follower)</td><td>Initializing (DBValidationFull)</td></tr><tr><td><code>DBValidationFailed</code></td><td>Initializing (DBValidationSanity) | Initializing (DBValidationFull)</td><td>Initializing (Restoration) | New</td></tr><tr><td><code>DBValidationSucceeded</code></td><td>Initializing (DBValidationSanity) | Initializing (DBValidationFull)</td><td>Started (Leader) | Started (Follower)</td></tr><tr><td><code>Initializing (Restoration)Succeeded</code></td><td>Initializing (Restoration)</td><td>Started (Leader)</td></tr><tr><td><code>WaitingToJoinAsLearner</code></td><td>New</td><td>Starting (PendingLearner)</td></tr><tr><td><code>JoinedAsLearner</code></td><td>Starting (PendingLearner)</td><td>Starting (Learner)</td></tr><tr><td><code>PromotedAsVotingMember</code></td><td>Starting (Learner)</td><td>Started (Follower)</td></tr><tr><td><code>GainedClusterLeadership</code></td><td>Started (Follower)</td><td>Started (Leader)</td></tr><tr><td><code>LostClusterLeadership</code></td><td>Started (Leader)</td><td>Started (Follower)</td></tr></tbody></table><h4 id=api>API</h4><h5 id=etcdmember>EtcdMember</h5><p>The authors propose to add the <code>EtcdMember</code> custom resource API to etcd-druid APIs and initially introduce it with <code>v1alpha1</code> version.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: druid.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: EtcdMember
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    gardener.cloud/owned-by: &lt;name of parent Etcd resource&gt;
</span></span><span style=display:flex><span>  name: &lt;name of the etcd-member&gt;
</span></span><span style=display:flex><span>  namespace: &lt;namespace | will be the same as that of parent Etcd resource&gt;
</span></span><span style=display:flex><span>  ownerReferences:
</span></span><span style=display:flex><span>  - apiVersion: druid.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    blockOwnerDeletion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    controller: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    kind: Etcd
</span></span><span style=display:flex><span>    name: &lt;name of the parent Etcd resource&gt;
</span></span><span style=display:flex><span>    uid: &lt;UID of the parent Etcd resource&gt; 
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  id: &lt;etcd-member id&gt;
</span></span><span style=display:flex><span>  clusterID: &lt;etcd cluster id&gt;
</span></span><span style=display:flex><span>  peerTLSEnabled: &lt;bool&gt;
</span></span><span style=display:flex><span>  dbSize: &lt;db-size&gt;
</span></span><span style=display:flex><span>  dbSizeInUse: &lt;db-size-in-use&gt;
</span></span><span style=display:flex><span>  snapshots:
</span></span><span style=display:flex><span>    lastFull:
</span></span><span style=display:flex><span>      timestamp: &lt;time of full snapshot&gt;
</span></span><span style=display:flex><span>      name: &lt;name of the file that is uploaded&gt;
</span></span><span style=display:flex><span>      size: &lt;size of the un-compressed snapshot file uploaded&gt;
</span></span><span style=display:flex><span>      startRevision: &lt;start revision of etcd db captured in the snapshot&gt;
</span></span><span style=display:flex><span>      endRevision: &lt;end revision of etcd db captured in the snapshot&gt;
</span></span><span style=display:flex><span>    lastDelta:
</span></span><span style=display:flex><span>      timestamp: &lt;time of delta snapshot&gt;
</span></span><span style=display:flex><span>      name: &lt;name of the file that is uploaded&gt;
</span></span><span style=display:flex><span>      size: &lt;size of the un-compressed snapshot file uploaded&gt;
</span></span><span style=display:flex><span>      startRevision: &lt;start revision of etcd db captured in the snapshot&gt;
</span></span><span style=display:flex><span>      endRevision: &lt;end revision of etcd db captured in the snapshot&gt;
</span></span><span style=display:flex><span>    accumulatedDeltaSize: &lt;total size of delta snapshots since last full snapshot&gt;
</span></span><span style=display:flex><span>  lastRestoration:
</span></span><span style=display:flex><span>    type: &lt;FromSnapshot | FromLeader&gt;
</span></span><span style=display:flex><span>    status: &lt;Failed | Success | In-Progress&gt;
</span></span><span style=display:flex><span>    startTime: &lt;start time of restoration&gt;
</span></span><span style=display:flex><span>    endTime: &lt;end time of restoration&gt;
</span></span><span style=display:flex><span>  lastDefragmentation:
</span></span><span style=display:flex><span>    startTime: &lt;start time of defragmentation&gt;
</span></span><span style=display:flex><span>    endTime: &lt;end time of defragmentation&gt;
</span></span><span style=display:flex><span>    reason: 
</span></span><span style=display:flex><span>    message:
</span></span><span style=display:flex><span>    initialDBSize: &lt;size of etcd DB prior to defragmentation&gt;
</span></span><span style=display:flex><span>    finalDBSize: &lt;size of etcd DB post defragmentation&gt;
</span></span><span style=display:flex><span>  volumeMismatches:
</span></span><span style=display:flex><span>  - identifiedAt: &lt;time at which wrong volume mount was identified&gt;
</span></span><span style=display:flex><span>    fixedAt: &lt;time at which correct volume was mounted&gt;
</span></span><span style=display:flex><span>    volumeID: &lt;volume ID of wrong volume that got mounted&gt;
</span></span><span style=display:flex><span>    numRestarts: &lt;num of pod restarts that were attempted&gt;
</span></span><span style=display:flex><span>  transitions:
</span></span><span style=display:flex><span>  - state: &lt;name of the state that the etcd-member has transitioned to&gt;
</span></span><span style=display:flex><span>    subState: &lt;name of the sub-state if any&gt;
</span></span><span style=display:flex><span>    reason: &lt;reason code for the transition&gt;
</span></span><span style=display:flex><span>    transitionTime: &lt;time of transition to this state&gt;
</span></span><span style=display:flex><span>    message: &lt;detailed message if any&gt;
</span></span></code></pre></div><h5 id=etcd>Etcd</h5><p>Authors propose the following changes to the <code>Etcd</code> API:</p><ol><li>In the <code>Etcd.Status</code> resource API, <a href=https://github.com/gardener/etcd-druid/blob/9a598d05e639099ddf404803f87376852261a052/api/v1alpha1/types_etcd.go#L419>member status</a> is computed and stored. This field will be marked as deprecated and in a later version of druid it will be removed. In its place, the authors propose to introduce the following:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>type</span> EtcdStatus <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// MemberRefs contains references to all existing EtcdMember resources
</span></span></span><span style=display:flex><span><span style=color:green></span>  MemberRefs []CrossVersionObjectReference
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><ol start=2><li>In <code>Etcd.Status</code> resource API, <a href=https://github.com/gardener/etcd-druid/blob/9a598d05e639099ddf404803f87376852261a052/api/v1alpha1/types_etcd.go#L422>PeerUrlTLSEnabled</a> reflects the status of enabling TLS for peer communication across all etcd-members. Currentlty this field is not been used anywhere. In this proposal, the authors have also proposed that each <code>EtcdMember</code> resource should capture the status of TLS enablement of peer URL. The authors propose to relook at the need to have this field under <code>EtcdStatus</code>.</li></ol><h3 id=lifecycle-of-an-etcdmember>Lifecycle of an EtcdMember</h3><h4 id=creation>Creation</h4><p>Druid creates an <code>EtcdMember</code> resource for every replica in <code>etcd.Spec.Replicas</code> during reconciliation of an etcd resource. For a fresh etcd cluster this is done prior to creation of the StatefulSet resource and for an existing cluster which has now been scaled-up, it is done prior to updating the StatefulSet resource.</p><h4 id=updation>Updation</h4><p>All fields in <code>EtcdMember.Status</code> are <em>only</em> updated by the corresponding etcd-member. Druid only consumes the information published via <code>EtcdMember</code> resources.</p><h4 id=deletion>Deletion</h4><p>Druid is responsible for deletion of all existing <code>EtcdMember</code> resources for an etcd cluster. There are three scenarios where an <code>EtcdMember</code> resource will be deleted:</p><ol><li><p>Deletion of etcd resource.</p></li><li><p>Scale down of an etcd cluster to 0 replicas due to hibernation of the k8s control plane.</p></li><li><p>Transient scale down of an etcd cluster to 0 replicas to recover from a quorum loss.</p></li></ol><p>Authors found no reason to retain EtcdMember resources when the etcd cluster is scale down to 0 replicas since the information contained in each EtcdMember resource would no longer represent the current state of each member and would thus be stale. Any controller in druid which acts upon the <code>EtcdMember.Status</code> could potentially take incorrect actions.</p><h4 id=reconciliation>Reconciliation</h4><p>Authors propose to introduce a new controller (let&rsquo;s call it <code>etcd-member-controller</code>) which watches for changes to the <code>EtcdMember</code> resource(s). If a reconciliation of an <code>Etcd</code> resource is required as a result of change in <code>EtcdMember</code> status then this controller should enqueue an event and force a reconciliation via existing <code>etcd-controller</code>, thus preserving the single-actor-principal constraint which ensures deterministic changes to etcd cluster resources.</p><blockquote><p>NOTE: Further decisions w.r.t responsibility segregation will be taken during implementation and will not be documented in this proposal.</p></blockquote><h5 id=stale-etcdmember-status-handling>Stale EtcdMember Status Handling</h5><p>It is possible that an etcd-member is unable to update its respective <code>EtcdMember</code> resource. Following can be some of the implications which should be kept in mind while reconciling <code>EtcdMember</code> resource in druid:</p><ul><li>Druid sees stale state transitions (this assumes that the backup-sidecar attempts to update the state/sub-state in <code>etcdMember.status.transitions</code> with best attempt). There is currently no implication other than an operator seeing a stale state.</li><li><code>dbSize</code> and <code>dbSizeInUse</code> could not be updated. A consequence could be that druid continues to see high value for <code>dbSize - dbSizeInUse</code> for a extended amount of time. Druid should ensure that it does not trigger repeated defragmentations.</li><li>If <code>VolumeMismatches</code> is stale, then druid should no longer attempt to recover by repeatedly restarting the pod.</li><li>Failed <code>restoration</code> was recorded last and further updates to this array failed. Druid should not repeatedly take full-snapshots.</li><li>If <code>snapshots.accumulatedDeltaSize</code> could not be updated, then druid should not schedule repeated compaction Jobs.</li></ul><h2 id=reference>Reference</h2><ul><li><p><a href=https://etcd.io/docs/v3.3/op-guide/recovery/#snapshotting-the-keyspace>Disaster recovery | etcd</a></p></li><li><p><a href=https://etcd.io/docs/v3.3/dev-guide/api_reference_v3/#message-statusresponse-etcdserveretcdserverpbrpcproto>etcd API Reference | etcd</a></p></li><li><p><a href=https://raft.github.io/raft.pdf>Raft Consensus Algorithm</a></p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-feb8be4a3199203fdee73330193347ff>3.17 - Feature Gates in Etcd-Druid</h1><h1 id=feature-gates-in-etcd-druid>Feature Gates in Etcd-Druid</h1><p>This page contains an overview of the various feature gates an administrator can specify on etcd-druid.</p><h2 id=overview>Overview</h2><p>Feature gates are a set of key=value pairs that describe etcd-druid features. You can turn these features on or off by passing them to the <code>--feature-gates</code> CLI flag in the etcd-druid command.</p><p>The following tables are a summary of the feature gates that you can set on etcd-druid.</p><ul><li>The “Since” column contains the etcd-druid release when a feature is introduced or its release stage is changed.</li><li>The “Until” column, if not empty, contains the last etcd-druid release in which you can still use a feature gate.</li><li>If a feature is in the <em>Alpha</em> or <em>Beta</em> state, you can find the feature listed in the Alpha/Beta feature gate table.</li><li>If a feature is stable you can find all stages for that feature listed in the Graduated/Deprecated feature gate table.</li><li>The Graduated/Deprecated feature gate table also lists deprecated and withdrawn features.</li></ul><h2 id=feature-gates-for-alpha-or-beta-features>Feature Gates for Alpha or Beta Features</h2><table><thead><tr><th>Feature</th><th>Default</th><th>Stage</th><th>Since</th><th>Until</th></tr></thead><tbody></tbody></table><h2 id=feature-gates-for-graduated-or-deprecated-features>Feature Gates for Graduated or Deprecated Features</h2><table><thead><tr><th>Feature</th><th>Default</th><th>Stage</th><th>Since</th><th>Until</th></tr></thead><tbody><tr><td><code>UseEtcdWrapper</code></td><td><code>false</code></td><td><code>Alpha</code></td><td><code>0.19</code></td><td><code>0.21</code></td></tr><tr><td><code>UseEtcdWrapper</code></td><td><code>true</code></td><td><code>Beta</code></td><td><code>0.22</code></td><td><code>0.24</code></td></tr><tr><td><code>UseEtcdWrapper</code></td><td><code>true</code></td><td><code>GA</code></td><td><code>0.25</code></td><td></td></tr></tbody></table><h2 id=using-a-feature>Using a Feature</h2><p>A feature can be in <em>Alpha</em>, <em>Beta</em> or <em>GA</em> stage.</p><h3 id=alpha-feature>Alpha feature</h3><ul><li>Disabled by default.</li><li>Might be buggy. Enabling the feature may expose bugs.</li><li>Support for feature may be dropped at any time without notice.</li><li>The API may change in incompatible ways in a later software release without notice.</li><li>Recommended for use only in short-lived testing clusters, due to increased
risk of bugs and lack of long-term support.</li></ul><h3 id=beta-feature>Beta feature</h3><ul><li>Enabled by default.</li><li>The feature is well tested. Enabling the feature is considered safe.</li><li>Support for the overall feature will not be dropped, though details may change.</li><li>The schema and/or semantics of objects may change in incompatible ways in a
subsequent beta or stable release. When this happens, we will provide instructions
for migrating to the next version. This may require deleting, editing, and
re-creating API objects. The editing process may require some thought.
This may require downtime for applications that rely on the feature.</li><li>Recommended for only non-critical uses because of potential for
incompatible changes in subsequent releases.</li></ul><blockquote><p>Please do try <em>Beta</em> features and give feedback on them!
After they exit beta, it may not be practical for us to make more changes.</p></blockquote><h3 id=general-availability-ga-feature>General Availability (GA) feature</h3><p>This is also referred to as a <em>stable</em> feature which should have the following characteristics:</p><ul><li>The feature is always enabled; you cannot disable it.</li><li>The corresponding feature gate is no longer needed.</li><li>Stable versions of features will appear in released software for many subsequent versions.</li></ul><h2 id=list-of-feature-gates>List of Feature Gates</h2><table><thead><tr><th>Feature</th><th>Description</th></tr></thead><tbody><tr><td><code>UseEtcdWrapper</code></td><td>Enables the use of etcd-wrapper image and a compatible version of etcd-backup-restore, along with component-specific configuration changes necessary for the usage of the etcd-wrapper image.</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-a86d218feba7b32a3048eae01a13feef>3.18 - Getting Started Locally</h1><h1 id=setup-etcd-druid-locally>Setup Etcd-Druid Locally</h1><p>This document will guide you on how to setup <code>etcd-druid</code> on your local machine and how to provision and manage <code>Etcd</code> cluster(s).</p><h2 id=00-prerequisites>00-Prerequisites</h2><p>Before we can setup <code>etcd-druid</code> and use it to provision <code>Etcd</code> clusters, we need to prepare the development environment. Follow the <a href=/docs/other-components/etcd-druid/prepare-dev-environment/>Prepare Dev Environment Guide</a> for detailed instructions.</p><h2 id=01-setting-up-kind-cluster>01-Setting up KIND cluster</h2><p><code>etcd-druid</code> uses <a href=https://kind.sigs.k8s.io/>kind</a> as it&rsquo;s local Kubernetes engine. The local setup is configured for kind due to its convenience only. Any other Kubernetes setup would also work.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make kind-up
</span></span></code></pre></div><p>This command sets up a new Kind cluster and stores the kubeconfig at <code>./hack/kind/kubeconfig</code>. Additionally, this command also deploys a local container registry as a docker container. This ensures faster image push/pull times. The local registry can be accessed as <code>localhost:5001</code> for pushing and pulling images.</p><p>To target this newly created cluster, set the <code>KUBECONFIG</code> environment variable to the kubeconfig file.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export KUBECONFIG=$PWD/hack/kind/kubeconfig
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> If you wish to configure kind cluster differently then you can directly invoke the script and check its help to know about all configuration options.</p></blockquote><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/kind-up.sh -h
</span></span><span style=display:flex><span>  usage: kind-up.sh [Options]
</span></span><span style=display:flex><span>  Options:
</span></span><span style=display:flex><span>    --cluster-name  &lt;cluster-name&gt;   Name of the kind cluster to create. Default value is <span style=color:#a31515>&#39;etcd-druid-e2e&#39;</span>
</span></span><span style=display:flex><span>    --skip-registry                  Skip creating a local docker registry. Default value is false.
</span></span><span style=display:flex><span>    --feature-gates &lt;feature-gates&gt;  Comma separated list of feature gates to enable on the cluster.
</span></span></code></pre></div><h2 id=02-setting-up-etcd-druid>02-Setting up etcd-druid</h2><h3 id=configuring-etcd-druid>Configuring etcd-druid</h3><p>Prior to deploying <code>etcd-druid</code>, it can be configured via CLI-args and environment variables.</p><ul><li>To configure CLI args you can modify <a href=https://github.com/gardener/etcd-druid/blob/master/charts/druid/values.yaml><code>charts/druid/values.yaml</code></a>. For e.g. if you wish to <code>auto-reconcile</code> any change done to <code>Etcd</code> CR then you should set <code>enableEtcdSpecAutoReconcile</code> to true. By default this will be switched off.</li><li><code>DRUID_E2E_TEST=true</code> : sets specific configuration for etcd-druid for optimal e2e test runs, like a lower sync period for the etcd controller.</li></ul><h3 id=deploying-etcd-druid>Deploying etcd-druid</h3><p>Any variant of <code>make deploy-*</code> command uses <a href=https://helm.sh/>helm</a> and <a href=https://skaffold.dev/>skaffold</a> to build and deploy <code>etcd-druid</code> to the target Kubernetes cluster. In addition to deploying <code>etcd-druid</code> it will also install the <a href=https://github.com/gardener/etcd-druid/blob/master/config/crd/bases/crd-druid.gardener.cloud_etcds.yaml>Etcd CRD</a> and <a href=https://github.com/gardener/etcd-druid/blob/master/config/crd/bases/crd-druid.gardener.cloud_etcdcopybackupstasks.yaml>EtcdCopyBackupTask CRD</a>.</p><h4 id=regular-mode>Regular mode</h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make deploy
</span></span></code></pre></div><p>The above command will use <a href=https://skaffold.dev/>skaffold</a> to build and deploy <code>etcd-druid</code> to the k8s kind cluster pointed to by <code>KUBECONFIG</code> environment variable.</p><h4 id=dev-mode>Dev mode</h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make deploy-dev
</span></span></code></pre></div><p>This is similar to <code>make deploy</code> but additionally starts a <a href=https://skaffold.dev/docs/workflows/dev/>skaffold dev loop</a>. After the initial deployment, skaffold starts watching source files. Once it has detected changes, you can press any key to update the <code>etcd-druid</code> deployment.</p><h4 id=debug-mode>Debug mode</h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make deploy-debug
</span></span></code></pre></div><p>This is similar to <code>make deploy-dev</code> but additionally configures containers in pods for debugging as required for each container&rsquo;s runtime technology. The associated debugging ports are exposed and labelled so that they can be port-forwarded to the local machine. Skaffold disables automatic image rebuilding and syncing when using the <code>debug</code> mode as compared to <code>dev</code> mode.</p><p>Go debugging uses <a href=https://github.com/go-delve/delve>Delve</a>. Please see the <a href=https://skaffold.dev/docs/workflows/debug/>skaffold debugging documentation</a> how to setup your IDE accordingly.</p><p>!!! note
Resuming or stopping only a single goroutine (Go Issue <a href=https://github.com/golang/go/issues/25578>25578</a>, <a href=https://github.com/golang/go/issues/31132>31132</a>) is currently not supported, so the action will cause all the goroutines to get activated or paused.</p><p>This means that when a goroutine is paused on a breakpoint, then all the other goroutines are also paused. This should be kept in mind when using <code>skaffold debug</code>.</p><h2 id=03-configure-backup-optional>03-Configure Backup [<em>Optional</em>]</h2><h3 id=deploying-a-local-backup-store-emulator>Deploying a Local Backup Store Emulator</h3><p>!!! info
This section is <em><strong>Optional</strong></em> and is only meant to describe steps to deploy a local object store which can be used for testing and development. If you either do not wish to enable backups or you wish to use remote (infra-provider-specific) object store then this section can be skipped.</p><p>An <code>Etcd</code> cluster provisioned via etcd-druid provides a capability to take regular delta and full snapshots which are stored in an object store. You can enable this functionality by ensuring that you fill in <a href=https://github.com/gardener/etcd-druid/blob/master/config/samples/druid_v1alpha1_etcd.yaml#L49-L54>spec.backup.store</a> section of the <code>Etcd</code> CR.</p><table><thead><tr><th>Backup Store Variant</th><th>Setup Guide</th></tr></thead><tbody><tr><td>Azure Object Storage Emulator</td><td><a href=/docs/other-components/etcd-druid/deployment/getting-started-locally/manage-azurite-emulator/>Manage Azurite</a> (Steps 00-03)</td></tr><tr><td>S3 Object Store Emulator</td><td><a href=/docs/other-components/etcd-druid/deployment/getting-started-locally/manage-s3-emulator/>Manage LocalStack</a> (Steps 00-03)</td></tr></tbody></table><h3 id=setting-up-cloud-provider-object-store-secret>Setting up Cloud Provider Object Store Secret</h3><p>!!! info
This section is <em><strong>Optional</strong></em>. If you have disabled backup functionality or if you are using local storage or one of the supported object store emulators then you can skip this section.</p><p>A Kubernetes <a href=https://kubernetes.io/docs/concepts/configuration/secret/>Secret</a> needs to be created for cloud provider Object Store access. You can refer to the Secret YAML templates <a href=https://github.com/gardener/etcd-backup-restore/tree/master/example/storage-provider-secrets>here</a>. Replace the dummy values with the actual configuration and ensure that you have added the <code>metadata.name</code> and <code>metadata.namespace</code> to the secret.</p><p>!!! tip
* Secret should be deployed in the same namespace as the <code>Etcd</code> resource.
* All the values in the data field of the secret YAML should in <code>base64</code> encoded format.</p><p>To apply the secret run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f &lt;path/to/secret&gt;
</span></span></code></pre></div><h2 id=04-preparing-etcd-cr>04-Preparing Etcd CR</h2><p>Choose an appropriate variant of <code>Etcd</code> CR from <a href=https://github.com/gardener/etcd-druid/tree/master/config/samples>samples directory</a>.</p><p>If you wish to enable functionality to backup delta & full snapshots then uncomment <code>spec.backup.store</code> section.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green># Configuration for storage provider</span>
</span></span><span style=display:flex><span>store:
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: etcd-backup-secret-name
</span></span><span style=display:flex><span>  container: object-storage-container-name
</span></span><span style=display:flex><span>  provider: aws # options: aws,azure,gcp,openstack,alicloud,dell,openshift,local
</span></span><span style=display:flex><span>  prefix: etcd-test
</span></span></code></pre></div><p>Brief explanation of the keys:</p><ul><li><code>secretRef.name</code> is the name of the secret that was applied as mentioned above.</li><li><code>store.container</code> is the object storage bucket name.</li><li><code>store.provider</code> is the bucket provider. Pick from the options mentioned in comment.</li><li><code>store.prefix</code> is the folder name that you want to use for your snapshots inside the bucket.</li></ul><p>!!! tip
For developer convenience we have provided object store emulator specific etcd CR variants which can be used as if as well.</p><h2 id=05-applying-etcd-cr>05-Applying Etcd CR</h2><p>Create the Etcd CR (Custom Resource) by applying the Etcd yaml to the cluster</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f &lt;path-to-etcd-cr-yaml&gt;
</span></span></code></pre></div><h2 id=06-verify-the-etcd-cluster>06-Verify the Etcd Cluster</h2><p>To obtain information on the etcd cluster you can invoke the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get etcd -o=wide
</span></span></code></pre></div><p>We adhere to a naming convention for all resources that are provisioned for an <code>Etcd</code> cluster. Refer to <a href=/docs/other-components/etcd-druid/concepts/etcd-cluster-components/>etcd-cluster-components</a> document to get details of all resources that are provisioned.</p><h3 id=verify-etcd-pods-functionality>Verify Etcd Pods&rsquo; Functionality</h3><p><code>etcd-wrapper</code> uses a <a href=https://github.com/GoogleContainerTools/distroless>distroless</a> image, which lacks a shell. To interact with etcd, use an <a href=https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/>Ephemeral container</a> as a debug container. Refer to this <a href=https://github.com/gardener/etcd-wrapper/blob/master/docs/deployment/ops.md#operations--debugging>documentation</a> for building and using an ephemeral container which gets attached to the <code>etcd-wrapper</code> pod.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Put a key-value pair into the etcd </span>
</span></span><span style=display:flex><span>etcdctl put &lt;key1&gt; &lt;value1&gt;
</span></span><span style=display:flex><span><span style=color:green># Retrieve all key-value pairs from the etcd db</span>
</span></span><span style=display:flex><span>etcdctl get --prefix <span style=color:#a31515>&#34;&#34;</span>
</span></span></code></pre></div><p>For a multi-node etcd cluster, insert the key-value pair using the <code>etcd</code> container of one etcd member and retrieve it from the <code>etcd</code> container of another member to verify consensus among the multiple etcd members.</p><h2 id=07-updating-etcd-cr>07-Updating Etcd CR</h2><p><code>Etcd</code> CR can be updated with new changes. To ensure that <code>etcd-druid</code> reconciles the changes you can refer to options that etcd-druid provides <a href=/docs/other-components/etcd-druid/managing-etcd-clusters/#update-&-reconcile-an-etcd-cluster>here</a>.</p><h2 id=08-cleaning-up-the-setup>08-Cleaning up the setup</h2><p>If you wish to only delete the <code>Etcd</code> cluster then you can use the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete etcd &lt;etcd-name&gt;
</span></span></code></pre></div><p>This will add the <code>deletionTimestamp</code> to the <code>Etcd</code> resource. At the time the creation of the <code>Etcd</code> cluster, etcd-druid will add a finalizer to ensure that it cleans up all <code>Etcd</code> cluster resources before the CR is removed.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  finalizers:
</span></span><span style=display:flex><span>  - druid.gardener.cloud/etcd-druid
</span></span></code></pre></div><p>etcd-druid will automatically pick up the deletion event and attempt clean up <code>Etcd</code> cluster resources. It will only remove the finaliser once all resources have been cleaned up.</p><p>If you only wish to remove <code>etcd-druid</code> but retain the kind cluster then you can use the following make target:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make undeploy
</span></span></code></pre></div><p>If you wish to delete the kind cluster then you can use the following make target:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make kind-down
</span></span></code></pre></div><p>This cleans up the entire setup as the kind cluster gets deleted.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a286096fb24dc61fc6c93cf614019225>3.19 - Getting Started Locally</h1><h1 id=developing-etcd-druid-locally>Developing etcd-druid locally</h1><p>You can setup <code>etcd-druid</code> locally by following detailed instructions in <a href=/docs/other-components/etcd-druid/deployment/getting-started-locally/getting-started-locally/>this document</a>.</p><ul><li>For best development experience you should use <code>make deploy-dev</code> - this helps during development where you wish to make changes to the code base and with a key-press allow automatic re-deployment of the application to the target Kubernetes cluster.</li><li>In case you wish to start a debugging session then use <code>make deploy-debug</code> - this will additionally disable leader election and prevent leases to expire and process to stop.</li></ul><p>!!! info
We leverage <a href=https://skaffold.dev/docs/workflows/debug/>skaffold debug</a> and <a href=https://skaffold.dev/docs/workflows/dev/>skaffold dev</a> features.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ae4eff84d3ee9c982c6b818e76a439b6>3.20 - Local e2e Tests</h1><h1 id=e2e-test-suite>e2e Test Suite</h1><p>Developers can run extended e2e tests, in addition to unit tests, for Etcd-Druid in or from
their local environments. This is recommended to verify the desired behavior of several features
and to avoid regressions in future releases.</p><p>The very same tests typically run as part of the component&rsquo;s release job as well as on demand, e.g.,
when triggered by Etcd-Druid maintainers for open pull requests.</p><p>Testing Etcd-Druid automatically involves a certain test coverage for <a href=https://github.com/gardener/etcd-backup-restore/>gardener/etcd-backup-restore</a>
which is deployed as a side-car to the actual <code>etcd</code> container.</p><h2 id=prerequisites>Prerequisites</h2><p>The e2e test lifecycle is managed with the help of <a href=https://skaffold.dev/>skaffold</a>. Every involved step like <code>setup</code>,
<code>deploy</code>, <code>undeploy</code> or <code>cleanup</code> is executed against a <strong>Kubernetes</strong> cluster which makes it a mandatory prerequisite at the same time.
Only <a href=https://skaffold.dev/>skaffold</a> itself with involved <code>docker</code>, <code>helm</code> and <code>kubectl</code> executions as well as
the e2e-tests are executed locally. Required binaries are automatically downloaded if you use the corresponding <code>make</code> target,
as described in this document.</p><p>It&rsquo;s expected that especially the <code>deploy</code> step is run against a Kubernetes cluster which doesn&rsquo;t contain an Druid deployment or any left-overs like <code>druid.gardener.cloud</code> CRDs.
The <code>deploy</code> step will likely fail in such scenarios.</p><blockquote><p>Tip: Create a fresh <a href=https://kind.sigs.k8s.io/>KinD</a> cluster or a similar one with a small footprint before executing the tests.</p></blockquote><h2 id=providers>Providers</h2><p>The following providers are supported for e2e tests:</p><ul><li>AWS</li><li>Azure</li><li>GCP</li><li>Local</li></ul><blockquote><p>Valid credentials need to be provided when tests are executed with mentioned cloud providers.</p></blockquote><h2 id=flow>Flow</h2><p>An e2e test execution involves the following steps:</p><table><thead><tr><th>Step</th><th>Description</th></tr></thead><tbody><tr><td><code>setup</code></td><td>Create a storage bucket which is used for etcd backups (only with cloud providers).</td></tr><tr><td><code>deploy</code></td><td>Build Docker image, upload it to registry (if remote cluster - see <a href=https://skaffold.dev/docs/pipeline-stages/builders/docker/>Docker build</a>), deploy Helm chart (<code>charts/druid</code>) to Kubernetes cluster.</td></tr><tr><td><code>test</code></td><td>Execute e2e tests as defined in <code>test/e2e</code>.</td></tr><tr><td><code>undeploy</code></td><td>Remove the deployed artifacts from Kubernetes cluster.</td></tr><tr><td><code>cleanup</code></td><td>Delete storage bucket and Druid deployment from test cluster.</td></tr></tbody></table><h3 id=make-target>Make target</h3><p>Executing e2e-tests is as easy as executing the following command <strong>with defined Env-Vars as desribed in the following
section and as needed for your test scenario</strong>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make test-e2e
</span></span></code></pre></div><h3 id=common-env-variables>Common Env Variables</h3><p>The following environment variables influence how the flow described above is executed:</p><ul><li><code>PROVIDERS</code>: Providers used for testing (<code>all</code>, <code>aws</code>, <code>azure</code>, <code>gcp</code>, <code>local</code>). Multiple entries must be comma separated.<blockquote><p><strong>Note</strong>: Some tests will use very first entry from env <code>PROVIDERS</code> for e2e testing (ex: multi-node tests). So for multi-node tests to use specific provider, specify that provider as first entry in env <code>PROVIDERS</code>.</p></blockquote></li><li><code>KUBECONFIG</code>: Kubeconfig pointing to cluster where Etcd-Druid will be deployed (preferably <a href=https://kind.sigs.k8s.io>KinD</a>).</li><li><code>TEST_ID</code>: Some ID which is used to create assets for and during testing.</li><li><code>STEPS</code>: Steps executed by <code>make</code> target (<code>setup</code>, <code>deploy</code>, <code>test</code>, <code>undeploy</code>, <code>cleanup</code> - default: all steps).</li></ul><h3 id=aws-env-variables>AWS Env Variables</h3><ul><li><code>AWS_ACCESS_KEY_ID</code>: Key ID of the user.</li><li><code>AWS_SECRET_ACCESS_KEY</code>: Access key of the user.</li><li><code>AWS_REGION</code>: Region in which the test bucket is created.</li></ul><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  AWS_ACCESS_KEY_ID=<span style=color:#a31515>&#34;abc&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  AWS_SECRET_ACCESS_KEY=<span style=color:#a31515>&#34;xyz&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  AWS_REGION=<span style=color:#a31515>&#34;eu-central-1&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  KUBECONFIG=<span style=color:#a31515>&#34;</span>$HOME<span style=color:#a31515>/.kube/config&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  PROVIDERS=<span style=color:#a31515>&#34;aws&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  TEST_ID=<span style=color:#a31515>&#34;some-test-id&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  STEPS=<span style=color:#a31515>&#34;setup,deploy,test,undeploy,cleanup&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>test-e2e
</span></span></code></pre></div><h3 id=azure-env-variables>Azure Env Variables</h3><ul><li><code>STORAGE_ACCOUNT</code>: Storage account used for managing the storage container.</li><li><code>STORAGE_KEY</code>: Key of storage account.</li></ul><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  STORAGE_ACCOUNT=<span style=color:#a31515>&#34;abc&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  STORAGE_KEY=<span style=color:#a31515>&#34;eHl6Cg==&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  KUBECONFIG=<span style=color:#a31515>&#34;</span>$HOME<span style=color:#a31515>/.kube/config&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  PROVIDERS=<span style=color:#a31515>&#34;azure&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  TEST_ID=<span style=color:#a31515>&#34;some-test-id&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  STEPS=<span style=color:#a31515>&#34;setup,deploy,test,undeploy,cleanup&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>test-e2e
</span></span></code></pre></div><h3 id=gcp-env-variables>GCP Env Variables</h3><ul><li><code>GCP_SERVICEACCOUNT_JSON_PATH</code>: Path to the service account json file used for this test.</li><li><code>GCP_PROJECT_ID</code>: ID of the GCP project.</li></ul><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  GCP_SERVICEACCOUNT_JSON_PATH=<span style=color:#a31515>&#34;/var/lib/secrets/serviceaccount.json&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  GCP_PROJECT_ID=<span style=color:#a31515>&#34;xyz-project&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  KUBECONFIG=<span style=color:#a31515>&#34;</span>$HOME<span style=color:#a31515>/.kube/config&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  PROVIDERS=<span style=color:#a31515>&#34;gcp&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  TEST_ID=<span style=color:#a31515>&#34;some-test-id&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  STEPS=<span style=color:#a31515>&#34;setup,deploy,test,undeploy,cleanup&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>test-e2e
</span></span></code></pre></div><h3 id=local-env-variables>Local Env Variables</h3><p>No special environment variables are required for running e2e tests with <code>Local</code> provider.</p><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  KUBECONFIG=<span style=color:#a31515>&#34;</span>$HOME<span style=color:#a31515>/.kube/config&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  PROVIDERS=<span style=color:#a31515>&#34;local&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  TEST_ID=<span style=color:#a31515>&#34;some-test-id&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  STEPS=<span style=color:#a31515>&#34;setup,deploy,test,undeploy,cleanup&#34;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>test-e2e
</span></span></code></pre></div><h2 id=e2e-test-with-localstack>e2e test with localstack</h2><p>The above-mentioned e2e tests need storage from real cloud providers to be setup. But there is a tool named <a href=https://docs.localstack.cloud/user-guide/aws/s3/>localstack</a> that enables to run e2e test with mock AWS storage. We can also provision KIND cluster for e2e tests. So, together with localstack and KIND cluster, we don&rsquo;t need to depend on any actual cloud provider infrastructure to be setup to run e2e tests.</p><h3 id=how-are-the-kind-cluster-and-localstack-set-up>How are the KIND cluster and localstack set up</h3><p>KIND or Kubernetes-In-Docker is a kubernetes cluster that is set up inside a docker container. This cluster is with limited capability as it does not have much compute power. But this cluster can easily be setup inside a container and can be tear down easily just by removing a container. That&rsquo;s why KIND cluster is very easy to use for e2e tests. <code>Makefile</code> command helps to spin up a KIND cluster and use the cluster to run e2e tests.</p><p>There is a docker image for localstack. The image is deployed as pod inside the KIND cluster through <code>hack/e2e-test/infrastructure/localstack/localstack.yaml</code>. <code>Makefile</code> takes care of deploying the yaml file in a KIND cluster.</p><p>The developer needs to run <code>make ci-e2e-kind</code> command. This command in turn runs <code>hack/ci-e2e-kind.sh</code> which spin up the KIND cluster and deploy localstack in it and then run the e2e tests using localstack as mock AWS storage provider. e2e tests are actually run on host machine but deploy the druid controller inside KIND cluster. Druid controller spawns multinode etcd clusters inside KIND cluster. e2e tests verify whether the druid controller performs its jobs correctly or not. Mock localstack storage is cleaned up after every e2e tests. That&rsquo;s why the e2e tests need to access the localstack pod running inside KIND cluster. The network traffic between host machine and localstack pod is resolved via mapping localstack pod port to host port while setting up the KIND cluster via <code>hack/e2e-test/infrastructure/kind/cluster.yaml</code></p><h3 id=how-to-execute-e2e-tests-with-localstack-and-kind-cluster>How to execute e2e tests with localstack and KIND cluster</h3><p>Run the following <code>make</code> command to spin up a KinD cluster, deploy localstack and run the e2e tests with provider <code>aws</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make ci-e2e-kind
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-506b865cd2e8b8792ba180dc995bd913>3.21 - Manage Azurite Emulator</h1><h1 id=manage-azure-blob-storage-emulator>Manage Azure Blob Storage Emulator</h1><p>This document is a step-by-step guide on how to configure, deploy and cleanup <a href=https://github.com/Azure/Azurite#introduction>Azurite</a>, the <code>Azure Blob Storage</code> emulator, within a <a href=https://kind.sigs.k8s.io/>kind</a> cluster. This setup is ideal for local development and testing.</p><h2 id=00-prerequisites>00-Prerequisites</h2><p>Ensure that you have setup the development environment as per the <a href=/docs/other-components/etcd-druid/prepare-dev-environment/>documentation</a>.</p><blockquote><p><strong>Note:</strong> It is assumed that you have already created kind cluster and the <code>KUBECONFIG</code> is pointing to this Kubernetes cluster.</p></blockquote><h3 id=installing-azure-cli>Installing Azure CLI</h3><p>To interact with <code>Azurite</code> you must also install the Azure CLI <code>(version >=2.55.0)</code>
On macOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install azure-cli
</span></span></code></pre></div><p>For other OS, please check the <a href=https://learn.microsoft.com/en-us/cli/azure/install-azure-cli>Azure CLI installation documentation</a>.</p><h2 id=01-deploy-azurite>01-Deploy Azurite</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make deploy-azurite
</span></span></code></pre></div><p>The above make target will deploy <code>Azure</code> emulator in the target Kubernetes cluster.</p><h2 id=02-setup-abs-container>02-Setup ABS Container</h2><p>We will be using the <code>azure-cli</code> to create an ABS container. Export the connection string to enable <code>azure-cli</code> to connect to <code>Azurite</code> emulator.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export AZURE_STORAGE_CONNECTION_STRING=<span style=color:#a31515>&#34;DefaultEndpointsProtocol=http;AccountName=devstoreaccount1;AccountKey=Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==;BlobEndpoint=http://127.0.0.1:10000/devstoreaccount1;&#34;</span>
</span></span></code></pre></div><p>To create an Azure Blob Storage Container in Azurite, run the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>az storage container create -n &lt;container-name&gt;
</span></span></code></pre></div><h2 id=03-configure-secret>03-Configure Secret</h2><p>Connection details for an Azure Object Store Container are put into a Kubernetes <a href=https://kubernetes.io/docs/concepts/configuration/secret/>Secret</a>. Apply the Kubernetes Secret manifest through:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f config/samples/etcd-secret-azurite.yaml
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> The secret created should be referred to in the <code>Etcd</code> CR in <code>spec.backup.store.secretRef</code>.</p></blockquote><h2 id=04-cleanup>04-Cleanup</h2><p>In addition to the kind cluster cleanup you should also unset the environment variable set in step-03 above.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>unset AZURE_STORAGE_CONNECTION_STRING
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-386dee4539278d1bdbcee79f74eb99e2>3.22 - Manage S3 Emulator</h1><h1 id=manage-s3-emulator>Manage S3 Emulator</h1><p>This document is a step-by-step guide on how to configure, deploy and cleanup <a href=https://localstack.cloud/>LocalStack</a>, within a <a href=https://kind.sigs.k8s.io/>kind</a> cluster. LocalStack emulates AWS services locally, which allows the <code>Etcd</code> cluster to interact with AWS S3. This setup is ideal for local development and testing.</p><h2 id=00-prerequisites>00-Prerequisites</h2><p>Ensure that you have setup the development environment as per the <a href=/docs/other-components/etcd-druid/prepare-dev-environment/>documentation</a>.</p><blockquote><p><strong>Note:</strong> It is assumed that you have already created kind cluster and the <code>KUBECONFIG</code> is pointing to this Kubernetes cluster.</p></blockquote><h3 id=installing-aws-cli>Installing AWS CLI</h3><p>To interact with <code>LocalStack</code> you must also install the AWS CLI <code>(version >=1.29.0 or version >=2.13.0)</code>
On macOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install awscli
</span></span></code></pre></div><p>For other OS, please check the <a href=https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html>AWS CLI installation documentation</a>.</p><h2 id=01-deploy-localstack>01-Deploy LocalStack</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make deploy-localstack
</span></span></code></pre></div><p>The above make target will deploy <code>LocalStack</code> in the target Kubernetes cluster.</p><h2 id=02-setup-s3-bucket>02-Setup S3 Bucket</h2><p>Configure <code>AWS CLI</code> to interact with LocalStack by setting the necessary environment variables. This configuration redirects S3 commands to the LocalStack endpoint and provides the required credentials for authentication.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export AWS_ENDPOINT_URL_S3=<span style=color:#a31515>&#34;http://localhost:4566&#34;</span>
</span></span><span style=display:flex><span>export AWS_ACCESS_KEY_ID=ACCESSKEYAWSUSER
</span></span><span style=display:flex><span>export AWS_SECRET_ACCESS_KEY=sEcreTKey
</span></span><span style=display:flex><span>export AWS_DEFAULT_REGION=us-east-2
</span></span></code></pre></div><p>Create a S3 bucket using the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>aws s3api create-bucket --bucket &lt;bucket-name&gt; --region &lt;region&gt; --create-bucket-configuration LocationConstraint=&lt;region&gt; --acl private
</span></span></code></pre></div><p>To verify if the bucket has been created, you can use the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>aws s3api head-bucket --bucket &lt;bucket-name&gt;
</span></span></code></pre></div><h2 id=03-configure-secret>03-Configure Secret</h2><p>Connection details for an Azure S3 Object Store are put into a Kubernetes <a href=https://kubernetes.io/docs/concepts/configuration/secret/>Secret</a>. Apply the Kubernetes Secret manifest through:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f config/samples/etcd-secret-localstack.yaml
</span></span></code></pre></div><blockquote><p><strong>Note:</strong> The secret created should be referred to in the <code>Etcd</code> CR in <code>spec.backup.store.secretRef</code>.</p></blockquote><h2 id=04-cleanup>04-Cleanup</h2><p>In addition to the kind cluster cleanup you should also unset the environment variable set in step-03 above.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>unset AWS_ENDPOINT_URL_S3 AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_DEFAULT_REGION
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-ea098b4ac43cf7810c90aec0c6477a12>3.23 - Managing Etcd Clusters</h1><h1 id=managing-etcd-clusters>Managing ETCD Clusters</h1><h2 id=create-an-etcd-cluster>Create an Etcd Cluster</h2><p>Creating an <code>Etcd</code> cluster can be done either by explicitly creating a manifest file or it can also be done programmatically. You can refer to and/or modify any <a href=https://github.com/gardener/etcd-druid/tree/master/config/samples>sample</a> <code>Etcd</code> manifest to create an etcd cluster. In order to programmatically create an <code>Etcd</code> cluster you can refer to the <code>Golang</code> <a href=https://github.com/gardener/etcd-druid/tree/master/api>API</a> to create an <code>Etcd</code> custom resource and using a <a href=https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/client#Client>k8s client</a> you can apply an instance of a <code>Etcd</code> custom resource targetting any namespace in a k8s cluster.</p><p>Prior to <code>v0.23.0</code> version of etcd-druid, after creating an <code>Etcd</code> custom resource, you will have to annotate the resource with <code>gardener.cloud/operation=reconcile</code> in order to trigger a reconciliation for the newly created <code>Etcd</code> resource. Post <code>v0.23.0</code> version of etcd-druid, there is no longer any need to explicitly trigger reconciliations for creating new <code>Etcd</code> clusters.</p><h3 id=track-etcd-cluster-creation>Track etcd cluster creation</h3><p>In order to track the progress of creation of etcd cluster resources you can do the following:</p><ul><li><p><code>status.lastOperation</code> can be monitored to check the status of reconciliation.</p></li><li><p>Additional printer columns have been defined for <code>Etcd</code> custom resource. You can execute the following command to know if an <code>Etcd</code> cluster is ready/quorate.</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get etcd &lt;etcd-name&gt; -n &lt;namespace&gt; -owide
</span></span><span style=display:flex><span>  <span style=color:green># you will see additional columns which will indicate the state of an etcd cluster</span>
</span></span><span style=display:flex><span>  NAME        READY   QUORATE   ALL MEMBERS READY   BACKUP READY   AGE    CLUSTER SIZE   CURRENT REPLICAS   READY REPLICAS
</span></span><span style=display:flex><span>  etcd-main   true    True      True                True           235d   3              3                  3
</span></span></code></pre></div><ul><li><p>You can additional monitor <a href=/docs/other-components/etcd-druid/concepts/etcd-cluster-components/>all etcd cluster resources</a> that are created for every etcd cluster.</p><p>For etcd-druid version &lt;v0.23.0 use the following command:</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get all,cm,role,rolebinding,lease,sa -n &lt;namespace&gt; --selector=instance=&lt;etcd-name&gt;
</span></span></code></pre></div><p>For etcd-druid version >=v0.23.0 use the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get all,cm,role,rolebinding,lease,sa -n &lt;namespace&gt; --selector=app.kubernetes.io/managed-by=etcd-druid,app.kubernetes.io/part-of=&lt;etcd-name&gt;
</span></span></code></pre></div><h2 id=update--reconcile-an-etcd-cluster>Update & Reconcile an Etcd Cluster</h2><h3 id=edit-the-etcd-custom-resource>Edit the Etcd custom resource</h3><p>To update an etcd cluster, you should usually <em>only</em> be updating the <code>Etcd</code> custom resource representing the etcd cluster.
You can make changes to the existing <code>Etcd</code> resource by invoking the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl edit etcd &lt;etcd-name&gt; -n &lt;namespace&gt;
</span></span></code></pre></div><p>This will open up the linked editor where you can make the edits.</p><h3 id=reconcile>Reconcile</h3><p>There are two ways to control reconciliation of any changes done to <code>Etcd</code> custom resources.</p><h4 id=auto-reconciliation>Auto reconciliation</h4><p>If <code>etcd-druid</code> has been deployed with auto-reconciliation then any change done to an <code>Etcd</code> resource will be automatically reconciled.
Prior to v0.23.0 you can do this by using <code>--ignore-operation-annotation</code> CLI flag. This flag has been marked as deprecated and will be removed in later versions of <code>etcd-druid</code>. With etcd-druid version v0.23.x it is recommended that you use <code>--enable-etcd-spec-auto-reconcile</code> CLI flag to enable auto-reconcile.</p><blockquote><p>For a complete list of CLI args you can see <a href=/docs/other-components/etcd-druid/deployment/configure-etcd-druid/>this</a> document.</p></blockquote><h4 id=explicit-reconciliation>Explicit reconciliation</h4><p>If <code>--enable-etcd-spec-auto-reconcile</code> or <code>--ignore-operation-annotation</code> is set to false or not set at all, then any change to an <code>Etcd</code> resource will not be automatically reconciled. To trigger a reconcile you must set the following annotation on the <code>Etcd</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl annotate etcd &lt;etcd-name&gt; gardener.cloud/operation=reconcile -n &lt;namespace&gt;
</span></span></code></pre></div><p>This option is sometimes recommeded as you would like avoid auto-reconciliation of accidental changes to <code>Etcd</code> resources outside the maintenance time window, thus preventing a potential transient quorum loss due to misconfiguration, attach-detach issues of persistent volumes etc.</p><h2 id=overwrite-container-oci-images>Overwrite Container OCI Images</h2><p>To find out image versions of <code>etcd-backup-restore</code> and <code>etcd-wrapper</code> used by a specific version of <code>etcd-druid</code> one way is look for the image versions in <a href=https://github.com/gardener/etcd-druid/blob/master/internal/images/images.yaml>images.yaml</a>. There are times that you might wish to override these images that come bundled with <code>etcd-druid</code>. There are two ways in which you can do that:</p><p><strong>Option #1</strong>
We leverage <a href=/docs/gardener/deployment/image_vector/#overwriting-image-vector>Overwrite ImageVector</a> facility provided by <a href=https://github.com/gardener/gardener/>gardener</a>. This capability can be used without bringing in gardener as well. To illustrate this in context of <code>etcd-druid</code> you will create a <code>ConfigMap</code> with the following content:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: etcd-druid-images-overwrite
</span></span><span style=display:flex><span>  namespace: &lt;etcd-druid-namespace&gt;
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  images_overwrite.yaml: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    images:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - name: etcd-backup-restore
</span></span></span><span style=display:flex><span><span style=color:#a31515>      sourceRepository: github.com/gardener/etcd-backup-restore
</span></span></span><span style=display:flex><span><span style=color:#a31515>      repository: &lt;your-own-custom-etcd-backup-restore-repo-url&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>      tag: &#34;v&lt;custom-tag&gt;&#34;
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - name: etcd-wrapper
</span></span></span><span style=display:flex><span><span style=color:#a31515>      sourceRepository: github.com/gardener/etcd-wrapper
</span></span></span><span style=display:flex><span><span style=color:#a31515>      repository: &lt;your-own-custom-etcd-wrapper-repo-url&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>      tag: &#34;v&lt;custom-tag&gt;&#34;
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - name: alpine
</span></span></span><span style=display:flex><span><span style=color:#a31515>      repository: &lt;your-own-custom-alpine-repo-url&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>      tag: &#34;v&lt;custom-tag&gt;&#34;</span>    
</span></span></code></pre></div><p>You can use <a href=https://github.com/gardener/etcd-druid/blob/master/internal/images/images.yaml>images.yaml</a> as a reference to create the overwrite images YAML <code>ConfigMap</code>.</p><p>Edit the <code>etcd-druid</code> <code>Deployment</code> with:</p><ul><li>Mount the <code>ConfigMap</code></li><li>Set <code>IMAGEVECTOR_OVERWRITE</code> environment variable whose value must be the path you choose to mount the <code>ConfigMap</code>.</li></ul><p>To illustrate the changes you can see the following <code>etcd-druid</code> Deployment YAML:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: etcd-druid
</span></span><span style=display:flex><span>  namespace: &lt;etcd-druid-namespace&gt;
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: etcd-druid
</span></span><span style=display:flex><span>        env:
</span></span><span style=display:flex><span>        - name: IMAGEVECTOR_OVERWRITE
</span></span><span style=display:flex><span>          value: /imagevector-overwrite/images_overwrite.yaml
</span></span><span style=display:flex><span>        volumeMounts:
</span></span><span style=display:flex><span>        - name: etcd-druid-images-overwrite
</span></span><span style=display:flex><span>          mountPath: /imagevector-overwrite
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>      - name: etcd-druid-images-overwrite
</span></span><span style=display:flex><span>        configMap:
</span></span><span style=display:flex><span>          name: etcd-druid-images-overwrite
</span></span></code></pre></div><p>!!! info
Image overwrites specified in the mounted <code>ConfigMap</code> will be respected by successive reconciliations for this <code>Etcd</code> custom resource.</p><p><strong>Option #2</strong></p><p>We provide a generic way to suspend etcd cluster reconciliation via etcd-druid, allowing a human operator to take control. This option should be excercised only in case of troubleshooting or quick fixes which are not possible to do via the reconciliation loop in etcd-druid. However one of the use cases to use this option is to perhaps update the container image to apply a hot patch and speed up recovery of an etcd cluster.</p><h2 id=manually-modify-individual-etcd-cluster-resources>Manually modify individual etcd cluster resources</h2><p><code>etcd</code> cluster resources are managed by <code>etcd-druid</code> and since v0.23.0 version of <code>etcd-druid</code> any changes to these managed resources are protected via a validating webhook. You can find more information about this webhook <a href=/docs/other-components/etcd-druid/concepts/etcd-cluster-resource-protection/>here</a>. To be able to manually modify etcd cluster managed resources two things needs to be done:</p><ol><li>Annotate the target <code>Etcd</code> resource suspending any reconciliation by <code>etcd-druid</code>. You can do this by invoking the following command:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>   kubectl annotate etcd &lt;etcd-name&gt; -n &lt;namespace&gt; druid.gardener.cloud/suspend-etcd-spec-reconcile=
</span></span></code></pre></div><ol start=2><li>Add another annotation to the target <code>Etcd</code> resource disabling managed resource protection via the webhook. You can do this by invoking the following command:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>   kubectl annotate etcd &lt;etcd-name&gt; -n &lt;namespace&gt; druid.gardener.cloud/disable-etcd-component-protection=
</span></span></code></pre></div><p>Now you are free to make changes to any managed etcd cluster resource.</p><p>!!! note
As long as the above two annotations are there, no reconciliation will be done for this etcd cluster by <code>etcd-druid</code>. Therefore it is essential that you remove this annotations eventually.ß</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ab7f535d0c55df42036fa948cede6c01>3.24 - Metrics</h1><h1 id=monitoring>Monitoring</h1><p>etcd-druid uses [Prometheus][prometheus] for metrics reporting. The metrics can be used for real-time monitoring and debugging of compaction jobs.</p><p>The simplest way to see the available metrics is to cURL the metrics endpoint <code>/metrics</code>. The format is described <a href=http://prometheus.io/docs/instrumenting/exposition_formats/>here</a>.</p><p>Follow the [Prometheus getting started doc][prometheus-getting-started] to spin up a Prometheus server to collect etcd metrics.</p><p>The naming of metrics follows the suggested [Prometheus best practices][prometheus-naming]. All compaction related metrics are put under namespace <code>etcddruid</code> and the respective subsystems.</p><h2 id=snapshot-compaction>Snapshot Compaction</h2><p>These metrics provide information about the compaction jobs that run after some interval in shoot control planes. Studying the metrics, we can deduce how many compaction job ran successfully, how many failed, how many delta events compacted etc.</p><table><thead><tr><th>Name</th><th>Description</th><th>Type</th></tr></thead><tbody><tr><td>etcddruid_compaction_jobs_total</td><td>Total number of compaction jobs initiated by compaction controller.</td><td>Counter</td></tr><tr><td>etcddruid_compaction_jobs_current</td><td>Number of currently running compaction job.</td><td>Gauge</td></tr><tr><td>etcddruid_compaction_job_duration_seconds</td><td>Total time taken in seconds to finish a running compaction job.</td><td>Histogram</td></tr><tr><td>etcddruid_compaction_num_delta_events</td><td>Total number of etcd events to be compacted by a compaction job.</td><td>Gauge</td></tr></tbody></table><p>There are two labels for <code>etcddruid_compaction_jobs_total</code> metrics. The label <code>succeeded</code> shows how many of the compaction jobs are succeeded and label <code>failed</code> shows how many of compaction jobs are failed.</p><p>There are two labels for <code>etcddruid_compaction_job_duration_seconds</code> metrics. The label <code>succeeded</code> shows how much time taken by a successful job to complete and label <code>failed</code> shows how much time taken by a failed compaction job.</p><p><code>etcddruid_compaction_jobs_current</code> metric comes with label <code>etcd_namespace</code> that indicates the namespace of the Etcd running in the control plane of a shoot cluster..</p><h2 id=etcd>Etcd</h2><p>These metrics are exposed by the <a href=https://etcd.io/>etcd</a> process that runs in each etcd pod.</p><p>The following list metrics is applicable to clustering of a multi-node etcd cluster. The full list of metrics exposed by <code>etcd</code> is available <a href=https://etcd.io/docs/v3.4/metrics>here</a>.</p><table><thead><tr><th>No.</th><th>Metrics Name</th><th>Description</th><th>Comments</th></tr></thead><tbody><tr><td>1</td><td>etcd_disk_wal_fsync_duration_seconds</td><td>latency distributions of fsync called by WAL.</td><td>High disk operation latencies indicate disk issues.</td></tr><tr><td>2</td><td>etcd_disk_backend_commit_duration_seconds</td><td>latency distributions of commit called by backend.</td><td>High disk operation latencies indicate disk issues.</td></tr><tr><td>3</td><td>etcd_server_has_leader</td><td>whether or not a leader exists. 1: leader exists, 0: leader not exists.</td><td>To capture quorum loss or to check the availability of etcd cluster.</td></tr><tr><td>4</td><td>etcd_server_is_leader</td><td>whether or not this member is a leader. 1 if it is, 0 otherwise.</td><td></td></tr><tr><td>5</td><td>etcd_server_leader_changes_seen_total</td><td>number of leader changes seen.</td><td>Helpful in fine tuning the zonal cluster like etcd-heartbeat time etc, it can also indicates the etcd load and network issues.</td></tr><tr><td>6</td><td>etcd_server_is_learner</td><td>whether or not this member is a learner. 1 if it is, 0 otherwise.</td><td></td></tr><tr><td>7</td><td>etcd_server_learner_promote_successes</td><td>total number of successful learner promotions while this member is leader.</td><td>Might be helpful in checking the success of API calls called by backup-restore.</td></tr><tr><td>8</td><td>etcd_network_client_grpc_received_bytes_total</td><td>total number of bytes received from grpc clients.</td><td>Client Traffic In.</td></tr><tr><td>9</td><td>etcd_network_client_grpc_sent_bytes_total</td><td>total number of bytes sent to grpc clients.</td><td>Client Traffic Out.</td></tr><tr><td>10</td><td>etcd_network_peer_sent_bytes_total</td><td>total number of bytes sent to peers.</td><td>Useful for network usage.</td></tr><tr><td>11</td><td>etcd_network_peer_received_bytes_total</td><td>total number of bytes received from peers.</td><td>Useful for network usage.</td></tr><tr><td>12</td><td>etcd_network_active_peers</td><td>current number of active peer connections.</td><td>Might be useful in detecting issues like network partition.</td></tr><tr><td>13</td><td>etcd_server_proposals_committed_total</td><td>total number of consensus proposals committed.</td><td>A consistently large lag between a single member and its leader indicates that member is slow or unhealthy.</td></tr><tr><td>14</td><td>etcd_server_proposals_pending</td><td>current number of pending proposals to commit.</td><td>Pending proposals suggests there is a high client load or the member cannot commit proposals.</td></tr><tr><td>15</td><td>etcd_server_proposals_failed_total</td><td>total number of failed proposals seen.</td><td>Might indicates downtime caused by a loss of quorum.</td></tr><tr><td>16</td><td>etcd_server_proposals_applied_total</td><td>total number of consensus proposals applied.</td><td>Difference between etcd_server_proposals_committed_total and etcd_server_proposals_applied_total should usually be small.</td></tr><tr><td>17</td><td>etcd_mvcc_db_total_size_in_bytes</td><td>total size of the underlying database physically allocated in bytes.</td><td></td></tr><tr><td>18</td><td>etcd_server_heartbeat_send_failures_total</td><td>total number of leader heartbeat send failures.</td><td>Might be helpful in fine-tuning the cluster or detecting slow disk or any network issues.</td></tr><tr><td>19</td><td>etcd_network_peer_round_trip_time_seconds</td><td>round-trip-time histogram between peers.</td><td>Might be helpful in fine-tuning network usage specially for zonal etcd cluster.</td></tr><tr><td>20</td><td>etcd_server_slow_apply_total</td><td>total number of slow apply requests.</td><td>Might indicate overloaded from slow disk.</td></tr><tr><td>21</td><td>etcd_server_slow_read_indexes_total</td><td>total number of pending read indexes not in sync with leader&rsquo;s or timed out read index requests.</td><td></td></tr></tbody></table><p>The full list of metrics is available <a href=https://etcd.io/docs/v3.4/metrics/>here</a>.</p><h2 id=etcd-backup-restore>Etcd-Backup-Restore</h2><p>These metrics are exposed by the <a href=https://github.com/gardener/etcd-backup-restore>etcd-backup-restore</a> container in each etcd pod.</p><p>The following list metrics is applicable to clustering of a multi-node etcd cluster. The full list of metrics exposed by <code>etcd-backup-restore</code> is available <a href=https://github.com/gardener/etcd-backup-restore/blob/master/docs/operations/metrics.md>here</a>.</p><table><thead><tr><th>No.</th><th>Metrics Name</th><th>Description</th></tr></thead><tbody><tr><td>1.</td><td>etcdbr_cluster_size</td><td>to capture the scale-up/scale-down scenarios.</td></tr><tr><td>2.</td><td>etcdbr_is_learner</td><td>whether or not this member is a learner. 1 if it is, 0 otherwise.</td></tr><tr><td>3.</td><td>etcdbr_is_learner_count_total</td><td>total number times member added as the learner.</td></tr><tr><td>4.</td><td>etcdbr_restoration_duration_seconds</td><td>total latency distribution required to restore the etcd member.</td></tr><tr><td>5.</td><td>etcdbr_add_learner_duration_seconds</td><td>total latency distribution of adding the etcd member as a learner to the cluster.</td></tr><tr><td>6.</td><td>etcdbr_member_remove_duration_seconds</td><td>total latency distribution removing the etcd member from the cluster.</td></tr><tr><td>7.</td><td>etcdbr_member_promote_duration_seconds</td><td>total latency distribution of promoting the learner to the voting member.</td></tr><tr><td>8.</td><td>etcdbr_defragmentation_duration_seconds</td><td>total latency distribution of defragmentation of each etcd cluster member.</td></tr></tbody></table><h2 id=prometheus-supplied-metrics>Prometheus supplied metrics</h2><p>The Prometheus client library provides a number of metrics under the <code>go</code> and <code>process</code> namespaces.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2d6bf3aad1d62f476e7662122ed9db62>3.25 - operator out-of-band tasks</h1><h1 id=dep-05-operator-out-of-band-tasks>DEP-05: Operator Out-of-band Tasks</h1><h2 id=summary>Summary</h2><p>This DEP proposes an enhancement to <code>etcd-druid</code>&rsquo;s capabilities to handle <a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#terminology>out-of-band</a> tasks, which are presently performed manually or invoked programmatically via suboptimal APIs. The document proposes the establishment of a unified interface by defining a well-structured API to harmonize the initiation of any <code>out-of-band</code> task, monitor its status, and simplify the process of adding new tasks and managing their lifecycles.</p><h2 id=terminology>Terminology</h2><ul><li><p><strong>etcd-druid:</strong> <a href=https://github.com/gardener/etcd-druid>etcd-druid</a> is an operator to manage the etcd clusters.</p></li><li><p><strong>backup-sidecar:</strong> It is the etcd-backup-restore sidecar container running in each etcd-member pod of etcd cluster.</p></li><li><p><strong>leading-backup-sidecar:</strong> A backup-sidecar that is associated to an etcd leader of an etcd cluster.</p></li><li><p><strong>out-of-band task:</strong> Any on-demand tasks/operations that can be executed on an etcd cluster without modifying the <a href=https://github.com/gardener/etcd-druid/blob/9c5f8254e3aeb24c1e3e88d17d8d1de336ce981b/api/v1alpha1/types_etcd.go#L272-L273>Etcd custom resource spec</a> (desired state).</p></li></ul><h2 id=motivation>Motivation</h2><p>Today, <a href=https://github.com/gardener/etcd-druid>etcd-druid</a> mainly acts as an etcd cluster provisioner (creation, maintenance and deletion). In future, capabilities of etcd-druid will be enhanced via <a href=https://github.com/gardener/etcd-druid/blob/8ac70d512969c2e12e666d923d7d35fdab1e0f8e/docs/proposals/04-etcd-member-custom-resource.md>etcd-member</a> proposal by providing it access to much more detailed information about each etcd cluster member. While we enhance the reconciliation and monitoring capabilities of etcd-druid, it still lacks the ability to allow users to invoke <code>out-of-band</code> tasks on an existing etcd cluster.</p><p>There are new learnings while operating etcd clusters at scale. It has been observed that we regularly need capabilities to trigger <code>out-of-band</code> tasks which are outside of the purview of a regular etcd reconciliation run. Many of these tasks are multi-step processes, and performing them manually is error-prone, even if an operator follows a well-written step-by-step guide. Thus, there is a need to automate these tasks.
Some examples of an <code>on-demand/out-of-band</code> tasks:</p><ul><li>Recover from a permanent quorum loss of etcd cluster.</li><li>Trigger an on-demand full/delta snapshot.</li><li>Trigger an on-demand snapshot compaction.</li><li>Trigger an on-demand maintenance of etcd cluster.</li><li>Copy the backups from one object store to another object store.</li></ul><h2 id=goals>Goals</h2><ul><li>Establish a unified interface for operator tasks by defining a single dedicated custom resource for <code>out-of-band</code> tasks.</li><li>Define a contract (in terms of prerequisites) which needs to be adhered to by any task implementation.</li><li>Facilitate the easy addition of new <code>out-of-band</code> task(s) through this custom resource.</li><li>Provide CLI capabilities to operators, making it easy to invoke supported <code>out-of-band</code> tasks.</li></ul><h2 id=non-goals>Non-Goals</h2><ul><li>In the current scope, capability to abort/suspend an <code>out-of-band</code> task is not going to be provided. This could be considered as an enhancement based on pull.</li><li>Ordering (by establishing dependency) of <code>out-of-band</code> tasks submitted for the same etcd cluster has not been considered in the first increment. In a future version based on how operator tasks are used, we will enhance this proposal and the implementation.</li></ul><h2 id=proposal>Proposal</h2><p>Authors propose creation of a new single dedicated custom resource to represent an <code>out-of-band</code> task. Etcd-druid will be enhanced to process the task requests and update its status which can then be tracked/observed.</p><h3 id=custom-resource-golang-api>Custom Resource Golang API</h3><p><code>EtcdOperatorTask</code> is the new custom resource that will be introduced. This API will be in <code>v1alpha1</code> version and will be subject to change. We will be respecting <a href=https://kubernetes.io/docs/reference/using-api/deprecation-policy/>Kubernetes Deprecation Policy</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:green>// EtcdOperatorTask represents an out-of-band operator task resource.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> EtcdOperatorTask <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  metav1.TypeMeta
</span></span><span style=display:flex><span>  metav1.ObjectMeta
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// Spec is the specification of the EtcdOperatorTask resource.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Spec EtcdOperatorTaskSpec <span style=color:#a31515>`json:&#34;spec&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// Status is most recently observed status of the EtcdOperatorTask resource.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Status EtcdOperatorTaskStatus <span style=color:#a31515>`json:&#34;status,omitempty&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h4 id=spec>Spec</h4><p>The authors propose that the following fields should be specified in the spec (desired state) of the <code>EtcdOperatorTask</code> custom resource.</p><ul><li>To capture the type of <code>out-of-band</code> operator task to be performed, <code>.spec.type</code> field should be defined. It can have values from all supported <code>out-of-band</code> tasks eg. &ldquo;OnDemandSnaphotTask&rdquo;, &ldquo;QuorumLossRecoveryTask&rdquo; etc.</li><li>To capture the configuration specific to each task, a <code>.spec.config</code> field should be defined of type <code>string</code> as each task can have different input configuration.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:green>// EtcdOperatorTaskSpec is the spec for a EtcdOperatorTask resource.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> EtcdOperatorTaskSpec <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>  <span style=color:green>// Type specifies the type of out-of-band operator task to be performed. 
</span></span></span><span style=display:flex><span><span style=color:green></span>  Type <span style=color:#2b91af>string</span> <span style=color:#a31515>`json:&#34;type&#34;`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// Config is a task specific configuration.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Config <span style=color:#2b91af>string</span> <span style=color:#a31515>`json:&#34;config,omitempty&#34;`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// TTLSecondsAfterFinished is the time-to-live to garbage collect the 
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// related resource(s) of task once it has been completed.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// +optional
</span></span></span><span style=display:flex><span><span style=color:green></span>  TTLSecondsAfterFinished *<span style=color:#2b91af>int32</span> <span style=color:#a31515>`json:&#34;ttlSecondsAfterFinished,omitempty&#34;`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// OwnerEtcdReference refers to the name and namespace of the corresponding 
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// Etcd owner for which the task has been invoked.
</span></span></span><span style=display:flex><span><span style=color:green></span>  OwnerEtcdRefrence types.NamespacedName <span style=color:#a31515>`json:&#34;ownerEtcdRefrence&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h4 id=status>Status</h4><p>The authors propose the following fields for the Status (current state) of the <code>EtcdOperatorTask</code> custom resource to monitor the progress of the task.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:green>// EtcdOperatorTaskStatus is the status for a EtcdOperatorTask resource.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> EtcdOperatorTaskStatus <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// ObservedGeneration is the most recent generation observed for the resource.
</span></span></span><span style=display:flex><span><span style=color:green></span>  ObservedGeneration *<span style=color:#2b91af>int64</span> <span style=color:#a31515>`json:&#34;observedGeneration,omitempty&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// State is the last known state of the task.
</span></span></span><span style=display:flex><span><span style=color:green></span>  State TaskState <span style=color:#a31515>`json:&#34;state&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// Time at which the task has moved from &#34;pending&#34; state to any other state.
</span></span></span><span style=display:flex><span><span style=color:green></span>  InitiatedAt metav1.Time <span style=color:#a31515>`json:&#34;initiatedAt&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// LastError represents the errors when processing the task.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// +optional
</span></span></span><span style=display:flex><span><span style=color:green></span>  LastErrors []LastError <span style=color:#a31515>`json:&#34;lastErrors,omitempty&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// Captures the last operation status if task involves many stages.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// +optional
</span></span></span><span style=display:flex><span><span style=color:green></span>  LastOperation *LastOperation <span style=color:#a31515>`json:&#34;lastOperation,omitempty&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>type</span> LastOperation <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// Name of the LastOperation.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Name opsName <span style=color:#a31515>`json:&#34;name&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// Status of the last operation, one of pending, progress, completed, failed.
</span></span></span><span style=display:flex><span><span style=color:green></span>  State OperationState <span style=color:#a31515>`json:&#34;state&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// LastTransitionTime is the time at which the operation state last transitioned from one state to another.
</span></span></span><span style=display:flex><span><span style=color:green></span>  LastTransitionTime metav1.Time <span style=color:#a31515>`json:&#34;lastTransitionTime&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// A human readable message indicating details about the last operation.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Reason <span style=color:#2b91af>string</span> <span style=color:#a31515>`json:&#34;reason&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// LastError stores details of the most recent error encountered for the task.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> LastError <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// Code is an error code that uniquely identifies an error.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Code ErrorCode <span style=color:#a31515>`json:&#34;code&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// Description is a human-readable message indicating details of the error.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Description <span style=color:#2b91af>string</span> <span style=color:#a31515>`json:&#34;description&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// ObservedAt is the time at which the error was observed.
</span></span></span><span style=display:flex><span><span style=color:green></span>  ObservedAt metav1.Time <span style=color:#a31515>`json:&#34;observedAt&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// TaskState represents the state of the task.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> TaskState <span style=color:#2b91af>string</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>const</span> (
</span></span><span style=display:flex><span>  TaskStateFailed TaskState = <span style=color:#a31515>&#34;Failed&#34;</span>
</span></span><span style=display:flex><span>  TaskStatePending TaskState = <span style=color:#a31515>&#34;Pending&#34;</span>
</span></span><span style=display:flex><span>  TaskStateRejected TaskState = <span style=color:#a31515>&#34;Rejected&#34;</span>
</span></span><span style=display:flex><span>  TaskStateSucceeded TaskState = <span style=color:#a31515>&#34;Succeeded&#34;</span>
</span></span><span style=display:flex><span>  TaskStateInProgress TaskState = <span style=color:#a31515>&#34;InProgress&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// OperationState represents the state of last operation.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> OperationState <span style=color:#2b91af>string</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>const</span> (
</span></span><span style=display:flex><span>  OperationStateFailed OperationState = <span style=color:#a31515>&#34;Failed&#34;</span>
</span></span><span style=display:flex><span>  OperationStatePending OperationState = <span style=color:#a31515>&#34;Pending&#34;</span>
</span></span><span style=display:flex><span>  OperationStateCompleted OperationState = <span style=color:#a31515>&#34;Completed&#34;</span>
</span></span><span style=display:flex><span>  OperationStateInProgress OperationState = <span style=color:#a31515>&#34;InProgress&#34;</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h3 id=custom-resource-yaml-api>Custom Resource YAML API</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: druid.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: EtcdOperatorTask
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>    name: &lt;name of operator task resource&gt;
</span></span><span style=display:flex><span>    namespace: &lt;cluster namespace&gt;
</span></span><span style=display:flex><span>    generation: &lt;specific generation of the desired state&gt;
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>    type: &lt;type/category of supported out-of-band task&gt;
</span></span><span style=display:flex><span>    ttlSecondsAfterFinished: &lt;time-to-live to garbage collect the custom resource after it has been completed&gt;
</span></span><span style=display:flex><span>    config: &lt;task specific configuration&gt;
</span></span><span style=display:flex><span>    ownerEtcdRefrence: &lt;refer to corresponding etcd owner name and namespace for which task has been invoked&gt;
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>    observedGeneration: &lt;specific observedGeneration of the resource&gt;
</span></span><span style=display:flex><span>    state: &lt;last known current state of the out-of-band task&gt;
</span></span><span style=display:flex><span>    initiatedAt: &lt;time at which task move to any other state from &#34;pending&#34; state&gt;
</span></span><span style=display:flex><span>    lastErrors:
</span></span><span style=display:flex><span>    - code: &lt;error-code&gt;
</span></span><span style=display:flex><span>      description: &lt;description of the error&gt;
</span></span><span style=display:flex><span>      observedAt: &lt;time the error was observed&gt;
</span></span><span style=display:flex><span>    lastOperation:
</span></span><span style=display:flex><span>      name: &lt;operation-name&gt;
</span></span><span style=display:flex><span>      state: &lt;task state as seen at the completion of last operation&gt;
</span></span><span style=display:flex><span>      lastTransitionTime: &lt;time of transition to this state&gt;
</span></span><span style=display:flex><span>      reason: &lt;reason/message if any&gt;
</span></span></code></pre></div><h3 id=lifecycle>Lifecycle</h3><h4 id=creation>Creation</h4><p>Task(s) can be created by creating an instance of the <code>EtcdOperatorTask</code> custom resource specific to a task.</p><blockquote><p>Note: In future, either a <code>kubectl</code> extension plugin or a <code>druidctl</code> tool will be introduced. Dedicated sub-commands will be created for each <code>out-of-band</code> task. This will drastically increase the usability for an operator for performing such tasks, as the CLI extension will automatically create relevant instance(s) of <code>EtcdOperatorTask</code> with the provided configuration.</p></blockquote><h4 id=execution>Execution</h4><ul><li>Authors propose to introduce a new controller which watches for <code>EtcdOperatorTask</code> custom resource.</li><li>Each <code>out-of-band</code> task may have some task specific configuration defined in <a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#spec>.spec.config</a>.</li><li>The controller needs to parse this task specific config, which comes as a <a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#spec>string</a>, according to the schema defined for each task.</li><li>For every <code>out-of-band</code> task, a set of <code>pre-conditions</code> can be defined. These pre-conditions are evaluated against the current state of the target etcd cluster. Based on the evaluation result (boolean), the task is permitted or denied execution.</li><li>If multiple tasks are invoked simultaneously or in <code>pending</code> state, then they will be executed in a First-In-First-Out (FIFO) manner.</li></ul><blockquote><p>Note: Dependent ordering among tasks will be addressed later which will enable concurrent execution of tasks when possible.</p></blockquote><h4 id=deletion>Deletion</h4><p>Upon completion of the task, irrespective of its final state, <code>Etcd-druid</code> will ensure the garbage collection of the task custom resource and any other Kubernetes resources created to execute the task. This will be done according to the <code>.spec.ttlSecondsAfterFinished</code> if defined in the <a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#spec>spec</a>, or a default expiry time will be assumed.</p><h3 id=use-cases>Use Cases</h3><h4 id=recovery-from-permanent-quorum-loss>Recovery from permanent quorum loss</h4><p>Recovery from permanent quorum loss involves two phases - identification and recovery - both of which are done manually today. This proposal intends to automate the latter. Recovery today is a <a href=https://github.com/gardener/etcd-druid/blob/master/docs/operations/recovery-from-permanent-quorum-loss-in-etcd-cluster.md>multi-step process</a> and needs to be performed carefully by a human operator. Automating these steps would be prudent, to make it quicker and error-free. The identification of the permanent quorum loss would remain a manual process, requiring a human operator to investigate and confirm that there is indeed a permanent quorum loss with no possibility of auto-healing.</p><h5 id=task-config>Task Config</h5><p>We do not need any config for this task. When creating an instance of <code>EtcdOperatorTask</code> for this scenario, <code>.spec.config</code> will be set to nil (unset).</p><h5 id=pre-conditions>Pre-Conditions</h5><ul><li>There should be a quorum loss in a multi-member etcd cluster. For a single-member etcd cluster, invoking this task is unnecessary as the restoration of the single member is automatically handled by the backup-restore process.</li><li>There should not already be a permanent-quorum-loss-recovery-task running for the same etcd cluster.</li></ul><h4 id=trigger-on-demand-snapshot-compaction>Trigger on-demand snapshot compaction</h4><p><code>Etcd-druid</code> provides a configurable <a href=/docs/other-components/etcd-druid/proposals/02-snapshot-compaction/#druid-flags>etcd-events-threshold</a> flag. When this threshold is breached, then a <a href=/docs/other-components/etcd-druid/proposals/02-snapshot-compaction/>snapshot compaction</a> is triggered for the etcd cluster. However, there are scenarios where an ad-hoc snapshot compaction may be required.</p><h5 id=possible-scenarios>Possible scenarios</h5><ul><li>If an operator anticipates a scenario of permanent quorum loss, they can trigger an <code>on-demand snapshot compaction</code> to create a compacted full-snapshot. This can potentially reduce the recovery time from a permanent quorum loss.</li><li>As an additional benefit, a human operator can leverage the current implementation of <a href=/docs/other-components/etcd-druid/proposals/02-snapshot-compaction/>snapshot compaction</a>, which internally triggers <code>restoration</code>. Hence, by initiating an <code>on-demand snapshot compaction</code> task, the operator can verify the integrity of etcd cluster backups, particularly in cases of potential backup corruption or re-encryption. The success or failure of this snapshot compaction can offer valuable insights into these scenarios.</li></ul><h5 id=task-config-1>Task Config</h5><p>We do not need any config for this task. When creating an instance of <code>EtcdOperatorTask</code> for this scenario, <code>.spec.config</code> will be set to nil (unset).</p><h5 id=pre-conditions-1>Pre-Conditions</h5><ul><li>There should not be a <code>on-demand snapshot compaction</code> task already running for the same etcd cluster.</li></ul><blockquote><p>Note: <code>on-demand snapshot compaction</code> runs as a separate job in a separate pod, which interacts with the backup bucket and not the etcd cluster itself, hence it doesn&rsquo;t depend on the health of etcd cluster members.</p></blockquote><h4 id=trigger-on-demand-fulldelta-snapshot>Trigger on-demand full/delta snapshot</h4><p><code>Etcd</code> custom resource provides an ability to set <a href=https://github.com/gardener/etcd-druid/blob/master/api/v1alpha1/etcd.go#L158>FullSnapshotSchedule</a> which currently defaults to run once in 24 hrs. <a href=https://github.com/gardener/etcd-druid/blob/master/api/v1alpha1/etcd.go#L167>DeltaSnapshotPeriod</a> is also made configurable which defines the duration after which a delta snapshot will be taken.
If a human operator does not wish to wait for the scheduled full/delta snapshot, they can trigger an on-demand (out-of-schedule) full/delta snapshot on the etcd cluster, which will be taken by the <code>leading-backup-restore</code>.</p><h5 id=possible-scenarios-1>Possible scenarios</h5><ul><li>An on-demand full snapshot can be triggered if scheduled snapshot fails due to any reason.</li><li><a href=https://github.com/gardener/gardener/blob/master/docs/usage/shoot_hibernate.md>Gardener Shoot Hibernation</a>: Every etcd cluster incurs an inherent cost of preserving the volumes even when a gardener shoot control plane is scaled down, i.e the shoot is in a hibernated state. However, it is possible to save on hyperscaler costs by invoking this task to take a full snapshot before scaling down the etcd cluster, and deleting the etcd data volumes afterwards.</li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md>Gardener Control Plane Migration</a>: In <a href=https://github.com/gardener/gardener>gardener</a>, a cluster control plane can be moved from one seed cluster to another. This process currently requires the etcd data to be replicated on the target cluster, so a full snapshot of the etcd cluster in the source seed before the migration would allow for faster restoration of the etcd cluster in the target seed.</li></ul><h5 id=task-config-2>Task Config</h5><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:green>// SnapshotType can be full or delta snapshot.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> SnapshotType <span style=color:#2b91af>string</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>const</span> (
</span></span><span style=display:flex><span>  SnapshotTypeFull SnapshotType = <span style=color:#a31515>&#34;full&#34;</span>
</span></span><span style=display:flex><span>  SnapshotTypeDelta SnapshotType = <span style=color:#a31515>&#34;delta&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>type</span> OnDemandSnapshotTaskConfig <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// Type of on-demand snapshot.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Type SnapshotType <span style=color:#a31515>`json:&#34;type&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  config: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    type: &lt;type of on-demand snapshot&gt;</span>    
</span></span></code></pre></div><h5 id=pre-conditions-2>Pre-Conditions</h5><ul><li>Etcd cluster should have a quorum.</li><li>There should not already be a <code>on-demand snapshot</code> task running with the same <code>SnapshotType</code> for the same etcd cluster.</li></ul><h4 id=trigger-on-demand-maintenance-of-etcd-cluster>Trigger on-demand maintenance of etcd cluster</h4><p>Operator can trigger on-demand <a href=https://etcd.io/docs/v3.5/op-guide/maintenance>maintenance of etcd cluster</a> which includes operations like <a href=https://etcd.io/docs/v3.5/op-guide/maintenance/#history-compaction-v3-api-key-value-database>etcd compaction</a>, <a href=https://etcd.io/docs/v3.5/op-guide/maintenance/#defragmentation>etcd defragmentation</a> etc.</p><h5 id=possible-scenarios-2>Possible Scenarios</h5><ul><li>If an etcd cluster is heavily loaded, which is causing performance degradation of an etcd cluster, and the operator does not want to wait for the scheduled maintenance window then an <code>on-demand maintenance</code> task can be triggered which will invoke etcd-compaction, etcd-defragmentation etc. on the target etcd cluster. This will make the etcd cluster lean and clean, thus improving cluster performance.</li></ul><h5 id=task-config-3>Task Config</h5><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>type</span> OnDemandMaintenanceTaskConfig <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// MaintenanceType defines the maintenance operations need to be performed on etcd cluster.
</span></span></span><span style=display:flex><span><span style=color:green></span>  MaintenanceType maintenanceOps <span style=color:#a31515>`json:&#34;maintenanceType`</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>type</span> maintenanceOps <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// EtcdCompaction if set to true will trigger an etcd compaction on the target etcd.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// +optional
</span></span></span><span style=display:flex><span><span style=color:green></span>  EtcdCompaction <span style=color:#2b91af>bool</span> <span style=color:#a31515>`json:&#34;etcdCompaction,omitempty&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// EtcdDefragmentation if set to true will trigger a etcd defragmentation on the target etcd.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// +optional
</span></span></span><span style=display:flex><span><span style=color:green></span>  EtcdDefragmentation <span style=color:#2b91af>bool</span> <span style=color:#a31515>`json:&#34;etcdDefragmentation,omitempty&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  config: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    maintenanceType:
</span></span></span><span style=display:flex><span><span style=color:#a31515>      etcdCompaction: &lt;true/false&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>      etcdDefragmentation: &lt;true/false&gt;</span>    
</span></span></code></pre></div><h5 id=pre-conditions-3>Pre-Conditions</h5><ul><li>Etcd cluster should have a quorum.</li><li>There should not already be a duplicate task running with same <code>maintenanceType</code>.</li></ul><h4 id=copy-backups-task>Copy Backups Task</h4><p>Copy the backups(full and delta snapshots) of etcd cluster from one object store(source) to another object store(target).</p><h5 id=possible-scenarios-3>Possible Scenarios</h5><ul><li>In <a href=https://github.com/gardener/gardener>Gardener</a>, the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md>Control Plane Migration</a> process utilizes the copy-backups task. This task is responsible for copying backups from one object store to another, typically located in different regions.</li></ul><h5 id=task-config-4>Task Config</h5><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:green>// EtcdCopyBackupsTaskConfig defines the parameters for the copy backups task.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> EtcdCopyBackupsTaskConfig <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// SourceStore defines the specification of the source object store provider.
</span></span></span><span style=display:flex><span><span style=color:green></span>  SourceStore StoreSpec <span style=color:#a31515>`json:&#34;sourceStore&#34;`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// TargetStore defines the specification of the target object store provider for storing backups.
</span></span></span><span style=display:flex><span><span style=color:green></span>  TargetStore StoreSpec <span style=color:#a31515>`json:&#34;targetStore&#34;`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// MaxBackupAge is the maximum age in days that a backup must have in order to be copied.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// By default all backups will be copied.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// +optional
</span></span></span><span style=display:flex><span><span style=color:green></span>  MaxBackupAge *<span style=color:#2b91af>uint32</span> <span style=color:#a31515>`json:&#34;maxBackupAge,omitempty&#34;`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// MaxBackups is the maximum number of backups that will be copied starting with the most recent ones.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// +optional
</span></span></span><span style=display:flex><span><span style=color:green></span>  MaxBackups *<span style=color:#2b91af>uint32</span> <span style=color:#a31515>`json:&#34;maxBackups,omitempty&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  config: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    sourceStore: &lt;source object store specification&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>    targetStore: &lt;target object store specification&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>    maxBackupAge: &lt;maximum age in days that a backup must have in order to be copied&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>    maxBackups: &lt;maximum no. of backups that will be copied&gt;</span>    
</span></span></code></pre></div><blockquote><p>Note: For detailed object store specification please refer <a href=https://github.com/gardener/etcd-druid/blob/9c5f8254e3aeb24c1e3e88d17d8d1de336ce981b/api/v1alpha1/types_common.go#L15-L29>here</a></p></blockquote><h5 id=pre-conditions-4>Pre-Conditions</h5><ul><li>There should not already be a <code>copy-backups</code> task running.</li></ul><blockquote><p>Note: <code>copy-backups-task</code> runs as a separate job, and it operates only on the backup bucket, hence it doesn&rsquo;t depend on health of etcd cluster members.</p></blockquote><blockquote><p>Note: <code>copy-backups-task</code> has already been implemented and it&rsquo;s currently being used in <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md>Control Plane Migration</a> but <code>copy-backups-task</code> will be harmonized with <code>EtcdOperatorTask</code> custom resource.</p></blockquote><h2 id=metrics>Metrics</h2><p>Authors proposed to introduce the following metrics:</p><ul><li><p><code>etcddruid_operator_task_duration_seconds</code> : Histogram which captures the runtime for each etcd operator task.
Labels:</p><ul><li>Key: <code>type</code>, Value: all supported tasks</li><li>Key: <code>state</code>, Value: One-Of {failed, succeeded, rejected}</li><li>Key: <code>etcd</code>, Value: name of the target etcd resource</li><li>Key: <code>etcd_namespace</code>, Value: namespace of the target etcd resource</li></ul></li><li><p><code>etcddruid_operator_tasks_total</code>: Counter which counts the number of etcd operator tasks.
Labels:</p><ul><li>Key: <code>type</code>, Value: all supported tasks</li><li>Key: <code>state</code>, Value: One-Of {failed, succeeded, rejected}</li><li>Key: <code>etcd</code>, Value: name of the target etcd resource</li><li>Key: <code>etcd_namespace</code>, Value: namespace of the target etcd resource</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0ce99d63319cb51781ca946d12ce413a>3.26 - Prepare Dev Environment</h1><h1 id=prepare-dev-environment>Prepare Dev Environment</h1><p>This guide will provide with detailed instructions on installing all dependencies and tools that are required to start developing and testing <code>etcd-druid</code>.</p><h2 id=macos-only-installing-homebrew>[macOS only] Installing Homebrew</h2><p>Hombrew is a popular package manager for macOS. You can install it by executing the following command in a terminal:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>/bin/bash -c <span style=color:#a31515>&#34;</span><span style=color:#00f>$(</span>curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh<span style=color:#00f>)</span><span style=color:#a31515>&#34;</span>
</span></span></code></pre></div><h2 id=installing-go>Installing Go</h2><p>On macOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install go
</span></span></code></pre></div><p>Alternatively you can also follow the <a href=https://go.dev/doc/install>Go installation documentation</a>.</p><h2 id=installing-git>Installing Git</h2><p>We use <code>git</code> as VCS which you need to install.
On macOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install git
</span></span></code></pre></div><p>For other OS, please check the <a href=https://git-scm.com/book/en/v2/Getting-Started-Installing-Git>Git installation documentation</a>.</p><h2 id=installing-docker>Installing Docker</h2><p>You need to have docker installed and running. This will allow starting a <a href=https://kind.sigs.k8s.io/>kind</a> cluster or a <a href=https://minikube.sigs.k8s.io/docs/>minikube</a> cluster for locally deploying <code>etcd-druid</code>.</p><p>On macOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install docker
</span></span></code></pre></div><p>Alternatively you can also follow the <a href=https://docs.docker.com/get-docker/>Docker installation documentation</a>.</p><h2 id=installing-kubectl>Installing Kubectl</h2><p>To interact with the local Kubernetes cluster you will need kubectl.
On macOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install kubernetes-cli
</span></span></code></pre></div><p>For other OS, please check the <a href=https://kubernetes.io/docs/tasks/tools/>Kubectl installation documentation</a>.</p><h2 id=other-tools-that-might-come-in-handy>Other tools that might come in handy</h2><p>To operate <code>etcd-druid</code> you do not need these tools but they usually come in handy when working with YAML/JSON files.
On macOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># jq (https://jqlang.github.io/jq/) is a lightweight and flexible command-line JSON processor</span>
</span></span><span style=display:flex><span>brew install jq
</span></span><span style=display:flex><span><span style=color:green># yq (https://mikefarah.gitbook.io/yq) is a lightweight and portable command-line YAML processor.</span>
</span></span><span style=display:flex><span>brew install yq
</span></span></code></pre></div><h2 id=get-the-sources>Get the sources</h2><p>Clone the repository from Github into your <code>$GOPATH</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir -p <span style=color:#00f>$(</span>go env GOPATH<span style=color:#00f>)</span>/src/github.com/gardener
</span></span><span style=display:flex><span>cd <span style=color:#00f>$(</span>go env GOPATH<span style=color:#00f>)</span>src/github.com/gardener
</span></span><span style=display:flex><span>git clone https://github.com/gardener/etcd-druid.git
</span></span><span style=display:flex><span><span style=color:green># alternatively you can also use `git clone git@github.com:gardener/etcd-druid.git`</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-2e36b4da506fc910b6926d31e069f467>3.27 - Production Setup Recommendations</h1><h1 id=setting-up-etcd-druid-in-production>Setting up etcd-druid in Production</h1><p>You can get familiar with <code>etcd-druid</code> and all the resources that it creates by setting up etcd-druid locally by following the <a href=/docs/other-components/etcd-druid/deployment/getting-started-locally/getting-started-locally/>detailed guide</a>. This document lists down recommendations for a productive setup of etcd-druid.</p><h2 id=helm-charts>Helm Charts</h2><p>You can use <a href=https://helm.sh/>helm</a> charts at <a href=https://github.com/gardener/etcd-druid/tree/55efca1c8f6c852b0a4e97f08488ffec2eed0e68/charts/druid>this</a> location to deploy druid. Values for charts are present <a href=https://github.com/gardener/etcd-druid/blob/55efca1c8f6c852b0a4e97f08488ffec2eed0e68/charts/druid/values.yaml>here</a> and can be configured as per your requirement. Following charts are present:</p><ul><li><p><code>deployment.yaml</code> - defines a kubernetes <a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/>Deployment</a> for etcd-druid. To configure the CLI flags for druid you can refer to <a href=/docs/other-components/etcd-druid/deployment/configure-etcd-druid/>this</a> document which explains these flags in detail.</p></li><li><p><code>serviceaccount.yaml</code> - defines a kubernetes <a href=https://kubernetes.io/docs/concepts/security/service-accounts/>ServiceAccount</a> which will serve as a technical user to which role/clusterroles can be bound.</p></li><li><p><code>clusterrole.yaml</code> - etcd-druid can manage multiple etcd clusters. In a <code>hosted control plane</code> setup (e.g. <a href=https://github.com/gardener/gardener>Gardener</a>), one would typically create separate <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/>namespace</a> per control-plane. This would require a <a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole>ClusterRole</a> to be defined which gives etcd-druid permissions to operate across namespaces. Packing control-planes via namespaces provides you better resource utilisation while providing you isolation from the data-plane (where the actual workload is scheduled).</p></li><li><p><code>rolebinding.yaml</code> - binds the <a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole>ClusterRole</a> defined in <code>druid-clusterrole.yaml</code> to the <a href=https://kubernetes.io/docs/concepts/security/service-accounts/>ServiceAccount</a> defined in <code>service-account.yaml</code>.</p></li><li><p><code>service.yaml</code> - defines a <code>Cluster IP</code> <a href=https://kubernetes.io/docs/concepts/services-networking/service/>Service</a> allowing other control-plane components to communicate to <code>http</code> endpoints exposed out of etcd-druid (e.g. enables <a href=https://prometheus.io/>prometheus</a> to scrap metrics, validating webhook to be invoked upon change to <code>Etcd</code> CR etc.)</p></li><li><p><code>secret-ca-crt.yaml</code> - Contains the base64 encoded CA certificate used for the etcd-druid webhook server.</p></li><li><p><code>secret-server-tls-crt.yaml</code> - Contains the base64 encoded server certificate used for the etcd-druid webhook server.</p></li><li><p><code>validating-webhook-config.yaml</code> - Configuration for all webhooks that etcd-druid registers to the webhook server. At the time of writing this document <a href=/docs/other-components/etcd-druid/concepts/etcd-cluster-resource-protection/>EtcdComponents</a> webhook gets registered.</p></li></ul><h2 id=etcd-cluster-size>Etcd cluster size</h2><p><a href=https://etcd.io/docs/v3.3/faq/#why-an-odd-number-of-cluster-members>Recommendation</a> from upstream etcd is to always have an odd number of members in an <code>Etcd</code> cluster.</p><h2 id=mounted-volume>Mounted Volume</h2><p>All <code>Etcd</code> cluster member <a href=https://kubernetes.io/docs/concepts/workloads/pods/>Pods</a> provisioned by etcd-druid mount a <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes/>Persistent Volume</a>. A mounted persistent storage helps in faster recovery in case of single-member transient failures. <code>etcd</code> is I/O intensive and its performance is heavily dependent on the <a href=https://kubernetes.io/docs/concepts/storage/storage-classes/>Storage Class</a>. It is therefore recommended that high performance SSD drives be used.</p><p>At the time of writing this document etcd-druid provisions the following volume types:</p><table><thead><tr><th>Cloud Provider</th><th>Type</th><th>Size</th></tr></thead><tbody><tr><td>AWS</td><td>GP3</td><td>25Gi</td></tr><tr><td>Azure</td><td>Premium SSD</td><td>33Gi</td></tr><tr><td>GCP</td><td>Performance (SSD) Persistent Disks (pd-ssd)</td><td>25Gi</td></tr></tbody></table><blockquote><p>Also refer: <a href=https://etcd.io/docs/v3.4/op-guide/hardware/#disks>Etcd Disk recommendation</a>.</p><p>Additionally, each cloud provider offers redundancy for managed disks. You should choose redundancy as per your availability requirement.</p></blockquote><h2 id=backup--restore>Backup & Restore</h2><p>A permanent quorum loss or data-volume corruption is a reality in production clusters and one must ensure that data loss is minimized. <code>Etcd</code> clusters provisioned via etcd-druid offer two levels of data-protection</p><p>Via <a href=https://github.com/gardener/etcd-backup-restore>etcd-backup-restore</a> all clusters started via etcd-druid get the capability to regularly take delta & full snapshots. These snapshots are stored in an object store. Additionally, a <code>snapshot-compaction</code> job is run to compact and defragment the latest snapshot, thereby reducing the time it takes to restore a cluster in case of a permanent quorum loss. You can read the <a href=/docs/other-components/etcd-druid/recovering-etcd-clusters/>detailed guide</a> on how to restore from permanent quorum loss.</p><p>It is therefore recommended that you configure an <code>Object store</code> in the cloud/infra provider of your choice, enabled backup & restore functionality by filling in <a href=https://github.com/gardener/etcd-druid/blob/55efca1c8f6c852b0a4e97f08488ffec2eed0e68/api/v1alpha1/etcd.go#L143>store</a> configuration of an <code>Etcd</code> custom CR.</p><h3 id=ransomware-protection>Ransomware protection</h3><p>Ransomware is a form of malware designed to encrypt files on a device, rendering any files and the systems that rely on them unusable. All cloud providers (<a href=https://aws.amazon.com/s3/features/object-lock/>aws</a>, <a href=https://cloud.google.com/storage/docs/bucket-lock>gcp</a>, <a href=https://learn.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview>azure</a>) provide a feature of immutability that can be set at the bucket/object level which provides <code>WORM</code> access to objects as long as the bucket/lock retention duration.</p><p>All delta & full snapshots that are periodically taken by <code>etcd-backup-restore</code> are stored in Object store provided by a cloud provider. It is recommended that these backups be protected from ransomware protection by turning locking at the bucket/object level.</p><h2 id=security>Security</h2><h3 id=use-distroless-container-images>Use Distroless Container Images</h3><p>It is generally recommended to use a minimal base image which additionally reduces the attack surface. Google&rsquo;s <a href=https://github.com/GoogleContainerTools/distroless>Distroless</a> is one way to reduce the attack surface and also minimize the size of the base image. It provides the following benefits:</p><ul><li>Reduces the attack surface</li><li>Minimizes vulnerabilities</li><li>No shell</li><li>Reduced size - only includes what is necessary</li></ul><p>For every <code>Etcd</code> cluster provisioned by etcd-druid, <code>distroless</code> images are used as base images.</p><h3 id=enable-tls-for-peer-and-client-communication>Enable TLS for Peer and Client communication</h3><p>Generally you should enable TLS for peer and client communication for an <code>Etcd</code> cluster. To enable TLS CA certificate, server and client certificates needs to be generated.
You can refer to the list of TLS artifacts that are generated for an <code>Etcd</code> cluster provisioned by etcd-druid <a href=/docs/other-components/etcd-druid/securing-etcd-clusters/>here</a>.</p><h3 id=enable-tls-for-druid-webhooks>Enable TLS for Druid Webhooks</h3><p>If you choose to enable webhooks in <code>etcd-druid</code> then it is necessary to create a separate CA and server certificate to be used by the webhooks.</p><h3 id=rotate-tls-artifacts>Rotate TLS artifacts</h3><p>It is generally recommended to rotate all TLS certificates to reduce the chances of it getting leaked or have expired. Kubernetes does not support revocation of certificates (see <a href=https://github.com/kubernetes/kubernetes/issues/18982>issue#18982</a>). One possible way to revoke certificates is to also revoke the entire chain including CA certificates.</p><h2 id=scaling-etcd-pods>Scaling etcd pods</h2><p><code>etcd</code> clusters cannot be scaled-out horizontly to meet the increased traffic/storage demand for the following reasons:</p><ul><li>There is a soft limit of 8GB and a hard limit of 10GB for the etcd DB beyond which perfomance and stability of etcd is not guaranteed.</li><li>All members of etcd maintain the entire replica of the entire DB, thus scaling-out will not really help if the storage demand grows.</li><li>Increasing the number of cluster members beyond 5 also increases the cost of consensus amongst now a larger quorum, increases load on the single leader as it needs to also participate in bringing up <a href=https://etcd.io/docs/v3.3/learning/learner/>etcd learner</a>.</li></ul><p>Therefore the following is recommended:</p><ul><li>To meet the increased demand, configure a <a href=https://github.com/kubernetes/autoscaler/tree/cecb34cb863fb015264098b5379bdba40a9113cf/vertical-pod-autoscaler>VPA</a>. You have to be careful on selection of <code>containerPolicies</code>, <code>targetRef</code>.</li><li>To meet the increased demand in storage etcd-druid already configures each etcd member to <a href=https://etcd.io/docs/v3.4/op-guide/maintenance/#auto-compaction>auto-compact</a> and it also configures periodic <a href=https://etcd.io/docs/v3.4/op-guide/maintenance/#defragmentation>defragmentation</a> of the etcd DB. The only case this will not help is when you only have unique writes all the time.</li></ul><p>!!! note
Care should be taken with usage of VPA. While it helps to vertically scale up etcd-member pods, it also can cause transient quorum loss. This is a direct consequence of the design of VPA - where recommendation is done by <a href=https://github.com/kubernetes/autoscaler/blob/2800c70d425b89e88cb6e608df494a0cd21f242d/vertical-pod-autoscaler/pkg/recommender/README.md>Recommender</a> component, <a href=https://github.com/kubernetes/autoscaler/blob/2800c70d425b89e88cb6e608df494a0cd21f242d/vertical-pod-autoscaler/pkg/updater/README.md>Updater</a> evicts the pods that do not have the resources recommended by the <code>Recommender</code> and <a href=https://github.com/kubernetes/autoscaler/blob/2800c70d425b89e88cb6e608df494a0cd21f242d/vertical-pod-autoscaler/pkg/admission-controller/README.md>Admission Controller</a> which updates the resources on the Pods. All these three components act asynchronously and can fail independently, so while VPA respects PDB&rsquo;s it can easily enter into a state where updater evicts a pod while respecting PDB but the admission controller fails to apply the recommendation. The pod comes with a default resources which still differ from the recommended values, thus causing a repeat eviction. There are other race conditions that can also occur and one needs to be careful of using VPA for quorum based workloads.</p><h2 id=high-availability>High Availability</h2><p>To ensure that an <code>Etcd</code> cluster is highly available, following is recommended:</p><h3 id=ensure-that-the-etcd-cluster-members-are-spread>Ensure that the <code>Etcd</code> cluster members are spread</h3><p><code>Etcd</code> cluster members should always be spread across nodes. This provides you failure tolerance at the node level. For failure tolerance of a zone, it is recommended that you spread the <code>Etcd</code> cluster members across zones.
We recommend that you use a combination of <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/>TopologySpreadConstraints</a> and <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>Pod Anti-Affinity</a>. To set the scheduling constraints you can either specify these constraints using <a href=https://github.com/gardener/etcd-druid/blob/55efca1c8f6c852b0a4e97f08488ffec2eed0e68/api/v1alpha1/etcd.go#L257-L265>SchedulingConstraints</a> in the <code>Etcd</code> custom resource or use a <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/>MutatingWebhook</a> to dynamically inject these into pods.</p><p>An example of scheduling constraints for a multi-node cluster with zone failure tolerance will be:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - labelSelector:
</span></span><span style=display:flex><span>      matchLabels:
</span></span><span style=display:flex><span>        app.kubernetes.io/component: etcd-statefulset
</span></span><span style=display:flex><span>        app.kubernetes.io/managed-by: etcd-druid
</span></span><span style=display:flex><span>        app.kubernetes.io/name: etcd-main
</span></span><span style=display:flex><span>        app.kubernetes.io/part-of: etcd-main
</span></span><span style=display:flex><span>    maxSkew: 1
</span></span><span style=display:flex><span>    minDomains: 3
</span></span><span style=display:flex><span>    topologyKey: kubernetes.io/hostname
</span></span><span style=display:flex><span>    whenUnsatisfiable: DoNotSchedule
</span></span><span style=display:flex><span>  - labelSelector:
</span></span><span style=display:flex><span>      matchLabels:
</span></span><span style=display:flex><span>        app.kubernetes.io/component: etcd-statefulset
</span></span><span style=display:flex><span>        app.kubernetes.io/managed-by: etcd-druid
</span></span><span style=display:flex><span>        app.kubernetes.io/name: etcd-main
</span></span><span style=display:flex><span>        app.kubernetes.io/part-of: etcd-main
</span></span><span style=display:flex><span>    maxSkew: 1
</span></span><span style=display:flex><span>    minDomains: 3
</span></span><span style=display:flex><span>    topologyKey: topology.kubernetes.io/zone
</span></span><span style=display:flex><span>    whenUnsatisfiable: DoNotSchedule
</span></span></code></pre></div><p>For a 3 member etcd-cluster, the above TopologySpreadConstraints will ensure that the members will be spread across zones (assuming there are 3 zones -> minDomains=3) and no two members will be on the same node.</p><h3 id=optimize-network-cost>Optimize Network Cost</h3><p>In most cloud providers there is no network cost (ingress/egress) for any traffic that is confined within a single zone. For <code>Zonal</code> failure tolerance, it will become imperative to spread the <code>Etcd</code> cluster across zones within a region. Knowing that an <code>Etcd</code> cluster members are quite chatty (leader election, consensus building for writes and linearizable reads etc.), this can add to the network cost.</p><p>One could evaluate using <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/>TopologyAwareRouting</a> which reduces cross-zonal traffic thus saving costs and latencies.</p><p>!!! tip
You can read about how it is done in Gardener <a href=/docs/gardener/topology_aware_routing/>here</a>.</p><h2 id=metrics--alerts>Metrics & Alerts</h2><p>Monitoring <code>etcd</code> metrics is essential for fine tuning <code>Etcd</code> clusters. etcd already exports a lot of <a href=https://etcd.io/docs/v3.4/metrics/>metrics</a>. You can see the complete list of metrics that are exposed out of an <code>Etcd</code> cluster provisioned by etcd-druid <a href=/docs/other-components/etcd-druid/monitoring/metrics/>here</a>. It is also recommended that you configure an alert for <a href=https://etcd.io/docs/v3.2/op-guide/maintenance/#space-quota>etcd space quota alarms</a>.</p><h2 id=hibernation>Hibernation</h2><p>If you have a concept of <code>hibernating</code> kubernetes clusters, then following should be kept in mind:</p><ul><li>Before you bring down the <code>Etcd</code> cluster, leverage the capability to take a <code>full snapshot</code> which captures the state of the etcd DB and stores it in the configured Object store. This ensures that when the cluster is woken up from hibernation it can restore from the last state with no data loss.</li><li>To save costs you should consider deleting the <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims>PersistentVolumeClaims</a> associated to the StatefulSet pods. However, it must be ensured that you take a full snapshot as highlighted in the previous point.</li><li>When the cluster is woken up from hibernation then you should do the following (assuming prior to hibernation the cluster had a size of 3 members):<ul><li>Start the <code>Etcd</code> cluster with 1 replica. Let it restore from the last full snapshot.</li><li>Once the cluster reports that it is ready, only then increase the replicas to its original value (e.g. 3). The other two members will start up each as learners and post learning they will join as voting members (<code>Followers</code>).</li></ul></li></ul><h2 id=reference>Reference</h2><ul><li>A nicely written <a href=https://gardener.cloud/blog/2023/03-27-high-availability-and-zone-outage-toleration/>blog post</a> on <code>High Availability and Zone Outage Toleration</code> has a lot of recommendations that one can borrow from.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-25a92afa187f613d6816ffd406564d15>3.28 - Raising A Pr</h1><h1 id=raising-a-pull-request>Raising a Pull Request</h1><p>We welcome active contributions from the community. This document details out the things-to-be-done in order for us to consider a PR for review. Contributors should follow the guidelines mentioned in this document to minimize the time it takes to get the PR reviewed.</p><h2 id=00-prerequisites>00-Prerequisites</h2><p>In order to make code contributions you must setup your development environment. Follow the <a href=/docs/other-components/etcd-druid/prepare-dev-environment/>Prepare Dev Environment Guide</a> for detailed instructions.</p><h2 id=01-raise-an-issue>01-Raise an Issue</h2><p>For every pull-request, it is <em><strong>mandatory</strong></em> to raise an <a href=https://github.com/gardener/etcd-druid/issues>Issue</a> which should describe the problem in detail. We have created a few categories, each having its own dedicated <a href=https://github.com/gardener/etcd-druid/tree/master/.github/ISSUE_TEMPLATE>template</a>.</p><h2 id=03-prepare-code-changes>03-Prepare Code Changes</h2><ul><li><p>It is <em><strong>not</strong></em> recommended to create a branch on the main repository for raising pull-requests. Instead you must fork the <code>etcd-druid</code> <a href=https://github.com/gardener/etcd-druid>repository</a> and create a branch in the fork. You can follow the <a href=https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo>detailed instructions</a> on how to fork a repository and set it up for contributions.</p></li><li><p>Ensure that you follow the <a href=https://google.github.io/styleguide/go/decisions>coding guidelines</a> while introducing new code.</p></li><li><p>If you are making changes to the API then please read <a href=/docs/other-components/etcd-druid/changing-api/>Changing-API documentation</a>.</p></li><li><p>If you are introducing new go mod dependencies then please read <a href=/docs/other-components/etcd-druid/dependency-management/>Dependency Management documentation</a>.</p></li><li><p>If you are introducing a new <code>Etcd</code> cluster component then please read <a href=/docs/other-components/etcd-druid/add-new-etcd-cluster-component/>Add new Cluster Component documentation</a>.</p></li><li><p>For guidance on testing, follow the detailed instructions <a href=/docs/other-components/etcd-druid/testing/>here</a>.</p></li><li><p>Before you submit your PR, please ensure that the following is done:</p><ul><li><p>Run <code>make check</code> which will do the following:</p><ul><li>Runs <code>make format</code> - this target will ensure a common formatting of the code and ordering of imports across all source files.</li><li>Runs <code>make manifests</code> - this target will re-generate manifests if there are any changes in the API.</li><li>Only when the above targets have run without errorrs, then <code>make check</code> will be run linters against the code. The rules for the linter are configured <a href=https://github.com/gardener/etcd-druid/blob/3383e0219a6c21c6ef1d5610db964cc3524807c8/.golangci.yaml>here</a>.</li></ul></li><li><p>Ensure that all the tests pass by running the following <code>make</code> targets:</p><ul><li><code>make test-unit</code> - this target will run all unit tests.</li><li><code>make test-integration</code> - this target will run all integration tests (controller level tests) using <code>envtest</code> framework.</li><li><code>make ci-e2e-kind</code> or any of its variants - these targets will run etcd-druid e2e tests.</li></ul><p>!!! warning
Please ensure that after introduction of new code the code coverage does not reduce. An increase in code coverage is always welcome.</p></li></ul></li><li><p>If you add new features, make sure that you create relevant documentation under <code>/docs</code>.</p></li></ul><h2 id=04-raise-a-pull-request>04-Raise a pull request</h2><ul><li>Create <em>Work In Progress [WIP]</em> pull requests only if you need a clarification or an explicit review before you can continue your work item.</li><li>Ensure that you have rebased your fork&rsquo;s development branch with <code>upstream</code> main/master branch.</li><li>Squash all commits into a minimal number of commits.</li><li>Fill in the PR template with appropriate details and provide the link to the <code>Issue</code> for which a PR has been raised.</li><li>If your patch is not getting reviewed, or you need a specific person to review it, you can @-reply a reviewer asking for a review in the pull request or a comment.</li></ul><h2 id=05-post-review>05-Post review</h2><ul><li>If a reviewer requires you to change your commit(s), please test the changes again.</li><li>Amend the affected commit(s) and force push onto your branch.</li><li>Set respective comments in your GitHub review as resolved.</li><li>Create a general PR comment to notify the reviewers that your amendments are ready for another round of review.</li></ul><h2 id=06-merging-a-pull-request>06-Merging a pull request</h2><ul><li>Merge can only be done if the PR has approvals from atleast 2 reviewers.</li><li>Add an appropriate release note detailing what is introduced as part of this PR.</li><li>Before merging the PR, ensure that you squash and then merge.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-50569cf53666643633214b2c827b0cc5>3.29 - Recovering Etcd Clusters</h1><h1 id=recovery-from-quorum-loss>Recovery from Quorum Loss</h1><p>In an <code>Etcd</code> cluster, <code>quorum</code> is a majority of nodes/members that must agree on updates to a cluster state before the cluster can authorise the DB modification. For a cluster with <code>n</code> members, quorum is <code>(n/2)+1</code>. An <code>Etcd</code> cluster is said to have <a href=https://etcd.io/docs/v3.4/op-guide/recovery/>lost quorum</a> when majority of nodes (greater than or equal to <code>(n/2)+1</code>) are unhealthy or down and as a consequence cannot participate in consensus building.</p><p>For a multi-node <code>Etcd</code> cluster quorum loss can either be <code>Transient</code> or <code>Permanent</code>.</p><h2 id=transient-quorum-loss>Transient quorum loss</h2><p>If quorum is lost through transient network failures (e.g. n/w partitions) or there is a spike in resource usage which results in OOM, <code>etcd</code> automatically and safely resumes (once the network recovers or the resource consumption has come down) and restores quorum. In other cases like transient power loss, etcd persists the Raft log to disk and replays the log to the point of failure and resumes cluster operation.</p><h2 id=permanent-quorum-loss>Permanent quorum loss</h2><p>In case the quorum is lost due to hardware failures or disk corruption etc, automatic recovery is no longer possible and it is categorized as a permanent quorum loss.</p><blockquote><p><strong>Note:</strong> If one has capability to detect <code>Failed</code> nodes and replace them, then eventually new nodes can be launched and etcd cluster can recover automatically. But sometimes this is just not possible.</p></blockquote><h3 id=recovery>Recovery</h3><p>At present, recovery from a permanent quorum loss is achieved by manually executing the steps listed in this section.</p><blockquote><p><strong>Note:</strong> In the near future etcd-druid will offer capability to automate the recovery from a permanent quorum loss via <a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/>Out-Of-Band Operator Tasks</a>. An operator only needs to ascertain that there is a permanent quorum loss and the etcd-cluster is beyond auto-recovery. Once that is established then an operator can invoke a task whose status an operator can check.</p></blockquote><p>!!! warning
Please note that manually restoring etcd can result in data loss. This guide is the last resort to bring an Etcd cluster up and running again.</p><h4 id=00-identify-the-etcd-cluster>00-Identify the etcd cluster</h4><p>It is possible to shard the etcd cluster based on resource types using <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/>&ndash;etcd-servers-overrides</a> CLI flag of <code>kube-apiserver</code>. Any sharding results in more than one etcd-cluster.</p><p>!!! info
In <code>gardener</code>, each shoot control plane has two etcd clusters, <code>etcd-events</code> which only stores events and <code>etcd-main</code> - stores everything else except events.</p><p>Identify the etcd-cluster which has a permanent quorum loss. Most of the resources of an etcd-cluster can be identified by its name. The resources of interest to recover from permanent quorum loss are: <code>Etcd</code> CR, <code>StatefulSet</code>, <code>ConfigMap</code> and <code>PVC</code>.</p><blockquote><p>To identify the <code>ConfigMap</code> resource use the following command:</p></blockquote><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span> kubectl get sts &lt;sts-name&gt; -o jsonpath=<span style=color:#a31515>&#39;{.spec.template.spec.volumes[?(@.name==&#34;etcd-config-file&#34;)].configMap.name}&#39;</span>
</span></span></code></pre></div><h4 id=01-prepare-etcd-resource-to-allow-manual-updates>01-Prepare Etcd Resource to allow manual updates</h4><p>To ensure that only one actor (in this case an operator) makes changes to the <code>Etcd</code> resource and also to the <code>Etcd</code> cluster resources, following must be done:</p><p>Add the annotation to the <code>Etcd</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl annotate etcd &lt;etcd-name&gt; -n &lt;namespace&gt; druid.gardener.cloud/suspend-etcd-spec-reconcile=
</span></span></code></pre></div><p>The above annotation will prevent any reconciliation by etcd-druid for this <code>Etcd</code> cluster.</p><p>Add another annotation to the <code>Etcd</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl annotate etcd &lt;etcd-name&gt; -n &lt;namespace&gt; druid.gardener.cloud/disable-etcd-component-protection=
</span></span></code></pre></div><p>The above annotation will allow manual edits to <code>Etcd</code> cluster resources that are managed by etcd-druid.</p><h4 id=02-scale-down-etcd-statefulset-resource-to-0>02-Scale-down Etcd StatefulSet resource to 0</h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl scale sts &lt;sts-name&gt; --replicas=0 -n &lt;namespace&gt;
</span></span></code></pre></div><h4 id=03-delete-all-pvcs-for-the-etcd-cluster>03-Delete all PVCs for the Etcd cluster</h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete pvc -l instance=&lt;sts-name&gt; -n &lt;namespace&gt;
</span></span></code></pre></div><h4 id=04-delete-all-member-leases>04-Delete All Member Leases</h4><p>For a <code>n</code> member <code>Etcd</code> cluster there should be <code>n</code> member <code>Lease</code> objects. The lease names should start with the <code>Etcd</code> name.</p><p>Example leases for a 3 node <code>Etcd</code> cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-b data-lang=b><span style=display:flex><span><span style=color:green> NAME          HOLDER                  AGE
</span></span></span><span style=display:flex><span><span style=color:green> </span>&lt;<span style=color:green>etcd</span>-<span style=color:green>name</span>&gt;-<span style=color:green>0 4c37667312a3912b:Member 1m
</span></span></span><span style=display:flex><span><span style=color:green> </span>&lt;<span style=color:green>etcd</span>-<span style=color:green>name</span>&gt;-<span style=color:green>1 75a9b74cfd3077cc:Member 1m
</span></span></span><span style=display:flex><span><span style=color:green> </span>&lt;<span style=color:green>etcd</span>-<span style=color:green>name</span>&gt;-<span style=color:green>2 c62ee6af755e890d:Leader 1m
</span></span></span></code></pre></div><p>Delete all the member leases.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete lease &lt;space separated lease names&gt;
</span></span><span style=display:flex><span><span style=color:green># Alternatively you can use label selector. From v0.23.0 onwards leases will have common set of labels</span>
</span></span><span style=display:flex><span>kubectl delete lease -l app.kubernetes.io.component=etcd-member-lease, app.kubernetes.io/part-of=&lt;etcd-name&gt; -n &lt;namespace&gt;
</span></span></code></pre></div><h4 id=05-modify-configmap>05-Modify ConfigMap</h4><p>Prerequisite to scale up etcd-cluster from 0->1 is to change the fields <code>initial-cluster</code>, <code>initial-advertise-peer-urls</code>, and <code>advertise-client-urls</code> in the ConfigMap.</p><p>Assuming that prior to scale-down to 0, there were 3 members:</p><p>The <code>initial-cluster</code> field would look like the following (assuming that the name of the etcd resource is <code>etcd-main</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green># Initial cluster</span>
</span></span><span style=display:flex><span>initial-cluster: etcd-main-0=https://etcd-main-0.etcd-main-peer.default.svc:2380,etcd-main-1=https://etcd-main-1.etcd-main-peer.default.svc:2380,etcd-main-2=https://etcd-main-2.etcd-main-peer.default.svc:2380
</span></span></code></pre></div><p>Change the <code>initial-cluster</code> field to have only one member (in this case <code>etcd-main-0</code>). After the change it should look like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green># Initial cluster</span>
</span></span><span style=display:flex><span>initial-cluster: etcd-main-0=https://etcd-main-0.etcd-main-peer.default.svc:2380
</span></span></code></pre></div><p>The <code>initial-advertise-peer-urls</code> field would look like the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green># Initial advertise peer urls</span>
</span></span><span style=display:flex><span>initial-advertise-peer-urls:
</span></span><span style=display:flex><span>  etcd-main-0:
</span></span><span style=display:flex><span>  - http://etcd-main-0.etcd-main-peer.default.svc:2380
</span></span><span style=display:flex><span>  etcd-main-1:
</span></span><span style=display:flex><span>  - http://etcd-main-1.etcd-main-peer.default.svc:2380
</span></span><span style=display:flex><span>  etcd-main-2:
</span></span><span style=display:flex><span>  - http://etcd-main-2.etcd-main-peer.default.svc:2380
</span></span></code></pre></div><p>Change the <code>initial-advertise-peer-urls</code> field to have only one member (in this case <code>etcd-main-0</code>). After the change it should look like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green># Initial advertise peer urls</span>
</span></span><span style=display:flex><span>initial-advertise-peer-urls:
</span></span><span style=display:flex><span>  etcd-main-0:
</span></span><span style=display:flex><span>  - http://etcd-main-0.etcd-main-peer.default.svc:2380
</span></span></code></pre></div><p>The <code>advertise-client-urls</code> field would look like the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>advertise-client-urls:
</span></span><span style=display:flex><span>  etcd-main-0:
</span></span><span style=display:flex><span>  - http://etcd-main-0.etcd-main-peer.default.svc:2379
</span></span><span style=display:flex><span>  etcd-main-1:
</span></span><span style=display:flex><span>  - http://etcd-main-1.etcd-main-peer.default.svc:2379
</span></span><span style=display:flex><span>  etcd-main-2:
</span></span><span style=display:flex><span>  - http://etcd-main-2.etcd-main-peer.default.svc:2379
</span></span></code></pre></div><p>Change the <code>advertise-client-urls</code> field to have only one member (in this case <code>etcd-main-0</code>). After the change it should look like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>advertise-client-urls:
</span></span><span style=display:flex><span>  etcd-main-0:
</span></span><span style=display:flex><span>  - http://etcd-main-0.etcd-main-peer.default.svc:2379
</span></span></code></pre></div><h4 id=06-scale-up-etcd-cluster-to-size-1>06-Scale up Etcd cluster to size 1</h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl scale sts &lt;sts-name&gt; -n &lt;namespace&gt; --replicas=1 
</span></span></code></pre></div><h4 id=07-wait-for-single-member-etcd-cluster-to-be-completely-ready>07-Wait for Single-Member etcd cluster to be completely ready</h4><p>To check if the <code>single-member</code> etcd cluster is ready check the status of the pod.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods &lt;etcd-name-0&gt; -n &lt;namespace&gt;
</span></span><span style=display:flex><span>NAME            READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>&lt;etcd-name&gt;-0   2/2     Running   0          1m
</span></span></code></pre></div><p>If both containers report readiness (as seen above), then the etcd-cluster is considered ready.</p><h4 id=08-enable-etcd-reconciliation-and-resource-protection>08-Enable Etcd reconciliation and resource protection</h4><p>All manual changes are now done. We must now re-enable etcd-cluster resource protection and also enable reconciliation by etcd-druid by doing the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl annotate etcd &lt;etcd-name&gt; -n &lt;namespace&gt; druid.gardener.cloud/suspend-etcd-spec-reconcile-
</span></span><span style=display:flex><span>kubectl annotate etcd &lt;etcd-name&gt; -n &lt;namespace&gt; druid.gardener.cloud/disable-etcd-component-protection-
</span></span></code></pre></div><h4 id=09-scale-up-etcd-cluster-to-3-and-trigger-reconcile>09-Scale-up Etcd Cluster to 3 and trigger reconcile</h4><p>Scale etcd-cluster to its original size (we assumed 3 below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl scale sts &lt;sts-name&gt; -n namespace --replicas=3
</span></span></code></pre></div><p>If etcd-druid has been set up with <code>--enable-etcd-spec-auto-reconcile</code> switched-off then to ensure reconciliation one must annotate <code>Etcd</code> resource with the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Annotate etcd CR to reconcile</span>
</span></span><span style=display:flex><span>kubectl annotate etcd &lt;etcd-name&gt; -n &lt;namespace&gt; gardener.cloud/operation=<span style=color:#a31515>&#34;reconcile&#34;</span>
</span></span></code></pre></div><h4 id=10-verify-etcd-cluster-health>10-Verify Etcd cluster health</h4><p>Check if all the member pods have both of their containers in <code>Running</code> state.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods -n &lt;namespace&gt; -l app.kubernetes.io/part-of=&lt;etcd-name&gt;
</span></span><span style=display:flex><span>NAME            READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>&lt;etcd-name&gt;-0   2/2     Running   0          5m
</span></span><span style=display:flex><span>&lt;etcd-name&gt;-1   2/2     Running   0          1m
</span></span><span style=display:flex><span>&lt;etcd-name&gt;-2   2/2     Running   0          1m
</span></span></code></pre></div><p>Additionally, check if the <code>Etcd</code> CR is ready:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get etcd &lt;etcd-name&gt; -n &lt;namespace&gt;
</span></span><span style=display:flex><span>NAME          READY   AGE
</span></span><span style=display:flex><span>&lt;etcd-name&gt;   true    13d
</span></span></code></pre></div><p>Check member leases, whose <code>holderIdentity</code> should reflect the member role. Check if all members are voting members (their role should either be <code>Member</code> or <code>Leader</code>). Monitor the leases for some time and check if the leases are getting updated. You can monitor the <code>AGE</code> field.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>NAME          HOLDER                  AGE
</span></span><span style=display:flex><span>&lt;etcd-name&gt;-0 4c37667312a3912b:Member 1m
</span></span><span style=display:flex><span>&lt;etcd-name&gt;-1 75a9b74cfd3077cc:Member 1m
</span></span><span style=display:flex><span>&lt;etcd-name&gt;-2 c62ee6af755e890d:Leader 1m
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-9ba281bcd2c71c7d18238add13ee189d>3.30 - Securing Etcd Clusters</h1><h1 id=securing-etcd-cluster>Securing etcd cluster</h1><p>This document will describe all the TLS artifacts that are typically generated for setting up etcd-druid and etcd clusters in Gardener clusters. You can take inspiration from this and decide which communication lines are essential to be TLS enabled.</p><h2 id=communication-lines>Communication lines</h2><p>In order to undertand all the TLS artifacts that are required to setup etcd-druid and one or more etcd-clusters, one must have a clear view of all the communication channels that needs to be protected via TLS. In the diagram below all communication lines in a typical 3-node etcd cluster along with <code>kube-apiserver</code> and <code>etcd-druid</code> is illustrated.</p><p>!!! info
For <a href=https://github.com/gardener/gardener>Gardener</a> setup all the communication lines are TLS enabled.</p><img src=/__resources/tls-communication-lines.excalidraw_ebf6a7.png alt=communication-lines style=width:1200px><h2 id=tls-artifacts>TLS artifacts</h2><p>An etcd cluster setup by <code>etcd-druid</code> leverages the following TLS artifacts:</p><ul><li><p>Certificate Authority used to sign server and client certificate key-pair for <code>etcd-backup-restore</code> specified via <code>etcd.spec.backup.tls.tlsCASecretRef</code>.</p></li><li><p>Server certificate key-pair specified via <code>etcd.spec.backup.tls.serverTLSSecretRef</code> used by <code>etcd-backup-restore</code> HTTPS server.</p></li><li><p>Client certificate key-pair specified via <code>etcd.spec.backup.tls.clientTLSSecretRef</code> used by <code>etcd-wrapper</code> to securely communicate to the <code>etcd-backup-restore</code> HTTPS server.</p></li><li><p>Certificate Authority used to sign server and client certificate key-pair for <code>etcd</code> and <code>etcd-wrapper</code> specified via <code>etcd.spec.etcd.clientUrlTls.tlsCASecretRef</code> for etcd client communication.</p></li><li><p>Server certificate key-pair specified via <code>etcd.spec.etcd.clientUrlTls.serverTLSSecretRef</code> used by <code>etcd</code> and <code>etcd-wrapper</code> HTTPS servers.</p></li><li><p>Client certificate key-pair specified via <code>etcd.spec.etcd.clientUrlTls.clientTLSSecretRef</code> used by:</p><ul><li><code>etcd-wrapper</code> and <code>etcd-backup-restore</code> to securely communicate to the <code>etcd</code> HTTPS server.</li><li><code>etcd-backup-restore</code> to securely communicate to the <code>etcd-wrapper</code> HTTPS server.</li></ul></li><li><p>Certificate Authority used to sign server certificate key-pair for <code>etcd</code> peer communication specified via <code>etcd.spec.etcd.peerUrlTls.tlsCASecretRef</code>.</p></li><li><p>Server certificate key-pair specified via <code>etcd.spec.etcd.peerUrlTls.serverTLSSecretRef</code> used for <code>etcd</code> peer communication.</p></li></ul><p>!!! note
TLS artifacts should be created prior to creating <code>Etcd</code> clusters. <code>etcd-druid</code> currently does not provide a convenience way to generate these TLS artifacts. <a href=https://etcd.io/docs/v3.4/op-guide/security/>etcd</a> recommends to use <a href=https://github.com/cloudflare/cfssl>cfssl</a> to generate certificates. However you can use any other tool as well. We do provide a convenience script for local development <a href=https://github.com/gardener/etcd-wrapper/blob/main/hack/local-dev/generate_pki.sh>here</a> which can be used to generate TLS artifacts. Currently this script is part of <a href=https://github.com/gardener/etcd-wrapper>etcd-wrapper</a> github repository but we will harmonize these scripts to be used across all github projects under the <code>etcd-druid</code> ecosystem.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ef4ed1451ba44ec64a481ffe5e79831f>3.31 - Testing</h1><h1 id=testing-strategy-and-developer-guideline>Testing Strategy and Developer Guideline</h1><p>Intent of this document is to introduce you (the developer) to the following:</p><ul><li>Libraries that are used to write tests.</li><li>Best practices to write tests that are correct, stable, fast and maintainable.</li><li>How to run tests.</li></ul><p>The guidelines are not meant to be absolute rules. Always apply common sense and adapt the guideline if it doesn&rsquo;t make much sense for some cases. If in doubt, don&rsquo;t hesitate to ask questions during a PR review (as an author, but also as a reviewer). Add new learnings as soon as we make them!</p><p>For any new contributions <strong>tests are a strict requirement</strong>. <code>Boy Scouts Rule</code> is followed: If you touch a code for which either no tests exist or coverage is insufficient then it is expected that you will add relevant tests.</p><h2 id=common-guidelines-for-writing-tests>Common guidelines for writing tests</h2><ul><li><p>We use the <code>Testing</code> package provided by the standard library in golang for writing all our tests. Refer to its <a href=https://pkg.go.dev/testing>official documentation</a> to learn how to write tests using <code>Testing</code> package. You can also refer to <a href=https://go.dev/doc/tutorial/add-a-test>this</a> example.</p></li><li><p>We use gomega as our matcher or assertion library. Refer to Gomega&rsquo;s <a href=https://onsi.github.io/gomega/>official documentation</a> for details regarding its installation and application in tests.</p></li><li><p>For naming the individual test/helper functions, ensure that the name describes what the function tests/helps-with. Naming is important for code readability even when writing tests - <a href=https://github.com/gardener/etcd-druid/blob/90995898b231a49a8f211e85160600e9e6019fe0/internal/webhook/etcdcomponents/handler_test.go#L157>example-testcase-naming</a>.</p></li><li><p>Introduce helper functions for assertions to make test more readable where applicable - <a href=https://github.com/gardener/etcd-druid/blob/4e9971aba3c3880a4cb6583d05843eabb8ca1409/test/it/controller/etcd/assertions.go#L117>example-assertion-function</a>.</p></li><li><p>Introduce custom matchers to make tests more readable where applicable - <a href=https://github.com/gardener/etcd-druid/blob/4e9971aba3c3880a4cb6583d05843eabb8ca1409/test/utils/matcher.go#L89>example-custom-matcher</a>.</p></li><li><p>Do not use <code>time.Sleep</code> and friends as it renders the tests flaky.</p></li><li><p>If a function returns a specific error then ensure that the test correctly asserts the expected error instead of just asserting that an error occurred. To help make this assertion consider using <a href=https://github.com/gardener/etcd-druid/blob/4e9971aba3c3880a4cb6583d05843eabb8ca1409/internal/errors/errors.go#L24>DruidError</a> where possible. <a href=https://github.com/gardener/etcd-druid/blob/4e9971aba3c3880a4cb6583d05843eabb8ca1409/test/utils/errors.go#L23>example-test-utility</a> & <a href=https://github.com/gardener/etcd-druid/blob/4e9971aba3c3880a4cb6583d05843eabb8ca1409/internal/component/clientservice/clientservice_test.go#L74>usage</a>.</p></li><li><p>Creating sample data for tests can be a high effort. Consider writing test utilities to generate sample data instead. <a href=https://github.com/gardener/etcd-druid/blob/4e9971aba3c3880a4cb6583d05843eabb8ca1409/test/utils/etcd.go#L61>example-test-object-builder</a>.</p></li><li><p>If tests require any arbitrary sample data then ensure that you create a <code>testdata</code> directory within the package and keep the sample data as files in it. From <a href=https://pkg.go.dev/cmd/go/internal/test>https://pkg.go.dev/cmd/go/internal/test</a></p><blockquote><p>The go tool will ignore a directory named &ldquo;testdata&rdquo;, making it available to hold ancillary data needed by the tests.</p></blockquote></li><li><p>Avoid defining shared variable/state across tests. This can lead to race conditions causing non-deterministic state. Additionally it limits the capability to run tests concurrently via <code>t.Parallel()</code>.</p></li><li><p>Do not assume or try and establish an order amongst different tests. This leads to brittle tests as the codebase evolves.</p></li><li><p>If you need to have logs produced by test runs (especially helpful in failing tests), then consider using <a href=https://pkg.go.dev/testing#T.Log>t.Log</a> or <a href=https://pkg.go.dev/testing#T.Logf>t.Logf</a>.</p></li></ul><h2 id=unit-tests>Unit Tests</h2><ul><li>If you need a kubernetes <code>client.Client</code>, prefer using <a href=https://github.com/gardener/etcd-druid/blob/master/test/utils/client.go#L67>fake client</a> instead of mocking the client. You can inject errors when building the client which enables you test error handling code paths.<ul><li>Mocks decrease maintainability because they expect the tested component to follow a certain way to reach the desired goal (e.g., call specific functions with particular arguments).</li></ul></li><li>All unit tests should be run quickly. Do not use <a href=https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/envtest>envtest</a> and do not set up a <a href=https://kind.sigs.k8s.io/>Kind</a> cluster in unit tests.</li><li>If you have common setup for variations of a function, consider using <a href=https://go.dev/wiki/TableDrivenTests>table-driven</a> tests. See <a href=https://github.com/gardener/etcd-druid/blob/4e9971aba3c3880a4cb6583d05843eabb8ca1409/internal/component/rolebinding/rolebinding_test.go#L27>this</a> as an example.</li><li>An individual test should only test one and only one thing. Do not try and test multiple variants in a single test. Either use <a href=https://go.dev/wiki/TableDrivenTests>table-driven</a> tests or write individual tests for each variation.</li><li>If a function/component has multiple steps, its probably better to split/refactor it into multiple functions/components that can be unit tested individually.</li><li>If there are a lot of edge cases, extract dedicated functions that cover them and use unit tests to test them.</li></ul><h3 id=running-unit-tests>Running Unit Tests</h3><p>!!! info
For unit tests we are currently transitioning away from <a href=https://github.com/onsi/ginkgo>ginkgo</a> to using golang native tests. The <code>make test-unit</code> target runs both ginkgo and golang native tests. Once the transition is complete this target will be simplified.</p><p>Run all unit tests</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make test-unit
</span></span></code></pre></div><p>Run unit tests of specific packages:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># if you have not already installed gotestfmt tool then install it once.</span>
</span></span><span style=display:flex><span><span style=color:green># make test-unit target automatically installs this in ./hack/tools/bin. You can alternatively point the GOBIN to this directory and then directly invoke test-go.sh</span>
</span></span><span style=display:flex><span>&gt; go install github.com/gotesttools/gotestfmt/v2/cmd/gotestfmt@v2.5.0
</span></span><span style=display:flex><span>&gt; ./hack/test-go.sh &lt;package-1&gt; &lt;package-2&gt;
</span></span></code></pre></div><h3 id=de-flaking-unit-tests>De-flaking Unit Tests</h3><p>If tests have sporadic failures, then trying running <code>./hack/stress-test.sh</code> which internally uses <a href=https://pkg.go.dev/golang.org/x/tools/cmd/stress>stress tool</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># install the stress tool</span>
</span></span><span style=display:flex><span>go install golang.org/x/tools/cmd/stress@latest
</span></span><span style=display:flex><span><span style=color:green># invoke the helper script to execute the stress test</span>
</span></span><span style=display:flex><span>./hack/stress-test.sh test-package=&lt;test-package&gt; test-func=&lt;test-function&gt; tool-params=<span style=color:#a31515>&#34;&lt;tool-params&gt;&#34;</span>
</span></span></code></pre></div><p>An example invocation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/stress-test.sh test-package=./internal/utils test-func=TestRunConcurrentlyWithAllSuccessfulTasks tool-params=<span style=color:#a31515>&#34;-p 10&#34;</span>
</span></span><span style=display:flex><span>5s: 877 runs so far, 0 failures
</span></span><span style=display:flex><span>10s: 1906 runs so far, 0 failures
</span></span><span style=display:flex><span>15s: 2885 runs so far, 0 failures
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p><code>stress</code> tool will output a path to a file containing the full failure message when a test run fails.</p><h2 id=integration-tests-envtests>Integration Tests (envtests)</h2><p>Integration tests in etcd-druid use <a href=https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/envtest>envtest</a>. It sets up a minimal temporary control plane (etcd + kube-apiserver) and runs the test against it. Test suites (group of tests) start their individual <code>envtest</code> environment before running the tests for the respective controller/webhook. Before exiting, the temporary test environment is shutdown.</p><p>!!! info
For integration-tests we are currently transitioning away from <a href=https://github.com/onsi/ginkgo>ginkgo</a> to using golang native tests. All ginkgo integration tests can be found <a href=https://github.com/gardener/etcd-druid/tree/4e9971aba3c3880a4cb6583d05843eabb8ca1409/test/integration>here</a> and golang native integration tests can be found <a href=https://github.com/gardener/etcd-druid/tree/4e9971aba3c3880a4cb6583d05843eabb8ca1409/test/it>here</a>.</p><ul><li>Integration tests in etcd-druid only targets a single controller. It is therefore advised that code (other than common utility functions should not be shared between any two controllers).</li><li>If you are sharing a common <code>envtest</code> environment across tests then it is recommended that an individual test is run in a dedicated <code>namespace</code>.</li><li>Since <code>envtest</code> is used to setup a minimum environment where no controller (e.g. KCM, Scheduler) other than <code>etcd</code> and <code>kube-apiserver</code> runs, status updates to resources controller/reconciled by not-deployed-controllers will not happen. Tests should refrain from asserting changes to status. In case status needs to be set as part of a test setup then it must be done explicitly.</li><li>If you have common setup and teardown, then consider using <a href=https://pkg.go.dev/testing#hdr-Main>TestMain</a> -<a href=https://github.com/gardener/etcd-druid/blob/4e9971aba3c3880a4cb6583d05843eabb8ca1409/test/it/controller/etcd/reconciler_test.go#L34>example</a>.</li><li>If you have to wait for resources to be provisioned or reach a specific state, then it is recommended that you create smaller assertion functions and use Gomega&rsquo;s <a href=https://pkg.go.dev/github.com/onsi/gomega#AsyncAssertion>AsyncAssertion</a> functions - <a href=https://github.com/gardener/etcd-druid/blob/4e9971aba3c3880a4cb6583d05843eabb8ca1409/test/it/controller/etcd/assertions.go#L117-L140>example</a>.<ul><li>Beware of the default <code>Eventually</code> / <code>Consistently</code> timeouts / poll intervals: <a href=https://onsi.github.io/gomega/#eventually>docs</a>.</li><li>Don&rsquo;t forget to call <code>{Eventually,Consistently}.Should()</code>, otherwise the assertions always silently succeeds without errors: <a href=https://github.com/onsi/gomega/issues/561>onsi/gomega#561</a></li></ul></li></ul><h3 id=running-integration-tests>Running Integration Tests</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make test-integration
</span></span></code></pre></div><h3 id=debugging-integration-tests>Debugging Integration Tests</h3><p>There are two ways in which you can debug Integration Tests:</p><h4 id=using-ide>Using IDE</h4><p>All commonly used IDE&rsquo;s provide in-built or easy integration with <a href=https://pkg.go.dev/github.com/go-delve/delve>delve</a> debugger. For debugging integration tests the only additional requirement is to set <code>KUBEBUILDER_ASSETS</code> environment variable. You can get the value of this environment variable by executing the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># ENVTEST_K8S_VERSION is the k8s version that you wish to use for testing.</span>
</span></span><span style=display:flex><span>setup-envtest --os <span style=color:#00f>$(</span>go env GOOS<span style=color:#00f>)</span> --arch <span style=color:#00f>$(</span>go env GOARCH<span style=color:#00f>)</span> use $ENVTEST_K8S_VERSION -p path
</span></span></code></pre></div><p>!!! tip
All integration tests usually have a timeout. If you wish to debug a failing integration-test then increase the timeouts.</p><h4 id=use-standalone-envtest>Use standalone envtest</h4><p>We also provide a capability to setup a stand-alone <code>envtest</code> and leverage the cluster to run individual integration-test. This allows you more control over when this k8s control plane is destroyed and allows you to inspect the resources at the end of the integration-test run using <code>kubectl</code>.</p><blockquote><p>While you can use an existing cluster (e.g., <code>kind</code>), some test suites expect that no controllers and no nodes are running in the test environment (as it is the case in <code>envtest</code> test environments). Hence, using a full-blown cluster with controllers and nodes might sometimes be impractical, as you would need to stop cluster components for the tests to work.</p></blockquote><p>To setup a standalone <code>envtest</code> and run an integration test against it, do the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># In a terminal session use the following make target to setup a standalone envtest</span>
</span></span><span style=display:flex><span>make start-envtest
</span></span><span style=display:flex><span><span style=color:green># As part of output path to kubeconfig will be also be printed on the console.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># In another terminal session setup resource(s) watch:</span>
</span></span><span style=display:flex><span>kubectl get po -A -w <span style=color:green># alternatively you can also use `watch -d &lt;command&gt;` utility.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># In another terminal session:</span>
</span></span><span style=display:flex><span>export KUBECONFIG=&lt;envtest-kubeconfig-path&gt;
</span></span><span style=display:flex><span>export USE_EXISTING_K8S_CLUSTER=true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># run the test</span>
</span></span><span style=display:flex><span>go test -run=<span style=color:#a31515>&#34;&lt;regex-for-test&gt;&#34;</span> &lt;package&gt;
</span></span><span style=display:flex><span><span style=color:green># example: go test -run=&#34;^TestEtcdDeletion/test deletion of all*&#34; ./test/it/controller/etcd</span>
</span></span></code></pre></div><p>Once you are done the testing you can press <code>Ctrl+C</code> in the terminal session where you started <code>envtest</code>. This will shutdown the kubernetes control plane.</p><h2 id=end-to-end-e2e-tests>End-To-End (e2e) Tests</h2><p>End-To-End tests are run using <a href=https://kind.sigs.k8s.io/>Kind</a> cluster and <a href=https://skaffold.dev/>Skaffold</a>. These tests provide a high level of confidence that the code runs as expected by users when deployed to production.</p><ul><li><p>Purpose of running these tests is to be able to catch bugs which result from interaction amongst different components within etcd-druid.</p></li><li><p>In CI pipelines e2e tests are run with S3 compatible <a href=https://www.localstack.cloud/>LocalStack</a> (in cases where backup functionality has been enabled for an etcd cluster).</p><blockquote><p>In future we will only be using a file-system based local provider to reduce the run times for the e2e tests when run in a CI pipeline.</p></blockquote></li><li><p>e2e tests can be triggered either with other cloud provider object-store emulators or they can also be run against actual/remove cloud provider object-store services.</p></li><li><p>In contrast to integration tests, in e2e tests, it might make sense to specify higher timeouts for Gomega&rsquo;s <a href=https://pkg.go.dev/github.com/onsi/gomega#AsyncAssertion>AsyncAssertion</a> calls.</p></li></ul><h3 id=running-e2e-tests-locally>Running e2e tests locally</h3><p>Detailed instructions on how to run e2e tests can be found <a href=https://github.com/gardener/etcd-druid/blob/4e9971aba3c3880a4cb6583d05843eabb8ca1409/docs/development/local-e2e-tests.md>here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-511230d8bc2a5f9d99bfcb0f84f9356e>3.32 - Updating Documentation</h1><h1 id=updating-documentation>Updating Documentation</h1><p>All documentation for <code>etcd-druid</code> resides in <a href=https://github.com/gardener/etcd-druid/tree/master/docs>docs</a> directory. If you wish to update the existing documentation or create new documentation files then read on.</p><h2 id=prerequisite-setup-mkdocs-locally>Prerequisite: Setup Mkdocs locally</h2><p><a href=https://squidfunk.github.io/mkdocs-material/>Material for Mkdocs</a> is used to generate GitHub Pages from all the Markdown files present under the <a href=https://github.com/gardener/etcd-druid/tree/master/docs>docs</a> directory. To locally validate that the documentation renders correctly, it is recommended that you perform the following setup.</p><ul><li>Install python3 if not already installed.</li><li>Setup a virtual environment via</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python -m venv venv
</span></span></code></pre></div><ul><li>Activate the virtual environment</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>source venv/bin/activate
</span></span></code></pre></div><ul><li>In the virtual environment install the packages.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>(venv) &gt; pip install mkdocs-material
</span></span><span style=display:flex><span>(venv) &gt; pip install pymdown-extensions
</span></span><span style=display:flex><span>(venv) &gt; pip install mkdocs-glightbox
</span></span><span style=display:flex><span>(venv) &gt; pip install mkdocs-pymdownx-material-extras
</span></span></code></pre></div><p>!!! note
Complete list of packages installed should be in sync with <a href=https://github.com/gardener/etcd-druid/blob/master/.github/workflows/publish-docs.yml#L23-L27>Github Actions Configuration</a>.</p><ul><li>Serve the documentation</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>(venv) &gt; mkdocs serve
</span></span></code></pre></div><p>You can now view the rendered documentation at <code>localhost:8000</code>. Any changes that you make to the docs will get hot-reloaded and you can immediately view the changes.</p><h2 id=updating-documentation-1>Updating Documentation</h2><p>All documentation <em>should</em> be in <code>markdown</code> <em>only</em>. Ensure that you take care of the following:</p><ul><li>The <a href=/docs/other-components/etcd-druid/index/>index.md</a> is the home page for the documentation rendered as Github Pages. Please do not remove this file.</li><li>If you are using a new feature (that is not already used) by <code>Mkdocs</code> then ensure that it is properly configured in <a href=https://github.com/gardener/etcd-druid/blob/master/mkdocs.yml>mkdocs.yml</a>. Additionally, if new plugins or Markdown extensions are used, make sure that you update the <a href=https://github.com/gardener/etcd-druid/blob/master/.github/workflows/publish-docs.yml#L23-L27>Github Actions Configuration</a> accordingly.</li><li>If new files are being added and you wish to show these files in Github Pages then ensure that you have added them under appropriate sections in the <a href=https://github.com/gardener/etcd-druid/blob/master/mkdocs.yml#L70>navigation</a> section of <code>mkdocs.yml</code>.</li><li>If you are linking to any file outside the <a href=https://github.com/gardener/etcd-druid/tree/master/docs>docs</a> directory then relative links will not work on Github Pages. Please get the <code>https</code> link to the file or section of the file that you wish to link.</li></ul><h2 id=raise-a-pull-request>Raise a Pull Request</h2><p>Once you have made the documentation changes then follow the guide on <a href=/docs/other-components/etcd-druid/raising-a-pr/>how to raise a PR</a>.</p><p>!!! info
Once the documentation update PR has been merged, you will be able to see the updated documentation <a href=https://gardener.github.io/etcd-druid/>here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fde42b3dc03e59d5a0f284a0a5cc18ee>3.33 - Version Compatibility Matrix</h1><h1 id=version-compatibility>Version Compatibility</h1><h2 id=kubernetes>Kubernetes</h2><p>We strongly recommend using <code>etcd-druid</code> with the supported kubernetes versions, published in this document.
The following is a list of kubernetes versions supported by the respective <code>etcd-druid</code> versions.</p><table><thead><tr><th>etcd-druid version</th><th>Kubernetes version</th></tr></thead><tbody><tr><td>>=v0.20</td><td>>=v1.21</td></tr><tr><td>>=v0.14 && &lt;0.20</td><td>All versions supported</td></tr><tr><td>&lt;v0.14</td><td>&lt; v1.25</td></tr></tbody></table><h2 id=etcd-backup-restore--etcd-wrapper>etcd-backup-restore & etcd-wrapper</h2><table><thead><tr><th>etcd-druid version</th><th>etcd-backup-restore version</th><th>etcd-wrapper version</th></tr></thead><tbody><tr><td>>=v0.23.1</td><td>>=v0.30.2</td><td>>=v0.2.0</td></tr></tbody></table></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2023 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=/js/main.min.69e2c1ae9320465ab10236d9ef752c6a4442c54b48b883b17c497b7c7d96a796.js integrity="sha256-aeLBrpMgRlqxAjbZ73UsakRCxUtIuIOxfEl7fH2Wp5Y=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>