<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/other-components/etcd-druid/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/other-components/etcd-druid/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Etcd Druid | Gardener</title><meta name=description content="A druid for etcd management in Gardener"><meta property="og:title" content="Etcd Druid"><meta property="og:description" content="A druid for etcd management in Gardener"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/other-components/etcd-druid/"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Etcd Druid"><meta itemprop=description content="A druid for etcd management in Gardener"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Etcd Druid"><meta name=twitter:description content="A druid for etcd management in Gardener"><link rel=preload href=/scss/main.min.52b703b92d167c14e82f904cd88f9dbe92798d607a8949235304e48c7cd0a116.css as=style><link href=/scss/main.min.52b703b92d167c14e82f904cd88f9dbe92798d607a8949235304e48c7cd0a116.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.467d1f755e9eb4088627b33ca265ecba.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/other-components/etcd-druid/>Return to the regular view of this page</a>.</p></div><h1 class=title>Etcd Druid</h1><div class=lead>A druid for etcd management in Gardener</div><div class=content><h1 id=etcd-druid>ETCD Druid</h1><p><image src=logo/etcd-druid-logo.png style=width:300px></image></p><p><a href=https://api.reuse.software/info/github.com/gardener/etcd-druid><img src=https://api.reuse.software/badge/github.com/gardener/etcd-druid alt="REUSE status"></a>
<a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/etcd-druid-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/etcd-druid-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/etcd-druid><img src=https://goreportcard.com/badge/github.com/gardener/gardener alt="Go Report Card"></a></p><h2 id=background>Background</h2><p><a href=https://github.com/etcd-io/etcd>Etcd</a> in the control plane of Kubernetes clusters which are managed by Gardener is deployed as a StatefulSet. The statefulset has replica of a pod containing two containers namely, etcd and <a href=https://github.com/gardener/etcd-backup-restore>backup-restore</a>. The etcd container calls components in etcd-backup-restore via REST api to perform data validation before etcd is started. If this validation fails etcd data is restored from the latest snapshot stored in the cloud-provider&rsquo;s object store. Once etcd has started, the etcd-backup-restore periodically creates full and delta snapshots. It also performs defragmentation of etcd data periodically.</p><p>The etcd-backup-restore needs as input the cloud-provider information comprising of security credentials to access the object store, the object store bucket name and prefix for the directory used to store snapshots. Currently, for operations like migration and validation, the bash script has to be updated to initiate the operation.</p><h2 id=goals>Goals</h2><ul><li>Deploy etcd and etcd-backup-restore using an etcd CRD.</li><li>Support more than one etcd replica.</li><li>Perform scheduled snapshots.</li><li>Support operations such as restores, defragmentation and scaling with zero-downtime.</li><li>Handle cloud-provider specific operation logic.</li><li>Trigger a full backup on request before volume deletion.</li><li>Offline compaction of full and delta snapshots stored in object store.</li></ul><h2 id=proposal>Proposal</h2><p>The existing method of deploying etcd and backup-sidecar as a StatefulSet alleviates the pain of ensuring the pods are live and ready after node crashes. However, deploying etcd as a Statefulset introduces a plethora of challenges. The etcd controller should be smart enough to handle etcd statefulsets taking into account limitations imposed by statefulsets. The controller shall update the status regarding how to target the K8s objects it has created. This field in the status can be leveraged by <code>HVPA</code> to scale etcd resources eventually.</p><h2 id=crd-specification>CRD specification</h2><p>The etcd CRD should contain the information required to create the etcd and backup-restore sidecar in a pod/statefulset.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: druid.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Etcd
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  finalizers:
</span></span><span style=display:flex><span>  - druid.gardener.cloud/etcd
</span></span><span style=display:flex><span>  name: test
</span></span><span style=display:flex><span>  namespace: demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    app: etcd-statefulset
</span></span><span style=display:flex><span>    gardener.cloud/role: controlplane
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-dns: allowed
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-private-networks: allowed
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-public-networks: allowed
</span></span><span style=display:flex><span>    role: test
</span></span><span style=display:flex><span>  backup:
</span></span><span style=display:flex><span>    deltaSnapshotMemoryLimit: 1Gi
</span></span><span style=display:flex><span>    deltaSnapshotPeriod: 300s
</span></span><span style=display:flex><span>    fullSnapshotSchedule: 0 <span style=color:#00f>*/24</span> * * *
</span></span><span style=display:flex><span>    garbageCollectionPeriod: 43200s
</span></span><span style=display:flex><span>    garbageCollectionPolicy: Exponential
</span></span><span style=display:flex><span>    imageRepository: europe-docker.pkg.dev/gardener-project/public/gardener/etcdbrctl
</span></span><span style=display:flex><span>    imageVersion: v0.25.0
</span></span><span style=display:flex><span>    port: 8080
</span></span><span style=display:flex><span>    resources:
</span></span><span style=display:flex><span>      limits:
</span></span><span style=display:flex><span>        cpu: 500m
</span></span><span style=display:flex><span>        memory: 2Gi
</span></span><span style=display:flex><span>      requests:
</span></span><span style=display:flex><span>        cpu: 23m
</span></span><span style=display:flex><span>        memory: 128Mi
</span></span><span style=display:flex><span>    snapstoreTempDir: /var/etcd/data/temp
</span></span><span style=display:flex><span>  etcd:
</span></span><span style=display:flex><span>    Quota: 8Gi
</span></span><span style=display:flex><span>    clientPort: 2379
</span></span><span style=display:flex><span>    defragmentationSchedule: 0 <span style=color:#00f>*/24</span> * * *
</span></span><span style=display:flex><span>    enableTLS: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    imageRepository: europe-docker.pkg.dev/gardener-project/public/gardener/etcd-wrapper
</span></span><span style=display:flex><span>    imageVersion: v0.1.0
</span></span><span style=display:flex><span>    initialClusterState: new
</span></span><span style=display:flex><span>    initialClusterToken: new
</span></span><span style=display:flex><span>    metrics: basic
</span></span><span style=display:flex><span>    pullPolicy: IfNotPresent
</span></span><span style=display:flex><span>    resources:
</span></span><span style=display:flex><span>      limits:
</span></span><span style=display:flex><span>        cpu: 2500m
</span></span><span style=display:flex><span>        memory: 4Gi
</span></span><span style=display:flex><span>      requests:
</span></span><span style=display:flex><span>        cpu: 500m
</span></span><span style=display:flex><span>        memory: 1000Mi
</span></span><span style=display:flex><span>    serverPort: 2380
</span></span><span style=display:flex><span>    storageCapacity: 80Gi
</span></span><span style=display:flex><span>    storageClass: gardener.cloud-fast
</span></span><span style=display:flex><span>  sharedConfig:
</span></span><span style=display:flex><span>    autoCompactionMode: periodic
</span></span><span style=display:flex><span>    autoCompactionRetention: 30m
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: etcd-statefulset
</span></span><span style=display:flex><span>    gardener.cloud/role: controlplane
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-dns: allowed
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-private-networks: allowed
</span></span><span style=display:flex><span>    networking.gardener.cloud/to-public-networks: allowed
</span></span><span style=display:flex><span>    role: test
</span></span><span style=display:flex><span>  pvcRetentionPolicy: DeleteAll
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  storageCapacity: 80Gi
</span></span><span style=display:flex><span>  storageClass: gardener.cloud-fast
</span></span><span style=display:flex><span>  store:
</span></span><span style=display:flex><span>    storageContainer: test
</span></span><span style=display:flex><span>    storageProvider: S3
</span></span><span style=display:flex><span>    storePrefix: etcd-test
</span></span><span style=display:flex><span>    storeSecret: etcd-backup
</span></span><span style=display:flex><span>  tlsClientSecret: etcd-client-tls
</span></span><span style=display:flex><span>  tlsServerSecret: etcd-server-tls
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  etcd:
</span></span><span style=display:flex><span>    apiVersion: apps/v1
</span></span><span style=display:flex><span>    kind: StatefulSet
</span></span><span style=display:flex><span>    name: etcd-test
</span></span></code></pre></div><h2 id=implementation-agenda>Implementation Agenda</h2><p>As first step implement defragmentation during maintenance windows. Subsequently, we will add zero-downtime upgrades and defragmentation.</p><h2 id=workflow>Workflow</h2><h3 id=deployment-workflow>Deployment workflow</h3><p><img src=/__resources/controller_989379.png alt=controller-diagram></p><h3 id=defragmentation-workflow>Defragmentation workflow</h3><p><img src=/__resources/defrag_ab05c8.png alt=defrag-diagram></p><h2 id=local-setup>Local Setup</h2><p>To setup Etcd-druid locally as a pod running inside a <a href=https://kind.sigs.k8s.io/>kind</a> cluster, follow this <a href=/docs/other-components/etcd-druid/getting-started-locally/>document</a></p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-0fd23e3f8def1f60476fbcc7a230dc6f>1 - API Reference</h1><p>Packages:</p><ul><li><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1>druid.gardener.cloud/v1alpha1</a></li></ul><h2 id=druid.gardener.cloud/v1alpha1>druid.gardener.cloud/v1alpha1</h2><p><p>Package v1alpha1 is the v1alpha1 version of the etcd-druid API.</p></p>Resource Types:<ul></ul><h3 id=druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a>)</p><p><p>BackupSpec defines parameters associated with the full and delta snapshots of etcd.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>port</code></br><em>int32</em></td><td><em>(Optional)</em><p>Port define the port on which etcd-backup-restore server will be exposed.</p></td></tr><tr><td><code>tls</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>image</code></br><em>string</em></td><td><em>(Optional)</em><p>Image defines the etcd container image and tag</p></td></tr><tr><td><code>store</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><em>(Optional)</em><p>Store defines the specification of object store provider for storing backups.</p></td></tr><tr><td><code>resources</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core>Kubernetes core/v1.ResourceRequirements</a></em></td><td><em>(Optional)</em><p>Resources defines compute Resources required by backup-restore container.
More info: <a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/</a></p></td></tr><tr><td><code>compactionResources</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core>Kubernetes core/v1.ResourceRequirements</a></em></td><td><em>(Optional)</em><p>CompactionResources defines compute Resources required by compaction job.
More info: <a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/</a></p></td></tr><tr><td><code>fullSnapshotSchedule</code></br><em>string</em></td><td><em>(Optional)</em><p>FullSnapshotSchedule defines the cron standard schedule for full snapshots.</p></td></tr><tr><td><code>garbageCollectionPolicy</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.GarbageCollectionPolicy>GarbageCollectionPolicy</a></em></td><td><em>(Optional)</em><p>GarbageCollectionPolicy defines the policy for garbage collecting old backups</p></td></tr><tr><td><code>garbageCollectionPeriod</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>GarbageCollectionPeriod defines the period for garbage collecting old backups</p></td></tr><tr><td><code>deltaSnapshotPeriod</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>DeltaSnapshotPeriod defines the period after which delta snapshots will be taken</p></td></tr><tr><td><code>deltaSnapshotMemoryLimit</code></br><em>k8s.io/apimachinery/pkg/api/resource.Quantity</em></td><td><em>(Optional)</em><p>DeltaSnapshotMemoryLimit defines the memory limit after which delta snapshots will be taken</p></td></tr><tr><td><code>compression</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompressionSpec>CompressionSpec</a></em></td><td><em>(Optional)</em><p>SnapshotCompression defines the specification for compression of Snapshots.</p></td></tr><tr><td><code>enableProfiling</code></br><em>bool</em></td><td><em>(Optional)</em><p>EnableProfiling defines if profiling should be enabled for the etcd-backup-restore-sidecar</p></td></tr><tr><td><code>etcdSnapshotTimeout</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>EtcdSnapshotTimeout defines the timeout duration for etcd FullSnapshot operation</p></td></tr><tr><td><code>leaderElection</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.LeaderElectionSpec>LeaderElectionSpec</a></em></td><td><em>(Optional)</em><p>LeaderElection defines parameters related to the LeaderElection configuration.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.ClientService>ClientService</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a>)</p><p><p>ClientService defines the parameters of the client service that a user can specify</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Annotations specify the annotations that should be added to the client service</p></td></tr><tr><td><code>labels</code></br><em>map[string]string</em></td><td><em>(Optional)</em><p>Labels specify the labels that should be added to the client service</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.CompactionMode>CompactionMode
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SharedConfig>SharedConfig</a>)</p><p><p>CompactionMode defines the auto-compaction-mode: ‘periodic’ or ‘revision’.
‘periodic’ for duration based retention and ‘revision’ for revision number based retention.</p></p><h3 id=druid.gardener.cloud/v1alpha1.CompressionPolicy>CompressionPolicy
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompressionSpec>CompressionSpec</a>)</p><p><p>CompressionPolicy defines the type of policy for compression of snapshots.</p></p><h3 id=druid.gardener.cloud/v1alpha1.CompressionSpec>CompressionSpec</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>)</p><p><p>CompressionSpec defines parameters related to compression of Snapshots(full as well as delta).</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><em>(Optional)</em></td></tr><tr><td><code>policy</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompressionPolicy>CompressionPolicy</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.Condition>Condition</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus>EtcdCopyBackupsTaskStatus</a>,
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</a>)</p><p><p>Condition holds the information about the state of a resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>type</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.ConditionType>ConditionType</a></em></td><td><p>Type of the Etcd condition.</p></td></tr><tr><td><code>status</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.ConditionStatus>ConditionStatus</a></em></td><td><p>Status of the condition, one of True, False, Unknown.</p></td></tr><tr><td><code>lastTransitionTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last time the condition transitioned from one status to another.</p></td></tr><tr><td><code>lastUpdateTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>Last time the condition was updated.</p></td></tr><tr><td><code>reason</code></br><em>string</em></td><td><p>The reason for the condition’s last transition.</p></td></tr><tr><td><code>message</code></br><em>string</em></td><td><p>A human-readable message indicating details about the transition.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.ConditionStatus>ConditionStatus
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition>Condition</a>)</p><p><p>ConditionStatus is the status of a condition.</p></p><h3 id=druid.gardener.cloud/v1alpha1.ConditionType>ConditionType
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition>Condition</a>)</p><p><p>ConditionType is the type of condition.</p></p><h3 id=druid.gardener.cloud/v1alpha1.CrossVersionObjectReference>CrossVersionObjectReference</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</a>)</p><p><p>CrossVersionObjectReference contains enough information to let you identify the referred resource.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>kind</code></br><em>string</em></td><td><p>Kind of the referent</p></td></tr><tr><td><code>name</code></br><em>string</em></td><td><p>Name of the referent</p></td></tr><tr><td><code>apiVersion</code></br><em>string</em></td><td><em>(Optional)</em><p>API version of the referent</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.Etcd>Etcd</h3><p><p>Etcd is the Schema for the etcds API</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a></em></td><td><br><br><table><tr><td><code>selector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><p>selector is a label query over pods that should match the replica count.
It must match the pod template’s labels.
More info: <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors>https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors</a></p></td></tr><tr><td><code>labels</code></br><em>map[string]string</em></td><td></td></tr><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em></td></tr><tr><td><code>etcd</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a></em></td><td></td></tr><tr><td><code>backup</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a></em></td><td></td></tr><tr><td><code>sharedConfig</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SharedConfig>SharedConfig</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>schedulingConstraints</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SchedulingConstraints>SchedulingConstraints</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>replicas</code></br><em>int32</em></td><td></td></tr><tr><td><code>priorityClassName</code></br><em>string</em></td><td><em>(Optional)</em><p>PriorityClassName is the name of a priority class that shall be used for the etcd pods.</p></td></tr><tr><td><code>storageClass</code></br><em>string</em></td><td><em>(Optional)</em><p>StorageClass defines the name of the StorageClass required by the claim.
More info: <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1>https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1</a></p></td></tr><tr><td><code>storageCapacity</code></br><em>k8s.io/apimachinery/pkg/api/resource.Quantity</em></td><td><em>(Optional)</em><p>StorageCapacity defines the size of persistent volume.</p></td></tr><tr><td><code>volumeClaimTemplate</code></br><em>string</em></td><td><em>(Optional)</em><p>VolumeClaimTemplate defines the volume claim template to be created</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</a></em></td><td></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a>)</p><p><p>EtcdConfig defines parameters associated etcd deployed</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>quota</code></br><em>k8s.io/apimachinery/pkg/api/resource.Quantity</em></td><td><em>(Optional)</em><p>Quota defines the etcd DB quota.</p></td></tr><tr><td><code>defragmentationSchedule</code></br><em>string</em></td><td><em>(Optional)</em><p>DefragmentationSchedule defines the cron standard schedule for defragmentation of etcd.</p></td></tr><tr><td><code>serverPort</code></br><em>int32</em></td><td><em>(Optional)</em></td></tr><tr><td><code>clientPort</code></br><em>int32</em></td><td><em>(Optional)</em></td></tr><tr><td><code>image</code></br><em>string</em></td><td><em>(Optional)</em><p>Image defines the etcd container image and tag</p></td></tr><tr><td><code>authSecretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>metrics</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.MetricsLevel>MetricsLevel</a></em></td><td><em>(Optional)</em><p>Metrics defines the level of detail for exported metrics of etcd, specify ‘extensive’ to include histogram metrics.</p></td></tr><tr><td><code>resources</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core>Kubernetes core/v1.ResourceRequirements</a></em></td><td><em>(Optional)</em><p>Resources defines the compute Resources required by etcd container.
More info: <a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/</a></p></td></tr><tr><td><code>clientUrlTls</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</a></em></td><td><em>(Optional)</em><p>ClientUrlTLS contains the ca, server TLS and client TLS secrets for client communication to ETCD cluster</p></td></tr><tr><td><code>peerUrlTls</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</a></em></td><td><em>(Optional)</em><p>PeerUrlTLS contains the ca and server TLS secrets for peer communication within ETCD cluster
Currently, PeerUrlTLS does not require client TLS secrets for gardener implementation of ETCD cluster.</p></td></tr><tr><td><code>etcdDefragTimeout</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>EtcdDefragTimeout defines the timeout duration for etcd defrag call</p></td></tr><tr><td><code>heartbeatDuration</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>HeartbeatDuration defines the duration for members to send heartbeats. The default value is 10s.</p></td></tr><tr><td><code>clientService</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.ClientService>ClientService</a></em></td><td><em>(Optional)</em><p>ClientService defines the parameters of the client service that a user can specify</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask>EtcdCopyBackupsTask</h3><p><p>EtcdCopyBackupsTask is a task for copying etcd backups from a source to a target store.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>metadata</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta>Kubernetes meta/v1.ObjectMeta</a></em></td><td>Refer to the Kubernetes API documentation for the fields of the
<code>metadata</code> field.</td></tr><tr><td><code>spec</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec>EtcdCopyBackupsTaskSpec</a></em></td><td><br><br><table><tr><td><code>sourceStore</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><p>SourceStore defines the specification of the source object store provider for storing backups.</p></td></tr><tr><td><code>targetStore</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><p>TargetStore defines the specification of the target object store provider for storing backups.</p></td></tr><tr><td><code>maxBackupAge</code></br><em>uint32</em></td><td><em>(Optional)</em><p>MaxBackupAge is the maximum age in days that a backup must have in order to be copied.
By default all backups will be copied.</p></td></tr><tr><td><code>maxBackups</code></br><em>uint32</em></td><td><em>(Optional)</em><p>MaxBackups is the maximum number of backups that will be copied starting with the most recent ones.</p></td></tr><tr><td><code>waitForFinalSnapshot</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec>WaitForFinalSnapshotSpec</a></em></td><td><em>(Optional)</em><p>WaitForFinalSnapshot defines the parameters for waiting for a final full snapshot before copying backups.</p></td></tr></table></td></tr><tr><td><code>status</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus>EtcdCopyBackupsTaskStatus</a></em></td><td></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec>EtcdCopyBackupsTaskSpec</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask>EtcdCopyBackupsTask</a>)</p><p><p>EtcdCopyBackupsTaskSpec defines the parameters for the copy backups task.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>sourceStore</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><p>SourceStore defines the specification of the source object store provider for storing backups.</p></td></tr><tr><td><code>targetStore</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a></em></td><td><p>TargetStore defines the specification of the target object store provider for storing backups.</p></td></tr><tr><td><code>maxBackupAge</code></br><em>uint32</em></td><td><em>(Optional)</em><p>MaxBackupAge is the maximum age in days that a backup must have in order to be copied.
By default all backups will be copied.</p></td></tr><tr><td><code>maxBackups</code></br><em>uint32</em></td><td><em>(Optional)</em><p>MaxBackups is the maximum number of backups that will be copied starting with the most recent ones.</p></td></tr><tr><td><code>waitForFinalSnapshot</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec>WaitForFinalSnapshotSpec</a></em></td><td><em>(Optional)</em><p>WaitForFinalSnapshot defines the parameters for waiting for a final full snapshot before copying backups.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus>EtcdCopyBackupsTaskStatus</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask>EtcdCopyBackupsTask</a>)</p><p><p>EtcdCopyBackupsTaskStatus defines the observed state of the copy backups task.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>conditions</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition>[]Condition</a></em></td><td><em>(Optional)</em><p>Conditions represents the latest available observations of an object’s current state.</p></td></tr><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed for this resource.</p></td></tr><tr><td><code>lastError</code></br><em>string</em></td><td><em>(Optional)</em><p>LastError represents the last occurred error.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdMemberConditionStatus>EtcdMemberConditionStatus
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberStatus>EtcdMemberStatus</a>)</p><p><p>EtcdMemberConditionStatus is the status of an etcd cluster member.</p></p><h3 id=druid.gardener.cloud/v1alpha1.EtcdMemberStatus>EtcdMemberStatus</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</a>)</p><p><p>EtcdMemberStatus holds information about a etcd cluster membership.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>name</code></br><em>string</em></td><td><p>Name is the name of the etcd member. It is the name of the backing <code>Pod</code>.</p></td></tr><tr><td><code>id</code></br><em>string</em></td><td><em>(Optional)</em><p>ID is the ID of the etcd member.</p></td></tr><tr><td><code>role</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdRole>EtcdRole</a></em></td><td><em>(Optional)</em><p>Role is the role in the etcd cluster, either <code>Leader</code> or <code>Member</code>.</p></td></tr><tr><td><code>status</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberConditionStatus>EtcdMemberConditionStatus</a></em></td><td><p>Status of the condition, one of True, False, Unknown.</p></td></tr><tr><td><code>reason</code></br><em>string</em></td><td><p>The reason for the condition’s last transition.</p></td></tr><tr><td><code>lastTransitionTime</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta>Kubernetes meta/v1.Time</a></em></td><td><p>LastTransitionTime is the last time the condition’s status changed.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdRole>EtcdRole
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberStatus>EtcdMemberStatus</a>)</p><p><p>EtcdRole is the role of an etcd cluster member.</p></p><h3 id=druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Etcd>Etcd</a>)</p><p><p>EtcdSpec defines the desired state of Etcd</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>selector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><p>selector is a label query over pods that should match the replica count.
It must match the pod template’s labels.
More info: <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors>https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors</a></p></td></tr><tr><td><code>labels</code></br><em>map[string]string</em></td><td></td></tr><tr><td><code>annotations</code></br><em>map[string]string</em></td><td><em>(Optional)</em></td></tr><tr><td><code>etcd</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a></em></td><td></td></tr><tr><td><code>backup</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a></em></td><td></td></tr><tr><td><code>sharedConfig</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SharedConfig>SharedConfig</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>schedulingConstraints</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SchedulingConstraints>SchedulingConstraints</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>replicas</code></br><em>int32</em></td><td></td></tr><tr><td><code>priorityClassName</code></br><em>string</em></td><td><em>(Optional)</em><p>PriorityClassName is the name of a priority class that shall be used for the etcd pods.</p></td></tr><tr><td><code>storageClass</code></br><em>string</em></td><td><em>(Optional)</em><p>StorageClass defines the name of the StorageClass required by the claim.
More info: <a href=https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1>https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1</a></p></td></tr><tr><td><code>storageCapacity</code></br><em>k8s.io/apimachinery/pkg/api/resource.Quantity</em></td><td><em>(Optional)</em><p>StorageCapacity defines the size of persistent volume.</p></td></tr><tr><td><code>volumeClaimTemplate</code></br><em>string</em></td><td><em>(Optional)</em><p>VolumeClaimTemplate defines the volume claim template to be created</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.EtcdStatus>EtcdStatus</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Etcd>Etcd</a>)</p><p><p>EtcdStatus defines the observed state of Etcd.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>observedGeneration</code></br><em>int64</em></td><td><em>(Optional)</em><p>ObservedGeneration is the most recent generation observed for this resource.</p></td></tr><tr><td><code>etcd</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CrossVersionObjectReference>CrossVersionObjectReference</a></em></td><td><em>(Optional)</em></td></tr><tr><td><code>conditions</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition>[]Condition</a></em></td><td><em>(Optional)</em><p>Conditions represents the latest available observations of an etcd’s current state.</p></td></tr><tr><td><code>serviceName</code></br><em>string</em></td><td><em>(Optional)</em><p>ServiceName is the name of the etcd service.</p></td></tr><tr><td><code>lastError</code></br><em>string</em></td><td><em>(Optional)</em><p>LastError represents the last occurred error.</p></td></tr><tr><td><code>clusterSize</code></br><em>int32</em></td><td><em>(Optional)</em><p>Cluster size is the size of the etcd cluster.</p></td></tr><tr><td><code>currentReplicas</code></br><em>int32</em></td><td><em>(Optional)</em><p>CurrentReplicas is the current replica count for the etcd cluster.</p></td></tr><tr><td><code>replicas</code></br><em>int32</em></td><td><em>(Optional)</em><p>Replicas is the replica count of the etcd resource.</p></td></tr><tr><td><code>readyReplicas</code></br><em>int32</em></td><td><em>(Optional)</em><p>ReadyReplicas is the count of replicas being ready in the etcd cluster.</p></td></tr><tr><td><code>ready</code></br><em>bool</em></td><td><em>(Optional)</em><p>Ready is <code>true</code> if all etcd replicas are ready.</p></td></tr><tr><td><code>updatedReplicas</code></br><em>int32</em></td><td><em>(Optional)</em><p>UpdatedReplicas is the count of updated replicas in the etcd cluster.</p></td></tr><tr><td><code>labelSelector</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta>Kubernetes meta/v1.LabelSelector</a></em></td><td><em>(Optional)</em><p>LabelSelector is a label query over pods that should match the replica count.
It must match the pod template’s labels.</p></td></tr><tr><td><code>members</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberStatus>[]EtcdMemberStatus</a></em></td><td><em>(Optional)</em><p>Members represents the members of the etcd cluster</p></td></tr><tr><td><code>peerUrlTLSEnabled</code></br><em>bool</em></td><td><em>(Optional)</em><p>PeerUrlTLSEnabled captures the state of peer url TLS being enabled for the etcd member(s)</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.GarbageCollectionPolicy>GarbageCollectionPolicy
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>)</p><p><p>GarbageCollectionPolicy defines the type of policy for snapshot garbage collection.</p></p><h3 id=druid.gardener.cloud/v1alpha1.LeaderElectionSpec>LeaderElectionSpec</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>)</p><p><p>LeaderElectionSpec defines parameters related to the LeaderElection configuration.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>reelectionPeriod</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>ReelectionPeriod defines the Period after which leadership status of corresponding etcd is checked.</p></td></tr><tr><td><code>etcdConnectionTimeout</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>EtcdConnectionTimeout defines the timeout duration for etcd client connection during leader election.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.MetricsLevel>MetricsLevel
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a>)</p><p><p>MetricsLevel defines the level ‘basic’ or ‘extensive’.</p></p><h3 id=druid.gardener.cloud/v1alpha1.SchedulingConstraints>SchedulingConstraints</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a>)</p><p><p>SchedulingConstraints defines the different scheduling constraints that must be applied to the
pod spec in the etcd statefulset.
Currently supported constraints are Affinity and TopologySpreadConstraints.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>affinity</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#affinity-v1-core>Kubernetes core/v1.Affinity</a></em></td><td><em>(Optional)</em><p>Affinity defines the various affinity and anti-affinity rules for a pod
that are honoured by the kube-scheduler.</p></td></tr><tr><td><code>topologySpreadConstraints</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#topologyspreadconstraint-v1-core>[]Kubernetes core/v1.TopologySpreadConstraint</a></em></td><td><em>(Optional)</em><p>TopologySpreadConstraints describes how a group of pods ought to spread across topology domains,
that are honoured by the kube-scheduler.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.SecretReference>SecretReference</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</a>)</p><p><p>SecretReference defines a reference to a secret.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>SecretReference</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><p>(Members of <code>SecretReference</code> are embedded into this type.)</p></td></tr><tr><td><code>dataKey</code></br><em>string</em></td><td><em>(Optional)</em><p>DataKey is the name of the key in the data map containing the credentials.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.SharedConfig>SharedConfig</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec>EtcdSpec</a>)</p><p><p>SharedConfig defines parameters shared and used by Etcd as well as backup-restore sidecar.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>autoCompactionMode</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompactionMode>CompactionMode</a></em></td><td><em>(Optional)</em><p>AutoCompactionMode defines the auto-compaction-mode:‘periodic’ mode or ‘revision’ mode for etcd and embedded-Etcd of backup-restore sidecar.</p></td></tr><tr><td><code>autoCompactionRetention</code></br><em>string</em></td><td><em>(Optional)</em><p>AutoCompactionRetention defines the auto-compaction-retention length for etcd as well as for embedded-Etcd of backup-restore sidecar.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.StorageProvider>StorageProvider
(<code>string</code> alias)</p></h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</a>)</p><p><p>StorageProvider defines the type of object store provider for storing backups.</p></p><h3 id=druid.gardener.cloud/v1alpha1.StoreSpec>StoreSpec</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>,
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec>EtcdCopyBackupsTaskSpec</a>)</p><p><p>StoreSpec defines parameters related to ObjectStore persisting backups</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>container</code></br><em>string</em></td><td><em>(Optional)</em><p>Container is the name of the container the backup is stored at.</p></td></tr><tr><td><code>prefix</code></br><em>string</em></td><td><p>Prefix is the prefix used for the store.</p></td></tr><tr><td><code>provider</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StorageProvider>StorageProvider</a></em></td><td><em>(Optional)</em><p>Provider is the name of the backup provider.</p></td></tr><tr><td><code>secretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><em>(Optional)</em><p>SecretRef is the reference to the secret which used to connect to the backup store.</p></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.TLSConfig>TLSConfig</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec>BackupSpec</a>,
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig>EtcdConfig</a>)</p><p><p>TLSConfig hold the TLS configuration details.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>tlsCASecretRef</code></br><em><a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SecretReference>SecretReference</a></em></td><td></td></tr><tr><td><code>serverTLSSecretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td></td></tr><tr><td><code>clientTLSSecretRef</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core>Kubernetes core/v1.SecretReference</a></em></td><td><em>(Optional)</em></td></tr></tbody></table><h3 id=druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec>WaitForFinalSnapshotSpec</h3><p>(<em>Appears on:</em>
<a href=/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec>EtcdCopyBackupsTaskSpec</a>)</p><p><p>WaitForFinalSnapshotSpec defines the parameters for waiting for a final full snapshot before copying backups.</p></p><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody><tr><td><code>enabled</code></br><em>bool</em></td><td><p>Enabled specifies whether to wait for a final full snapshot before copying backups.</p></td></tr><tr><td><code>timeout</code></br><em><a href=https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta>Kubernetes meta/v1.Duration</a></em></td><td><em>(Optional)</em><p>Timeout is the timeout for waiting for a final full snapshot. When this timeout expires, the copying of backups
will be performed anyway. No timeout or 0 means wait forever.</p></td></tr></tbody></table><hr><p><em>Generated with <a href=https://github.com/ahmetb/gen-crd-api-reference-docs>gen-crd-api-reference-docs</a></em></p></div><div class=td-content style=page-break-before:always><h1 id=pg-26b0cf1c7a98444327254b2dca32b345>2 - 01 Multi Node Etcd Clusters</h1><h1 id=multi-node-etcd-cluster-instances-via-etcd-druid>Multi-node etcd cluster instances via etcd-druid</h1><p>This document proposes an approach (along with some alternatives) to support provisioning and management of multi-node etcd cluster instances via <a href=https://github.com/gardener/etcd-druid>etcd-druid</a> and <a href=https://github.com/gardener/etcd-backup-restore>etcd-backup-restore</a>.</p><h2 id=content>Content</h2><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#multi-node-etcd-cluster-instances-via-etcd-druid>Multi-node etcd cluster instances via etcd-druid</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#content>Content</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#goal>Goal</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#background-and-motivation>Background and Motivation</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#single-node-etcd-cluster>Single-node etcd cluster</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#multi-node-etcd-cluster>Multi-node etcd-cluster</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#dynamic-multi-node-etcd-cluster>Dynamic multi-node etcd cluster</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#prior-art>Prior Art</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-operator-from-coreos>ETCD Operator from CoreOS</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcdadm-from-kubernetes-sigs>etcdadm from kubernetes-sigs</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-cluster-operator-from-improbable-engineering>Etcd Cluster Operator from Improbable-Engineering</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#general-approach-to-etcd-cluster-management>General Approach to ETCD Cluster Management</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping>Bootstrapping</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#assumptions>Assumptions</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>Adding a new member to an etcd cluster</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note>Note</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative>Alternative</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#managing-failures>Managing Failures</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster>Removing an existing member from an etcd cluster</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>Restarting an existing member of an etcd cluster</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>Recovering an etcd cluster from failure of majority of members</a></li></ul></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#kubernetes-context>Kubernetes Context</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-1>Alternative</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-configuration>ETCD Configuration</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-2>Alternative</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#data-persistence>Data Persistence</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent>Persistent</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>Ephemeral</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#disk>Disk</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#in-memory>In-memory</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#how-to-detect-if-valid-metadata-exists-in-an-etcd-member>How to detect if valid metadata exists in an etcd member</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommendation>Recommendation</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#how-to-detect-if-valid-data-exists-in-an-etcd-member>How to detect if valid data exists in an etcd member</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommendation-1>Recommendation</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#separating-peer-and-client-traffic>Separating peer and client traffic</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#cutting-off-client-requests>Cutting off client requests</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector>Manipulating Client Service podSelector</a></li></ul></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#health-check>Health Check</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure>Backup Failure</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-3>Alternative</a></li></ul></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status>Status</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#members>Members</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note-1>Note</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-name-as-the-key>Member name as the key</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>Member Leases</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions>Conditions</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#clustersize>ClusterSize</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-4>Alternative</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#decision-table-for-etcd-druid-based-on-the-status>Decision table for etcd-druid based on the status</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#1-pink-of-health>1. Pink of health</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#2-member-status-is-out-of-sync-with-their-leases>2. Member status is out of sync with their leases</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-1>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-1>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#3-all-members-are-ready-but-allmembersready-condition-is-stale>3. All members are <code>Ready</code> but <code>AllMembersReady</code> condition is stale</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-2>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-2>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#4-not-all-members-are-ready-but-allmembersready-condition-is-stale>4. Not all members are <code>Ready</code> but <code>AllMembersReady</code> condition is stale</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-3>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-3>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#5-majority-members-are-ready-but-ready-condition-is-stale>5. Majority members are <code>Ready</code> but <code>Ready</code> condition is stale</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-4>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-4>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#6-majority-members-are-notready-but-ready-condition-is-stale>6. Majority members are <code>NotReady</code> but <code>Ready</code> condition is stale</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-5>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-5>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#7-some-members-have-been-in-unknown-status-for-a-while>7. Some members have been in <code>Unknown</code> status for a while</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-6>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-6>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#8-some-member-pods-are-not-ready-but-have-not-had-the-chance-to-update-their-status>8. Some member pods are not <code>Ready</code> but have not had the chance to update their status</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-7>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-7>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#9-quorate-cluster-with-a-minority-of-members-notready>9. Quorate cluster with a minority of members <code>NotReady</code></a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-8>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-8>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#10-quorum-lost-with-a-majority-of-members-notready>10. Quorum lost with a majority of members <code>NotReady</code></a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-9>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-9>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#11-scale-up-of-a-healthy-cluster>11. Scale up of a healthy cluster</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-10>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-10>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#12-scale-down-of-a-healthy-cluster>12. Scale down of a healthy cluster</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-11>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-11>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status>13. Superfluous member entries in <code>Etcd</code> status</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-12>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12>Recommended Action</a></li></ul></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#decision-table-for-etcd-backup-restore-during-initialization>Decision table for etcd-backup-restore during initialization</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#1-first-member-during-bootstrap-of-a-fresh-etcd-cluster>1. First member during bootstrap of a fresh etcd cluster</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-13>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-13>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#2-addition-of-a-new-following-member-during-bootstrap-of-a-fresh-etcd-cluster>2. Addition of a new following member during bootstrap of a fresh etcd cluster</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-14>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-14>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#3-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-and-data>3. Restart of an existing member of a quorate cluster with valid metadata and data</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-15>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-15>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#4-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-but-without-valid-data>4. Restart of an existing member of a quorate cluster with valid metadata but without valid data</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-16>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-16>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#5-restart-of-an-existing-member-of-a-quorate-cluster-without-valid-metadata>5. Restart of an existing member of a quorate cluster without valid metadata</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-17>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-17>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#6-restart-of-an-existing-member-of-a-non-quorate-cluster-with-valid-metadata-and-data>6. Restart of an existing member of a non-quorate cluster with valid metadata and data</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-18>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-18>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#7-restart-of-the-first-member-of-a-non-quorate-cluster-without-valid-data>7. Restart of the first member of a non-quorate cluster without valid data</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-19>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-19>Recommended Action</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#8-restart-of-a-following-member-of-a-non-quorate-cluster-without-valid-data>8. Restart of a following member of a non-quorate cluster without valid data</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-20>Observed state</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-20>Recommended Action</a></li></ul></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup>Backup</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#leading-etcd-main-containers-sidecar-is-the-backup-leader>Leading ETCD main container’s sidecar is the backup leader</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#independent-leader-election-between-backup-restore-sidecars>Independent leader election between backup-restore sidecars</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#history-compaction>History Compaction</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#defragmentation>Defragmentation</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-in-etcd-backup-restore>Work-flows in etcd-backup-restore</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-independent-of-leader-election-in-all-members>Work-flows independent of leader election in all members</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member>Work-flows only on the leading member</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#high-availability>High Availability</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#zonal-cluster---single-availability-zone>Zonal Cluster - Single Availability Zone</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-5>Alternative</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#regional-cluster---multiple-availability-zones>Regional Cluster - Multiple Availability Zones</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-6>Alternative</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#poddisruptionbudget>PodDisruptionBudget</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#rolling-updates-to-etcd-members>Rolling updates to etcd members</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#follow-up>Follow Up</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral-volumes>Ephemeral Volumes</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#shoot-control-plane-migration>Shoot Control-Plane Migration</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#performance-impact-of-multi-node-etcd-clusters>Performance impact of multi-node etcd clusters</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#metrics-dashboards-and-alerts>Metrics, Dashboards and Alerts</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#costs>Costs</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#future-work>Future Work</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#gardener-ring>Gardener Ring</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#autonomous-shoot-clusters>Autonomous Shoot Clusters</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#optimization-of-recovery-from-non-quorate-cluster-with-some-member-containing-valid-data>Optimization of recovery from non-quorate cluster with some member containing valid data</a></li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#optimization-of-rolling-updates-to-unhealthy-etcd-clusters>Optimization of rolling updates to unhealthy etcd clusters</a></li></ul></li></ul></li></ul><h2 id=goal>Goal</h2><ul><li>Enhance etcd-druid and etcd-backup-restore to support provisioning and management of multi-node etcd cluster instances within a single Kubernetes cluster.</li><li>The etcd CRD interface should be simple to use. It should preferably work with just setting the <code>spec.replicas</code> field to the desired value and should not require any more configuration in the CRD than currently required for the single-node etcd instances. The <code>spec.replicas</code> field is part of the <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource><code>scale</code> sub-resource</a> <a href=https://github.com/gardener/etcd-druid/blob/eaf04a2d0e6c7a4f2c8c220182b7a141aabfc70b/api/v1alpha1/etcd_types.go#L299>implementation</a> in <code>Etcd</code> CRD.</li><li>The single-node and multi-node scenarios must be automatically identified and managed by <code>etcd-druid</code> and <code>etcd-backup-restore</code>.</li><li>The etcd clusters (single-node or multi-node) managed by <code>etcd-druid</code> and <code>etcd-backup-restore</code> must automatically recover from failures (even quorum loss) and disaster (e.g. etcd member persistence/data loss) as much as possible.</li><li>It must be possible to dynamically scale an etcd cluster horizontally (even between single-node and multi-node scenarios) by simply scaling the <code>Etcd</code> scale sub-resource.</li><li>It must be possible to (optionally) schedule the individual members of an etcd clusters on different nodes or even infrastructure availability zones (within the hosting Kubernetes cluster).</li></ul><p>Though this proposal tries to cover most aspects related to single-node and multi-node etcd clusters, there are some more points that are not goals for this document but are still in the scope of either etcd-druid/etcd-backup-restore and/or gardener.
In such cases, a high-level description of how they can be <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#future-work>addressed in the future</a> are mentioned at the end of the document.</p><h2 id=background-and-motivation>Background and Motivation</h2><h3 id=single-node-etcd-cluster>Single-node etcd cluster</h3><p>At present, <code>etcd-druid</code> supports only single-node etcd cluster instances.
The advantages of this approach are given below.</p><ul><li>The problem domain is smaller.
There are no leader election and quorum related issues to be handled.
It is simpler to setup and manage a single-node etcd cluster.</li><li>Single-node etcd clusters instances have <a href=https://etcd.io/docs/v2/admin_guide/#optimal-cluster-size>less request latency</a> than multi-node etcd clusters because there is no requirement to replicate the changes to the other members before committing the changes.</li><li><code>etcd-druid</code> provisions etcd cluster instances as pods (actually as <code>statefulsets</code>) in a Kubernetes cluster and Kubernetes is quick (&lt;<code>20s</code>) to restart container/pods if they go down.</li><li>Also, <code>etcd-druid</code> is currently only used by gardener to provision etcd clusters to act as back-ends for Kubernetes control-planes and Kubernetes control-plane components (<code>kube-apiserver</code>, <code>kubelet</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code> etc.) can tolerate etcd going down and recover when it comes back up.</li><li>Single-node etcd clusters incur less cost (CPU, memory and storage)</li><li>It is easy to cut-off client requests if backups fail by using <a href=https://github.com/gardener/etcd-druid/blob/eaf04a2d0e6c7a4f2c8c220182b7a141aabfc70b/charts/etcd/templates/etcd-statefulset.yaml#L54-L62><code>readinessProbe</code> on the <code>etcd-backup-restore</code> healthz endpoint</a> to minimize the gap between the latest revision and the backup revision.</li></ul><p>The disadvantages of using single-node etcd clusters are given below.</p><ul><li>The <a href=https://github.com/gardener/etcd-backup-restore/blob/master/docs/proposals/design.md#workflow>database verification</a> step by <code>etcd-backup-restore</code> can introduce additional delays whenever etcd container/pod restarts (in total ~<code>20-25s</code>).
This can be much longer if a database restoration is required.
Especially, if there are incremental snapshots that need to be replayed (this can be mitigated by <a href=https://github.com/gardener/etcd-druid/issues/88>compacting the incremental snapshots in the background</a>).</li><li>Kubernetes control-plane components can go into <code>CrashloopBackoff</code> if etcd is down for some time. This is mitigated by the <a href=https://github.com/gardener/gardener/blob/9e4a809008fb122a6d02045adc08b9c98b5cd564/charts/seed-bootstrap/charts/dependency-watchdog/templates/endpoint-configmap.yaml#L29-L41>dependency-watchdog</a>.
But Kubernetes control-plane components require a lot of resources and create a lot of load on the etcd cluster and the apiserver when they come out of <code>CrashloopBackoff</code>.
Especially, in medium or large sized clusters (> <code>20</code> nodes).</li><li>Maintenance operations such as updates to etcd (and updates to <code>etcd-druid</code> of <code>etcd-backup-restore</code>), rolling updates to the nodes of the underlying Kubernetes cluster and vertical scaling of etcd pods are disruptive because they cause etcd pods to be restarted.
The vertical scaling of etcd pods is somewhat mitigated during scale down by doing it only during the target clusters&rsquo; <a href=https://github.com/gardener/gardener/blob/86aa30dfd095f7960ae50a81d2cee27c0d18408b/charts/seed-controlplane/charts/etcd/templates/etcd-hvpa.yaml#L53>maintenance window</a>.
But scale up is still disruptive.</li><li>We currently use some form of elastic storage (via <code>persistentvolumeclaims</code>) for storing which have some upper-bounds on the I/O latency and throughput. This can be potentially be a problem for large clusters (> <code>220</code> nodes).
Also, some cloud providers (e.g. Azure) take a long time to attach/detach volumes to and from machines which increases the down time to the Kubernetes components that depend on etcd.
It is difficult to use ephemeral/local storage (to achieve better latency/throughput as well as to circumvent volume attachment/detachment) for single-node etcd cluster instances.</li></ul><h3 id=multi-node-etcd-cluster>Multi-node etcd-cluster</h3><p>The advantages of introducing support for multi-node etcd clusters via <code>etcd-druid</code> are below.</p><ul><li>Multi-node etcd cluster is highly-available. It can tolerate disruption to individual etcd pods as long as the quorum is not lost (i.e. more than half the etcd member pods are healthy and ready).</li><li>Maintenance operations such as updates to etcd (and updates to <code>etcd-druid</code> of <code>etcd-backup-restore</code>), rolling updates to the nodes of the underlying Kubernetes cluster and vertical scaling of etcd pods can be done non-disruptively by <a href=https://kubernetes.io/docs/concepts/workloads/pods/disruptions/>respecting <code>poddisruptionbudgets</code></a> for the various multi-node etcd cluster instances hosted on that cluster.</li><li>Kubernetes control-plane components do not see any etcd cluster downtime unless quorum is lost (which is expected to be lot less frequent than current frequency of etcd container/pod restarts).</li><li>We can consider using ephemeral/local storage for multi-node etcd cluster instances because individual member restarts can afford to take time to restore from backup before (re)joining the etcd cluster because the remaining members serve the requests in the meantime.</li><li>High-availability across availability zones is also possible by specifying (anti)affinity for the etcd pods (possibly via <a href=https://github.com/gardener/kupid><code>kupid</code></a>).</li></ul><p>Some disadvantages of using multi-node etcd clusters due to which it might still be desirable, in some cases, to continue to use single-node etcd cluster instances in the gardener context are given below.</p><ul><li>Multi-node etcd cluster instances are more complex to manage.
The problem domain is larger including the following.<ul><li>Leader election</li><li>Quorum loss</li><li>Managing rolling changes</li><li>Backups to be taken from only the leading member.</li><li>More complex to cut-off client requests if backups fail to minimize the gap between the latest revision and the backup revision is under control.</li></ul></li><li>Multi-node etcd cluster instances incur more cost (CPU, memory and storage).</li></ul><h3 id=dynamic-multi-node-etcd-cluster>Dynamic multi-node etcd cluster</h3><p>Though it is <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#non-goal>not part of this proposal</a>, it is conceivable to convert a single-node etcd cluster into a multi-node etcd cluster temporarily to perform some disruptive operation (etcd, <code>etcd-backup-restore</code> or <code>etcd-druid</code> updates, etcd cluster vertical scaling and perhaps even node rollout) and convert it back to a single-node etcd cluster once the disruptive operation has been completed. This will necessarily still involve a down-time because scaling from a single-node etcd cluster to a three-node etcd cluster will involve etcd pod restarts, it is still probable that it can be managed with a shorter down time than we see at present for single-node etcd clusters (on the other hand, converting a three-node etcd cluster to five node etcd cluster can be non-disruptive).</p><p>This is <em>definitely not</em> to argue in favour of such a dynamic approach in all cases (eventually, if/when dynamic multi-node etcd clusters are supported). On the contrary, it makes sense to make use of <em>static</em> (fixed in size) multi-node etcd clusters for production scenarios because of the high-availability.</p><h2 id=prior-art>Prior Art</h2><h3 id=etcd-operator-from-coreos>ETCD Operator from CoreOS</h3><blockquote><p><a href=https://github.com/coreos/etcd-operator#etcd-operator>etcd operator</a></p><p><a href=https://github.com/coreos/etcd-operator#project-status-archived>Project status: archived</a></p><p>This project is no longer actively developed or maintained. The project exists here for historical reference. If you are interested in the future of the project and taking over stewardship, please contact <a href=mailto:etcd-dev@googlegroups.com>etcd-dev@googlegroups.com</a>.</p></blockquote><h3 id=etcdadm-from-kubernetes-sigs>etcdadm from kubernetes-sigs</h3><blockquote><p><a href=https://github.com/kubernetes-sigs/etcdadm#etcdadm>etcdadm</a> is a command-line tool for operating an etcd cluster. It makes it easy to create a new cluster, add a member to, or remove a member from an existing cluster. Its user experience is inspired by kubeadm.</p></blockquote><p>It is a tool more tailored for manual command-line based management of etcd clusters with no API&rsquo;s.
It also makes no assumptions about the underlying platform on which the etcd clusters are provisioned and hence, doesn&rsquo;t leverage any capabilities of Kubernetes.</p><h3 id=etcd-cluster-operator-from-improbable-engineering>Etcd Cluster Operator from Improbable-Engineering</h3><blockquote><p><a href=https://github.com/improbable-eng/etcd-cluster-operator>Etcd Cluster Operator</a></p><p>Etcd Cluster Operator is an Operator for automating the creation and management of etcd inside of Kubernetes. It provides a custom resource definition (CRD) based API to define etcd clusters with Kubernetes resources, and enable management with native Kubernetes tooling._</p></blockquote><p>Out of all the alternatives listed here, this one seems to be the only possible viable alternative.
Parts of its design/implementations are similar to some of the approaches mentioned in this proposal. However, we still don&rsquo;t propose to use it as -</p><ol><li>The project is still in early phase and is not mature enough to be consumed as is in productive scenarios of ours.</li><li>The resotration part is completely different which makes it difficult to adopt as-is and requries lot of re-work with the current restoration semantics with etcd-backup-restore making the usage counter-productive.</li></ol><h2 id=general-approach-to-etcd-cluster-management>General Approach to ETCD Cluster Management</h2><h3 id=bootstrapping>Bootstrapping</h3><p>There are three ways to bootstrap an etcd cluster which are <a href=https://etcd.io/docs/v3.4.0/op-guide/clustering/#static>static</a>, <a href=https://etcd.io/docs/v3.4.0/op-guide/clustering/#etcd-discovery>etcd discovery</a> and <a href=https://etcd.io/docs/v3.4.0/op-guide/clustering/#dns-discovery>DNS discovery</a>.
Out of these, the static way is the simplest (and probably faster to bootstrap the cluster) and has the least external dependencies.
Hence, it is preferred in this proposal.
But it requires that the initial (during bootstrapping) etcd cluster size (number of members) is already known before bootstrapping and that all of the members are already addressable (DNS,IP,TLS etc.).
Such information needs to be passed to the individual members during startup using the following static configuration.</p><ul><li>ETCD_INITIAL_CLUSTER<ul><li>The list of peer URLs including all the members. This must be the same as the advertised peer URLs configuration. This can also be passed as <code>initial-cluster</code> flag to etcd.</li></ul></li><li>ETCD_INITIAL_CLUSTER_STATE<ul><li>This should be set to <code>new</code> while bootstrapping an etcd cluster.</li></ul></li><li>ETCD_INITIAL_CLUSTER_TOKEN<ul><li>This is a token to distinguish the etcd cluster from any other etcd cluster in the same network.</li></ul></li></ul><h4 id=assumptions>Assumptions</h4><ul><li>ETCD_INITIAL_CLUSTER can use DNS instead of IP addresses. We need to verify this by deleting a pod (as against scaling down the statefulset) to ensure that the pod IP changes and see if the recreated pod (by the statefulset controller) re-joins the cluster automatically.</li><li>DNS for the individual members is known or computable. This is true in the case of etcd-druid setting up an etcd cluster using a single statefulset. But it may not necessarily be true in other cases (multiple statefulset per etcd cluster or deployments instead of statefulsets or in the case of etcd cluster with members distributed across more than one Kubernetes cluster.</li></ul><h3 id=adding-a-new-member-to-an-etcd-cluster>Adding a new member to an etcd cluster</h3><p>A <a href=https://etcd.io/docs/v3.4.0/op-guide/runtime-configuration/#add-a-new-member>new member can be added</a> to an existing etcd cluster instance using the following steps.</p><ol><li>If the latest backup snapshot exists, restore the member&rsquo;s etcd data to the latest backup snapshot. This can reduce the load on the leader to bring the new member up to date when it joins the cluster.<ol><li>If the latest backup snapshot doesn&rsquo;t exist or if the latest backup snapshot is not accessible (please see <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure>backup failure</a>) and if the cluster itself is quorate, then the new member can be started with an empty data. But this will will be suboptimal because the new member will fetch all the data from the leading member to get up-to-date.</li></ol></li><li>The cluster is informed that a new member is being added using the <a href=https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L40><code>MemberAdd</code> API</a> including information like the member name and its advertised peer URLs.</li><li>The new etcd member is then started with <code>ETCD_INITIAL_CLUSTER_STATE=existing</code> apart from other required configuration.</li></ol><p>This proposal recommends this approach.</p><h4 id=note>Note</h4><ul><li>If there are incremental snapshots (taken by <code>etcd-backup-restore</code>), they cannot be applied because that requires the member to be started in isolation without joining the cluster which is not possible.
This is acceptable if the amount of incremental snapshots are managed to be relatively small.
This adds one more reason to increase the priority of the issue of <a href=https://github.com/gardener/etcd-druid/issues/88>incremental snapshot compaction</a>.</li><li>There is a time window, between the <code>MemberAdd</code> call and the new member joining the cluster and getting up to date, where the cluster is <a href=https://etcd.io/docs/v3.3.12/learning/learner/#background>vulnerable to leader elections which could be disruptive</a>.</li></ul><h4 id=alternative>Alternative</h4><p>With <code>v3.4</code>, the new <a href=https://etcd.io/docs/v3.3.12/learning/learner/#raft-learner>raft learner approach</a> can be used to mitigate some of the possible disruptions mentioned <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note>above</a>.
Then the steps will be as follows.</p><ol><li>If the latest backup snapshot exists, restore the member&rsquo;s etcd data to the latest backup snapshot. This can reduce the load on the leader to bring the new member up to date when it joins the cluster.</li><li>The cluster is informed that a new member is being added using the <a href=https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L43><code>MemberAddAsLearner</code> API</a> including information like the member name and its advertised peer URLs.</li><li>The new etcd member is then started with <code>ETCD_INITIAL_CLUSTER_STATE=existing</code> apart from other required configuration.</li><li>Once the new member (learner) is up to date, it can be promoted to a full voting member by using the <a href=https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L52><code>MemberPromote</code> API</a></li></ol><p>This approach is new and involves more steps and is not recommended in this proposal.
It can be considered in future enhancements.</p><h3 id=managing-failures>Managing Failures</h3><p>A multi-node etcd cluster may face failures of <a href=https://etcd.io/docs/v3.1.12/op-guide/failures/>diffent kinds</a> during its life-cycle.
The actions that need to be taken to manage these failures depend on the failure mode.</p><h4 id=removing-an-existing-member-from-an-etcd-cluster>Removing an existing member from an etcd cluster</h4><p>If a member of an etcd cluster becomes unhealthy, it must be explicitly removed from the etcd cluster, as soon as possible.
This can be done by using the <a href=https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L46><code>MemberRemove</code> API</a>.
This ensures that only healthy members participate as voting members.</p><p>A member of an etcd cluster may be removed not just for managing failures but also for other reasons such as -</p><ul><li>The etcd cluster is being scaled down. I.e. the cluster size is being reduced</li><li>An existing member is being replaced by a new one for some reason (e.g. upgrades)</li></ul><p>If the majority of the members of the etcd cluster are healthy and the member that is unhealthy/being removed happens to be the <a href=https://etcd.io/docs/v3.1.12/op-guide/failures/#leader-failure>leader</a> at that moment then the etcd cluster will automatically elect a new leader.
But if only a minority of etcd clusters are healthy after removing the member then the the cluster will no longer be <a href=https://etcd.io/docs/v3.1.12/op-guide/failures/#majority-failure>quorate</a> and will stop accepting write requests.
Such an etcd cluster needs to be recovered via some kind of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>disaster-recovery</a>.</p><h4 id=restarting-an-existing-member-of-an-etcd-cluster>Restarting an existing member of an etcd cluster</h4><p>If the existing member of an etcd cluster restarts and retains an uncorrupted data directory after the restart, then it can simply re-join the cluster as an existing member without any API calls or configuration changes.
This is because the relevant metadata (including member ID and cluster ID) are <a href=https://etcd.io/docs/v2/admin_guide/#lifecycle>maintained in the write ahead logs</a>.
However, if it doesn&rsquo;t retain an uncorrupted data directory after the restart, then it must first be <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster>removed</a> and <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>added</a> as a new member.</p><h4 id=recovering-an-etcd-cluster-from-failure-of-majority-of-members>Recovering an etcd cluster from failure of majority of members</h4><p>If a majority of members of an etcd cluster fail but if they retain their uncorrupted data directory then they can be simply restarted and they will re-form the existing etcd cluster when they come up.
However, if they do not retain their uncorrupted data directory, then the etcd cluster must be <a href=https://etcd.io/docs/v3.4.0/op-guide/recovery/#restoring-a-cluster>recovered from latest snapshot in the backup</a>.
This is very similar to <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping>bootstrapping</a> with the additional initial step of restoring the latest snapshot in each of the members.
However, the same <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note>limitation</a> about incremental snapshots, as in the case of adding a new member, applies here.
But unlike in the case of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>adding a new member</a>, not applying incremental snapshots is not acceptable in the case of etcd cluster recovery.
Hence, if incremental snapshots are required to be applied, the etcd cluster must be <a href=https://etcd.io/docs/v3.4.0/op-guide/runtime-configuration/#restart-cluster-from-majority-failure>recovered</a> in the following steps.</p><ol><li>Restore a new single-member cluster using the latest snapshot.</li><li>Apply incremental snapshots on the single-member cluster.</li><li>Take a full snapshot which can now be used while adding the remaining members.</li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>Add</a> new members using the latest snapshot created in the step above.</li></ol><h2 id=kubernetes-context>Kubernetes Context</h2><ul><li>Users will provision an etcd cluster in a Kubernetes cluster by creating an etcd CRD resource instance.</li><li>A multi-node etcd cluster is indicated if the <code>spec.replicas</code> field is set to any value greater than 1. The etcd-druid will add validation to ensure that the <code>spec.replicas</code> value is an odd number according to the requirements of etcd.</li><li>The etcd-druid controller will provision a statefulset with the etcd main container and the etcd-backup-restore sidecar container. It will pass on the <code>spec.replicas</code> field from the etcd resource to the statefulset. It will also supply the right pre-computed configuration to both the containers.</li><li>The statefulset controller will create the pods based on the pod template in the statefulset spec and these individual pods will be the members that form the etcd cluster.</li></ul><p><img src=/__resources/01-multi-node-etcd_1afcbd.png alt="Component diagram"></p><p>This approach makes it possible to satisfy the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#assumption>assumption</a> that the DNS for the individual members of the etcd cluster must be known/computable.
This can be achieved by using a <code>headless</code> service (along with the statefulset) for each etcd cluster instance.
Then we can address individual pods/etcd members via the predictable DNS name of <code>&lt;statefulset_name>-{0|1|2|3|…|n}.&lt;headless_service_name></code> from within the Kubernetes namespace (or from outside the Kubernetes namespace by appending <code>.&lt;namespace>.svc.&lt;cluster_domain> suffix)</code>.
The etcd-druid controller can compute the above configurations automatically based on the <code>spec.replicas</code> in the etcd resource.</p><p>This proposal recommends this approach.</p><h4 id=alternative-1>Alternative</h4><p>One statefulset is used for each member (instead of one statefulset for all members).
While this approach gives a flexibility to have different pod specifications for the individual members, it makes managing the individual members (e.g. rolling updates) more complicated.
Hence, this approach is not recommended.</p><h2 id=etcd-configuration>ETCD Configuration</h2><p>As mentioned in the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#general-approach-to-etcd-cluster-management>general approach section</a>, there are differences in the configuration that needs to be passed to individual members of an etcd cluster in different scenarios such as <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping>bootstrapping</a>, <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>adding</a> a new member, <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster>removing</a> a member, <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>restarting</a> an existing member etc.
Managing such differences in configuration for individual pods of a statefulset is tricky in the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#kubernetes-context>recommended approach</a> of using a single statefulset to manage all the member pods of an etcd cluster.
This is because statefulset uses the same pod template for all its pods.</p><p>The recommendation is for <code>etcd-druid</code> to provision the base configuration template in a <code>ConfigMap</code> which is passed to all the pods via the pod template in the <code>StatefulSet</code>.
The <code>initialization</code> flow of <code>etcd-backup-restore</code> (which is invoked every time the etcd container is (re)started) is then enhanced to generate the customized etcd configuration for the corresponding member pod (in a shared <em>volume</em> between etcd and the backup-restore containers) based on the supplied template configuration.
This will require that <code>etcd-backup-restore</code> will have to have a mechanism to detect which scenario listed <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-configuration>above</a> applies during any given member container/pod restart.</p><h3 id=alternative-2>Alternative</h3><p>As mentioned <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-1>above</a>, one statefulset is used for each member of the etcd cluster.
Then different configuration (generated directly by <code>etcd-druid</code>) can be passed in the pod templates of the different statefulsets.
Though this approach is advantageous in the context of managing the different configuration, it is not recommended in this proposal because it makes the rest of the management (e.g. rolling updates) more complicated.</p><h2 id=data-persistence>Data Persistence</h2><p>The type of persistence used to store etcd data (including the member ID and cluster ID) has an impact on the steps that are needed to be taken when the member pods or containers (<a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster>minority</a> of them or <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>majority</a>) need to be recovered.</p><h3 id=persistent>Persistent</h3><p>Like the single-node case, <code>persistentvolumes</code> can be used to persist ETCD data for all the member pods. The individual member pods then get their own <code>persistentvolumes</code>.
The advantage is that individual members retain their member ID across pod restarts and even pod deletion/recreation across Kubernetes nodes.
This means that member pods that crash (or are unhealthy) can be <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>restarted</a> automatically (by configuring <code>livenessProbe</code>) and they will re-join the etcd cluster using their existing member ID without any need for explicit etcd cluster management).</p><p>The disadvantages of this approach are as follows.</p><ul><li>The number of persistentvolumes increases linearly with the cluster size which is a cost-related concern.</li><li>Network-mounted persistentvolumes might eventually become a performance bottleneck under heavy load for a latency-sensitive component like ETCD.</li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#single-node-etcd-cluster>Volume attach/detach issues</a> when associated with etcd cluster instances cause downtimes to the target shoot clusters that are backed by those etcd cluster instances.</li></ul><h3 id=ephemeral>Ephemeral</h3><p>The ephemeral volumes use-case is considered as an optimization and may be planned as a follow-up action.</p><h4 id=disk>Disk</h4><p>Ephemeral persistence can be achieved in Kubernetes by using either <a href=https://kubernetes.io/docs/concepts/storage/volumes/#emptydir><code>emptyDir</code></a> volumes or <a href=https://kubernetes.io/docs/concepts/storage/volumes/#local><code>local</code> persistentvolumes</a> to persist ETCD data.
The advantages of this approach are as follows.</p><ul><li>Potentially faster disk I/O.</li><li>The number of persistent volumes does not increase linearly with the cluster size (at least not technically).</li><li>Issues related volume attachment/detachment can be avoided.</li></ul><p>The main disadvantage of using ephemeral persistence is that the individual members may retain their identity and data across container restarts but not across pod deletion/recreation across Kubernetes nodes. If the data is lost then on restart of the member pod, the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>older member (represented by the container) has to be removed and a new member has to be added</a>.</p><p>Using <code>emptyDir</code> ephemeral persistence has the disadvantage that the volume doesn&rsquo;t have its own identity.
So, if the member pod is recreated but scheduled on the same node as before then it will not retain the identity as the persistence is lost.
But it has the advantage that scheduling of pods is unencumbered especially during pod recreation as they are free to be scheduled anywhere.</p><p>Using <code>local</code> persistentvolumes has the advantage that the volume has its own indentity and hence, a recreated member pod will retain its identity if scheduled on the same node.
But it has the disadvantage of tying down the member pod to a node which is a problem if the node becomes unhealthy requiring etcd druid to take additional actions (such as deleting the local persistent volume).</p><p>Based on these constraints, if ephemeral persistence is opted for, it is recommended to use <code>emptyDir</code> ephemeral persistence.</p><h4 id=in-memory>In-memory</h4><p>In-memory ephemeral persistence can be achieved in Kubernetes by using <code>emptyDir</code> with <a href=https://kubernetes.io/docs/concepts/storage/volumes/#emptydir><code>medium: Memory</code></a>.
In this case, a <code>tmpfs</code> (RAM-backed file-system) volume will be used.
In addition to the advantages of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral persistence</a>, this approach can achieve the fastest possible <em>disk I/O</em>.
Similarly, in addition to the disadvantages of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral persistence</a>, in-memory persistence has the following additional disadvantages.</p><ul><li>More memory required for the individual member pods.</li><li>Individual members may not at all retain their data and identity across container restarts let alone across pod restarts/deletion/recreation across Kubernetes nodes.
I.e. every time an etcd container restarts, <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>the old member (represented by the container) will have to be removed and a new member has to be added</a>.</li></ul><h3 id=how-to-detect-if-valid-metadata-exists-in-an-etcd-member>How to detect if valid metadata exists in an etcd member</h3><p>Since the likelyhood of a member not having valid metadata in the WAL files is much more likely in the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral</a> persistence scenario, one option is to pass the information that ephemeral persistence is being used to the <code>etcd-backup-restore</code> sidecar (say, via command-line flags or environment variables).</p><p>But in principle, it might be better to determine this from the WAL files directly so that the possibility of corrupted WAL files also gets handled correctly.
To do this, the <a href=https://github.com/etcd-io/etcd/tree/main/server/storage/wal>wal</a> package has <a href=https://github.com/etcd-io/etcd/blob/57a092b45d0eae6c9e600e62513ffcd2f1f25a92/server/wal/wal.go#L324-L326>some</a> <a href=https://github.com/etcd-io/etcd/blob/57a092b45d0eae6c9e600e62513ffcd2f1f25a92/server/wal/wal.go#L429-L548>functions</a> that might be useful.</p><h4 id=recommendation>Recommendation</h4><p>It might be possible that using the <a href=https://github.com/etcd-io/etcd/tree/main/server/storage/wal>wal</a> package for verifying if valid metadata exists might be performance intensive.
So, the performance impact needs to be measured.
If the performance impact is acceptable (both in terms of resource usage and time), it is recommended to use this way to verify if the member contains valid metadata.
Otherwise, alternatives such as a simple check that WAL folder exists coupled with the static information about use of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent>persistent</a> or <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral</a> storage might be considered.</p><h3 id=how-to-detect-if-valid-data-exists-in-an-etcd-member>How to detect if valid data exists in an etcd member</h3><p>The <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#decision-table-for-etcd-backup-restore-during-initialization>initialization sequence</a> in <code>etcd-backup-restore</code> already includes <a href=https://github.com/gardener/etcd-backup-restore/blob/c98f76c7c55f7d1039687cc293536d7caf893ba5/pkg/initializer/validator/datavalidator.go#L78-L94>database verification</a>.
This would suffice to determine if the member has valid data.</p><h3 id=recommendation-1>Recommendation</h3><p>Though <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral</a> persistence has performance and logistics advantages,
it is recommended to start with <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent>persistent</a> data for the member pods.
In addition to the reasons and concerns listed above, there is also the additional concern that in case of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure>backup failure</a>, the risk of additional data loss is a bit higher if ephemeral persistence is used (simultaneous quoram loss is sufficient) when compared to persistent storage (simultaenous quorum loss with majority persistence loss is needed).
The risk might still be acceptable but the idea is to gain experience about how frequently member containers/pods get restarted/recreated, how frequently leader election happens among members of an etcd cluster and how frequently etcd clusters lose quorum.
Based on this experience, we can move towards using <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral</a> (perhaps even <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#in-memory>in-memory</a>) persistence for the member pods.</p><h2 id=separating-peer-and-client-traffic>Separating peer and client traffic</h2><p>The current single-node ETCD cluster implementation in <code>etcd-druid</code> and <code>etcd-backup-restore</code> uses a single <code>service</code> object to act as the entry point for the client traffic.
There is no separation or distinction between the client and peer traffic because there is not much benefit to be had by making that distinction.</p><p>In the multi-node ETCD cluster scenario, it makes sense to distinguish between and separate the peer and client traffic.
This can be done by using two <code>services</code>.</p><ul><li>peer<ul><li>To be used for peer communication. This could be a <code>headless</code> service.</li></ul></li><li>client<ul><li>To be used for client communication. This could be a normal <code>ClusterIP</code> service like it is in the single-node case.</li></ul></li></ul><p>The main advantage of this approach is that it makes it possible (if needed) to allow only peer to peer communication while blocking client communication. Such a thing might be required during some phases of some maintenance tasks (manual or automated).</p><h3 id=cutting-off-client-requests>Cutting off client requests</h3><p>At present, in the single-node ETCD instances, etcd-druid configures the readinessProbe of the etcd main container to probe the healthz endpoint of the etcd-backup-restore sidecar which considers the status of the latest backup upload in addition to the regular checks about etcd and the side car being up and healthy. This has the effect of setting the etcd main container (and hence the etcd pod) as not ready if the latest backup upload failed. This results in the endpoints controller removing the pod IP address from the endpoints list for the service which eventually cuts off ingress traffic coming into the etcd pod via the etcd client service. The rationale for this is to fail early when the backup upload fails rather than continuing to serve requests while the gap between the last backup and the current data increases which might lead to unacceptably large amount of data loss if disaster strikes.</p><p>This approach will not work in the multi-node scenario because we need the individual member pods to be able to talk to each other to maintain the cluster quorum when backup upload fails but need to cut off only client ingress traffic.</p><p>It is recommended to separate the backup health condition tracking taking appropriate remedial actions.
With that, the backup health condition tracking is now separated to the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions><code>BackupReady</code> condition</a> in the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status><code>Etcd</code> resource <code>status</code></a> and the cutting off of client traffic (which could now be done for more reasons than failed backups) can be achieved in a different way described <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector>below</a>.</p><h4 id=manipulating-client-service-podselector>Manipulating Client Service podSelector</h4><p>The client traffic can be cut off by updating (manually or automatically by some component) the <code>podSelector</code> of the client service to add an additional label (say, unhealthy or disabled) such that the <code>podSelector</code> no longer matches the member pods created by the statefulset.
This will result in the client ingress traffic being cut off.
The peer service is left unmodified so that peer communication is always possible.</p><h2 id=health-check>Health Check</h2><p>The etcd main container and the etcd-backup-restore sidecar containers will be configured with livenessProbe and readinessProbe which will indicate the health of the containers and effectively the corresponding ETCD cluster member pod.</p><h3 id=backup-failure>Backup Failure</h3><p>As described <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#cutting-off-client-requests>above</a> using <code>readinessProbe</code> failures based on latest backup failure is not viable in the multi-node ETCD scenario.</p><p>Though cutting off traffic by <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector>manipulating client <code>service</code> <code>podSelector</code></a> is workable, it may not be desirable.</p><p>It is recommended that on backup failure, the leading <code>etcd-backup-restore</code> sidecar (the one that is responsible for taking backups at that point in time, as explained in the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup>backup section below</a>, updates the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions><code>BackupReady</code> condition</a> in the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status><code>Etcd</code> status</a> and raises a high priority alert to the landscape operators but <em><em>does not</em></em> cut off the client traffic.</p><p>The reasoning behind this decision to not cut off the client traffic on backup failure is to allow the Kubernetes cluster&rsquo;s control plane (which relies on the ETCD cluster) to keep functioning as long as possible and to avoid bringing down the control-plane due to a missed backup.</p><p>The risk of this approach is that with a cascaded sequence of failures (on top of the backup failure), there is a chance of more data loss than the frequency of backup would otherwise indicate.</p><p>To be precise, the risk of such an additional data loss manifests only when backup failure as well as a special case of quorum loss (majority of the members are not ready) happen in such a way that the ETCD cluster needs to be re-bootstrapped from the backup.
As described <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>here</a>, re-bootstrapping the ETCD cluster requires restoration from the latest backup only when a majority of members no longer have uncorrupted data persistence.</p><p>If <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent>persistent storage</a> is used, this will happen only when backup failure as well as a majority of the disks/volumes backing the ETCD cluster members fail simultaneously.
This would indeed be rare and might be an acceptable risk.</p><p>If <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral storage</a> is used (especially, <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#in-memory>in-memory</a>), the data loss will happen if a majority of the ETCD cluster members become <code>NotReady</code> (requiring a pod restart) at the same time as the backup failure.
This may not be as rare as majority members&rsquo; disk/volume failure.
The risk can be somewhat mitigated at least for planned maintenance operations by postponing potentially disruptive maintenance operations when <code>BackupReady</code> condition is <code>false</code> (vertical scaling, rolling updates, evictions due to node roll-outs).</p><p>But in practice (when <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral>ephemeral storage</a> is used), the current proposal suggests restoring from the latest full backup even when a minority of ETCD members (even a single pod) <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>restart</a> both to speed up the process of the new member catching up to the latest revision but also to avoid load on the leading member which needs to supply the data to bring the new member up-to-date.
But as described <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>here</a>, in case of a minority member failure while using ephemeral storage, it is possible to restart the new member with empty data and let it fetch all the data from the leading member (only if backup is not accessible).
Though this is suboptimal, it is workable given the constraints and conditions.
With this, the risk of additional data loss in the case of ephemeral storage is only if backup failure as well as quorum loss happens.
While this is still less rare than the risk of additional data loss in case of persistent storage, the risk might be tolerable. Provided the risk of quorum loss is not too high. This needs to be monitored/evaluated before opting for ephemeral storage.</p><p>Given these constraints, it is better to dynamically avoid/postpone some potentially disruptive operations when <code>BackupReady</code> condition is <code>false</code>.
This has the effect of allowing <code>n/2</code> members to be evicted when the backups are healthy and completely disabling evictions when backups are not healthy.</p><ol><li>Skip/postpone potentially disruptive maintenance operations (listed below) when the <code>BackupReady</code> condition is <code>false</code>.</li><li>Vertical scaling.</li><li>Rolling updates, Basically, any updates to the <code>StatefulSet</code> spec which includes vertical scaling.</li><li>Dynamically toggle the <code>minAvailable</code> field of the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#poddisruptionbudget><code>PodDisruptionBudget</code></a> between <code>n/2 + 1</code> and <code>n</code> (where <code>n</code> is the ETCD desired cluster size) whenever the <code>BackupReady</code> condition toggles between <code>true</code> and <code>false</code>.</li></ol><p>This will mean that <code>etcd-backup-restore</code> becomes Kubernetes-aware. But there might be reasons for making <code>etcd-backup-restore</code> Kubernetes-aware anyway (e.g. to update the <code>etcd</code> resource <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status>status</a> with latest full snapshot details).
This enhancement should keep <code>etcd-backup-restore</code> backward compatible.
I.e. it should be possible to use <code>etcd-backup-restore</code> Kubernetes-unaware as before this proposal.
This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as <code>--enable-client-service-updates</code> which can be defaulted to <code>false</code> for backward compatibility).</p><h5 id=alternative-3>Alternative</h5><p>The alternative is for <code>etcd-druid</code> to implement the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector>above functionality</a>.</p><p>But <code>etcd-druid</code> is centrally deployed in the host Kubernetes cluster and cannot scale well horizontally.
So, it can potentially be a bottleneck if it is involved in regular health check mechanism for all the etcd clusters it manages.
Also, the recommended approach above is more robust because it can work even if <code>etcd-druid</code> is down when the backup upload of a particular etcd cluster fails.</p><h2 id=status>Status</h2><p>It is desirable (for the <code>etcd-druid</code> and landscape administrators/operators) to maintain/expose status of the etcd cluster instances in the <code>status</code> sub-resource of the <code>Etcd</code> CRD.
The proposed structure for maintaining the status is as shown in the example below.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: druid.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Etcd
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: etcd-main
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 3
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>  - type: Ready                 <span style=color:green># Condition type for the readiness of the ETCD cluster</span>
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>              <span style=color:green># Indicates of the ETCD Cluster is ready or not</span>
</span></span><span style=display:flex><span>    lastHeartbeatTime:          <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    lastTransitionTime:         <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    reason: Quorate             <span style=color:green># Quorate|QuorumLost</span>
</span></span><span style=display:flex><span>  - type: AllMembersReady       <span style=color:green># Condition type for the readiness of all the member of the ETCD cluster</span>
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>              <span style=color:green># Indicates if all the members of the ETCD Cluster are ready</span>
</span></span><span style=display:flex><span>    lastHeartbeatTime:          <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    lastTransitionTime:         <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    reason: AllMembersReady     <span style=color:green># AllMembersReady|NotAllMembersReady</span>
</span></span><span style=display:flex><span>  - type: BackupReady           <span style=color:green># Condition type for the readiness of the backup of the ETCD cluster</span>
</span></span><span style=display:flex><span>    status: <span style=color:#a31515>&#34;True&#34;</span>              <span style=color:green># Indicates if the backup of the ETCD cluster is ready</span>
</span></span><span style=display:flex><span>    lastHeartbeatTime:          <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    lastTransitionTime:         <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    reason: FullBackupSucceeded <span style=color:green># FullBackupSucceeded|IncrementalBackupSucceeded|FullBackupFailed|IncrementalBackupFailed</span>
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  clusterSize: 3
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  replicas: 3
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  members:
</span></span><span style=display:flex><span>  - name: etcd-main-0          <span style=color:green># member pod name</span>
</span></span><span style=display:flex><span>    id: 272e204152             <span style=color:green># member Id</span>
</span></span><span style=display:flex><span>    role: Leader               <span style=color:green># Member|Leader</span>
</span></span><span style=display:flex><span>    status: Ready              <span style=color:green># Ready|NotReady|Unknown</span>
</span></span><span style=display:flex><span>    lastTransitionTime:        <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    reason: LeaseSucceeded     <span style=color:green># LeaseSucceeded|LeaseExpired|UnknownGracePeriodExceeded|PodNotRead</span>
</span></span><span style=display:flex><span>  - name: etcd-main-1          <span style=color:green># member pod name</span>
</span></span><span style=display:flex><span>    id: 272e204153             <span style=color:green># member Id</span>
</span></span><span style=display:flex><span>    role: Member               <span style=color:green># Member|Leader</span>
</span></span><span style=display:flex><span>    status: Ready              <span style=color:green># Ready|NotReady|Unknown</span>
</span></span><span style=display:flex><span>    lastTransitionTime:        <span style=color:#a31515>&#34;2020-11-10T12:48:01Z&#34;</span>
</span></span><span style=display:flex><span>    reason: LeaseSucceeded     <span style=color:green># LeaseSucceeded|LeaseExpired|UnknownGracePeriodExceeded|PodNotRead</span>
</span></span></code></pre></div><p>This proposal recommends that <code>etcd-druid</code> (preferrably, the <code>custodian</code> controller in <code>etcd-druid</code>) maintains most of the information in the <code>status</code> of the <code>Etcd</code> resources described above.</p><p>One exception to this is the <code>BackupReady</code> condition which is recommended to be maintained by the <em>leading</em> <code>etcd-backup-restore</code> sidecar container.
This will mean that <code>etcd-backup-restore</code> becomes Kubernetes-aware. But there are other reasons for making <code>etcd-backup-restore</code> Kubernetes-aware anyway (e.g. to <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#health-check>maintain health conditions</a>).
This enhancement should keep <code>etcd-backup-restore</code> backward compatible.
But it should be possible to use <code>etcd-backup-restore</code> Kubernetes-unaware as before this proposal. This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as <code>--enable-etcd-status-updates</code> which can be defaulted to <code>false</code> for backward compatibility).</p><h3 id=members>Members</h3><p>The <code>members</code> section of the status is intended to be maintained by <code>etcd-druid</code> (preferraby, the <code>custodian</code> controller of <code>etcd-druid</code>) based on the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases><code>leases</code> of the individual members</a>.</p><h4 id=note-1>Note</h4><p>An earlier design in this proposal was for the individual <code>etcd-backup-restore</code> sidecars to update the corresponding <code>status.members</code> entries themselves. But this was redesigned to use <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>member <code>leases</code></a> to avoid conflicts rising from frequent updates and the limitations in the support for <a href=https://kubernetes.io/docs/reference/using-api/server-side-apply/>Server-Side Apply</a> in some versions of Kubernetes.</p><p>The <code>spec.holderIdentity</code> field in the <code>leases</code> is used to communicate the ETCD member <code>id</code> and <code>role</code> between the <code>etcd-backup-restore</code> sidecars and <code>etcd-druid</code>.</p><h4 id=member-name-as-the-key>Member name as the key</h4><p>In an ETCD cluster, the member <code>id</code> is the <a href=https://etcd.io/docs/v3.4/dev-guide/api_reference_v3/#message-member-etcdserveretcdserverpbrpcproto>unique identifier for a member</a>.
However, this proposal recommends using a <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#kubernetes-context>single <code>StatefulSet</code></a> whose pods form the members of the ETCD cluster and <code>Pods</code> of a <code>StatefulSet</code> have <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#ordinal-index>uniquely indexed names</a> as well as <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-network-id>uniquely addressible DNS</a>.</p><p>This proposal recommends that the <code>name</code> of the member (which is the same as the name of the member <code>Pod</code>) be used as the unique key to identify a member in the <code>members</code> array.
This can minimise the need to cleanup superfluous entries in the <code>members</code> array after the member pods are gone to some extent because the replacement pods for any member will share the same <code>name</code> and will overwrite the entry with a <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>possibly new</a> member <code>id</code>.</p><p>There is still the possibility of not only <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status>superfluous entries in the <code>members</code> array</a> but also <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member>superfluous <code>members</code> in the ETCD cluster</a> for which there is no corresponding pod in the <code>StatefulSet</code> anymore.</p><p>For example, if an ETCD cluster is scaled up from <code>3</code> to <code>5</code> and the new members were failing constantly due to insufficient resources and then if the ETCD client is scaled back down to <code>3</code> and failing member pods may not have the chance to clean up their <code>member</code> entries (from the <code>members</code> array as well as from the ETCD cluster) leading to superfluous members in the cluster that may have adverse effect on quorum of the cluster.</p><p>Hence, the superfluous entries in both <code>members</code> array as well as the ETCD cluster need to be <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12>cleaned up</a> <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member>as appropriate</a>.</p><h4 id=member-leases>Member Leases</h4><p>One <a href=https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/>Kubernetes <code>lease</code> object</a> per desired ETCD member is maintained by <code>etcd-druid</code> (preferrably, the <code>custodian</code> controller in <code>etcd-druid</code>).
The <code>lease</code> objects will be created in the same <code>namespace</code> as their owning <code>Etcd</code> object and will have the same <code>name</code> as the member to which they correspond (which, in turn would be the same as <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-name-as-the-key>the <code>pod</code> name in which the member ETCD process runs</a>).</p><p>The <code>lease</code> objects are created and deleted only by <code>etcd-druid</code> but are continually renewed within the <code>leaseDurationSeconds</code> by the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-independent-of-leader-election-in-all-members>individual <code>etcd-backup-restore</code> sidecars</a> (corresponding to their members) if the the corresponding ETCD member is ready and is part of the ETCD cluster.</p><p>This will mean that <code>etcd-backup-restore</code> becomes Kubernetes-aware. But there are other reasons for making <code>etcd-backup-restore</code> Kubernetes-aware anyway (e.g. to <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#health-check>maintain health conditions</a>).
This enhancement should keep <code>etcd-backup-restore</code> backward compatible.
But it should be possible to use <code>etcd-backup-restore</code> Kubernetes-unaware as before this proposal. This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as <code>--enable-etcd-lease-renewal</code> which can be defaulted to <code>false</code> for backward compatibility).</p><p>A <code>member</code> entry in the <code>Etcd</code> resource <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status><code>status</code></a> would be marked as <code>Ready</code> (with <code>reason: LeaseSucceeded</code>) if the corresponding <code>pod</code> is ready and the corresponding <code>lease</code> has not yet expired.
The <code>member</code> entry would be marked as <code>NotReady</code> if the corresponding <code>pod</code> is not ready (with reason <code>PodNotReady</code>) or as <code>Unknown</code> if the corresponding <code>lease</code> has expired (with <code>reason: LeaseExpired</code>).</p><p>While renewing the lease, the <code>etcd-backup-restore</code> sidecars also maintain the ETCD member <code>id</code> and their <code>role</code> (<code>Leader</code> or <code>Member</code>) separated by <code>:</code> in the <code>spec.holderIdentity</code> field of the corresponding <code>lease</code> object since this information is only available to the <code>ETCD</code> member processes and the <code>etcd-backup-restore</code> sidecars (e.g. <code>272e204152:Leader</code> or <code>272e204153:Member</code>).
When the <code>lease</code> objects are created by <code>etcd-druid</code>, the <code>spec.holderIdentity</code> field would be empty.</p><p>The value in <code>spec.holderIdentity</code> in the <code>leases</code> is parsed and copied onto the <code>id</code> and <code>role</code> fields of the corresponding <code>status.members</code> by <code>etcd-druid</code>.</p><h3 id=conditions>Conditions</h3><p>The <code>conditions</code> section in the status describe the overall condition of the ETCD cluster.
The condition type <code>Ready</code> indicates if the ETCD cluster as a whole is ready to serve requests (i.e. the cluster is quorate) even though some minority of the members are not ready.
The condition type <code>AllMembersReady</code> indicates of all the members of the ETCD cluster are ready.
The distinction between these conditions could be significant for both external consumers of the status as well as <code>etcd-druid</code> itself.
Some maintenance operations might be safe to do (e.g. rolling updates) only when all members of the cluster are ready.
The condition type <code>BackupReady</code> indicates of the most recent backup upload (full or incremental) succeeded.
This information also might be significant because some maintenance operations might be safe to do (e.g. anything that involves re-bootstrapping the ETCD cluster) only when backup is ready.</p><p>The <code>Ready</code> and <code>AllMembersReady</code> conditions can be maintained by <code>etcd-druid</code> based on the status in the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#members><code>members</code> section</a>.
The <code>BackupReady</code> condition will be maintained by the leading <code>etcd-backup-restore</code> sidecar that is in charge of taking backups.</p><p>More condition types could be introduced in the future if specific purposes arise.</p><h3 id=clustersize>ClusterSize</h3><p>The <code>clusterSize</code> field contains the current size of the ETCD cluster. It will be actively kept up-to-date by <code>etcd-druid</code> in all scenarios.</p><ul><li>Before <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping>bootstrapping</a> the ETCD cluster (during cluster creation or later bootstrapping because of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-9>quorum failure</a>), <code>etcd-druid</code> will clear the <code>status.members</code> array and set <code>status.clusterSize</code> to be equal to <code>spec.replicas</code>.</li><li>While the ETCD cluster is quorate, <code>etcd-druid</code> will actively set <code>status.clusterSize</code> to be equal to length of the <code>status.members</code> whenever the length of the array changes (say, due to scaling of the ETCD cluster).</li></ul><p>Given that <code>clusterSize</code> reliably represents the size of the ETCD cluster, it can be used to calculate the <code>Ready</code> <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions>condition</a>.</p><h3 id=alternative-4>Alternative</h3><p>The alternative is for <code>etcd-druid</code> to maintain the status in the <code>Etcd</code> status sub-resource.
But <code>etcd-druid</code> is centrally deployed in the host Kubernetes cluster and cannot scale well horizontally.
So, it can potentially be a bottleneck if it is involved in regular health check mechanism for all the etcd clusters it manages.
Also, the recommended approach above is more robust because it can work even if <code>etcd-druid</code> is down when the backup upload of a particular etcd cluster fails.</p><h2 id=decision-table-for-etcd-druid-based-on-the-status>Decision table for etcd-druid based on the status</h2><p>The following decision table describes the various criteria <code>etcd-druid</code> takes into consideration to determine the different etcd cluster management scenarios and the corresponding reconciliation actions it must take.
The general principle is to detect the scenario and take the minimum action to move the cluster along the path to good health.
The path from any one scenario to a state of good health will typically involve going through multiple reconciliation actions which probably take the cluster through many other cluster management scenarios.
Especially, it is proposed that individual members auto-heal where possible, even in the case of the failure of a majority of members of the etcd cluster and that <code>etcd-druid</code> takes action only if the auto-healing doesn&rsquo;t happen for a configured period of time.</p><h3 id=1-pink-of-health>1. Pink of health</h3><h4 id=observed-state>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: <code>n</code></li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: <code>n</code></li><li>Ready: <code>n</code></li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>0</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>0</code></li><li>Members with expired <code>lease</code>: <code>0</code></li></ul></li><li>conditions:<ul><li>Ready: <code>true</code></li><li>AllMembersReady: <code>true</code></li><li>BackupReady: <code>true</code></li></ul></li></ul></li></ul><h4 id=recommended-action>Recommended Action</h4><p>Nothing to do</p><h3 id=2-member-status-is-out-of-sync-with-their-leases>2. Member status is out of sync with their leases</h3><h4 id=observed-state-1>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: <code>n</code></li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: <code>n</code></li><li>Ready: <code>n</code></li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>r</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>0</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>0</code></li><li>Members with expired <code>lease</code>: <code>l</code></li></ul></li><li>conditions:<ul><li>Ready: <code>true</code></li><li>AllMembersReady: <code>true</code></li><li>BackupReady: <code>true</code></li></ul></li></ul></li></ul><h4 id=recommended-action-1>Recommended Action</h4><p>Mark the <code>l</code> members corresponding to the expired <code>leases</code> as <code>Unknown</code> with reason <code>LeaseExpired</code> and with <code>id</code> populated from <code>spec.holderIdentity</code> of the <code>lease</code> if they are not already updated so.</p><p>Mark the <code>n - l</code> members corresponding to the active <code>leases</code> as <code>Ready</code> with reason <code>LeaseSucceeded</code> and with <code>id</code> populated from <code>spec.holderIdentity</code> of the <code>lease</code> if they are not already updated so.</p><p>Please refer <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>here</a> for more details.</p><h3 id=3-all-members-are-ready-but-allmembersready-condition-is-stale>3. All members are <code>Ready</code> but <code>AllMembersReady</code> condition is stale</h3><h4 id=observed-state-2>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: N/A</li><li>Current: N/A</li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: <code>n</code></li><li>Ready: N/A</li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>0</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>0</code></li><li>Members with expired <code>lease</code>: <code>0</code></li></ul></li><li>conditions:<ul><li>Ready: N/A</li><li>AllMembersReady: false</li><li>BackupReady: N/A</li></ul></li></ul></li></ul><h4 id=recommended-action-2>Recommended Action</h4><p>Mark the status condition type <code>AllMembersReady</code> to <code>true</code>.</p><h3 id=4-not-all-members-are-ready-but-allmembersready-condition-is-stale>4. Not all members are <code>Ready</code> but <code>AllMembersReady</code> condition is stale</h3><h4 id=observed-state-3>Observed state</h4><ul><li><p>Cluster Size</p><ul><li>Desired: N/A</li><li>Current: N/A</li></ul></li><li><p><code>StatefulSet</code> replicas</p><ul><li>Desired: <code>n</code></li><li>Ready: N/A</li></ul></li><li><p><code>Etcd</code> status</p><ul><li>members<ul><li>Total: N/A</li><li>Ready: <code>r</code> where <code>0 &lt;= r &lt; n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>nr</code> where <code>0 &lt; nr &lt; n</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>u</code> where <code>0 &lt; u &lt; n</code></li><li>Members with expired <code>lease</code>: <code>h</code> where <code>0 &lt; h &lt; n</code></li></ul></li><li>conditions:<ul><li>Ready: N/A</li><li>AllMembersReady: true</li><li>BackupReady: N/A</li></ul></li></ul><p>where <code>(nr + u + h) > 0</code> or <code>r &lt; n</code></p></li></ul><h4 id=recommended-action-3>Recommended Action</h4><p>Mark the status condition type <code>AllMembersReady</code> to <code>false</code>.</p><h3 id=5-majority-members-are-ready-but-ready-condition-is-stale>5. Majority members are <code>Ready</code> but <code>Ready</code> condition is stale</h3><h4 id=observed-state-4>Observed state</h4><ul><li><p>Cluster Size</p><ul><li>Desired: N/A</li><li>Current: N/A</li></ul></li><li><p><code>StatefulSet</code> replicas</p><ul><li>Desired: <code>n</code></li><li>Ready: N/A</li></ul></li><li><p><code>Etcd</code> status</p><ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>r</code> where <code>r > n/2</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>nr</code> where <code>0 &lt; nr &lt; n/2</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>u</code> where <code>0 &lt; u &lt; n/2</code></li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions:<ul><li>Ready: <code>false</code></li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul><p>where <code>0 &lt; (nr + u + h) &lt; n/2</code></p></li></ul><h4 id=recommended-action-4>Recommended Action</h4><p>Mark the status condition type <code>Ready</code> to <code>true</code>.</p><h3 id=6-majority-members-are-notready-but-ready-condition-is-stale>6. Majority members are <code>NotReady</code> but <code>Ready</code> condition is stale</h3><h4 id=observed-state-5>Observed state</h4><ul><li><p>Cluster Size</p><ul><li>Desired: N/A</li><li>Current: N/A</li></ul></li><li><p><code>StatefulSet</code> replicas</p><ul><li>Desired: <code>n</code></li><li>Ready: N/A</li></ul></li><li><p><code>Etcd</code> status</p><ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>r</code> where <code>0 &lt; r &lt; n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>nr</code> where <code>0 &lt; nr &lt; n</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>u</code> where <code>0 &lt; u &lt; n</code></li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions:<ul><li>Ready: <code>true</code></li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul><p>where <code>(nr + u + h) > n/2</code> or <code>r &lt; n/2</code></p></li></ul><h4 id=recommended-action-5>Recommended Action</h4><p>Mark the status condition type <code>Ready</code> to <code>false</code>.</p><h3 id=7-some-members-have-been-in-unknown-status-for-a-while>7. Some members have been in <code>Unknown</code> status for a while</h3><h4 id=observed-state-6>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: N/A</li><li>Ready: N/A</li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: N/A</li><li>Ready: N/A</li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: N/A</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>u</code> where <code>u &lt;= n</code></li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions:<ul><li>Ready: N/A</li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul></li></ul><h4 id=recommended-action-6>Recommended Action</h4><p>Mark the <code>u</code> members as <code>NotReady</code> in <code>Etcd</code> status with <code>reason: UnknownGracePeriodExceeded</code>.</p><h3 id=8-some-member-pods-are-not-ready-but-have-not-had-the-chance-to-update-their-status>8. Some member pods are not <code>Ready</code> but have not had the chance to update their status</h3><h4 id=observed-state-7>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: <code>n</code></li><li>Ready: <code>s</code> where <code>s &lt; n</code></li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: N/A</li><li>Ready: N/A</li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: N/A</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: N/A</li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions:<ul><li>Ready: N/A</li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul></li></ul><h4 id=recommended-action-7>Recommended Action</h4><p>Mark the <code>n - s</code> members (corresponding to the pods that are not <code>Ready</code>) as <code>NotReady</code> in <code>Etcd</code> status with <code>reason: PodNotReady</code></p><h3 id=9-quorate-cluster-with-a-minority-of-members-notready>9. Quorate cluster with a minority of members <code>NotReady</code></h3><h4 id=observed-state-8>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: N/A</li><li>Ready: N/A</li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>n - f</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>f</code> where <code>f &lt; n/2</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: <code>0</code></li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions:<ul><li>Ready: true</li><li>AllMembersReady: false</li><li>BackupReady: true</li></ul></li></ul></li></ul><h4 id=recommended-action-8>Recommended Action</h4><p>Delete the <code>f</code> <code>NotReady</code> member pods to force restart of the pods if they do not automatically restart via failed <code>livenessProbe</code>. The expectation is that they will either re-join the cluster as an existing member or remove themselves and join as new members on restart of the container or pod and <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>renew their <code>leases</code></a>.</p><h3 id=10-quorum-lost-with-a-majority-of-members-notready>10. Quorum lost with a majority of members <code>NotReady</code></h3><h4 id=observed-state-9>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: N/A</li><li>Ready: N/A</li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>n - f</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: <code>f</code> where <code>f >= n/2</code></li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: N/A</li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions:<ul><li>Ready: false</li><li>AllMembersReady: false</li><li>BackupReady: true</li></ul></li></ul></li></ul><h4 id=recommended-action-9>Recommended Action</h4><p>Scale down the <code>StatefulSet</code> to <code>replicas: 0</code>. Ensure that all member pods are deleted. Ensure that all the members are removed from <code>Etcd</code> status. Delete and recreate all the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>member <code>leases</code></a>. Recover the cluster from loss of quorum as discussed <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>here</a>.</p><h3 id=11-scale-up-of-a-healthy-cluster>11. Scale up of a healthy cluster</h3><h4 id=observed-state-10>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: <code>d</code></li><li>Current: <code>n</code> where <code>d > n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: N/A</li><li>Ready: <code>n</code></li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: 0</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: 0</li><li>Members with expired <code>lease</code>: 0</li></ul></li><li>conditions:<ul><li>Ready: true</li><li>AllMembersReady: true</li><li>BackupReady: true</li></ul></li></ul></li></ul><h4 id=recommended-action-10>Recommended Action</h4><p>Add <code>d - n</code> new members by scaling the <code>StatefulSet</code> to <code>replicas: d</code>. The rest of the <code>StatefulSet</code> spec need not be updated until the next cluster bootstrapping (alternatively, the rest of the <code>StatefulSet</code> spec can be updated pro-actively once the new members join the cluster. This will trigger a rolling update).</p><p>Also, create the additional <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>member <code>leases</code></a> for the <code>d - n</code> new members.</p><h3 id=12-scale-down-of-a-healthy-cluster>12. Scale down of a healthy cluster</h3><h4 id=observed-state-11>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: <code>d</code></li><li>Current: <code>n</code> where <code>d &lt; n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: <code>n</code></li><li>Ready: <code>n</code></li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>n</code></li><li>Ready: <code>n</code></li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: 0</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: 0</li><li>Members with expired <code>lease</code>: 0</li></ul></li><li>conditions:<ul><li>Ready: true</li><li>AllMembersReady: true</li><li>BackupReady: true</li></ul></li></ul></li></ul><h4 id=recommended-action-11>Recommended Action</h4><p>Remove <code>d - n</code> existing members (numbered <code>d</code>, <code>d + 1</code> &mldr; <code>n</code>) by scaling the <code>StatefulSet</code> to <code>replicas: d</code>. The <code>StatefulSet</code> spec need not be updated until the next cluster bootstrapping (alternatively, the <code>StatefulSet</code> spec can be updated pro-actively once the superfluous members exit the cluster. This will trigger a rolling update).</p><p>Also, delete the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>member <code>leases</code></a> for the <code>d - n</code> members being removed.</p><p>The <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status>superfluous entries in the <code>members</code> array</a> will be cleaned up as explained <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12>here</a>.
The superfluous members in the ETCD cluster will be cleaned up by the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member>leading <code>etcd-backup-restore</code> sidecar</a>.</p><h3 id=13-superfluous-member-entries-in-etcd-status>13. Superfluous member entries in <code>Etcd</code> status</h3><h4 id=observed-state-12>Observed state</h4><ul><li>Cluster Size<ul><li>Desired: N/A</li><li>Current: <code>n</code></li></ul></li><li><code>StatefulSet</code> replicas<ul><li>Desired: n</li><li>Ready: n</li></ul></li><li><code>Etcd</code> status<ul><li>members<ul><li>Total: <code>m</code> where <code>m > n</code></li><li>Ready: N/A</li><li>Members <code>NotReady</code> for long enough to be evicted, i.e. <code>lastTransitionTime > notReadyGracePeriod</code>: N/A</li><li>Members with readiness status <code>Unknown</code> long enough to be considered <code>NotReady</code>, i.e. <code>lastTransitionTime > unknownGracePeriod</code>: N/A</li><li>Members with expired <code>lease</code>: N/A</li></ul></li><li>conditions:<ul><li>Ready: N/A</li><li>AllMembersReady: N/A</li><li>BackupReady: N/A</li></ul></li></ul></li></ul><h4 id=recommended-action-12>Recommended Action</h4><p>Remove the superfluous <code>m - n</code> member entries from <code>Etcd</code> status (numbered <code>n</code>, <code>n+1</code> &mldr; <code>m</code>).
Remove the superfluous <code>m - n</code> <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>member <code>leases</code></a> if they exist.
The superfluous members in the ETCD cluster will be cleaned up by the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member>leading <code>etcd-backup-restore</code> sidecar</a>.</p><h2 id=decision-table-for-etcd-backup-restore-during-initialization>Decision table for etcd-backup-restore during initialization</h2><p>As discussed above, the initialization sequence of <code>etcd-backup-restore</code> in a member pod needs to <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-configuration>generate suitable etcd configuration</a> for its etcd container.
It also might have to handle the etcd database verification and restoration functionality differently in <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster>different</a> <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>scenarios</a>.</p><p>The initialization sequence itself is proposed to be as follows.
It is an enhancement of the <a href=https://github.com/gardener/etcd-backup-restore/blob/master/doc/proposals/design.md#workflow>existing</a> initialization sequence.
<img src=/__resources/01-etcd-member-initialization-sequence_364f5e.png alt="etcd member initialization sequence"></p><p>The details of the decisions to be taken during the initialization are given below.</p><h3 id=1-first-member-during-bootstrap-of-a-fresh-etcd-cluster>1. First member during bootstrap of a fresh etcd cluster</h3><h4 id=observed-state-13>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>0</code></li><li>Ready: <code>0</code></li><li>Status contains own member: <code>false</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: <code>false</code></li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup<ul><li>Backup exists: <code>false</code></li><li>Backup has incremental snapshots: <code>false</code></li></ul></li></ul><h4 id=recommended-action-13>Recommended Action</h4><p>Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state new and return success.</p><h3 id=2-addition-of-a-new-following-member-during-bootstrap-of-a-fresh-etcd-cluster>2. Addition of a new following member during bootstrap of a fresh etcd cluster</h3><h4 id=observed-state-14>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>m</code> where <code>0 &lt; m &lt; n</code></li><li>Ready: <code>m</code></li><li>Status contains own member: <code>false</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: <code>false</code></li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup<ul><li>Backup exists: <code>false</code></li><li>Backup has incremental snapshots: <code>false</code></li></ul></li></ul><h4 id=recommended-action-14>Recommended Action</h4><p>Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state new and return success.</p><h3 id=3-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-and-data>3. Restart of an existing member of a quorate cluster with valid metadata and data</h3><h4 id=observed-state-15>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>m</code> where <code>m > n/2</code></li><li>Ready: <code>r</code> where <code>r > n/2</code></li><li>Status contains own member: <code>true</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: <code>true</code></li><li>Data directory is valid and up-to-date: <code>true</code></li></ul></li><li>Backup<ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id=recommended-action-15>Recommended Action</h4><p>Re-use previously generated etcd configuration and return success.</p><h3 id=4-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-but-without-valid-data>4. Restart of an existing member of a quorate cluster with valid metadata but without valid data</h3><h4 id=observed-state-16>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>m</code> where <code>m > n/2</code></li><li>Ready: <code>r</code> where <code>r > n/2</code></li><li>Status contains own member: <code>true</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: <code>true</code></li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup<ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id=recommended-action-16>Recommended Action</h4><p><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster>Remove</a> self as a member (old member ID) from the etcd cluster as well as <code>Etcd</code> status. <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>Add</a> self as a new member of the etcd cluster as well as in the <code>Etcd</code> status. If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>here</a> for the reason for not restoring incremental snapshots). Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state <code>existing</code> and return success.</p><h3 id=5-restart-of-an-existing-member-of-a-quorate-cluster-without-valid-metadata>5. Restart of an existing member of a quorate cluster without valid metadata</h3><h4 id=observed-state-17>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>m</code> where <code>m > n/2</code></li><li>Ready: <code>r</code> where <code>r > n/2</code></li><li>Status contains own member: <code>true</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: <code>false</code></li><li>Data directory is valid and up-to-date: N/A</li></ul></li><li>Backup<ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id=recommended-action-17>Recommended Action</h4><p><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster>Remove</a> self as a member (old member ID) from the etcd cluster as well as <code>Etcd</code> status. <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster>Add</a> self as a new member of the etcd cluster as well as in the <code>Etcd</code> status. If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>here</a> for the reason for not restoring incremental snapshots). Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state <code>existing</code> and return success.</p><h3 id=6-restart-of-an-existing-member-of-a-non-quorate-cluster-with-valid-metadata-and-data>6. Restart of an existing member of a non-quorate cluster with valid metadata and data</h3><h4 id=observed-state-18>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>m</code> where <code>m &lt; n/2</code></li><li>Ready: <code>r</code> where <code>r &lt; n/2</code></li><li>Status contains own member: <code>true</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: <code>true</code></li><li>Data directory is valid and up-to-date: <code>true</code></li></ul></li><li>Backup<ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id=recommended-action-18>Recommended Action</h4><p>Re-use previously generated etcd configuration and return success.</p><h3 id=7-restart-of-the-first-member-of-a-non-quorate-cluster-without-valid-data>7. Restart of the first member of a non-quorate cluster without valid data</h3><h4 id=observed-state-19>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>0</code></li><li>Ready: <code>0</code></li><li>Status contains own member: <code>false</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: N/A</li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup<ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id=recommended-action-19>Recommended Action</h4><p>If backups do not exist, create an empty data and WAL directory. If backups exist, restore the latest full snapshot. Start a single-node embedded etcd with initial cluster peer URLs containing only own peer URL and initial cluster state <code>new</code>. If incremental snapshots exist, apply them serially (honouring source transactions). Take and upload a full snapshot after incremental snapshots are applied successfully (please see <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>here</a> for more reasons why). Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state <code>new</code> and return success.</p><h3 id=8-restart-of-a-following-member-of-a-non-quorate-cluster-without-valid-data>8. Restart of a following member of a non-quorate cluster without valid data</h3><h4 id=observed-state-20>Observed state</h4><ul><li>Cluster Size: <code>n</code></li><li><code>Etcd</code> status members:<ul><li>Total: <code>m</code> where <code>1 &lt; m &lt; n</code></li><li>Ready: <code>r</code> where <code>1 &lt; r &lt; n</code></li><li>Status contains own member: <code>false</code></li></ul></li><li>Data persistence<ul><li>WAL directory has cluster/ member metadata: N/A</li><li>Data directory is valid and up-to-date: <code>false</code></li></ul></li><li>Backup<ul><li>Backup exists: N/A</li><li>Backup has incremental snapshots: N/A</li></ul></li></ul><h4 id=recommended-action-20>Recommended Action</h4><p>If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>here</a> for the reason for not restoring incremental snapshots). Generate etcd configuration with <code>n</code> initial cluster peer URLs and initial cluster state <code>existing</code> and return success.</p><h2 id=backup>Backup</h2><p>Only one of the etcd-backup-restore sidecars among the members are required to take the backup for a given ETCD cluster. This can be called a <code>backup leader</code>. There are two possibilities to ensure this.</p><h3 id=leading-etcd-main-containers-sidecar-is-the-backup-leader>Leading ETCD main container’s sidecar is the backup leader</h3><p>The backup-restore sidecar could poll the etcd cluster and/or its own etcd main container to see if it is the leading member in the etcd cluster.
This information can be used by the backup-restore sidecars to decide that sidecar of the leading etcd main container is the backup leader (i.e. responsible to for taking/uploading backups regularly).</p><p>The advantages of this approach are as follows.</p><ul><li>The approach is operationally and conceptually simple. The leading etcd container and backup-restore sidecar are always located in the same pod.</li><li>Network traffic between the backup container and the etcd cluster will always be local.</li></ul><p>The disadvantage is that this approach may not age well in the future if we think about moving the backup-restore container as a separate pod rather than a sidecar container.</p><h3 id=independent-leader-election-between-backup-restore-sidecars>Independent leader election between backup-restore sidecars</h3><p>We could use the etcd <code>lease</code> mechanism to perform leader election among the backup-restore sidecars. For example, using something like <a href=https://pkg.go.dev/go.etcd.io/etcd/clientv3/concurrency#Election.Campaign><code>go.etcd.io/etcd/clientv3/concurrency</code></a>.</p><p>The advantage and disadvantages are pretty much the opposite of the approach <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#leading-etcd-main-containers-sidecar-is-the-backup-leader>above</a>.
The advantage being that this approach may age well in the future if we think about moving the backup-restore container as a separate pod rather than a sidecar container.</p><p>The disadvantages are as follows.</p><ul><li>The approach is operationally and conceptually a bit complex. The leading etcd container and backup-restore sidecar might potentially belong to different pods.</li><li>Network traffic between the backup container and the etcd cluster might potentially be across nodes.</li></ul><h2 id=history-compaction>History Compaction</h2><p>This proposal recommends to configure <a href=https://etcd.io/docs/v3.2.17/op-guide/maintenance/#history-compaction>automatic history compaction</a> on the individual members.</p><h2 id=defragmentation>Defragmentation</h2><p>Defragmentation is already <a href=https://github.com/gardener/etcd-backup-restore/blob/0dfdd50fbfc5ebc88238be3bc79c3ac3fc242c08/cmd/options.go#L209>triggered periodically</a> by <code>etcd-backup-restore</code>.
This proposal recommends to enhance this functionality to be performed only by the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup>leading</a> backup-restore container.
The defragmentation must be performed only when etcd cluster is in full health and must be done in a rolling manner for each members to <a href=https://etcd.io/docs/v3.2.17/op-guide/maintenance/#defragmentation>avoid disruption</a>.
The leading member should be defragmented last after all the rest of the members have been defragmented to minimise potential leadership changes caused by defragmentation.
If the etcd cluster is unhealthy when it is time to trigger scheduled defragmentation, the defragmentation must be postponed until the cluster becomes healthy. This check must be done before triggering defragmentation for each member.</p><h2 id=work-flows-in-etcd-backup-restore>Work-flows in etcd-backup-restore</h2><p>There are different work-flows in etcd-backup-restore.
Some existing flows like initialization, scheduled backups and defragmentation have been enhanced or modified.
Some new work-flows like status updates have been introduced.
Some of these work-flows are sensitive to which <code>etcd-backup-restore</code> container is <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup>leading</a> and some are not.</p><p>The life-cycle of these work-flows is shown below.
<img src=/__resources/01-etcd-backup-restore-work-flows-life-cycle_eec586.png alt="etcd-backup-restore work-flows life-cycle"></p><h3 id=work-flows-independent-of-leader-election-in-all-members>Work-flows independent of leader election in all members</h3><ul><li>Serve the <a href=https://github.com/gardener/etcd-backup-restore/blob/master/pkg/server/httpAPI.go#L101-L107>HTTP API</a> that all members are expected to support currently but some HTTP API call which are used to take <a href=https://github.com/gardener/etcd-backup-restore/blob/5dfcc1f848a9f325d41a24eae4defb70d997c215/pkg/server/httpAPI.go#L103-L105>out-of-sync delta or full snapshot</a> should delegate the incoming HTTP requests to the <code>leading-sidecar</code> and one of the possible approach to achieve this is via an <a href=https://pkg.go.dev/net/http/httputil#ReverseProxy.ServeHTTP>HTTP reverse proxy</a>.</li><li>Check the health of the respective etcd member and renew the corresponding <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases>member <code>lease</code></a>.</li></ul><h3 id=work-flows-only-on-the-leading-member>Work-flows only on the leading member</h3><ul><li>Take <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup>backups</a> (full and incremental) at configured regular intervals</li><li><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#defragmentation>Defragment</a> all the members sequentially at configured regular intervals</li><li>Cleanup superflous members from the ETCD cluster for which there is no corresponding pod (the <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#ordinal-index>ordinal</a> in the pod name is greater than the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#clustersize>cluster size</a>) at regular intervals (or whenever the <code>Etcd</code> resource <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status>status</a> changes by watching it)<ul><li>The cleanup of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status>superfluous entries in <code>status.members</code> array</a> is already covered <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12>here</a></li></ul></li></ul><h2 id=high-availability>High Availability</h2><p>Considering that high-availability is the primary reason for using a multi-node etcd cluster, it makes sense to distribute the individual member pods of the etcd cluster across different physical nodes.
If the underlying Kubernetes cluster has nodes from multiple availability zones, it makes sense to also distribute the member pods across nodes from different availability zones.</p><p>One possibility to do this is via <a href=https://kubernetes.io/docs/reference/scheduling/policies/#priorities><code>SelectorSpreadPriority</code></a> of <code>kube-scheduler</code> but this is only <a href=https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone>best-effort</a> and may not always be enforced strictly.</p><p>It is better to use <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>pod anti-affinity</a> to enforce such distribution of member pods.</p><h3 id=zonal-cluster---single-availability-zone>Zonal Cluster - Single Availability Zone</h3><p>A zonal cluster is configured to consist of nodes belonging to only a single availability zone in a region of the cloud provider.
In such a case, we can at best distribute the member pods of a multi-node etcd cluster instance only across different nodes in the configured availability zone.</p><p>This can be done by specifying <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>pod anti-affinity</a> in the specification of the member pods using <a href=https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#kubernetes-io-hostname><code>kubernetes.io/hostname</code></a> as the topology key.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: StatefulSet
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>      affinity:
</span></span><span style=display:flex><span>        podAntiAffinity:
</span></span><span style=display:flex><span>          requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span style=display:flex><span>          - labelSelector: {} <span style=color:green># podSelector that matches the member pods of the given etcd cluster instance</span>
</span></span><span style=display:flex><span>            topologyKey: <span style=color:#a31515>&#34;kubernetes.io/hostname&#34;</span>
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>The recommendation is to keep <code>etcd-druid</code> agnostic of such topics related scheduling and cluster-topology and to use <a href=https://github.com/gardener/kupid>kupid</a> to <a href=https://github.com/gardener/kupid#mutating-higher-order-controllers>orthogonally inject</a> the desired <a href=https://github.com/gardener/kupid/blob/master/config/samples/cpsp-pod-affinity-anti-affinity.yaml>pod anti-affinity</a>.</p><h4 id=alternative-5>Alternative</h4><p>Another option is to build the functionality into <code>etcd-druid</code> to include the required pod anti-affinity when it provisions the <code>StatefulSet</code> that manages the member pods.
While this has the advantage of avoiding a dependency on an external component like <a href=https://github.com/gardener/kupid>kupid</a>, the disadvantage is that we might need to address development or testing use-cases where it might be desirable to avoid distributing member pods and schedule them on as less number of nodes as possible.
Also, as mentioned <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#regional-cluster---multiple-availability-zones>below</a>, <a href=https://github.com/gardener/kupid>kupid</a> can be used to distribute member pods of an etcd cluster instance across nodes in a single availability zone as well as across nodes in multiple availability zones with very minor variation.
This keeps the solution uniform regardless of the topology of the underlying Kubernetes cluster.</p><h3 id=regional-cluster---multiple-availability-zones>Regional Cluster - Multiple Availability Zones</h3><p>A regional cluster is configured to consist of nodes belonging to multiple availability zones (typically, three) in a region of the cloud provider.
In such a case, we can distribute the member pods of a multi-node etcd cluster instance across nodes belonging to different availability zones.</p><p>This can be done by specifying <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>pod anti-affinity</a> in the specification of the member pods using <a href=https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone><code>topology.kubernetes.io/zone</code></a> as the topology key.
In Kubernetes clusters using Kubernetes release older than <code>1.17</code>, the older (and now deprecated) <a href=https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#failure-domainbetakubernetesiozone><code>failure-domain.beta.kubernetes.io/zone</code></a> might have to be used as the topology key.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: StatefulSet
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>      affinity:
</span></span><span style=display:flex><span>        podAntiAffinity:
</span></span><span style=display:flex><span>          requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span style=display:flex><span>          - labelSelector: {} <span style=color:green># podSelector that matches the member pods of the given etcd cluster instance</span>
</span></span><span style=display:flex><span>            topologyKey: &#34;topology.kubernetes.io/zone
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>The recommendation is to keep <code>etcd-druid</code> agnostic of such topics related scheduling and cluster-topology and to use <a href=https://github.com/gardener/kupid>kupid</a> to <a href=https://github.com/gardener/kupid#mutating-higher-order-controllers>orthogonally inject</a> the desired <a href=https://github.com/gardener/kupid/blob/master/config/samples/cpsp-pod-affinity-anti-affinity.yaml>pod anti-affinity</a>.</p><h4 id=alternative-6>Alternative</h4><p>Another option is to build the functionality into <code>etcd-druid</code> to include the required pod anti-affinity when it provisions the <code>StatefulSet</code> that manages the member pods.
While this has the advantage of avoiding a dependency on an external component like <a href=https://github.com/gardener/kupid>kupid</a>, the disadvantage is that such built-in support necessarily limits what kind of topologies of the underlying cluster will be supported.
Hence, it is better to keep <code>etcd-druid</code> altogether agnostic of issues related to scheduling and cluster-topology.</p><h3 id=poddisruptionbudget>PodDisruptionBudget</h3><p>This proposal recommends that <code>etcd-druid</code> should deploy <a href=https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets><code>PodDisruptionBudget</code></a> (<code>minAvailable</code> set to <code>floor(&lt;cluster size>/2) + 1</code>) for multi-node etcd clusters (if <code>AllMembersReady</code> <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions>condition</a> is <code>true</code>) to ensure that any planned disruptive operation can try and honour the disruption budget to ensure high availability of the etcd cluster while making potentially disrupting maintenance operations.</p><p>Also, it is recommended to toggle the <code>minAvailable</code> field between <code>floor(&lt;cluster size>/2)</code> and <code>&lt;number of members with status Ready true></code> whenever the <code>AllMembersReady</code> condition toggles between <code>true</code> and <code>false</code>.
This is to disable eviction of any member pods when not all members are <code>Ready</code>.</p><p>In case of a conflict, the recommendation is to use the highest of the applicable values for <code>minAvailable</code>.</p><h2 id=rolling-updates-to-etcd-members>Rolling updates to etcd members</h2><p>Any changes to the <code>Etcd</code> resource spec that might result in a change to <code>StatefulSet</code> spec or otherwise result in a rolling update of member pods should be applied/propagated by <code>etcd-druid</code> only when the etcd cluster is fully healthy to reduce the risk of quorum loss during the updates.
This would include vertical autoscaling changes (via, <a href=https://github.com/gardener/hvpa-controller>HVPA</a>).
If the cluster <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status>status</a> unhealthy (i.e. if either <code>AllMembersReady</code> or <code>BackupReady</code> <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions>conditions</a> are <code>false</code>), <code>etcd-druid</code> must restore it to full health <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure>before proceeding</a> with such operations that lead to rolling updates.
This can be further optimized in the future to handle the cases where rolling updates can still be performed on an etcd cluster that is not fully healthy.</p><h2 id=follow-up>Follow Up</h2><h3 id=ephemeral-volumes>Ephemeral Volumes</h3><p>See section <em><a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#Ephemeral_Volumes>Ephemeral Volumes</a></em>.</p><h3 id=shoot-control-plane-migration>Shoot Control-Plane Migration</h3><p>This proposal adds support for multi-node etcd clusters but it should not have significant impact on <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md>shoot control-plane migration</a> any more than what already present in the single-node etcd cluster scenario.
But to be sure, this needs to be discussed further.</p><h3 id=performance-impact-of-multi-node-etcd-clusters>Performance impact of multi-node etcd clusters</h3><p>Multi-node etcd clusters incur a cost on <a href=https://etcd.io/docs/v2/admin_guide/#optimal-cluster-size>write performance</a> as compared to single-node etcd clusters.
This performance impact needs to be measured and documented.
Here, we should compare different persistence option for the multi-nodeetcd clusters so that we have all the information necessary to take the decision balancing the high-availability, performance and costs.</p><h3 id=metrics-dashboards-and-alerts>Metrics, Dashboards and Alerts</h3><p>There are already metrics exported by etcd and <code>etcd-backup-restore</code> which are visualized in monitoring dashboards and also used in triggering alerts.
These might have hidden assumptions about single-node etcd clusters.
These might need to be enhanced and potentially new metrics, dashboards and alerts configured to cover the multi-node etcd cluster scenario.</p><p>Especially, a high priority alert must be raised if <code>BackupReady</code> <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#condition>condition</a> becomes <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure><code>false</code></a>.</p><h3 id=costs>Costs</h3><p>Multi-node etcd clusters will clearly involve higher cost (when compared with single-node etcd clusters) just going by the CPU and memory usage for the additional members.
Also, the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#data-persistence>different options</a> for persistence for etcd data for the members will have different cost implications.
Such cost impact needs to be assessed and documented to help navigate the trade offs between high availability, performance and costs.</p><h2 id=future-work>Future Work</h2><h3 id=gardener-ring>Gardener Ring</h3><p><a href=https://github.com/gardener/gardener/issues/233>Gardener Ring</a>, requires provisioning and management of an etcd cluster with the members distributed across more than one Kubernetes cluster.
This cannot be achieved by etcd-druid alone which has only the view of a single Kubernetes cluster.
An additional component that has the view of all the Kubernetes clusters involved in setting up the gardener ring will be required to achieve this.
However, etcd-druid can be used by such a higher-level component/controller (for example, by supplying the initial cluster configuration) such that individual etcd-druid instances in the individual Kubernetes clusters can manage the corresponding etcd cluster members.</p><h3 id=autonomous-shoot-clusters>Autonomous Shoot Clusters</h3><p><a href=https://github.com/gardener/gardener/issues/2906>Autonomous Shoot Clusters</a> also will require a highly availble etcd cluster to back its control-plane and the multi-node support proposed here can be leveraged in that context.
However, the current proposal will not meet all the needs of a autonomous shoot cluster.
Some additional components will be required that have the overall view of the autonomous shoot cluster and they can use etcd-druid to manage the multi-node etcd cluster. But this scenario may be different from that of <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#gardener-ring>Gardener Ring</a> in that the individual etcd members of the cluster may not be hosted on different Kubernetes clusters.</p><h3 id=optimization-of-recovery-from-non-quorate-cluster-with-some-member-containing-valid-data>Optimization of recovery from non-quorate cluster with some member containing valid data</h3><p>It might be possible to optimize the actions during the recovery of a non-quorate cluster where some of the members contain valid data and some other don&rsquo;t.
The optimization involves verifying the data of the valid members to determine the data of which member is the most recent (even considering the latest backup) so that the <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members>full snapshot</a> can be taken from it before recovering the etcd cluster.
Such an optimization can be attempted in the future.</p><h3 id=optimization-of-rolling-updates-to-unhealthy-etcd-clusters>Optimization of rolling updates to unhealthy etcd clusters</h3><p>As mentioned <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#rolling-updates-to-etcd-members>above</a>, optimizations to proceed with rolling updates to unhealthy etcd clusters (without first restoring the cluster to full health) can be pursued in future work.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-68692e55d72dded65ed0a27990d8ff03>3 - 02 Snapshot Compaction</h1><h1 id=snapshot-compaction-for-etcd>Snapshot Compaction for Etcd</h1><h2 id=current-problem>Current Problem</h2><p>To ensure recoverability of Etcd, backups of the database are taken at regular interval.
Backups are of two types: Full Snapshots and Incremental Snapshots.</p><h3 id=full-snapshots>Full Snapshots</h3><p>Full snapshot is a snapshot of the complete database at given point in time.The size of the database keeps changing with time and typically the size is relatively large (measured in 100s of megabytes or even in gigabytes. For this reason, full snapshots are taken after some large intervals.</p><h3 id=incremental-snapshots>Incremental Snapshots</h3><p>Incremental Snapshots are collection of events on Etcd database, obtained through running WATCH API Call on Etcd. After some short intervals, all the events that are accumulated through WATCH API Call are saved in a file and named as Incremental Snapshots at relatively short time intervals.</p><h3 id=recovery-from-the-snapshots>Recovery from the Snapshots</h3><h4 id=recovery-from-full-snapshots>Recovery from Full Snapshots</h4><p>As the full snapshots are snapshots of the complete database, the whole database can be recovered from a full snapshot in one go. Etcd provides API Call to restore the database from a full snapshot file.</p><h4 id=recovery-from-incremental-snapshots>Recovery from Incremental Snapshots</h4><p>Delta snapshots are collection of retrospective Etcd events. So, to restore from Incremental snapshot file, the events from the file are needed to be applied sequentially on Etcd database through Etcd Put/Delete API calls. As it is heavily dependent on Etcd calls sequentially, restoring from Incremental Snapshot files can take long if there are numerous commands captured in Incremental Snapshot files.</p><p>Delta snapshots are applied on top of running Etcd database. So, if there is inconsistency between the state of database at the point of applying and the state of the database when the delta snapshot commands were captured, restoration will fail.</p><p>Currently, in Gardener setup, Etcd is restored from the last full snapshot and then the delta snapshots, which were captured after the last full snapshot.</p><p>The main problem with this is that the complete restoration time can be unacceptably large if the rate of change coming into the etcd database is quite high because there are large number of events in the delta snapshots to be applied sequentially.
A secondary problem is that, though auto-compaction is enabled for etcd, it is not quick enough to compact all the changes from the incremental snapshots being re-applied during the relatively short period of time of restoration (as compared to the actual period of time when the incremental snapshots were accumulated). This may lead to the etcd pod (the backup-restore sidecar container, to be precise) to run out of memory and/or storage space even if it is sufficient for normal operations.</p><h2 id=solution>Solution</h2><h3 id=compaction-command>Compaction command</h3><p>To help with the problem mentioned earlier, our proposal is to introduce <code>compact</code> subcommand with <code>etcdbrctl</code>. On execution of <code>compact</code> command, A separate embedded Etcd process will be started where the Etcd data will be restored from the snapstore (exactly as in the restoration scenario today). Then the new Etcd database will be compacted and defragmented using Etcd API calls. The compaction will strip off the Etcd database of old revisions as per the Etcd auto-compaction configuration. The defragmentation will free up the unused fragment memory space released after compaction. Then a full snapshot of the compacted database will be saved in snapstore which then can be used as the base snapshot during any subsequent restoration (or backup compaction).</p><h3 id=how-the-solution-works>How the solution works</h3><p>The newly introduced compact command does not disturb the running Etcd while compacting the backup snapshots. The command is designed to run potentially separately (from the main Etcd process/container/pod). Etcd Druid can be configured to run the newly introduced compact command as a separate job (scheduled periodically) based on total number of Etcd events accumulated after the most recent full snapshot.</p><h3 id=etcd-druid-flags>Etcd-druid flags:</h3><p>Etcd-druid introduces the following flags to configure the compaction job:</p><ul><li><code>--enable-backup-compaction</code> (default <code>false</code>): Set this flag to <code>true</code> to enable the automatic compaction of etcd backups when the threshold value denoted by CLI flag <code>--etcd-events-threshold</code> is exceeded.</li><li><code>--compaction-workers</code> (default <code>3</code>): Number of worker threads of the CompactionJob controller. The controller creates a backup compaction job if a certain etcd event threshold is reached. If compaction is enabled, the value for this flag must be greater than zero.</li><li><code>--etcd-events-threshold</code> (default <code>1000000</code>): Total number of etcd events that can be allowed before a backup compaction job is triggered.</li><li><code>--active-deadline-duration</code> (default <code>3h</code>): Duration after which a running backup compaction job will be terminated.</li><li><code>--metrics-scrape-wait-duration</code> (default <code>0s</code>): Duration to wait for after compaction job is completed, to allow Prometheus metrics to be scraped.</li></ul><h3 id=points-to-take-care-while-saving-the-compacted-snapshot><strong>Points to take care while saving the compacted snapshot:</strong></h3><p>As compacted snapshot and the existing periodic full snapshots are taken by different processes running in different pods but accessing same store to save the snapshots, some problems may arise:</p><ol><li>When uploading the compacted snapshot to the snapstore, there is the problem of how does the restorer know when to start using the newly compacted snapshot. This communication needs to be atomic.</li><li>With a regular schedule for compaction that happens potentially separately from the main etcd pod, is there a need for regular scheduled full snapshots anymore?</li><li>We are planning to introduce new directory structure, under v2 prefix, for saving the snapshots (compacted and full), as mentioned in details below. But for backward compatibility, we also need to consider the older directory, which is currently under v1 prefix, during accessing snapshots.</li></ol><h4 id=how-to-swap-full-snapshot-with-compacted-snapshot-atomically><strong>How to swap full snapshot with compacted snapshot atomically</strong></h4><p>Currently, full snapshots and the subsequent delta snapshots are grouped under same prefix path in the snapstore. When a full snapshot is created, it is placed under a prefix/directory with the name comprising of timestamp. Then subsequent delta snapshots are also pushed into the same directory. Thus each prefix/directory contains a single full snapshot and the subsequent delta snapshots. So far, it is the job of ETCDBR to start main Etcd process and snapshotter process which takes full snapshot and delta snapshot periodically. But as per our proposal, compaction will be running as parallel process to main Etcd process and snapshotter process. So we can&rsquo;t reliably co-ordinate between the processes to achieve switching to the compacted snapshot as the base snapshot atomically.</p><h5 id=current-directory-structure><strong>Current Directory Structure</strong></h5><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>- Backup-192345
</span></span><span style=display:flex><span>    - Full-Snapshot-0-1-192345
</span></span><span style=display:flex><span>    - Incremental-Snapshot-1-100-192355
</span></span><span style=display:flex><span>    - Incremental-Snapshot-100-200-192365
</span></span><span style=display:flex><span>    - Incremental-Snapshot-200-300-192375
</span></span><span style=display:flex><span>- Backup-192789
</span></span><span style=display:flex><span>    - Full-Snapshot-0-300-192789
</span></span><span style=display:flex><span>    - Incremental-Snapshot-300-400-192799
</span></span><span style=display:flex><span>    - Incremental-Snapshot-400-500-192809
</span></span><span style=display:flex><span>    - Incremental-Snapshot-500-600-192819
</span></span></code></pre></div><p>To solve the problem, proposal is:</p><ol><li>ETCDBR will take the first full snapshot after it starts main Etcd Process and snapshotter process. After taking the first full snapshot, snapshotter will continue taking full snapshots. On the other hand, ETCDBR compactor command will be run as periodic job in a separate pod and use the existing full or compacted snapshots to produce further compacted snapshots. Full snapshots and compacted snapshots will be named after same fashion. So, there is no need of any mechanism to choose which snapshots(among full and compacted snapshot) to consider as base snapshots.</li><li>Flatten the directory structure of backup folder. Save all the full snapshots, delta snapshots and compacted snapshots under same directory/prefix. Restorer will restore from full/compacted snapshots and delta snapshots sorted based on the revision numbers in name (or timestamp if the revision numbers are equal).</li></ol><h5 id=proposed-directory-structure><strong>Proposed Directory Structure</strong></h5><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>Backup :
</span></span><span style=display:flex><span>    - Full-Snapshot-0-1-192355 (Taken by snapshotter)
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-1-100-192365
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-100-200-192375
</span></span><span style=display:flex><span>    - Full-Snapshot-revision-0-200-192379 (Taken by snapshotter)
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-200-300-192385
</span></span><span style=display:flex><span>    - Full-Snapshot-revision-0-300-192386 (Taken by compaction job)
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-300-400-192396
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-400-500-192406
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-500-600-192416
</span></span><span style=display:flex><span>    - Full-Snapshot-revision-0-600-192419 (Taken by snapshotter)
</span></span><span style=display:flex><span>    - Full-Snapshot-revision-0-600-192420 (Taken by compaction job)
</span></span></code></pre></div><h5 id=what-happens-to-the-delta-snapshots-that-were-compacted>What happens to the delta snapshots that were compacted?</h5><p>The proposed <code>compaction</code> sub-command in <code>etcdbrctl</code> (and hence, the <code>CronJob</code> provisioned by <code>etcd-druid</code> that will schedule it at a regular interval) would only upload the compacted full snapshot.
It will not delete the snapshots (delta or full snapshots) that were compacted.
These snapshots which were superseded by a freshly uploaded compacted snapshot would follow the same life-cycle as other older snapshots.
I.e. they will be garbage collected according to the configured backup snapshot retention policy.
For example, if an <code>exponential</code> retention policy is configured and if compaction is done every <code>30m</code> then there might be at most <code>48</code> additional (compacted) full snapshots (<code>24h * 2</code>) in the backup for the latest day. As time rolls forward to the next day, these additional compacted snapshots (along with the delta snapshots that were compacted into them) will get garbage collected retaining only one full snapshot for the day before according to the retention policy.</p><h5 id=future-work><strong>Future work</strong></h5><p>In the future, we have plan to stop the snapshotter just after taking the first full snapshot. Then, the compaction job will be solely responsible for taking subsequent full snapshots. The directory structure would be looking like following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>Backup :
</span></span><span style=display:flex><span>    - Full-Snapshot-0-1-192355 (Taken by snapshotter)
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-1-100-192365
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-100-200-192375
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-200-300-192385
</span></span><span style=display:flex><span>    - Full-Snapshot-revision-0-300-192386 (Taken by compaction job)
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-300-400-192396
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-400-500-192406
</span></span><span style=display:flex><span>    - Incremental-Snapshot-revision-500-600-192416
</span></span><span style=display:flex><span>    - Full-Snapshot-revision-0-600-192420 (Taken by compaction job)
</span></span></code></pre></div><h4 id=backward-compatibility>Backward Compatibility</h4><ol><li><strong>Restoration</strong> : The changes to handle the newly proposed backup directory structure must be backward compatible with older structures at least for restoration because we need have to restore from backups in the older structure. This includes the support for restoring from a backup without a metadata file if that is used in the actual implementation.</li><li><strong>Backup</strong> : For new snapshots (even on a backup containing the older structure), the new structure may be used. The new structure must be setup automatically including creating the base full snapshot.</li><li><strong>Garbage collection</strong> : The existing functionality of garbage collection of snapshots (full and incremental) according to the backup retention policy must be compatible with both old and new backup folder structure. I.e. the snapshots in the older backup structure must be retained in their own structure and the snapshots in the proposed backup structure should be retained in the proposed structure. Once all the snapshots in the older backup structure go out of the retention policy and are garbage collected, we can think of removing the support for older backup folder structure.</li></ol><p><strong>Note:</strong> Compactor will run parallel to current snapshotter process and work only if there is any full snapshot already present in the store. By current design, a full snapshot will be taken if there is already no full snapshot or the existing full snapshot is older than 24 hours. It is not limitation but a design choice. As per proposed design, the backup storage will contain both periodic full snapshots as well as periodic compacted snapshot. Restorer will pickup the base snapshot whichever is latest one.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4b238c4f12151eab9f3e4bd2376dc879>4 - 03 Scaling Up An Etcd Cluster</h1><h1 id=scaling-up-a-single-node-to-multi-node-etcd-cluster-deployed-by-etcd-druid>Scaling-up a single-node to multi-node etcd cluster deployed by etcd-druid</h1><p>To mark a cluster for scale-up from single node to multi-node etcd, just patch the etcd custom resource&rsquo;s <code>.spec.replicas</code> from <code>1</code> to <code>3</code> (for example).</p><h2 id=challenges-for-scale-up>Challenges for scale-up</h2><ol><li>Etcd cluster with single replica don&rsquo;t have any peers, so no peer communication is required hence peer URL may or may not be TLS enabled. However, while scaling up from single node etcd to multi-node etcd, there will be a requirement to have peer communication between members of the etcd cluster. Peer communication is required for various reasons, for instance for members to sync up cluster state, data, and to perform leader election or any cluster wide operation like removal or addition of a member etc. Hence in a multi-node etcd cluster we need to have TLS enable peer URL for peer communication.</li><li>Providing the correct configuration to start new etcd members as it is different from boostrapping a cluster since these new etcd members will join an existing cluster.</li></ol><h2 id=approach>Approach</h2><p>We first went through the etcd doc of <a href=https://etcd.io/docs/v3.4/op-guide/runtime-configuration/#update-advertise-peer-urls>update-advertise-peer-urls</a> to find out information regarding peer URL updation. Interestingly, etcd doc has mentioned the following:</p><pre tabindex=0><code>To update the advertise peer URLs of a member, first update it explicitly via member command and then restart the member.
</code></pre><p>But we can&rsquo;t assume peer URL is not TLS enabled for single-node cluster as it depends on end-user. A user may or may not enable the TLS for peer URL for a single node etcd cluster. So, How do we detect whether peer URL was enabled or not when cluster is marked for scale-up?</p><h2 id=detecting-if-peerurl-tls-is-enabled-or-not>Detecting if peerURL TLS is enabled or not</h2><p>For this we use an annotation in member lease object <code>member.etcd.gardener.cloud/tls-enabled</code> set by backup-restore sidecar of etcd. As etcd configuration is provided by backup-restore, so it can find out whether TLS is enabled or not and accordingly set this annotation <code>member.etcd.gardener.cloud/tls-enabled</code> to either <code>true</code> or <code>false</code> in member lease object.
And with the help of this annotation and config-map values etcd-druid is able to detect whether there is a change in a peer URL or not.</p><h2 id=etcd-druid-helps-in-scaling-up-etcd-cluster>Etcd-Druid helps in scaling up etcd cluster</h2><p>Now, it is detected whether peer URL was TLS enabled or not for single node etcd cluster. Etcd-druid can now use this information to take action:</p><ul><li>If peer URL was already TLS enabled then no action is required from etcd-druid side. Etcd-druid can proceed with scaling up the cluster.</li><li>If peer URL was not TLS enabled then etcd-druid has to intervene and make sure peer URL should be TLS enabled first for the single node before marking the cluster for scale-up.</li></ul><h2 id=action-taken-by-etcd-druid-to-enable-the-peerurl-tls>Action taken by etcd-druid to enable the peerURL TLS</h2><ol><li>Etcd-druid will update the <code>etcd-bootstrap</code> config-map with new config like initial-cluster,initial-advertise-peer-urls etc. Backup-restore will detect this change and update the member lease annotation to <code>member.etcd.gardener.cloud/tls-enabled: "true"</code>.</li><li>In case the peer URL TLS has been changed to <code>enabled</code>: Etcd-druid will add tasks to the deployment flow:<ul><li>Check if peer TLS has been enabled for existing StatefulSet pods, by checking the member leases for the annotation <code>member.etcd.gardener.cloud/tls-enabled</code>.</li><li>If peer TLS enablement is pending for any of the members, then check and patch the StatefulSet with the peer TLS volume mounts, if not already patched. This will cause a rolling update of the existing StatefulSet pods, which allows etcd-backup-restore to update the member peer URL in the etcd cluster.</li><li>Requeue this reconciliation flow until peer TLS has been enabled for all the existing etcd members.</li></ul></li></ol><h2 id=after-peerurl-is-tls-enabled>After PeerURL is TLS enabled</h2><p>After peer URL TLS enablement for single node etcd cluster, now etcd-druid adds a scale-up annotation: <code>gardener.cloud/scaled-to-multi-node</code> to the etcd statefulset and etcd-druid will patch the statefulsets <code>.spec.replicas</code> to <code>3</code>(for example). The statefulset controller will then bring up new pods(etcd with backup-restore as a sidecar). Now etcd&rsquo;s sidecar i.e backup-restore will check whether this member is already a part of a cluster or not and incase it is unable to check (may be due to some network issues) then backup-restore checks presence of this annotation: <code>gardener.cloud/scaled-to-multi-node</code> in etcd statefulset to detect scale-up. If it finds out it is the scale-up case then backup-restore adds new etcd member as a <a href=https://etcd.io/docs/v3.3/learning/learner/>learner</a> first and then starts the etcd learner by providing the correct configuration. Once learner gets in sync with the etcd cluster leader, it will get promoted to a voting member.</p><h2 id=providing-the-correct-etcd-config>Providing the correct etcd config</h2><p>As backup-restore detects that it&rsquo;s a scale-up scenario, backup-restore sets <code>initial-cluster-state</code> to <code>existing</code> as this member will join an existing cluster and it calculates the rest of the config from the updated config-map provided by etcd-druid.</p><p><img src=/__resources/03-scale-up-sequenceDiagram_76558b.png alt="Sequence diagram"></p><h2 id=future-improvements>Future improvements:</h2><p>The need of restarting etcd pods twice will change in the future. please refer: <a href=https://github.com/gardener/etcd-backup-restore/issues/538>https://github.com/gardener/etcd-backup-restore/issues/538</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-07375cb408c3ec79aee3400029137fb1>5 - Cli Flags</h1><h1 id=cli-flags>CLI Flags</h1><p>Etcd-druid exposes the following CLI flags that allow for configuring its behavior.</p><table><thead><tr><th>CLI FLag</th><th>Component</th><th>Description</th><th>Default</th></tr></thead><tbody><tr><td><code>feature-gates</code></td><td><code>etcd-druid</code></td><td>A set of key=value pairs that describe feature gates for alpha/experimental features. Please check <a href=/docs/other-components/etcd-druid/deployment/feature-gates/>feature-gates</a> for more information.</td><td><code>""</code></td></tr><tr><td><code>metrics-bind-address</code></td><td><code>controller-manager</code></td><td>The IP address that the metrics endpoint binds to.</td><td><code>""</code></td></tr><tr><td><code>metrics-port</code></td><td><code>controller-manager</code></td><td>The port used for the metrics endpoint.</td><td><code>8080</code></td></tr><tr><td><code>metrics-addr</code></td><td><code>controller-manager</code></td><td>The fully qualified address:port that the metrics endpoint binds to.<br>Deprecated: this field will be eventually removed. Please use <code>--metrics-bind-address</code> and &ndash;<code>metrics-port</code> instead.</td><td><code>":8080"</code></td></tr><tr><td><code>webhook-server-bind-address</code></td><td><code>controller-manager</code></td><td>The IP address on which to listen for the HTTPS webhook server.</td><td><code>""</code></td></tr><tr><td><code>webhook-server-port</code></td><td><code>controller-manager</code></td><td>The port on which to listen for the HTTPS webhook server.</td><td><code>9443</code></td></tr><tr><td><code>webhook-server-tls-server-cert-dir</code></td><td><code>controller-manager</code></td><td>The path to a directory containing the server&rsquo;s TLS certificate and key (the files must be named tls.crt and tls.key respectively).</td><td><code>"/etc/webhook-server-tls"</code></td></tr><tr><td><code>enable-leader-election</code></td><td><code>controller-manager</code></td><td>Enable leader election for controller manager. Enabling this will ensure there is only one active controller manager.</td><td><code>false</code></td></tr><tr><td><code>leader-election-id</code></td><td><code>controller-manager</code></td><td>Name of the resource that leader election will use for holding the leader lock.</td><td><code>"druid-leader-election"</code></td></tr><tr><td><code>leader-election-resource-lock</code></td><td><code>controller-manager</code></td><td>Specifies which resource type to use for leader election. Supported options are &rsquo;endpoints&rsquo;, &lsquo;configmaps&rsquo;, &rsquo;leases&rsquo;, &rsquo;endpointsleases&rsquo; and &lsquo;configmapsleases&rsquo;.<br>Deprecated. Will be removed in the future in favour of using only <code>leases</code> as the leader election resource lock for the controller manager.</td><td><code>"leases"</code></td></tr><tr><td><code>disable-lease-cache</code></td><td><code>controller-manager</code></td><td>Disable cache for lease.coordination.k8s.io resources.</td><td><code>false</code></td></tr><tr><td><code>etcd-workers</code></td><td><code>etcd-controller</code></td><td>Number of workers spawned for concurrent reconciles of etcd spec and status changes. If not specified then default of 3 is assumed.</td><td><code>3</code></td></tr><tr><td><code>ignore-operation-annotation</code></td><td><code>etcd-controller</code></td><td>Specifies whether to ignore or honour the annotation <code>gardener.cloud/operation: reconcile</code> on resources to be reconciled.<br>Deprecated: please use <code>--enable-etcd-spec-auto-reconcile</code> instead.</td><td><code>false</code></td></tr><tr><td><code>enable-etcd-spec-auto-reconcile</code></td><td><code>etcd-controller</code></td><td>If true then automatically reconciles Etcd Spec. If false, waits for explicit annotation <code>gardener.cloud/operation: reconcile</code> to be placed on the Etcd resource to trigger reconcile.</td><td><code>false</code></td></tr><tr><td><code>disable-etcd-serviceaccount-automount</code></td><td><code>etcd-controller</code></td><td>If true then .automountServiceAccountToken will be set to false for the ServiceAccount created for etcd StatefulSets.</td><td><code>false</code></td></tr><tr><td><code>etcd-status-sync-period</code></td><td><code>etcd-controller</code></td><td>Period after which an etcd status sync will be attempted.</td><td><code>15s</code></td></tr><tr><td><code>etcd-member-notready-threshold</code></td><td><code>etcd-controller</code></td><td>Threshold after which an etcd member is considered not ready if the status was unknown before.</td><td><code>5m</code></td></tr><tr><td><code>etcd-member-unknown-threshold</code></td><td><code>etcd-controller</code></td><td>Threshold after which an etcd member is considered unknown.</td><td><code>1m</code></td></tr><tr><td><code>enable-backup-compaction</code></td><td><code>compaction-controller</code></td><td>Enable automatic compaction of etcd backups.</td><td><code>false</code></td></tr><tr><td><code>compaction-workers</code></td><td><code>compaction-controller</code></td><td>Number of worker threads of the CompactionJob controller. The controller creates a backup compaction job if a certain etcd event threshold is reached. If compaction is enabled, the value for this flag must be greater than zero.</td><td><code>3</code></td></tr><tr><td><code>etcd-events-threshold</code></td><td><code>compaction-controller</code></td><td>Total number of etcd events that can be allowed before a backup compaction job is triggered.</td><td><code>1000000</code></td></tr><tr><td><code>active-deadline-duration</code></td><td><code>compaction-controller</code></td><td>Duration after which a running backup compaction job will be terminated.</td><td><code>3h</code></td></tr><tr><td><code>metrics-scrape-wait-duration</code></td><td><code>compaction-controller</code></td><td>Duration to wait for after compaction job is completed, to allow Prometheus metrics to be scraped.</td><td><code>0s</code></td></tr><tr><td><code>etcd-copy-backups-task-workers</code></td><td><code>etcdcopybackupstask-controller</code></td><td>Number of worker threads for the etcdcopybackupstask controller.</td><td><code>3</code></td></tr><tr><td><code>secret-workers</code></td><td><code>secret-controller</code></td><td>Number of worker threads for the secrets controller.</td><td><code>10</code></td></tr><tr><td><code>enable-etcd-components-webhook</code></td><td><code>etcdcomponents-webhook</code></td><td>Enable EtcdComponents Webhook to prevent unintended changes to resources managed by etcd-druid.</td><td><code>false</code></td></tr><tr><td><code>reconciler-service-account</code></td><td><code>etcdcomponents-webhook</code></td><td>The fully qualified name of the service account used by etcd-druid for reconciling etcd resources. If unspecified, the default service account mounted for etcd-druid will be used.</td><td><code>&lt;etcd-druid-service-account></code></td></tr><tr><td><code>etcd-components-exempt-service-accounts</code></td><td><code>etcdcomponents-webhook</code></td><td>The comma-separated list of fully qualified names of service accounts that are exempt from EtcdComponents Webhook checks.</td><td><code>""</code></td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-345bf147b3710cef597e41aa041ef8b5>6 - Controllers</h1><h1 id=controllers>Controllers</h1><p>etcd-druid is an operator to manage etcd clusters, and follows the <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/operator/><code>Operator</code></a> pattern for Kubernetes.
It makes use of the <a href=https://github.com/kubernetes-sigs/kubebuilder>Kubebuilder</a> framework which makes it quite easy to define Custom Resources (CRs) such as <code>Etcd</code>s and <code>EtcdCopyBackupTask</code>s through <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/><em>Custom Resource Definitions</em></a> (CRDs), and define controllers for these CRDs.
etcd-druid uses Kubebuilder to define the <code>Etcd</code> CR and its corresponding controllers.</p><p>All controllers that are a part of etcd-druid reside in package <code>internal/controller</code>, as sub-packages.</p><p>Etcd-druid currently consists of the following controllers, each having its own responsibility:</p><ul><li><em>etcd</em> : responsible for the reconciliation of the <code>Etcd</code> CR spec, which allows users to run etcd clusters within the specified Kubernetes cluster, and also responsible for periodically updating the <code>Etcd</code> CR status with the up-to-date state of the managed etcd cluster.</li><li><em>compaction</em> : responsible for <a href=/docs/other-components/etcd-druid/proposals/02-snapshot-compaction/>snapshot compaction</a>.</li><li><em>etcdcopybackupstask</em> : responsible for the reconciliation of the <code>EtcdCopyBackupsTask</code> CR, which helps perform the job of copying snapshot backups from one object store to another.</li><li><em>secret</em> : responsible in making sure <code>Secret</code>s being referenced by <code>Etcd</code> resources are not deleted while in use.</li></ul><h2 id=package-structure>Package Structure</h2><p>The typical package structure for the controllers that are part of etcd-druid is shown with the <em>compaction controller</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>internal/controller/compaction
</span></span><span style=display:flex><span>├── config.go
</span></span><span style=display:flex><span>├── reconciler.go
</span></span><span style=display:flex><span>└── register.go
</span></span></code></pre></div><ul><li><code>config.go</code>: contains all the logic for the configuration of the controller, including feature gate activations, CLI flag parsing and validations.</li><li><code>register.go</code>: contains the logic for registering the controller with the etcd-druid controller manager.</li><li><code>reconciler.go</code>: contains the controller reconciliation logic.</li></ul><p>Each controller package also contains auxiliary files which are relevant to that specific controller.</p><h2 id=controller-manager>Controller Manager</h2><p>A <em>manager</em> is first created for all controllers that are a part of etcd-druid.
The <em>controller manager</em> is responsible for all the controllers that are associated with CRDs.
Once the manager is <code>Start()</code>ed, all the controllers that are <em>registered</em> with it are started.</p><p>Each controller is built using a controller builder, configured with details such as the type of object being reconciled, owned objects whose owner object is reconciled, event filters (predicates), etc. <code>Predicates</code> are filters which allow controllers to filter which type of events the controller should respond to and which ones to ignore.</p><p>The logic relevant to the controller manager like the creation of the controller manager and registering each of the controllers with the manager, is contained in <a href=https://github.com/gardener/etcd-druid/blob/master/internal/manager/manager.go><code>internal/manager/manager.go</code></a>.</p><h2 id=etcd-controller>Etcd Controller</h2><p>The <em>etcd controller</em> is responsible for the reconciliation of the <code>Etcd</code> resource spec and status. It handles the provisioning and management of the etcd cluster. Different components that are required for the functioning of the cluster like <code>Leases</code>, <code>ConfigMap</code>s, and the <code>Statefulset</code> for the etcd cluster are all deployed and managed by the <em>etcd controller</em>.</p><p>Additionally, <em>etcd controller</em> also periodically updates the <code>Etcd</code> resource status with the latest available information from the etcd cluster, as well as results and errors from the recent-most reconciliation of the <code>Etcd</code> resource spec.</p><p>The <em>etcd controller</em> is essential to the functioning of the etcd cluster and etcd-druid, thus the minimum number of worker threads is 1 (default being 3), controlled by the CLI flag <code>--etcd-workers</code>.</p><h3 id=etcd-spec-reconciliation><code>Etcd</code> Spec Reconciliation</h3><p>While building the controller, an event filter is set such that the behavior of the controller, specifically for <code>Etcd</code> update operations, depends on the <code>gardener.cloud/operation: reconcile</code> <em>annotation</em>. This is controlled by the <code>--enable-etcd-spec-auto-reconcile</code> CLI flag, which, if set to <code>false</code>, tells the controller to perform reconciliation only when this annotation is present. If the flag is set to <code>true</code>, the controller will reconcile the etcd cluster anytime the <code>Etcd</code> spec, and thus <code>generation</code>, changes, and the next queued event for it is triggered.</p><blockquote><p><strong>Note:</strong> Creation and deletion of <code>Etcd</code> resources are not affected by the above flag or annotation.</p></blockquote><p>The reason this filter is present is that any disruption in the <code>Etcd</code> resource due to reconciliation (due to changes in the <code>Etcd</code> spec, for example) while workloads are being run would cause unwanted downtimes to the etcd cluster. Hence, any user who wishes to avoid such disruptions, can choose to set the <code>--enable-etcd-spec-auto-reconcile</code> CLI flag to <code>false</code>. An example of this is Gardener&rsquo;s <a href=/docs/gardener/concepts/gardenlet/>gardenlet</a>, which reconciles the <code>Etcd</code> resource only during a shoot cluster&rsquo;s <a href=/docs/gardener/shoot_maintenance/><em>maintenance window</em></a>.</p><p>The controller adds a finalizer to the <code>Etcd</code> resource in order to ensure that it does not get deleted until all dependent resources managed by etcd-druid, aka managed components, are properly cleaned up. Only the <em>etcd controller</em> can delete a resource once it adds finalizers to it. This ensures that the proper deletion flow steps are followed while deleting the resource. During deletion flow, managed components are deleted in parallel.</p><h3 id=etcd-status-updates><code>Etcd</code> Status Updates</h3><p>The <code>Etcd</code> resource status is updated periodically by <code>etcd controller</code>, the interval for which is determined by the CLI flag <code>--etcd-status-sync-period</code>.</p><p>Status fields of the <code>Etcd</code> resource such as <code>LastOperation</code>, <code>LastErrors</code> and <code>ObservedGeneration</code>, are updated to reflect the result of the recent reconciliation of the <code>Etcd</code> resource spec.</p><ul><li><code>LastOperation</code> holds information about the last operation performed on the etcd cluster, indicated by fields <code>Type</code>, <code>State</code>, <code>Description</code> and <code>LastUpdateTime</code>. Additionally, a field <code>RunID</code> indicates the unique ID assigned to the specific reconciliation run, to allow for better debugging of issues.</li><li><code>LastErrors</code> is a slice of errors encountered by the last reconciliation run. Each error consists of fields <code>Code</code> to indicate the custom etcd-druid error code for the error, a human-readable <code>Description</code>, and the <code>ObservedAt</code> time when the error was seen.</li><li><code>ObservedGeneration</code> indicates the latest <code>generation</code> of the <code>Etcd</code> resource that etcd-druid has &ldquo;observed&rdquo; and consequently reconciled. It helps identify whether a change in the <code>Etcd</code> resource spec was acted upon by druid or not.</li></ul><p>Status fields of the <code>Etcd</code> resource which correspond to the <code>StatefulSet</code> like <code>CurrentReplicas</code>, <code>ReadyReplicas</code> and <code>Replicas</code> are updated to reflect those of the <code>StatefulSet</code> by the controller.</p><p>Status fields related to the etcd cluster itself, such as <code>Members</code>, <code>PeerUrlTLSEnabled</code> and <code>Ready</code> are updated as follows:</p><ul><li>Cluster Membership: The controller updates the information about etcd cluster membership like <code>Role</code>, <code>Status</code>, <code>Reason</code>, <code>LastTransitionTime</code> and identifying information like the <code>Name</code> and <code>ID</code>. For the <code>Status</code> field, the member is checked for the <em>Ready</em> condition, where the member can be in <code>Ready</code>, <code>NotReady</code> and <code>Unknown</code> statuses.</li></ul><p><code>Etcd</code> resource conditions are indicated by status field <code>Conditions</code>. The condition checks that are currently performed are:</p><ul><li><code>AllMembersReady</code>: indicates readiness of all members of the etcd cluster.</li><li><code>Ready</code>: indicates overall readiness of the etcd cluster in serving traffic.</li><li><code>BackupReady</code>: indicates health of the etcd backups, i.e., whether etcd backups are being taken regularly as per schedule. This condition is applicable only when backups are enabled for the etcd cluster.</li><li><code>DataVolumesReady</code>: indicates health of the persistent volumes containing the etcd data.</li></ul><h2 id=compaction-controller>Compaction Controller</h2><p>The <em>compaction controller</em> deploys the snapshot compaction job whenever required. To understand the rationale behind this controller, please read <a href=/docs/other-components/etcd-druid/proposals/02-snapshot-compaction/>snapshot-compaction.md</a>.
The controller watches the number of events accumulated as part of delta snapshots in the etcd cluster&rsquo;s backups, and triggers a snapshot compaction when the number of delta events crosses the set threshold, which is configurable through the <code>--etcd-events-threshold</code> CLI flag (1M events by default).</p><p>The controller watches for changes in <em>snapshot</em> <code>Leases</code> associated with <code>Etcd</code> resources.
It checks the full and delta snapshot <code>Leases</code> and calculates the difference in events between the latest delta snapshot and the previous full snapshot, and initiates the compaction job if the event threshold is crossed.</p><p>The number of worker threads for the <em>compaction controller</em> needs to be greater than or equal to 0 (default 3), controlled by the CLI flag <code>--compaction-workers</code>.
This is unlike other controllers which need at least one worker thread for the proper functioning of etcd-druid as snapshot compaction is not a core functionality for the etcd clusters to be deployed.
The compaction controller should be explicitly enabled by the user, through the <code>--enable-backup-compaction</code> CLI flag.</p><h2 id=etcdcopybackupstask-controller>EtcdCopyBackupsTask Controller</h2><p>The <em>etcdcopybackupstask controller</em> is responsible for deploying the <a href=https://github.com/gardener/etcd-backup-restore/blob/master/cmd/copy.go><code>etcdbrctl copy</code></a> command as a job.
This controller reacts to create/update events arising from EtcdCopyBackupsTask resources, and deploys the <code>EtcdCopyBackupsTask</code> job with source and target backup storage providers as arguments, which are derived from source and target bucket secrets referenced by the <code>EtcdCopyBackupsTask</code> resource.</p><p>The number of worker threads for the <em>etcdcopybackupstask controller</em> needs to be greater than or equal to 0 (default being 3), controlled by the CLI flag <code>--etcd-copy-backups-task-workers</code>.
This is unlike other controllers who need at least one worker thread for the proper functioning of etcd-druid as <code>EtcdCopyBackupsTask</code> is not a core functionality for the etcd clusters to be deployed.</p><h2 id=secret-controller>Secret Controller</h2><p>The <em>secret controller</em>&rsquo;s primary responsibility is to add a finalizer on <code>Secret</code>s referenced by the <code>Etcd</code> resource.
The <em>secret controller</em> is registered for <code>Secret</code>s, and the controller keeps a watch on the <code>Etcd</code> CR.
This finalizer is added to ensure that <code>Secret</code>s which are referenced by the <code>Etcd</code> CR aren&rsquo;t deleted while still being used by the <code>Etcd</code> resource.</p><p>Events arising from the <code>Etcd</code> resource are mapped to a list of <code>Secret</code>s such as backup and TLS secrets that are referenced by the <code>Etcd</code> resource, and are enqueued into the request queue, which the reconciler then acts on.</p><p>The number of worker threads for the secret controller must be at least 1 (default being 10) for this core controller, controlled by the CLI flag <code>--secret-workers</code>, since the referenced TLS and infrastructure access secrets are essential to the proper functioning of the etcd cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-576e725c12b217f7eef3768da38537e7>7 - DEP Title</h1><h1 id=dep-nn-your-short-descriptive-title>DEP-NN: Your short, descriptive title</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=/docs/other-components/etcd-druid/proposals/00-template/#summary>Summary</a></li><li><a href=/docs/other-components/etcd-druid/proposals/00-template/#motivation>Motivation</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/00-template/#goals>Goals</a></li><li><a href=/docs/other-components/etcd-druid/proposals/00-template/#non-goals>Non-Goals</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/00-template/#proposal>Proposal</a></li><li><a href=/docs/other-components/etcd-druid/proposals/00-template/#alternatives>Alternatives</a></li></ul><h2 id=summary>Summary</h2><h2 id=motivation>Motivation</h2><h3 id=goals>Goals</h3><h3 id=non-goals>Non-Goals</h3><h2 id=proposal>Proposal</h2><h2 id=alternatives>Alternatives</h2></div><div class=td-content style=page-break-before:always><h1 id=pg-cf41fa42cf45a406ca97a89be81a729e>8 - etcd Network Latency</h1><h1 id=network-latency-analysis-sn-etcd-sz-vs--mn-etcd-sz-vs-mn-etcd-mz>Network Latency analysis: <code>sn-etcd-sz</code> vs <code>mn-etcd-sz</code> vs <code>mn-etcd-mz</code></h1><p>This page captures the etcd cluster latency analysis for below scenarios using the benchmark tool (build from <a href=https://github.com/seshachalam-yv/etcd>etcd benchmark tool</a>).</p><p><code>sn-etcd-sz</code> -> single-node etcd single zone (Only single replica of etcd will be running)</p><p><code>mn-etcd-sz</code> -> multi-node etcd single zone (Multiple replicas of etcd pods will be running across nodes in a single zone)</p><p><code>mn-etcd-mz</code> -> multi-node etcd multi zone (Multiple replicas of etcd pods will be running across nodes in multiple zones)</p><h2 id=put-analysis>PUT Analysis</h2><h3 id=summary>Summary</h3><ul><li><code>sn-etcd-sz</code> latency is <strong>~20% less than</strong> <code>mn-etcd-sz</code> when benchmark tool with single client.</li><li><code>mn-etcd-sz</code> latency is less than <code>mn-etcd-mz</code> but the difference is <code>~+/-5%</code>.</li><li>Compared to <code>mn-etcd-sz</code>, <code>sn-etcd-sz</code> latency is higher and gradually grows with more clients and larger value size.</li><li>Compared to <code>mn-etcd-mz</code>, <code>mn-etcd-sz</code> latency is higher and gradually grows with more clients and larger value size.</li><li><em>Compared to <code>follower</code>, <code>leader</code> latency is less</em>, when benchmark tool with single client for all cases.</li><li><em>Compared to <code>follower</code>, <code>leader</code> latency is high</em>, when benchmark tool with multiple clients for all cases.</li></ul><p>Sample commands:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># write to leader</span>
</span></span><span style=display:flex><span>benchmark put --target-leader --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0 --val-size=256 --total=10000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># write to follower</span>
</span></span><span style=display:flex><span>benchmark put  --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0 --val-size=256 --total=10000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_FOLLOWER_HOST
</span></span></code></pre></div><h3 id=latency-analysis-during-put-requests-to-etcd>Latency analysis during PUT requests to etcd</h3><ul><li><details><summary>In this case benchmark tool tries to put key with random 256 bytes value.</summary><ul><li><p>Benchmark tool loads key/value to <code>leader</code> with single client .</p><ul><li><code>sn-etcd-sz</code> latency (~0.815ms) is <strong>~50% lesser than</strong> <code>mn-etcd-sz</code> (~1.74ms ).</li><li><ul><li><code>mn-etcd-sz</code> latency (~1.74ms ) is slightly lesser than <code>mn-etcd-mz</code> (~1.8ms) but the difference is negligible (within same ms).</li></ul></li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>1220.0520</td><td style=text-align:center>0.815ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>586.545</td><td style=text-align:center>1.74ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>554.0155654442634</td><td style=text-align:center>1.8ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>follower</code> with single client.</p><ul><li><code>mn-etcd-sz</code> latency(<code>~2.2ms</code>) is <strong>20% to 30% lesser than</strong> <code>mn-etcd-mz</code>(<code>~2.7ms</code>).</li><li><em>Compare to <code>follower</code>, <code>leader</code> has lower latency.</em></li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>445.743</td><td style=text-align:center>2.23ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>378.9366747610789</td><td style=text-align:center>2.63ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>457.967</td><td style=text-align:center>2.17ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>345.6586129825796</td><td style=text-align:center>2.89ms</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>leader</code> with multiple clients.</p><ul><li><code>sn-etcd-sz</code> latency(<code>~78.3ms</code>) is <strong>~10% greater than</strong> <code>mn-etcd-sz</code>(<code>~71.81ms</code>).</li><li><code>mn-etcd-sz</code> latency(<code>~71.81ms</code>) is less than <code>mn-etcd-mz</code>(<code>~72.5ms</code>) but the difference is negligible.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>leader</td><td style=text-align:center>12638.905</td><td style=text-align:center>78.32ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>leader</td><td style=text-align:center>13789.248</td><td style=text-align:center>71.81ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>leader</td><td style=text-align:center>13728.446436395223</td><td style=text-align:center>72.5ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>follower</code> with multiple clients.</p><ul><li><code>mn-etcd-sz</code> latency(<code>~69.8ms</code>) is <strong>~5% greater than</strong> <code>mn-etcd-mz</code>(<code>~72.6ms</code>).</li><li><em>Compare to <code>leader</code>, <code>follower</code> has lower latency</em>.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>follower-1</td><td style=text-align:center>14271.983</td><td style=text-align:center>69.80ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>follower-1</td><td style=text-align:center>13695.98</td><td style=text-align:center>72.62ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>follower-2</td><td style=text-align:center>14325.436</td><td style=text-align:center>69.47ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>follower-2</td><td style=text-align:center>15750.409490407475</td><td style=text-align:center>63.3ms</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li></ul></details></li><li><details><summary>In this case benchmark tool tries to put key with random 1 MB value.</summary><ul><li><p>Benchmark tool loads key/value to <code>leader</code> with single client.</p><ul><li><code>sn-etcd-sz</code> latency(<code>~16.35ms</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-sz</code>(<code>~20.64ms</code>).</li><li><code>mn-etcd-sz</code> latency(<code>~20.64ms</code>) is less than <code>mn-etcd-mz</code>(<code>~21.08ms</code>) but the difference is negligible..</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>61.117</td><td style=text-align:center>16.35ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>48.416</td><td style=text-align:center>20.64ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>leader</td><td style=text-align:center>45.7517341664802</td><td style=text-align:center>21.08ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value withto <code>follower</code> single client.</p><ul><li><code>mn-etcd-sz</code> latency(<code>~23.10ms</code>) is <strong>~10% greater than</strong> <code>mn-etcd-mz</code>(<code>~21.8ms</code>).</li><li><em>Compare to <code>follower</code>, <code>leader</code> has lower latency</em>.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>43.261</td><td style=text-align:center>23.10ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>45.7517341664802</td><td style=text-align:center>21.8ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-1</td><td style=text-align:center>45.33</td><td style=text-align:center>22.05ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>40.0518</td><td style=text-align:center>24.95ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>43.28573155709838</td><td style=text-align:center>23.09ms</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>45.92</td><td style=text-align:center>21.76ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>follower-2</td><td style=text-align:center>35.5705</td><td style=text-align:center>28.1ms</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>leader</code> with multiple clients.</p><ul><li><code>sn-etcd-sz</code> latency(<code>~6.0375secs</code>) is <strong>~30% greater than</strong> <code>mn-etcd-sz``~4.000secs</code>).</li><li><code>mn-etcd-sz</code> latency(<code>~4.000secs</code>) is less than <code>mn-etcd-mz</code>(<code>~ 4.09secs</code>) but the difference is negligible.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>leader</td><td style=text-align:center>55.373</td><td style=text-align:center>6.0375secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>leader</td><td style=text-align:center>67.319</td><td style=text-align:center>4.000secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>leader</td><td style=text-align:center>65.91914167957594</td><td style=text-align:center>4.09secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool loads key/value to <code>follower</code> with multiple clients.</p><ul><li><em><code>mn-etcd-sz</code> latency(<code>~4.04secs</code>) is <strong>~5% greater than</strong> <code>mn-etcd-mz</code>(<code>~ 3.90secs</code>).</em></li><li><em>Compare to <code>leader</code>, <code>follower</code> has lower latency</em>.</li><li><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-1</td><td style=text-align:center>66.528</td><td style=text-align:center>4.0417secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-1</td><td style=text-align:center>70.6493461856332</td><td style=text-align:center>3.90secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-1</td><td style=text-align:center>71.95</td><td style=text-align:center>3.84secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of keys</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-2</td><td style=text-align:center>66.447</td><td style=text-align:center>4.0164secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-2</td><td style=text-align:center>67.53038086369484</td><td style=text-align:center>3.87secs</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>300</td><td style=text-align:center>follower-2</td><td style=text-align:center>68.46</td><td style=text-align:center>3.92secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li></ul></details></li></ul><hr><br><h2 id=range-analysis>Range Analysis</h2><p>Sample commands are:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Single connection read request with sequential keys</span>
</span></span><span style=display:flex><span>benchmark range 0 --target-leader --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0  --total=10000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=l <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span><span style=display:flex><span><span style=color:green># --consistency=s [Serializable]</span>
</span></span><span style=display:flex><span>benchmark range 0 --target-leader --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0  --total=10000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=s <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span><span style=display:flex><span><span style=color:green># Each read request with range query matches key 0 9999 and repeats for total number of requests.  </span>
</span></span><span style=display:flex><span>benchmark range 0 9999 --target-leader --conns=1 --clients=1 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --total=10 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=s <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=https://etcd-main-client:2379
</span></span><span style=display:flex><span><span style=color:green># Read requests with multiple connections</span>
</span></span><span style=display:flex><span>benchmark range 0 --target-leader --conns=100 --clients=1000 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0  --total=100000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=l <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span><span style=display:flex><span>benchmark range 0 --target-leader --conns=100 --clients=1000 --precise <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --sequential-keys --key-starts 0  --total=100000 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --consistency=s <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>    --endpoints=$ETCD_HOST 
</span></span></code></pre></div><h3 id=latency-analysis-during-range-requests-to-etcd>Latency analysis during Range requests to etcd</h3><ul><li><details><summary>In this case benchmark tool tries to get specific key with random 256 bytes value.</summary><ul><li><p>Benchmark tool range requests to <code>leader</code> with single client.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~1.24ms</code>) is <strong>~40% greater than</strong> <code>mn-etcd-sz</code>(<code>~0.67ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~0.67ms</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-mz</code>(<code>~0.85ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>800.272</td><td style=text-align:center>1.24ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>1173.9081</td><td style=text-align:center>0.67ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>999.3020189178693</td><td style=text-align:center>0.85ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~40% less</strong> for all cases</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>1411.229</td><td style=text-align:center>0.70ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>2033.131</td><td style=text-align:center>0.35ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>2100.2426362012025</td><td style=text-align:center>0.47ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> with single client .</p><ul><li><code>mn-etcd-sz</code> latency(<code>~1.3ms</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-mz</code>(<code>~1.6ms</code>).</li><li><em>Compare to <code>follower</code>, <code>leader</code> read request latency is <strong>~50% less</strong> for both <code>mn-etcd-sz</code>, <code>mn-etcd-mz</code></em></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>765.325</td><td style=text-align:center>1.3ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>596.1</td><td style=text-align:center>1.6ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~50% less</strong> for all cases</li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>1823.631</td><td style=text-align:center>0.54ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>1442.6</td><td style=text-align:center>0.69ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>1416.39</td><td style=text-align:center>0.70ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr><tr><td style=text-align:center>10000</td><td style=text-align:center>256</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>2077.449</td><td style=text-align:center>0.47ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>leader</code> with multiple client.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~84.66ms</code>) is <strong>~20% greater than</strong> <code>mn-etcd-sz</code>(<code>~73.95ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~73.95ms</code>) is <strong>more or less equal to</strong> <code>mn-etcd-mz</code>(<code>~ 73.8ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>11775.721</td><td style=text-align:center>84.66ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>13446.9598</td><td style=text-align:center>73.95ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>13527.19810605353</td><td style=text-align:center>73.8ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~20% lesser</strong> for all cases</p></li><li><p><code>sn-etcd-sz</code> latency(<code>~69.37ms</code>) is <strong>more or less equal to</strong> <code>mn-etcd-sz</code>(<code>~69.89ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~69.89ms</code>) is <strong>slightly higher than</strong> <code>mn-etcd-mz</code>(<code>~67.63ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>14334.9027</td><td style=text-align:center>69.37ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>14270.008</td><td style=text-align:center>69.89ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>14715.287354023869</td><td style=text-align:center>67.63ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> with multiple client.</p><ul><li><p><code>mn-etcd-sz</code> latency(<code>~60.69ms</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-mz</code>(<code>~70.76ms</code>).</p></li><li><p>Compare to <code>leader</code>, <code>follower</code> has lower read request latency.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>11586.032</td><td style=text-align:center>60.69ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>14050.5</td><td style=text-align:center>70.76ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p><code>mn-etcd-sz</code> latency(<code>~86.09ms</code>) is <strong>~20 higher than</strong> <code>mn-etcd-mz</code>(<code>~64.6ms</code>).</p></li><li><ul><li>Compare to <code>mn-etcd-sz</code> consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~20% higher</strong>.*</li></ul></li><li><p>Compare to <code>mn-etcd-mz</code> consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~slightly less</strong>.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>11582.438</td><td style=text-align:center>86.09ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>100000</td><td style=text-align:center>256</td><td style=text-align:center>100</td><td style=text-align:center>1000</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>15422.2</td><td style=text-align:center>64.6ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>leader</code> all keys.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~678.77ms</code>) is <strong>~5% slightly lesser than</strong> <code>mn-etcd-sz</code>(<code>~697.29ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~697.29ms</code>) is less than <code>mn-etcd-mz</code>(<code>~701ms</code>) but the difference is negligible.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>6.8875</td><td style=text-align:center>678.77ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>6.720</td><td style=text-align:center>697.29ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>6.7</td><td style=text-align:center>701ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><ul><li>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~5% slightly higher</strong> for all cases</li></ul></li><li><p><code>sn-etcd-sz</code> latency(<code>~687.36ms</code>) is less than <code>mn-etcd-sz</code>(<code>~692.68ms</code>) but the difference is negligible.</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~692.68ms</code>) is <strong>~5% slightly lesser than</strong> <code>mn-etcd-mz</code>(<code>~735.7ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>6.76</td><td style=text-align:center>687.36ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>6.635</td><td style=text-align:center>692.68ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>6.3</td><td style=text-align:center>735.7ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> all keys</p><ul><li><p><code>mn-etcd-sz</code>(<code>~737.68ms</code>) latency is <strong>~5% slightly higher than</strong> <code>mn-etcd-mz</code>(<code>~713.7ms</code>).</p></li><li><p>Compare to <code>leader</code> consistency <code>Linearizable</code>read request, <code>follower</code> is <em>~5% slightly higher</em>.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>6.163</td><td style=text-align:center>737.68ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>6.52</td><td style=text-align:center>713.7ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p><code>mn-etcd-sz</code> latency(<code>~757.73ms</code>) is <strong>~10% higher than</strong> <code>mn-etcd-mz</code>(<code>~690.4ms</code>).</p></li><li><p>Compare to <code>follower</code> consistency <code>Linearizable</code>read request, <code>follower</code> consistency <code>Serializable</code> is <em>~3% slightly higher</em> for <code>mn-etcd-sz</code>.</p></li><li><p><em>Compare to <code>follower</code> consistency <code>Linearizable</code>read request, <code>follower</code> consistency <code>Serializable</code> is <em>~5% less</em> for <code>mn-etcd-mz</code>.</em></p></li><li><p>*Compare to <code>leader</code> consistency <code>Serializable</code>read request, <code>follower</code> consistency <code>Serializable</code> is <em>~5% less</em> for <code>mn-etcd-mz</code>. *</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>6.0295</td><td style=text-align:center>757.73ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>256</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>6.87</td><td style=text-align:center>690.4ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li></ul><hr><br></details></li><li><details><summary>In this case benchmark tool tries to get specific key with random `1MB` value.</summary><ul><li><p>Benchmark tool range requests to <code>leader</code> with single client.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~5.96ms</code>) is <strong>~5% lesser than</strong> <code>mn-etcd-sz</code>(<code>~6.28ms</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~6.28ms</code>) is <strong>~10% higher than</strong> <code>mn-etcd-mz</code>(<code>~5.3ms</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>167.381</td><td style=text-align:center>5.96ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>158.822</td><td style=text-align:center>6.28ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>187.94</td><td style=text-align:center>5.3ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~15% less</strong> for <code>sn-etcd-sz</code>, <code>mn-etcd-sz</code>, <code>mn-etcd-mz</code></p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>184.95</td><td style=text-align:center>5.398ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>176.901</td><td style=text-align:center>5.64ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>209.99</td><td style=text-align:center>4.7ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> with single client.</p><ul><li><p><code>mn-etcd-sz</code> latency(<code>~6.66ms</code>) is <strong>~10% higher than</strong> <code>mn-etcd-mz</code>(<code>~6.16ms</code>).</p></li><li><p><em>Compare to <code>leader</code>, <code>follower</code> read request latency is <strong>~10% high</strong> for <code>mn-etcd-sz</code></em></p></li><li><p><em>Compare to <code>leader</code>, <code>follower</code> read request latency is <strong>~20% high</strong> for <code>mn-etcd-mz</code></em></p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>150.680</td><td style=text-align:center>6.66ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>162.072</td><td style=text-align:center>6.16ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~15% less</strong> for <code>mn-etcd-sz</code>(<code>~5.84ms</code>), <code>mn-etcd-mz</code>(<code>~5.01ms</code>).</p></li><li><p><em>Compare to <code>leader</code>, <code>follower</code> read request latency is <strong>~5% slightly high</strong> for <code>mn-etcd-sz</code>, <code>mn-etcd-mz</code></em></p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>170.918</td><td style=text-align:center>5.84ms</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>199.01</td><td style=text-align:center>5.01ms</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>leader</code> with multiple clients.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~1.593secs</code>) is <strong>~20% lesser than</strong> <code>mn-etcd-sz</code>(<code>~1.974secs</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~1.974secs</code>) is <strong>~5% greater than</strong> <code>mn-etcd-mz</code>(<code>~1.81secs</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>252.149</td><td style=text-align:center>1.593secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>205.589</td><td style=text-align:center>1.974secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>230.42</td><td style=text-align:center>1.81secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p><em>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>more or less same</strong> for <code>sn-etcd-sz</code>(<code>~1.57961secs</code>), <code>mn-etcd-mz</code>(<code>~1.8secs</code>) not a big difference</em></p></li><li><p>Compare to consistency <code>Linearizable</code>, <code>Serializable</code> is <strong>~10% high</strong> for <code>mn-etcd-sz</code>(<code>~ 2.277secs</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>252.406</td><td style=text-align:center>1.57961secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>181.905</td><td style=text-align:center>2.277secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>227.64</td><td style=text-align:center>1.8secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> with multiple client.</p><ul><li><p><code>mn-etcd-sz</code> latency is <strong>~20% less than</strong> <code>mn-etcd-mz</code>.</p></li><li><p>Compare to <code>leader</code> consistency <code>Linearizable</code>, <code>follower</code> read request latency is ~15 less for <code>mn-etcd-sz</code>(<code>~1.694secs</code>).</p></li><li><p>Compare to <code>leader</code> consistency <code>Linearizable</code>, <code>follower</code> read request latency is ~10% higher for <code>mn-etcd-sz</code>(<code>~1.977secs</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>248.489</td><td style=text-align:center>1.694secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>210.22</td><td style=text-align:center>1.977secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-2</td><td style=text-align:center>205.765</td><td style=text-align:center>1.967secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>l</td><td style=text-align:center>follower-2</td><td style=text-align:center>195.2</td><td style=text-align:center>2.159secs</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>231.458</td><td style=text-align:center>1.7413secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>214.80</td><td style=text-align:center>1.907secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-2</td><td style=text-align:center>183.320</td><td style=text-align:center>2.2810secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>1000</td><td style=text-align:center>1000000</td><td style=text-align:center>100</td><td style=text-align:center>500</td><td style=text-align:center>true</td><td style=text-align:center>s</td><td style=text-align:center>follower-2</td><td style=text-align:center>195.40</td><td style=text-align:center>2.164secs</td><td style=text-align:center>eu-west-1b</td><td style=text-align:center>etcd-main-2</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>leader</code> all keys.</p><ul><li><p><code>sn-etcd-sz</code> latency(<code>~8.993secs</code>) is <strong>~3% slightly lower than</strong> <code>mn-etcd-sz</code>(<code>~9.236secs</code>).</p></li><li><p><code>mn-etcd-sz</code> latency(<code>~9.236secs</code>) is <strong>~2% slightly lower than</strong> <code>mn-etcd-mz</code>(<code>~9.100secs</code>).</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>0.5139</td><td style=text-align:center>8.993secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>0.506</td><td style=text-align:center>9.236secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>leader</td><td style=text-align:center>0.508</td><td style=text-align:center>9.100secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>sn-etcd-sz</code>(<code>~9.secs</code>) is <strong>a slight difference <code>10ms</code></strong>.</p></li><li><p>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>mn-etcd-sz</code>(<code>~9.113secs</code>) is <strong>~1% less</strong>, not a big difference.</p></li><li><p>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>mn-etcd-mz</code>(<code>~8.799secs</code>) is <strong>~3% less</strong>, not a big difference.</p></li><li><p><code>sn-etcd-sz</code> latency(<code>~9.secs</code>) is <strong>~1% slightly less than</strong> <code>mn-etcd-sz</code>(<code>~9.113secs</code>).</p></li><li><p><em><code>mn-etcd-sz</code> latency(<code>~9.113secs</code>) is <strong>~3% slightly higher than</strong> <code>mn-etcd-mz</code>(<code>~8.799secs</code>)</em>.</p><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>0.51125</td><td style=text-align:center>9.0003secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>sn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>0.4993</td><td style=text-align:center>9.113secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>leader</td><td style=text-align:center>0.522</td><td style=text-align:center>8.799secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-1</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li><li><p>Benchmark tool range requests to <code>follower</code> all keys</p><ul><li><p><code>mn-etcd-sz</code> latency(<code>~9.065secs</code>) is <strong>~1% slightly higher than</strong> <code>mn-etcd-mz</code>(<code>~9.007secs</code>).</p></li><li><p>Compare to <code>leader</code> consistency <code>Linearizable</code>read request, <code>follower</code> is <em>~1% slightly higher</em> for both cases <code>mn-etcd-sz</code>, <code>mn-etcd-mz</code> .</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>0.512</td><td style=text-align:center>9.065secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>l</td><td style=text-align:center>follower-1</td><td style=text-align:center>0.533</td><td style=text-align:center>9.007secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li><li><p>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>mn-etcd-sz</code>(<code>~9.553secs</code>) is <strong>~5% high</strong>.</p></li><li><p><em>Compare to consistency <code>Linearizable</code>read request, <code>follower</code> for <code>mn-etcd-mz</code>(<code>~7.7433secs</code>) is <strong>~15% less</strong></em>.</p></li><li><p><em><code>mn-etcd-sz</code>(<code>~9.553secs</code>) latency is <strong>~20% higher than</strong> <code>mn-etcd-mz</code>(<code>~7.7433secs</code>)</em>.</p></li><li><table><thead><tr><th style=text-align:center>Number of requests</th><th style=text-align:center>Value size</th><th style=text-align:center>Number of connections</th><th style=text-align:center>Number of clients</th><th style=text-align:center>sequential-keys</th><th style=text-align:center>Consistency</th><th style=text-align:center>Target etcd server</th><th style=text-align:center>Average write QPS</th><th style=text-align:center>Average latency per request</th><th style=text-align:center>zone</th><th style=text-align:center>server name</th><th style=text-align:center>Test name</th></tr></thead><tbody><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>0.4743</td><td style=text-align:center>9.553secs</td><td style=text-align:center>eu-west-1a</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-sz</td></tr><tr><td style=text-align:center>20</td><td style=text-align:center>1000000</td><td style=text-align:center>2</td><td style=text-align:center>5</td><td style=text-align:center>false</td><td style=text-align:center>s</td><td style=text-align:center>follower-1</td><td style=text-align:center>0.5500</td><td style=text-align:center>7.7433secs</td><td style=text-align:center>eu-west-1c</td><td style=text-align:center>etcd-main-0</td><td style=text-align:center>mn-etcd-mz</td></tr></tbody></table></li></ul></li></ul><hr><br></details></li></ul><hr><br><br><blockquote><p>NOTE: This Network latency analysis is inspired by <a href=https://etcd.io/docs/v3.5/op-guide/performance/>etcd performance</a>.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-c2230bfa442d7dc5bd7c232f0253c256>9 - EtcdMember Custom Resource</h1><h1 id=dep-04-etcdmember-custom-resource>DEP-04: EtcdMember Custom Resource</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#dep-04-etcdmember-custom-resource>DEP-04: EtcdMember Custom Resource</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#table-of-contents>Table of Contents</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#summary>Summary</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#terminology>Terminology</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#motivation>Motivation</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#goals>Goals</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#non-goals>Non-Goals</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#proposal>Proposal</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#etcd-member-metadata>Etcd Member Metadata</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#etcd-member-state-transitions>Etcd Member State Transitions</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#states-and-sub-states>States and Sub-States</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#top-level-state-transitions>Top Level State Transitions</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#starting-an-etcd-member-in-a-single-node-etcd-cluster>Starting an Etcd-Member in a Single-Node Etcd Cluster</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#addition-of-a-new-etcd-member-in-a-multi-node-etcd-cluster>Addition of a New Etcd-Member in a Multi-Node Etcd Cluster</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#restart-of-a-voting-etcd-member-in-a-multi-node-etcd-cluster>Restart of a Voting Etcd-Member in a Multi-Node Etcd Cluster</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#deterministic-etcd-member-creationrestart-during-scale-up>Deterministic Etcd Member Creation/Restart During Scale-Up</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#tls-enablement-for-peer-communication>TLS Enablement for Peer Communication</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#monitoring-backup-health>Monitoring Backup Health</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#enhanced-snapshot-compaction>Enhanced Snapshot Compaction</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#enhanced-defragmentation>Enhanced Defragmentation</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#monitoring-defragmentations>Monitoring Defragmentations</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#monitoring-restorations>Monitoring Restorations</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#monitoring-volume-mismatches>Monitoring Volume Mismatches</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#custom-resource-api>Custom Resource API</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#spec-vs-status>Spec vs Status</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#representing-state-transitions>Representing State Transitions</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#reason-codes>Reason Codes</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#api>API</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#etcdmember>EtcdMember</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#etcd>Etcd</a></li></ul></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#lifecycle-of-an-etcdmember>Lifecycle of an EtcdMember</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#creation>Creation</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#updation>Updation</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#deletion>Deletion</a></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#reconciliation>Reconciliation</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#stale-etcdmember-status-handling>Stale EtcdMember Status Handling</a></li></ul></li></ul></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#reference>Reference</a></li></ul></li></ul><h2 id=summary>Summary</h2><p>Today, <a href=https://github.com/gardener/etcd-druid>etcd-druid</a> mainly acts as an etcd cluster provisioner, and seldom takes remediatory actions if the <a href=https://etcd.io/>etcd</a> cluster goes into an undesired state that needs to be resolved by a human operator. In other words, etcd-druid cannot perform day-2 operations on etcd clusters in its current form, and hence cannot carry out its full set of responsibilities as a true &ldquo;operator&rdquo; of etcd clusters. For etcd-druid to be fully capable of its responsibilities, it must know the latest state of the etcd clusters and their individual members at all times.</p><p>This proposal aims to bridge that gap by introducing <code>EtcdMember</code> custom resource allowing individual etcd cluster members to publish information/state (previously unknown to etcd-druid). This provides etcd-druid a handle to potentially take cluster-scoped remediatory actions.</p><h2 id=terminology>Terminology</h2><ul><li><p><strong>druid:</strong> <a href=https://github.com/gardener/etcd-druid>etcd-druid</a> - an operator for etcd clusters.</p></li><li><p><strong>etcd-member:</strong> A single etcd pod in an etcd cluster that is realised as a <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a>.</p></li><li><p><strong>backup-sidecar:</strong> It is the etcd-backup-restore sidecar container in each etcd-member pod.</p><blockquote><p><strong>NOTE:</strong> Term sidecar can now be confused with the latest definition in <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/753-sidecar-containers/README.md>KEP-73</a>. etcd-backup-restore container is currently not set as an <code>init-container</code> as proposed in the KEP but as a regular container in a multi-container [Pod](<a href=https://kubernetes.io/docs/concepts/workloads/pods/>Pods | Kubernetes</a>).</p></blockquote></li><li><p><strong>leading-backup-sidecar:</strong> A backup-sidecar that is associated to an etcd leader.</p></li><li><p><strong>restoration:</strong> It refers to an individual etcd-member restoring etcd data from an existing backup (comprising of full and delta snapshots). The authors have deliberately chosen to distinguish between restoration and learning. Learning refers to a process where a <a href=https://etcd.io/docs/v3.3/learning/learner/#raft-learner>learner</a> &ldquo;learns&rdquo; from an etcd-cluster leader.</p></li></ul><h2 id=motivation>Motivation</h2><p>Sharing state of an individual etcd-member with druid is essential for diagnostics, monitoring, cluster-wide-operations and potential remediation. At present, only a subset of etcd-member state is shared with druid using <a href=https://kubernetes.io/docs/concepts/architecture/leases/>leases</a>. It was always meant as a stopgap arrangement as mentioned in the <a href=https://github.com/gardener/etcd-druid/pull/207>corresponding issue</a> and is not the best use of leases.</p><p>There is a need to have a clear distinction between an etcd-member state and etcd cluster state since most of an etcd cluster state is often derived by looking at individual etcd-member states. In addition, actors which update each of these states should be clearly identified so as to prevent multiple actors updating a single resource holding the state of either an etcd cluster or an etcd-member. As a consequence, etcd-members should not directly update the <code>Etcd</code> resource status and would therefore need a new custom resource allowing each member to publish detailed information about its latest state.</p><h3 id=goals>Goals</h3><ul><li>Introduce <code>EtcdMember</code> custom resource via which each etcd-member can publish information about its state. This enables druid to deterministically orchestrate out-of-turn operations like compaction, defragmentation, volume management etc.</li><li>Define and capture states, sub-states and deterministic transitions amongst states of an etcd-member.</li><li>Today <a href=https://kubernetes.io/docs/concepts/architecture/leases/>leases</a> are <em>misused</em> to share member-specific information with druid. Their usage to share member state [leader, follower, learner], member-id, snapshot revisions etc should be removed.</li></ul><h3 id=non-goals>Non-Goals</h3><ul><li>Auto-recovery from quorum loss or cluster-split due to network partitioning.</li><li>Auto-recovery of an etcd-member due to volume mismatch.</li><li>Relooking at segregating responsiblities between <code>etcd</code> and <code>backup-sidecar</code> containers.</li></ul><h2 id=proposal>Proposal</h2><p>This proposal introduces a new custom resource <code>EtcdMember</code>, and in the following sections describes different sets of information that should be captured as part of the new resource.</p><h3 id=etcd-member-metadata>Etcd Member Metadata</h3><p>Every etcd-member has a unique <code>memberID</code> and it is part of an etcd cluster which has a unique <code>clusterID</code>. In a well-formed etcd cluster every member must have the same <code>clusterID</code>. Publishing this information to druid helps in identifying issues when one or more etcd-members form their own individual clusters, thus resulting in multiple clusters where only one was expected. Issues <a href=https://github.com/gardener/etcd-druid/issues/419>Issue#419</a>, Canary#4027, Canary#3973 are some such occurrences.</p><p>Today, this information is published by using a member <a href=https://kubernetes.io/docs/concepts/architecture/leases/>lease</a>. Both these fields are populated in the leases&rsquo; <code>Spec.HolderIdentity</code> by the backup-sidecar container.</p><p>The authors propose to publish member metadata information in <code>EtcdMember</code> resource.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>id: &lt;etcd-member id&gt;
</span></span><span style=display:flex><span>clusterID: &lt;etcd cluster id&gt;
</span></span></code></pre></div><blockquote><p><strong>NOTE:</strong> Druid would not do any auto-recovery when it finds out that there are more than one clusters being formed. Instead this information today will be used for diagnostic and alerting.</p></blockquote><h3 id=etcd-member-state-transitions>Etcd Member State Transitions</h3><p>Each etcd-member goes through different <code>States</code> during its lifetime. <code>State</code> is a derived high-level summary of where an etcd-member is in its lifecycle. A <code>SubState</code> gives additional information about the state. This proposal extends the concept of states with the notion of a <code>SubState</code>, since <code>State</code> indicates a top-level state of an <code>EtcdMember</code> resource, which can have one or more <code>SubState</code>s.</p><p>While <code>State</code> is sufficient for many human operators, the notion of a <code>SubState</code> provides operators with an insight about the discrete stage of an etcd-member in its lifecycle. For example, consider a top-level <code>State: Starting</code>, which indicates that an etcd-member is starting. <code>Starting</code> is meant to be a transient state for an etcd-member. If an etcd-member remains in this <code>State</code> longer than expected, then an operator would require additional insight, which the authors propose to provide via <code>SubState</code> (in this case, the possible <code>SubStates</code> could be <code>PendingLearner</code> and <code>Learner</code>, which are detailed in the following sections).</p><p>At present, these states are not captured and only the final state is known - i.e the etcd-member either fails to come up (all re-attempts to bring up the pod via the StatefulSet controller has exhausted) or it comes up. Getting an insight into all its state transitions would help in diagnostics.</p><p>The status of an etcd-member at any given point in time can be best categorized as a combination of a top-level <code>State</code> and a <code>SubState</code>. The authors propose to introduce the following states and sub-states:</p><h4 id=states-and-sub-states>States and Sub-States</h4><blockquote><p><strong>NOTE</strong>: Abbreviations have been used wherever possible, only to represent sub-states. These representations are chosen only for brevity and will have proper longer names.</p></blockquote><table><thead><tr><th>States</th><th>Sub-States</th><th>Description</th></tr></thead><tbody><tr><td>New</td><td>-</td><td>Every newly created etcd-member will start in this state and is termed as the initial state or the start state.</td></tr><tr><td>Initializing</td><td>DBV-S (DBValidationSanity)</td><td>This state denotes that backup-restore container in etcd-member pod has started initialization. Sub-State <code>DBV-S</code> which is an abbreviation for <code>DBValidationSanity</code> denotes that currently sanity etcd DB validation is in progress.</td></tr><tr><td>Initializing</td><td>DBV-F (DBValidationFull)</td><td>This state denotes that backup-restore container in etcd-member pod has started initialization. Sub-State <code>DBV-F</code> which is an abbreviation for <code>DBValidationFull</code> denotes that currently full etcd DB validation is in progress.</td></tr><tr><td>Initializing</td><td>R (Restoration)</td><td>This state denotes that backup-restore container in etcd-member pod has started initialization. Sub-State <code>R</code> which is an abbreviation for <code>Restoration</code> denotes that DB validation failed and now backup-restore has commenced restoration of etcd DB from the backup (comprising of full snapshot and delta-snapshots). An etcd-member will transition to this sub-state only when it is part of a single-node etcd-cluster.</td></tr><tr><td>Starting (SI)</td><td>PL (PendingLearner)</td><td>An etcd-member can transition from <code>Initializing</code> state to <code>PendingLearner</code> state. In this state backup-restore container will optionally delete any existing etcd data directory and then attempts to add its peer etcd-member process as a learner. Since there can be only one learner at a time in an etcd cluster, an etcd-member could be in this state for some time till its request to get added as a learner is accepted.</td></tr><tr><td>Starting (SI)</td><td>Learner</td><td>When backup-restore is successfully able to add its peer etcd-member process as a <code>Learner</code>. In this state the etcd-member process will start its DB sync from an etcd leader.</td></tr><tr><td>Started (Sd)</td><td>Follower</td><td>A follower is a voting raft member. A <code>Learner</code> etcd-member will get promoted to a <code>Follower</code> once its DB is in sync with the leader. It could also become a follower if during a re-election it loses leadership and transitions from being a <code>Leader</code> to <code>Follower</code>.</td></tr><tr><td>Started (Sd)</td><td>Leader</td><td>A leader is an etcd-member which will handle all client write requests and linearizable read requests. A member could transition to being a <code>Leader</code> from an existing <code>Follower</code> role due to winning a leader election or for a single node etcd cluster it directly transitions from <code>Initializing</code> state to <code>Leader</code> state as there is no other member.</td></tr></tbody></table><p>In the following sub-sections, the state transitions are categorized into several flows making it easier to grasp the different transitions.</p><h4 id=top-level-state-transitions>Top Level State Transitions</h4><p>Following DFA represents top level state transitions (without any representation of sub-states). As described in the table above there are 4 top level states:</p><ul><li><p><code>New</code>- this is a start state for all newly created etcd-members</p></li><li><p><code>Initializing</code> - In this state backup-restore will perform pre-requisite actions before it triggers the start of an etcd process. DB validation and optionally restoration is done in this state. Possible sub-states are: <code>DBValidationSanity</code>, <code>DBValidationFull</code> and <code>Restoration</code></p></li><li><p><code>Starting</code> - Once the optional initialization is done backup-restore will trigger the start of an etcd process. It can either directly go to <code>Learner</code> sub-state or wait for getting added as a learner and therefore be in <code>PendingLearner</code> sub-state.</p></li><li><p><code>Started</code> - In this state the etcd-member is a full voting member. It can either be in <code>Leader</code> or <code>Follower</code> sub-states.</p></li></ul><img title src=/__resources/04-top-level-state-transitions.excalidraw_0af3b5.png alt data-align=center><h4 id=starting-an-etcd-member-in-a-single-node-etcd-cluster>Starting an Etcd-Member in a Single-Node Etcd Cluster</h4><p>Following DFA represents the states, sub-states and transitions of a single etcd-member for a cluster that is bootstrapped from cluster size of 0 -> 1.</p><img title src=/__resources/04-state-transitions-bootstrap-0-1.excalidraw_5a2726.png alt data-align=center><h4 id=addition-of-a-new-etcd-member-in-a-multi-node-etcd-cluster>Addition of a New Etcd-Member in a Multi-Node Etcd Cluster</h4><p>Following DFA represents the states, sub-states and transitions of an etcd cluster which starts with having a single member (Leader) and then one or more new members are added which represents a scale-up of an etcd cluster from 1 -> n, where n is <a href=https://etcd.io/docs/v3.5/faq/#why-an-odd-number-of-cluster-members>odd</a>.</p><img title src=/__resources/04-state-transitions-scaleup-1-n.excalidraw_cfa4bf.png alt data-align=center><h4 id=restart-of-a-voting-etcd-member-in-a-multi-node-etcd-cluster>Restart of a Voting Etcd-Member in a Multi-Node Etcd Cluster</h4><p>Following DFA represents the states, sub-states and transitions when a voting etcd-member in a multi-node etcd cluster restarts.</p><img title src=/__resources/04-state-transitions-restart-member.excalidraw_d23a8d.png alt data-align=center><blockquote><p>NOTE: If the DB validation fails then data directory of the etcd-member is removed and etcd-member is removed from cluster membership, thus transitioning it to <code>New</code> state. The state transitions from <code>New</code> state are depicted by <a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#addition-of-a-new-etcd-member-in-a-multi-node-etcd-cluster>this section</a>.</p></blockquote><h3 id=deterministic-etcd-member-creationrestart-during-scale-up>Deterministic Etcd Member Creation/Restart During Scale-Up</h3><p><strong>Bootstrap information:</strong></p><p>When an etcd-member starts, then it needs to find out:</p><ul><li><p>If it should join an existing cluster or start a new cluster.</p></li><li><p>If it should add itself as a <code>Learner</code> or directly start as a voting member.</p></li></ul><p><strong>Issue with the current approach</strong>:</p><p><em>At present</em>, this is facilitated by three things:</p><ol><li><p>During scale-up, druid adds an annotation <code>gardener.cloud/scaled-to-multi-node</code> to the <code>StatefulSet</code>. Each etcd-members looks up this annotation.</p></li><li><p>backup-sidecar attempts to fetch etcd cluster member-list and checks if this etcd-member is already part of the cluster.</p></li><li><p>Size of the cluster by checking <code>initial-cluster</code> in the etcd config.</p></li></ol><p>Druid adds an annotation <code>gardener.cloud/scaled-to-multi-node</code> on the <code>StatefulSet</code> which is then shared by all etcd-members irrespective of the starting state of an etcd-member (as <code>Learner</code> or <code>Voting-Member</code>). This especially creates an issue for the current leader (often pod with index 0) during the scale-up of an etcd cluster as described in <a href=https://github.com/gardener/etcd-backup-restore/issues/646>this</a> issue.</p><p>It has been agreed that the current solution to <a href=https://github.com/gardener/etcd-backup-restore/issues/646>this issue</a> is a quick and dirty fix and needs to be revisited to be uniformly applied to all etcd-members. The authors propose to provide a more deterministic approach to scale-up using the <code>EtcdMember</code> resource.</p><p><strong>New approach</strong></p><p>Instead of adding an annotation <code>gardener.cloud/scaled-to-multi-node</code> on the <code>StatefulSet</code>, a new annotation <code>druid.gardener.cloud/create-as-learner</code> should be added by druid on an <code>EtcdMember</code> resource. This annotation will only be added to newly created members during scale-up.</p><p>Each etcd-member should look at the following to deterministically compute the <code>bootstrap information</code> specified above:</p><ul><li><p><code>druid.gardener.cloud/create-as-learner</code> annotation on its respective <code>EtcdMember</code> resource. This new annotation will be honored in the following cases:</p><ul><li><p>When an etcd-member is created for the very first time.</p></li><li><p>An etcd-member is restarted while it is in <code>Starting</code> state (<code>PendingLearner</code> and <code>Learner</code> sub-states).</p></li></ul></li><li><p>Etcd-cluster member list. to check if it is already part of the cluster.</p></li><li><p>Existing etcd data directory and its validity.</p></li></ul><blockquote><p><strong>NOTE:</strong> When the etcd-member gets promoted to a voting-member, then it should remove the annotation on its respective <code>EtcdMember</code> resource.</p></blockquote><h3 id=tls-enablement-for-peer-communication>TLS Enablement for Peer Communication</h3><p>Etcd-members in a cluster use <a href=https://etcd.io/docs/v3.4/op-guide/configuration/#--initial-advertise-peer-urls>peer URL(s)</a> to communicate amongst each other. If the advertised peer URL(s) for an etcd-member are updated then <a href=https://etcd.io/docs/v3.4/op-guide/runtime-configuration/#update-advertise-peer-urls>etcd mandates a restart</a> of the etcd-member.</p><p>Druid only supports toggling the transport level security for the advertised peer URL(s). To indicate that the etcd process within the etcd-member has the updated advertised peer URL(s), an annotation <code>member.etcd.gardener.cloud/tls-enabled</code> is added by backup-sidecar container to the member lease object.</p><p>During the reconciliation run for an <code>Etcd</code> resource in druid, if reconciler detects a change in advertised peer URL(s) TLS configuration then it will watch for the above mentioned annotation on the member lease. If the annotation has a value of <code>false</code> then it will trigger a restart of the etcd-member pod.</p><p>The authors propose to publish member metadata information in <code>EtcdMember</code> resource and not misuse member <a href=https://kubernetes.io/docs/concepts/architecture/leases/>lease</a>s.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>peerTLSEnabled: &lt;bool&gt;
</span></span></code></pre></div><h3 id=monitoring-backup-health>Monitoring Backup Health</h3><p>Backup-sidecar takes delta and full snapshot both periodically and threshold based. These backed-up snapshots are essential for restoration operations for bootstrapping an etcd cluster from 0 -> 1 replicas. It is essential that leading-backup-sidecar container which is responsible for taking delta/full snapshots and uploading these snapshots to the configured backup store, publishes this information for druid to consume.</p><p>At present, information about backed-up snapshot (<em>only</em> <code>latest-revision-number</code>) is published by leading-backup-sidecar container by updating <code>Spec.HolderIdentity</code> of the delta-snapshot and full-snapshot <a href=https://kubernetes.io/docs/concepts/architecture/leases/>lease</a>s.</p><p>Druid maintains <code>conditions</code> in the <code>Etcd</code> resource status, which include but are not limited to maintaining information on whether backups being taken for an etcd cluster are healthy (up-to-date) or stale (outdated in context to a configured schedule). Druid computes these conditions using information from full/delta snapshot leases.</p><p>In order to provide a holistic view of the health of backups to human operators, druid requires additional information about the snapshots that are being backed-up. The authors propose to not misuse <a href=https://kubernetes.io/docs/concepts/architecture/leases/>lease</a>s and instead publish the following snapshot information as part <code>EtcdMember</code> custom resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>snapshots:
</span></span><span style=display:flex><span>  lastFull:
</span></span><span style=display:flex><span>    timestamp: &lt;time of full snapshot&gt;
</span></span><span style=display:flex><span>    name: &lt;name of the file that is uploaded&gt;
</span></span><span style=display:flex><span>    size: &lt;size of the un-compressed snapshot file uploaded&gt;
</span></span><span style=display:flex><span>    startRevision: &lt;start revision of etcd db captured in the snapshot&gt;
</span></span><span style=display:flex><span>    endRevision: &lt;end revision of etcd db captured in the snapshot&gt;
</span></span><span style=display:flex><span>  lastDelta:
</span></span><span style=display:flex><span>    timestamp: &lt;time of delta snapshot&gt;
</span></span><span style=display:flex><span>    name: &lt;name of the file that is uploaded&gt;
</span></span><span style=display:flex><span>    size: &lt;size of the un-compressed snapshot file uploaded&gt;
</span></span><span style=display:flex><span>    startRevision: &lt;start revision of etcd db captured in the snapshot&gt;
</span></span><span style=display:flex><span>    endRevision: &lt;end revision of etcd db captured in the snapshot&gt;
</span></span></code></pre></div><p>While this information will primarily help druid compute accurate conditions regarding backup health from snapshot information and publish this to human operators, it could be further utilised by human operators to take remediatory actions (e.g. manually triggering a full or delta snapshot or further restarting the leader if the issue is still not resolved) if backup is unhealthy.</p><h3 id=enhanced-snapshot-compaction>Enhanced Snapshot Compaction</h3><p>Druid can be configured to perform regular snapshot compactions for etcd clusters, to reduce the total number of delta snapshots to be restored if and when a DB restoration for an etcd cluster is required. Druid triggers a snapshot compaction job when the accumulated etcd events in the latest set of delta snapshots (taken after the last full snapshot) crosses a specified threshold.</p><p>As described in <a href=https://github.com/gardener/etcd-druid/issues/591>Issue#591</a> scheduling compaction only based on number of accumulated etcd events is not sufficient to ensure a successful compaction. This is specifically targeted for kubernetes clusters where each etcd event is larger in size owing to large spec or status fields or respective resources.</p><p>Druid will now need information regarding snapshot sizes, and more importantly the total size of accumulated delta snapshots since the last full snapshot.</p><p>The authors propose to enhance the proposed <code>snapshots</code> field described in <a href=/docs/other-components/etcd-druid/proposals/04-etcd-member-custom-resource/#use-case-3-monitoring-backup-health>Use Case #3</a> with the following additional field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>snapshots:
</span></span><span style=display:flex><span>  accumulatedDeltaSize: &lt;total size of delta snapshots since last full snapshot&gt;
</span></span></code></pre></div><p>Druid can then use this information in addition to the existing revision information to decide to trigger an early snapshot compaction job. This effectively allows druid to be proactive in performing regular compactions for etcds receiving large events, reducing the probability of a failed snapshot compaction or restoration.</p><h3 id=enhanced-defragmentation>Enhanced Defragmentation</h3><p>Reader is recommended to read <a href=https://etcd.io/docs/v3.4/op-guide/maintenance/>Etcd Compaction & Defragmentation</a> in order to understand the following terminology:</p><p><a href=https://github.com/etcd-io/etcd/blob/a603c0798948941d453b158af794edab1a8230be/mvcc/backend/backend.go#L60-L64><code>dbSize</code></a> - total storage space used by the etcd database</p><p><a href=https://github.com/etcd-io/etcd/blob/a603c0798948941d453b158af794edab1a8230be/mvcc/backend/backend.go#L65-L68><code>dbSizeInUse</code></a> - logical storage space used by the etcd database, not accounting for free pages in the DB due to etcd history compaction</p><p>The leading-backup-sidecar performs periodic defragmentations of the DBs of all the etcd-members in the cluster, controlled via a defragmentation cron schedule provided to each backup-sidecar. Defragmentation is a costly maintenance operation and causes a brief downtime to the etcd-member being defragmented, due to which the leading-backup-sidecar defragments each etcd-member sequentially. This ensures that only one etcd-member would be unavailable at any given time, thus avoiding an accidental quorum loss in the etcd cluster.</p><p>The authors propose to move the responsibility of orchestrating these individual defragmentations to druid due to the following reasons:</p><ul><li>Since each backup-sidecar only has knowledge of the health of its own etcd, it can only determine whether its own etcd can be defragmented or not, based on etcd-member health. Trying to defragment a different healthy etcd-member while another etcd-member is unhealthy would lead to a transient quorum loss.</li><li>Each backup-sidecar is only a <code>sidecar</code> to its own etcd-member, and by good design principles, it must not be performing any cluster-wide maintenance operations, and this responsibility should remain with the etcd cluster operator.</li></ul><p>Additionally, defragmentation of an etcd DB becomes inevitable if the DB size exceeds the specified DB space quota, since the etcd DB then becomes read-only, ie no write operations on the etcd would be possible unless the etcd DB is defragmented and storage space is freed up. In order to automate this, druid will now need information about the etcd DB size from each member, specifically the leading etcd-member, so that a cluster-wide defragmentation can be triggered if the DB size reaches a certain threshold, as already described by <a href=https://github.com/gardener/etcd-backup-restore/issues/556>this issue</a>.</p><p>The authors propose to enhance each etcd-member to regularly publish information about the <code>dbSize</code> and <code>dbSizeInUse</code> so that druid may trigger defragmentation for the etcd cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>dbSize: &lt;db-size&gt; <span style=color:green># e.g 6Gi</span>
</span></span><span style=display:flex><span>dbSizeInUse: &lt;db-size-in-use&gt; <span style=color:green># e.g 3.5Gi</span>
</span></span></code></pre></div><p>Difference between <code>dbSize</code> and <code>dbSizeInUse</code> gives a clear indication of how much storage space would be freed up if a defragmentation is performed. If the difference is not significant (based on a configurable threshold provided to druid), then no defragmentation should be performed. This will ensure that druid does not perform frequent defragmentations that do not yield much benefit. Effectively it is to maximise the benefit of defragmentation since this operations involves transient downtime for each etcd-member.</p><h3 id=monitoring-defragmentations>Monitoring Defragmentations</h3><p>As discussed in the previous section, every etcd-member is defragmented periodically, and can also be defragmented based on the DB size reaching a certain threshold. It is beneficial for druid to have knowledge of this data from each etcd-member for the following reasons:</p><ul><li><p>[<strong>Diagnostics</strong>] It is expected that <code>backup-sidecar</code> will push releveant metrics and configure alerts on these metrics.</p></li><li><p>[<strong>Operational</strong>] Derive status of defragmentation at etcd cluster level. In case of partial failures for a subset of etcd-members druid can potentially re-trigger defragmentation only for those etcd-members.</p></li></ul><p>The authors propose to capture this information as part of <code>lastDefragmentation</code> section in the <code>EtcdMember</code> resource.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>lastDefragmentation:
</span></span><span style=display:flex><span>  startTime: &lt;start time of defragmentation&gt;
</span></span><span style=display:flex><span>  endTime: &lt;end time of defragmentation&gt;
</span></span><span style=display:flex><span>  status: &lt;Succeeded | Failed&gt;
</span></span><span style=display:flex><span>  message: &lt;success or failure message&gt;
</span></span><span style=display:flex><span>  initialDBSize: &lt;size of etcd DB prior to defragmentation&gt;
</span></span><span style=display:flex><span>  finalDBSize: &lt;size of etcd DB post defragmentation&gt;
</span></span></code></pre></div><blockquote><p><strong>NOTE</strong>: Defragmentation is a cluster-wide operation, and insights derived from aggregating defragmentation data from individual etcd-members would be captured in the <code>Etcd</code> resource status</p></blockquote><h3 id=monitoring-restorations>Monitoring Restorations</h3><p>Each etcd-member may perform restoration of data multiple times throughout its lifecycle, possibly owing to data corruptions. It would be useful to capture this information as part of an <code>EtcdMember</code> resource, for the following use cases:</p><ul><li><p>[<strong>Diagnostics</strong>] It is expected that <code>backup-sidecar</code> will push a metric indicating failure to restore.</p></li><li><p>[<strong>Operational</strong>] Restoration from backup-bucket only happens for a single node etcd cluster. If restoration is failing then druid cannot take any remediatory actions since there is no etcd quorum.</p></li></ul><p>The authors propose to capture this information under <code>lastRestoration</code> section in the <code>EtcdMember</code> resource.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>lastRestoration:
</span></span><span style=display:flex><span>  status: &lt;Failed | Success | In-Progress&gt;
</span></span><span style=display:flex><span>  reason: &lt;reason-code for status&gt;
</span></span><span style=display:flex><span>  message: &lt;human readable message for status&gt;
</span></span><span style=display:flex><span>  startTime: &lt;start time of restoration&gt;
</span></span><span style=display:flex><span>  endTime: &lt;end time of restoration&gt;
</span></span></code></pre></div><p>Authors have considered the following cases to better understand how errors during restoration will be handled:</p><p><strong>Case #1</strong> - <em>Failure to connect to Provider Object Store</em></p><p>At present full and delta snapshots are downloaded during restoration. If there is a failure then initialization status transitions to <code>Failed</code> followed by <code>New</code> which forces <code>etcd-wrapper</code> to trigger the initialization again. This in a way forces a retry and currently there is no limit on the number of attempts.</p><p>Authors propose to improve the retry logic but keep the overall behavior of not forcing a container restart the same.</p><p><strong>Case #2</strong> - <em>Read-Only Mounted volume</em></p><p>If a mounted volume which is used to create the etcd data directory turns <code>read-only</code> then authors propose to capture this state via <code>EtcdMember</code>.</p><p>Authors propose that <code>druid</code> should initiate recovery by deleting the PVC for this etcd-member and letting <code>StatefulSet</code> controller re-create the Pod and the PVC. Removing PVC and deleting the pod is considered safe because:</p><ul><li>Data directory is present and is the DB is corrupt resulting in an un-usasble etcd.</li><li>Data directory is not present but any attempt to create a directory structure fails due to <code>read-only</code> FS.</li></ul><p>In both these cases there is no side-effect of deleting the PVC and the Pod.</p><p><strong>Case #3</strong> - <em>Revision mismatch</em></p><p>There is currently an issue in <code>backup-sidecar</code> which results in a revision mismatch in the snapshots (full/delta) taken by leading the <code>backup-sidecar</code> container. This results in a restoration failure. One occurance of such issue has been captured in <a href=https://github.com/gardener/etcd-backup-restore/issues/583>Issue#583</a>. This occurence points to a bug which should be fixed however there is a rare possibility that these snapshots (full/delta) get corrupted. In this rare situation, <code>backup-sidecar</code> should only raise an alert.</p><p>Authors propose that <code>druid</code> should not take any remediatory actions as this involves:</p><ul><li>Inspecting snapshots</li><li>If the full snapshot is corrupt then a decision needs to be taken to recover from the last full snapshot as the base snapshot. This can result in data loss and therefore needs manual intervention.</li><li>If a delta snapshot is corrupt, then recovery can be done till the corrupt revision in the delta snapshot. Since this will also result in a loss of data therefore this decision needs to be take by an operator.</li></ul><h3 id=monitoring-volume-mismatches>Monitoring Volume Mismatches</h3><p>Each etcd-member checks for possible etcd data volume mismatches, based on which it decides whether to start the etcd process or not, but this information is not captured anywhere today. It would be beneficial to capture this information as part of the <code>EtcdMember</code> resource so that a human operator may check this and manually fix the underlying problem with the wrong volume being attached or mounted to an etcd-member pod.</p><p>The authors propose to capture this information under <code>volumeMismatches</code> section in the <code>EtcdMember</code> resource.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>volumeMismatches:
</span></span><span style=display:flex><span>- identifiedAt: &lt;time at which wrong volume mount was identified&gt;
</span></span><span style=display:flex><span>  fixedAt: &lt;time at which correct volume was mounted&gt;
</span></span><span style=display:flex><span>  volumeID: &lt;volume ID of wrong volume that got mounted&gt;
</span></span><span style=display:flex><span>  numRestarts: &lt;num of etcd-member restarts that were attempted&gt;
</span></span></code></pre></div><p>Each entry under <code>volumeMismatches</code> will be for a unique <code>volumeID</code>. If there is a pod restart and it results in yet another unexpected <code>volumeID</code> (different from the already captured volumeIDs) then a new entry will get created. <code>numRestarts</code> denotes the number of restarts seen by the etcd-member for a specific <code>volumeID</code>.</p><p>Based on information from the <code>volumeMismatches</code> section, druid <em>may</em> choose to perform rudimentary remediatory actions as simple as restarting the member pod to force a possible rescheduling of the pod to a different node which could potentially force the correct volume to be mounted to the member.</p><h3 id=custom-resource-api>Custom Resource API</h3><h4 id=spec-vs-status>Spec vs Status</h4><p>Information that is captured in the etcd-member custom resource could be represented either as <code>EtcdMember.Status</code> or <code>EtcdMemberState.Spec</code>.</p><p>Gardener has a similar need to capture a shoot state and they have taken the decision to represent it via <a href=https://github.com/gardener/gardener/blob/9e1a20aa9cc32fc806123003ba6079b284948767/pkg/apis/core/types_shootstate.go#L27-L33>ShootState</a> resource where the state or status of a shoot is captured as part of the <code>Spec</code> field in the <code>ShootState</code> custom resource.</p><p>The authors wish to instead align themselves with the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>K8S API conventions</a> and choose to use <code>EtcdMember</code> custom resource and capture the status of each member in <code>Status</code> field of this resource. This has the following advantages:</p><ul><li><p><code>Spec</code> represents a desired state of a resource and what is intended to be captured is the <code>As-Is</code> state of a resource which <code>Status</code> is meant to capture. Therefore, semantically using <code>Status</code> is the correct choice.</p></li><li><p>Not mis-using <code>Spec</code> now to represent <code>As-Is</code> state provides us with a choice to extend the custom resource with any future need for a <code>Spec</code> a.k.a desired state.</p></li></ul><h4 id=representing-state-transitions>Representing State Transitions</h4><p>The authors propose to use a custom representation for states, sub-states and transitions.</p><p>Consider the following representation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>transitions:
</span></span><span style=display:flex><span>- state: &lt;name of the state that the etcd-member has transitioned to&gt;
</span></span><span style=display:flex><span>  subState: &lt;name of the sub-state if any&gt;
</span></span><span style=display:flex><span>  reason: &lt;reason code for the transition&gt;
</span></span><span style=display:flex><span>  transitionTime: &lt;time of transition to this state&gt;
</span></span><span style=display:flex><span>  message: &lt;detailed message if any&gt;
</span></span></code></pre></div><p>As an example, consider the following transitions which represent addition of an etcd-member during scale-up of an etcd cluster, followed by a restart of the etcd-member which detects a corrupt DB:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  transitions:
</span></span><span style=display:flex><span>  - state: New
</span></span><span style=display:flex><span>    subState: New
</span></span><span style=display:flex><span>    reason: ClusterScaledUp
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T05:00:00Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;New member added due to etcd cluster scale-up&#34;</span>
</span></span><span style=display:flex><span>  - state: Starting
</span></span><span style=display:flex><span>    subState: PendingLearner
</span></span><span style=display:flex><span>    reason: WaitingToJoinAsLearner
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T05:00:30Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Waiting to join the cluster as a learner&#34;</span>
</span></span><span style=display:flex><span>  - state: Starting
</span></span><span style=display:flex><span>    subState: Learner
</span></span><span style=display:flex><span>    reason: JoinedAsLearner
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T05:01:20Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Joined the cluster as a learner&#34;</span>
</span></span><span style=display:flex><span>  - state: Started
</span></span><span style=display:flex><span>    subState: Follower
</span></span><span style=display:flex><span>    reason: PromotedAsVotingMember
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T05:02:00Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Now in sync with leader, promoted as voting member&#34;</span>
</span></span><span style=display:flex><span>  - state: Initializing
</span></span><span style=display:flex><span>    subState: DBValidationFull
</span></span><span style=display:flex><span>    reason: DetectedPreviousUncleanExit
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T08:00:00Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Detected previous unclean exit, requires full DB validation&#34;</span>
</span></span><span style=display:flex><span>  - state: New
</span></span><span style=display:flex><span>    subState: New
</span></span><span style=display:flex><span>    reason: DBCorruptionDetected
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T08:01:30Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Detected DB corruption during initialization, removing member from cluster&#34;</span>
</span></span><span style=display:flex><span>  - state: Starting
</span></span><span style=display:flex><span>    subState: PendingLearner
</span></span><span style=display:flex><span>    reason: WaitingToJoinAsLearner
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T08:02:10Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Waiting to join the cluster as a learner&#34;</span>
</span></span><span style=display:flex><span>  - state: Starting
</span></span><span style=display:flex><span>    subState: Learner
</span></span><span style=display:flex><span>    reason: JoinedAsLearner
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T08:02:20Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Joined the cluster as a learner&#34;</span>
</span></span><span style=display:flex><span>  - state: Started
</span></span><span style=display:flex><span>    subState: Follower
</span></span><span style=display:flex><span>    reason: PromotedAsVotingMember
</span></span><span style=display:flex><span>    transitionTime: <span style=color:#a31515>&#34;2023-07-17T08:04:00Z&#34;</span>
</span></span><span style=display:flex><span>    message: <span style=color:#a31515>&#34;Now in sync with leader, promoted as voting member&#34;</span>
</span></span></code></pre></div><h5 id=reason-codes>Reason Codes</h5><p>The authors propose the following list of possible reason codes for transitions. This list is not exhaustive, and can be further enhanced to capture any new transitions in the future.</p><table><thead><tr><th>Reason</th><th>Transition From State (SubState)</th><th>Transition To State (SubState)</th></tr></thead><tbody><tr><td><code>ClusterScaledUp</code> | <code>NewSingleNodeClusterCreated</code></td><td>nil</td><td>New</td></tr><tr><td><code>DetectedPreviousCleanExit</code></td><td>New | Started (Leader) | Started (Follower)</td><td>Initializing (DBValidationSanity)</td></tr><tr><td><code>DetectedPreviousUncleanExit</code></td><td>New | Started (Leader) | Started (Follower)</td><td>Initializing (DBValidationFull)</td></tr><tr><td><code>DBValidationFailed</code></td><td>Initializing (DBValidationSanity) | Initializing (DBValidationFull)</td><td>Initializing (Restoration) | New</td></tr><tr><td><code>DBValidationSucceeded</code></td><td>Initializing (DBValidationSanity) | Initializing (DBValidationFull)</td><td>Started (Leader) | Started (Follower)</td></tr><tr><td><code>Initializing (Restoration)Succeeded</code></td><td>Initializing (Restoration)</td><td>Started (Leader)</td></tr><tr><td><code>WaitingToJoinAsLearner</code></td><td>New</td><td>Starting (PendingLearner)</td></tr><tr><td><code>JoinedAsLearner</code></td><td>Starting (PendingLearner)</td><td>Starting (Learner)</td></tr><tr><td><code>PromotedAsVotingMember</code></td><td>Starting (Learner)</td><td>Started (Follower)</td></tr><tr><td><code>GainedClusterLeadership</code></td><td>Started (Follower)</td><td>Started (Leader)</td></tr><tr><td><code>LostClusterLeadership</code></td><td>Started (Leader)</td><td>Started (Follower)</td></tr></tbody></table><h4 id=api>API</h4><h5 id=etcdmember>EtcdMember</h5><p>The authors propose to add the <code>EtcdMember</code> custom resource API to etcd-druid APIs and initially introduce it with <code>v1alpha1</code> version.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: druid.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: EtcdMember
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    gardener.cloud/owned-by: &lt;name of parent Etcd resource&gt;
</span></span><span style=display:flex><span>  name: &lt;name of the etcd-member&gt;
</span></span><span style=display:flex><span>  namespace: &lt;namespace | will be the same as that of parent Etcd resource&gt;
</span></span><span style=display:flex><span>  ownerReferences:
</span></span><span style=display:flex><span>  - apiVersion: druid.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    blockOwnerDeletion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    controller: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    kind: Etcd
</span></span><span style=display:flex><span>    name: &lt;name of the parent Etcd resource&gt;
</span></span><span style=display:flex><span>    uid: &lt;UID of the parent Etcd resource&gt; 
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  id: &lt;etcd-member id&gt;
</span></span><span style=display:flex><span>  clusterID: &lt;etcd cluster id&gt;
</span></span><span style=display:flex><span>  peerTLSEnabled: &lt;bool&gt;
</span></span><span style=display:flex><span>  dbSize: &lt;db-size&gt;
</span></span><span style=display:flex><span>  dbSizeInUse: &lt;db-size-in-use&gt;
</span></span><span style=display:flex><span>  snapshots:
</span></span><span style=display:flex><span>    lastFull:
</span></span><span style=display:flex><span>      timestamp: &lt;time of full snapshot&gt;
</span></span><span style=display:flex><span>      name: &lt;name of the file that is uploaded&gt;
</span></span><span style=display:flex><span>      size: &lt;size of the un-compressed snapshot file uploaded&gt;
</span></span><span style=display:flex><span>      startRevision: &lt;start revision of etcd db captured in the snapshot&gt;
</span></span><span style=display:flex><span>      endRevision: &lt;end revision of etcd db captured in the snapshot&gt;
</span></span><span style=display:flex><span>    lastDelta:
</span></span><span style=display:flex><span>      timestamp: &lt;time of delta snapshot&gt;
</span></span><span style=display:flex><span>      name: &lt;name of the file that is uploaded&gt;
</span></span><span style=display:flex><span>      size: &lt;size of the un-compressed snapshot file uploaded&gt;
</span></span><span style=display:flex><span>      startRevision: &lt;start revision of etcd db captured in the snapshot&gt;
</span></span><span style=display:flex><span>      endRevision: &lt;end revision of etcd db captured in the snapshot&gt;
</span></span><span style=display:flex><span>    accumulatedDeltaSize: &lt;total size of delta snapshots since last full snapshot&gt;
</span></span><span style=display:flex><span>  lastRestoration:
</span></span><span style=display:flex><span>    type: &lt;FromSnapshot | FromLeader&gt;
</span></span><span style=display:flex><span>    status: &lt;Failed | Success | In-Progress&gt;
</span></span><span style=display:flex><span>    startTime: &lt;start time of restoration&gt;
</span></span><span style=display:flex><span>    endTime: &lt;end time of restoration&gt;
</span></span><span style=display:flex><span>  lastDefragmentation:
</span></span><span style=display:flex><span>    startTime: &lt;start time of defragmentation&gt;
</span></span><span style=display:flex><span>    endTime: &lt;end time of defragmentation&gt;
</span></span><span style=display:flex><span>    reason: 
</span></span><span style=display:flex><span>    message:
</span></span><span style=display:flex><span>    initialDBSize: &lt;size of etcd DB prior to defragmentation&gt;
</span></span><span style=display:flex><span>    finalDBSize: &lt;size of etcd DB post defragmentation&gt;
</span></span><span style=display:flex><span>  volumeMismatches:
</span></span><span style=display:flex><span>  - identifiedAt: &lt;time at which wrong volume mount was identified&gt;
</span></span><span style=display:flex><span>    fixedAt: &lt;time at which correct volume was mounted&gt;
</span></span><span style=display:flex><span>    volumeID: &lt;volume ID of wrong volume that got mounted&gt;
</span></span><span style=display:flex><span>    numRestarts: &lt;num of pod restarts that were attempted&gt;
</span></span><span style=display:flex><span>  transitions:
</span></span><span style=display:flex><span>  - state: &lt;name of the state that the etcd-member has transitioned to&gt;
</span></span><span style=display:flex><span>    subState: &lt;name of the sub-state if any&gt;
</span></span><span style=display:flex><span>    reason: &lt;reason code for the transition&gt;
</span></span><span style=display:flex><span>    transitionTime: &lt;time of transition to this state&gt;
</span></span><span style=display:flex><span>    message: &lt;detailed message if any&gt;
</span></span></code></pre></div><h5 id=etcd>Etcd</h5><p>Authors propose the following changes to the <code>Etcd</code> API:</p><ol><li>In the <code>Etcd.Status</code> resource API, <a href=https://github.com/gardener/etcd-druid/blob/9a598d05e639099ddf404803f87376852261a052/api/v1alpha1/types_etcd.go#L419>member status</a> is computed and stored. This field will be marked as deprecated and in a later version of druid it will be removed. In its place, the authors propose to introduce the following:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>type</span> EtcdStatus <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// MemberRefs contains references to all existing EtcdMember resources
</span></span></span><span style=display:flex><span><span style=color:green></span>  MemberRefs []CrossVersionObjectReference
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><ol start=2><li>In <code>Etcd.Status</code> resource API, <a href=https://github.com/gardener/etcd-druid/blob/9a598d05e639099ddf404803f87376852261a052/api/v1alpha1/types_etcd.go#L422>PeerUrlTLSEnabled</a> reflects the status of enabling TLS for peer communication across all etcd-members. Currentlty this field is not been used anywhere. In this proposal, the authors have also proposed that each <code>EtcdMember</code> resource should capture the status of TLS enablement of peer URL. The authors propose to relook at the need to have this field under <code>EtcdStatus</code>.</li></ol><h3 id=lifecycle-of-an-etcdmember>Lifecycle of an EtcdMember</h3><h4 id=creation>Creation</h4><p>Druid creates an <code>EtcdMember</code> resource for every replica in <code>etcd.Spec.Replicas</code> during reconciliation of an etcd resource. For a fresh etcd cluster this is done prior to creation of the StatefulSet resource and for an existing cluster which has now been scaled-up, it is done prior to updating the StatefulSet resource.</p><h4 id=updation>Updation</h4><p>All fields in <code>EtcdMember.Status</code> are <em>only</em> updated by the corresponding etcd-member. Druid only consumes the information published via <code>EtcdMember</code> resources.</p><h4 id=deletion>Deletion</h4><p>Druid is responsible for deletion of all existing <code>EtcdMember</code> resources for an etcd cluster. There are three scenarios where an <code>EtcdMember</code> resource will be deleted:</p><ol><li><p>Deletion of etcd resource.</p></li><li><p>Scale down of an etcd cluster to 0 replicas due to hibernation of the k8s control plane.</p></li><li><p>Transient scale down of an etcd cluster to 0 replicas to recover from a quorum loss.</p></li></ol><p>Authors found no reason to retain EtcdMember resources when the etcd cluster is scale down to 0 replicas since the information contained in each EtcdMember resource would no longer represent the current state of each member and would thus be stale. Any controller in druid which acts upon the <code>EtcdMember.Status</code> could potentially take incorrect actions.</p><h4 id=reconciliation>Reconciliation</h4><p>Authors propose to introduce a new controller (let&rsquo;s call it <code>etcd-member-controller</code>) which watches for changes to the <code>EtcdMember</code> resource(s). If a reconciliation of an <code>Etcd</code> resource is required as a result of change in <code>EtcdMember</code> status then this controller should enqueue an event and force a reconciliation via existing <code>etcd-controller</code>, thus preserving the single-actor-principal constraint which ensures deterministic changes to etcd cluster resources.</p><blockquote><p>NOTE: Further decisions w.r.t responsibility segregation will be taken during implementation and will not be documented in this proposal.</p></blockquote><h5 id=stale-etcdmember-status-handling>Stale EtcdMember Status Handling</h5><p>It is possible that an etcd-member is unable to update its respective <code>EtcdMember</code> resource. Following can be some of the implications which should be kept in mind while reconciling <code>EtcdMember</code> resource in druid:</p><ul><li>Druid sees stale state transitions (this assumes that the backup-sidecar attempts to update the state/sub-state in <code>etcdMember.status.transitions</code> with best attempt). There is currently no implication other than an operator seeing a stale state.</li><li><code>dbSize</code> and <code>dbSizeInUse</code> could not be updated. A consequence could be that druid continues to see high value for <code>dbSize - dbSizeInUse</code> for a extended amount of time. Druid should ensure that it does not trigger repeated defragmentations.</li><li>If <code>VolumeMismatches</code> is stale, then druid should no longer attempt to recover by repeatedly restarting the pod.</li><li>Failed <code>restoration</code> was recorded last and further updates to this array failed. Druid should not repeatedly take full-snapshots.</li><li>If <code>snapshots.accumulatedDeltaSize</code> could not be updated, then druid should not schedule repeated compaction Jobs.</li></ul><h2 id=reference>Reference</h2><ul><li><p><a href=https://etcd.io/docs/v3.3/op-guide/recovery/#snapshotting-the-keyspace>Disaster recovery | etcd</a></p></li><li><p><a href=https://etcd.io/docs/v3.3/dev-guide/api_reference_v3/#message-statusresponse-etcdserveretcdserverpbrpcproto>etcd API Reference | etcd</a></p></li><li><p><a href=https://raft.github.io/raft.pdf>Raft Consensus Algorithm</a></p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-feb8be4a3199203fdee73330193347ff>10 - Feature Gates in Etcd-Druid</h1><h1 id=feature-gates-in-etcd-druid>Feature Gates in Etcd-Druid</h1><p>This page contains an overview of the various feature gates an administrator can specify on etcd-druid.</p><h2 id=overview>Overview</h2><p>Feature gates are a set of key=value pairs that describe etcd-druid features. You can turn these features on or off by passing them to the <code>--feature-gates</code> CLI flag in the etcd-druid command.</p><p>The following tables are a summary of the feature gates that you can set on etcd-druid.</p><ul><li>The “Since” column contains the etcd-druid release when a feature is introduced or its release stage is changed.</li><li>The “Until” column, if not empty, contains the last etcd-druid release in which you can still use a feature gate.</li><li>If a feature is in the <em>Alpha</em> or <em>Beta</em> state, you can find the feature listed in the Alpha/Beta feature gate table.</li><li>If a feature is stable you can find all stages for that feature listed in the Graduated/Deprecated feature gate table.</li><li>The Graduated/Deprecated feature gate table also lists deprecated and withdrawn features.</li></ul><h2 id=feature-gates-for-alpha-or-beta-features>Feature Gates for Alpha or Beta Features</h2><table><thead><tr><th>Feature</th><th>Default</th><th>Stage</th><th>Since</th><th>Until</th></tr></thead><tbody><tr><td><code>UseEtcdWrapper</code></td><td><code>false</code></td><td><code>Alpha</code></td><td><code>0.19</code></td><td><code>0.21</code></td></tr><tr><td><code>UseEtcdWrapper</code></td><td><code>true</code></td><td><code>Beta</code></td><td><code>0.22</code></td><td></td></tr></tbody></table><h2 id=feature-gates-for-graduated-or-deprecated-features>Feature Gates for Graduated or Deprecated Features</h2><table><thead><tr><th>Feature</th><th>Default</th><th>Stage</th><th>Since</th><th>Until</th></tr></thead></table><h2 id=using-a-feature>Using a Feature</h2><p>A feature can be in <em>Alpha</em>, <em>Beta</em> or <em>GA</em> stage.
An <em>Alpha</em> feature means:</p><ul><li>Disabled by default.</li><li>Might be buggy. Enabling the feature may expose bugs.</li><li>Support for feature may be dropped at any time without notice.</li><li>The API may change in incompatible ways in a later software release without notice.</li><li>Recommended for use only in short-lived testing clusters, due to increased
risk of bugs and lack of long-term support.</li></ul><p>A <em>Beta</em> feature means:</p><ul><li>Enabled by default.</li><li>The feature is well tested. Enabling the feature is considered safe.</li><li>Support for the overall feature will not be dropped, though details may change.</li><li>The schema and/or semantics of objects may change in incompatible ways in a
subsequent beta or stable release. When this happens, we will provide instructions
for migrating to the next version. This may require deleting, editing, and
re-creating API objects. The editing process may require some thought.
This may require downtime for applications that rely on the feature.</li><li>Recommended for only non-critical uses because of potential for
incompatible changes in subsequent releases.</li></ul><blockquote><p>Please do try <em>Beta</em> features and give feedback on them!
After they exit beta, it may not be practical for us to make more changes.</p></blockquote><p>A <em>General Availability</em> (GA) feature is also referred to as a <em>stable</em> feature. It means:</p><ul><li>The feature is always enabled; you cannot disable it.</li><li>The corresponding feature gate is no longer needed.</li><li>Stable versions of features will appear in released software for many subsequent versions.</li></ul><h2 id=list-of-feature-gates>List of Feature Gates</h2><table><thead><tr><th>Feature</th><th>Description</th></tr></thead><tbody><tr><td><code>UseEtcdWrapper</code></td><td>Enables the use of etcd-wrapper image and a compatible version of etcd-backup-restore, along with component-specific configuration changes necessary for the usage of the etcd-wrapper image.</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-a286096fb24dc61fc6c93cf614019225>11 - Getting Started Locally</h1><h1 id=etcd-druid-local-setup>Etcd-Druid Local Setup</h1><p>This page aims to provide steps on how to setup Etcd-Druid locally with and without storage providers.</p><h2 id=clone-the-etcd-druid-github-repo>Clone the etcd-druid github repo</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># clone the repo</span>
</span></span><span style=display:flex><span>git clone https://github.com/gardener/etcd-druid.git
</span></span><span style=display:flex><span><span style=color:green># cd into etcd-druid folder</span>
</span></span><span style=display:flex><span>cd etcd-druid
</span></span></code></pre></div><hr><blockquote><p><strong>Note:</strong></p><ul><li>Etcd-druid uses <a href=https://kind.sigs.k8s.io/>kind</a> as it&rsquo;s local Kubernetes engine. The local setup is configured for kind due to its convenience but any other kubernetes setup would also work.</li><li>To set up etcd-druid with backups enabled on a <a href=https://github.com/localstack/localstack>LocalStack</a> provider, refer <a href=/docs/other-components/etcd-druid/getting-started-locally-localstack/>this document</a></li><li>In the section <a href=/docs/other-components/etcd-druid/getting-started-locally/#annotate-etcd-cr-with-the-reconcile-annotation>Annotate Etcd CR with the reconcile annotation</a>, the flag <code>--enable-etcd-spec-auto-reconcile</code> is set to <code>false</code>, which means a special annotation is required on the Etcd CR, for etcd-druid to reconcile it. To disable this behavior and allow auto-reconciliation of the Etcd CR for any change in the Etcd spec, set the <code>controllers.etcd.enableEtcdSpecAutoReconcile</code> value to <code>true</code> in the <code>values.yaml</code> located at <a href=https://github.com/gardener/etcd-druid/blob/master/charts/druid/values.yaml><code>charts/druid/values.yaml</code></a>. Or if etcd-druid is being run as a process, then while starting the process, set the CLI flag <code>--enable-etcd-spec-auto-reconcile=true</code> for it.</li></ul></blockquote><h2 id=setting-up-the-kind-cluster>Setting up the kind cluster</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># Create a kind cluster</span>
</span></span><span style=display:flex><span>make kind-up
</span></span></code></pre></div><p>This creates a new kind cluster and stores the kubeconfig in the <code>./hack/e2e-test/infrastructure/kind/kubeconfig</code> file.</p><p>To target this newly created cluster, set the <code>KUBECONFIG</code> environment variable to the kubeconfig file located at <code>./hack/e2e-test/infrastructure/kind/kubeconfig</code> by using the following</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>export KUBECONFIG=$PWD/hack/e2e-test/infrastructure/kind/kubeconfig
</span></span></code></pre></div><h2 id=setting-up-etcd-druid>Setting up etcd-druid</h2><p>Either one of these commands may be used to deploy etcd-druid to the configured k8s cluster.</p><ol><li><p>The following command deploys etcd-druid to the configured k8s cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>make deploy
</span></span></code></pre></div></li><li><p>The following command deploys etcd-druid to the configured k8s cluster using Skaffold <code>dev</code> mode, such that changes in the etcd-druid code are automatically picked up and applied to the deployment. This helps with local development and quick iterative changes:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>make deploy-dev
</span></span></code></pre></div></li><li><p>The following command deploys etcd-druid to the configured k8s cluster using Skaffold <code>debug</code> mode, so that a debugger can be attached to the running etcd-druid deployment. Please refer to <a href=https://skaffold.dev/docs/workflows/debug/>this guide</a> for more information on Skaffold-based debugging:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>make deploy-debug
</span></span></code></pre></div></li></ol><p>This generates the <code>Etcd</code> and <code>EtcdCopyBackupsTask</code> CRDs and deploys an etcd-druid pod into the cluster.</p><blockquote><p><strong>Note:</strong> Before calling any of the <code>make deploy*</code> commands, certain environment variables may be set in order to enable/disable certain functionalities of etcd-druid. These are:</p><ul><li><code>DRUID_ENABLE_ETCD_COMPONENTS_WEBHOOK=true</code> : enables the <a href=/docs/other-components/etcd-druid/concepts/webhooks/#etcd-components-webhook>etcdcomponents webhook</a></li><li><code>DRUID_E2E_TEST=true</code> : sets specific configuration for etcd-druid for optimal e2e test runs, like a lower sync period for the etcd controller.</li><li><code>USE_ETCD_DRUID_FEATURE_GATES=false</code> : enables etcd-druid feature gates.</li></ul></blockquote><h3 id=prepare-the-etcd-cr>Prepare the Etcd CR</h3><p>Etcd CR can be configured in 2 ways. Either to take backups to the store or disable them. Follow the appropriate section below based on the requirement.</p><p>The Etcd CR can be found at this location <code>$PWD/config/samples/druid_v1alpha1_etcd.yaml</code></p><ul><li><p><strong>Without Backups enabled</strong></p><p>To set up etcd-druid without backups enabled, make sure the <code>spec.backup.store</code> section of the Etcd CR is commented out.</p></li><li><p><strong>With Backups enabled (On Cloud Provider Object Stores)</strong></p><ul><li><p><strong>Prepare the secret</strong></p><p>Create a secret for cloud provider access. Find the secret yaml templates for different cloud providers <a href=https://github.com/gardener/etcd-backup-restore/tree/master/example/storage-provider-secrets>here</a>.</p><p>Replace the dummy values with the actual configurations and make sure to add a name and a namespace to the secret as intended.</p><blockquote><p><strong>Note 1:</strong> The secret should be applied in the same namespace as druid.</p><p><strong>Note 2:</strong> All the values in the data field of secret yaml should be in base64 encoded format.</p></blockquote></li><li><p><strong>Apply the secret</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl apply -f path/to/secret
</span></span></code></pre></div></li><li><p><strong>Adapt <code>Etcd</code> resource</strong></p><p>Uncomment the <code>spec.backup.store</code> section of the druid yaml and set the keys to allow backuprestore to take backups by connecting to an object store.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green># Configuration for storage provider</span>
</span></span><span style=display:flex><span>store:
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>        name: etcd-backup-secret-name
</span></span><span style=display:flex><span>    container: object-storage-container-name
</span></span><span style=display:flex><span>    provider: aws # options: aws,azure,gcp,openstack,alicloud,dell,openshift,local
</span></span><span style=display:flex><span>    prefix: etcd-test
</span></span></code></pre></div><p>Brief explanation of keys:</p><ul><li><code>secretRef.name</code> is the name of the secret that was applied as mentioned above</li><li><code>store.container</code> is the object storage bucket name</li><li><code>store.provider</code> is the bucket provider. Pick from the options mentioned in comment</li><li><code>store.prefix</code> is the folder name that you want to use for your snapshots inside the bucket.</li></ul></li></ul></li></ul><h3 id=applying-the-etcd-cr>Applying the Etcd CR</h3><blockquote><p><strong>Note:</strong> With backups enabled, make sure the bucket is created in corresponding cloud provider before applying the Etcd yaml</p></blockquote><p>Create the Etcd CR (Custom Resource) by applying the Etcd yaml to the cluster</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># Apply the prepared etcd CR yaml</span>
</span></span><span style=display:flex><span>kubectl apply -f config/samples/druid_v1alpha1_etcd.yaml
</span></span></code></pre></div><h3 id=verify-the-etcd-cluster>Verify the Etcd cluster</h3><p>To obtain information regarding the newly instantiated etcd cluster, perform the following step, which gives details such as the cluster size, readiness status of its members, and various other attributes.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl get etcd -o=wide
</span></span></code></pre></div><h4 id=verify-etcd-member-pods>Verify Etcd Member Pods</h4><p>To check the etcd member pods, do the following and look out for pods starting with the name <code>etcd-</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><h4 id=verify-etcd-pods-functionality>Verify Etcd Pods&rsquo; Functionality</h4><p>Verify the working conditions of the etcd pods by putting data through a etcd container and access the db from same/another container depending on single/multi node etcd cluster.</p><p>Ideally, you can exec into the etcd container using <code>kubectl exec -it &lt;etcd_pod> -c etcd -- bash</code> if it utilizes a base image containing a shell. However, note that the <code>etcd-wrapper</code> Docker image employs a <a href=https://github.com/GoogleContainerTools/distroless>distroless image</a>, which lacks a shell. To interact with etcd, use an <a href=https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/>Ephemeral container</a> as a debug container. Refer to this <a href=https://github.com/gardener/etcd-wrapper/blob/main/docs/deployment/ops.md#build-image>documentation</a> for building and using an ephemeral container by attaching it to the etcd container.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># Put a key-value pair into the etcd </span>
</span></span><span style=display:flex><span>etcdctl put key1 value1
</span></span><span style=display:flex><span><span style=color:green># Retrieve all key-value pairs from the etcd db</span>
</span></span><span style=display:flex><span>etcdctl get --prefix <span style=color:#a31515>&#34;&#34;</span>
</span></span></code></pre></div><p>For a multi-node etcd cluster, insert the key-value pair from the <code>etcd</code> container of one etcd member and retrieve it from the <code>etcd</code> container of another member to verify consensus among the multiple etcd members.</p><h4 id=view-etcd-database-file>View Etcd Database File</h4><p>The Etcd database file is located at <code>var/etcd/data/new.etcd/snap/db</code> inside the <code>backup-restore</code> container. In versions with an <code>alpine</code> base image, you can exec directly into the container. However, in recent versions where the <code>backup-restore</code> docker image started using a distroless image, a debug container is required to communicate with it, as mentioned in the previous section.</p><h3 id=updating-the-etcd-cr>Updating the Etcd CR</h3><p>The <code>Etcd</code> spec can be updated with new changes, such as etcd cluster configuration or backup-restore configuration, and etcd-druid will reconcile these changes as expected, under certain conditions:</p><ol><li>If the <code>--enable-etcd-spec-auto-reconcile</code> flag is set to <code>true</code>, the spec change is automatically picked up and reconciled by etcd-druid.</li><li>If the <code>--enable-etcd-spec-auto-reconcile</code> flag is unset, or set to <code>false</code>, then etcd-druid will expect an additional annotation <code>gardener.cloud/operation: reconcile</code> on the <code>Etcd</code> resource in order to pick it up for reconciliation. Upon successful reconciliation, this annotation is removed by etcd-druid. The annotation can be added as follows:<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># Annotate etcd-test CR to reconcile</span>
</span></span><span style=display:flex><span>kubectl annotate etcd etcd-test gardener.cloud/operation=<span style=color:#a31515>&#34;reconcile&#34;</span>
</span></span></code></pre></div></li></ol><h2 id=cleaning-the-setup>Cleaning the setup</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># Delete the cluster</span>
</span></span><span style=display:flex><span>make kind-down
</span></span></code></pre></div><p>This cleans up the entire setup as the kind cluster gets deleted. It deletes the created Etcd, all pods that got created along the way and also other resources such as statefulsets, services, PV&rsquo;s, PVC&rsquo;s, etc.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f81d5e023027d0e6df44f393e2de8bfd>12 - Getting Started Locally Localstack</h1><h1 id=getting-started-with-etcd-druid-localstack-and-kind>Getting Started with etcd-druid, LocalStack, and Kind</h1><p>This guide provides step-by-step instructions on how to set up etcd-druid with <a href=https://localstack.cloud/>LocalStack</a> and Kind on your local machine. LocalStack emulates AWS services locally, which allows the etcd cluster to interact with AWS S3 without the need for an actual AWS connection. This setup is ideal for local development and testing.</p><h2 id=prerequisites>Prerequisites</h2><ul><li>Docker (installed and running)</li><li>AWS CLI (version <code>>=1.29.0</code> or <code>>=2.13.0</code>)</li></ul><h2 id=environment-setup>Environment Setup</h2><h3 id=step-1-provision-the-kind-cluster>Step 1: Provision the Kind Cluster</h3><p>Execute the command below to provision a <code>kind</code> cluster. This command also forwards port <code>4566</code> from the <a href=https://github.com/gardener/etcd-druid/blob/master/hack/e2e-test/infrastructure/kind/cluster.yaml>kind cluster</a> to your local machine, enabling LocalStack access:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make kind-up
</span></span></code></pre></div><h3 id=step-2-deploy-localstack>Step 2: Deploy LocalStack</h3><p>Deploy LocalStack onto the Kubernetes cluster using the command below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make deploy-localstack
</span></span></code></pre></div><h3 id=step-3-set-up-an-s3-bucket>Step 3: Set up an S3 Bucket</h3><ol><li>Set up the AWS CLI to interact with LocalStack by setting the necessary environment variables. This configuration redirects S3 commands to the LocalStack endpoint and provides the required credentials for authentication:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export AWS_ENDPOINT_URL_S3=<span style=color:#a31515>&#34;http://localhost:4566&#34;</span>
</span></span><span style=display:flex><span>export AWS_ACCESS_KEY_ID=ACCESSKEYAWSUSER
</span></span><span style=display:flex><span>export AWS_SECRET_ACCESS_KEY=sEcreTKey
</span></span><span style=display:flex><span>export AWS_DEFAULT_REGION=us-east-2
</span></span></code></pre></div><ol start=2><li>Create an S3 bucket for etcd-druid backup purposes:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>aws s3api create-bucket --bucket etcd-bucket --region us-east-2 --create-bucket-configuration LocationConstraint=us-east-2 --acl private
</span></span></code></pre></div><h3 id=step-4-deploy-etcd-druid>Step 4: Deploy etcd-druid</h3><p>Deploy etcd-druid onto the Kind cluster using the command below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make deploy
</span></span></code></pre></div><h3 id=step-5-configure-etcd-with-localstack-store>Step 5: Configure etcd with LocalStack Store</h3><p>Apply the required Kubernetes manifests to create an etcd custom resource (CR) and a secret for AWS credentials, facilitating LocalStack access:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export KUBECONFIG=hack/e2e-test/infrastructure/kind/kubeconfig
</span></span><span style=display:flex><span>kubectl apply -f config/samples/druid_v1alpha1_etcd_localstack.yaml -f config/samples/etcd-secret-localstack.yaml
</span></span></code></pre></div><h3 id=step-6-reconcile-the-etcd>Step 6: Reconcile the etcd</h3><p>Initiate etcd reconciliation by annotating the etcd resource with the <code>gardener.cloud/operation=reconcile</code> annotation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl annotate etcd etcd-test gardener.cloud/operation=reconcile
</span></span></code></pre></div><p>Congratulations! You have successfully configured <code>etcd-druid</code>, <code>LocalStack</code>, and <code>kind</code> on your local machine. Inspect the etcd-druid logs and LocalStack to ensure the setup operates as anticipated.</p><p>To validate the buckets, execute the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>aws s3 ls etcd-bucket/etcd-test/v2/
</span></span></code></pre></div><h3 id=cleanup>Cleanup</h3><p>To dismantle the setup, execute the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make kind-down
</span></span><span style=display:flex><span>unset AWS_ENDPOINT_URL_S3 AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY AWS_DEFAULT_REGION KUBECONFIG
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-399837f9d0fa26b6faa00abd1ab901ff>13 - Local e2e Tests</h1><h1 id=e2e-test-suite>e2e Test Suite</h1><p>Developers can run extended e2e tests, in addition to unit tests, for Etcd-Druid in or from
their local environments. This is recommended to verify the desired behavior of several features
and to avoid regressions in future releases.</p><p>The very same tests typically run as part of the component&rsquo;s release job as well as on demand, e.g.,
when triggered by Etcd-Druid maintainers for open pull requests.</p><p>Testing Etcd-Druid automatically involves a certain test coverage for <a href=https://github.com/gardener/etcd-backup-restore/>gardener/etcd-backup-restore</a>
which is deployed as a side-car to the actual <code>etcd</code> container.</p><h2 id=prerequisites>Prerequisites</h2><p>The e2e test lifecycle is managed with the help of <a href=https://skaffold.dev/>skaffold</a>. Every involved step like <code>setup</code>,
<code>deploy</code>, <code>undeploy</code> or <code>cleanup</code> is executed against a <strong>Kubernetes</strong> cluster which makes it a mandatory prerequisite at the same time.
Only <a href=https://skaffold.dev/>skaffold</a> itself with involved <code>docker</code>, <code>helm</code> and <code>kubectl</code> executions as well as
the e2e-tests are executed locally. Required binaries are automatically downloaded if you use the corresponding <code>make</code> target,
as described in this document.</p><p>It&rsquo;s expected that especially the <code>deploy</code> step is run against a Kubernetes cluster which doesn&rsquo;t contain an Druid deployment or any left-overs like <code>druid.gardener.cloud</code> CRDs.
The <code>deploy</code> step will likely fail in such scenarios.</p><blockquote><p>Tip: Create a fresh <a href=https://kind.sigs.k8s.io/>KinD</a> cluster or a similar one with a small footprint before executing the tests.</p></blockquote><h2 id=providers>Providers</h2><p>The following providers are supported for e2e tests:</p><ul><li>AWS</li><li>Azure</li><li>GCP</li><li>Local</li></ul><blockquote><p>Valid credentials need to be provided when tests are executed with mentioned cloud providers.</p></blockquote><h2 id=flow>Flow</h2><p>An e2e test execution involves the following steps:</p><table><thead><tr><th>Step</th><th>Description</th></tr></thead><tbody><tr><td><code>setup</code></td><td>Create a storage bucket which is used for etcd backups (only with cloud providers).</td></tr><tr><td><code>deploy</code></td><td>Build Docker image, upload it to registry (if remote cluster - see <a href=https://skaffold.dev/docs/pipeline-stages/builders/docker/>Docker build</a>), deploy Helm chart (<code>charts/druid</code>) to Kubernetes cluster.</td></tr><tr><td><code>test</code></td><td>Execute e2e tests as defined in <code>test/e2e</code>.</td></tr><tr><td><code>undeploy</code></td><td>Remove the deployed artifacts from Kubernetes cluster.</td></tr><tr><td><code>cleanup</code></td><td>Delete storage bucket and Druid deployment from test cluster.</td></tr></tbody></table><h3 id=make-target>Make target</h3><p>Executing e2e-tests is as easy as executing the following command <strong>with defined Env-Vars as desribed in the following
section and as needed for your test scenario</strong>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make test-e2e
</span></span></code></pre></div><h3 id=common-env-variables>Common Env Variables</h3><p>The following environment variables influence how the flow described above is executed:</p><ul><li><code>PROVIDERS</code>: Providers used for testing (<code>all</code>, <code>aws</code>, <code>azure</code>, <code>gcp</code>, <code>local</code>). Multiple entries must be comma separated.<blockquote><p><strong>Note</strong>: Some tests will use very first entry from env <code>PROVIDERS</code> for e2e testing (ex: multi-node tests). So for multi-node tests to use specific provider, specify that provider as first entry in env <code>PROVIDERS</code>.</p></blockquote></li><li><code>KUBECONFIG</code>: Kubeconfig pointing to cluster where Etcd-Druid will be deployed (preferably <a href=https://kind.sigs.k8s.io>KinD</a>).</li><li><code>TEST_ID</code>: Some ID which is used to create assets for and during testing.</li><li><code>STEPS</code>: Steps executed by <code>make</code> target (<code>setup</code>, <code>deploy</code>, <code>test</code>, <code>undeploy</code>, <code>cleanup</code> - default: all steps).</li></ul><h3 id=aws-env-variables>AWS Env Variables</h3><ul><li><code>AWS_ACCESS_KEY_ID</code>: Key ID of the user.</li><li><code>AWS_SECRET_ACCESS_KEY</code>: Access key of the user.</li><li><code>AWS_REGION</code>: Region in which the test bucket is created.</li></ul><p>Example:</p><pre tabindex=0><code>make \
  AWS_ACCESS_KEY_ID=&#34;abc&#34; \
  AWS_SECRET_ACCESS_KEY=&#34;xyz&#34; \
  AWS_REGION=&#34;eu-central-1&#34; \
  KUBECONFIG=&#34;$HOME/.kube/config&#34; \
  PROVIDERS=&#34;aws&#34; \
  TEST_ID=&#34;some-test-id&#34; \
  STEPS=&#34;setup,deploy,test,undeploy,cleanup&#34; \
test-e2e
</code></pre><h3 id=azure-env-variables>Azure Env Variables</h3><ul><li><code>STORAGE_ACCOUNT</code>: Storage account used for managing the storage container.</li><li><code>STORAGE_KEY</code>: Key of storage account.</li></ul><p>Example:</p><pre tabindex=0><code>make \
  STORAGE_ACCOUNT=&#34;abc&#34; \
  STORAGE_KEY=&#34;eHl6Cg==&#34; \
  KUBECONFIG=&#34;$HOME/.kube/config&#34; \
  PROVIDERS=&#34;azure&#34; \
  TEST_ID=&#34;some-test-id&#34; \
  STEPS=&#34;setup,deploy,test,undeploy,cleanup&#34; \
test-e2e
</code></pre><h3 id=gcp-env-variables>GCP Env Variables</h3><ul><li><code>GCP_SERVICEACCOUNT_JSON_PATH</code>: Path to the service account json file used for this test.</li><li><code>GCP_PROJECT_ID</code>: ID of the GCP project.</li></ul><p>Example:</p><pre tabindex=0><code>make \
  GCP_SERVICEACCOUNT_JSON_PATH=&#34;/var/lib/secrets/serviceaccount.json&#34; \
  GCP_PROJECT_ID=&#34;xyz-project&#34; \
  KUBECONFIG=&#34;$HOME/.kube/config&#34; \
  PROVIDERS=&#34;gcp&#34; \
  TEST_ID=&#34;some-test-id&#34; \
  STEPS=&#34;setup,deploy,test,undeploy,cleanup&#34; \
test-e2e
</code></pre><h3 id=local-env-variables>Local Env Variables</h3><p>No special environment variables are required for running e2e tests with <code>Local</code> provider.</p><p>Example:</p><pre tabindex=0><code>make \
  KUBECONFIG=&#34;$HOME/.kube/config&#34; \
  PROVIDERS=&#34;local&#34; \
  TEST_ID=&#34;some-test-id&#34; \
  STEPS=&#34;setup,deploy,test,undeploy,cleanup&#34; \
test-e2e
</code></pre><h2 id=e2e-test-with-localstack>e2e test with localstack</h2><p>The above-mentioned e2e tests need storage from real cloud providers to be setup. But there is a tool named <a href=https://docs.localstack.cloud/user-guide/aws/s3/>localstack</a> that enables to run e2e test with mock AWS storage. We can also provision KIND cluster for e2e tests. So, together with localstack and KIND cluster, we don&rsquo;t need to depend on any actual cloud provider infrastructure to be setup to run e2e tests.</p><h3 id=how-are-the-kind-cluster-and-localstack-set-up>How are the KIND cluster and localstack set up</h3><p>KIND or Kubernetes-In-Docker is a kubernetes cluster that is set up inside a docker container. This cluster is with limited capability as it does not have much compute power. But this cluster can easily be setup inside a container and can be tear down easily just by removing a container. That&rsquo;s why KIND cluster is very easy to use for e2e tests. <code>Makefile</code> command helps to spin up a KIND cluster and use the cluster to run e2e tests.</p><p>There is a docker image for localstack. The image is deployed as pod inside the KIND cluster through <code>hack/e2e-test/infrastructure/localstack/localstack.yaml</code>. <code>Makefile</code> takes care of deploying the yaml file in a KIND cluster.</p><p>The developer needs to run <code>make ci-e2e-kind</code> command. This command in turn runs <code>hack/ci-e2e-kind.sh</code> which spin up the KIND cluster and deploy localstack in it and then run the e2e tests using localstack as mock AWS storage provider. e2e tests are actually run on host machine but deploy the druid controller inside KIND cluster. Druid controller spawns multinode etcd clusters inside KIND cluster. e2e tests verify whether the druid controller performs its jobs correctly or not. Mock localstack storage is cleaned up after every e2e tests. That&rsquo;s why the e2e tests need to access the localstack pod running inside KIND cluster. The network traffic between host machine and localstack pod is resolved via mapping localstack pod port to host port while setting up the KIND cluster via <code>hack/e2e-test/infrastructure/kind/cluster.yaml</code></p><h3 id=how-to-execute-e2e-tests-with-localstack-and-kind-cluster>How to execute e2e tests with localstack and KIND cluster</h3><p>Run the following <code>make</code> command to spin up a KinD cluster, deploy localstack and run the e2e tests with provider <code>aws</code>:</p><pre tabindex=0><code>make ci-e2e-kind
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-45bed7d3d8ddd56e4e0bc3a2bb000f5a>14 - Metrics</h1><h1 id=monitoring>Monitoring</h1><p>etcd-druid uses <a href=http://prometheus.io/>Prometheus</a> for metrics reporting. The metrics can be used for real-time monitoring and debugging of compaction jobs.</p><p>The simplest way to see the available metrics is to cURL the metrics endpoint <code>/metrics</code>. The format is described <a href=http://prometheus.io/docs/instrumenting/exposition_formats/>here</a>.</p><p>Follow the <a href=http://prometheus.io/docs/introduction/getting_started/>Prometheus getting started doc</a> to spin up a Prometheus server to collect etcd metrics.</p><p>The naming of metrics follows the suggested <a href=http://prometheus.io/docs/practices/naming/>Prometheus best practices</a>. All compaction related metrics are put under namespace <code>etcddruid</code> and the respective subsystems.</p><h2 id=snapshot-compaction>Snapshot Compaction</h2><p>These metrics provide information about the compaction jobs that run after some interval in shoot control planes. Studying the metrics, we can deduce how many compaction job ran successfully, how many failed, how many delta events compacted etc.</p><table><thead><tr><th>Name</th><th>Description</th><th>Type</th></tr></thead><tbody><tr><td>etcddruid_compaction_jobs_total</td><td>Total number of compaction jobs initiated by compaction controller.</td><td>Counter</td></tr><tr><td>etcddruid_compaction_jobs_current</td><td>Number of currently running compaction job.</td><td>Gauge</td></tr><tr><td>etcddruid_compaction_job_duration_seconds</td><td>Total time taken in seconds to finish a running compaction job.</td><td>Histogram</td></tr><tr><td>etcddruid_compaction_num_delta_events</td><td>Total number of etcd events to be compacted by a compaction job.</td><td>Gauge</td></tr></tbody></table><p>There are two labels for <code>etcddruid_compaction_jobs_total</code> metrics. The label <code>succeeded</code> shows how many of the compaction jobs are succeeded and label <code>failed</code> shows how many of compaction jobs are failed.</p><p>There are two labels for <code>etcddruid_compaction_job_duration_seconds</code> metrics. The label <code>succeeded</code> shows how much time taken by a successful job to complete and label <code>failed</code> shows how much time taken by a failed compaction job.</p><p><code>etcddruid_compaction_jobs_current</code> metric comes with label <code>etcd_namespace</code> that indicates the namespace of the Etcd running in the control plane of a shoot cluster..</p><h2 id=etcd>Etcd</h2><p>These metrics are exposed by the <a href=https://etcd.io/>etcd</a> process that runs in each etcd pod.</p><p>The following list metrics is applicable to clustering of a multi-node etcd cluster. The full list of metrics exposed by <code>etcd</code> is available <a href=https://etcd.io/docs/v3.4/metrics>here</a>.</p><table><thead><tr><th>No.</th><th>Metrics Name</th><th>Description</th><th>Comments</th></tr></thead><tbody><tr><td>1</td><td>etcd_disk_wal_fsync_duration_seconds</td><td>latency distributions of fsync called by WAL.</td><td>High disk operation latencies indicate disk issues.</td></tr><tr><td>2</td><td>etcd_disk_backend_commit_duration_seconds</td><td>latency distributions of commit called by backend.</td><td>High disk operation latencies indicate disk issues.</td></tr><tr><td>3</td><td>etcd_server_has_leader</td><td>whether or not a leader exists. 1: leader exists, 0: leader not exists.</td><td>To capture quorum loss or to check the availability of etcd cluster.</td></tr><tr><td>4</td><td>etcd_server_is_leader</td><td>whether or not this member is a leader. 1 if it is, 0 otherwise.</td><td></td></tr><tr><td>5</td><td>etcd_server_leader_changes_seen_total</td><td>number of leader changes seen.</td><td>Helpful in fine tuning the zonal cluster like etcd-heartbeat time etc, it can also indicates the etcd load and network issues.</td></tr><tr><td>6</td><td>etcd_server_is_learner</td><td>whether or not this member is a learner. 1 if it is, 0 otherwise.</td><td></td></tr><tr><td>7</td><td>etcd_server_learner_promote_successes</td><td>total number of successful learner promotions while this member is leader.</td><td>Might be helpful in checking the success of API calls called by backup-restore.</td></tr><tr><td>8</td><td>etcd_network_client_grpc_received_bytes_total</td><td>total number of bytes received from grpc clients.</td><td>Client Traffic In.</td></tr><tr><td>9</td><td>etcd_network_client_grpc_sent_bytes_total</td><td>total number of bytes sent to grpc clients.</td><td>Client Traffic Out.</td></tr><tr><td>10</td><td>etcd_network_peer_sent_bytes_total</td><td>total number of bytes sent to peers.</td><td>Useful for network usage.</td></tr><tr><td>11</td><td>etcd_network_peer_received_bytes_total</td><td>total number of bytes received from peers.</td><td>Useful for network usage.</td></tr><tr><td>12</td><td>etcd_network_active_peers</td><td>current number of active peer connections.</td><td>Might be useful in detecting issues like network partition.</td></tr><tr><td>13</td><td>etcd_server_proposals_committed_total</td><td>total number of consensus proposals committed.</td><td>A consistently large lag between a single member and its leader indicates that member is slow or unhealthy.</td></tr><tr><td>14</td><td>etcd_server_proposals_pending</td><td>current number of pending proposals to commit.</td><td>Pending proposals suggests there is a high client load or the member cannot commit proposals.</td></tr><tr><td>15</td><td>etcd_server_proposals_failed_total</td><td>total number of failed proposals seen.</td><td>Might indicates downtime caused by a loss of quorum.</td></tr><tr><td>16</td><td>etcd_server_proposals_applied_total</td><td>total number of consensus proposals applied.</td><td>Difference between etcd_server_proposals_committed_total and etcd_server_proposals_applied_total should usually be small.</td></tr><tr><td>17</td><td>etcd_mvcc_db_total_size_in_bytes</td><td>total size of the underlying database physically allocated in bytes.</td><td></td></tr><tr><td>18</td><td>etcd_server_heartbeat_send_failures_total</td><td>total number of leader heartbeat send failures.</td><td>Might be helpful in fine-tuning the cluster or detecting slow disk or any network issues.</td></tr><tr><td>19</td><td>etcd_network_peer_round_trip_time_seconds</td><td>round-trip-time histogram between peers.</td><td>Might be helpful in fine-tuning network usage specially for zonal etcd cluster.</td></tr><tr><td>20</td><td>etcd_server_slow_apply_total</td><td>total number of slow apply requests.</td><td>Might indicate overloaded from slow disk.</td></tr><tr><td>21</td><td>etcd_server_slow_read_indexes_total</td><td>total number of pending read indexes not in sync with leader&rsquo;s or timed out read index requests.</td><td></td></tr></tbody></table><p>The full list of metrics is available <a href=https://etcd.io/docs/v3.4/metrics/>here</a>.</p><h2 id=etcd-backup-restore>Etcd-Backup-Restore</h2><p>These metrics are exposed by the <a href=https://github.com/gardener/etcd-backup-restore>etcd-backup-restore</a> container in each etcd pod.</p><p>The following list metrics is applicable to clustering of a multi-node etcd cluster. The full list of metrics exposed by <code>etcd-backup-restore</code> is available <a href=https://github.com/gardener/etcd-backup-restore/blob/master/docs/operations/metrics.md>here</a>.</p><table><thead><tr><th>No.</th><th>Metrics Name</th><th>Description</th></tr></thead><tbody><tr><td>1.</td><td>etcdbr_cluster_size</td><td>to capture the scale-up/scale-down scenarios.</td></tr><tr><td>2.</td><td>etcdbr_is_learner</td><td>whether or not this member is a learner. 1 if it is, 0 otherwise.</td></tr><tr><td>3.</td><td>etcdbr_is_learner_count_total</td><td>total number times member added as the learner.</td></tr><tr><td>4.</td><td>etcdbr_restoration_duration_seconds</td><td>total latency distribution required to restore the etcd member.</td></tr><tr><td>5.</td><td>etcdbr_add_learner_duration_seconds</td><td>total latency distribution of adding the etcd member as a learner to the cluster.</td></tr><tr><td>6.</td><td>etcdbr_member_remove_duration_seconds</td><td>total latency distribution removing the etcd member from the cluster.</td></tr><tr><td>7.</td><td>etcdbr_member_promote_duration_seconds</td><td>total latency distribution of promoting the learner to the voting member.</td></tr><tr><td>8.</td><td>etcdbr_defragmentation_duration_seconds</td><td>total latency distribution of defragmentation of each etcd cluster member.</td></tr></tbody></table><h2 id=prometheus-supplied-metrics>Prometheus supplied metrics</h2><p>The Prometheus client library provides a number of metrics under the <code>go</code> and <code>process</code> namespaces.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2d6bf3aad1d62f476e7662122ed9db62>15 - operator out-of-band tasks</h1><h1 id=dep-05-operator-out-of-band-tasks>DEP-05: Operator Out-of-band Tasks</h1><h2 id=table-of-contents>Table of Contents</h2><ul><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#dep-05-operator-out-of-band-tasks>DEP-05: Operator Out-of-band Tasks</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#table-of-contents>Table of Contents</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#summary>Summary</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#terminology>Terminology</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#motivation>Motivation</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#goals>Goals</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#non-goals>Non-Goals</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#proposal>Proposal</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#custom-resource-golang-api>Custom Resource Golang API</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#spec>Spec</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#status>Status</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#custom-resource-yaml-api>Custom Resource YAML API</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#lifecycle>Lifecycle</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#creation>Creation</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#execution>Execution</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#deletion>Deletion</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#use-cases>Use Cases</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#recovery-from-permanent-quorum-loss>Recovery from permanent quorum loss</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#task-config>Task Config</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#pre-conditions>Pre-Conditions</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#trigger-on-demand-snapshot-compaction>Trigger on-demand snapshot compaction</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#possible-scenarios>Possible scenarios</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#task-config-1>Task Config</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#pre-conditions-1>Pre-Conditions</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#trigger-on-demand-fulldelta-snapshot>Trigger on-demand full/delta snapshot</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#possible-scenarios-1>Possible scenarios</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#task-config-2>Task Config</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#pre-conditions-2>Pre-Conditions</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#trigger-on-demand-maintenance-of-etcd-cluster>Trigger on-demand maintenance of etcd cluster</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#possible-scenarios-2>Possible Scenarios</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#task-config-3>Task Config</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#pre-conditions-3>Pre-Conditions</a></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#copy-backups-task>Copy Backups Task</a><ul><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#possible-scenarios-3>Possible Scenarios</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#task-config-4>Task Config</a></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#pre-conditions-4>Pre-Conditions</a></li></ul></li></ul></li></ul></li><li><a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#metrics>Metrics</a></li></ul></li></ul><h2 id=summary>Summary</h2><p>This DEP proposes an enhancement to <code>etcd-druid</code>&rsquo;s capabilities to handle <a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#terminology>out-of-band</a> tasks, which are presently performed manually or invoked programmatically via suboptimal APIs. The document proposes the establishment of a unified interface by defining a well-structured API to harmonize the initiation of any <code>out-of-band</code> task, monitor its status, and simplify the process of adding new tasks and managing their lifecycles.</p><h2 id=terminology>Terminology</h2><ul><li><p><strong>etcd-druid:</strong> <a href=https://github.com/gardener/etcd-druid>etcd-druid</a> is an operator to manage the etcd clusters.</p></li><li><p><strong>backup-sidecar:</strong> It is the etcd-backup-restore sidecar container running in each etcd-member pod of etcd cluster.</p></li><li><p><strong>leading-backup-sidecar:</strong> A backup-sidecar that is associated to an etcd leader of an etcd cluster.</p></li><li><p><strong>out-of-band task:</strong> Any on-demand tasks/operations that can be executed on an etcd cluster without modifying the <a href=https://github.com/gardener/etcd-druid/blob/9c5f8254e3aeb24c1e3e88d17d8d1de336ce981b/api/v1alpha1/types_etcd.go#L272-L273>Etcd custom resource spec</a> (desired state).</p></li></ul><h2 id=motivation>Motivation</h2><p>Today, <a href=https://github.com/gardener/etcd-druid>etcd-druid</a> mainly acts as an etcd cluster provisioner (creation, maintenance and deletion). In future, capabilities of etcd-druid will be enhanced via <a href=https://github.com/gardener/etcd-druid/blob/8ac70d512969c2e12e666d923d7d35fdab1e0f8e/docs/proposals/04-etcd-member-custom-resource.md>etcd-member</a> proposal by providing it access to much more detailed information about each etcd cluster member. While we enhance the reconciliation and monitoring capabilities of etcd-druid, it still lacks the ability to allow users to invoke <code>out-of-band</code> tasks on an existing etcd cluster.</p><p>There are new learnings while operating etcd clusters at scale. It has been observed that we regularly need capabilities to trigger <code>out-of-band</code> tasks which are outside of the purview of a regular etcd reconciliation run. Many of these tasks are multi-step processes, and performing them manually is error-prone, even if an operator follows a well-written step-by-step guide. Thus, there is a need to automate these tasks.
Some examples of an <code>on-demand/out-of-band</code> tasks:</p><ul><li>Recover from a permanent quorum loss of etcd cluster.</li><li>Trigger an on-demand full/delta snapshot.</li><li>Trigger an on-demand snapshot compaction.</li><li>Trigger an on-demand maintenance of etcd cluster.</li><li>Copy the backups from one object store to another object store.</li></ul><h2 id=goals>Goals</h2><ul><li>Establish a unified interface for operator tasks by defining a single dedicated custom resource for <code>out-of-band</code> tasks.</li><li>Define a contract (in terms of prerequisites) which needs to be adhered to by any task implementation.</li><li>Facilitate the easy addition of new <code>out-of-band</code> task(s) through this custom resource.</li><li>Provide CLI capabilities to operators, making it easy to invoke supported <code>out-of-band</code> tasks.</li></ul><h2 id=non-goals>Non-Goals</h2><ul><li>In the current scope, capability to abort/suspend an <code>out-of-band</code> task is not going to be provided. This could be considered as an enhancement based on pull.</li><li>Ordering (by establishing dependency) of <code>out-of-band</code> tasks submitted for the same etcd cluster has not been considered in the first increment. In a future version based on how operator tasks are used, we will enhance this proposal and the implementation.</li></ul><h2 id=proposal>Proposal</h2><p>Authors propose creation of a new single dedicated custom resource to represent an <code>out-of-band</code> task. Etcd-druid will be enhanced to process the task requests and update its status which can then be tracked/observed.</p><h3 id=custom-resource-golang-api>Custom Resource Golang API</h3><p><code>EtcdOperatorTask</code> is the new custom resource that will be introduced. This API will be in <code>v1alpha1</code> version and will be subject to change. We will be respecting <a href=https://kubernetes.io/docs/reference/using-api/deprecation-policy/>Kubernetes Deprecation Policy</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:green>// EtcdOperatorTask represents an out-of-band operator task resource.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> EtcdOperatorTask <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  metav1.TypeMeta
</span></span><span style=display:flex><span>  metav1.ObjectMeta
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// Spec is the specification of the EtcdOperatorTask resource.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Spec EtcdOperatorTaskSpec <span style=color:#a31515>`json:&#34;spec&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// Status is most recently observed status of the EtcdOperatorTask resource.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Status EtcdOperatorTaskStatus <span style=color:#a31515>`json:&#34;status,omitempty&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h4 id=spec>Spec</h4><p>The authors propose that the following fields should be specified in the spec (desired state) of the <code>EtcdOperatorTask</code> custom resource.</p><ul><li>To capture the type of <code>out-of-band</code> operator task to be performed, <code>.spec.type</code> field should be defined. It can have values from all supported <code>out-of-band</code> tasks eg. &ldquo;OnDemandSnaphotTask&rdquo;, &ldquo;QuorumLossRecoveryTask&rdquo; etc.</li><li>To capture the configuration specific to each task, a <code>.spec.config</code> field should be defined of type <code>string</code> as each task can have different input configuration.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:green>// EtcdOperatorTaskSpec is the spec for a EtcdOperatorTask resource.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> EtcdOperatorTaskSpec <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>  <span style=color:green>// Type specifies the type of out-of-band operator task to be performed. 
</span></span></span><span style=display:flex><span><span style=color:green></span>  Type <span style=color:#2b91af>string</span> <span style=color:#a31515>`json:&#34;type&#34;`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// Config is a task specific configuration.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Config <span style=color:#2b91af>string</span> <span style=color:#a31515>`json:&#34;config,omitempty&#34;`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// TTLSecondsAfterFinished is the time-to-live to garbage collect the 
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// related resource(s) of task once it has been completed.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// +optional
</span></span></span><span style=display:flex><span><span style=color:green></span>  TTLSecondsAfterFinished *<span style=color:#2b91af>int32</span> <span style=color:#a31515>`json:&#34;ttlSecondsAfterFinished,omitempty&#34;`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// OwnerEtcdReference refers to the name and namespace of the corresponding 
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// Etcd owner for which the task has been invoked.
</span></span></span><span style=display:flex><span><span style=color:green></span>  OwnerEtcdRefrence types.NamespacedName <span style=color:#a31515>`json:&#34;ownerEtcdRefrence&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h4 id=status>Status</h4><p>The authors propose the following fields for the Status (current state) of the <code>EtcdOperatorTask</code> custom resource to monitor the progress of the task.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:green>// EtcdOperatorTaskStatus is the status for a EtcdOperatorTask resource.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> EtcdOperatorTaskStatus <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// ObservedGeneration is the most recent generation observed for the resource.
</span></span></span><span style=display:flex><span><span style=color:green></span>  ObservedGeneration *<span style=color:#2b91af>int64</span> <span style=color:#a31515>`json:&#34;observedGeneration,omitempty&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// State is the last known state of the task.
</span></span></span><span style=display:flex><span><span style=color:green></span>  State TaskState <span style=color:#a31515>`json:&#34;state&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// Time at which the task has moved from &#34;pending&#34; state to any other state.
</span></span></span><span style=display:flex><span><span style=color:green></span>  InitiatedAt metav1.Time <span style=color:#a31515>`json:&#34;initiatedAt&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// LastError represents the errors when processing the task.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// +optional
</span></span></span><span style=display:flex><span><span style=color:green></span>  LastErrors []LastError <span style=color:#a31515>`json:&#34;lastErrors,omitempty&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// Captures the last operation status if task involves many stages.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// +optional
</span></span></span><span style=display:flex><span><span style=color:green></span>  LastOperation *LastOperation <span style=color:#a31515>`json:&#34;lastOperation,omitempty&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>type</span> LastOperation <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// Name of the LastOperation.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Name opsName <span style=color:#a31515>`json:&#34;name&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// Status of the last operation, one of pending, progress, completed, failed.
</span></span></span><span style=display:flex><span><span style=color:green></span>  State OperationState <span style=color:#a31515>`json:&#34;state&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// LastTransitionTime is the time at which the operation state last transitioned from one state to another.
</span></span></span><span style=display:flex><span><span style=color:green></span>  LastTransitionTime metav1.Time <span style=color:#a31515>`json:&#34;lastTransitionTime&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// A human readable message indicating details about the last operation.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Reason <span style=color:#2b91af>string</span> <span style=color:#a31515>`json:&#34;reason&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// LastError stores details of the most recent error encountered for the task.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> LastError <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// Code is an error code that uniquely identifies an error.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Code ErrorCode <span style=color:#a31515>`json:&#34;code&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// Description is a human-readable message indicating details of the error.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Description <span style=color:#2b91af>string</span> <span style=color:#a31515>`json:&#34;description&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// ObservedAt is the time at which the error was observed.
</span></span></span><span style=display:flex><span><span style=color:green></span>  ObservedAt metav1.Time <span style=color:#a31515>`json:&#34;observedAt&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// TaskState represents the state of the task.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> TaskState <span style=color:#2b91af>string</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>const</span> (
</span></span><span style=display:flex><span>  TaskStateFailed TaskState = <span style=color:#a31515>&#34;Failed&#34;</span>
</span></span><span style=display:flex><span>  TaskStatePending TaskState = <span style=color:#a31515>&#34;Pending&#34;</span>
</span></span><span style=display:flex><span>  TaskStateRejected TaskState = <span style=color:#a31515>&#34;Rejected&#34;</span>
</span></span><span style=display:flex><span>  TaskStateSucceeded TaskState = <span style=color:#a31515>&#34;Succeeded&#34;</span>
</span></span><span style=display:flex><span>  TaskStateInProgress TaskState = <span style=color:#a31515>&#34;InProgress&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// OperationState represents the state of last operation.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> OperationState <span style=color:#2b91af>string</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>const</span> (
</span></span><span style=display:flex><span>  OperationStateFailed OperationState = <span style=color:#a31515>&#34;Failed&#34;</span>
</span></span><span style=display:flex><span>  OperationStatePending OperationState = <span style=color:#a31515>&#34;Pending&#34;</span>
</span></span><span style=display:flex><span>  OperationStateCompleted OperationState = <span style=color:#a31515>&#34;Completed&#34;</span>
</span></span><span style=display:flex><span>  OperationStateInProgress OperationState = <span style=color:#a31515>&#34;InProgress&#34;</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h3 id=custom-resource-yaml-api>Custom Resource YAML API</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: druid.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: EtcdOperatorTask
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>    name: &lt;name of operator task resource&gt;
</span></span><span style=display:flex><span>    namespace: &lt;cluster namespace&gt;
</span></span><span style=display:flex><span>    generation: &lt;specific generation of the desired state&gt;
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>    type: &lt;type/category of supported out-of-band task&gt;
</span></span><span style=display:flex><span>    ttlSecondsAfterFinished: &lt;time-to-live to garbage collect the custom resource after it has been completed&gt;
</span></span><span style=display:flex><span>    config: &lt;task specific configuration&gt;
</span></span><span style=display:flex><span>    ownerEtcdRefrence: &lt;refer to corresponding etcd owner name and namespace for which task has been invoked&gt;
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>    observedGeneration: &lt;specific observedGeneration of the resource&gt;
</span></span><span style=display:flex><span>    state: &lt;last known current state of the out-of-band task&gt;
</span></span><span style=display:flex><span>    initiatedAt: &lt;time at which task move to any other state from &#34;pending&#34; state&gt;
</span></span><span style=display:flex><span>    lastErrors:
</span></span><span style=display:flex><span>    - code: &lt;error-code&gt;
</span></span><span style=display:flex><span>      description: &lt;description of the error&gt;
</span></span><span style=display:flex><span>      observedAt: &lt;time the error was observed&gt;
</span></span><span style=display:flex><span>    lastOperation:
</span></span><span style=display:flex><span>      name: &lt;operation-name&gt;
</span></span><span style=display:flex><span>      state: &lt;task state as seen at the completion of last operation&gt;
</span></span><span style=display:flex><span>      lastTransitionTime: &lt;time of transition to this state&gt;
</span></span><span style=display:flex><span>      reason: &lt;reason/message if any&gt;
</span></span></code></pre></div><h3 id=lifecycle>Lifecycle</h3><h4 id=creation>Creation</h4><p>Task(s) can be created by creating an instance of the <code>EtcdOperatorTask</code> custom resource specific to a task.</p><blockquote><p>Note: In future, either a <code>kubectl</code> extension plugin or a <code>druidctl</code> tool will be introduced. Dedicated sub-commands will be created for each <code>out-of-band</code> task. This will drastically increase the usability for an operator for performing such tasks, as the CLI extension will automatically create relevant instance(s) of <code>EtcdOperatorTask</code> with the provided configuration.</p></blockquote><h4 id=execution>Execution</h4><ul><li>Authors propose to introduce a new controller which watches for <code>EtcdOperatorTask</code> custom resource.</li><li>Each <code>out-of-band</code> task may have some task specific configuration defined in <a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#spec>.spec.config</a>.</li><li>The controller needs to parse this task specific config, which comes as a <a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#spec>string</a>, according to the schema defined for each task.</li><li>For every <code>out-of-band</code> task, a set of <code>pre-conditions</code> can be defined. These pre-conditions are evaluated against the current state of the target etcd cluster. Based on the evaluation result (boolean), the task is permitted or denied execution.</li><li>If multiple tasks are invoked simultaneously or in <code>pending</code> state, then they will be executed in a First-In-First-Out (FIFO) manner.</li></ul><blockquote><p>Note: Dependent ordering among tasks will be addressed later which will enable concurrent execution of tasks when possible.</p></blockquote><h4 id=deletion>Deletion</h4><p>Upon completion of the task, irrespective of its final state, <code>Etcd-druid</code> will ensure the garbage collection of the task custom resource and any other Kubernetes resources created to execute the task. This will be done according to the <code>.spec.ttlSecondsAfterFinished</code> if defined in the <a href=/docs/other-components/etcd-druid/proposals/05-etcd-operator-tasks/#spec>spec</a>, or a default expiry time will be assumed.</p><h3 id=use-cases>Use Cases</h3><h4 id=recovery-from-permanent-quorum-loss>Recovery from permanent quorum loss</h4><p>Recovery from permanent quorum loss involves two phases - identification and recovery - both of which are done manually today. This proposal intends to automate the latter. Recovery today is a <a href=/docs/other-components/etcd-druid/recovery-from-permanent-quorum-loss-in-etcd-cluster/>multi-step process</a> and needs to be performed carefully by a human operator. Automating these steps would be prudent, to make it quicker and error-free. The identification of the permanent quorum loss would remain a manual process, requiring a human operator to investigate and confirm that there is indeed a permanent quorum loss with no possibility of auto-healing.</p><h5 id=task-config>Task Config</h5><p>We do not need any config for this task. When creating an instance of <code>EtcdOperatorTask</code> for this scenario, <code>.spec.config</code> will be set to nil (unset).</p><h5 id=pre-conditions>Pre-Conditions</h5><ul><li>There should be a quorum loss in a multi-member etcd cluster. For a single-member etcd cluster, invoking this task is unnecessary as the restoration of the single member is automatically handled by the backup-restore process.</li><li>There should not already be a permanent-quorum-loss-recovery-task running for the same etcd cluster.</li></ul><h4 id=trigger-on-demand-snapshot-compaction>Trigger on-demand snapshot compaction</h4><p><code>Etcd-druid</code> provides a configurable <a href=/docs/other-components/etcd-druid/proposals/02-snapshot-compaction/#druid-flags>etcd-events-threshold</a> flag. When this threshold is breached, then a <a href=/docs/other-components/etcd-druid/proposals/02-snapshot-compaction/>snapshot compaction</a> is triggered for the etcd cluster. However, there are scenarios where an ad-hoc snapshot compaction may be required.</p><h5 id=possible-scenarios>Possible scenarios</h5><ul><li>If an operator anticipates a scenario of permanent quorum loss, they can trigger an <code>on-demand snapshot compaction</code> to create a compacted full-snapshot. This can potentially reduce the recovery time from a permanent quorum loss.</li><li>As an additional benefit, a human operator can leverage the current implementation of <a href=/docs/other-components/etcd-druid/proposals/02-snapshot-compaction/>snapshot compaction</a>, which internally triggers <code>restoration</code>. Hence, by initiating an <code>on-demand snapshot compaction</code> task, the operator can verify the integrity of etcd cluster backups, particularly in cases of potential backup corruption or re-encryption. The success or failure of this snapshot compaction can offer valuable insights into these scenarios.</li></ul><h5 id=task-config-1>Task Config</h5><p>We do not need any config for this task. When creating an instance of <code>EtcdOperatorTask</code> for this scenario, <code>.spec.config</code> will be set to nil (unset).</p><h5 id=pre-conditions-1>Pre-Conditions</h5><ul><li>There should not be a <code>on-demand snapshot compaction</code> task already running for the same etcd cluster.</li></ul><blockquote><p>Note: <code>on-demand snapshot compaction</code> runs as a separate job in a separate pod, which interacts with the backup bucket and not the etcd cluster itself, hence it doesn&rsquo;t depend on the health of etcd cluster members.</p></blockquote><h4 id=trigger-on-demand-fulldelta-snapshot>Trigger on-demand full/delta snapshot</h4><p><code>Etcd</code> custom resource provides an ability to set <a href=https://github.com/gardener/etcd-druid/blob/master/api/v1alpha1/etcd.go#L158>FullSnapshotSchedule</a> which currently defaults to run once in 24 hrs. <a href=https://github.com/gardener/etcd-druid/blob/master/api/v1alpha1/types_etcd.go#L171>DeltaSnapshotPeriod</a> is also made configurable which defines the duration after which a delta snapshot will be taken.
If a human operator does not wish to wait for the scheduled full/delta snapshot, they can trigger an on-demand (out-of-schedule) full/delta snapshot on the etcd cluster, which will be taken by the <code>leading-backup-restore</code>.</p><h5 id=possible-scenarios-1>Possible scenarios</h5><ul><li>An on-demand full snapshot can be triggered if scheduled snapshot fails due to any reason.</li><li><a href=/docs/gardener/shoot_hibernate/>Gardener Shoot Hibernation</a>: Every etcd cluster incurs an inherent cost of preserving the volumes even when a gardener shoot control plane is scaled down, i.e the shoot is in a hibernated state. However, it is possible to save on hyperscaler costs by invoking this task to take a full snapshot before scaling down the etcd cluster, and deleting the etcd data volumes afterwards.</li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md>Gardener Control Plane Migration</a>: In <a href=https://github.com/gardener/gardener>gardener</a>, a cluster control plane can be moved from one seed cluster to another. This process currently requires the etcd data to be replicated on the target cluster, so a full snapshot of the etcd cluster in the source seed before the migration would allow for faster restoration of the etcd cluster in the target seed.</li></ul><h5 id=task-config-2>Task Config</h5><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:green>// SnapshotType can be full or delta snapshot.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> SnapshotType <span style=color:#2b91af>string</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>const</span> (
</span></span><span style=display:flex><span>  SnapshotTypeFull SnapshotType = <span style=color:#a31515>&#34;full&#34;</span>
</span></span><span style=display:flex><span>  SnapshotTypeDelta SnapshotType = <span style=color:#a31515>&#34;delta&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>type</span> OnDemandSnapshotTaskConfig <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// Type of on-demand snapshot.
</span></span></span><span style=display:flex><span><span style=color:green></span>  Type SnapshotType <span style=color:#a31515>`json:&#34;type&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  config: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    </span>    type: &lt;type of on-demand snapshot&gt;
</span></span></code></pre></div><h5 id=pre-conditions-2>Pre-Conditions</h5><ul><li>Etcd cluster should have a quorum.</li><li>There should not already be a <code>on-demand snapshot</code> task running with the same <code>SnapshotType</code> for the same etcd cluster.</li></ul><h4 id=trigger-on-demand-maintenance-of-etcd-cluster>Trigger on-demand maintenance of etcd cluster</h4><p>Operator can trigger on-demand <a href=https://etcd.io/docs/v3.5/op-guide/maintenance>maintenance of etcd cluster</a> which includes operations like <a href=https://etcd.io/docs/v3.5/op-guide/maintenance/#history-compaction-v3-api-key-value-database>etcd compaction</a>, <a href=https://etcd.io/docs/v3.5/op-guide/maintenance/#defragmentation>etcd defragmentation</a> etc.</p><h5 id=possible-scenarios-2>Possible Scenarios</h5><ul><li>If an etcd cluster is heavily loaded, which is causing performance degradation of an etcd cluster, and the operator does not want to wait for the scheduled maintenance window then an <code>on-demand maintenance</code> task can be triggered which will invoke etcd-compaction, etcd-defragmentation etc. on the target etcd cluster. This will make the etcd cluster lean and clean, thus improving cluster performance.</li></ul><h5 id=task-config-3>Task Config</h5><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#00f>type</span> OnDemandMaintenanceTaskConfig <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// MaintenanceType defines the maintenance operations need to be performed on etcd cluster.
</span></span></span><span style=display:flex><span><span style=color:green></span>  MaintenanceType maintenanceOps <span style=color:#a31515>`json:&#34;maintenanceType`</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>type</span> maintenanceOps <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// EtcdCompaction if set to true will trigger an etcd compaction on the target etcd.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// +optional
</span></span></span><span style=display:flex><span><span style=color:green></span>  EtcdCompaction <span style=color:#2b91af>bool</span> <span style=color:#a31515>`json:&#34;etcdCompaction,omitempty&#34;`</span>
</span></span><span style=display:flex><span>  <span style=color:green>// EtcdDefragmentation if set to true will trigger a etcd defragmentation on the target etcd.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// +optional
</span></span></span><span style=display:flex><span><span style=color:green></span>  EtcdDefragmentation <span style=color:#2b91af>bool</span> <span style=color:#a31515>`json:&#34;etcdDefragmentation,omitempty&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  config: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    maintenanceType:
</span></span></span><span style=display:flex><span><span style=color:#a31515>      etcdCompaction: &lt;true/false&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>      etcdDefragmentation: &lt;true/false&gt;</span>    
</span></span></code></pre></div><h5 id=pre-conditions-3>Pre-Conditions</h5><ul><li>Etcd cluster should have a quorum.</li><li>There should not already be a duplicate task running with same <code>maintenanceType</code>.</li></ul><h4 id=copy-backups-task>Copy Backups Task</h4><p>Copy the backups(full and delta snapshots) of etcd cluster from one object store(source) to another object store(target).</p><h5 id=possible-scenarios-3>Possible Scenarios</h5><ul><li>In <a href=https://github.com/gardener/gardener>Gardener</a>, the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md>Control Plane Migration</a> process utilizes the copy-backups task. This task is responsible for copying backups from one object store to another, typically located in different regions.</li></ul><h5 id=task-config-4>Task Config</h5><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:green>// EtcdCopyBackupsTaskConfig defines the parameters for the copy backups task.
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>type</span> EtcdCopyBackupsTaskConfig <span style=color:#00f>struct</span> {
</span></span><span style=display:flex><span>  <span style=color:green>// SourceStore defines the specification of the source object store provider.
</span></span></span><span style=display:flex><span><span style=color:green></span>  SourceStore StoreSpec <span style=color:#a31515>`json:&#34;sourceStore&#34;`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// TargetStore defines the specification of the target object store provider for storing backups.
</span></span></span><span style=display:flex><span><span style=color:green></span>  TargetStore StoreSpec <span style=color:#a31515>`json:&#34;targetStore&#34;`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// MaxBackupAge is the maximum age in days that a backup must have in order to be copied.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// By default all backups will be copied.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// +optional
</span></span></span><span style=display:flex><span><span style=color:green></span>  MaxBackupAge *<span style=color:#2b91af>uint32</span> <span style=color:#a31515>`json:&#34;maxBackupAge,omitempty&#34;`</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>// MaxBackups is the maximum number of backups that will be copied starting with the most recent ones.
</span></span></span><span style=display:flex><span><span style=color:green></span>  <span style=color:green>// +optional
</span></span></span><span style=display:flex><span><span style=color:green></span>  MaxBackups *<span style=color:#2b91af>uint32</span> <span style=color:#a31515>`json:&#34;maxBackups,omitempty&#34;`</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  config: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    sourceStore: &lt;source object store specification&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>    targetStore: &lt;target object store specification&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>    maxBackupAge: &lt;maximum age in days that a backup must have in order to be copied&gt;
</span></span></span><span style=display:flex><span><span style=color:#a31515>    maxBackups: &lt;maximum no. of backups that will be copied&gt;</span>    
</span></span></code></pre></div><blockquote><p>Note: For detailed object store specification please refer <a href=https://github.com/gardener/etcd-druid/blob/9c5f8254e3aeb24c1e3e88d17d8d1de336ce981b/api/v1alpha1/types_common.go#L15-L29>here</a></p></blockquote><h5 id=pre-conditions-4>Pre-Conditions</h5><ul><li>There should not already be a <code>copy-backups</code> task running.</li></ul><blockquote><p>Note: <code>copy-backups-task</code> runs as a separate job, and it operates only on the backup bucket, hence it doesn&rsquo;t depend on health of etcd cluster members.</p></blockquote><blockquote><p>Note: <code>copy-backups-task</code> has already been implemented and it&rsquo;s currently being used in <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md>Control Plane Migration</a> but <code>copy-backups-task</code> will be harmonized with <code>EtcdOperatorTask</code> custom resource.</p></blockquote><h2 id=metrics>Metrics</h2><p>Authors proposed to introduce the following metrics:</p><ul><li><p><code>etcddruid_operator_task_duration_seconds</code> : Histogram which captures the runtime for each etcd operator task.
Labels:</p><ul><li>Key: <code>type</code>, Value: all supported tasks</li><li>Key: <code>state</code>, Value: One-Of {failed, succeeded, rejected}</li><li>Key: <code>etcd</code>, Value: name of the target etcd resource</li><li>Key: <code>etcd_namespace</code>, Value: namespace of the target etcd resource</li></ul></li><li><p><code>etcddruid_operator_tasks_total</code>: Counter which counts the number of etcd operator tasks.
Labels:</p><ul><li>Key: <code>type</code>, Value: all supported tasks</li><li>Key: <code>state</code>, Value: One-Of {failed, succeeded, rejected}</li><li>Key: <code>etcd</code>, Value: name of the target etcd resource</li><li>Key: <code>etcd_namespace</code>, Value: namespace of the target etcd resource</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b9ff0666d7d242bb8b08bb74f25db9bd>16 - Recovery From Permanent Quorum Loss In Etcd Cluster</h1><h1 id=recovery-from-permanent-quorum-loss-in-an-etcd-cluster>Recovery from Permanent Quorum Loss in an Etcd Cluster</h1><h2 id=quorum-loss-in-etcd-cluster>Quorum loss in Etcd Cluster</h2><p><a href=https://etcd.io/docs/v3.4/op-guide/recovery/>Quorum loss</a> means when the majority of Etcd pods (greater than or equal to n/2 + 1) are down simultaneously for some reason.</p><p>There are two types of quorum loss that can happen to an <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/>Etcd multinode cluster</a>:</p><ol><li><p><strong>Transient quorum loss</strong> - A quorum loss is called transient when the majority of Etcd pods are down simultaneously for some time. The pods may be down due to network unavailability, high resource usages, etc. When the pods come back after some time, they can re-join the cluster and quorum is recovered automatically without any manual intervention. There should not be a permanent failure for the majority of etcd pods due to hardware failure or disk corruption.</p></li><li><p><strong>Permanent quorum loss</strong> - A quorum loss is called permanent when the majority of Etcd cluster members experience permanent failure, whether due to hardware failure or disk corruption, etc. In that case, the etcd cluster is not going to recover automatically from the quorum loss. A human operator will now need to intervene and execute the following steps to recover the multi-node Etcd cluster.</p></li></ol><p>If permanent quorum loss occurs to a multinode Etcd cluster, the operator needs to note down the PVCs, configmaps, statefulsets, CRs, etc. related to that Etcd cluster and work on those resources only. The following steps guide a human operator to recover from permanent quorum loss of an etcd cluster. We assume the name of the Etcd CR for the Etcd cluster is <code>etcd-main</code>.</p><p><strong>Etcd cluster in shoot control plane of gardener deployment:</strong>
There are two <a href=/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/>Etcd clusters</a> running in the shoot control plane. One is named <code>etcd-events</code> and another is named <code>etcd-main</code>. The operator needs to take care of permanent quorum loss to a specific cluster. If permanent quorum loss occurs to <code>etcd-events</code> cluster, the operator needs to note down the PVCs, configmaps, statefulsets, CRs, etc. related to the <code>etcd-events</code> cluster and work on those resources only.</p><p>⚠️ <strong>Note:</strong> Please note that manually restoring etcd can result in data loss. This guide is the last resort to bring an Etcd cluster up and running again.</p><p>If etcd-druid and etcd-backup-restore is being used with gardener, then:</p><p>Target the control plane of affected shoot cluster via <code>kubectl</code>. Alternatively, you can use <a href=https://github.com/gardener/gardenctl-v2>gardenctl</a> to target the control plane of the affected shoot cluster. You can get the details to target the control plane from the Access tile in the shoot cluster details page on the Gardener dashboard. Ensure that you are targeting the correct namespace.</p><ol><li><p>Add the following annotations to the <code>Etcd</code> resource <code>etcd-main</code>:</p><ol><li><p><code>kubectl annotate etcd etcd-main druid.gardener.cloud/suspend-etcd-spec-reconcile=</code></p></li><li><p><code>kubectl annotate etcd etcd-main druid.gardener.cloud/disable-resource-protection=</code></p></li></ol></li><li><p>Note down the configmap name that is attached to the <code>etcd-main</code> statefulset. If you describe the statefulset with <code>kubectl describe sts etcd-main</code>, look for the lines similar to following lines to identify attached configmap name. It will be needed at later stages:</p><pre tabindex=0><code>Volumes:
  etcd-config-file:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      etcd-bootstrap-4785b0
    Optional:  false
</code></pre><p>Alternatively, the related configmap name can be obtained by executing following command as well:</p><p><code>kubectl get sts etcd-main -o jsonpath='{.spec.template.spec.volumes[?(@.name=="etcd-config-file")].configMap.name}'</code></p></li><li><p>Scale down the <code>etcd-main</code> statefulset replicas to <code>0</code>:</p><p><code>kubectl scale sts etcd-main --replicas=0</code></p></li><li><p>The PVCs will look like the following on listing them with the command <code>kubectl get pvc</code>:</p><pre tabindex=0><code>main-etcd-etcd-main-0        Bound    pv-shoot--garden--aws-ha-dcb51848-49fa-4501-b2f2-f8d8f1fad111   80Gi       RWO            gardener.cloud-fast   13d
main-etcd-etcd-main-1        Bound    pv-shoot--garden--aws-ha-b4751b28-c06e-41b7-b08c-6486e03090dd   80Gi       RWO            gardener.cloud-fast   13d
main-etcd-etcd-main-2        Bound    pv-shoot--garden--aws-ha-ff17323b-d62e-4d5e-a742-9de823621490   80Gi       RWO            gardener.cloud-fast   13d
</code></pre><p>Delete all PVCs that are attached to <code>etcd-main</code> cluster.</p><p><code>kubectl delete pvc -l instance=etcd-main</code></p></li><li><p>Check the etcd&rsquo;s member leases. There should be leases starting with <code>etcd-main</code> as many as <code>etcd-main</code> replicas.
One of those leases will have holder identity as <code>&lt;etcd-member-id>:Leader</code> and rest of etcd member leases have holder identities as <code>&lt;etcd-member-id>:Member</code>.
Please ignore the snapshot leases, i.e., those leases which have the suffix <code>snap</code>.</p><p>etcd-main member leases:</p><pre tabindex=0><code> NAME        HOLDER                  AGE
 etcd-main-0 4c37667312a3912b:Member 1m
 etcd-main-1 75a9b74cfd3077cc:Member 1m
 etcd-main-2 c62ee6af755e890d:Leader 1m
</code></pre><p>Delete all <code>etcd-main</code> member leases.</p></li><li><p>Edit the <code>etcd-main</code> cluster&rsquo;s configmap (ex: <code>etcd-bootstrap-4785b0</code>) as follows:</p><p>Find the <code>initial-cluster</code> field in the configmap. It should look similar to the following:</p><pre tabindex=0><code># Initial cluster
  initial-cluster: etcd-main-0=https://etcd-main-0.etcd-main-peer.default.svc:2380,etcd-main-1=https://etcd-main-1.etcd-main-peer.default.svc:2380,etcd-main-2=https://etcd-main-2.etcd-main-peer.default.svc:2380
</code></pre><p>Change the <code>initial-cluster</code> field to have only one member (<code>etcd-main-0</code>) in the string. It should now look like this:</p><pre tabindex=0><code># Initial cluster
  initial-cluster: etcd-main-0=https://etcd-main-0.etcd-main-peer.default.svc:2380
</code></pre></li><li><p>Scale up the <code>etcd-main</code> statefulset replicas to <code>1</code>:</p><p><code>kubectl scale sts etcd-main --replicas=1</code></p></li><li><p>Wait for the single-member etcd cluster to be completely ready.</p><p><code>kubectl get pods etcd-main-0</code> will give the following output when ready:</p><pre tabindex=0><code>NAME          READY   STATUS    RESTARTS   AGE
etcd-main-0   2/2     Running   0          1m
</code></pre></li><li><p>Remove the following annotations from the <code>Etcd</code> resource <code>etcd-main</code>:</p><ol><li><p><code>kubectl annotate etcd etcd-main druid.gardener.cloud/suspend-etcd-spec-reconcile-</code></p></li><li><p><code>kubectl annotate etcd etcd-main druid.gardener.cloud/disable-resource-protection-</code></p></li></ol></li><li><p>Finally, add the following annotation to the <code>Etcd</code> resource <code>etcd-main</code>:</p><p><code>kubectl annotate etcd etcd-main gardener.cloud/operation='reconcile'</code></p></li><li><p>Verify that the etcd cluster is formed correctly.</p><p>All the <code>etcd-main</code> pods will have outputs similar to following:</p><pre tabindex=0><code>NAME          READY   STATUS    RESTARTS   AGE
etcd-main-0   2/2     Running   0          5m
etcd-main-1   2/2     Running   0          1m
etcd-main-2   2/2     Running   0          1m
</code></pre><p>Additionally, check if the Etcd CR is ready with <code>kubectl get etcd etcd-main</code>:</p><pre tabindex=0><code>NAME        READY   AGE
etcd-main   true    13d
</code></pre><p>Additionally, check the leases for 30 seconds at least. There should be leases starting with <code>etcd-main</code> as many as <code>etcd-main</code> replicas. One of those leases will have holder identity as <code>&lt;etcd-member-id>:Leader</code> and rest of those leases have holder identities as <code>&lt;etcd-member-id>:Member</code>. The <code>AGE</code> of those leases can also be inspected to identify if those leases were updated in conjunction with the restart of the Etcd cluster: Example:</p><pre tabindex=0><code>NAME        HOLDER                  AGE
etcd-main-0 4c37667312a3912b:Member 1m
etcd-main-1 75a9b74cfd3077cc:Member 1m
etcd-main-2 c62ee6af755e890d:Leader 1m
</code></pre></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-1bfa608fb8be2d85d5ccb4207e839b9e>17 - Restoring Single Member In Multi Node Etcd Cluster</h1><h1 id=restoration-of-a-single-member-in-multi-node-etcd-deployed-by-etcd-druid>Restoration of a single member in multi-node etcd deployed by etcd-druid.</h1><p><strong>Note</strong>:</p><ul><li>For a cluster with n members, we are proposing the solution to only single member restoration within a etcd cluster not the quorum loss scenario (when majority of members within a cluster fail).</li><li>In this proposal we are not targetting the recovery of single member which got separated from cluster due to <a href=https://etcd.io/docs/v3.3/op-guide/failures/#network-partition>network partition</a>.</li></ul><h2 id=motivation>Motivation</h2><p>If a single etcd member within a multi-node etcd cluster goes down due to DB corruption/PVC corruption/Invalid data-dir then it needs to be brought back. Unlike in the single-node case, a minority member of a multi-node cluster can&rsquo;t be restored from the snapshots present in storage container as you can&rsquo;t restore from the old snapshots as it contains the metadata information of cluster which leads to <strong>memberID mismatch</strong> that prevents the new member from coming up as new member is getting its metadata information from db which got restore from old snapshots.</p><h2 id=solution>Solution</h2><ul><li>If a corresponding backup-restore sidecar detects that its corresponding etcd is down due to <a href=https://github.com/gardener/etcd-backup-restore/blob/7d27a47f5793b0949492d225ada5fd8344b6b6a2/pkg/initializer/validator/datavalidator.go#L177>data-dir corruption</a> or <a href=https://github.com/gardener/etcd-backup-restore/blob/7d27a47f5793b0949492d225ada5fd8344b6b6a2/pkg/initializer/validator/datavalidator.go#L204>Invalid data-dir</a></li><li>Then backup-restore will first remove the failing etcd member from the cluster using the <a href=https://github.com/etcd-io/etcd/blob/ae9734ed278b7a1a7dfc82e800471ebbf9fce56f/clientv3/cluster.go#L45-L46>MemberRemove API</a> call and clean the data-dir of failed etcd member.</li><li>It won&rsquo;t affect the etcd cluster as quorum is still maintained.</li><li>After successfully removing failed etcd member from the cluster, backup-restore sidecar will try to add a new etcd member to a cluster to get the same cluster size as before.</li><li>Backup-restore firstly adds new member as a <a href=https://etcd.io/docs/v3.3/learning/learner/>Learner</a> using the <a href=https://github.com/etcd-io/etcd/blob/ae9734ed278b7a1a7dfc82e800471ebbf9fce56f/clientv3/cluster.go#L42-L43>MemberAddAsLearner API</a> call, once learner is added to the cluster and it&rsquo;s get in sync with leader and becomes up-to-date then promote the learner(non-voting member) to a voting member using <a href=https://github.com/etcd-io/etcd/blob/ae9734ed278b7a1a7dfc82e800471ebbf9fce56f/clientv3/cluster.go#L51-L52>MemberPromote API</a> call.</li><li>So, the failed member first needs to be removed from the cluster and then added as a new member.</li></ul><h2 id=example>Example:</h2><ol><li>If a <code>3</code> member etcd cluster has 1 downed member(due to invalid data-dir), the cluster can still make forward progress because the quorum is <code>2</code>.</li><li>Etcd downed member get restarted and it&rsquo;s corresponding backup-restore sidecar receives an <a href=https://github.com/gardener/etcd-backup-restore/blob/master/doc/proposals/design.md#workflow>initialization</a> request.</li><li>Then, backup-restore sidecar checks for data corruption/invalid data-dir.</li><li>Backup-restore sidecar detects that data-dir is invalid and its a multi-node etcd cluster.</li><li>Then, backup-restore sidecar removed the downed etcd member from cluster.</li><li>The number of members in a cluster becomes <code>2</code> and the quorum remains at <code>2</code>, so it won&rsquo;t affect the etcd cluster.</li><li>Clean the data-dir and add a member as a learner(non-voting member).</li><li>As soon as learner gets in sync with leader, promote the learner to a voting member, hence increasing number of members in a cluster back to <code>3</code>.</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-4659d2c99bc224e3fc446a2a5cdbe9f1>18 - Supported K8s Versions</h1><h1 id=supported-kubernetes-versions>Supported Kubernetes Versions</h1><p>We strongly recommend using <code>etcd-druid</code> with the supported kubernetes versions, published in this document.
The following is a list of kubernetes versions supported by the respective <code>etcd-druid</code> versions.</p><table><thead><tr><th>Etcd-druid version</th><th>Kubernetes version</th></tr></thead><tbody><tr><td>>=0.20</td><td>>=1.21</td></tr><tr><td>>=0.14 && &lt;0.20</td><td>All versions supported</td></tr><tr><td>&lt;0.14</td><td>&lt; 1.25</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-cecdca0a56e5cb1e472572fe3a161c53>19 - Webhooks</h1><h1 id=webhooks>Webhooks</h1><p>The <a href=/docs/other-components/etcd-druid/concepts/controllers/#controller-manager>etcd-druid controller-manager</a> registers certain <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/>admission webhooks</a> that allow for validation or mutation of requests on resources in the cluster, in order to prevent misconfiguration and restrict access to the etcd cluster resources.</p><p>All webhooks that are a part of etcd-druid reside in package <code>internal/webhook</code>, as sub-packages.</p><h2 id=package-structure>Package Structure</h2><p>The typical package structure for the webhooks that are part of etcd-druid is shown with the <em>EtcdComponents Webhook</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>internal/webhook/etcdcomponents
</span></span><span style=display:flex><span>├── config.go
</span></span><span style=display:flex><span>├── handler.go
</span></span><span style=display:flex><span>└── register.go
</span></span></code></pre></div><ul><li><code>config.go</code>: contains all the logic for the configuration of the webhook, including feature gate activations, CLI flag parsing and validations.</li><li><code>register.go</code>: contains the logic for registering the webhook with the etcd-druid controller manager.</li><li><code>handler.go</code>: contains the webhook admission handler logic.</li></ul><p>Each webhook package may also contain auxiliary files which are relevant to that specific webhook.</p><h2 id=etcd-components-webhook>Etcd Components Webhook</h2><p>Druid controller-manager registers and runs the <a href=/docs/other-components/etcd-druid/concepts/controllers/#etcd-controller>etcd controller</a>, which creates and manages various components/resources such as <code>Leases</code>, <code>ConfigMap</code>s, and the <code>Statefulset</code> for the etcd cluster. It is essential for all these resources to contain correct configuration for the proper functioning of the etcd cluster.</p><p>Unintended changes to any of these <em>managed resources</em> can lead to misconfiguration of the etcd cluster, leading to unwanted downtime for etcd traffic. To prevent such unintended changes, a validating webhook called <em>EtcdComponents Webhook</em> guards these managed resources, ensuring that only authorized entities can perform operations on these managed resources.</p><p><em>EtcdComponents webhook</em> prevents <em>UPDATE</em> and <em>DELETE</em> operations on all resources managed by <em>etcd controller</em>, unless such an operation is performed by druid itself, and during reconciliation of the <code>Etcd</code> resource. Operations are also allowed if performed by one of the authorized entities specified by CLI flag <code>--etcd-components-webhook-exempt-service-accounts</code>, but only if the <code>Etcd</code> resource is not being reconciled by etcd-druid at that time.</p><p>There may be specific cases where a human operator may need to make changes to the managed resources, possibly to test or fix an etcd cluster. An example of this is <a href=/docs/other-components/etcd-druid/recovery-from-permanent-quorum-loss-in-etcd-cluster/>recovery from permanent quorum loss</a>, where a human operator will need to suspend reconciliation of the <code>Etcd</code> resource, make changes to the underlying managed resources such as <code>StatefulSet</code> and <code>ConfigMap</code>, and then resume reconciliation for the <code>Etcd</code> resource. Such manual interventions will require out-of-band changes to the managed resources. Protection of managed resources for such <code>Etcd</code> resources can be turned off by adding an annotation <code>druid.gardener.cloud/disable-etcd-component-protection</code> on the <code>Etcd</code> resource. This will effectively disable <em>EtcdComponents Webhook</em> protection for all managed resources for the specific <code>Etcd</code>.</p><p><strong>Note:</strong> <em>UPDATE</em> operations for <code>Lease</code>s by etcd members are always allowed, since these are regularly updated by the etcd-backup-restore sidecar.</p><p>The <em>Etcd Components Webhook</em> is disabled by default, and can be enabled via the CLI flag `&ndash;enable-etcd-components-webhook.</p></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2023 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.11dbee029dba1a98021fb7be4d7405a7392afb38ff5640a21ff4f4c4c5057b2f.js integrity="sha256-EdvuAp26GpgCH7e+TXQFpzkq+zj/VkCiH/T0xMUFey8=" crossorigin=anonymous></script></body></html>