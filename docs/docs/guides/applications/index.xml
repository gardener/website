<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Applications on Gardener</title><link>https://gardener.cloud/docs/guides/applications/</link><description>Recent content in Applications on Gardener</description><generator>Hugo</generator><language>en-US</language><lastBuildDate>Wed, 24 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://gardener.cloud/docs/guides/applications/index.xml" rel="self" type="application/rss+xml"/><item><title>Shoot Pod Autoscaling Best Practices</title><link>https://gardener.cloud/docs/guides/applications/shoot-pod-autoscaling-best-practices/</link><pubDate>Wed, 24 Jul 2024 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/shoot-pod-autoscaling-best-practices/</guid><description>&lt;h1 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h1>&lt;p>There are two types of pod autoscaling in Kubernetes: Horizontal Pod Autoscaling (HPA) and Vertical Pod Autoscaling (VPA). HPA (implemented as part of the kube-controller-manager) scales the number of pod replicas, while VPA (implemented as independent community project) adjusts the CPU and memory requests for the pods. Both types of autoscaling aim to optimize resource usage/costs and maintain the performance and (high) availability of applications running on Kubernetes.&lt;/p></description></item><item><title>Specifying a Disruption Budget for Kubernetes Controllers</title><link>https://gardener.cloud/docs/guides/applications/pod-disruption-budget/</link><pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/pod-disruption-budget/</guid><description>&lt;h2 id="introduction-of-disruptions">Introduction of Disruptions&lt;a class="td-heading-self-link" href="#introduction-of-disruptions" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>We need to understand that some kind of voluntary disruptions can happen to pods.
For example, they can be caused by cluster administrators who want to perform automated cluster actions, like upgrading and autoscaling clusters.
Typical application owner actions include:&lt;/p>
&lt;ul>
&lt;li>deleting the deployment or other controller that manages the pod&lt;/li>
&lt;li>updating a deployment&amp;rsquo;s pod template causing a restart&lt;/li>
&lt;li>directly deleting a pod (e.g., by accident)&lt;/li>
&lt;/ul>
&lt;h2 id="setup-pod-disruption-budgets">Setup Pod Disruption Budgets&lt;a class="td-heading-self-link" href="#setup-pod-disruption-budgets" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>Kubernetes offers a feature called PodDisruptionBudget (PDB) for each application.
A PDB limits the number of pods of a replicated application that are down simultaneously from voluntary disruptions.&lt;/p></description></item><item><title>Access a Port of a Pod Locally</title><link>https://gardener.cloud/docs/guides/applications/access-pod-from-local/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/access-pod-from-local/</guid><description>&lt;h2 id="question">Question&lt;a class="td-heading-self-link" href="#question" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>You have deployed an application with a web UI or an internal endpoint in your Kubernetes (K8s) cluster. How to access this endpoint &lt;strong>without an external load balancer&lt;/strong> (e.g., Ingress)?&lt;/p>
&lt;p>This tutorial presents two options:&lt;/p>
&lt;ul>
&lt;li>Using Kubernetes port forward&lt;/li>
&lt;li>Using Kubernetes apiserver proxy&lt;/li>
&lt;/ul>
&lt;p>Please note that the options described here are mostly for quick testing or troubleshooting your application. For enabling access to your application for productive environment, please refer to the &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/">official Kubernetes documentation&lt;/a>.&lt;/p></description></item><item><title>Auditing Kubernetes for Secure Setup</title><link>https://gardener.cloud/docs/guides/applications/insecure-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/insecure-configuration/</guid><description>&lt;p>&lt;img src="https://gardener.cloud/__resources/teaser_f209ec.svg" alt="teaser">&lt;/p>
&lt;h2 id="increasing-the-security-of-all-gardener-stakeholders">Increasing the Security of All Gardener Stakeholders&lt;a class="td-heading-self-link" href="#increasing-the-security-of-all-gardener-stakeholders" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>In summer 2018, the &lt;a href="https://github.com/gardener/gardener">Gardener project team&lt;/a> asked &lt;a href="https://kinvolk.io/">Kinvolk&lt;/a> to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work was to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a &lt;a href="https://kubernetes.io/blog/2018/05/17/gardener/#kubernetes-control-plane">Control-Plane-as-a-Service&lt;/a> with a &lt;a href="https://kubernetes.io/blog/2018/05/17/gardener/#network-air-gap">network air gap&lt;/a>.&lt;/p></description></item><item><title>Container Image Not Pulled</title><link>https://gardener.cloud/docs/guides/applications/missing-registry-permission/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/missing-registry-permission/</guid><description>&lt;h2 id="problem">Problem&lt;a class="td-heading-self-link" href="#problem" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>Two of the most common causes of this problems are specifying the wrong container image or trying to use private images without providing registry credentials.&lt;/p>


&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>

 There is no observable difference in pod status between a missing image and incorrect registry permissions. In either case, Kubernetes will report an &lt;code>ErrImagePull&lt;/code> status for the pods. For this reason, this article deals with both scenarios.

&lt;/div>

&lt;h2 id="example">Example&lt;a class="td-heading-self-link" href="#example" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>Let&amp;rsquo;s see an example. We&amp;rsquo;ll create a pod named &lt;em>fail&lt;/em>, referencing a non-existent Docker image:&lt;/p></description></item><item><title>Container Image Not Updating</title><link>https://gardener.cloud/docs/guides/applications/image-pull-policy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/image-pull-policy/</guid><description>&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>A container image should use a fixed tag or the SHA of the image. It should not use the tags &lt;strong>latest&lt;/strong>, &lt;strong>head&lt;/strong>, &lt;strong>canary&lt;/strong>, or other tags that are designed to be &lt;em>floating&lt;/em>.&lt;/p>
&lt;h2 id="problem">Problem&lt;a class="td-heading-self-link" href="#problem" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>If you have encountered this issue, you have probably done something along the lines of:&lt;/p>
&lt;ul>
&lt;li>Deploy anything using an image tag (e.g., &lt;code>cp-enablement/awesomeapp:1.0&lt;/code>)&lt;/li>
&lt;li>Fix a bug in awesomeapp&lt;/li>
&lt;li>Build a new image and push it with the &lt;strong>same tag&lt;/strong> (&lt;code>cp-enablement/awesomeapp:1.0&lt;/code>)&lt;/li>
&lt;li>Update the deployment&lt;/li>
&lt;li>Realize that the bug is still present&lt;/li>
&lt;li>Repeat steps 3-5 without any improvement&lt;/li>
&lt;/ul>
&lt;p>The problem relates to how Kubernetes decides whether to do a &lt;em>docker pull&lt;/em> when starting a container.
Since we tagged our image as &lt;em>:1.0&lt;/em>, the default pull policy is &lt;strong>IfNotPresent&lt;/strong>. The Kubelet already has a local
copy of &lt;code>cp-enablement/awesomeapp:1.0&lt;/code>, so it doesn&amp;rsquo;t attempt to do a docker pull. When the new Pods come up,
they&amp;rsquo;re still using the old broken Docker image.&lt;/p></description></item><item><title>Custom Seccomp Profile</title><link>https://gardener.cloud/docs/guides/applications/secure-seccomp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/secure-seccomp/</guid><description>&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Seccomp">Seccomp&lt;/a> (secure computing mode) is a security facility in the Linux kernel for restricting the set of system calls applications can make.&lt;/p>
&lt;p>Starting from Kubernetes v1.3.0, the Seccomp feature is in &lt;code>Alpha&lt;/code>. To configure it on a &lt;code>Pod&lt;/code>, the following annotations can be used:&lt;/p>
&lt;ul>
&lt;li>&lt;code>seccomp.security.alpha.kubernetes.io/pod: &amp;lt;seccomp-profile&amp;gt;&lt;/code> where &lt;code>&amp;lt;seccomp-profile&amp;gt;&lt;/code> is the seccomp profile to apply to all containers in a &lt;code>Pod&lt;/code>.&lt;/li>
&lt;li>&lt;code>container.seccomp.security.alpha.kubernetes.io/&amp;lt;container-name&amp;gt;: &amp;lt;seccomp-profile&amp;gt;&lt;/code> where &lt;code>&amp;lt;seccomp-profile&amp;gt;&lt;/code> is the seccomp profile to apply to &lt;code>&amp;lt;container-name&amp;gt;&lt;/code> in a &lt;code>Pod&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>More details can be found in the &lt;code>PodSecurityPolicy&lt;/code> &lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp">documentation&lt;/a>.&lt;/p></description></item><item><title>Dockerfile Pitfalls</title><link>https://gardener.cloud/docs/guides/applications/dockerfile-pitfall/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/dockerfile-pitfall/</guid><description>&lt;h2 id="using-the-latest-tag-for-an-image">Using the &lt;code>latest&lt;/code> Tag for an Image&lt;a class="td-heading-self-link" href="#using-the-latest-tag-for-an-image" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>Many Dockerfiles use the &lt;code>FROM package:latest&lt;/code> pattern at the top of their Dockerfiles to pull the latest image from a Docker registry.&lt;/p>
&lt;h3 id="bad-dockerfile">Bad Dockerfile&lt;a class="td-heading-self-link" href="#bad-dockerfile" aria-label="Heading self-link">&lt;/a>&lt;/h3>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-Dockerfile" data-lang="Dockerfile">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">FROM&lt;/span>&lt;span style="color:#a31515"> alpine&lt;/span>&lt;span style="">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>While simple, using the latest tag for an image means that your build can suddenly break if that image gets updated. This can lead to problems where everything builds fine locally (because your local cache thinks it is the latest), while a build server may fail, because some pipelines make a clean pull on every build. Additionally, troubleshooting can prove to be difficult, since the maintainer of the Dockerfile didn&amp;rsquo;t actually make any changes.&lt;/p></description></item><item><title>Dynamic Volume Provisioning</title><link>https://gardener.cloud/docs/guides/applications/dynamic-pvc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/dynamic-pvc/</guid><description>&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>The example shows how to run a Postgres database on Kubernetes and how to dynamically provision and mount the storage volumes needed by the database&lt;/p>
&lt;h2 id="run-postgres-database">Run Postgres Database&lt;a class="td-heading-self-link" href="#run-postgres-database" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>Define the following Kubernetes resources in a yaml file:&lt;/p>
&lt;ul>
&lt;li>PersistentVolumeClaim (PVC)&lt;/li>
&lt;li>Deployment&lt;/li>
&lt;/ul>
&lt;h3 id="persistentvolumeclaim">PersistentVolumeClaim&lt;a class="td-heading-self-link" href="#persistentvolumeclaim" aria-label="Heading self-link">&lt;/a>&lt;/h3>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: PersistentVolumeClaim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: postgresdb-pvc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> accessModes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - ReadWriteOnce
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requests:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> storage: 9Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> storageClassName: &lt;span style="color:#a31515">&amp;#39;default&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This defines a PVC using the storage class &lt;code>default&lt;/code>. Storage classes abstract from the underlying storage provider as well as other parameters, like disk-type (e.g., solid-state vs standard disks).&lt;/p></description></item><item><title>Install Knative in Gardener Clusters</title><link>https://gardener.cloud/docs/guides/applications/knative-install/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/knative-install/</guid><description>&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>This guide walks you through the installation of the latest version of Knative using pre-built images on a &lt;a href="https://gardener.cloud">Gardener&lt;/a> created cluster environment. To set up your own Gardener, see the &lt;a href="https://gardener.cloud/docs/gardener/">documentation&lt;/a> or have a look at the &lt;a href="https://github.com/gardener/landscape-setup-template">landscape-setup-template&lt;/a> project. To learn more about this open source project, read the &lt;a href="https://kubernetes.io/blog/2018/05/17/gardener/">blog on kubernetes.io&lt;/a>.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;a class="td-heading-self-link" href="#prerequisites" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>Knative requires a Kubernetes cluster v1.15 or newer.&lt;/p>
&lt;h2 id="steps">Steps&lt;a class="td-heading-self-link" href="#steps" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;h3 id="install-and-configure-kubectl">Install and Configure kubectl&lt;a class="td-heading-self-link" href="#install-and-configure-kubectl" aria-label="Heading self-link">&lt;/a>&lt;/h3>&lt;ol>
&lt;li>
&lt;p>If you already have &lt;code>kubectl&lt;/code> CLI, run &lt;code>kubectl version --short&lt;/code> to check the version. You need v1.10 or newer. If your &lt;code>kubectl&lt;/code> is older, follow the next step to install a newer version.&lt;/p></description></item><item><title>Integrity and Immutability</title><link>https://gardener.cloud/docs/guides/applications/content_trust/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/content_trust/</guid><description>&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>When transferring data among networked systems, &lt;strong>trust is a central concern&lt;/strong>. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the &lt;strong>integrity and immutability&lt;/strong> of all the data a system operates on. Especially if you use Docker Engine to push and pull images (data) to a &lt;strong>public registry&lt;/strong>.&lt;/p>
&lt;p>This immutability offers you a guarantee that any and all containers that you instantiate will be absolutely identical at inception. Surprise surprise, deterministic operations.&lt;/p></description></item><item><title>Kubernetes Antipatterns</title><link>https://gardener.cloud/docs/guides/applications/antipattern/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/antipattern/</guid><description>&lt;p>&lt;img src="https://gardener.cloud/__resources/howto-antipattern_572177.png" alt="antipattern">&lt;/p>
&lt;p>This HowTo covers common Kubernetes antipatterns that we have seen over the past months.&lt;/p>
&lt;h2 id="running-as-root-user">Running as Root User&lt;a class="td-heading-self-link" href="#running-as-root-user" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>Whenever possible, do not run containers as root user. One could be tempted to say that Kubernetes pods and nodes are well separated. Host and containers running on it share the same kernel. If a container is compromised, the root user in the container has full control over the underlying node.&lt;/p></description></item><item><title>Namespace Isolation</title><link>https://gardener.cloud/docs/guides/applications/network-isolation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/network-isolation/</guid><description>&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>You can configure a &lt;strong>NetworkPolicy&lt;/strong> to deny all the traffic from other namespaces while allowing all the traffic coming from the same namespace the pod was deployed into.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/howto-namespaceisolation_77a6bb.png" alt="howto-namespaceisolation" width="100%">
&lt;p>&lt;strong>There are many reasons why you may chose to employ Kubernetes network policies:&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Isolate multi-tenant deployments&lt;/li>
&lt;li>Regulatory compliance&lt;/li>
&lt;li>Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each other&lt;/li>
&lt;/ul>
&lt;p>Kubernetes &lt;strong>network policies&lt;/strong> are application centric compared to infrastructure/network centric standard firewalls.
&lt;strong>There are no explicit CIDRs or IP addresses used&lt;/strong> for matching source or destination IP’s.
&lt;strong>Network policies build up on labels and selectors&lt;/strong> which are key concepts of Kubernetes that are used to organize (for example, all DB tier pods of an app) and select subsets of objects.&lt;/p></description></item><item><title>Orchestration of Container Startup</title><link>https://gardener.cloud/docs/guides/applications/container-startup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/container-startup/</guid><description>&lt;h2 id="disclaimer">Disclaimer&lt;a class="td-heading-self-link" href="#disclaimer" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>If an application depends on other services deployed separately, do not rely on a certain start sequence of containers. Instead, ensure that the application can cope with unavailability of the services it depends on.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>Kubernetes offers a feature called &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">InitContainers&lt;/a> to perform some tasks during a pod&amp;rsquo;s initialization.
In this tutorial, we demonstrate how to use &lt;code>InitContainers&lt;/code> in order to orchestrate a starting sequence of multiple containers. The tutorial uses the example app &lt;a href="https://medium.com/@xcoulon/deploying-your-first-web-app-on-minikube-6e98d2884b3a">url-shortener&lt;/a>, which consists of two components:&lt;/p></description></item><item><title>Out-Dated HTML and JS Files Delivered</title><link>https://gardener.cloud/docs/guides/applications/service-cache-control/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/service-cache-control/</guid><description>&lt;h2 id="problem">Problem&lt;a class="td-heading-self-link" href="#problem" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>&lt;strong>After updating your HTML and JavaScript sources in your web application, the Kubernetes cluster delivers outdated versions - why?&lt;/strong>&lt;/p>
&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>By default, Kubernetes service pods are not accessible from the external network, but only from other pods within the same Kubernetes cluster.&lt;/p>
&lt;p>The Gardener cluster has a built-in configuration for HTTP load balancing called &lt;strong>Ingress&lt;/strong>, defining rules for external connectivity to Kubernetes services. Users who want external access to their Kubernetes services create an ingress resource that defines rules, including the URI path, backing service name, and other information. The Ingress controller can then automatically program a frontend load balancer to enable Ingress configuration.&lt;/p></description></item><item><title>Remove Committed Secrets in Github 💀</title><link>https://gardener.cloud/docs/guides/applications/commit-secret-fail/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/commit-secret-fail/</guid><description>&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>If you commit sensitive data, such as a &lt;code>kubeconfig.yaml&lt;/code> or &lt;code>SSH key&lt;/code> into a Git repository, you can remove it from
the history. To entirely remove unwanted files from a repository&amp;rsquo;s history you can use the git &lt;code>filter-branch&lt;/code> command.&lt;/p>
&lt;p>The git &lt;code>filter-branch&lt;/code> command rewrites your repository&amp;rsquo;s history, which changes the SHAs for existing commits that you alter and any dependent commits. Changed commit SHAs may affect open pull requests in your repository. &lt;strong>Merging or closing all open pull requests before removing files from your repository is recommended.&lt;/strong>&lt;/p></description></item><item><title>Using Prometheus and Grafana to Monitor K8s</title><link>https://gardener.cloud/docs/guides/applications/prometheus/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/prometheus/</guid><description>&lt;h2 id="disclaimer">Disclaimer&lt;a class="td-heading-self-link" href="#disclaimer" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>This post is meant to give a basic end-to-end description for deploying and using Prometheus and Grafana. Both applications offer a wide range of flexibility, which needs to be considered in case you have specific requirements. Such advanced details are not in the scope of this topic.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;a class="td-heading-self-link" href="#introduction" aria-label="Heading self-link">&lt;/a>&lt;/h2>&lt;p>&lt;a href="https://prometheus.io/">Prometheus&lt;/a> is an open-source systems monitoring and alerting toolkit for recording numeric time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a particular strength.&lt;/p></description></item></channel></rss>