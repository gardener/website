<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener – Administer Client (Shoot) Clusters</title><link>https://gardener.cloud/docs/guides/administer_shoots/</link><description>Recent content in Administer Client (Shoot) Clusters on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Tue, 01 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://gardener.cloud/docs/guides/administer_shoots/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Authenticating with an Identity Provider</title><link>https://gardener.cloud/docs/guides/administer_shoots/oidc-login/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/administer_shoots/oidc-login/</guid><description>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>Please read the following background material on &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens">Authenticating&lt;/a>.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Kubernetes on its own doesn’t provide any user management. In other words, users aren’t managed through Kubernetes resources. Whenever you refer to a human user it’s sufficient to use a unique ID, for example, an email address. Nevertheless, Gardener project owners can use an identity provider to authenticate user access for shoot clusters in the following way:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="#configure-an-identity-provider">Configure an Identity Provider&lt;/a> using &lt;strong>OpenID Connect&lt;/strong> (OIDC).&lt;/li>
&lt;li>&lt;a href="#configure-a-local-kubectl-oidc-login">Configure a local kubectl oidc-login&lt;/a> to enable &lt;code>oidc-login&lt;/code>.&lt;/li>
&lt;li>&lt;a href="#configure-the-shoot-cluster">Configure the shoot cluster&lt;/a> to share details of the OIDC-compliant identity provider with the Kubernetes API Server.&lt;/li>
&lt;li>&lt;a href="#authorize-an-authenticated-user">Authorize an authenticated user&lt;/a> using role-based access control (RBAC).&lt;/li>
&lt;li>&lt;a href="#verify-the-result">Verify the result&lt;/a>&lt;/li>
&lt;/ol>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Gardener allows administrators to modify aspects of the control plane setup. It gives administrators full control of how the control plane is parameterized. While this offers much flexibility, administrators need to ensure that they don’t configure a control plane that goes beyond the service level agreements of the responsible operators team.
&lt;/div>
&lt;h2 id="configure-an-identity-provider">Configure an Identity Provider&lt;/h2>
&lt;p>Create a tenant in an OIDC compatible Identity Provider. For simplicity, we use &lt;em>Auth0&lt;/em>, which has a free plan.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>In your tenant, create a client application to use authentication with &lt;code>kubectl&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Create-client-application_6d15eb.png" alt="Create client application">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Provide a &lt;em>Name&lt;/em>, choose &lt;em>Native&lt;/em> as application type, and choose &lt;em>CREATE&lt;/em>.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Choose-application-type_070f3f.png" alt="Choose application type">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In the tab &lt;em>Settings&lt;/em>, copy the following parameters to a local text file:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;em>Domain&lt;/em>&lt;/p>
&lt;p>Corresponds to the &lt;strong>issuer&lt;/strong> in OIDC. It must be an &lt;code>https&lt;/code>-secured endpoint (Auth0 requires a trailing &lt;code>/&lt;/code> at the end). For more information, see &lt;a href="https://openid.net/specs/openid-connect-core-1_0.html#Terminology">Issuer Identifier&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Client ID&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Client Secret&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Basic-information_ffb8f0.png" alt="Basic information">&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Configure the client to have a callback url of &lt;code>http://localhost:8000&lt;/code>. This callback connects to your local &lt;code>kubectl oidc-login&lt;/code> plugin:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Configure-callback_7ae7ef.png" alt="Configure callback">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Save your changes.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Verify that &lt;code>https://&amp;lt;Auth0 Domain&amp;gt;/.well-known/openid-configuration&lt;/code> is reachable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Choose &lt;em>Users &amp;amp; Roles&lt;/em> &amp;gt; &lt;em>Users&lt;/em> &amp;gt; &lt;em>CREATE USERS&lt;/em> to create a user with a user and password:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Create-user_df6840.png" alt="Create user">&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Users must have a &lt;em>verified&lt;/em> email address.
&lt;/div>
&lt;h2 id="configure-a-local-kubectl-oidc-login">Configure a Local &lt;code>kubectl&lt;/code> &lt;code>oidc-login&lt;/code>&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Install the &lt;code>kubectl&lt;/code> plugin &lt;a href="https://github.com/int128/kubelogin">oidc-login&lt;/a>. We highly recommend the &lt;a href="https://github.com/kubernetes-sigs/krew">krew&lt;/a> installation tool, which also makes other plugins easily available.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl krew install oidc-login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The response looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>Updated the local copy of plugin index.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Installing plugin: oidc-login
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>CAVEATS:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>\
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| See https://github.com/int128/kubelogin for more.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Installed plugin: oidc-login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Prepare a &lt;code>kubeconfig&lt;/code> for later use:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>cp ~/.kube/config ~/.kube/config-oidc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Modify the configuration of &lt;code>~/.kube/config-oidc&lt;/code> as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: shoot--project--mycluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: my-oidc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: shoot--project--mycluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>users:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: my-oidc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: client.authentication.k8s.io/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> command: kubectl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> args:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - oidc-login
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - get-token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --oidc-issuer-url=https://&amp;lt;Issuer&amp;gt;/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --oidc-client-id=&amp;lt;Client ID&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --oidc-client-secret=&amp;lt;Client Secret&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --oidc-extra-scope=email,offline_access,profile
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;p>To test our OIDC-based authentication, the context &lt;code>shoot--project--mycluster&lt;/code> of &lt;code>~/.kube/config-oidc&lt;/code> is used in a later step. For now, continue to use the configuration &lt;code>~/.kube/config&lt;/code> with administration rights for your cluster.&lt;/p>
&lt;h2 id="configure-the-shoot-cluster">Configure the Shoot Cluster&lt;/h2>
&lt;p>Modify the shoot cluster YAML as follows, using the client ID and the domain (as issuer) from the settings of the client application you created in Auth0:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: garden.sapcloud.io/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: mycluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden-project
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeAPIServer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidcConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientID: &amp;lt;Client ID&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerURL: &lt;span style="color:#a31515">&amp;#34;https://&amp;lt;Issuer&amp;gt;/&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernameClaim: email
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This change of the &lt;code>Shoot&lt;/code> manifest triggers a reconciliation. Once the reconciliation is finished, your OIDC configuration is applied. It &lt;strong>doesn&amp;rsquo;t&lt;/strong> invalidate other certificate-based authentication methods. Wait for Gardener to reconcile the change. It can take up to 5 minutes.&lt;/p>
&lt;h2 id="authorize-an-authenticated-user">Authorize an Authenticated User&lt;/h2>
&lt;p>In Auth0, you created a user with a verified email address, &lt;code>test@test.com&lt;/code> in our example. For simplicity, we authorize a single user identified by this email address with the cluster role &lt;code>view&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: rbac.authorization.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterRoleBinding
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: viewer-test
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>roleRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: ClusterRole
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: view
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>subjects:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: User
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: test@test.com
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As administrator, apply the cluster role binding in your shoot cluster.&lt;/p>
&lt;h2 id="verify-the-result">Verify the Result&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>To step into the shoes of your user, use the prepared &lt;code>kubeconfig&lt;/code> file &lt;code>~/.kube/config-oidc&lt;/code>, and switch to the context that uses &lt;code>oidc-login&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>cd ~/.kube
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export KUBECONFIG=$(pwd)/config-oidc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl config use-context `shoot--project--mycluster`
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;code>kubectl&lt;/code> delegates the authentication to plugin &lt;code>oidc-login&lt;/code> the first time the user uses &lt;code>kubectl&lt;/code> to contact the API server, for example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl get all
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The plugin opens a browser for an interactive authentication session with Auth0, and in parallel serves a local webserver for the configured callback.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Enter your login credentials.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Login-through-identity-provider_30610b.png" alt="Login through identity provider">&lt;/p>
&lt;p>You should get a successful response from the API server:&lt;/p>
&lt;pre tabindex="0">&lt;code>Opening in existing browser session.
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/kubernetes ClusterIP 100.64.0.1 &amp;lt;none&amp;gt; 443/TCP 86m
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
&lt;p>After a successful login, &lt;code>kubectl&lt;/code> uses a token for authentication so that you don’t have to provide user and password for every new &lt;code>kubectl&lt;/code> command. How long the token is valid can be configured. If you want to log in again earlier, reset plugin &lt;code>oidc-login&lt;/code>:&lt;/p>
&lt;ol>
&lt;li>Delete directory &lt;code>~/.kube/cache/oidc-login&lt;/code>.&lt;/li>
&lt;li>Delete the browser cache.&lt;/li>
&lt;/ol>
&lt;/div>
&lt;ol>
&lt;li>
&lt;p>To see if your user uses the cluster role &lt;code>view&lt;/code>, do some checks with &lt;code>kubectl auth can-i&lt;/code>.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The response for the following commands should be &lt;code>no&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl auth can-i create clusterrolebindings
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl auth can-i get secrets
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl auth can-i describe secrets
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>The response for the following commands should be &lt;code>yes&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl auth can-i list pods
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl auth can-i get pods
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>If the last step is successful, you’ve configured your cluster to authenticate against an identity provider using OIDC.&lt;/p>
&lt;h2 id="related-links">Related Links&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://auth0.com/pricing/">Auth0 Pricing&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Backup and Restore of Kubernetes Objects</title><link>https://gardener.cloud/docs/guides/administer_shoots/backup-restore/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/administer_shoots/backup-restore/</guid><description>
&lt;p>&lt;img src="https://gardener.cloud/__resources/teaser_0e8021.png" alt="Don&amp;amp;rsquo;t worry &amp;amp;hellip; have a backup">&lt;/p>
&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Details of the description might change in the near future since Heptio was taken over by VMWare which might result in different GitHub repositories or other changes. Please don&amp;rsquo;t hesitate to inform us in case you encounter any issues.
&lt;/div>
&lt;p>In general, Backup and Restore (BR) covers activities enabling an organization to bring a system back in a consistent state, e.g., after a disaster or to setup a new system. These activities vary in a very broad way depending on the applications and its persistency.&lt;/p>
&lt;p>Kubernetes objects like Pods, Deployments, NetworkPolicies, etc. configure Kubernetes internal components and might as well include external components like load balancer and persistent volumes of the cloud provider. The BR of external components and their configurations might be difficult to handle in case manual configurations were needed to prepare these components.&lt;/p>
&lt;p>To set the expectations right from the beginning, this tutorial covers the BR of Kubernetes deployments which might use persistent volumes. The BR of any manual configuration of external components, e.g., via the cloud providers console, is not covered here, as well as the BR of a whole Kubernetes system.&lt;/p>
&lt;p>This tutorial puts the focus on the open source tool &lt;a href="https://velero.io/">Velero&lt;/a> (formerly Heptio Ark) and its functionality to explain the BR process.&lt;/p>
&lt;style>
#body-inner blockquote {
border: 0;
padding: 10px;
margin-top: 40px;
margin-bottom: 40px;
border-radius: 4px;
background-color: rgba(0,0,0,0.05);
box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
position:relative;
padding-left:60px;
}
#body-inner blockquote:before {
content: "i";
font-weight: bold;
position: absolute;
top: 0;
bottom: 0;
left: 0;
background-color: #00a273;
color: white;
vertical-align: middle;
margin: auto;
width: 36px;
font-size: 30px;
text-align: center;
}
&lt;/style>
&lt;p>Basically, Velero allows you to:&lt;/p>
&lt;ul>
&lt;li>backup and restore your Kubernetes cluster resources and persistent volumes (on-demand or scheduled)&lt;/li>
&lt;li>backup or restore all objects in your cluster, or filter resources by type, namespace, and/or label&lt;/li>
&lt;li>by default, all persistent volumes are backed up (configurable)&lt;/li>
&lt;li>replicate your production environment for development and testing environments&lt;/li>
&lt;li>define an expiration date per backup&lt;/li>
&lt;li>execute pre- and post-activities in a container of a pod when a backup is created (see &lt;a href="https://velero.io/docs/main/backup-hooks/#docs">Hooks&lt;/a>)&lt;/li>
&lt;li>extend Velero by Plugins, e.g., for Object and Block store (see &lt;a href="https://velero.io/docs/main/custom-plugins/#docs">Plugins&lt;/a>)&lt;/li>
&lt;/ul>
&lt;p>Velero consists of a server side component and a client tool. The server components consists of Custom Resource Definitions (CRD) and controllers to perform the activities. The client tool communicates with the K8s API server to, e.g., create objects like a Backup object.&lt;/p>
&lt;p>The diagram below explains the backup process. When creating a backup, Velero client makes a call to the Kubernetes API server to create a Backup object (1). The BackupController notices the new Backup object, validates the object (2) and begins the backup process (3). Based on the filter settings provided by the Velero client it collects the resources in question (3). The BackupController creates a tar ball with the Kubernetes objects and stores it in the backup location, e.g., AWS S3 (4) as well as snapshots of persistent volumes (5).&lt;/p>
&lt;p>The size of the backup tar ball corresponds to the number of objects in etcd. The gzipped archive contains the &lt;code>Json&lt;/code> representations of the objects.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/backup-process_275943.png" alt="Backup process">&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
As of the writing of this tutorial, Velero or any other BR tool for Shoot clusters is not provided by Gardener.
&lt;/div>
&lt;h2 id="getting-started">Getting Started&lt;/h2>
&lt;p>At first, clone the &lt;a href="https://github.com/vmware-tanzu/velero-plugin-for-aws">Velero GitHub repository&lt;/a> and get the Velero client from the &lt;a href="https://github.com/vmware-tanzu/velero-plugin-for-aws/releases">releases&lt;/a> or build it from source via &lt;code>make all&lt;/code> in the main directory of the cloned GitHub repository.&lt;/p>
&lt;p>To use an AWS S3 bucket as storage for the backup files and the persistent volumes, you need to:&lt;/p>
&lt;ul>
&lt;li>create a S3 bucket as the backup target&lt;/li>
&lt;li>create an AWS IAM user for Velero&lt;/li>
&lt;li>configure the Velero server&lt;/li>
&lt;li>create a secret for your AWS credentials&lt;/li>
&lt;/ul>
&lt;p>For details about this setup, check the &lt;a href="https://github.com/vmware-tanzu/velero-plugin-for-aws#set-permissions-for-velero">Set Permissions for Velero&lt;/a> documentation. Moreover, it is possible to use other &lt;a href="https://velero.io/docs/main/supported-providers/">supported storage providers&lt;/a>.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Per default, Velero is installed in the namespace &lt;code>velero&lt;/code>. To change the namespace, check the &lt;a href="https://velero.io/docs/main/namespace/#customize-the-namespace-during-install">documentation&lt;/a>.
&lt;/div>
&lt;p>Velero offers a wide range of filter possibilities for Kubernetes resources, e.g filter by namespaces, labels or resource types. The filter settings can be combined and used as &lt;em>include&lt;/em> or &lt;em>exclude&lt;/em>, which gives a great flexibility for selecting resources.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Carefully set labels and/or use namespaces for your deployments to make the selection of the resources to be backed up easier. The best practice would be to check in advance which resources are selected with the defined filter.
&lt;/div>
&lt;h2 id="exemplary-use-cases">Exemplary Use Cases&lt;/h2>
&lt;p>Below are some use cases which could give you an idea on how to use Velero. You can also check &lt;a href="https://velero.io/docs/main/">Velero&amp;rsquo;s documentation&lt;/a> for other introductory examples.&lt;/p>
&lt;h3 id="helm-based-deployments">Helm Based Deployments&lt;/h3>
&lt;p>To be able to use Helm charts in your Kubernetes cluster, you need to install the Helm client &lt;code>helm&lt;/code> and the server component &lt;code>tiller&lt;/code>. Per default the server component is installed in the namespace &lt;code>kube-system&lt;/code>. Even if it is possible to select single deployments via the filter settings of Velero, you should consider to install &lt;code>tiller&lt;/code> in a separate namespace via &lt;code>helm init --tiller-namespace &amp;lt;your namespace&amp;gt;&lt;/code>. This approach applies as well for all Helm charts to be deployed - consider separate namespaces for your deployments as well by using the parameter &lt;code>--namespace&lt;/code>.&lt;/p>
&lt;p>To backup a Helm based deployment, you need to backup both Tiller &lt;em>and&lt;/em> the deployment. Only then the deployments could be managed via Helm. As mentioned above, the selection of resources would be easier in case they are separated in namespaces.&lt;/p>
&lt;h3 id="separate-backup-locations">Separate Backup Locations&lt;/h3>
&lt;p>In case you run all your Kubernetes clusters on a single cloud provider, there is probably no need to store the backups in a bucket of a different cloud provider. However, if you run Kubernetes clusters on different cloud provider, you might consider to use a bucket on just one cloud provider as the target for the backups, e.g., to benefit from a lower price tag for the storage.&lt;/p>
&lt;p>Per default, Velero assumes that both the persistent volumes and the backup location are on the same cloud provider. During the setup of Velero, a secret is created using the credentials for a cloud provider user who has access to both objects (see the policies, e.g., for the &lt;a href="https://github.com/vmware-tanzu/velero-plugin-for-aws#set-permissions-for-velero">AWS configuration&lt;/a>).&lt;/p>
&lt;p>Now, since the backup location is different from the volume location, you need to follow these steps (described here for AWS):&lt;/p>
&lt;ul>
&lt;li>
&lt;p>configure as documented the volume storage location in &lt;code>examples/aws/06-volumesnapshotlocation.yaml&lt;/code> and provide the user credentials. In this case, the S3 related settings like the policies can be omitted&lt;/p>
&lt;/li>
&lt;li>
&lt;p>create the bucket for the backup in the cloud provider in question and a user with the appropriate credentials and store them in a separate file similar to &lt;code>credentials-ark&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>create a secret which contains two credentials, one for the volumes and one for the backup target, e.g., by using the command &lt;code>kubectl create secret generic cloud-credentials --namespace heptio-ark --from-file cloud=credentials-ark --from-file backup-target=backup-ark&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>configure in the deployment manifest &lt;code>examples/aws/10-deployment.yaml&lt;/code> the entries in &lt;code>volumeMounts&lt;/code>, &lt;code>env&lt;/code> and &lt;code>volumes&lt;/code> accordingly, e.g., for a cluster running on AWS and the backup target bucket on GCP a configuration could look similar to:&lt;/p>
&lt;details>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Some links might get broken in the near future since Heptio was taken over by VMWare which might result in different GitHub repositories or other changes. Please don&amp;rsquo;t hesitate to inform us in case you encounter any issues.
&lt;/div>
&lt;summary>Example Velero deployment&lt;/summary>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Copyright 2017 the Heptio Ark contributors.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># you may not use this file except in compliance with the License.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># You may obtain a copy of the License at&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># http://www.apache.org/licenses/LICENSE-2.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Unless required by applicable law or agreed to in writing, software&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># See the License for the specific language governing permissions and&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># limitations under the License.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Deployment
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: velero
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: velero
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> replicas: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> component: velero
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> prometheus.io/scrape: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> prometheus.io/port: &lt;span style="color:#a31515">&amp;#34;8085&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> prometheus.io/path: &lt;span style="color:#a31515">&amp;#34;/metrics&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> restartPolicy: Always
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> serviceAccountName: velero
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: velero
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> image: gcr.io/heptio-images/velero:latest
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> command:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - /velero
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> args:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumeMounts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: cloud-credentials
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /credentials
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: plugins
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /plugins
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: scratch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /scratch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> env:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: AWS_SHARED_CREDENTIALS_FILE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: /credentials/cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: GOOGLE_APPLICATION_CREDENTIALS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: /credentials/backup-target
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: VELERO_SCRATCH_DIR
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: /scratch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: cloud-credentials
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secret:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretName: cloud-credentials
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: plugins
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> emptyDir: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: scratch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> emptyDir: {}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/details>
&lt;/li>
&lt;li>
&lt;p>finally, configure the backup storage location in &lt;code>examples/aws/05-backupstoragelocation.yaml&lt;/code> to use, in this case, a GCP bucket&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="limitations">Limitations&lt;/h2>
&lt;p>Below is a potentially incomplete list of limitations. You can also consult &lt;a href="https://velero.io/docs/main/">Velero&amp;rsquo;s documentation&lt;/a> to get up to date information.&lt;/p>
&lt;ul>
&lt;li>Only full backups of selected resources are supported. Incremental backups are not (yet) supported. However, by using filters it is possible to restrict the backup to specific resources&lt;/li>
&lt;li>Inconsistencies might occur in case of changes during the creation of the backup&lt;/li>
&lt;li>Application specific actions are not considered by default. However, they might be handled by using Velero&amp;rsquo;s &lt;a href="https://velero.io/docs/main/backup-hooks/#docs">Hooks&lt;/a> or &lt;a href="https://velero.io/docs/main/custom-plugins/#docs">Plugins&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Create / Delete a Shoot Cluster</title><link>https://gardener.cloud/docs/guides/administer_shoots/create-delete-shoot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/administer_shoots/create-delete-shoot/</guid><description>
&lt;h2 id="create-a-shoot-cluster">Create a Shoot Cluster&lt;/h2>
&lt;p>As you have already prepared an &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml">example Shoot manifest&lt;/a> in the steps described in the development documentation, please open another Terminal pane/window with the &lt;code>KUBECONFIG&lt;/code> environment variable pointing to the Garden development cluster and send the manifest to the Kubernetes API server:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl apply -f your-shoot-aws.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You should see that Gardener has immediately picked up your manifest and has started to deploy the Shoot cluster.&lt;/p>
&lt;p>In order to investigate what is happening in the Seed cluster, please download its proper Kubeconfig yourself (see next paragraph). The namespace of the Shoot cluster in the Seed cluster will look like that: &lt;code>shoot-johndoe-johndoe-1&lt;/code>, whereas the first &lt;code>johndoe&lt;/code> is your namespace in the Garden cluster (also called &amp;ldquo;project&amp;rdquo;) and the &lt;code>johndoe-1&lt;/code> suffix is the actual name of the Shoot cluster.&lt;/p>
&lt;p>To connect to the newly created Shoot cluster, you must download its Kubeconfig as well. Please connect to the proper Seed cluster, navigate to the Shoot namespace, and download the Kubeconfig from the &lt;code>kubecfg&lt;/code> secret in that namespace.&lt;/p>
&lt;h2 id="delete-a-shoot-cluster">Delete a Shoot Cluster&lt;/h2>
&lt;p>In order to delete your cluster, you have to set an annotation confirming the deletion first, and trigger the deletion after that. You can use the prepared &lt;code>delete shoot&lt;/code> script which takes the Shoot name as first parameter. The namespace can be specified by the second parameter, but it is optional. If you don&amp;rsquo;t state it, it defaults to your namespace (the username you are logged in with to your machine).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ ./hack/usage/delete shoot johndoe-1 johndoe
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>(the &lt;code>hack&lt;/code> bash script can be found at &lt;a href="https://github.com/gardener/gardener/blob/master/hack/usage/delete">GitHub&lt;/a>)&lt;/p>
&lt;h1 id="configure-a-shoot-cluster-alert-receiver">Configure a Shoot cluster alert receiver&lt;/h1>
&lt;p>The receiver of the Shoot alerts can be configured from the &lt;code>.spec.monitoring.alerting.emailReceivers&lt;/code> section in the Shoot specification. The value of the field has to be a list of valid mail addresses.&lt;/p>
&lt;p>The alerting for the Shoot clusters is handled by the Prometheus Alertmanager. The Alertmanager will be deployed next to the control plane when the &lt;code>Shoot&lt;/code> resource specifies &lt;code>.spec.monitoring.alerting.emailReceivers&lt;/code> and if a &lt;a href="https://github.com/gardener/gardener/blob/master/example/10-secret-alerting.yaml">SMTP secret&lt;/a> exists.&lt;/p>
&lt;p>If the field gets removed then the Alertmanager will be also removed during the next reconcilation of the cluster. The opposite is also valid if the field is added to an existing cluster.&lt;/p></description></item><item><title>Docs: Create a Shoot Cluster Into an Existing AWS VPC</title><link>https://gardener.cloud/docs/guides/administer_shoots/create-shoot-into-existing-aws-vpc/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/administer_shoots/create-shoot-into-existing-aws-vpc/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Gardener can create a new VPC, or use an existing one for your shoot cluster. Depending on your needs, you may want to create shoot(s) into an already created VPC.
The tutorial describes how to create a shoot cluster into an existing AWS VPC. The steps are identical for Alicloud, Azure, and GCP. Please note that the existing VPC must be in the same region like the shoot cluster that you want to deploy into the VPC.&lt;/p>
&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;p>If &lt;code>.spec.provider.infrastructureConfig.networks.vpc.cidr&lt;/code> is specified, Gardener will create a new VPC with the given CIDR block and respectively will delete it on shoot deletion.&lt;br>
If &lt;code>.spec.provider.infrastructureConfig.networks.vpc.id&lt;/code> is specified, Gardener will use the existing VPC and respectively won&amp;rsquo;t delete it on shoot deletion.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
&lt;p>It&amp;rsquo;s not recommended to create a shoot cluster into a VPC that is managed by Gardener (that is created for another shoot cluster). In this case the deletion of the initial shoot cluster will fail to delete the VPC because there will be resources attached to it.&lt;/p>
&lt;p>Gardener won&amp;rsquo;t delete any manually created (unmanaged) resources in your cloud provider account.&lt;/p>
&lt;/div>
&lt;h2 id="1-configure-the-aws-cli">1. Configure the AWS CLI&lt;/h2>
&lt;p>The &lt;code>aws configure&lt;/code> command is a convenient way to setup your AWS CLI. It will prompt you for your credentials and settings which will be used in the following AWS CLI invocations:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ aws configure
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>AWS Access Key ID [None]: &amp;lt;ACCESS_KEY_ID&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>AWS Secret Access Key [None]: &amp;lt;SECRET_ACCESS_KEY&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Default region name [None]: &amp;lt;DEFAULT_REGION&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Default output format [None]: &amp;lt;DEFAULT_OUTPUT_FORMAT&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="2-create-a-vpc">2. Create a VPC&lt;/h2>
&lt;p>Create the VPC by running the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ aws ec2 create-vpc --cidr-block &amp;lt;cidr-block&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;Vpc&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;VpcId&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;vpc-ff7bbf86&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;InstanceTenancy&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;default&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;Tags&amp;#34;&lt;/span>: [],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;CidrBlockAssociations&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;AssociationId&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;vpc-cidr-assoc-6e42b505&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;CidrBlock&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;10.0.0.0/16&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;CidrBlockState&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;State&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;associated&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;Ipv6CidrBlockAssociationSet&amp;#34;&lt;/span>: [],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;State&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;pending&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;DhcpOptionsId&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;dopt-38f7a057&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;CidrBlock&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;10.0.0.0/16&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;IsDefault&amp;#34;&lt;/span>: false
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Gardener requires the VPC to have enabled &lt;a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html">DNS support&lt;/a>, i.e the attributes &lt;code>enableDnsSupport&lt;/code> and &lt;code>enableDnsHostnames&lt;/code> must be set to &lt;em>true&lt;/em>. &lt;code>enableDnsSupport&lt;/code> attribute is enabled by default, &lt;code>enableDnsHostnames&lt;/code> - not. Set the &lt;code>enableDnsHostnames&lt;/code> attribute to &lt;em>true&lt;/em>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ aws ec2 modify-vpc-attribute --vpc-id vpc-ff7bbf86 --enable-dns-hostnames
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="3-create-an-internet-gateway">3. Create an Internet Gateway&lt;/h2>
&lt;p>Gardener also requires that an internet gateway is attached to the VPC. You can create one by using:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ aws ec2 create-internet-gateway
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;InternetGateway&amp;#34;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;Tags&amp;#34;&lt;/span>: [],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;InternetGatewayId&amp;#34;&lt;/span>: &lt;span style="color:#a31515">&amp;#34;igw-c0a643a9&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;Attachments&amp;#34;&lt;/span>: []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>and attach it to the VPC using:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ aws ec2 attach-internet-gateway --internet-gateway-id igw-c0a643a9 --vpc-id vpc-ff7bbf86
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="4-create-the-shoot">4. Create the Shoot&lt;/h2>
&lt;p>Prepare your shoot manifest (you could check the &lt;a href="https://github.com/gardener/gardener/tree/master/example">example manifests&lt;/a>). Please make sure that you choose the region in which you had created the VPC earlier (step 2). Also, put your VPC ID in the &lt;code>.spec.provider.infrastructureConfig.networks.vpc.id&lt;/code> field:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> region: &amp;lt;aws-region-of-vpc&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: aws
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> infrastructureConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: InfrastructureConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networks:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> vpc:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> id: vpc-ff7bbf86
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># ...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Apply your shoot manifest:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl apply -f your-shoot-aws.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Ensure that the shoot cluster is properly created:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl get shoot $SHOOT_NAME -n $SHOOT_NAMESPACE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME CLOUDPROFILE VERSION SEED DOMAIN OPERATION PROGRESS APISERVER CONTROL NODES SYSTEM AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;SHOOT_NAME&amp;gt; aws 1.15.0 aws &amp;lt;SHOOT_DOMAIN&amp;gt; Succeeded 100 True True True True 20m
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: GPU Enabled Cluster</title><link>https://gardener.cloud/docs/guides/administer_shoots/gpu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/administer_shoots/gpu/</guid><description>
&lt;h2 id="disclaimer">Disclaimer&lt;/h2>
&lt;p>Be aware, that the following sections might be opinionated. Kubernetes, and the GPU support in particular,
are rapidly evolving, which means that this guide is likely to be outdated sometime soon. For this reason,
&lt;strong>contributions are highly appreciated&lt;/strong> to update this guide.&lt;/p>
&lt;h2 id="create-a-cluster">Create a Cluster&lt;/h2>
&lt;p>First thing first, let’s create a Kubernetes (K8s) cluster with GPU accelerated nodes. In this example we will use an AWS
&lt;strong>p2.xlarge&lt;/strong> EC2 instance because it&amp;rsquo;s the cheapest available option at the moment. Use such cheap instances
for learning to limit your resource costs. &lt;strong>This costs around 1€/hour per GPU&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/howto-gpu_c83e8f.png" alt="gpu-selection">&lt;/p>
&lt;h2 id="install-nvidia-driver-as-daemonset">Install NVidia Driver as Daemonset&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: DaemonSet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nvidia-driver-installer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> k8s-app: nvidia-driver-installer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> selector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nvidia-driver-installer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> k8s-app: nvidia-driver-installer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nvidia-driver-installer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> k8s-app: nvidia-driver-installer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPID: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> initContainers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - image: squat/modulus:4a1799e7aa0143bcbb70d354bab3e419b1f54972
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: modulus
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> args:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - compile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - nvidia
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - &lt;span style="color:#a31515">&amp;#34;410.104&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> securityContext:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> privileged: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> env:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: MODULUS_CHROOT
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: MODULUS_INSTALL
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: MODULUS_INSTALL_DIR
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: /opt/drivers
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: MODULUS_CACHE_DIR
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: /opt/modulus/cache
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: MODULUS_LD_ROOT
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: /root
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: IGNORE_MISSING_MODULE_SYMVERS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: &lt;span style="color:#a31515">&amp;#34;1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumeMounts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: etc-coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /etc/coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> readOnly: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: usr-share-coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /usr/share/coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> readOnly: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: ld-root
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /root
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: module-cache
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /opt/modulus/cache
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: module-install-dir-base
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /opt/drivers
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - image: &lt;span style="color:#a31515">&amp;#34;gcr.io/google-containers/pause:3.1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: pause
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tolerations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: &lt;span style="color:#a31515">&amp;#34;nvidia.com/gpu&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> effect: &lt;span style="color:#a31515">&amp;#34;NoSchedule&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operator: &lt;span style="color:#a31515">&amp;#34;Exists&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: etc-coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /etc/coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: usr-share-coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /usr/share/coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: ld-root
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: module-cache
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /opt/modulus/cache
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: module-install-dir-base
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /opt/drivers
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="install-device-plugin">Install Device Plugin&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: DaemonSet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nvidia-gpu-device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> k8s-app: nvidia-gpu-device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#addonmanager.kubernetes.io/mode: Reconcile&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> selector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> k8s-app: nvidia-gpu-device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> k8s-app: nvidia-gpu-device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scheduler.alpha.kubernetes.io/critical-pod: &lt;span style="color:#a31515">&amp;#39;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> priorityClassName: system-node-critical
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /var/lib/kubelet/device-plugins
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hostPath:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: /dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - image: &lt;span style="color:#a31515">&amp;#34;k8s.gcr.io/nvidia-gpu-device-plugin@sha256:08509a36233c5096bb273a492251a9a5ca28558ab36d74007ca2a9d3f0b61e1d&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> command: [&lt;span style="color:#a31515">&amp;#34;/usr/bin/nvidia-gpu-device-plugin&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;-logtostderr&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;-host-path=/opt/drivers/nvidia&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nvidia-gpu-device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requests:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpu: 50m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memory: 10Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> limits:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpu: 50m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memory: 10Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> securityContext:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> privileged: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumeMounts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /device-plugin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> updateStrategy:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: RollingUpdate
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="test">Test&lt;/h2>
&lt;p>To run an example training on a GPU node, first start a base image with Tensorflow with GPU support &amp;amp; Keras:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Deployment
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: deeplearning-workbench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> replicas: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> selector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: deeplearning-workbench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: deeplearning-workbench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: deeplearning-workbench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> image: afritzler/deeplearning-workbench
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> limits:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nvidia.com/gpu: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tolerations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: &lt;span style="color:#a31515">&amp;#34;nvidia.com/gpu&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> effect: &lt;span style="color:#a31515">&amp;#34;NoSchedule&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operator: &lt;span style="color:#a31515">&amp;#34;Exists&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
&lt;p>the &lt;code>tolerations&lt;/code> section above is not required if you deploy the &lt;code>ExtendedResourceToleration&lt;/code>
admission controller to your cluster. You can do this in the &lt;code>kubernetes&lt;/code> section of your Gardener
cluster &lt;code>shoot.yaml&lt;/code> as follows:&lt;/p>
&lt;pre tabindex="0">&lt;code> kubernetes:
kubeAPIServer:
admissionPlugins:
- name: ExtendedResourceToleration
&lt;/code>&lt;/pre>
&lt;/div>
&lt;p>Now exec into the container and start an example Keras training:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl exec -it deeplearning-workbench-8676458f5d-p4d2v -- /bin/bash
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cd /keras/example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>python imdb_cnn.py
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="related-links">Related Links&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/afritzler/kubernetes-gpu">Andreas Fritzler&lt;/a> from the Gardener Core team for the R&amp;amp;D, who has provided this setup.&lt;/li>
&lt;li>&lt;a href="https://github.com/squat/modulus">Build and install NVIDIA driver on CoreOS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml">Nvidia Device Plugin&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Shoot Cluster Maintenance</title><link>https://gardener.cloud/docs/guides/administer_shoots/maintain-shoot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/administer_shoots/maintain-shoot/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Day two operations for shoot clusters are related to:&lt;/p>
&lt;ul>
&lt;li>The Kubernetes version of the control plane and the worker nodes&lt;/li>
&lt;li>The operating system version of the worker nodes&lt;/li>
&lt;/ul>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
When referring to an update of the &amp;ldquo;operating system version&amp;rdquo; in this document, the update of the machine image of the shoot cluster&amp;rsquo;s worker nodes is meant. For example, Amazon Machine Images (AMI) for AWS.
&lt;/div>
&lt;p>The following table summarizes what options Gardener offers to maintain these versions:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;/th>
&lt;th style="text-align:left">Auto-Update&lt;/th>
&lt;th style="text-align:left">Forceful Updates&lt;/th>
&lt;th style="text-align:left">Manual Updates&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">Kubernetes version&lt;/td>
&lt;td style="text-align:left">Patches only&lt;/td>
&lt;td style="text-align:left">Patches and consecutive minor updates only&lt;/td>
&lt;td style="text-align:left">yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Operating system version&lt;/td>
&lt;td style="text-align:left">yes&lt;/td>
&lt;td style="text-align:left">yes&lt;/td>
&lt;td style="text-align:left">yes&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="allowed-target-versions-in-the-cloudprofile">Allowed Target Versions in the &lt;code>CloudProfile&lt;/code>&lt;/h2>
&lt;p>Administrators maintain the allowed target versions that you can update to in the &lt;code>CloudProfile&lt;/code> for each IaaS-Provider. Users with access to a Gardener project can check supported target versions with:&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl get cloudprofile [IAAS-SPECIFIC-PROFILE] -o yaml
&lt;/code>&lt;/pre>&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Path&lt;/th>
&lt;th style="text-align:left">Description&lt;/th>
&lt;th style="text-align:left">More Information&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">&lt;code>spec.kubernetes.versions&lt;/code>&lt;/td>
&lt;td style="text-align:left">The supported Kubernetes version &lt;code>major.minor.patch&lt;/code>.&lt;/td>
&lt;td style="text-align:left">&lt;a href="https://github.com/kubernetes/design-proposals-archive/blob/main/release/versioning.md#patch-releases">Patch releases&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">&lt;code>spec.machineImages&lt;/code>&lt;/td>
&lt;td style="text-align:left">The supported operating system versions for worker nodes&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Both the Kubernetes version and the operating system version follow semantic versioning that allows Gardener to handle updates automatically.&lt;/p>
&lt;p>For more information, see &lt;a href="http://semver.org/">Semantic Versioning&lt;/a>.&lt;/p>
&lt;h3 id="impact-of-version-classifications-on-updates">Impact of Version Classifications on Updates&lt;/h3>
&lt;p>Gardener allows to classify versions in the &lt;code>CloudProfile&lt;/code> as &lt;code>preview&lt;/code>, &lt;code>supported&lt;/code>, &lt;code>deprecated&lt;/code>, or &lt;code>expired&lt;/code>. During maintenance operations, &lt;code>preview&lt;/code> versions are excluded from updates, because they’re often recently released versions that haven’t yet undergone thorough testing and may contain bugs or security issues.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_versions/#version-classifications">Version Classifications&lt;/a>.&lt;/p>
&lt;h2 id="let-gardener-manage-your-updates">Let Gardener Manage Your Updates&lt;/h2>
&lt;h3 id="the-maintenance-window">The Maintenance Window&lt;/h3>
&lt;p>Gardener can manage updates for you automatically. It offers users to specify a &lt;em>maintenance window&lt;/em> during which updates are scheduled:&lt;/p>
&lt;ul>
&lt;li>The time interval of the maintenance window can’t be less than 30 minutes or more than 6 hours.&lt;/li>
&lt;li>If there’s no maintenance window specified during the creation of a shoot cluster, Gardener chooses a maintenance window randomly to spread the load.&lt;/li>
&lt;/ul>
&lt;p>You can either specify the maintenance window in the shoot cluster specification (&lt;code>.spec.maintenance.timeWindow&lt;/code>) or the start time of the maintenance window using the Gardener dashboard (&lt;strong>CLUSTERS&lt;/strong> &amp;gt; &lt;strong>[YOUR-CLUSTER]&lt;/strong> &amp;gt; &lt;strong>OVERVIEW&lt;/strong> &amp;gt; &lt;strong>Lifecycle&lt;/strong> &amp;gt; &lt;strong>Maintenance&lt;/strong>).&lt;/p>
&lt;h3 id="auto-update-and-forceful-updates">Auto-Update and Forceful Updates&lt;/h3>
&lt;p>To trigger updates during the maintenance window automatically, Gardener offers the following methods:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;em>Auto-update&lt;/em>: &lt;br>Gardener starts an update during the next maintenance window whenever there’s a version available in the &lt;code>CloudProfile&lt;/code> that is higher than the one of your shoot cluster specification, and that isn’t classified as &lt;code>preview&lt;/code> version. For Kubernetes versions, auto-update only updates to higher patch levels.&lt;/p>
&lt;p>You can either activate auto-update on the Gardener dashboard (&lt;strong>CLUSTERS&lt;/strong> &amp;gt; &lt;strong>[YOUR-CLUSTER]&lt;/strong> &amp;gt; &lt;strong>OVERVIEW&lt;/strong> &amp;gt; &lt;strong>Lifecycle&lt;/strong> &amp;gt; &lt;strong>Maintenance&lt;/strong>) or in the shoot cluster specification:&lt;/p>
&lt;ul>
&lt;li>&lt;code>.spec.maintenance.autoUpdate.kubernetesVersion: true&lt;/code>&lt;/li>
&lt;li>&lt;code>.spec.maintenance.autoUpdate.machineImageVersion: true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Forceful updates&lt;/em>: &lt;br>In the maintenance window, Gardener compares the current version given in the shoot cluster specification with the version list in the &lt;code>CloudProfile&lt;/code>. If the version has an expiration date and if the date is before the start of the maintenance window, Gardener starts an update to the highest version available in the &lt;code>CloudProfile&lt;/code> that isn’t classified as &lt;code>preview&lt;/code> version. The highest version in &lt;code>CloudProfile&lt;/code> can’t have an expiration date. For Kubernetes versions, Gardener only updates to higher patch levels or consecutive minor versions.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>If you don’t want to wait for the next maintenance window, you can annotate the shoot cluster specification with &lt;code>shoot.gardener.cloud/operation: maintain&lt;/code>. Gardener then checks immediately if there’s an auto-update or a forceful update needed.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Forceful version updates are executed even if the auto-update for the Kubernetes version(or the auto-update for the machine image version) is deactivated (set to &lt;code>false&lt;/code>).
&lt;/div>
&lt;p>With expiration dates, administrators can give shoot cluster owners more time for testing before the actual version update happens, which allows for smoother transitions to new versions.&lt;/p>
&lt;h3 id="kubernetes-update-paths">Kubernetes Update Paths&lt;/h3>
&lt;p>The bigger the delta of the Kubernetes source version and the Kubernetes target version, the better it must be planned and executed by operators. Gardener only provides automatic support for updates that can be applied safely to the cluster workload:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Update Type&lt;/th>
&lt;th style="text-align:left">Example&lt;/th>
&lt;th style="text-align:left">Update Method&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">Patches&lt;/td>
&lt;td style="text-align:left">&lt;code>1.10.12&lt;/code> to &lt;code>1.10.13&lt;/code>&lt;/td>
&lt;td style="text-align:left">auto-update or Forceful update&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Update to consecutive minor version&lt;/td>
&lt;td style="text-align:left">&lt;code>1.10.12&lt;/code> to &lt;code>1.11.10&lt;/code>&lt;/td>
&lt;td style="text-align:left">Forceful update&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">Other&lt;/td>
&lt;td style="text-align:left">&lt;code>1.10.12&lt;/code> to &lt;code>1.12.0&lt;/code>&lt;/td>
&lt;td style="text-align:left">Manual update&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Gardener doesn’t support automatic updates of nonconsecutive minor versions, because Kubernetes doesn’t guarantee updateability in this case. However, multiple minor version updates are possible if not only the minor source version is expired, but also the minor target version is expired. Gardener then updates the Kubernetes version first to the expired target version, and waits for the next maintenance window to update this version to the next minor target version.&lt;/p>
&lt;div class="alert alert-warning" role="alert">
&lt;h4 class="alert-heading">Warning&lt;/h4>
The administrator who maintains the &lt;code>CloudProfile&lt;/code> has to ensure that the list of Kubernetes versions consists of consecutive minor versions, for example, from &lt;code>1.10.x&lt;/code> to &lt;code>1.11.y&lt;/code>. If the minor version increases in bigger steps, for example, from &lt;code>1.10.x&lt;/code> to &lt;code>1.12.y&lt;/code>, then the shoot cluster updates will fail during the maintenance window.
&lt;/div>
&lt;h2 id="manual-updates">Manual Updates&lt;/h2>
&lt;p>To update the Kubernetes version or the node operating system manually, change the &lt;code>.spec.kubernetes.version&lt;/code> field or the &lt;code>.spec.provider.workers.machine.image.version&lt;/code> field correspondingly.&lt;/p>
&lt;p>Manual updates are required if you would like to do a minor update of the Kubernetes version. Gardener doesn’t do such updates automatically, as they can have breaking changes that could impact the cluster workload.&lt;/p>
&lt;p>Manual updates are either executed immediately (default) or can be confined to the maintenance time window.&lt;br>
Choosing the latter option causes changes to the cluster (for example, node pool rolling-updates) and the subsequent reconciliation to only predictably happen during a defined time window (available since &lt;a href="https://github.com/gardener/gardener/releases/tag/v1.4.0">Gardener version 1.4&lt;/a>).&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/usage/shoot_maintenance/#confine-specification-changesupdates-roll-out">Confine Specification Changes/Update Roll Out&lt;/a>.&lt;/p>
&lt;div class="alert alert-warning" role="alert">
&lt;h4 class="alert-heading">Warning&lt;/h4>
Before applying such an update on minor or major releases, operators should check for all the breaking changes introduced in the target Kubernetes release changelog.
&lt;/div>
&lt;h2 id="examples">Examples&lt;/h2>
&lt;p>In the examples for the &lt;code>CloudProfile&lt;/code> and the shoot cluster specification, only the fields relevant for the example are shown.&lt;/p>
&lt;h3 id="auto-update-of-kubernetes-version">Auto-Update of Kubernetes Version&lt;/h3>
&lt;p>Let&amp;rsquo;s assume that the Kubernetes versions &lt;code>1.10.5&lt;/code> and &lt;code>1.11.0&lt;/code> were added in the following &lt;code>CloudProfile&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.11.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.10.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.10.0
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Before this change, the shoot cluster specification looked like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.10.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maintenance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> timeWindow:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> begin: 220000+0000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> end: 230000+0000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> autoUpdate:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetesVersion: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As a consequence, the shoot cluster is updated to Kubernetes version &lt;code>1.10.5&lt;/code> between 22:00-23:00 UTC. Your shoot cluster isn&amp;rsquo;t updated automatically to &lt;code>1.11.0&lt;/code>, even though it&amp;rsquo;s the highest Kubernetes version in the &lt;code>CloudProfile&lt;/code>, because Gardener only does automatic updates of the Kubernetes patch level.&lt;/p>
&lt;h3 id="forceful-update-due-to-expired-kubernetes-version">Forceful Update Due to Expired Kubernetes Version&lt;/h3>
&lt;p>Let&amp;rsquo;s assume the following &lt;code>CloudProfile&lt;/code> exists on the cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.12.8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.11.10
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.10.13
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.10.12
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expirationDate: &lt;span style="color:#a31515">&amp;#34;2019-04-13T08:00:00Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s assume the shoot cluster has the following specification:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.10.12
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maintenance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> timeWindow:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> begin: 220000+0100
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> end: 230000+0100
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> autoUpdate:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetesVersion: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The shoot cluster specification refers to a Kubernetes version that has an &lt;code>expirationDate&lt;/code>. In the maintenance window on &lt;code>2019-04-12&lt;/code>, the Kubernetes version stays the same as it’s still not expired. But in the maintenance window on &lt;code>2019-04-14&lt;/code>, the Kubernetes version of the shoot cluster is updated to &lt;code>1.10.13&lt;/code> (independently of the value of &lt;code>.spec.maintenance.autoUpdate.kubernetesVersion&lt;/code>).&lt;/p>
&lt;h3 id="forceful-update-to-new-minor-kubernetes-version">Forceful Update to New Minor Kubernetes Version&lt;/h3>
&lt;p>Let&amp;rsquo;s assume the following &lt;code>CloudProfile&lt;/code> exists on the cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.12.8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.11.10
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.11.09
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 1.10.12
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expirationDate: &lt;span style="color:#a31515">&amp;#34;2019-04-13T08:00:00Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s assume the shoot cluster has the following specification:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.10.12
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maintenance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> timeWindow:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> begin: 220000+0100
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> end: 230000+0100
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> autoUpdate:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetesVersion: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The shoot cluster specification refers a Kubernetes version that has an &lt;code>expirationDate&lt;/code>. In the maintenance window on &lt;code>2019-04-14&lt;/code>, the Kubernetes version of the shoot cluster is updated to &lt;code>1.11.10&lt;/code>, which is the highest patch version of minor target version &lt;code>1.11&lt;/code> that follows the source version &lt;code>1.10&lt;/code>.&lt;/p>
&lt;h3 id="automatic-update-from-expired-machine-image-version">Automatic Update from Expired Machine Image Version&lt;/h3>
&lt;p>Let&amp;rsquo;s assume the following &lt;code>CloudProfile&lt;/code> exists on the cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineImages:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> versions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 2191.5.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 2191.4.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - version: 2135.6.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expirationDate: &lt;span style="color:#a31515">&amp;#34;2019-04-13T08:00:00Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Let&amp;rsquo;s assume the shoot cluster has the following specification:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: aws
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: name
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maximum: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> minimum: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maxSurge: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maxUnavailable: 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> image:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: coreos
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 2135.6.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: m5.large
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volume:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: gp2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> size: 20Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maintenance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> timeWindow:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> begin: 220000+0100
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> end: 230000+0100
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> autoUpdate:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineImageVersion: &lt;span style="color:#00f">false&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The shoot cluster specification refers a machine image version that has an &lt;code>expirationDate&lt;/code>. In the maintenance window on &lt;code>2019-04-12&lt;/code>, the machine image version stays the same as it’s still not expired. But in the maintenance window on &lt;code>2019-04-14&lt;/code>, the machine image version of the shoot cluster is updated to &lt;code>2191.5.0&lt;/code> (independently of the value of &lt;code>.spec.maintenance.autoUpdate.machineImageVersion&lt;/code>) as version &lt;code>2135.6.0&lt;/code> is expired.&lt;/p></description></item></channel></rss>