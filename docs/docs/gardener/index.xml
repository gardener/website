<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener – Gardener</title><link>https://gardener.cloud/docs/gardener/</link><description>Recent content on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://gardener.cloud/docs/gardener/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: API Reference</title><link>https://gardener.cloud/docs/gardener/api-reference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/api-reference/</guid><description>
&lt;h1 id="gardener-api-reference">Gardener API Reference&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/authentication/">&lt;code>authentication.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/">&lt;code>core.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/extensions/">&lt;code>extensions.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/operations/">&lt;code>operations.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/resources/">&lt;code>resources.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/security/">&lt;code>security.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/seedmanagement/">&lt;code>seedmanagement.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/api-reference/settings/">&lt;code>settings.gardener.cloud&lt;/code> API Group&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Concepts</title><link>https://gardener.cloud/docs/gardener/concepts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/concepts/</guid><description/></item><item><title>Docs: Extensions</title><link>https://gardener.cloud/docs/gardener/extensions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/extensions/</guid><description>
&lt;h1 id="extensibility-overview">Extensibility Overview&lt;/h1>
&lt;p>Initially, everything was developed in-tree in the Gardener project. All cloud providers and the configuration for all the supported operating systems were released together with the Gardener core itself.
But as the project grew, it got more and more difficult to add new providers and maintain the existing code base.
As a consequence and in order to become agile and flexible again, we proposed &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md">GEP-1&lt;/a> (Gardener Enhancement Proposal).
The document describes an out-of-tree extension architecture that keeps the Gardener core logic independent of provider-specific knowledge (similar to what Kubernetes has achieved with &lt;a href="https://github.com/kubernetes/enhancements/issues/88">out-of-tree cloud providers&lt;/a> or with &lt;a href="https://github.com/kubernetes/community/pull/1258">CSI volume plugins&lt;/a>).&lt;/p>
&lt;h2 id="basic-concepts">Basic Concepts&lt;/h2>
&lt;p>Gardener keeps running in the &amp;ldquo;garden cluster&amp;rdquo; and implements the core logic of shoot cluster reconciliation / deletion.
Extensions are Kubernetes controllers themselves (like Gardener) and run in the seed clusters.
As usual, we try to use Kubernetes wherever applicable.
We rely on Kubernetes extension concepts in order to enable extensibility for Gardener.
The main ideas of GEP-1 are the following:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>During the shoot reconciliation process, Gardener will write CRDs into the seed cluster that are watched and managed by the extension controllers. They will reconcile (based on the &lt;code>.spec&lt;/code>) and report whether everything went well or errors occurred in the CRD&amp;rsquo;s &lt;code>.status&lt;/code> field.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Gardener keeps deploying the provider-independent control plane components (etcd, kube-apiserver, etc.). However, some of these components might still need little customization by providers, e.g., additional configuration, flags, etc. In this case, the extension controllers register webhooks in order to manipulate the manifests.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;strong>Example 1&lt;/strong>:&lt;/p>
&lt;p>Gardener creates a new AWS shoot cluster and requires the preparation of infrastructure in order to proceed (networks, security groups, etc.).
It writes the following CRD into the seed cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Infrastructure
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: infrastructure
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: shoot--core--aws-01
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: aws
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: InfrastructureConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networks:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> vpc:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cidr: 10.250.0.0/16
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> internal:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - 10.250.112.0/22
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> public:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - 10.250.96.0/22
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - 10.250.0.0/19
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> zones:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - eu-west-1a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiserver: api.aws-01.core.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> region: eu-west-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-aws-credentials
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sshPublicKey: |&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> &lt;/span> base64(key)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please note that the &lt;code>.spec.providerConfig&lt;/code> is a raw blob and not evaluated or known in any way by Gardener.
Instead, it was specified by the user (in the &lt;code>Shoot&lt;/code> resource) and just &amp;ldquo;forwarded&amp;rdquo; to the extension controller.
Only the AWS controller understands this configuration and will now start provisioning/reconciling the infrastructure.
It reports in the &lt;code>.status&lt;/code> field the result:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>status:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> observedGeneration: ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> state: ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastError: ..
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastOperation: ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerStatus:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: InfrastructureStatus
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> vpc:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> id: vpc-1234
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> subnets:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - id: subnet-acbd1234
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: workers
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> zone: eu-west-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> securityGroups:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - id: sg-xyz12345
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: workers
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> iam:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodesRoleARN: &amp;lt;some-arn&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> instanceProfileName: foo
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ec2:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> keyName: bar
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Gardener waits until the &lt;code>.status.lastOperation&lt;/code> / &lt;code>.status.lastError&lt;/code> indicates that the operation reached a final state and either continuous with the next step, or stops and reports the potential error.
The extension-specific output in &lt;code>.status.providerStatus&lt;/code> is - similar to &lt;code>.spec.providerConfig&lt;/code> - not evaluated, and simply forwarded to CRDs in subsequent steps.&lt;/p>
&lt;p>&lt;strong>Example 2&lt;/strong>:&lt;/p>
&lt;p>Gardener deploys the control plane components into the seed cluster, e.g. the &lt;code>kube-controller-manager&lt;/code> deployment with the following flags:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Deployment
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - command:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - /usr/local/bin/kube-controller-manager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --allocate-node-cidrs=true
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --attach-detach-reconcile-sync-period=1m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --controllers=*,bootstrapsigner,tokencleaner
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --cluster-cidr=100.96.0.0/11
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --cluster-name=shoot--core--aws-01
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --cluster-signing-cert-file=/srv/kubernetes/ca/ca.crt
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --cluster-signing-key-file=/srv/kubernetes/ca/ca.key
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --concurrent-deployment-syncs=10
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --concurrent-replicaset-syncs=10
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The AWS controller requires some additional flags in order to make the cluster functional.
It needs to provide a Kubernetes cloud-config and also some cloud-specific flags.
Consequently, it registers a &lt;code>MutatingWebhookConfiguration&lt;/code> on &lt;code>Deployment&lt;/code>s and adds these flags to the container:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span> - --cloud-provider=external
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --external-cloud-volume-plugin=aws
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --cloud-config=/etc/kubernetes/cloudprovider/cloudprovider.conf
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Of course, it would have needed to create a &lt;code>ConfigMap&lt;/code> containing the cloud config and to add the proper &lt;code>volume&lt;/code> and &lt;code>volumeMounts&lt;/code> to the manifest as well.&lt;/p>
&lt;p>(Please note for this special example: The Kubernetes community is also working on making the &lt;code>kube-controller-manager&lt;/code> provider-independent.
However, there will most probably be still components other than the &lt;code>kube-controller-manager&lt;/code> which need to be adapted by extensions.)&lt;/p>
&lt;p>If you are interested in writing an extension, or generally in digging deeper to find out the nitty-gritty details of the extension concepts, please read &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md">GEP-1&lt;/a>.
We are truly looking forward to your feedback!&lt;/p>
&lt;h2 id="current-status">Current Status&lt;/h2>
&lt;p>Meanwhile, the out-of-tree extension architecture of Gardener is in place and has been productively validated. We are tracking all internal and external extensions of Gardener in the &lt;a href="https://github.com/gardener/gardener/tree/master/extensions#known-extension-implementations">Gardener Extensions Library&lt;/a> repo.&lt;/p></description></item><item><title>Docs: Deployment</title><link>https://gardener.cloud/docs/gardener/deployment/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/deployment/</guid><description/></item><item><title>Docs: Monitoring</title><link>https://gardener.cloud/docs/gardener/monitoring/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/monitoring/</guid><description/></item><item><title>Docs: Accessing Shoot Clusters</title><link>https://gardener.cloud/docs/gardener/shoot_access/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/shoot_access/</guid><description>
&lt;h1 id="accessing-shoot-clusters">Accessing Shoot Clusters&lt;/h1>
&lt;p>After creation of a shoot cluster, end-users require a &lt;code>kubeconfig&lt;/code> to access it. There are several options available to get to such &lt;code>kubeconfig&lt;/code>.&lt;/p>
&lt;h2 id="shootsadminkubeconfig-subresource">&lt;code>shoots/adminkubeconfig&lt;/code> Subresource&lt;/h2>
&lt;p>The &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/16-adminkubeconfig-subresource.md">&lt;code>shoots/adminkubeconfig&lt;/code>&lt;/a> subresource allows users to dynamically generate temporary &lt;code>kubeconfig&lt;/code>s that can be used to access shoot cluster with &lt;code>cluster-admin&lt;/code> privileges. The credentials associated with this &lt;code>kubeconfig&lt;/code> are client certificates which have a very short validity and must be renewed before they expire (by calling the subresource endpoint again).&lt;/p>
&lt;p>The username associated with such &lt;code>kubeconfig&lt;/code> will be the same which is used for authenticating to the Gardener API. Apart from this advantage, the created &lt;code>kubeconfig&lt;/code> will not be persisted anywhere.&lt;/p>
&lt;p>In order to request such a &lt;code>kubeconfig&lt;/code>, you can run the following commands (targeting the garden cluster):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>export NAMESPACE=garden-my-namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export SHOOT_NAME=my-shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export KUBECONFIG=&amp;lt;kubeconfig &lt;span style="color:#00f">for&lt;/span> garden cluster&amp;gt; &lt;span style="color:#008000"># can be set using &amp;#34;gardenctl target --garden &amp;lt;landscape&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl create &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> -f &amp;lt;(printf &lt;span style="color:#a31515">&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;expirationSeconds&amp;#34;:600}}&amp;#39;&lt;/span>) &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --raw /apis/core.gardener.cloud/v1beta1/namespaces/&lt;span style="color:#a31515">${&lt;/span>NAMESPACE&lt;span style="color:#a31515">}&lt;/span>/shoots/&lt;span style="color:#a31515">${&lt;/span>SHOOT_NAME&lt;span style="color:#a31515">}&lt;/span>/adminkubeconfig | &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> jq -r &lt;span style="color:#a31515">&amp;#34;.status.kubeconfig&amp;#34;&lt;/span> | &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> base64 -d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You also can use controller-runtime &lt;code>client&lt;/code> (&amp;gt;= v0.14.3) to create such a kubeconfig from your go code like so:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>expiration := 10 * time.Minute
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>expirationSeconds := int64(expiration.Seconds())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>adminKubeconfigRequest := &amp;amp;authenticationv1alpha1.AdminKubeconfigRequest{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Spec: authenticationv1alpha1.AdminKubeconfigRequestSpec{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ExpirationSeconds: &amp;amp;expirationSeconds,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>err := client.SubResource(&lt;span style="color:#a31515">&amp;#34;adminkubeconfig&amp;#34;&lt;/span>).Create(ctx, shoot, adminKubeconfigRequest)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">if&lt;/span> err != &lt;span style="color:#00f">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00f">return&lt;/span> err
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>config = adminKubeconfigRequest.Status.Kubeconfig
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In Python, you can use the native &lt;a href="https://github.com/kubernetes-client/python">&lt;code>kubernetes&lt;/code> client&lt;/a> to create such a kubeconfig like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-python" data-lang="python">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># This script first loads an existing kubeconfig from your system, and then sends a request to the Gardener API to create a new kubeconfig for a shoot cluster. &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># The received kubeconfig is then decoded and a new API client is created for interacting with the shoot cluster.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">import&lt;/span> base64
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">import&lt;/span> json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">from&lt;/span> kubernetes &lt;span style="color:#00f">import&lt;/span> client, config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">import&lt;/span> yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Set configuration options&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>shoot_name=&lt;span style="color:#a31515">&amp;#34;my-shoot&amp;#34;&lt;/span> &lt;span style="color:#008000"># Name of the shoot&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>project_namespace=&lt;span style="color:#a31515">&amp;#34;garden-my-namespace&amp;#34;&lt;/span> &lt;span style="color:#008000"># Namespace of the project&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Load kubeconfig from default ~/.kube/config&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>config.load_kube_config()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>api = client.ApiClient()
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Create kubeconfig request&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubeconfig_request = {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#39;apiVersion&amp;#39;&lt;/span>: &lt;span style="color:#a31515">&amp;#39;authentication.gardener.cloud/v1alpha1&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#39;kind&amp;#39;&lt;/span>: &lt;span style="color:#a31515">&amp;#39;AdminKubeconfigRequest&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#39;spec&amp;#39;&lt;/span>: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#39;expirationSeconds&amp;#39;&lt;/span>: 600
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>response = api.call_api(resource_path=&lt;span style="color:#a31515">f&lt;/span>&lt;span style="color:#a31515">&amp;#39;/apis/core.gardener.cloud/v1beta1/namespaces/&lt;/span>&lt;span style="color:#a31515">{&lt;/span>project_namespace&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">/shoots/&lt;/span>&lt;span style="color:#a31515">{&lt;/span>shoot_name&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">/adminkubeconfig&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> method=&lt;span style="color:#a31515">&amp;#39;POST&amp;#39;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> body=kubeconfig_request,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> auth_settings=[&lt;span style="color:#a31515">&amp;#39;BearerToken&amp;#39;&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> _preload_content=&lt;span style="color:#00f">False&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> _return_http_data_only=&lt;span style="color:#00f">True&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> )
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>decoded_kubeconfig = base64.b64decode(json.loads(response.data)[&lt;span style="color:#a31515">&amp;#34;status&amp;#34;&lt;/span>][&lt;span style="color:#a31515">&amp;#34;kubeconfig&amp;#34;&lt;/span>]).decode(&lt;span style="color:#a31515">&amp;#39;utf-8&amp;#39;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print(decoded_kubeconfig)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Create an API client to interact with the shoot cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>shoot_api_client = config.new_client_from_config_dict(yaml.safe_load(decoded_kubeconfig))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>v1 = client.CoreV1Api(shoot_api_client)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> The &lt;a href="https://github.com/gardener/gardenctl-v2">&lt;code>gardenctl-v2&lt;/code>&lt;/a> tool simplifies targeting shoot clusters. It automatically downloads a kubeconfig that uses the &lt;a href="https://github.com/gardener/gardenlogin">gardenlogin&lt;/a> kubectl auth plugin. This transparently manages authentication and certificate renewal without containing any credentials.&lt;/p>
&lt;/blockquote>
&lt;h2 id="shootsviewerkubeconfig-subresource">&lt;code>shoots/viewerkubeconfig&lt;/code> Subresource&lt;/h2>
&lt;p>The &lt;code>shoots/viewerkubeconfig&lt;/code> subresource works similar to the &lt;a href="https://gardener.cloud/docs/gardener/shoot_access/#shootsadminkubeconfig-subresource">&lt;code>shoots/adminkubeconfig&lt;/code>&lt;/a>.
The difference is that it returns a kubeconfig with read-only access for all APIs except the &lt;code>core/v1.Secret&lt;/code> API and the resources which are specified in the &lt;code>spec.kubernetes.kubeAPIServer.encryptionConfig&lt;/code> field in the Shoot (see &lt;a href="https://gardener.cloud/docs/gardener/etcd_encryption_config/">this document&lt;/a>).&lt;/p>
&lt;p>In order to request such a &lt;code>kubeconfig&lt;/code>, you can run follow almost the same code as above - the only difference is that you need to use the &lt;code>viewerkubeconfig&lt;/code> subresource.
For example, in bash this looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>export NAMESPACE=garden-my-namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export SHOOT_NAME=my-shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl create &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> -f &amp;lt;(printf &lt;span style="color:#a31515">&amp;#39;{&amp;#34;spec&amp;#34;:{&amp;#34;expirationSeconds&amp;#34;:600}}&amp;#39;&lt;/span>) &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --raw /apis/core.gardener.cloud/v1beta1/namespaces/&lt;span style="color:#a31515">${&lt;/span>NAMESPACE&lt;span style="color:#a31515">}&lt;/span>/shoots/&lt;span style="color:#a31515">${&lt;/span>SHOOT_NAME&lt;span style="color:#a31515">}&lt;/span>/viewerkubeconfig | &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> jq -r &lt;span style="color:#a31515">&amp;#34;.status.kubeconfig&amp;#34;&lt;/span> | &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> base64 -d
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The examples for other programming languages are similar to &lt;a href="https://gardener.cloud/docs/gardener/shoot_access/#shootsadminkubeconfig-subresource">the above&lt;/a> and can be adapted accordingly.&lt;/p>
&lt;h2 id="openid-connect">OpenID Connect&lt;/h2>
&lt;p>The &lt;code>kube-apiserver&lt;/code> of shoot clusters can be provided with &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens">OpenID Connect configuration&lt;/a> via the Shoot spec:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidcConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It is the end-user&amp;rsquo;s responsibility to incorporate the OpenID Connect configurations in the &lt;code>kubeconfig&lt;/code> for accessing the cluster (i.e., Gardener will not automatically generate the &lt;code>kubeconfig&lt;/code> based on these OIDC settings).
The recommended way is using the &lt;code>kubectl&lt;/code> plugin called &lt;a href="https://github.com/int128/kubelogin">&lt;code>kubectl oidc-login&lt;/code>&lt;/a> for OIDC authentication.&lt;/p>
&lt;p>If you want to use the same OIDC configuration for all your shoots by default, then you can use the &lt;code>ClusterOpenIDConnectPreset&lt;/code> and &lt;code>OpenIDConnectPreset&lt;/code> API resources. They allow defaulting the &lt;code>.spec.kubernetes.kubeAPIServer.oidcConfig&lt;/code> fields for newly created &lt;code>Shoot&lt;/code>s such that you don&amp;rsquo;t have to repeat yourself every time (similar to &lt;code>PodPreset&lt;/code> resources in Kubernetes).
&lt;code>ClusterOpenIDConnectPreset&lt;/code> specified OIDC configuration applies to &lt;code>Projects&lt;/code> and &lt;code>Shoots&lt;/code> cluster-wide (hence, only available to Gardener operators), while &lt;code>OpenIDConnectPreset&lt;/code> is &lt;code>Project&lt;/code>-scoped.
Shoots have to &amp;ldquo;opt-in&amp;rdquo; for such defaulting by using the &lt;code>oidc=enable&lt;/code> label.&lt;/p>
&lt;p>For further information on &lt;code>(Cluster)OpenIDConnectPreset&lt;/code>, refer to &lt;a href="https://gardener.cloud/docs/gardener/openidconnect-presets/">ClusterOpenIDConnectPreset and OpenIDConnectPreset&lt;/a>.&lt;/p>
&lt;h2 id="static-token-kubeconfig">Static Token kubeconfig&lt;/h2>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> Static token kubeconfig is not available for Shoot clusters using Kubernetes version &amp;gt;= 1.27. The &lt;a href="https://gardener.cloud/docs/gardener/shoot_access/#shootsadminkubeconfig-subresource">&lt;code>shoots/adminkubeconfig&lt;/code> subresource&lt;/a> should be used instead.&lt;/p>
&lt;/blockquote>
&lt;p>This &lt;code>kubeconfig&lt;/code> contains a &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#static-token-file">static token&lt;/a> and provides &lt;code>cluster-admin&lt;/code> privileges.
It is created by default and persisted in the &lt;code>&amp;lt;shoot-name&amp;gt;.kubeconfig&lt;/code> secret in the project namespace in the garden cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enableStaticTokenKubeconfig: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It is &lt;strong>not&lt;/strong> the recommended method to access the shoot cluster, as the static token &lt;code>kubeconfig&lt;/code> has some security flaws associated with it:&lt;/p>
&lt;ul>
&lt;li>The static token in the &lt;code>kubeconfig&lt;/code> doesn&amp;rsquo;t have any expiration date. Read &lt;a href="https://gardener.cloud/docs/gardener/shoot_credentials_rotation/#kubeconfig">Credentials Rotation for Shoot Clusters&lt;/a> to learn how to rotate the static token.&lt;/li>
&lt;li>The static token doesn&amp;rsquo;t have any user identity associated with it. The user in that token will always be &lt;code>system:cluster-admin&lt;/code>, irrespective of the person accessing the cluster. Hence, it is impossible to audit the events in cluster.&lt;/li>
&lt;/ul>
&lt;p>When the &lt;code>enableStaticTokenKubeconfig&lt;/code> field is not explicitly set in the Shoot spec:&lt;/p>
&lt;ul>
&lt;li>for Shoot clusters using Kubernetes version &amp;lt; 1.26, the field is defaulted to &lt;code>true&lt;/code>.&lt;/li>
&lt;li>for Shoot clusters using Kubernetes version &amp;gt;= 1.26, the field is defaulted to &lt;code>false&lt;/code>.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> Starting with Kubernetes 1.27, the &lt;code>enableStaticTokenKubeconfig&lt;/code> field will be locked to &lt;code>false&lt;/code>.&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Admission Configuration for the `PodSecurity` Admission Plugin</title><link>https://gardener.cloud/docs/gardener/pod-security/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/pod-security/</guid><description>
&lt;h2 id="admission-configuration-for-the-podsecurity-admission-plugin">Admission Configuration for the &lt;code>PodSecurity&lt;/code> Admission Plugin&lt;/h2>
&lt;p>If you wish to add your custom configuration for the &lt;code>PodSecurity&lt;/code> plugin, you can do so in the Shoot spec under &lt;code>.spec.kubernetes.kubeAPIServer.admissionPlugins&lt;/code> by adding:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>admissionPlugins:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: PodSecurity
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: pod-security.admission.config.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: PodSecurityConfiguration
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Defaults applied when a mode label is not set.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Level label values must be one of:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - &amp;#34;privileged&amp;#34; (default)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - &amp;#34;baseline&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - &amp;#34;restricted&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Version label values must be one of:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - &amp;#34;latest&amp;#34; (default) &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - specific version like &amp;#34;v1.25&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> defaults:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enforce: &lt;span style="color:#a31515">&amp;#34;privileged&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enforce-version: &lt;span style="color:#a31515">&amp;#34;latest&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> audit: &lt;span style="color:#a31515">&amp;#34;privileged&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> audit-version: &lt;span style="color:#a31515">&amp;#34;latest&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> warn: &lt;span style="color:#a31515">&amp;#34;privileged&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> warn-version: &lt;span style="color:#a31515">&amp;#34;latest&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exemptions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Array of authenticated usernames to exempt.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernames: []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Array of runtime class names to exempt.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> runtimeClasses: []
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Array of namespaces to exempt.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespaces: []
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For proper functioning of Gardener, &lt;code>kube-system&lt;/code> namespace will also be automatically added to the &lt;code>exemptions.namespaces&lt;/code> list.&lt;/p></description></item><item><title>Docs: Audit a Kubernetes Cluster</title><link>https://gardener.cloud/docs/gardener/shoot_auditpolicy/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/shoot_auditpolicy/</guid><description>
&lt;h1 id="audit-a-kubernetes-cluster">Audit a Kubernetes Cluster&lt;/h1>
&lt;p>The shoot cluster is a Kubernetes cluster and its &lt;code>kube-apiserver&lt;/code> handles the audit events. In order to define which audit events must be logged, a proper audit policy file must be passed to the Kubernetes API server. You could find more information about auditing a kubernetes cluster in the &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">Auditing&lt;/a> topic.&lt;/p>
&lt;h2 id="default-audit-policy">Default Audit Policy&lt;/h2>
&lt;p>By default, the Gardener will deploy the shoot cluster with audit policy defined in the &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/component/kubernetes/apiserver/secrets.go">kube-apiserver package&lt;/a>.&lt;/p>
&lt;h2 id="custom-audit-policy">Custom Audit Policy&lt;/h2>
&lt;p>If you need specific audit policy for your shoot cluster, then you could deploy the required audit policy in the garden cluster as &lt;code>ConfigMap&lt;/code> resource and set up your shoot to refer this &lt;code>ConfigMap&lt;/code>. Note that the policy must be stored under the key &lt;code>policy&lt;/code> in the data section of the &lt;code>ConfigMap&lt;/code>.&lt;/p>
&lt;p>For example, deploy the auditpolicy &lt;code>ConfigMap&lt;/code> in the same namespace as your &lt;code>Shoot&lt;/code> resource:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f example/95-configmap-custom-audit-policy.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>then set your shoot to refer that &lt;code>ConfigMap&lt;/code> (only related fields are shown):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeAPIServer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> auditConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> auditPolicy:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> configMapRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: auditpolicy
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Gardener validate the &lt;code>Shoot&lt;/code> resource to refer only existing &lt;code>ConfigMap&lt;/code> containing valid audit policy, and rejects the &lt;code>Shoot&lt;/code> on failure.
If you want to switch back to the default audit policy, you have to remove the section&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>auditPolicy:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> configMapRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: &amp;lt;configmap-name&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>from the shoot spec.&lt;/p>
&lt;h2 id="rolling-out-changes-to-the-audit-policy">Rolling Out Changes to the Audit Policy&lt;/h2>
&lt;p>Gardener is not automatically rolling out changes to the Audit Policy to minimize the amount of Shoot reconciliations in order to prevent cloud provider rate limits, etc.
Gardener will pick up the changes on the next reconciliation of Shoots referencing the Audit Policy ConfigMap.
If users want to immediately rollout Audit Policy changes, they can manually trigger a Shoot reconciliation as described in &lt;a href="https://gardener.cloud/docs/gardener/shoot_operations/#immediate-reconciliation">triggering an immediate reconciliation&lt;/a>.
This is similar to changes to the cloud provider secret referenced by Shoots.&lt;/p></description></item><item><title>Docs: Autoscaling Specifics for Components</title><link>https://gardener.cloud/docs/gardener/autoscaling-specifics-for-components/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/autoscaling-specifics-for-components/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>This document describes the used autoscaling mechanism for several components.&lt;/p>
&lt;h2 id="garden-or-shoot-cluster-etcd">Garden or Shoot Cluster etcd&lt;/h2>
&lt;p>By default, if none of the autoscaling modes is requested the &lt;code>etcd&lt;/code> is deployed with static resources, without autoscaling.&lt;/p>
&lt;p>However, there are two supported autoscaling modes for the Garden or Shoot cluster etcd.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>HVPA&lt;/code>&lt;/p>
&lt;p>In &lt;code>HVPA&lt;/code> mode, the etcd is scaled by the &lt;a href="https://github.com/gardener/hvpa-controller">hvpa-controller&lt;/a>. The gardenlet/gardener-operator is creating an &lt;code>HVPA&lt;/code> resource for the etcd (&lt;code>main&lt;/code> or &lt;code>events&lt;/code>).
The &lt;code>HVPA&lt;/code> enables a vertical scaling for etcd.&lt;/p>
&lt;p>The &lt;code>HVPA&lt;/code> mode is the used autoscaling mode when the &lt;code>HVPA&lt;/code> feature gate is enabled and the &lt;code>VPAForETCD&lt;/code> feature gate is disabled.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>VPA&lt;/code>&lt;/p>
&lt;p>In &lt;code>VPA&lt;/code> mode, the etcd is scaled by a native &lt;code>VPA&lt;/code> resource.&lt;/p>
&lt;p>The &lt;code>VPA&lt;/code> mode is the used autoscaling mode when the &lt;code>VPAForETCD&lt;/code> feature gate is enabled (takes precedence over the &lt;code>HVPA&lt;/code> feature gate).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>[!NOTE]
Starting with release &lt;code>v1.97&lt;/code>, the &lt;code>VPAForETCD&lt;/code> feature gate is enabled by default.&lt;/p>
&lt;/blockquote>
&lt;p>For both of the autoscaling modes downscaling is handled more pessimistically to prevent many subsequent etcd restarts. Thus, for &lt;code>production&lt;/code> and &lt;code>infrastructure&lt;/code> Shoot clusters (or all Garden clusters), downscaling is deactivated for the main etcd. For all other Shoot clusters, lower advertised requests/limits are only applied during the Shoot&amp;rsquo;s maintenance time window.&lt;/p>
&lt;h2 id="shoot-kubernetes-api-server">Shoot Kubernetes API Server&lt;/h2>
&lt;p>There are three supported autoscaling modes for the Shoot Kubernetes API server.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>Baseline&lt;/code>&lt;/p>
&lt;p>In &lt;code>Baseline&lt;/code> mode, the Shoot Kubernetes API server is scaled by active HPA and VPA in passive, recommend-only mode.&lt;/p>
&lt;p>The API server resource requests are computed based on the Shoot&amp;rsquo;s minimum Nodes count:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Range&lt;/th>
&lt;th>Resource Requests&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>[0, 2]&lt;/td>
&lt;td>&lt;code>800m&lt;/code>, &lt;code>800Mi&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>(2, 10]&lt;/td>
&lt;td>&lt;code>1000m&lt;/code>, &lt;code>1100Mi&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>(10, 50]&lt;/td>
&lt;td>&lt;code>1200m&lt;/code>, &lt;code>1600Mi&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>(50, 100]&lt;/td>
&lt;td>&lt;code>2500m&lt;/code>, &lt;code>5200Mi&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>(100, inf.)&lt;/td>
&lt;td>&lt;code>3000m&lt;/code>, &lt;code>5200Mi&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The API server&amp;rsquo;s min replicas count is 2, the max replicas count - 3.&lt;/p>
&lt;p>The &lt;code>Baseline&lt;/code> mode is the used autoscaling mode when the &lt;code>HVPA&lt;/code> and &lt;code>VPAAndHPAForAPIServer&lt;/code> feature gates are not enabled.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>HVPA&lt;/code>&lt;/p>
&lt;p>In &lt;code>HVPA&lt;/code> mode, the Shoot Kubernetes API server is scaled by the &lt;a href="https://github.com/gardener/hvpa-controller">hvpa-controller&lt;/a>. The gardenlet is creating an &lt;code>HVPA&lt;/code> resource for the API server. The &lt;code>HVPA&lt;/code> resource is backed by HPA and VPA both in recommend-only mode. The hvpa-controller is responsible for enabling simultaneous horizontal and vertical scaling by incorporating the recommendations from the HPA and VPA.&lt;/p>
&lt;p>The initial API server resource requests are &lt;code>500m&lt;/code> and &lt;code>1Gi&lt;/code>.
HVPA&amp;rsquo;s HPA is scaling only on CPU (average utilization 80%). HVPA&amp;rsquo;s VPA max allowed values are &lt;code>8&lt;/code> CPU and &lt;code>25G&lt;/code>.&lt;/p>
&lt;p>The API server&amp;rsquo;s min replicas count is 2, the max replicas count - 3.&lt;/p>
&lt;p>The &lt;code>HVPA&lt;/code> mode is the used autoscaling mode when the &lt;code>HVPA&lt;/code> feature gate is enabled (and the &lt;code>VPAAndHPAForAPIServer&lt;/code> feature gate is disabled).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>VPAAndHPA&lt;/code>&lt;/p>
&lt;p>In &lt;code>VPAAndHPA&lt;/code> mode, the Shoot Kubernetes API server is scaled simultaneously by VPA and HPA on the same metric (CPU and memory usage). The pod-trashing cycle between VPA and HPA scaling on the same metric is avoided by configuring the HPA to scale on average usage (not on average utilization) and by picking the target average utilization values in sync with VPA&amp;rsquo;s allowed maximums. This makes possible VPA to first scale vertically on CPU/memory usage. Once all Pods&amp;rsquo; average CPU/memory usage is close to exceed the VPA&amp;rsquo;s allowed maximum CPU/memory (the HPA&amp;rsquo;s target average utilization, 1/7 less than VPA&amp;rsquo;s allowed maximums), HPA is scaling horizontally (by adding a new replica).&lt;/p>
&lt;p>The &lt;code>VPAAndHPA&lt;/code> mode is introduced to address disadvantages with HVPA: additional component; modifies the deployment triggering unnecessary rollouts; vertical scaling only at max replicas; stuck vertical resource requests when scaling in again; etc.&lt;/p>
&lt;p>The initial API server resource requests are &lt;code>250m&lt;/code> and &lt;code>500Mi&lt;/code>.
VPA&amp;rsquo;s max allowed values are &lt;code>7&lt;/code> CPU and &lt;code>28G&lt;/code>. HPA&amp;rsquo;s average target usage values are &lt;code>6&lt;/code> CPU and &lt;code>24G&lt;/code>.&lt;/p>
&lt;p>The API server&amp;rsquo;s min replicas count is 2, the max replicas count - 6.&lt;/p>
&lt;p>The &lt;code>VPAAndHPA&lt;/code> mode is the used autoscaling mode when the &lt;code>VPAAndHPAForAPIServer&lt;/code> feature gate is enabled (takes precedence over the &lt;code>HVPA&lt;/code> feature gate).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>[!NOTE]
Starting with release &lt;code>v1.101&lt;/code>, the &lt;code>VPAAndHPAForAPIServer&lt;/code> feature gate is enabled by default.&lt;/p>
&lt;/blockquote>
&lt;p>In all scaling modes the min replicas count of 2 is imposed by the &lt;a href="https://gardener.cloud/docs/gardener/high-availability/#control-plane-components">High Availability of Shoot Control Plane Components&lt;/a>.&lt;/p>
&lt;p>The gardenlet sets the initial API server resource requests only when the Deployment is not found. When the Deployment exists, it is not overwriting the kube-apiserver container resources.&lt;/p>
&lt;h2 id="disabling-scale-down-for-components-in-the-shoot-control-plane">Disabling Scale Down for Components in the Shoot Control Plane&lt;/h2>
&lt;p>Some Shoot clusters&amp;rsquo; control plane components can be overloaded and can have very high resource usage. The existing autoscaling solution could be imperfect to cover these cases. Scale down actions for such overloaded components could be disruptive.&lt;/p>
&lt;p>To prevent such disruptive scale-down actions it is possible to disable scale down of the etcd, Kubernetes API server and Kubernetes controller manager in the Shoot control plane by annotating the Shoot with &lt;code>alpha.control-plane.scaling.shoot.gardener.cloud/scale-down-disabled=true&lt;/code>.&lt;/p>
&lt;p>There are the following specifics for when disabling scale-down for the Kubernetes API server component:&lt;/p>
&lt;ul>
&lt;li>In &lt;code>Baseline&lt;/code> and &lt;code>HVPA&lt;/code> modes the HPA&amp;rsquo;s min and max replicas count are set to 4.&lt;/li>
&lt;li>In &lt;code>VPAAndHPA&lt;/code> mode if the HPA resource exists and HPA&amp;rsquo;s &lt;code>spec.minReplicas&lt;/code> is not nil then the min replicas count is &lt;code>max(spec.minReplicas, status.desiredReplicas)&lt;/code>. When scale-down is disabled, this allows operators to specify a custom value for HPA &lt;code>spec.minReplicas&lt;/code> and this value not to be reverted by gardenlet. I.e, HPA &lt;em>does&lt;/em> scale down to min replicas but not below min replicas. HPA&amp;rsquo;s max replicas count is 6.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Note: The &lt;code>alpha.control-plane.scaling.shoot.gardener.cloud/scale-down-disabled&lt;/code> annotation is alpha and can be removed anytime without further notice. Only use it if you know what you do.&lt;/p>
&lt;/blockquote>
&lt;h2 id="virtual-kubernetes-api-server-and-gardener-api-server">Virtual Kubernetes API Server and Gardener API Server&lt;/h2>
&lt;p>The virtual Kubernetes API server&amp;rsquo;s autoscaling is same as the Shoot Kubernetes API server&amp;rsquo;s with the following differences:&lt;/p>
&lt;ul>
&lt;li>The initial API server resource requests are &lt;code>600m&lt;/code> and &lt;code>512Mi&lt;/code> in all autoscaling modes.&lt;/li>
&lt;li>The min replicas count is 2 for a non-HA virtual cluster and 3 for an HA virtual cluster. The max replicas count is 6.&lt;/li>
&lt;li>In &lt;code>HVPA&lt;/code> mode, HVPA&amp;rsquo;s HPA is scaling on both CPU and memory (average utilization 80% for both).&lt;/li>
&lt;/ul>
&lt;p>The Gardener API server&amp;rsquo;s autoscaling is the same as the Shoot Kubernetes API server&amp;rsquo;s with the following differences:&lt;/p>
&lt;ul>
&lt;li>The initial API server resource requests are &lt;code>600m&lt;/code> and &lt;code>512Mi&lt;/code> in all autoscaling modes.&lt;/li>
&lt;li>The min replicas count is 2 for a non-HA virtual cluster and 3 for an HA virtual cluster. The max replicas count is 6.&lt;/li>
&lt;li>In &lt;code>HVPA&lt;/code> mode, HVPA&amp;rsquo;s HPA is scaling on both CPU and memory (average utilization 80% for both).&lt;/li>
&lt;li>In &lt;code>HVPA&lt;/code> mode, HVPA&amp;rsquo;s VPA max allowed values are &lt;code>4&lt;/code> CPU and &lt;code>25G&lt;/code>.&lt;/li>
&lt;/ul></description></item><item><title>Docs: Changing the API</title><link>https://gardener.cloud/docs/gardener/changing-the-api/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/changing-the-api/</guid><description>
&lt;h1 id="changing-the-api">Changing the API&lt;/h1>
&lt;p>This document describes the steps that need to be performed when changing the API.
It provides guidance for API changes to both (Gardener system in general or component configurations).&lt;/p>
&lt;p>Generally, as Gardener is a Kubernetes-native extension, it follows the same API conventions and guidelines like Kubernetes itself. The Kubernetes
&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md">API Conventions&lt;/a> as well as &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api_changes.md">Changing the API&lt;/a> topics already provide a good overview and general explanation of the basic concepts behind it.
We are following the same approaches.&lt;/p>
&lt;h2 id="gardener-api">Gardener API&lt;/h2>
&lt;p>The Gardener API is defined in the &lt;code>pkg/apis/{core,extensions,settings}&lt;/code> directories and is the main point of interaction with the system.
It must be ensured that the API is always backwards-compatible.&lt;/p>
&lt;h3 id="changing-the-api-1">Changing the API&lt;/h3>
&lt;p>&lt;strong>Checklist&lt;/strong> when changing the API:&lt;/p>
&lt;ol>
&lt;li>Modify the field(s) in the respective Golang files of all external versions and the internal version.
&lt;ol>
&lt;li>Make sure new fields are being added as &amp;ldquo;optional&amp;rdquo; fields, i.e., they are of pointer types, they have the &lt;code>// +optional&lt;/code> comment, and they have the &lt;code>omitempty&lt;/code> JSON tag.&lt;/li>
&lt;li>Make sure that the existing field numbers in the protobuf tags are not changed.&lt;/li>
&lt;li>Do not copy protobuf tags from other fields but create them with &lt;code>make generate WHAT=&amp;quot;protobuf&amp;quot;&lt;/code>.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>If necessary, implement/adapt the conversion logic defined in the versioned APIs (e.g., &lt;code>pkg/apis/core/v1beta1/conversions*.go&lt;/code>).&lt;/li>
&lt;li>If necessary, implement/adapt defaulting logic defined in the versioned APIs (e.g., &lt;code>pkg/apis/core/v1beta1/defaults*.go&lt;/code>).&lt;/li>
&lt;li>Run the code generation: &lt;code>make generate&lt;/code>&lt;/li>
&lt;li>If necessary, implement/adapt validation logic defined in the internal API (e.g., &lt;code>pkg/apis/core/validation/validation*.go&lt;/code>).&lt;/li>
&lt;li>If necessary, adapt the exemplary YAML manifests of the Gardener resources defined in &lt;code>example/*.yaml&lt;/code>.&lt;/li>
&lt;li>In most cases, it makes sense to add/adapt the documentation for administrators/operators and/or end-users in the &lt;code>docs&lt;/code> folder to provide information on purpose and usage of the added/changed fields.&lt;/li>
&lt;li>When opening the pull request, always add a release note so that end-users are becoming aware of the changes.&lt;/li>
&lt;/ol>
&lt;h3 id="removing-a-field">Removing a Field&lt;/h3>
&lt;p>If fields shall be removed permanently from the API, then a proper deprecation period must be adhered to so that end-users have enough time to adapt their clients.&lt;/p>
&lt;p>Once the deprecation period is over, the field should be dropped from the API in a two-step process, i.e., in two release cycles. In the first step, all the usages in the code base should be dropped. In the second step, the field should be dropped from API. We need to follow this two-step process cause there can be the case where &lt;code>gardener-apiserver&lt;/code> is upgraded to a new version in which the field has been removed but other controllers are still on the old version of Gardener. This can lead to &lt;code>nil&lt;/code> pointer exceptions or other unexpected behaviour.&lt;/p>
&lt;p>The steps for removing a field from the code base is:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The field in the external version(s) has to be commented out with appropriate doc string that the protobuf number of the corresponding field is reserved. Example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-diff" data-lang="diff">&lt;span style="display:flex;">&lt;span>- SeedTemplate *gardencorev1beta1.SeedTemplate `json:&amp;#34;seedTemplate,omitempty&amp;#34; protobuf:&amp;#34;bytes,2,opt,name=seedTemplate&amp;#34;`
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+ // SeedTemplate is tombstoned to show why 2 is reserved protobuf tag.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>+ // SeedTemplate *gardencorev1beta1.SeedTemplate `json:&amp;#34;seedTemplate,omitempty&amp;#34; protobuf:&amp;#34;bytes,2,opt,name=seedTemplate&amp;#34;`
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The reasoning behind this is to prevent the same protobuf number being used by a new field. Introducing a new field with the same protobuf number would be a breaking change for clients still using the old protobuf definitions that have the old field for the given protobuf number.
The field in the internal version can be removed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>A unit test has to be added to make sure that a new field does not reuse the already reserved protobuf tag.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Example of field removal can be found in the &lt;a href="https://github.com/gardener/gardener/pull/6972">Remove &lt;code>seedTemplate&lt;/code> field from ManagedSeed API&lt;/a> PR.&lt;/p>
&lt;h2 id="component-configuration-apis">Component Configuration APIs&lt;/h2>
&lt;p>Most Gardener components have a component configuration that follows similar principles to the Gardener API.
Those component configurations are defined in &lt;code>pkg/{controllermanager,gardenlet,scheduler},pkg/apis/config&lt;/code>.
Hence, the above checklist also applies for changes to those APIs.
However, since these APIs are only used internally and only during the deployment of Gardener, the guidelines with respect to changes and backwards-compatibility are slightly relaxed.
If necessary, it is allowed to remove fields without a proper deprecation period if the release note uses the &lt;code>breaking operator&lt;/code> keywords.&lt;/p>
&lt;p>In addition to the above checklist:&lt;/p>
&lt;ol>
&lt;li>If necessary, then adapt the Helm chart of Gardener defined in &lt;code>charts/gardener&lt;/code>. Adapt the &lt;code>values.yaml&lt;/code> file as well as the manifest templates.&lt;/li>
&lt;/ol></description></item><item><title>Docs: Cleanup of Shoot Clusters in Deletion</title><link>https://gardener.cloud/docs/gardener/shoot_cleanup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/shoot_cleanup/</guid><description>
&lt;h1 id="cleanup-of-shoot-clusters-in-deletion">Cleanup of Shoot Clusters in Deletion&lt;/h1>
&lt;p>When a shoot cluster is deleted then Gardener tries to gracefully remove most of the Kubernetes resources inside the cluster.
This is to prevent that any infrastructure or other artifacts remain after the shoot deletion.&lt;/p>
&lt;p>The cleanup is performed in four steps.
Some resources are deleted with a grace period, and all resources are forcefully deleted (by removing blocking finalizers) after some time to not block the cluster deletion entirely.&lt;/p>
&lt;p>&lt;strong>Cleanup steps:&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>All &lt;code>ValidatingWebhookConfiguration&lt;/code>s and &lt;code>MutatingWebhookConfiguration&lt;/code>s are deleted with a &lt;code>5m&lt;/code> grace period. Forceful finalization happens after &lt;code>5m&lt;/code>.&lt;/li>
&lt;li>All &lt;code>APIService&lt;/code>s and &lt;code>CustomResourceDefinition&lt;/code>s are deleted with a &lt;code>5m&lt;/code> grace period. Forceful finalization happens after &lt;code>1h&lt;/code>.&lt;/li>
&lt;li>All &lt;code>CronJob&lt;/code>s, &lt;code>DaemonSet&lt;/code>s, &lt;code>Deployment&lt;/code>s, &lt;code>Ingress&lt;/code>s, &lt;code>Job&lt;/code>s, &lt;code>Pod&lt;/code>s, &lt;code>ReplicaSet&lt;/code>s, &lt;code>ReplicationController&lt;/code>s, &lt;code>Service&lt;/code>s, &lt;code>StatefulSet&lt;/code>s, &lt;code>PersistentVolumeClaim&lt;/code>s are deleted with a &lt;code>5m&lt;/code> grace period. Forceful finalization happens after &lt;code>5m&lt;/code>.
&lt;blockquote>
&lt;p>If the &lt;code>Shoot&lt;/code> is annotated with &lt;code>shoot.gardener.cloud/skip-cleanup=true&lt;/code>, then only &lt;code>Service&lt;/code>s and &lt;code>PersistentVolumeClaim&lt;/code>s are considered.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>All &lt;code>VolumeSnapshot&lt;/code>s and &lt;code>VolumeSnapshotContent&lt;/code>s are deleted with a &lt;code>5m&lt;/code> grace period. Forceful finalization happens after &lt;code>1h&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>It is possible to override the finalization grace periods via annotations on the &lt;code>Shoot&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>shoot.gardener.cloud/cleanup-webhooks-finalize-grace-period-seconds&lt;/code> (for the resources handled in step 1)&lt;/li>
&lt;li>&lt;code>shoot.gardener.cloud/cleanup-extended-apis-finalize-grace-period-seconds&lt;/code> (for the resources handled in step 2)&lt;/li>
&lt;li>&lt;code>shoot.gardener.cloud/cleanup-kubernetes-resources-finalize-grace-period-seconds&lt;/code> (for the resources handled in step 3)&lt;/li>
&lt;/ul>
&lt;p>⚠️ If &lt;code>&amp;quot;0&amp;quot;&lt;/code> is provided, then all resources are finalized immediately without waiting for any graceful deletion.
Please be aware that this might lead to orphaned infrastructure artifacts.&lt;/p></description></item><item><title>Docs: Component Checklist</title><link>https://gardener.cloud/docs/gardener/component-checklist/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/component-checklist/</guid><description>
&lt;h1 id="checklist-for-adding-new-components">Checklist For Adding New Components&lt;/h1>
&lt;p>Adding new components that run in the garden, seed, or shoot cluster is theoretically quite simple - we just need a &lt;code>Deployment&lt;/code> (or other similar workload resource), the respective container image, and maybe a bit of configuration.
In practice, however, there are a couple of things to keep in mind in order to make the deployment production-ready.
This document provides a checklist for them that you can walk through.&lt;/p>
&lt;h2 id="general">General&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Avoid usage of Helm charts&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/tree/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/metricsserver">example&lt;/a>)&lt;/p>
&lt;p>Nowadays, we use &lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/interfaces.go">Golang components&lt;/a> instead of Helm charts for deploying components to a cluster.
Please find a typical structure of such components in the provided &lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/metricsserver/metrics_server.go#L80-L97">metrics_server.go&lt;/a> file (configuration values are typically managed in a &lt;code>Values&lt;/code> structure).
There are a few exceptions (e.g., &lt;a href="https://github.com/gardener/gardener/tree/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/istio">Istio&lt;/a>) still using charts, however the default should be using a Golang-based implementation.
For the exceptional cases, use Golang&amp;rsquo;s &lt;a href="https://pkg.go.dev/embed">embed&lt;/a> package to embed the Helm chart directory (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/istio/istiod.go#L59-L60">example 1&lt;/a>, &lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/istio/istiod.go#L297-L313">example 2&lt;/a>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Choose the proper deployment way&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/kubescheduler/kube_scheduler.go#L212-L232">example 1 (direct application w/ client)&lt;/a>, &lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/kubescheduler/kube_scheduler.go#L447-L488">example 2 (using &lt;code>ManagedResource&lt;/code>)&lt;/a>, &lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/kubestatemetrics/kube_state_metrics.go#L120">example 3 (mixed scenario)&lt;/a>)&lt;/p>
&lt;p>For historic reasons, resources related to shoot control plane components are applied directly with the client.
All other resources (seed or shoot system components) are deployed via &lt;code>gardener-resource-manager&lt;/code>&amp;rsquo;s &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/#managedresource-controller">Resource controller&lt;/a> (&lt;code>ManagedResource&lt;/code>s) since it performs health checks out-of-the-box and has a lot of other features (see its documentation for more information).
Components that can run as both seed system component or shoot control plane component (e.g., VPA or &lt;code>kube-state-metrics&lt;/code>) can make use of &lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/resourceconfig.go">these utility functions&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Use unique &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/kubescheduler/kube_scheduler.go#L183-L190">example 1&lt;/a>, &lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/kubescheduler/kube_scheduler.go#L353">example 2&lt;/a>)&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/#configmap-immutable">Unique &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s&lt;/a> are immutable for modification and have a unique name.
This has a couple of benefits, e.g. the &lt;code>kubelet&lt;/code> doesn&amp;rsquo;t watch these resources, and it is always clear which resource contains which data since it cannot be changed.
As a consequence, unique/immutable &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code> are superior to checksum annotations on the pod templates.
Stale/unused &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s are garbage-collected by &lt;code>gardener-resource-manager&lt;/code>&amp;rsquo;s &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/#garbage-collector-for-immutable-configmapssecrets">GarbageCollector&lt;/a>.
There are utility functions (see examples above) for using unique &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s in Golang components.
It is essential to inject the annotations into the workload resource to make the garbage-collection work.&lt;br>
Note that some &lt;code>ConfigMap&lt;/code>s/&lt;code>Secret&lt;/code>s should not be unique (e.g., those containing monitoring or logging configuration).
The reason is that the old revision stays in the cluster even if unused until the garbage-collector acts.
During this time, they would be wrongly aggregated to the full configuration.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Manage certificates/secrets via &lt;a href="https://github.com/gardener/gardener/tree/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/utils/secrets/manager">secrets manager&lt;/a>&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/metricsserver/metrics_server.go#L100-L109">example&lt;/a>)&lt;/p>
&lt;p>You should use the &lt;a href="https://gardener.cloud/docs/gardener/secrets_management/">secrets manager&lt;/a> for the management of any kind of credentials.
This makes sure that credentials rotation works out-of-the-box without you requiring to think about it.
Generally, do not use client certificates (see the &lt;a href="https://gardener.cloud/docs/gardener/component-checklist/#security">Security section&lt;/a>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Consider hibernation when calculating replica count&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/kubescheduler.go#L36">example&lt;/a>)&lt;/p>
&lt;p>Shoot clusters can be &lt;a href="https://gardener.cloud/docs/gardener/shoot_hibernate/">hibernated&lt;/a> meaning that all control plane components in the shoot namespace in the seed cluster are scaled down to zero and all worker nodes are terminated.
If your component runs in the seed cluster then you have to consider this case and provide the proper replica count.
There is a utility function available (see example).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Ensure task dependencies are as precise as possible in shoot flows&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/gardenlet/controller/shoot/shoot/reconciler_reconcile.go#L508-L512">example 1&lt;/a>, &lt;a href="https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/gardenlet/controller/shoot/shoot/reconciler_delete.go#L368-L372">example 2&lt;/a>)&lt;/p>
&lt;p>Only define the minimum of needed dependency tasks in the &lt;a href="https://github.com/gardener/gardener/tree/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/gardenlet/controller/shoot/shoot">shoot reconciliation/deletion flows&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Handle shoot system components&lt;/strong>&lt;/p>
&lt;p>Shoot system components deployed by &lt;code>gardener-resource-manager&lt;/code> are labelled with &lt;code>resource.gardener.cloud/managed-by: gardener&lt;/code>. This makes Gardener adding required label selectors and tolerations so that non-&lt;code>DaemonSet&lt;/code> managed &lt;code>Pod&lt;/code>s will exclusively run on selected nodes (for more information, see &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/#system-components-webhook">System Components Webhook&lt;/a>).
&lt;code>DaemonSet&lt;/code>s on the other hand, should generally tolerate any &lt;code>NoSchedule&lt;/code> or &lt;code>NoExecute&lt;/code> taints so that they can run on any &lt;code>Node&lt;/code>, regardless of user added taints.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="images">Images&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Do not hard-code container image references&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/charts/images.yaml#L130-L133">example 1&lt;/a>, &lt;a href="https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/metricsserver.go#L28-L31">example 2&lt;/a>, &lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/metricsserver/metrics_server.go#L82-L83">example 3&lt;/a>)&lt;/p>
&lt;p>We define all image references centrally in the &lt;a href="https://github.com/gardener/gardener/blob/master/imagevector/containers.yaml">&lt;code>imagevector/containers.yaml&lt;/code>&lt;/a> file.
Hence, the image references must not be hard-coded in the pod template spec but read from this so-called &lt;a href="https://gardener.cloud/docs/gardener/deployment/image_vector/">image vector&lt;/a> instead.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Do not use container images from registries that don&amp;rsquo;t support IPv6&lt;/strong> (example: &lt;a href="https://github.com/gardener/gardener/blob/6f4e64fe9494cafb5c5da9a2c0a491a5690161b6/charts/images.yaml#L619-L622">image vector&lt;/a>, &lt;a href="https://github.com/gardener/ci-infra/blob/92782bedd92815639abf4dc14b2c484f77c6e57d/config/images/images.yaml#L37-L40">prow configuration&lt;/a>)&lt;/p>
&lt;p>Registries such as ECR, GHCR (&lt;code>ghcr.io&lt;/code>), MCR (&lt;code>mcr.microsoft.com&lt;/code>) don&amp;rsquo;t support pulling images over IPv6.&lt;/p>
&lt;p>Check if the upstream image is being also maintained in a registry that support IPv6 natively such as Artifact Registry, Quay (&lt;code>quay.io&lt;/code>). If there is such image, use the image from registry with IPv6 support.&lt;/p>
&lt;p>If the image is not available in a registry with IPv6 then copy the image to the gardener GCR. There is a &lt;a href="https://github.com/gardener/ci-infra/blob/92782bedd92815639abf4dc14b2c484f77c6e57d/config/jobs/ci-infra/copy-images.yaml">prow job&lt;/a> copying images that are needed in gardener components from a source registry to the gardener GCR under the prefix &lt;code>europe-docker.pkg.dev/gardener-project/releases/3rd/&lt;/code> (see the &lt;a href="https://github.com/gardener/ci-infra/tree/master/config/images">documentation&lt;/a> or &lt;a href="https://github.com/gardener/ci-infra/issues/619">gardener/ci-infra#619&lt;/a>).&lt;/p>
&lt;p>If you want to use a new image from a registry without IPv6 support or upgrade an already used image to a newer tag, please open a PR to the ci-infra repository that modifies the job&amp;rsquo;s list of images to copy: &lt;a href="https://github.com/gardener/ci-infra/blob/master/config/images/images.yaml">&lt;code>images.yaml&lt;/code>&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Do not use container images from Docker Hub&lt;/strong> (example: &lt;a href="https://github.com/gardener/gardener/blob/6f4e64fe9494cafb5c5da9a2c0a491a5690161b6/charts/images.yaml#L619-L622">image vector&lt;/a>, &lt;a href="https://github.com/gardener/ci-infra/blob/92782bedd92815639abf4dc14b2c484f77c6e57d/config/images/images.yaml#L37-L40">prow configuration&lt;/a>)&lt;/p>
&lt;p>There is a strict &lt;a href="https://docs.docker.com/docker-hub/download-rate-limit/">rate-limit&lt;/a> that applies to the Docker Hub registry. As described in 2., use another registry (if possible) or copy the image to the gardener GCR.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Do not use Shoot container images that are not multi-arch&lt;/strong>&lt;/p>
&lt;p>Gardener supports Shoot clusters with both &lt;code>amd64&lt;/code> and &lt;code>arm64&lt;/code> based worker Nodes. &lt;code>amd64&lt;/code> container images cannot run on &lt;code>arm64&lt;/code> worker Nodes and vice-versa.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="security">Security&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Use a &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">dedicated &lt;code>ServiceAccount&lt;/code>&lt;/a> and disable auto-mount&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/metricsserver/metrics_server.go#L145-L151">example&lt;/a>)&lt;/p>
&lt;p>Components that need to talk to the API server of their runtime cluster must always use a dedicated &lt;code>ServiceAccount&lt;/code> (do not use &lt;code>default&lt;/code>), with &lt;code>automountServiceAccountToken&lt;/code> set to &lt;code>false&lt;/code>.
This makes &lt;code>gardener-resource-manager&lt;/code>&amp;rsquo;s &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/#tokeninvalidator">TokenInvalidator&lt;/a> invalidate the static token secret and its &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/#auto-mounting-projected-serviceaccount-tokens">&lt;code>ProjectedTokenMount&lt;/code> webhook&lt;/a> inject a projected token automatically.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Use shoot access tokens instead of a client certificates&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/kubescheduler/kube_scheduler.go#L234-L236">example&lt;/a>)&lt;/p>
&lt;p>For components that need to talk to a target cluster different from their runtime cluster (e.g., running in seed cluster but talking to shoot) the &lt;code>gardener-resource-manager&lt;/code>&amp;rsquo;s &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/#tokenrequestor">TokenRequestor&lt;/a> should be used to manage a so-called &amp;ldquo;shoot access token&amp;rdquo;.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Define RBAC roles with minimal privileges&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/metricsserver/metrics_server.go#L153-L223">example&lt;/a>)&lt;/p>
&lt;p>The component&amp;rsquo;s &lt;code>ServiceAccount&lt;/code> (if it exists) should have as little privileges as possible.
Consequently, please define proper &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">RBAC roles&lt;/a> for it.
This might include a combination of &lt;code>ClusterRole&lt;/code>s and &lt;code>Role&lt;/code>s.
Please do not provide elevated privileges due to laziness (e.g., because there is already a &lt;code>ClusterRole&lt;/code> that can be extended vs. creating a &lt;code>Role&lt;/code> only when access to a single namespace is needed).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Use &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">&lt;code>NetworkPolicy&lt;/code>s&lt;/a> to restrict network traffic&lt;/strong>&lt;/p>
&lt;p>You should restrict both ingress and egress traffic to/from your component as much as possible to ensure that it only gets access to/from other components if really needed.
Gardener provides a few default policies for typical usage scenarios. For more information, see &lt;a href="https://gardener.cloud/docs/gardener/network_policies/">&lt;code>NetworkPolicy&lt;/code>s In Garden, Seed, Shoot Clusters&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Do not run containers in privileged mode&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/nodelocaldns/nodelocaldns.go#L324-L328">example&lt;/a>, &lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/nodelocaldns/nodelocaldns.go#L501">example 2&lt;/a>)&lt;/p>
&lt;p>Avoid running containers with &lt;code>privileged=true&lt;/code>. Instead, define the needed &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container">Linux capabilities&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Do not run containers as root&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/Dockerfile#L12">example&lt;/a>)&lt;/p>
&lt;p>Avoid running containers as root. Usually, components such as Kubernetes controllers and admission webhook servers don&amp;rsquo;t need root user capabilities to do their jobs.&lt;/p>
&lt;p>The problem with running as root, starts with how the container is first built. Unless a non-privileged user is configured in the &lt;code>Dockerfile&lt;/code>, container build systems by default set up the container with the root user. Add a non-privileged user to your &lt;code>Dockerfile&lt;/code> or use a base image with a non-root user (for example the &lt;code>nonroot&lt;/code> images from &lt;a href="https://github.com/GoogleContainerTools/distroless">distroless&lt;/a> such as &lt;code>gcr.io/distroless/static-debian12:nonroot&lt;/code>).&lt;/p>
&lt;p>If the image is an upstream one, then consider configuring a securityContext for the container/Pod with a non-privileged user. For more information, see &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/">Configure a Security Context for a Pod or Container&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Choose the proper Seccomp profile&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/nodelocaldns/nodelocaldns.go#L283-L287">example 1&lt;/a>, &lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/nginxingress/nginxingress.go#L447">example 2&lt;/a>)&lt;/p>
&lt;p>For components deployed in the Seed cluster, the &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-seccomp-profile-for-a-container">Seccomp profile&lt;/a> will be defaulted to &lt;code>RuntimeDefault&lt;/code> by &lt;code>gardener-resource-manager&lt;/code>&amp;rsquo;s SeccompProfile webhook which works well for the majority of components. However, in some special cases you might need to overwrite it.&lt;/p>
&lt;p>The &lt;code>gardener-resource-manager&lt;/code>&amp;rsquo;s SeccompProfile webhook is not enabled for a Shoot cluster. For components deployed in the Shoot cluster, it is required [*] to explicitly specify the Seccomp profile.&lt;/p>
&lt;p>&lt;sub>[*] It is required because if a component deployed in the Shoot cluster does not specify a Seccomp profile and cannot run with the &lt;code>RuntimeDefault&lt;/code> Seccomp profile, then enabling the &lt;code>.spec.kubernetes.kubelet.seccompDefault&lt;/code> field in the Shoot spec would break the corresponding component.&lt;/sub>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="high-availability--stability">High Availability / Stability&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Specify the component type label for high availability&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/kubescheduler/kube_scheduler.go#L241">example&lt;/a>)&lt;/p>
&lt;p>To support high-availability deployments, &lt;code>gardener-resource-manager&lt;/code>s &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/#high-availability-config">HighAvailabilityConfig&lt;/a> webhook injects the proper specification like replica or topology spread constraints.
You only need to specify the type label. For more information, see &lt;a href="https://gardener.cloud/docs/gardener/high-availability/">High Availability Of Deployed Components&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Define a &lt;code>PodDisruptionBudget&lt;/code>&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/metricsserver/metrics_server.go#L384-L408">example&lt;/a>)&lt;/p>
&lt;p>Closely related to high availability but also to stability in general: The definition of a &lt;a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">&lt;code>PodDisruptionBudget&lt;/code>&lt;/a> with &lt;code>maxUnavailable=1&lt;/code> should be provided by default.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Choose the right &lt;code>PriorityClass&lt;/code>&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/kubescheduler/kube_scheduler.go#L307">example&lt;/a>)&lt;/p>
&lt;p>Each cluster runs many components with different priorities.
Gardener provides a set of default &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass">&lt;code>PriorityClass&lt;/code>es&lt;/a>. For more information, see &lt;a href="https://gardener.cloud/docs/gardener/priority-classes/">Priority Classes&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Consider defining liveness and readiness probes&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/metricsserver/metrics_server.go#L321-L344">example&lt;/a>)&lt;/p>
&lt;p>To ensure smooth rolling update behaviour, consider the definition of &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/">liveness and/or readiness probes&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Mark node-critical components&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/kubeproxy/resources.go#L328">example&lt;/a>)&lt;/p>
&lt;p>To ensure user workload pods are only scheduled to &lt;code>Nodes&lt;/code> where all node-critical components are ready, these components need to tolerate the &lt;code>node.gardener.cloud/critical-components-not-ready&lt;/code> taint (&lt;code>NoSchedule&lt;/code> effect).
Also, such &lt;code>DaemonSets&lt;/code> and the included &lt;code>PodTemplates&lt;/code> need to be labelled with &lt;code>node.gardener.cloud/critical-component=true&lt;/code>.
For more information, see &lt;a href="https://gardener.cloud/docs/gardener/node-readiness/">Readiness of Shoot Worker Nodes&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Consider making a &lt;code>Service&lt;/code> topology-aware&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/vpa/admissioncontroller.go#L154">example&lt;/a>)&lt;/p>
&lt;p>To reduce costs and to improve the network traffic latency in multi-zone Seed clusters, consider making a &lt;code>Service&lt;/code> topology-aware, if applicable. In short, when a &lt;code>Service&lt;/code> is topology-aware, Kubernetes routes network traffic to the &lt;code>Endpoint&lt;/code>s (&lt;code>Pod&lt;/code>s) which are located in the same zone where the traffic originated from. In this way, the cross availability zone traffic is avoided. See &lt;a href="https://gardener.cloud/docs/gardener/topology_aware_routing/">Topology-Aware Traffic Routing&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="scalability">Scalability&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Provide resource requirements&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/metricsserver/metrics_server.go#L345-L353">example&lt;/a>)&lt;/p>
&lt;p>All components should define reasonable (initial) &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container">CPU and memory &lt;code>requests&lt;/code>&lt;/a> and avoid limits (especially CPU limits) unless you know the healthy range for your component (almost impossible with most components today), but no more than the node allocatable remainder (after daemonset pods) of the largest eligible machine type. &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#how-pods-with-resource-requests-are-scheduled">Scheduling only takes &lt;code>requests&lt;/code> into account&lt;/a>!&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Define a &lt;code>VerticalPodAutoscaler&lt;/code>&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/metricsserver/metrics_server.go#L416-L444">example&lt;/a>)&lt;/p>
&lt;p>We typically (need to) perform vertical auto-scaling for containers that have a significant usage (&amp;gt;50m/100M) and a significant usage spread over time (&amp;gt;2x) by defining a &lt;a href="https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#intro">&lt;code>VerticalPodAutoscaler&lt;/code>&lt;/a> with &lt;code>updatePolicy.updateMode&lt;/code> &lt;a href="https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/README.md#quick-start">&lt;code>Auto&lt;/code>&lt;/a>, &lt;code>containerPolicies[].controlledValues&lt;/code> &lt;a href="https://github.com/kubernetes/autoscaler/blob/6da986f4ccefd2c2632e184f22cce30390dfb7d6/vertical-pod-autoscaler/pkg/apis/autoscaling.k8s.io/v1/types.go#L244-L245">&lt;code>RequestsOnly&lt;/code>&lt;/a>, reasonable &lt;code>minAllowed&lt;/code> configuration and no &lt;code>maxAllowed&lt;/code> configuration (will be taken care of in Gardener environments for you/capped at the largest eligible machine type).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Define a &lt;code>HorizontalPodAutoscaler&lt;/code> if needed&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/coredns/coredns.go#L671-L726">example&lt;/a>)&lt;/p>
&lt;p>If your component is capable of scaling horizontally, you should consider defining a &lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale">&lt;code>HorizontalPodAutoscaler&lt;/code>&lt;/a>.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>[!NOTE]
For more information and concrete configuration hints, please see our &lt;a href="https://gardener.cloud/docs/guides/applications/shoot-pod-autoscaling-best-practices/">best practices guide for pod auto scaling&lt;/a> and especially the &lt;a href="https://gardener.cloud/docs/guides/applications/shoot-pod-autoscaling-best-practices/#summary">summary&lt;/a> and &lt;a href="https://gardener.cloud/docs/guides/applications/shoot-pod-autoscaling-best-practices/#recommendations-in-a-box">recommendations&lt;/a> sections.&lt;/p>
&lt;/blockquote>
&lt;h2 id="observability--operations-productivity">Observability / Operations Productivity&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>Provide monitoring scrape config and alerting rules&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/coredns/monitoring.go">example 1&lt;/a>, &lt;a href="https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/monitoring.go#L97">example 2&lt;/a>)&lt;/p>
&lt;p>Components should provide scrape configuration and alerting rules for Prometheus/Alertmanager if appropriate.
This should be done inside a dedicated &lt;code>monitoring.go&lt;/code> file.
Extensions should follow the guidelines described in &lt;a href="https://gardener.cloud/docs/gardener/extensions/logging-and-monitoring/#extensions-monitoring-integration">Extensions Monitoring Integration&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Provide logging parsers and filters&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/coredns/logging.go">example 1&lt;/a>, &lt;a href="https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/gardenlet/controller/seed/seed/reconciler_reconcile.go#L563">example 2&lt;/a>)&lt;/p>
&lt;p>Components should provide parsers and filters for fluent-bit, if appropriate.
This should be done inside a dedicated &lt;code>logging.go&lt;/code> file.
Extensions should follow the guidelines described in &lt;a href="https://gardener.cloud/docs/gardener/extensions/logging-and-monitoring/#fluent-bit-log-parsers-and-filters">Fluent-bit log parsers and filters&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Set the &lt;code>revisionHistoryLimit&lt;/code> to &lt;code>2&lt;/code> for &lt;code>Deployment&lt;/code>s&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/metricsserver/metrics_server.go#L272">example&lt;/a>)&lt;/p>
&lt;p>In order to allow easy inspection of two &lt;code>ReplicaSet&lt;/code>s to quickly find the changes that lead to a rolling update, the &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#revision-history-limit">revision history limit&lt;/a> should be set to &lt;code>2&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Define health checks&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/180951eac9b8183175d4dcadc305c7722ce8122d/pkg/gardenlet/controller/shoot/care/health.go#L763-L795">example 1&lt;/a>)&lt;/p>
&lt;p>&lt;a href="https://gardener.cloud/docs/gardener/concepts/operator/#controllers">&lt;code>gardener-operators&lt;/code>&amp;rsquo;s&lt;/a> and &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/#controllers">&lt;code>gardenlet&lt;/code>&amp;rsquo;s&lt;/a> care controllers regularly check the health status of components relevant to the respective cluster (garden/seed/shoot).
For shoot control plane components, you need to enhance the lists of components to make sure your component is checked, see example above.
For components deployed via &lt;code>ManagedResource&lt;/code>, please consult the respective care controller documentation for more information (&lt;a href="https://gardener.cloud/docs/gardener/concepts/operator/#care-reconciler">garden&lt;/a>, &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/#-care--reconciler-1">seed&lt;/a>, &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/#-care--reconciler-2">shoot&lt;/a>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Configure automatic restarts in shoot maintenance time window&lt;/strong> (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/kubescheduler/kube_scheduler.go#L250">example 1&lt;/a>, &lt;a href="https://github.com/gardener/gardener/blob/6a0fea86850ffec8937d1956bdf1a8ca6d074f3b/pkg/operation/botanist/coredns.go#L90-L107">example 2&lt;/a>)&lt;/p>
&lt;p>Gardener offers to restart components during the maintenance time window. For more information, see &lt;a href="https://gardener.cloud/docs/gardener/shoot_maintenance/#restart-control-plane-controllers">Restart Control Plane Controllers&lt;/a> and &lt;a href="https://gardener.cloud/docs/gardener/shoot_maintenance/#restart-some-core-addons">Restart Some Core Addons&lt;/a>.
You can consider adding the needed label to your control plane component to get this automatic restart (probably not needed for most components).&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Docs: Configuration</title><link>https://gardener.cloud/docs/gardener/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/configuration/</guid><description>
&lt;h1 id="gardener-configuration-and-usage">Gardener Configuration and Usage&lt;/h1>
&lt;p>Gardener automates the full lifecycle of Kubernetes clusters as a service.
Additionally, it has several extension points allowing external controllers to plug-in to the lifecycle.
As a consequence, there are several configuration options for the various custom resources that are partially required.&lt;/p>
&lt;p>This document describes the:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/configuration/#configuration-and-usage-of-gardener-as-operatoradministrator">Configuration and usage of Gardener as operator/administrator&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/configuration/#configuration-and-usage-of-gardener-as-end-userstakeholdercustomer">Configuration and usage of Gardener as end-user/stakeholder/customer&lt;/a>.&lt;/li>
&lt;/ol>
&lt;h2 id="configuration-and-usage-of-gardener-as-operatoradministrator">Configuration and Usage of Gardener as Operator/Administrator&lt;/h2>
&lt;p>When we use the terms &amp;ldquo;operator/administrator&amp;rdquo;, we refer to both the people deploying and operating Gardener.
Gardener consists of the following components:&lt;/p>
&lt;ol>
&lt;li>&lt;code>gardener-apiserver&lt;/code>, a Kubernetes-native API extension that serves custom resources in the Kubernetes-style (like &lt;code>Seed&lt;/code>s and &lt;code>Shoot&lt;/code>s), and a component that contains multiple admission plugins.&lt;/li>
&lt;li>&lt;code>gardener-admission-controller&lt;/code>, an HTTP(S) server with several handlers to be used in a &lt;a href="https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/validatingwebhook-admission-controller.yaml">ValidatingWebhookConfiguration&lt;/a>.&lt;/li>
&lt;li>&lt;code>gardener-controller-manager&lt;/code>, a component consisting of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for maintaining &lt;code>Shoot&lt;/code>s, reconciling &lt;code>Project&lt;/code>s).&lt;/li>
&lt;li>&lt;code>gardener-scheduler&lt;/code>, a component that assigns newly created &lt;code>Shoot&lt;/code> clusters to appropriate &lt;code>Seed&lt;/code> clusters.&lt;/li>
&lt;li>&lt;code>gardenlet&lt;/code>, a component running in seed clusters and consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for reconciliation and deletion of &lt;code>Shoot&lt;/code>s).&lt;/li>
&lt;/ol>
&lt;p>Each of these components have various configuration options.
The &lt;code>gardener-apiserver&lt;/code> uses the standard API server library maintained by the Kubernetes community, and as such it mainly supports command line flags.
Other components use so-called componentconfig files that describe their configuration in a Kubernetes-style versioned object.&lt;/p>
&lt;h3 id="configuration-file-for-gardener-admission-controller">Configuration File for Gardener Admission Controller&lt;/h3>
&lt;p>The Gardener admission controller only supports one command line flag, which should be a path to a valid admission-controller configuration file.
Please take a look at this &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-admission-controller.yaml">example configuration&lt;/a>.&lt;/p>
&lt;h3 id="configuration-file-for-gardener-controller-manager">Configuration File for Gardener Controller Manager&lt;/h3>
&lt;p>The Gardener controller manager only supports one command line flag, which should be a path to a valid controller-manager configuration file.
Please take a look at this &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-controller-manager.yaml">example configuration&lt;/a>.&lt;/p>
&lt;h3 id="configuration-file-for-gardener-scheduler">Configuration File for Gardener Scheduler&lt;/h3>
&lt;p>The Gardener scheduler also only supports one command line flag, which should be a path to a valid scheduler configuration file.
Please take a look at this &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-scheduler.yaml">example configuration&lt;/a>.
Information about the concepts of the Gardener scheduler can be found at &lt;a href="https://gardener.cloud/docs/gardener/concepts/scheduler/">Gardener Scheduler&lt;/a>.&lt;/p>
&lt;h3 id="configuration-file-for-gardenlet">Configuration File for gardenlet&lt;/h3>
&lt;p>The gardenlet also only supports one command line flag, which should be a path to a valid gardenlet configuration file.
Please take a look at this &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml">example configuration&lt;/a>.
Information about the concepts of the Gardenlet can be found at &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/">gardenlet&lt;/a>.&lt;/p>
&lt;h3 id="system-configuration">System Configuration&lt;/h3>
&lt;p>After successful deployment of the four components, you need to setup the system.
Let&amp;rsquo;s first focus on some &amp;ldquo;static&amp;rdquo; configuration.
When the &lt;code>gardenlet&lt;/code> starts, it scans the &lt;code>garden&lt;/code> namespace of the garden cluster for &lt;code>Secret&lt;/code>s that have influence on its reconciliation loops, mainly the &lt;code>Shoot&lt;/code> reconciliation:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Internal domain secret&lt;/strong> - contains the DNS provider credentials (having appropriate privileges) which will be used to create/delete the so-called &amp;ldquo;internal&amp;rdquo; DNS records for the Shoot clusters, please see this &lt;a href="https://github.com/gardener/gardener/blob/master/example/10-secret-internal-domain.yaml">yaml file&lt;/a> for an example.&lt;/p>
&lt;ul>
&lt;li>This secret is used in order to establish a stable endpoint for shoot clusters, which is used internally by all control plane components.&lt;/li>
&lt;li>The DNS records are normal DNS records but called &amp;ldquo;internal&amp;rdquo; in our scenario because only the kubeconfigs for the control plane components use this endpoint when talking to the shoot clusters.&lt;/li>
&lt;li>It is forbidden to change the internal domain secret if there are existing shoot clusters.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Default domain secrets&lt;/strong> (optional) - contain the DNS provider credentials (having appropriate privileges) which will be used to create/delete DNS records for a default domain for shoots (e.g., &lt;code>example.com&lt;/code>), please see this &lt;a href="https://github.com/gardener/gardener/blob/master/example/10-secret-default-domain.yaml">yaml file&lt;/a> for an example.&lt;/p>
&lt;ul>
&lt;li>Not every end-user/stakeholder/customer has its own domain, however, Gardener needs to create a DNS record for every shoot cluster.&lt;/li>
&lt;li>As landscape operator you might want to define a default domain owned and controlled by you that is used for all shoot clusters that don&amp;rsquo;t specify their own domain.&lt;/li>
&lt;li>If you have multiple default domain secrets defined you can add a priority as an annotation (&lt;code>dns.gardener.cloud/domain-default-priority&lt;/code>) to select which domain should be used for new shoots during creation. The domain with the highest priority is selected during shoot creation. If there is no annotation defined, the default priority is &lt;code>0&lt;/code>, also all non integer values are considered as priority &lt;code>0&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Alerting secrets&lt;/strong> (optional) - contain the alerting configuration and credentials for the &lt;a href="https://prometheus.io/docs/alerting/alertmanager/">AlertManager&lt;/a> to send email alerts. It is also possible to configure the monitoring stack to send alerts to an AlertManager not deployed by Gardener to handle alerting. Please see this &lt;a href="https://github.com/gardener/gardener/blob/master/example/10-secret-alerting.yaml">yaml file&lt;/a> for an example.&lt;/p>
&lt;ul>
&lt;li>If email alerting is configured:
&lt;ul>
&lt;li>An AlertManager is deployed into each seed cluster that handles the alerting for all shoots on the seed cluster.&lt;/li>
&lt;li>Gardener will inject the SMTP credentials into the configuration of the AlertManager.&lt;/li>
&lt;li>The AlertManager will send emails to the configured email address in case any alerts are firing.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If an external AlertManager is configured:
&lt;ul>
&lt;li>Each shoot has a &lt;a href="https://prometheus.io/docs/introduction/overview/">Prometheus&lt;/a> responsible for monitoring components and sending out alerts. The alerts will be sent to a URL configured in the alerting secret.&lt;/li>
&lt;li>This external AlertManager is not managed by Gardener and can be configured however the operator sees fit.&lt;/li>
&lt;li>Supported authentication types are no authentication, basic, or mutual TLS.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Global monitoring secrets&lt;/strong> (optional) - contains basic authentication credentials for the Prometheus aggregating metrics for all clusters.&lt;/p>
&lt;ul>
&lt;li>These secrets are synced to each seed cluster and used to gain access to the aggregate monitoring components.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Shoot Service Account Issuer secret&lt;/strong> (optional) - contains the configuration needed to centrally configure gardenlets in order to implement &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/24-shoot-oidc-issuer.md">GEP-24&lt;/a>. Please see &lt;a href="https://github.com/gardener/gardener/blob/master/example/10-secret-shoot-service-account-issuer.yaml">the example configuration&lt;/a> for more details. In addition to that, the &lt;a href="https://gardener.cloud/docs/gardener/deployment/feature_gates/#list-of-feature-gates">&lt;code>ShootManagedIssuer&lt;/code>&lt;/a> gardenlet feature gate should be enabled in order for configurations to take effect.&lt;/p>
&lt;ul>
&lt;li>This secret contains the hostname which will be used to configure the shoot&amp;rsquo;s managed issuer, therefore the value of the hostname should not be changed once configured.
&lt;blockquote>
&lt;p>[!CAUTION]
&lt;a href="https://gardener.cloud/docs/gardener/concepts/operator/">Gardener Operator&lt;/a> manages this field automatically if Gardener Discovery Server is enabled and does not provide a way to change the default value of it as of now.
It calculates it based on the first ingress domain for the runtime Garden cluster. The domain is prefixed with &amp;ldquo;discovery.&amp;rdquo; using the formula &lt;code>discovery.{garden.spec.runtimeCluster.ingress.domains[0]}&lt;/code>.
If you are not yet using Gardener Operator but plan to enable the &lt;code>ShootManagedIssuer&lt;/code> feature gate, it is &lt;strong>EXTREMELY&lt;/strong> important to follow the same convention as Gardener Operator,
so that during migration to Gardener Operator the &lt;code>hostname&lt;/code> can stay the same and avoid disruptions for shoots that already have a managed service account issuer.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Apart from this &amp;ldquo;static&amp;rdquo; configuration there are several custom resources extending the Kubernetes API and used by Gardener.
As an operator/administrator, you have to configure some of them to make the system work.&lt;/p>
&lt;h3 id="configuration-and-usage-of-gardener-as-end-userstakeholdercustomer">Configuration and Usage of Gardener as End-User/Stakeholder/Customer&lt;/h3>
&lt;p>As an end-user/stakeholder/customer, you are using a Gardener landscape that has been setup for you by another team.
You don&amp;rsquo;t need to care about how Gardener itself has to be configured or how it has to be deployed.
Take a look at &lt;a href="https://gardener.cloud/docs/gardener/concepts/apiserver/">Gardener API Server&lt;/a> - the topic describes which resources are offered by Gardener.
You may want to have a more detailed look for &lt;code>Project&lt;/code>s, &lt;code>SecretBinding&lt;/code>s, &lt;code>Shoot&lt;/code>s, and &lt;code>(Cluster)OpenIDConnectPreset&lt;/code>s.&lt;/p></description></item><item><title>Docs: containerd Registry Configuration</title><link>https://gardener.cloud/docs/gardener/containerd-registry-configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/containerd-registry-configuration/</guid><description>
&lt;h1 id="containerd-registry-configuration">&lt;code>containerd&lt;/code> Registry Configuration&lt;/h1>
&lt;p>containerd supports configuring registries and mirrors. Using this native containerd feature, Shoot owners can configure containerd to use public or private mirrors for a given upstream registry. More details about the registry configuration can be found in the &lt;a href="https://github.com/containerd/containerd/blob/main/docs/hosts.md">corresponding upstream documentation&lt;/a>.&lt;/p>
&lt;h3 id="containerd-registry-configuration-patterns">&lt;code>containerd&lt;/code> Registry Configuration Patterns&lt;/h3>
&lt;p>At the time of writing this document, containerd support two patterns for configuring registries/mirrors.&lt;/p>
&lt;blockquote>
&lt;p>Note: Trying to use both of the patterns at the same time is not supported by containerd. Only one of the configuration patterns has to be followed strictly.&lt;/p>
&lt;/blockquote>
&lt;h5 id="old-and-deprecated-pattern">Old and Deprecated Pattern&lt;/h5>
&lt;p>The old and deprecated pattern is specifying &lt;code>registry.mirrors&lt;/code> and &lt;code>registry.configs&lt;/code> in the containerd&amp;rsquo;s config.toml file. See the &lt;a href="https://github.com/containerd/containerd/blob/main/docs/cri/registry.md">upstream documentation&lt;/a>.
Example of the old and deprecated pattern:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>version = 2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[plugins.&lt;span style="color:#a31515">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>.registry]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [plugins.&lt;span style="color:#a31515">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>.registry.mirrors]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> [plugins.&lt;span style="color:#a31515">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>.registry.mirrors.&lt;span style="color:#a31515">&amp;#34;docker.io&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> endpoint = [&lt;span style="color:#a31515">&amp;#34;https://public-mirror.example.com&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In the above example, containerd is configured to first try to pull &lt;code>docker.io&lt;/code> images from a configured endpoint (&lt;code>https://public-mirror.example.com&lt;/code>). If the image is not available in &lt;code>https://public-mirror.example.com&lt;/code>, then containerd will fall back to the upstream registry (&lt;code>docker.io&lt;/code>) and will pull the image from there.&lt;/p>
&lt;h5 id="hosts-directory-pattern">Hosts Directory Pattern&lt;/h5>
&lt;p>The hosts directory pattern is the new and recommended pattern for configuring registries. It is available starting &lt;code>containerd@v1.5.0&lt;/code>. See the &lt;a href="https://github.com/containerd/containerd/blob/main/docs/hosts.md">upstream documentation&lt;/a>.
The above example in the hosts directory pattern looks as follows.
The &lt;code>/etc/containerd/config.toml&lt;/code> file has the following section:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>version = 2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[plugins.&lt;span style="color:#a31515">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>.registry]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config_path = &lt;span style="color:#a31515">&amp;#34;/etc/containerd/certs.d&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The following hosts directory structure has to be created:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ tree /etc/containerd/certs.d
/etc/containerd/certs.d
└── docker.io
└── hosts.toml
&lt;/code>&lt;/pre>&lt;p>Finally, for the &lt;code>docker.io&lt;/code> upstream registry, we configure a &lt;code>hosts.toml&lt;/code> file as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>server = &lt;span style="color:#a31515">&amp;#34;https://registry-1.docker.io&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[host.&lt;span style="color:#a31515">&amp;#34;http://public-mirror.example.com&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> capabilities = [&lt;span style="color:#a31515">&amp;#34;pull&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;resolve&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="configuring-containerd-registries-for-a-shoot">Configuring &lt;code>containerd&lt;/code> Registries for a Shoot&lt;/h3>
&lt;p>Gardener supports configuring &lt;code>containerd&lt;/code> registries on a Shoot using the new &lt;a href="https://github.com/containerd/containerd/blob/main/docs/hosts.md">hosts directory pattern&lt;/a>. For each Shoot Node, Gardener creates the &lt;code>/etc/containerd/certs.d&lt;/code> directory and adds the following section to the containerd&amp;rsquo;s &lt;code>/etc/containerd/config.toml&lt;/code> file:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>[plugins.&lt;span style="color:#a31515">&amp;#34;io.containerd.grpc.v1.cri&amp;#34;&lt;/span>.registry] &lt;span style="color:#008000"># gardener-managed&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> config_path = &lt;span style="color:#a31515">&amp;#34;/etc/containerd/certs.d&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This allows Shoot owners to use the &lt;a href="https://github.com/containerd/containerd/blob/main/docs/hosts.md">hosts directory pattern&lt;/a> to configure registries for containerd. To do this, the Shoot owners need to create a directory under &lt;code>/etc/containerd/certs.d&lt;/code> that is named with the upstream registry host name. In the newly created directory, a &lt;code>hosts.toml&lt;/code> file needs to be created. For more details, see the &lt;a href="https://gardener.cloud/docs/gardener/containerd-registry-configuration/#hosts-directory-pattern">hosts directory pattern section&lt;/a> and the &lt;a href="https://github.com/containerd/containerd/blob/main/docs/hosts.md">upstream documentation&lt;/a>.&lt;/p>
&lt;h3 id="the-registry-cache-extension">The registry-cache Extension&lt;/h3>
&lt;p>There is a Gardener-native extension named &lt;a href="https://github.com/gardener/gardener-extension-registry-cache">registry-cache&lt;/a> that supports:&lt;/p>
&lt;ul>
&lt;li>Configuring containerd registry mirrors based on the above-described contract. The feature is added in &lt;a href="https://github.com/gardener/gardener-extension-registry-cache/releases/tag/v0.6.0">registry-cache@v0.6.0&lt;/a>.&lt;/li>
&lt;li>Running pull through cache(s) in the Shoot.&lt;/li>
&lt;/ul>
&lt;p>For more details, see the &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-registry-cache/">registry-cache documentation&lt;/a>.&lt;/p></description></item><item><title>Docs: Control Plane Endpoints And Ports</title><link>https://gardener.cloud/docs/gardener/control-plane-endpoints-and-ports/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/control-plane-endpoints-and-ports/</guid><description>
&lt;h1 id="endpoints-and-ports-of-a-shoot-control-plane">Endpoints and Ports of a Shoot Control-Plane&lt;/h1>
&lt;p>With the &lt;a href="https://gardener.cloud/docs/gardener/reversed-vpn-tunnel/">reversed VPN&lt;/a> tunnel, there are no endpoints with open ports in the shoot cluster required by Gardener.
In order to allow communication to the shoots control-plane in the seed cluster, there are endpoints shared by multiple shoots of a seed cluster.
Depending on the configured zones or &lt;a href="https://gardener.cloud/docs/gardener/exposureclasses/">exposure classes&lt;/a>, there are different endpoints in a seed cluster. The IP address(es) can be determined by a DNS query for the API Server URL.
The main entry-point into the seed cluster is the load balancer of the Istio ingress-gateway service. Depending on the infrastructure provider, there can be one IP address per zone.&lt;/p>
&lt;p>The load balancer of the Istio ingress-gateway service exposes the following TCP ports:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>443&lt;/strong> for requests to the shoot API Server. The request is dispatched according to the set TLS SNI extension.&lt;/li>
&lt;li>&lt;strong>8443&lt;/strong> for requests to the shoot API Server via &lt;code>api-server-proxy&lt;/code>, dispatched based on the proxy protocol target, which is the IP address of &lt;code>kubernetes.default.svc.cluster.local&lt;/code> in the shoot.&lt;/li>
&lt;li>&lt;strong>8132&lt;/strong> to establish the reversed VPN connection. It&amp;rsquo;s dispatched according to an HTTP header value.&lt;/li>
&lt;/ul>
&lt;h2 id="kube-apiserver-via-sni">&lt;code>kube-apiserver&lt;/code> via SNI&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/api-server-sni_feb16f.png" alt="kube-apiserver via SNI">&lt;/p>
&lt;p>DNS entries for &lt;code>api.&amp;lt;external-domain&amp;gt;&lt;/code> and &lt;code>api.&amp;lt;shoot&amp;gt;.&amp;lt;project&amp;gt;.&amp;lt;internal-domain&amp;gt;&lt;/code> point to the load balancer of an Istio ingress-gateway service.
The Kubernetes client sets the server name to &lt;code>api.&amp;lt;external-domain&amp;gt;&lt;/code> or &lt;code>api.&amp;lt;shoot&amp;gt;.&amp;lt;project&amp;gt;.&amp;lt;internal-domain&amp;gt;&lt;/code>.
Based on SNI, the connection is forwarded to the respective API Server at TCP layer. There is no TLS termination at the Istio ingress-gateway.
TLS termination happens on the shoots API Server. Traffic is end-to-end encrypted between the client and the API Server. The certificate authority and authentication are defined in the corresponding &lt;code>kubeconfig&lt;/code>.
Details can be found in &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md">GEP-08&lt;/a>.&lt;/p>
&lt;h2 id="kube-apiserver-via-apiserver-proxy">&lt;code>kube-apiserver&lt;/code> via &lt;code>apiserver-proxy&lt;/code>&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/api-server-proxy_b419fc.png" alt="apiserver-proxy">&lt;/p>
&lt;p>Inside the shoot cluster, the API Server can also be reached by the cluster internal name &lt;code>kubernetes.default.svc.cluster.local&lt;/code>.
The pods &lt;code>apiserver-proxy&lt;/code> are deployed in the host network as daemonset and intercept connections to the Kubernetes service IP address.
The destination address is changed to the cluster IP address of the service &lt;code>kube-apiserver.&amp;lt;shoot-namespace&amp;gt;.svc.cluster.local&lt;/code> in the seed cluster.
The connections are forwarded via the &lt;a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/listener_filters/proxy_protocol">HaProxy Proxy Protocol&lt;/a> to the Istio ingress-gateway in the seed cluster.
The Istio ingress-gateway forwards the connection to the respective shoot API Server by it&amp;rsquo;s cluster IP address.
As TLS termination happens at the API Server, the traffic is end-to-end encrypted the same way as with SNI.&lt;/p>
&lt;p>Details can be found in &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/11-apiserver-network-proxy.md">GEP-11&lt;/a>.&lt;/p>
&lt;h2 id="reversed-vpn-tunnel">Reversed VPN Tunnel&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/reversed-vpn_e89ad6.png" alt="Reversed VPN">&lt;/p>
&lt;p>As the API Server has to be able to connect to endpoints in the shoot cluster, a VPN connection is established.
This VPN connection is initiated from a VPN client in the shoot cluster.
The VPN client connects to the Istio ingress-gateway and is forwarded to the VPN server in the control-plane namespace of the shoot.
Once the VPN tunnel between the VPN client in the shoot and the VPN server in the seed cluster is established, the API Server can connect to nodes, services and pods in the shoot cluster.&lt;/p>
&lt;p>More details can be found in the &lt;a href="https://gardener.cloud/docs/gardener/reversed-vpn-tunnel/">usage document&lt;/a> and &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/14-reversed-cluster-vpn.md">GEP-14&lt;/a>.&lt;/p></description></item><item><title>Docs: Control Plane Migration</title><link>https://gardener.cloud/docs/gardener/control_plane_migration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/control_plane_migration/</guid><description>
&lt;h1 id="control-plane-migration">Control Plane Migration&lt;/h1>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>Also, the involved Seeds need to have enabled &lt;code>BackupBucket&lt;/code>s.&lt;/p>
&lt;h2 id="shootstate">ShootState&lt;/h2>
&lt;p>&lt;code>ShootState&lt;/code> is an API resource which stores non-reconstructible state and data required to completely recreate a &lt;code>Shoot&lt;/code>&amp;rsquo;s control plane on a new &lt;code>Seed&lt;/code>. The &lt;code>ShootState&lt;/code> resource is created on &lt;code>Shoot&lt;/code> creation in its &lt;code>Project&lt;/code> namespace and the required state/data is persisted during &lt;code>Shoot&lt;/code> creation or reconciliation.&lt;/p>
&lt;h2 id="shoot-control-plane-migration">Shoot Control Plane Migration&lt;/h2>
&lt;p>Triggering the migration is done by changing the &lt;code>Shoot&lt;/code>&amp;rsquo;s &lt;code>.spec.seedName&lt;/code> to a &lt;code>Seed&lt;/code> that differs from the &lt;code>.status.seedName&lt;/code>, we call this &lt;code>Seed&lt;/code> a &lt;code>&amp;quot;Destination Seed&amp;quot;&lt;/code>. This action can only be performed by an operator with the necessary RBAC. If the Destination &lt;code>Seed&lt;/code> does not have a backup and restore configuration, the change to &lt;code>spec.seedName&lt;/code> is rejected. Additionally, this Seed must not be set for deletion and must be healthy.&lt;/p>
&lt;p>If the &lt;code>Shoot&lt;/code> has different &lt;code>.spec.seedName&lt;/code> and &lt;code>.status.seedName&lt;/code>, a process is started to prepare the Control Plane for migration:&lt;/p>
&lt;ol>
&lt;li>&lt;code>.status.lastOperation&lt;/code> is changed to &lt;code>Migrate&lt;/code>.&lt;/li>
&lt;li>Kubernetes API Server is stopped and the extension resources are annotated with &lt;code>gardener.cloud/operation=migrate&lt;/code>.&lt;/li>
&lt;li>Full snapshot of the ETCD is created and terminating of the Control Plane in the &lt;code>Source Seed&lt;/code> is initiated.&lt;/li>
&lt;/ol>
&lt;p>If the process is successful, we update the status of the &lt;code>Shoot&lt;/code> by setting the &lt;code>.status.seedName&lt;/code> to the null value. That way, a restoration is triggered in the &lt;code>Destination Seed&lt;/code> and &lt;code>.status.lastOperation&lt;/code> is changed to &lt;code>Restore&lt;/code>. The control plane migration is completed when the &lt;code>Restore&lt;/code> operation has completed successfully.&lt;/p>
&lt;p>The etcd backups will be copied over to the &lt;code>BackupBucket&lt;/code> of the &lt;code>Destination Seed&lt;/code> during control plane migration and any future backups will be uploaded there.&lt;/p>
&lt;h2 id="triggering-the-migration">Triggering the Migration&lt;/h2>
&lt;p>For controlplane migration, operators with the necessary RBAC can use the &lt;a href="https://gardener.cloud/docs/gardener/concepts/scheduler/#shootsbinding-subresource">&lt;code>shoots/binding&lt;/code>&lt;/a> subresource to change the &lt;code>.spec.seedName&lt;/code>, with the following commands:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>NAMESPACE=my-namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>SHOOT_NAME=my-shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>DEST_SEED_NAME=destination-seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl get --raw /apis/core.gardener.cloud/v1beta1/namespaces/&lt;span style="color:#a31515">${&lt;/span>NAMESPACE&lt;span style="color:#a31515">}&lt;/span>/shoots/&lt;span style="color:#a31515">${&lt;/span>SHOOT_NAME&lt;span style="color:#a31515">}&lt;/span> | jq -c &lt;span style="color:#a31515">&amp;#39;.spec.seedName = &amp;#34;&amp;#39;&lt;/span>&lt;span style="color:#a31515">${&lt;/span>DEST_SEED_NAME&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">&amp;#39;&amp;#34;&amp;#39;&lt;/span> | kubectl replace --raw /apis/core.gardener.cloud/v1beta1/namespaces/&lt;span style="color:#a31515">${&lt;/span>NAMESPACE&lt;span style="color:#a31515">}&lt;/span>/shoots/&lt;span style="color:#a31515">${&lt;/span>SHOOT_NAME&lt;span style="color:#a31515">}&lt;/span>/binding -f - | jq -r &lt;span style="color:#a31515">&amp;#39;.spec.seedName&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Controlling the Kubernetes Versions for Specific Worker Pools</title><link>https://gardener.cloud/docs/gardener/worker_pool_k8s_versions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/worker_pool_k8s_versions/</guid><description>
&lt;h1 id="controlling-the-kubernetes-versions-for-specific-worker-pools">Controlling the Kubernetes Versions for Specific Worker Pools&lt;/h1>
&lt;p>Since Gardener &lt;code>v1.36&lt;/code>, worker pools can have different Kubernetes versions specified than the control plane.&lt;/p>
&lt;p>In earlier Gardener versions, all worker pools inherited the Kubernetes version of the control plane. Once the Kubernetes version of the control plane was modified, all worker pools have been updated as well (either by rolling the nodes in case of a minor version change, or in-place for patch version changes).&lt;/p>
&lt;p>In order to gracefully perform Kubernetes upgrades (triggering a rolling update of the nodes) with workloads sensitive to restarts (e.g., those dealing with lots of data), it might be required to be able to gradually perform the upgrade process.
In such cases, the Kubernetes version for the worker pools can be pinned (&lt;code>.spec.provider.workers[].kubernetes.version&lt;/code>) while the control plane Kubernetes version (&lt;code>.spec.kubernetes.version&lt;/code>) is updated.
This results in the nodes being untouched while the control plane is upgraded.
Now a new worker pool (with the version equal to the control plane version) can be added.
Administrators can then reschedule their workloads to the new worker pool according to their upgrade requirements and processes.&lt;/p>
&lt;h2 id="example-usage-in-a-shoot">Example Usage in a &lt;code>Shoot&lt;/code>&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.27.4
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: data1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.26.8
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: data2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>If &lt;code>.kubernetes.version&lt;/code> is not specified in a worker pool, then the Kubernetes version of the kubelet is inherited from the control plane (&lt;code>.spec.kubernetes.version&lt;/code>), i.e., in the above example, the &lt;code>data2&lt;/code> pool will use &lt;code>1.26.8&lt;/code>.&lt;/li>
&lt;li>If &lt;code>.kubernetes.version&lt;/code> is specified in a worker pool, then it must meet the following constraints:
&lt;ul>
&lt;li>It must be at most two minor versions lower than the control plane version.&lt;/li>
&lt;li>If it was not specified before, then no downgrade is possible (you cannot set it to &lt;code>1.26.8&lt;/code> while &lt;code>.spec.kubernetes.version&lt;/code> is already &lt;code>1.27.4&lt;/code>). The &amp;ldquo;two minor version skew&amp;rdquo; is only possible if the worker pool version is set to the control plane version and then the control plane was updated gradually by two minor versions.&lt;/li>
&lt;li>If the version is removed from the worker pool, only one minor version difference is allowed to the control plane (you cannot upgrade a pool from version &lt;code>1.25.0&lt;/code> to &lt;code>1.27.0&lt;/code> in one go).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Automatic updates of Kubernetes versions (see &lt;a href="https://gardener.cloud/docs/gardener/shoot_maintenance/#automatic-version-updates">Shoot Maintenance&lt;/a>) also apply to worker pool Kubernetes versions.&lt;/p></description></item><item><title>Docs: Custom containerd Configuration</title><link>https://gardener.cloud/docs/gardener/custom-containerd-config/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/custom-containerd-config/</guid><description>
&lt;h1 id="custom-containerd-configuration">Custom &lt;code>containerd&lt;/code> Configuration&lt;/h1>
&lt;p>In case a &lt;code>Shoot&lt;/code> cluster uses &lt;code>containerd&lt;/code>, it is possible to make the &lt;code>containerd&lt;/code> process load custom configuration files.
Gardener initializes &lt;code>containerd&lt;/code> with the following statement:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>imports = [&lt;span style="color:#a31515">&amp;#34;/etc/containerd/conf.d/*.toml&amp;#34;&lt;/span>]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This means that all &lt;code>*.toml&lt;/code> files in the &lt;code>/etc/containerd/conf.d&lt;/code> directory will be imported and merged with the default configuration.
To prevent unintended configuration overwrites, please be aware that containerd merges config sections, not individual keys (see &lt;a href="https://github.com/containerd/containerd/issues/5837#issuecomment-894840240">here&lt;/a> and &lt;a href="https://github.com/gardener/gardener/pull/7316">here&lt;/a>).
Please consult the &lt;a href="https://github.com/containerd/containerd/blob/main/docs/man/containerd-config.toml.5.md#format">upstream &lt;code>containerd&lt;/code> documentation&lt;/a> for more information.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ Note that this only applies to nodes which were newly created after &lt;code>gardener/gardener@v1.51&lt;/code> was deployed. Existing nodes are not affected.&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Custom DNS Configuration</title><link>https://gardener.cloud/docs/gardener/custom-dns-config/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/custom-dns-config/</guid><description>
&lt;h1 id="custom-dns-configuration">Custom DNS Configuration&lt;/h1>
&lt;p>Gardener provides Kubernetes-Clusters-As-A-Service where all the system components (e.g., kube-proxy, networking, dns) are managed.
As a result, Gardener needs to ensure and auto-correct additional configuration to those system components to avoid unnecessary down-time.&lt;/p>
&lt;p>In some cases, auto-correcting system components can prevent users from deploying applications on top of the cluster that requires bits of customization, DNS configuration can be a good example.&lt;/p>
&lt;p>To allow for customizations for DNS configuration (that could potentially lead to downtime) while having the option to &amp;ldquo;undo&amp;rdquo;, we utilize the &lt;code>import&lt;/code> plugin from CoreDNS [1].
which enables in-line configuration changes.&lt;/p>
&lt;h2 id="how-to-use">How to use&lt;/h2>
&lt;p>To customize your CoreDNS cluster config, you can simply edit a &lt;code>ConfigMap&lt;/code> named &lt;code>coredns-custom&lt;/code> in the &lt;code>kube-system&lt;/code> namespace.
By editing, this &lt;code>ConfigMap&lt;/code>, you are modifying CoreDNS configuration, therefore care is advised.&lt;/p>
&lt;p>For example, to apply new config to CoreDNS that would point all &lt;code>.global&lt;/code> DNS requests to another DNS pod, simply edit the configuration as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ConfigMap
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: coredns-custom
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> istio.server: |&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> global:8053 {
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> errors
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> cache 30
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> forward . 1.2.3.4
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> }&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> corefile.override: |&lt;span style="color:#a31515">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> # &amp;lt;some-plugin&amp;gt; &amp;lt;some-plugin-config&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> debug
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515"> whoami&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The port number 8053 in &lt;code>global:8053&lt;/code> is the specific port that CoreDNS is bound to and cannot be changed to any other port if it should act on ordinary name resolution requests from pods. Otherwise, CoreDNS will open a second port, but you are responsible to direct the traffic to this port. &lt;code>kube-dns&lt;/code> service in &lt;code>kube-system&lt;/code> namespace will direct name resolution requests within the cluster to port 8053 on the CoreDNS pods.
Moreover, additional network policies are needed to allow corresponding ingress traffic to CoreDNS pods.
In order for the destination DNS server to be reachable, it must listen on port 53 as it is required by network policies. Other ports are only possible if additional network policies allow corresponding egress traffic from CoreDNS pods.&lt;/p>
&lt;p>It is important to have the &lt;code>ConfigMap&lt;/code> keys ending with &lt;code>*.server&lt;/code> (if you would like to add a new server) or &lt;code>*.override&lt;/code>
if you want to customize the current server configuration (it is optional setting both).&lt;/p>
&lt;h2 id="optional-reload-coredns">[Optional] Reload CoreDNS&lt;/h2>
&lt;p>As Gardener is configuring the &lt;code>reload&lt;/code> &lt;a href="https://coredns.io/plugins/reload/">plugin&lt;/a> of CoreDNS a restart of the CoreDNS components is typically not necessary to propagate &lt;code>ConfigMap&lt;/code> changes. However, if you don&amp;rsquo;t want to wait for the default (30s) to kick in, you can roll-out your CoreDNS deployment using:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n kube-system rollout restart deploy coredns
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will reload the config into CoreDNS.&lt;/p>
&lt;p>The approach we follow here was inspired by AKS&amp;rsquo;s approach [2].&lt;/p>
&lt;h2 id="anti-pattern">Anti-Pattern&lt;/h2>
&lt;p>Applying a configuration that is in-compatible with the running version of CoreDNS is an anti-pattern (sometimes plugin configuration changes,
simply applying a configuration can break DNS).&lt;/p>
&lt;p>If incompatible changes are applied by mistake, simply delete the content of the &lt;code>ConfigMap&lt;/code> and re-apply.
This should bring the cluster DNS back to functioning state.&lt;/p>
&lt;h2 id="node-local-dns">Node Local DNS&lt;/h2>
&lt;p>Custom DNS configuration] may not work as expected in conjunction with &lt;code>NodeLocalDNS&lt;/code>.
With &lt;code>NodeLocalDNS&lt;/code>, ordinary DNS queries targeted at the upstream DNS servers, i.e. non-kubernetes domains,
will not end up at CoreDNS, but will instead be directly sent to the upstream DNS server. Therefore, configuration
applying to non-kubernetes entities, e.g. the &lt;code>istio.server&lt;/code> block in the
&lt;a href="https://gardener.cloud/docs/gardener/custom-dns-config/">custom DNS configuration&lt;/a> example, may not have any effect with &lt;code>NodeLocalDNS&lt;/code> enabled.
If this kind of custom configuration is required, forwarding to upstream DNS has to be disabled.
This can be done by setting the option (&lt;code>spec.systemComponents.nodeLocalDNS.disableForwardToUpstreamDNS&lt;/code>) in the &lt;code>Shoot&lt;/code> resource to &lt;code>true&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> systemComponents:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeLocalDNS:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> disableForwardToUpstreamDNS: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="references">References&lt;/h2>
&lt;p>[1] &lt;a href="https://github.com/coredns/coredns/tree/master/plugin/import">Import plugin&lt;/a>
[2] &lt;a href="https://docs.microsoft.com/en-us/azure/aks/coredns-custom">AKS Custom DNS&lt;/a>&lt;/p></description></item><item><title>Docs: Default Seccomp Profile</title><link>https://gardener.cloud/docs/gardener/default_seccomp_profile/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/default_seccomp_profile/</guid><description>
&lt;h1 id="default-seccomp-profile-and-configuration">Default Seccomp Profile and Configuration&lt;/h1>
&lt;p>This is a short guide describing how to enable the defaulting of seccomp profiles for Gardener managed workloads in the seed. Running pods in &lt;code>Unconfined&lt;/code> (seccomp disabled) mode is undesirable since this is the least restrictive profile. Also, mind that any privileged container will always run as &lt;code>Unconfined&lt;/code>. More information about seccomp can be found in this &lt;a href="https://kubernetes.io/docs/tutorials/security/seccomp/">Kubernetes tutorial&lt;/a>.&lt;/p>
&lt;h2 id="setting-the-seccomp-profile-to-runtimedefault-for-seed-clusters">Setting the Seccomp Profile to RuntimeDefault for Seed Clusters&lt;/h2>
&lt;p>To address the above issue, Gardener provides a webhook that is capable of mutating pods in the seed clusters, explicitly providing them with a seccomp profile type of &lt;code>RuntimeDefault&lt;/code>. This profile is defined by the container runtime and represents a set of default syscalls that are allowed or not.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> securityContext:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seccompProfile:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: RuntimeDefault
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A &lt;code>Pod&lt;/code> is mutated when all of the following preconditions are fulfilled:&lt;/p>
&lt;ol>
&lt;li>The &lt;code>Pod&lt;/code> is created in a Gardener managed namespace.&lt;/li>
&lt;li>The &lt;code>Pod&lt;/code> is NOT labeled with &lt;code>seccompprofile.resources.gardener.cloud/skip&lt;/code>.&lt;/li>
&lt;li>The &lt;code>Pod&lt;/code> does NOT explicitly specify &lt;code>.spec.securityContext.seccompProfile.type&lt;/code>.&lt;/li>
&lt;/ol>
&lt;h3 id="how-to-configure">How to Configure&lt;/h3>
&lt;p>To enable this feature, the gardenlet &lt;code>DefaultSeccompProfile&lt;/code> feature gate must be set to &lt;code>true&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>featureGates:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> DefaultSeccompProfile: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please refer to the examples in this &lt;a href="https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml">yaml file&lt;/a> for more information.&lt;/p>
&lt;p>Once the feature gate is enabled, the webhook will be registered and configured for the seed cluster. Newly created pods will be mutated to have their seccomp profile set to &lt;code>RuntimeDefault&lt;/code>.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> Please note that this feature is still in Alpha, so you might see instabilities every now and then.&lt;/p>
&lt;/blockquote>
&lt;h2 id="setting-the-seccomp-profile-to-runtimedefault-for-shoot-clusters">Setting the Seccomp Profile to RuntimeDefault for Shoot Clusters&lt;/h2>
&lt;p>You can enable the use of &lt;code>RuntimeDefault&lt;/code> as the default seccomp profile for all workloads. If enabled, the kubelet will use the &lt;code>RuntimeDefault&lt;/code> seccomp profile by default, which is defined by the container runtime, instead of using the &lt;code>Unconfined&lt;/code> mode. More information for this feature can be found in the &lt;a href="https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads">Kubernetes documentation&lt;/a>.&lt;/p>
&lt;p>To use seccomp profile defaulting, you must run the kubelet with the &lt;code>SeccompDefault&lt;/code> feature gate enabled (this is the default).&lt;/p>
&lt;h3 id="how-to-configure-1">How to Configure&lt;/h3>
&lt;p>To enable this feature, the kubelet &lt;code>seccompDefault&lt;/code> configuration parameter must be set to &lt;code>true&lt;/code> in the shoot&amp;rsquo;s spec.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.25.0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubelet:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seccompDefault: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please refer to the examples in this &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml">yaml file&lt;/a> for more information.&lt;/p></description></item><item><title>Docs: Defaulting</title><link>https://gardener.cloud/docs/gardener/defaulting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/defaulting/</guid><description>
&lt;h1 id="defaulting-strategy-and-developer-guidelines">Defaulting Strategy and Developer Guidelines&lt;/h1>
&lt;p>This document walks you through:&lt;/p>
&lt;ul>
&lt;li>Conventions to be followed when writing defaulting functions&lt;/li>
&lt;li>How to write a test for a defaulting function&lt;/li>
&lt;/ul>
&lt;p>The document is aimed towards developers who want to contribute code and need to write defaulting code and unit tests covering the defaulting functions, as well as maintainers and reviewers who review code.
It serves as a common guide that we commit to follow in our project to ensure consistency in our defaulting code, good coverage for high confidence, and good maintainability.&lt;/p>
&lt;h2 id="writing-defaulting-code">Writing defaulting code&lt;/h2>
&lt;ul>
&lt;li>Every kubernetes type should have a dedicated &lt;code>defaults_*.go&lt;/code> file. For instance, if you have a &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/apis/core/types_shoot.go">&lt;code>Shoot&lt;/code>&lt;/a> type, there should be a corresponding &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/apis/core/v1beta1/defaults_shoot.go">&lt;code>defaults_shoot.go&lt;/code>&lt;/a> file containing all defaulting logic for that type.&lt;/li>
&lt;li>If there is only one type under an api group then we can just have &lt;code>types.go&lt;/code> and a corresponding &lt;code>defaults.go&lt;/code>. For instance, &lt;code>resourcemanager&lt;/code> api has only one &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/resourcemanager/apis/config/v1alpha1/types.go">&lt;code>types.go&lt;/code>&lt;/a>, hence in this case only &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/resourcemanager/apis/config/v1alpha1/defaults.go">&lt;code>defaults.go&lt;/code>&lt;/a> file would suffice.&lt;/li>
&lt;li>Aim to segregate each struct type into its own &lt;code>SetDefaults_*&lt;/code> function. These functions encapsulate the defaulting logic specific to the corresponding struct type, enhancing modularity and maintainability. For example, &lt;a href="https://github.com/gardener/gardener/blob/ff5a5be6049777b0695659a50189e461e1b17796/pkg/resourcemanager/apis/config/v1alpha1/types.go#L64-L74">&lt;code>ServerConfiguration&lt;/code>&lt;/a> struct in &lt;code>resourcemanager&lt;/code> api has corresponding &lt;a href="https://github.com/gardener/gardener/blob/ff5a5be6049777b0695659a50189e461e1b17796/pkg/resourcemanager/apis/config/v1alpha1/defaults.go#L73-L92">&lt;code>SetDefaults_ServerConfiguration()&lt;/code>&lt;/a> function.&lt;/li>
&lt;/ul>
&lt;p>⚠️ Ensure to run the &lt;code>make generate WHAT=codegen&lt;/code> command when new &lt;code>SetDefaults_*&lt;/code> function is added, which generates the &lt;code>zz_generated.defaults.go&lt;/code> file containing the overall defaulting function.&lt;/p>
&lt;h2 id="writing-unit-tests-for-defaulting-code">Writing unit tests for defaulting code&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>Each test case should validate the overall defaulting function &lt;code>SetObjectDefaults_*&lt;/code> generated by &lt;code>defaulter-gen&lt;/code> and not a specific &lt;code>SetDefaults_*&lt;/code>. This way we also test if the &lt;code>zz_generated.defaults.go&lt;/code> was generated correctly.
For example, the &lt;code>spec.machineImages[].updateStrategy&lt;/code> field in the CloudProfile is defaulted as follows:
&lt;a href="https://github.com/gardener/gardener/blob/ff5a5be6049777b0695659a50189e461e1b17796/pkg/apis/core/v1beta1/defaults_cloudprofile.go#L23-L29">https://github.com/gardener/gardener/blob/ff5a5be6049777b0695659a50189e461e1b17796/pkg/apis/core/v1beta1/defaults_cloudprofile.go#L23-L29&lt;/a>
The defaulting should be tested with the overall defaulting function &lt;code>SetObjectDefaults_CloudProfile&lt;/code> (and not with &lt;code>SetDefaults_MachineImage&lt;/code>):
&lt;a href="https://github.com/gardener/gardener/blob/ff5a5be6049777b0695659a50189e461e1b17796/pkg/apis/core/v1beta1/defaults_cloudprofile_test.go#L40-L47">https://github.com/gardener/gardener/blob/ff5a5be6049777b0695659a50189e461e1b17796/pkg/apis/core/v1beta1/defaults_cloudprofile_test.go#L40-L47&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Test each defaulting function carefully to ensure:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Proper defaulting behaviour when fields are empty or nil. Note that some fields may be optional and should not be defaulted.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Preservation of existing values, ensuring that defaulting does not accidentally overwrite them.&lt;/p>
&lt;p>For example, when &lt;code>spec.secretRef.namespace&lt;/code> field of &lt;code>SecretBinding&lt;/code> is nil, it should be defaulted to the namespace of SecretBinding object. But &lt;code>spec.secretRef.namespace&lt;/code> field should not be overwritten by defaulting logic if it is already set.
&lt;a href="https://github.com/gardener/gardener/blob/ff5a5be6049777b0695659a50189e461e1b17796/pkg/apis/core/v1beta1/defaults_secretbinding_test.go#L26-L54">https://github.com/gardener/gardener/blob/ff5a5be6049777b0695659a50189e461e1b17796/pkg/apis/core/v1beta1/defaults_secretbinding_test.go#L26-L54&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: Dependencies</title><link>https://gardener.cloud/docs/gardener/dependencies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/dependencies/</guid><description>
&lt;h1 id="dependency-management">Dependency Management&lt;/h1>
&lt;p>We are using &lt;a href="https://github.com/golang/go/wiki/Modules">go modules&lt;/a> for dependency management.
In order to add a new package dependency to the project, you can perform &lt;code>go get &amp;lt;PACKAGE&amp;gt;@&amp;lt;VERSION&amp;gt;&lt;/code> or edit the &lt;code>go.mod&lt;/code> file and append the package along with the version you want to use.&lt;/p>
&lt;h2 id="updating-dependencies">Updating Dependencies&lt;/h2>
&lt;p>The &lt;code>Makefile&lt;/code> contains a rule called &lt;code>tidy&lt;/code> which performs &lt;code>go mod tidy&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>go mod tidy&lt;/code> makes sure &lt;code>go.mod&lt;/code> matches the source code in the module. It adds any missing modules necessary to build the current module&amp;rsquo;s packages and dependencies, and it removes unused modules that don&amp;rsquo;t provide any relevant packages.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make tidy
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>⚠️ Make sure that you test the code after you have updated the dependencies!&lt;/p>
&lt;h2 id="exported-packages">Exported Packages&lt;/h2>
&lt;p>This repository contains several packages that could be considered &amp;ldquo;exported packages&amp;rdquo;, in a sense that they are supposed to be reused in other Go projects.
For example:&lt;/p>
&lt;ul>
&lt;li>Gardener&amp;rsquo;s API packages: &lt;code>pkg/apis&lt;/code>&lt;/li>
&lt;li>Library for building Gardener extensions: &lt;code>extensions&lt;/code>&lt;/li>
&lt;li>Gardener&amp;rsquo;s Test Framework: &lt;code>test/framework&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>There are a few more folders in this repository (non-Go sources) that are reused across projects in the Gardener organization:&lt;/p>
&lt;ul>
&lt;li>GitHub templates: &lt;code>.github&lt;/code>&lt;/li>
&lt;li>Concourse / cc-utils related helpers: &lt;code>hack/.ci&lt;/code>&lt;/li>
&lt;li>Development, build and testing helpers: &lt;code>hack&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>These packages feature a dummy &lt;code>doc.go&lt;/code> file to allow other Go projects to pull them in as go mod dependencies.&lt;/p>
&lt;p>These packages are explicitly &lt;em>not&lt;/em> supposed to be used in other projects (consider them as &amp;ldquo;non-exported&amp;rdquo;):&lt;/p>
&lt;ul>
&lt;li>API validation packages: &lt;code>pkg/apis/*/*/validation&lt;/code>&lt;/li>
&lt;li>Operation package (main Gardener business logic regarding &lt;code>Seed&lt;/code> and &lt;code>Shoot&lt;/code> clusters): &lt;code>pkg/gardenlet/operation&lt;/code>&lt;/li>
&lt;li>Third party code: &lt;code>third_party&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Currently, we don&amp;rsquo;t have a mechanism yet for selectively syncing out these exported packages into dedicated repositories like kube&amp;rsquo;s &lt;a href="https://github.com/kubernetes/kubernetes/tree/master/staging">staging mechanism&lt;/a> (&lt;a href="https://github.com/kubernetes/publishing-bot">publishing-bot&lt;/a>).&lt;/p>
&lt;h2 id="import-restrictions">Import Restrictions&lt;/h2>
&lt;p>We want to make sure that other projects can depend on this repository&amp;rsquo;s &amp;ldquo;exported&amp;rdquo; packages without pulling in the entire repository (including &amp;ldquo;non-exported&amp;rdquo; packages) or a high number of other unwanted dependencies.
Hence, we have to be careful when adding new imports or references between our packages.&lt;/p>
&lt;blockquote>
&lt;p>ℹ️ General rule of thumb: the mentioned &amp;ldquo;exported&amp;rdquo; packages should be as self-contained as possible and depend on as few other packages in the repository and other projects as possible.&lt;/p>
&lt;/blockquote>
&lt;p>In order to support that rule and automatically check compliance with that goal, we leverage &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/cmd/import-boss">import-boss&lt;/a>.
The tool checks all imports of the given packages (including transitive imports) against rules defined in &lt;code>.import-restrictions&lt;/code> files in each directory.
An import is allowed if it matches at least one allowed prefix and does not match any forbidden prefixes.&lt;/p>
&lt;blockquote>
&lt;p>Note: &lt;code>''&lt;/code> (the empty string) is a prefix of everything.
For more details, see the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/cmd/import-boss/README.md">import-boss&lt;/a> topic.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;code>import-boss&lt;/code> is executed on every pull request and blocks the PR if it doesn&amp;rsquo;t comply with the defined import restrictions.
You can also run it locally using &lt;code>make check&lt;/code>.&lt;/p>
&lt;p>Import restrictions should be changed in the following situations:&lt;/p>
&lt;ul>
&lt;li>We spot a new pattern of imports across our packages that was not restricted before but makes it more difficult for other projects to depend on our &amp;ldquo;exported&amp;rdquo; packages.
In that case, the imports should be further restricted to disallow such problematic imports, and the code/package structure should be reworked to comply with the newly given restrictions.&lt;/li>
&lt;li>We want to share code between packages, but existing import restrictions prevent us from doing so.
In that case, please consider what additional dependencies it will pull in, when loosening existing restrictions.
Also consider possible alternatives, like code restructurings or extracting shared code into dedicated packages for minimal impact on dependent projects.&lt;/li>
&lt;/ul></description></item><item><title>Docs: DNS Autoscaling</title><link>https://gardener.cloud/docs/gardener/dns-autoscaling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/dns-autoscaling/</guid><description>
&lt;h1 id="dns-autoscaling">DNS Autoscaling&lt;/h1>
&lt;p>This is a short guide describing different options how to automatically scale CoreDNS in the shoot cluster.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Currently, Gardener uses CoreDNS as DNS server. Per default, it is installed as a deployment into the shoot cluster that is auto-scaled horizontally to cover for QPS-intensive applications. However, doing so does not seem to be enough to completely circumvent DNS bottlenecks such as:&lt;/p>
&lt;ul>
&lt;li>Cloud provider limits for DNS lookups.&lt;/li>
&lt;li>Unreliable UDP connections that forces a period of timeout in case packets are dropped.&lt;/li>
&lt;li>Unnecessary node hopping since CoreDNS is not deployed on all nodes, and as a result DNS queries end-up traversing multiple nodes before reaching the destination server.&lt;/li>
&lt;li>Inefficient load-balancing of services (e.g., round-robin might not be enough when using IPTables mode).&lt;/li>
&lt;li>Overload of the CoreDNS replicas as the maximum amount of replicas is fixed.&lt;/li>
&lt;li>and more &amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>As an alternative with extended configuration options, Gardener provides cluster-proportional autoscaling of CoreDNS. This guide focuses on the configuration of cluster-proportional autoscaling of CoreDNS and its advantages/disadvantages compared to the horizontal
autoscaling.
Please note that there is also the option to use a &lt;a href="https://gardener.cloud/docs/gardener/node-local-dns/">node-local DNS cache&lt;/a>, which helps mitigate potential DNS bottlenecks (see &lt;a href="https://gardener.cloud/docs/gardener/dns-autoscaling/#trade-offs-in-conjunction-with-nodelocaldns">Trade-offs in conjunction with NodeLocalDNS&lt;/a> for considerations regarding using NodeLocalDNS together with one of the CoreDNS autoscaling approaches).&lt;/p>
&lt;h2 id="configuring-cluster-proportional-dns-autoscaling">Configuring Cluster-Proportional DNS Autoscaling&lt;/h2>
&lt;p>All that needs to be done to enable the usage of cluster-proportional autoscaling of CoreDNS is to set the corresponding option (&lt;code>spec.systemComponents.coreDNS.autoscaling.mode&lt;/code>) in the &lt;code>Shoot&lt;/code> resource to &lt;code>cluster-proportional&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> systemComponents:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> coreDNS:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> autoscaling:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mode: cluster-proportional
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To switch back to horizontal DNS autoscaling, you can set the &lt;code>spec.systemComponents.coreDNS.autoscaling.mode&lt;/code> to &lt;code>horizontal&lt;/code> (or remove the &lt;code>coreDNS&lt;/code> section).&lt;/p>
&lt;p>Once the cluster-proportional autoscaling of CoreDNS has been enabled and the Shoot cluster has been reconciled afterwards, a ConfigMap called &lt;code>coredns-autoscaler&lt;/code> will be created in the &lt;code>kube-system&lt;/code> namespace with the default settings. The content will be similar to the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>linear: &lt;span style="color:#a31515">&amp;#39;{&amp;#34;coresPerReplica&amp;#34;:256,&amp;#34;min&amp;#34;:2,&amp;#34;nodesPerReplica&amp;#34;:16}&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It is possible to adapt the ConfigMap according to your needs in case the defaults do not work as desired. The number of CoreDNS replicas is calculated according to the following formula:&lt;/p>
&lt;pre tabindex="0">&lt;code>replicas = max( ceil( cores × 1 / coresPerReplica ) , ceil( nodes × 1 / nodesPerReplica ) )
&lt;/code>&lt;/pre>&lt;p>Depending on your needs, you can adjust &lt;code>coresPerReplica&lt;/code> or &lt;code>nodesPerReplica&lt;/code>, but it is also possible to override &lt;code>min&lt;/code> if required.&lt;/p>
&lt;h2 id="trade-offs-of-horizontal-and-cluster-proportional-dns-autoscaling">Trade-Offs of Horizontal and Cluster-Proportional DNS Autoscaling&lt;/h2>
&lt;p>The horizontal autoscaling of CoreDNS as implemented by Gardener is fully managed, i.e., you do not need to perform any configuration changes. It scales according to the CPU usage of CoreDNS replicas, meaning that it will create new replicas if the existing ones are under heavy load. This approach scales between 2 and 5 instances, which is sufficient for most workloads. In case this is not enough, the cluster-proportional autoscaling approach can be used instead, with its more flexible configuration options.&lt;/p>
&lt;p>The cluster-proportional autoscaling of CoreDNS as implemented by Gardener is fully managed, but allows more configuration options to adjust the default settings to your individual needs. It scales according to the cluster size, i.e., if your cluster grows in terms of cores/nodes so will the amount of CoreDNS replicas. However, it does not take the actual workload, e.g., CPU consumption, into account.&lt;/p>
&lt;p>Experience shows that the horizontal autoscaling of CoreDNS works for a variety of workloads. It does reach its limits if a cluster has a high amount of DNS requests, though. The cluster-proportional autoscaling approach allows to fine-tune the amount of CoreDNS replicas. It helps to scale in clusters of changing size. However, please keep in mind that you need to cater for the maximum amount of DNS requests as the replicas will not be adapted according to the workload, but only according to the cluster size (cores/nodes).&lt;/p>
&lt;h2 id="trade-offs-in-conjunction-with-nodelocaldns">Trade-Offs in Conjunction with NodeLocalDNS&lt;/h2>
&lt;p>Using a &lt;a href="https://gardener.cloud/docs/gardener/node-local-dns/">node-local DNS cache&lt;/a> can mitigate a lot of the potential DNS related problems. It works fine with a DNS workload that can be handle through the cache and reduces the inter-node DNS communication. As &lt;a href="https://gardener.cloud/docs/gardener/node-local-dns/">node-local DNS cache&lt;/a> reduces the amount of traffic being sent to the cluster&amp;rsquo;s CoreDNS replicas, it usually works fine with horizontally scaled CoreDNS. Nevertheless, it also works with CoreDNS scaled in a cluster-proportional approach. In this mode, though, it might make sense to adapt the default settings as the CoreDNS workload is likely significantly reduced.&lt;/p>
&lt;p>Overall, you can view the DNS options on a scale. Horizontally scaled DNS provides a small amount of DNS servers. Especially for bigger clusters, a cluster-proportional approach will yield more CoreDNS instances and hence may yield a more balanced DNS solution. By adapting the settings you can further increase the amount of CoreDNS replicas. On the other end of the spectrum, a &lt;a href="https://gardener.cloud/docs/gardener/node-local-dns/">node-local DNS cache&lt;/a> provides DNS on every node and allows to reduce the amount of (backend) CoreDNS instances regardless if they are horizontally or cluster-proportionally scaled.&lt;/p></description></item><item><title>Docs: DNS Search Path Optimization</title><link>https://gardener.cloud/docs/gardener/dns-search-path-optimization/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/dns-search-path-optimization/</guid><description>
&lt;h1 id="dns-search-path-optimization">DNS Search Path Optimization&lt;/h1>
&lt;h2 id="dns-search-path">DNS Search Path&lt;/h2>
&lt;p>Using fully qualified names has some downsides, e.g., it may become harder to move deployments from one landscape to the
next. It is far easier and simple to rely on short/local names, which may have different meaning depending on the context
they are used in.&lt;/p>
&lt;p>The DNS search path allows for the usage of short/local names. It is an ordered list of DNS suffixes to append to short/local
names to create a fully qualified name.&lt;/p>
&lt;p>If a short/local name should be resolved, each entry is appended to it one by one to check whether it can be resolved. The
process stops when either the name could be resolved or the DNS search path ends. As the last step after trying the search
path, the short/local name is attempted to be resolved on it own.&lt;/p>
&lt;h2 id="dns-option-ndots">DNS Option &lt;code>ndots&lt;/code>&lt;/h2>
&lt;p>As explained in the &lt;a href="https://gardener.cloud/docs/gardener/dns-search-path-optimization/#dns-search-path">section above&lt;/a>, the DNS search path is used for short/local names to create fully
qualified names. The DNS option &lt;code>ndots&lt;/code> specifies how many dots (&lt;code>.&lt;/code>) a name needs to have to be considered fully qualified.
For names with less than &lt;code>ndots&lt;/code> dots (&lt;code>.&lt;/code>), the &lt;a href="https://gardener.cloud/docs/gardener/dns-search-path-optimization/#dns-search-path">DNS search path&lt;/a> will be applied.&lt;/p>
&lt;h2 id="dns-search-path-ndots-and-kubernetes">DNS Search Path, &lt;code>ndots&lt;/code>, and Kubernetes&lt;/h2>
&lt;p>Kubernetes tries to make it easy/convenient for developers to use name resolution. It provides several means to address a
service, most notably by its name directly, using the namespace as suffix, utilizing &lt;code>&amp;lt;namespace&amp;gt;.svc&lt;/code> as suffix or as a
fully qualified name as &lt;code>&amp;lt;service&amp;gt;.&amp;lt;namespace&amp;gt;.svc.cluster.local&lt;/code> (assuming &lt;code>cluster.local&lt;/code> to be the cluster domain).&lt;/p>
&lt;p>This is why the DNS search path is fairly long in Kubernetes, usually consisting of &lt;code>&amp;lt;namespace&amp;gt;.svc.cluster.local&lt;/code>,
&lt;code>svc.cluster.local&lt;/code>, &lt;code>cluster.local&lt;/code>, and potentially some additional entries coming from the local network of the cluster.
For various reasons, the default &lt;code>ndots&lt;/code> value in the context of Kubernetes is with &lt;code>5&lt;/code>, also fairly large. See
&lt;a href="https://github.com/kubernetes/kubernetes/issues/33554#issuecomment-266251056">this comment&lt;/a> for a more detailed description.&lt;/p>
&lt;h2 id="dns-search-pathndots-problem-in-kubernetes">DNS Search Path/&lt;code>ndots&lt;/code> Problem in Kubernetes&lt;/h2>
&lt;p>As the DNS search path is long and &lt;code>ndots&lt;/code> is large, a lot of DNS queries might traverse the DNS search path. This results
in an explosion of DNS requests.&lt;/p>
&lt;p>For example, consider the name resolution of the default kubernetes service &lt;code>kubernetes.default.svc.cluster.local&lt;/code>. As this
name has only four dots, it is not considered a fully qualified name according to the default &lt;code>ndots=5&lt;/code> setting. Therefore,
the DNS search path is applied, resulting in the following queries being created&lt;/p>
&lt;ul>
&lt;li>&lt;code>kubernetes.default.svc.cluster.local.some-namespace.svc.cluster.local&lt;/code>&lt;/li>
&lt;li>&lt;code>kubernetes.default.svc.cluster.local.svc.cluster.local&lt;/code>&lt;/li>
&lt;li>&lt;code>kubernetes.default.svc.cluster.local.cluster.local&lt;/code>&lt;/li>
&lt;li>&lt;code>kubernetes.default.svc.cluster.local.network-domain&lt;/code>&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>In IPv4/IPv6 dual stack systems, the amount of DNS requests may even double as each name is resolved for IPv4 and IPv6.&lt;/p>
&lt;h2 id="general-workaroundsmitigations">General Workarounds/Mitigations&lt;/h2>
&lt;p>Kubernetes provides the capability to set the DNS options for each pod (see
&lt;a href="https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config">Pod DNS config&lt;/a> for details).
However, this has to be applied for every pod (doing name resolution) to resolve the problem. A mutating webhook may be
useful in this regard. Unfortunately, the DNS requirements may be different depending on the workload. Therefore, a general
solution may difficult to impossible.&lt;/p>
&lt;p>Another approach is to use always fully qualified names and append a dot (&lt;code>.&lt;/code>) to the name to prevent the name resolution
system from using the DNS search path. This might be somewhat counterintuitive as most developers are not used to the
trailing dot (&lt;code>.&lt;/code>). Furthermore, it makes moving to different landscapes more difficult/error-prone.&lt;/p>
&lt;h2 id="gardener-specific-workaroundsmitigations">Gardener Specific Workarounds/Mitigations&lt;/h2>
&lt;p>Gardener allows users to &lt;a href="https://gardener.cloud/docs/gardener/custom-dns-config/">customize their DNS configuration&lt;/a>. CoreDNS allows several approaches to deal with
the requests generated by the DNS search path. &lt;a href="https://coredns.io/plugins/cache/">Caching&lt;/a> is possible as well as
&lt;a href="https://coredns.io/plugins/rewrite/">query rewriting&lt;/a>. There are also several other &lt;a href="https://coredns.io/plugins/">plugins&lt;/a>
available, which may mitigate the situation.&lt;/p>
&lt;h2 id="gardener-dns-query-rewriting">Gardener DNS Query Rewriting&lt;/h2>
&lt;p>As explained &lt;a href="https://gardener.cloud/docs/gardener/dns-search-path-optimization/#dns-search-path-ndots-and-kubernetes">above&lt;/a>, the application of the DNS search path may lead to the undesired
creation of DNS requests. Especially with the default setting of &lt;code>ndots=5&lt;/code>, seemingly fully qualified names pointing to
services in the cluster may trigger the DNS search path application.&lt;/p>
&lt;p>Gardener allows to automatically rewrite some obviously incorrect DNS names, which stem from an application of the DNS search
path to the most likely desired name. This will automatically rewrite requests like &lt;code>service.namespace.svc.cluster.local.svc.cluster.local&lt;/code> to
&lt;code>service.namespace.svc.cluster.local&lt;/code>.&lt;/p>
&lt;p>In case the applications also target services for name resolution, which are outside of the cluster and have less than &lt;code>ndots&lt;/code> dots,
it might be helpful to prevent search path application for them as well. One way to achieve it is by adding them to the
&lt;code>commonSuffixes&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> systemComponents:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> coreDNS:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rewriting:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> commonSuffixes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - gardener.cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>DNS requests containing a common suffix and ending in &lt;code>.svc.cluster.local&lt;/code> are assumed to be incorrect application of the DNS
search path. Therefore, they are rewritten to everything ending in the common suffix. For example, &lt;code>www.gardener.cloud.svc.cluster.local&lt;/code>
would be rewritten to &lt;code>www.gardener.cloud&lt;/code>.&lt;/p>
&lt;p>Please note that the common suffixes should be long enough and include enough dots (&lt;code>.&lt;/code>) to prevent random overlap with
other DNS queries. For example, it would be a bad idea to simply put &lt;code>com&lt;/code> on the list of common suffixes, as there may be
services/namespaces which have &lt;code>com&lt;/code> as part of their name. The effect would be seemingly random DNS requests. Gardener
requires that common suffixes contain at least one dot (.) and adds a second dot at the beginning. For instance, a common
suffix of &lt;code>example.com&lt;/code> in the configuration would match &lt;code>*.example.com&lt;/code>.&lt;/p>
&lt;p>Since some clients verify the host in the response of a DNS query, the host must also be rewritten.
For that reason, we can&amp;rsquo;t rewrite a query for &lt;code>service.dst-namespace.svc.cluster.local.src-namespace.svc.cluster.local&lt;/code> or
&lt;code>www.example.com.src-namespace.svc.cluster.local&lt;/code>, as for an answer rewrite &lt;code>src-namespace&lt;/code> would not be known.&lt;/p></description></item><item><title>Docs: ETCD Encryption Config</title><link>https://gardener.cloud/docs/gardener/etcd_encryption_config/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/etcd_encryption_config/</guid><description>
&lt;h1 id="etcd-encryption-config">ETCD Encryption Config&lt;/h1>
&lt;p>The &lt;code>spec.kubernetes.kubeAPIServer.encryptionConfig&lt;/code> field in the Shoot API allows users to customize encryption configurations for the API server. It provides options to specify additional resources for encryption beyond secrets.&lt;/p>
&lt;h2 id="usage-guidelines">Usage Guidelines&lt;/h2>
&lt;ul>
&lt;li>The &lt;code>resources&lt;/code> field can be used to specify resources that should be encrypted in addition to secrets. Secrets are always encrypted.&lt;/li>
&lt;li>Each item is a Kubernetes resource name in plural (resource or resource.group). Wild cards are not supported.&lt;/li>
&lt;li>Adding an item to this list will cause patch requests for all the resources of that kind to encrypt them in the etcd. See &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data">Encrypting Confidential Data at Rest&lt;/a> for more details.&lt;/li>
&lt;li>Removing an item from this list will cause patch requests for all the resources of that type to decrypt and rewrite the resource as plain text. See &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/decrypt-data/">Decrypt Confidential Data that is Already Encrypted at Rest&lt;/a> for more details.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>ℹ️ Note that configuring encryption for a custom resource is only supported for Kubernetes versions &amp;gt;= 1.26.&lt;/p>
&lt;/blockquote>
&lt;h2 id="example-usage-in-a-shoot">Example Usage in a &lt;code>Shoot&lt;/code>&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeAPIServer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> encryptionConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - configmaps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - statefulsets.apps
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - customresource.fancyoperator.io
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: ExposureClasses</title><link>https://gardener.cloud/docs/gardener/exposureclasses/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/exposureclasses/</guid><description>
&lt;h1 id="exposureclasses">ExposureClasses&lt;/h1>
&lt;p>The Gardener API server provides a cluster-scoped &lt;code>ExposureClass&lt;/code> resource.
This resource is used to allow exposing the control plane of a Shoot cluster in various network environments like restricted corporate networks, DMZ, etc.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>The &lt;code>ExposureClass&lt;/code> resource is based on the concept for the &lt;code>RuntimeClass&lt;/code> resource in Kubernetes.&lt;/p>
&lt;p>A &lt;code>RuntimeClass&lt;/code> abstracts the installation of a certain container runtime (e.g., gVisor, Kata Containers) on all nodes or a subset of the nodes in a Kubernetes cluster.
See &lt;a href="https://kubernetes.io/docs/concepts/containers/runtime-class/">Runtime Class&lt;/a> for more information.&lt;/p>
&lt;p>In contrast, an &lt;code>ExposureClass&lt;/code> abstracts the ability to expose a Shoot clusters control plane in certain network environments (e.g., corporate networks, DMZ, internet) on all Seeds or a subset of the Seeds.&lt;/p>
&lt;p>Example: &lt;code>RuntimeClass&lt;/code> and &lt;code>ExposureClass&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: node.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: RuntimeClass
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: gvisor
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>handler: gvisorconfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># scheduling:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># nodeSelector:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># env: prod&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ExposureClass
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: internet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>handler: internet-config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># scheduling:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># seedSelector:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># matchLabels:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># network/env: internet&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Similar to &lt;code>RuntimeClasses&lt;/code>, &lt;code>ExposureClasses&lt;/code> also define a &lt;code>.handler&lt;/code> field reflecting the name reference for the corresponding CRI configuration of the &lt;code>RuntimeClass&lt;/code> and the control plane exposure configuration for the &lt;code>ExposureClass&lt;/code>.&lt;/p>
&lt;p>The CRI handler for &lt;code>RuntimeClasses&lt;/code> is usually installed by an administrator (e.g., via a &lt;code>DaemonSet&lt;/code> which installs the corresponding container runtime on the nodes).
The control plane exposure configuration for &lt;code>ExposureClasses&lt;/code> will be also provided by an administrator.
This exposure configuration is part of the gardenlet configuration, as this component is responsible to configure the control plane accordingly.
See the &lt;a href="https://gardener.cloud/docs/gardener/exposureclasses/#gardenlet-configuration-exposureclass-handlers">gardenlet Configuration &lt;code>ExposureClass&lt;/code> Handlers&lt;/a> section for more information.&lt;/p>
&lt;p>The &lt;code>RuntimeClass&lt;/code> also supports the selection of a node subset (which have the respective controller runtime binaries installed) for pod scheduling via its &lt;code>.scheduling&lt;/code> section.
The &lt;code>ExposureClass&lt;/code> also supports the selection of a subset of available Seed clusters whose gardenlet is capable of applying the exposure configuration for the Shoot control plane accordingly via its &lt;code>.scheduling&lt;/code> section.&lt;/p>
&lt;h2 id="usage-by-a-shoot">Usage by a &lt;code>Shoot&lt;/code>&lt;/h2>
&lt;p>A &lt;code>Shoot&lt;/code> can reference an &lt;code>ExposureClass&lt;/code> via the &lt;code>.spec.exposureClassName&lt;/code> field.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ When creating a &lt;code>Shoot&lt;/code> resource, the Gardener scheduler will try to assign the &lt;code>Shoot&lt;/code> to a &lt;code>Seed&lt;/code> which will host its control plane.&lt;/p>
&lt;/blockquote>
&lt;p>The scheduling behaviour can be influenced via the &lt;code>.spec.seedSelectors&lt;/code> and/or &lt;code>.spec.tolerations&lt;/code> fields in the &lt;code>Shoot&lt;/code>.
&lt;code>ExposureClass&lt;/code>es can also contain scheduling instructions.
If a &lt;code>Shoot&lt;/code> is referencing an &lt;code>ExposureClass&lt;/code>, then the scheduling instructions of both will be merged into the &lt;code>Shoot&lt;/code>.
Those unions of scheduling instructions might lead to a selection of a &lt;code>Seed&lt;/code> which is not able to deal with the &lt;code>handler&lt;/code> of the &lt;code>ExposureClass&lt;/code> and the &lt;code>Shoot&lt;/code> creation might end up in an error.
In such case, the &lt;code>Shoot&lt;/code> scheduling instructions should be revisited to check that they are not interfering with the ones from the &lt;code>ExposureClass&lt;/code>.
If this is not feasible, then the combination with the &lt;code>ExposureClass&lt;/code> might not be possible and you need to contact your Gardener administrator.&lt;/p>
&lt;details>
&lt;summary>Example: Shoot and ExposureClass scheduling instructions merge flow&lt;/summary>
&lt;ol>
&lt;li>Assuming there is the following &lt;code>Shoot&lt;/code> which is referencing the &lt;code>ExposureClass&lt;/code> below:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: abc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden-dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exposureClassName: abc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seedSelectors:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> env: prod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ExposureClass
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: abc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>handler: abc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>scheduling:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seedSelector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> network: internal
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="2">
&lt;li>Both &lt;code>seedSelectors&lt;/code> would be merged into the &lt;code>Shoot&lt;/code>. The result would be the following:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: abc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden-dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exposureClassName: abc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> seedSelectors:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> env: prod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> network: internal
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>Now the Gardener Scheduler would try to find a &lt;code>Seed&lt;/code> with those labels.&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>If there are &lt;strong>no&lt;/strong> Seeds with matching labels for the seed selector, then the &lt;code>Shoot&lt;/code> will be unschedulable.&lt;/li>
&lt;li>If there are Seeds with matching labels for the seed selector, then the Shoot will be assigned to the best candidate after the scheduling strategy is applied, see &lt;a href="https://gardener.cloud/docs/gardener/concepts/scheduler/#algorithm-overview">Gardener Scheduler&lt;/a>.
&lt;ul>
&lt;li>If the &lt;code>Seed&lt;/code> is &lt;strong>not&lt;/strong> able to serve the &lt;code>ExposureClass&lt;/code> handler &lt;code>abc&lt;/code>, then the Shoot will end up in error state.&lt;/li>
&lt;li>If the &lt;code>Seed&lt;/code> is able to serve the &lt;code>ExposureClass&lt;/code> handler &lt;code>abc&lt;/code>, then the &lt;code>Shoot&lt;/code> will be created.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/details>
&lt;h2 id="gardenlet-configuration-exposureclass-handlers">gardenlet Configuration &lt;code>ExposureClass&lt;/code> Handlers&lt;/h2>
&lt;p>The gardenlet is responsible to realize the control plane exposure strategy defined in the referenced &lt;code>ExposureClass&lt;/code> of a &lt;code>Shoot&lt;/code>.&lt;/p>
&lt;p>Therefore, the &lt;code>GardenletConfiguration&lt;/code> can contain an &lt;code>.exposureClassHandlers&lt;/code> list with the respective configuration.&lt;/p>
&lt;p>Example of the &lt;code>GardenletConfiguration&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>exposureClassHandlers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: internet-config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> loadBalancerService:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> loadbalancer/network: internet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: internal-config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> loadBalancerService:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> loadbalancer/network: internal
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> sni:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ingress:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: ingress-internal
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> network: internal
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Each gardenlet can define how the handler of a certain &lt;code>ExposureClass&lt;/code> needs to be implemented for the Seed(s) where it is responsible for.&lt;/p>
&lt;p>The &lt;code>.name&lt;/code> is the name of the handler config and it must match to the &lt;code>.handler&lt;/code> in the &lt;code>ExposureClass&lt;/code>.&lt;/p>
&lt;p>All control planes on a &lt;code>Seed&lt;/code> are exposed via a load balancer, either a dedicated one or a central shared one.
The load balancer service needs to be configured in a way that it is reachable from the target network environment.
Therefore, the configuration of load balancer service need to be specified, which can be done via the &lt;code>.loadBalancerService&lt;/code> section.
The common way to influence load balancer service behaviour is via annotations where the respective cloud-controller-manager will react on and configure the infrastructure load balancer accordingly.&lt;/p>
&lt;p>The control planes on a &lt;code>Seed&lt;/code> will be exposed via a central load balancer and with Envoy via TLS SNI passthrough proxy.
In this case, the gardenlet will install a dedicated ingress gateway (Envoy + load balancer + respective configuration) for each handler on the &lt;code>Seed&lt;/code>.
The configuration of the ingress gateways can be controlled via the &lt;code>.sni&lt;/code> section in the same way like for the default ingress gateways.&lt;/p></description></item><item><title>Docs: Getting Started Locally</title><link>https://gardener.cloud/docs/gardener/getting_started_locally/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/getting_started_locally/</guid><description>
&lt;h1 id="developing-gardener-locally">Developing Gardener Locally&lt;/h1>
&lt;p>&lt;a href="https://gardener.cloud/docs/gardener/deployment/getting_started_locally/">This document&lt;/a> explains how to setup a kind based environment for developing Gardener locally.&lt;/p>
&lt;p>For the best development experience you should especially check the &lt;a href="https://gardener.cloud/docs/gardener/deployment/getting_started_locally/#developing-gardener">Developing Gardener&lt;/a> section.&lt;/p>
&lt;p>In case you plan a debugging session please check the &lt;a href="https://gardener.cloud/docs/gardener/deployment/getting_started_locally/#debugging-gardener">Debugging Gardener&lt;/a> section.&lt;/p></description></item><item><title>Docs: High Availability</title><link>https://gardener.cloud/docs/gardener/high-availability/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/high-availability/</guid><description>
&lt;h1 id="high-availability-of-deployed-components">High Availability of Deployed Components&lt;/h1>
&lt;p>&lt;code>gardenlet&lt;/code>s and extension controllers are deploying components via &lt;code>Deployment&lt;/code>s, &lt;code>StatefulSet&lt;/code>s, etc., as part of the shoot control plane, or the seed or shoot system components.&lt;/p>
&lt;p>Some of the above component deployments must be further tuned to improve fault tolerance / resilience of the service. This document outlines what needs to be done to achieve this goal.&lt;/p>
&lt;p>Please be forwarded to the &lt;a href="https://gardener.cloud/docs/gardener/high-availability/#convenient-application-of-these-rules">Convenient Application Of These Rules&lt;/a> section, if you want to take a shortcut to the list of actions that require developers&amp;rsquo; attention.&lt;/p>
&lt;h2 id="seed-clusters">Seed Clusters&lt;/h2>
&lt;p>The worker nodes of seed clusters can be deployed to one or multiple availability zones.
The &lt;code>Seed&lt;/code> specification allows you to provide the information which zones are available:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> region: europe-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> zones:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - europe-1a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - europe-1b
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - europe-1c
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Independent of the number of zones, seed system components like the &lt;code>gardenlet&lt;/code> or the extension controllers themselves, or others like &lt;code>etcd-druid&lt;/code>, &lt;code>dependency-watchdog&lt;/code>, etc., should always be running with multiple replicas.&lt;/p>
&lt;p>Concretely, all seed system components should respect the following conventions:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Replica Counts&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Component Type&lt;/th>
&lt;th>&lt;code>&amp;lt; 3&lt;/code> Zones&lt;/th>
&lt;th>&lt;code>&amp;gt;= 3&lt;/code> Zones&lt;/th>
&lt;th>Comment&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Observability (Monitoring, Logging)&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>Downtimes accepted due to cost reasons&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Controllers&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>/&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>(Webhook) Servers&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>/&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Apart from the above, there might be special cases where these rules do not apply, for example:&lt;/p>
&lt;ul>
&lt;li>&lt;code>istio-ingressgateway&lt;/code> is scaled horizontally, hence the above numbers are the minimum values.&lt;/li>
&lt;li>&lt;code>nginx-ingress-controller&lt;/code> in the seed cluster is used to advertise all shoot observability endpoints, so due to performance reasons it runs with &lt;code>2&lt;/code> replicas at all times. In the future, this component might disappear in favor of the &lt;code>istio-ingressgateway&lt;/code> anyways.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Topology Spread Constraints&lt;/strong>&lt;/p>
&lt;p>When the component has &lt;code>&amp;gt;= 2&lt;/code> replicas &amp;hellip;&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&amp;hellip; then it should also have a &lt;code>topologySpreadConstraint&lt;/code>, ensuring the replicas are spread over the nodes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologySpreadConstraints:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - maxSkew: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologyKey: kubernetes.io/hostname
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> whenUnsatisfiable: ScheduleAnyway
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels: ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Hence, the node spread is done on best-effort basis only.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&amp;hellip; and the seed cluster has &lt;code>&amp;gt;= 2&lt;/code> zones, then the component should also have a second &lt;code>topologySpreadConstraint&lt;/code>, ensuring the replicas are spread over the zones:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologySpreadConstraints:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - maxSkew: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> minDomains: 2 &lt;span style="color:#008000"># lower value of max replicas or number of zones&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologyKey: topology.kubernetes.io/zone
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> whenUnsatisfiable: DoNotSchedule
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels: ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>According to these conventions, even seed clusters with only one availability zone try to be highly available &amp;ldquo;as good as possible&amp;rdquo; by spreading the replicas across multiple nodes.
Hence, while such seed clusters obviously cannot handle zone outages, they can at least handle node failures.&lt;/p>
&lt;/blockquote>
&lt;h2 id="shoot-clusters">Shoot Clusters&lt;/h2>
&lt;p>The &lt;code>Shoot&lt;/code> specification allows configuring &amp;ldquo;high availability&amp;rdquo; as well as the failure tolerance type for the control plane components, see &lt;a href="https://gardener.cloud/docs/gardener/shoot_high_availability/">Highly Available Shoot Control Plane&lt;/a> for details.&lt;/p>
&lt;p>Regarding the seed cluster selection, the only constraint is that shoot clusters with failure tolerance type &lt;code>zone&lt;/code> are only allowed to run on seed clusters with at least three zones.
All other shoot clusters (non-HA or those with failure tolerance type &lt;code>node&lt;/code>) can run on seed clusters with any number of zones.&lt;/p>
&lt;h3 id="control-plane-components">Control Plane Components&lt;/h3>
&lt;p>All control plane components should respect the following conventions:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Replica Counts&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Component Type&lt;/th>
&lt;th>w/o HA&lt;/th>
&lt;th>w/ HA (&lt;code>node&lt;/code>)&lt;/th>
&lt;th>w/ HA (&lt;code>zone&lt;/code>)&lt;/th>
&lt;th>Comment&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Observability (Monitoring, Logging)&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>1&lt;/td>
&lt;td>Downtimes accepted due to cost reasons&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Controllers&lt;/td>
&lt;td>1&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>/&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>(Webhook) Servers&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;td>/&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Apart from the above, there might be special cases where these rules do not apply, for example:&lt;/p>
&lt;ul>
&lt;li>&lt;code>etcd&lt;/code> is a server, though the most critical component of a cluster requiring a quorum to survive failures. Hence, it should have &lt;code>3&lt;/code> replicas even when the failure tolerance is &lt;code>node&lt;/code> only.&lt;/li>
&lt;li>&lt;code>kube-apiserver&lt;/code> is scaled horizontally, hence the above numbers are the minimum values (even when the shoot cluster is not HA, there might be multiple replicas).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Topology Spread Constraints&lt;/strong>&lt;/p>
&lt;p>When the component has &lt;code>&amp;gt;= 2&lt;/code> replicas &amp;hellip;&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&amp;hellip; then it should also have a &lt;code>topologySpreadConstraint&lt;/code> ensuring the replicas are spread over the nodes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologySpreadConstraints:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - maxSkew: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologyKey: kubernetes.io/hostname
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> whenUnsatisfiable: ScheduleAnyway
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels: ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Hence, the node spread is done on best-effort basis only.&lt;/p>
&lt;p>However, if the shoot cluster has defined a failure tolerance type, the &lt;code>whenUnsatisfiable&lt;/code> field should be set to &lt;code>DoNotSchedule&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&amp;hellip; and the failure tolerance type of the shoot cluster is &lt;code>zone&lt;/code>, then the component should also have a second &lt;code>topologySpreadConstraint&lt;/code> ensuring the replicas are spread over the zones:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologySpreadConstraints:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - maxSkew: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> minDomains: 2 &lt;span style="color:#008000"># lower value of max replicas or number of zones&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologyKey: topology.kubernetes.io/zone
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> whenUnsatisfiable: DoNotSchedule
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels: ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Node Affinity&lt;/strong>&lt;/p>
&lt;p>The &lt;code>gardenlet&lt;/code> annotates the shoot namespace in the seed cluster with the &lt;code>high-availability-config.resources.gardener.cloud/zones&lt;/code> annotation.&lt;/p>
&lt;ul>
&lt;li>If the shoot cluster is non-HA or has failure tolerance type &lt;code>node&lt;/code>, then the value will be always exactly one zone (e.g., &lt;code>high-availability-config.resources.gardener.cloud/zones=europe-1b&lt;/code>).&lt;/li>
&lt;li>If the shoot cluster has failure tolerance type &lt;code>zone&lt;/code>, then the value will always contain exactly three zones (e.g., &lt;code>high-availability-config.resources.gardener.cloud/zones=europe-1a,europe-1b,europe-1c&lt;/code>).&lt;/li>
&lt;/ul>
&lt;p>For backwards-compatibility, this annotation might contain multiple zones for shoot clusters created before &lt;code>gardener/gardener@v1.60&lt;/code> and not having failure tolerance type &lt;code>zone&lt;/code>.
This is because their volumes might already exist in multiple zones, hence pinning them to only one zone would not work.&lt;/p>
&lt;p>Hence, in case this annotation is present, the components should have the following node affinity:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> affinity:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeAffinity:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requiredDuringSchedulingIgnoredDuringExecution:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeSelectorTerms:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - matchExpressions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: topology.kubernetes.io/zone
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operator: In
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> values:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - europe-1a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - ...&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This is to ensure all pods are running in the same (set of) availability zone(s) such that cross-zone network traffic is avoided as much as possible (such traffic is typically charged by the underlying infrastructure provider).&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="system-components">System Components&lt;/h3>
&lt;p>The availability of system components is independent of the control plane since they run on the shoot worker nodes while the control plane components run on the seed worker nodes (for more information, see the &lt;a href="https://gardener.cloud/docs/gardener/concepts/architecture/">Kubernetes architecture overview&lt;/a>).
Hence, it only depends on the number of availability zones configured in the shoot worker pools via &lt;code>.spec.provider.workers[].zones&lt;/code>.
Concretely, the highest number of zones of a worker pool with &lt;code>systemComponents.allow=true&lt;/code> is considered.&lt;/p>
&lt;p>All system components should respect the following conventions:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Replica Counts&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Component Type&lt;/th>
&lt;th>&lt;code>1&lt;/code> or &lt;code>2&lt;/code> Zones&lt;/th>
&lt;th>&lt;code>&amp;gt;= 3&lt;/code> Zones&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Controllers&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>(Webhook) Servers&lt;/td>
&lt;td>2&lt;/td>
&lt;td>2&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Apart from the above, there might be special cases where these rules do not apply, for example:&lt;/p>
&lt;ul>
&lt;li>&lt;code>coredns&lt;/code> is scaled horizontally (today), hence the above numbers are the minimum values (possibly, scaling these components vertically may be more appropriate, but that&amp;rsquo;s unrelated to the HA subject matter).&lt;/li>
&lt;li>Optional addons like &lt;code>nginx-ingress&lt;/code> or &lt;code>kubernetes-dashboard&lt;/code> are only provided on best-effort basis for evaluation purposes, hence they run with &lt;code>1&lt;/code> replica at all times.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Topology Spread Constraints&lt;/strong>&lt;/p>
&lt;p>When the component has &lt;code>&amp;gt;= 2&lt;/code> replicas &amp;hellip;&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&amp;hellip; then it should also have a &lt;code>topologySpreadConstraint&lt;/code> ensuring the replicas are spread over the nodes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologySpreadConstraints:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - maxSkew: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologyKey: kubernetes.io/hostname
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> whenUnsatisfiable: ScheduleAnyway
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels: ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Hence, the node spread is done on best-effort basis only.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&amp;hellip; and the cluster has &lt;code>&amp;gt;= 2&lt;/code> zones, then the component should also have a second &lt;code>topologySpreadConstraint&lt;/code> ensuring the replicas are spread over the zones:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologySpreadConstraints:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - maxSkew: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> minDomains: 2 &lt;span style="color:#008000"># lower value of max replicas or number of zones&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologyKey: topology.kubernetes.io/zone
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> whenUnsatisfiable: DoNotSchedule
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels: ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="convenient-application-of-these-rules">Convenient Application of These Rules&lt;/h2>
&lt;p>According to above scenarios and conventions, the &lt;code>replicas&lt;/code>, &lt;code>topologySpreadConstraints&lt;/code> or &lt;code>affinity&lt;/code> settings of the deployed components might need to be adapted.&lt;/p>
&lt;p>In order to apply those conveniently and easily for developers, Gardener installs a mutating webhook into both seed and shoot clusters which reacts on &lt;code>Deployment&lt;/code>s and &lt;code>StatefulSet&lt;/code>s deployed to namespaces with the &lt;code>high-availability-config.resources.gardener.cloud/consider=true&lt;/code> label set.&lt;/p>
&lt;p>&lt;strong>The following actions have to be taken by developers:&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Check if &lt;code>components&lt;/code> are prepared to run concurrently with multiple replicas, e.g. controllers usually use &lt;a href="https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/leaderelection">leader election&lt;/a> to achieve this.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>All components should be generally equipped with &lt;code>PodDisruptionBudget&lt;/code>s with &lt;code>.spec.maxUnavailable=1&lt;/code> and &lt;code>unhealthyPodEvictionPolicy=AlwaysAllow&lt;/code>:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maxUnavailable: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> unhealthyPodEvictionPolicy: AlwaysAllow
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> selector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels: ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>Add the label &lt;code>high-availability-config.resources.gardener.cloud/type&lt;/code> to &lt;code>deployment&lt;/code>s or &lt;code>statefulset&lt;/code>s, as well as optionally involved &lt;code>horizontalpodautoscaler&lt;/code>s or &lt;code>HVPA&lt;/code>s where the following two values are possible:&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&lt;code>controller&lt;/code>&lt;/li>
&lt;li>&lt;code>server&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Type &lt;code>server&lt;/code> is also preferred if a component is a controller and (webhook) server at the same time.&lt;/p>
&lt;p>You can read more about the webhook&amp;rsquo;s internals in &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/#high-availability-config">High Availability Config&lt;/a>.&lt;/p>
&lt;h2 id="gardenlet-internals">&lt;code>gardenlet&lt;/code> Internals&lt;/h2>
&lt;p>Make sure you have read the above document about the webhook internals before continuing reading this section.&lt;/p>
&lt;h3 id="seed-controller">&lt;code>Seed&lt;/code> Controller&lt;/h3>
&lt;p>The &lt;code>gardenlet&lt;/code> performs the following changes on all namespaces running seed system components:&lt;/p>
&lt;ul>
&lt;li>adds the label &lt;code>high-availability-config.resources.gardener.cloud/consider=true&lt;/code>.&lt;/li>
&lt;li>adds the annotation &lt;code>high-availability-config.resources.gardener.cloud/zones=&amp;lt;zones&amp;gt;&lt;/code>, where &lt;code>&amp;lt;zones&amp;gt;&lt;/code> is the list provided in &lt;code>.spec.provider.zones[]&lt;/code> in the &lt;code>Seed&lt;/code> specification.&lt;/li>
&lt;/ul>
&lt;p>Note that neither the &lt;code>high-availability-config.resources.gardener.cloud/failure-tolerance-type&lt;/code>, nor the &lt;code>high-availability-config.resources.gardener.cloud/zone-pinning&lt;/code> annotations are set, hence the node affinity would never be touched by the webhook.&lt;/p>
&lt;p>The only exception to this rule are the istio ingress gateway namespaces. This includes the default istio ingress gateway when SNI is enabled, as well as analogous namespaces for exposure classes and zone-specific istio ingress gateways. Those namespaces
will additionally be annotated with &lt;code>high-availability-config.resources.gardener.cloud/zone-pinning&lt;/code> set to &lt;code>true&lt;/code>, resulting in the node affinities and the topology spread constraints being set. The replicas are not touched, as the istio ingress gateways
are scaled by a horizontal autoscaler instance.&lt;/p>
&lt;h3 id="shoot-controller">&lt;code>Shoot&lt;/code> Controller&lt;/h3>
&lt;h4 id="control-plane">Control Plane&lt;/h4>
&lt;p>The &lt;code>gardenlet&lt;/code> performs the following changes on the namespace running the shoot control plane components:&lt;/p>
&lt;ul>
&lt;li>adds the label &lt;code>high-availability-config.resources.gardener.cloud/consider=true&lt;/code>. This makes the webhook mutate the replica count and the topology spread constraints.&lt;/li>
&lt;li>adds the annotation &lt;code>high-availability-config.resources.gardener.cloud/failure-tolerance-type&lt;/code> with value equal to &lt;code>.spec.controlPlane.highAvailability.failureTolerance.type&lt;/code> (or &lt;code>&amp;quot;&amp;quot;&lt;/code>, if &lt;code>.spec.controlPlane.highAvailability=nil&lt;/code>). This makes the webhook mutate the node affinity according to the specified zone(s).&lt;/li>
&lt;li>adds the annotation &lt;code>high-availability-config.resources.gardener.cloud/zones=&amp;lt;zones&amp;gt;&lt;/code>, where &lt;code>&amp;lt;zones&amp;gt;&lt;/code> is a &amp;hellip;
&lt;ul>
&lt;li>&amp;hellip; random zone chosen from the &lt;code>.spec.provider.zones[]&lt;/code> list in the &lt;code>Seed&lt;/code> specification (always only one zone (even if there are multiple available in the seed cluster)) in case the &lt;code>Shoot&lt;/code> has no HA setting (i.e., &lt;code>spec.controlPlane.highAvailability=nil&lt;/code>) or when the &lt;code>Shoot&lt;/code> has HA setting with failure tolerance type &lt;code>node&lt;/code>.&lt;/li>
&lt;li>&amp;hellip; list of three randomly chosen zones from the &lt;code>.spec.provider.zones[]&lt;/code> list in the &lt;code>Seed&lt;/code> specification in case the &lt;code>Shoot&lt;/code> has HA setting with failure tolerance type &lt;code>zone&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="system-components-1">System Components&lt;/h4>
&lt;p>The &lt;code>gardenlet&lt;/code> performs the following changes on all namespaces running shoot system components:&lt;/p>
&lt;ul>
&lt;li>adds the label &lt;code>high-availability-config.resources.gardener.cloud/consider=true&lt;/code>. This makes the webhook mutate the replica count and the topology spread constraints.&lt;/li>
&lt;li>adds the annotation &lt;code>high-availability-config.resources.gardener.cloud/zones=&amp;lt;zones&amp;gt;&lt;/code> where &lt;code>&amp;lt;zones&amp;gt;&lt;/code> is the merged list of zones provided in &lt;code>.zones[]&lt;/code> with &lt;code>systemComponents.allow=true&lt;/code> for all worker pools in &lt;code>.spec.provider.workers[]&lt;/code> in the &lt;code>Shoot&lt;/code> specification.&lt;/li>
&lt;/ul>
&lt;p>Note that neither the &lt;code>high-availability-config.resources.gardener.cloud/failure-tolerance-type&lt;/code>, nor the &lt;code>high-availability-config.resources.gardener.cloud/zone-pinning&lt;/code> annotations are set, hence the node affinity would never be touched by the webhook.&lt;/p></description></item><item><title>Docs: Ipv6</title><link>https://gardener.cloud/docs/gardener/ipv6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/ipv6/</guid><description>
&lt;h1 id="ipv6-in-gardener-clusters">IPv6 in Gardener Clusters&lt;/h1>
&lt;blockquote>
&lt;p>🚧 IPv6 networking is currently under development.&lt;/p>
&lt;/blockquote>
&lt;h2 id="ipv6-single-stack-networking">IPv6 Single-Stack Networking&lt;/h2>
&lt;p>&lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/21-ipv6-singlestack-local.md">GEP-21&lt;/a> proposes IPv6 Single-Stack Support in the local Gardener environment.
This documentation will be enhanced while implementing GEP-21, see &lt;a href="https://github.com/gardener/gardener/issues/7051">gardener/gardener#7051&lt;/a>.&lt;/p>
&lt;p>To use IPv6 single-stack networking, the &lt;a href="https://gardener.cloud/docs/gardener/deployment/feature_gates/">feature gate&lt;/a> &lt;code>IPv6SingleStack&lt;/code> must be enabled on gardener-apiserver and gardenlet.&lt;/p>
&lt;h2 id="developmenttesting-setup">Development/Testing Setup&lt;/h2>
&lt;p>Developing or testing IPv6-related features requires a Linux machine (docker only supports IPv6 on Linux) and native IPv6 connectivity to the internet.
If you&amp;rsquo;re on a different OS or don&amp;rsquo;t have IPv6 connectivity in your office environment or via your home ISP, make sure to check out &lt;a href="https://github.com/gardener-community/dev-box-gcp">gardener-community/dev-box-gcp&lt;/a>, which allows you to circumvent these limitations.&lt;/p>
&lt;p>To get started with the IPv6 setup and create a local IPv6 single-stack shoot cluster, run the following commands:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make kind-up gardener-up IPFAMILY=ipv6
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>k apply -f example/provider-local/shoot-ipv6.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please also take a look at the guide on &lt;a href="https://gardener.cloud/docs/gardener/deployment/getting_started_locally/">Deploying Gardener Locally&lt;/a> for more details on setting up an IPv6 gardener for testing or development purposes.&lt;/p>
&lt;h2 id="container-images">Container Images&lt;/h2>
&lt;p>If you plan on using custom images, make sure your registry supports IPv6 access.&lt;/p>
&lt;p>Check the &lt;a href="https://gardener.cloud/docs/gardener/component-checklist/#images">component checklist&lt;/a> for tips concerning container registries and how to handle their IPv6 support.&lt;/p></description></item><item><title>Docs: Istio</title><link>https://gardener.cloud/docs/gardener/istio/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/istio/</guid><description>
&lt;h1 id="istio">Istio&lt;/h1>
&lt;p>&lt;a href="https://istio.io">Istio&lt;/a> offers a service mesh implementation with focus on several important features - traffic, observability, security, and policy.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;ul>
&lt;li>Third-party JWT is used, therefore each Seed cluster where this feature is enabled must have &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection">Service Account Token Volume Projection&lt;/a> enabled.&lt;/li>
&lt;li>Kubernetes 1.16+&lt;/li>
&lt;/ul>
&lt;h2 id="differences-with-istios-default-profile">Differences with Istio&amp;rsquo;s Default Profile&lt;/h2>
&lt;p>The &lt;a href="https://istio.io/docs/setup/additional-setup/config-profiles/">default profile&lt;/a> which is recommended for production deployment, is not suitable for the Gardener use case, as it offers more functionality than desired. The current installation goes through heavy refactorings due to the &lt;code>IstioOperator&lt;/code> and the mixture of Helm values + Kubernetes API specification makes configuring and fine-tuning it very hard. A more simplistic deployment is used by Gardener. The differences are the following:&lt;/p>
&lt;ul>
&lt;li>Telemetry is not deployed.&lt;/li>
&lt;li>&lt;code>istiod&lt;/code> is deployed.&lt;/li>
&lt;li>&lt;code>istio-ingress-gateway&lt;/code> is deployed in a separate &lt;code>istio-ingress&lt;/code> namespace.&lt;/li>
&lt;li>&lt;code>istio-egress-gateway&lt;/code> is not deployed.&lt;/li>
&lt;li>None of the Istio addons are deployed.&lt;/li>
&lt;li>Mixer (deprecated) is not deployed.&lt;/li>
&lt;li>Mixer CDRs are not deployed.&lt;/li>
&lt;li>Kubernetes &lt;code>Service&lt;/code>, Istio&amp;rsquo;s &lt;code>VirtualService&lt;/code> and &lt;code>ServiceEntry&lt;/code> are &lt;strong>NOT&lt;/strong> advertised in the service mesh. This means that if a &lt;code>Service&lt;/code> needs to be accessed directly from the Istio Ingress Gateway, it should have &lt;code>networking.istio.io/exportTo: &amp;quot;*&amp;quot;&lt;/code> annotation. &lt;code>VirtualService&lt;/code> and &lt;code>ServiceEntry&lt;/code> must have &lt;code>.spec.exportTo: [&amp;quot;*&amp;quot;]&lt;/code> set on them respectively.&lt;/li>
&lt;li>Istio injector is not enabled.&lt;/li>
&lt;li>mTLS is enabled by default.&lt;/li>
&lt;/ul>
&lt;h2 id="handling-multiple-availability-zones-with-istio">Handling Multiple Availability Zones with Istio&lt;/h2>
&lt;p>For various reasons, e.g., improved resiliency to certain failures, it may be beneficial to use multiple availability zones in a seed cluster. While availability zones have advantages in being able to cover some failure domains, they also come with some additional challenges. Most notably, the latency across availability zone boundaries is higher than within an availability zone. Furthermore, there might be additional cost implied by network traffic crossing an availability zone boundary. Therefore, it may be useful to try to keep traffic within an availability zone if possible. The istio deployment as part of Gardener has been adapted to allow this.&lt;/p>
&lt;p>A seed cluster spanning multiple availability zones may be used for &lt;a href="https://gardener.cloud/docs/gardener/shoot_high_availability/">highly-available shoot control planes&lt;/a>. Those control planes may use a single or multiple availability zones. In addition to that, ordinary non-highly-available shoot control planes may be scheduled to such a seed cluster as well. The result is that the seed cluster may have control planes spanning multiple availability zones and control planes that are pinned to exactly one availability zone. These two types need to be handled differently when trying to prevent unnecessary cross-zonal traffic.&lt;/p>
&lt;p>The goal is achieved by using multiple istio ingress gateways. The default istio ingress gateway spans all availability zones. It is used for multi-zonal shoot control planes. For each availability zone, there is an additional istio ingress gateway, which is utilized only for single-zone shoot control planes pinned to this availability zone. This is illustrated in the following diagram.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/multi-zonal-istio_8c8fa9.png" alt="Multi Availability Zone Handling in Istio">&lt;/p>
&lt;p>Please note that operators may need to perform additional tuning to prevent cross-zonal traffic completely. The &lt;a href="https://gardener.cloud/docs/gardener/seed_settings/#load-balancer-services">loadbalancer settings in the seed specification&lt;/a> offer various options, e.g., by setting the external traffic policy to &lt;code>local&lt;/code> or using infrastructure specific loadbalancer annotations.&lt;/p>
&lt;p>Furthermore, note that this approach is also taken in case &lt;a href="https://gardener.cloud/docs/gardener/exposureclasses/">&lt;code>ExposureClass&lt;/code>es&lt;/a> are used. For each exposure class, additional zonal istio ingress gateways may be deployed to cover for single-zone shoot control planes using the exposure class.&lt;/p></description></item><item><title>Docs: Kubernetes Clients</title><link>https://gardener.cloud/docs/gardener/kubernetes-clients/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/kubernetes-clients/</guid><description>
&lt;h1 id="kubernetes-clients-in-gardener">Kubernetes Clients in Gardener&lt;/h1>
&lt;p>This document aims at providing a general developer guideline on different aspects of using Kubernetes clients in a large-scale distributed system and project like Gardener.
The points included here are not meant to be consulted as absolute rules, but rather as general rules of thumb that allow developers to get a better feeling about certain gotchas and caveats.
It should be updated with lessons learned from maintaining the project and running Gardener in production.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites:&lt;/h2>
&lt;p>Please familiarize yourself with the following basic Kubernetes API concepts first, if you&amp;rsquo;re new to Kubernetes. A good understanding of these basics will help you better comprehend the following document.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/using-api/api-concepts/">Kubernetes API Concepts&lt;/a> (including terminology, watch basics, etc.)&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/">Extending the Kubernetes API&lt;/a> (including Custom Resources and aggregation layer / extension API servers)&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">Extend the Kubernetes API with CustomResourceDefinitions&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/">Working with Kubernetes Objects&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/sample-controller/blob/master/docs/controller-client-go.md">Sample Controller&lt;/a> (the diagram helps to build an understanding of an controller&amp;rsquo;s basic structure)&lt;/li>
&lt;/ul>
&lt;h2 id="client-types-client-go-generated-controller-runtime">Client Types: Client-Go, Generated, Controller-Runtime&lt;/h2>
&lt;p>For historical reasons, you will find different kinds of Kubernetes clients in Gardener:&lt;/p>
&lt;h3 id="client-go-clients">Client-Go Clients&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/client-go">client-go&lt;/a> is the default/official client for talking to the Kubernetes API in Golang.
It features the so called &lt;a href="https://github.com/kubernetes/client-go/blob/release-1.21/kubernetes/clientset.go#L72">&amp;ldquo;client sets&amp;rdquo;&lt;/a> for all built-in Kubernetes API groups and versions (e.g. &lt;code>v1&lt;/code> (aka &lt;code>core/v1&lt;/code>), &lt;code>apps/v1&lt;/code>).
client-go clients are generated from the built-in API types using &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/generating-clientset.md">client-gen&lt;/a> and are composed of interfaces for every known API GroupVersionKind.
A typical client-go usage looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx context.Context
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c kubernetes.Interface &lt;span style="color:#008000">// &amp;#34;k8s.io/client-go/kubernetes&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> deployment *appsv1.Deployment &lt;span style="color:#008000">// &amp;#34;k8s.io/api/apps/v1&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>updatedDeployment, err := c.AppsV1().Deployments(&lt;span style="color:#a31515">&amp;#34;default&amp;#34;&lt;/span>).Update(ctx, deployment, metav1.UpdateOptions{})
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;em>Important characteristics of client-go clients:&lt;/em>&lt;/p>
&lt;ul>
&lt;li>clients are specific to a given API GroupVersionKind, i.e., clients are hard-coded to corresponding API-paths (don&amp;rsquo;t need to use the discovery API to map GVK to a REST endpoint path).&lt;/li>
&lt;li>client&amp;rsquo;s don&amp;rsquo;t modify the passed in-memory object (e.g. &lt;code>deployment&lt;/code> in the above example). Instead, they return a new in-memory object.
This means that controllers have to continue working with the new in-memory object or overwrite the shared object to not lose any state updates.&lt;/li>
&lt;/ul>
&lt;h3 id="generated-client-sets-for-gardener-apis">Generated Client Sets for Gardener APIs&lt;/h3>
&lt;p>Gardener&amp;rsquo;s APIs extend the Kubernetes API by registering an extension API server (in the garden cluster) and &lt;code>CustomResourceDefinition&lt;/code>s (on Seed clusters), meaning that the Kubernetes API will expose additional REST endpoints to manage Gardener resources in addition to the built-in API resources.
In order to talk to these extended APIs in our controllers and components, client-gen is used to generate client-go-style clients to &lt;a href="https://github.com/gardener/gardener/tree/master/pkg/client">&lt;code>pkg/client/{core,extensions,seedmanagement,...}&lt;/code>&lt;/a>.&lt;/p>
&lt;p>Usage of these clients is equivalent to &lt;code>client-go&lt;/code> clients, and the same characteristics apply. For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx context.Context
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c gardencoreclientset.Interface &lt;span style="color:#008000">// &amp;#34;github.com/gardener/gardener/pkg/client/core/clientset/versioned&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> shoot *gardencorev1beta1.Shoot &lt;span style="color:#008000">// &amp;#34;github.com/gardener/gardener/pkg/apis/core/v1beta1&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>updatedShoot, err := c.CoreV1beta1().Shoots(&lt;span style="color:#a31515">&amp;#34;garden-my-project&amp;#34;&lt;/span>).Update(ctx, shoot, metav1.UpdateOptions{})
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="controller-runtime-clients">Controller-Runtime Clients&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes-sigs/controller-runtime">controller-runtime&lt;/a> is a Kubernetes community project (&lt;a href="https://github.com/kubernetes-sigs/kubebuilder">kubebuilder&lt;/a> subproject) for building controllers and operators for custom resources.
Therefore, it features a generic client that follows a different approach and does not rely on generated client sets. Instead, the client can be used for managing any Kubernetes resources (built-in or custom) homogeneously.
For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx context.Context
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c client.Client &lt;span style="color:#008000">// &amp;#34;sigs.k8s.io/controller-runtime/pkg/client&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> deployment *appsv1.Deployment &lt;span style="color:#008000">// &amp;#34;k8s.io/api/apps/v1&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> shoot *gardencorev1beta1.Shoot &lt;span style="color:#008000">// &amp;#34;github.com/gardener/gardener/pkg/apis/core/v1beta1&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>err := c.Update(ctx, deployment)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// or
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>err = c.Update(ctx, shoot)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A brief introduction to controller-runtime and its basic constructs can be found at the &lt;a href="https://pkg.go.dev/sigs.k8s.io/controller-runtime">official Go documentation&lt;/a>.&lt;/p>
&lt;p>&lt;em>Important characteristics of controller-runtime clients:&lt;/em>&lt;/p>
&lt;ul>
&lt;li>The client functions take a generic &lt;code>client.Object&lt;/code> or &lt;code>client.ObjectList&lt;/code> value. These interfaces are implemented by all Golang types, that represent Kubernetes API objects or lists respectively which can be interacted with via usual API requests. [1]&lt;/li>
&lt;li>The client first consults a &lt;code>runtime.Scheme&lt;/code> (configured during client creation) for recognizing the object&amp;rsquo;s &lt;code>GroupVersionKind&lt;/code> (this happens on the client-side only).
A &lt;code>runtime.Scheme&lt;/code> is basically a registry for Golang API types, defaulting and conversion functions. Schemes are usually provided per &lt;code>GroupVersion&lt;/code> (see &lt;a href="https://github.com/kubernetes/api/blob/release-1.21/apps/v1/register.go">this example&lt;/a> for &lt;code>apps/v1&lt;/code>) and can be combined to one single scheme for further usage (&lt;a href="https://github.com/gardener/gardener/blob/v1.29.0/pkg/client/kubernetes/types.go#L96">example&lt;/a>). In controller-runtime clients, schemes are used only for mapping a typed API object to its &lt;code>GroupVersionKind&lt;/code>.&lt;/li>
&lt;li>It then consults a &lt;code>meta.RESTMapper&lt;/code> (also configured during client creation) for mapping the &lt;code>GroupVersionKind&lt;/code> to a &lt;code>RESTMapping&lt;/code>, which contains the &lt;code>GroupVersionResource&lt;/code> and &lt;code>Scope&lt;/code> (namespaced or cluster-scoped). From these values, the client can unambiguously determine the REST endpoint path of the corresponding API resource. For instance: &lt;code>appsv1.DeploymentList&lt;/code> is available at &lt;code>/apis/apps/v1/deployments&lt;/code> or &lt;code>/apis/apps/v1/namespaces/&amp;lt;namespace&amp;gt;/deployments&lt;/code> respectively.
&lt;ul>
&lt;li>There are different &lt;code>RESTMapper&lt;/code> implementations, but generally they are talking to the API server&amp;rsquo;s discovery API for retrieving &lt;code>RESTMappings&lt;/code> for all API resources known to the API server (either built-in, registered via API extension or &lt;code>CustomResourceDefinition&lt;/code>s).&lt;/li>
&lt;li>The default implementation of a controller-runtime (which Gardener uses as well) is the &lt;a href="https://github.com/kubernetes-sigs/controller-runtime/blob/v0.9.0/pkg/client/apiutil/dynamicrestmapper.go#L77">dynamic &lt;code>RESTMapper&lt;/code>&lt;/a>. It caches discovery results (i.e. &lt;code>RESTMappings&lt;/code>) in-memory and only re-discovers resources from the API server when a client tries to use an unknown &lt;code>GroupVersionKind&lt;/code>, i.e., when it encounters a &lt;code>No{Kind,Resource}MatchError&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The client writes back results from the API server into the passed in-memory object.
&lt;ul>
&lt;li>This means that controllers don&amp;rsquo;t have to worry about copying back the results and should just continue to work on the given in-memory object.&lt;/li>
&lt;li>This is a nice and flexible pattern, and helper functions should try to follow it wherever applicable. Meaning, if possible accept an object param, pass it down to clients and keep working on the same in-memory object instead of creating a new one in your helper function.&lt;/li>
&lt;li>The benefit is that you don&amp;rsquo;t lose updates to the API object and always have the last-known state in memory. Therefore, you don&amp;rsquo;t have to read it again, e.g., for getting the current &lt;code>resourceVersion&lt;/code> when working with &lt;a href="https://gardener.cloud/docs/gardener/kubernetes-clients/#conflicts-concurrency-control-and-optimistic-locking">optimistic locking&lt;/a>, and thus minimize the chances for running into conflicts.&lt;/li>
&lt;li>However, controllers &lt;em>must not&lt;/em> use the same in-memory object concurrently in multiple goroutines. For example, decoding results from the API server in multiple goroutines into the same maps (e.g., labels, annotations) will cause panics because of &amp;ldquo;concurrent map writes&amp;rdquo;. Also, reading from an in-memory API object in one goroutine while decoding into it in another goroutine will yield non-atomic reads, meaning data might be corrupt and represent a non-valid/non-existing API object.&lt;/li>
&lt;li>Therefore, if you need to use the same in-memory object in multiple goroutines concurrently (e.g., shared state), remember to leverage proper synchronization techniques like channels, mutexes, &lt;code>atomic.Value&lt;/code> and/or copy the object prior to use. The average controller however, will not need to share in-memory API objects between goroutines, and it&amp;rsquo;s typically an indicator that the controller&amp;rsquo;s design should be improved.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The client decoder erases the object&amp;rsquo;s &lt;code>TypeMeta&lt;/code> (&lt;code>apiVersion&lt;/code> and &lt;code>kind&lt;/code> fields) after retrieval from the API server, see &lt;a href="https://github.com/kubernetes/kubernetes/issues/80609">kubernetes/kubernetes#80609&lt;/a>, &lt;a href="https://github.com/kubernetes-sigs/controller-runtime/issues/1517">kubernetes-sigs/controller-runtime#1517&lt;/a>.
Unstructured and metadata-only requests objects are an exception to this because the contained &lt;code>TypeMeta&lt;/code> is the only way to identify the object&amp;rsquo;s type.
Because of this behavior, &lt;code>obj.GetObjectKind().GroupVersionKind()&lt;/code> is likely to return an empty &lt;code>GroupVersionKind&lt;/code>.
I.e., you must not rely on &lt;code>TypeMeta&lt;/code> being set or &lt;code>GetObjectKind()&lt;/code> to return something usable.
If you need to identify an object&amp;rsquo;s &lt;code>GroupVersionKind&lt;/code>, use a scheme and its &lt;code>ObjectKinds&lt;/code> function instead (or the helper function &lt;code>apiutil.GVKForObject&lt;/code>).
This is not specific to controller-runtime clients and applies to client-go clients as well.&lt;/li>
&lt;/ul>
&lt;p>[1] Other lower level, config or internal API types (e.g., such as &lt;a href="https://github.com/kubernetes/api/blob/release-1.21/admission/v1/types.go#L29">&lt;code>AdmissionReview&lt;/code>&lt;/a>) don&amp;rsquo;t implement &lt;code>client.Object&lt;/code>. However, you also can&amp;rsquo;t interact with such objects via the Kubernetes API and thus also not via a client, so this can be disregarded at this point.&lt;/p>
&lt;h3 id="metadata-only-clients">Metadata-Only Clients&lt;/h3>
&lt;p>Additionally, controller-runtime clients can be used to easily retrieve metadata-only objects or lists.
This is useful for efficiently checking if at least one object of a given kind exists, or retrieving metadata of an object, if one is not interested in the rest (e.g., spec/status).
The &lt;code>Accept&lt;/code> header sent to the API server then contains &lt;code>application/json;as=PartialObjectMetadataList;g=meta.k8s.io;v=v1&lt;/code>, which makes the API server only return metadata of the retrieved object(s).
This saves network traffic and CPU/memory load on the API server and client side.
If the client fully lists all objects of a given kind including their spec/status, the resulting list can be quite large and easily exceed the controllers available memory.
That&amp;rsquo;s why it&amp;rsquo;s important to carefully check if a full list is actually needed, or if metadata-only list can be used instead.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx context.Context
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c client.Client &lt;span style="color:#008000">// &amp;#34;sigs.k8s.io/controller-runtime/pkg/client&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> shootList = &amp;amp;metav1.PartialObjectMetadataList{} &lt;span style="color:#008000">// &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>shootList.SetGroupVersionKind(gardencorev1beta1.SchemeGroupVersion.WithKind(&lt;span style="color:#a31515">&amp;#34;ShootList&amp;#34;&lt;/span>))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">if&lt;/span> err := c.List(ctx, shootList, client.InNamespace(&lt;span style="color:#a31515">&amp;#34;garden-my-project&amp;#34;&lt;/span>), client.Limit(1)); err != &lt;span style="color:#00f">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00f">return&lt;/span> err
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">if&lt;/span> len(shootList.Items) &amp;gt; 0 {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">// project has at least one shoot
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>} &lt;span style="color:#00f">else&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">// project doesn&amp;#39;t have any shoots
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="gardeners-client-collection-clientmaps">Gardener&amp;rsquo;s Client Collection, ClientMaps&lt;/h3>
&lt;p>The Gardener codebase has a collection of clients (&lt;a href="https://github.com/gardener/gardener/blob/v1.29.0/pkg/client/kubernetes/types.go#L149">&lt;code>kubernetes.Interface&lt;/code>&lt;/a>), which can return all the above mentioned client types.
Additionally, it contains helpers for rendering and applying helm charts (&lt;code>ChartRender&lt;/code>, &lt;code>ChartApplier&lt;/code>) and retrieving the API server&amp;rsquo;s version (&lt;code>Version&lt;/code>).
Client sets are managed by so called &lt;code>ClientMap&lt;/code>s, which are a form of registry for all client set for a given type of cluster, i.e., Garden, Seed and Shoot.
ClientMaps manage the whole lifecycle of clients: they take care of creating them if they don&amp;rsquo;t exist already, running their caches, refreshing their cached server version and invalidating them when they are no longer needed.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx context.Context
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cm clientmap.ClientMap &lt;span style="color:#008000">// &amp;#34;github.com/gardener/gardener/pkg/client/kubernetes/clientmap&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> shoot *gardencorev1beta1.Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cs, err := cm.GetClient(ctx, keys.ForShoot(shoot)) &lt;span style="color:#008000">// kubernetes.Interface
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>&lt;span style="color:#00f">if&lt;/span> err != &lt;span style="color:#00f">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00f">return&lt;/span> err
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>c := cs.Client() &lt;span style="color:#008000">// client.Client
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The client collection mainly exist for historical reasons (there used to be a lot of code using the client-go style clients).
However, Gardener is in the process of moving more towards controller-runtime and only using their clients, as they provide many benefits and are much easier to use.
Also, &lt;a href="https://github.com/gardener/gardener/issues/4251">gardener/gardener#4251&lt;/a> aims at refactoring our controller and admission components to native controller-runtime components.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ Please always prefer controller-runtime clients over other clients when writing new code or refactoring existing code.&lt;/p>
&lt;/blockquote>
&lt;h2 id="cache-types-informers-listers-controller-runtime-caches">Cache Types: Informers, Listers, Controller-Runtime Caches&lt;/h2>
&lt;p>Similar to the different types of client(set)s, there are also different kinds of Kubernetes client caches.
However, all of them are based on the same concept: &lt;code>Informer&lt;/code>s.
An &lt;code>Informer&lt;/code> is a watch-based cache implementation, meaning it opens &lt;a href="https://kubernetes.io/docs/reference/using-api/api-concepts/#efficient-detection-of-changes">watch connections&lt;/a> to the API server and continuously updates cached objects based on the received watch events (&lt;code>ADDED&lt;/code>, &lt;code>MODIFIED&lt;/code>, &lt;code>DELETED&lt;/code>).
&lt;code>Informer&lt;/code>s offer to add indices to the cache for efficient object lookup (e.g., by name or labels) and to add &lt;code>EventHandler&lt;/code>s for the watch events.
The latter is used by controllers to fill queues with objects that should be reconciled on watch events.&lt;/p>
&lt;p>Informers are used in and created via several higher-level constructs:&lt;/p>
&lt;h3 id="sharedinformerfactories-listers">SharedInformerFactories, Listers&lt;/h3>
&lt;p>The generated clients (built-in as well as extended) feature a &lt;code>SharedInformerFactory&lt;/code> for every API group, which can be used to create and retrieve &lt;code>Informers&lt;/code> for all GroupVersionKinds.
Similarly, it can be used to retrieve &lt;code>Listers&lt;/code> that allow getting and listing objects from the &lt;code>Informer&lt;/code>&amp;rsquo;s cache.
However, both of these constructs are only used for historical reasons, and we are in the process of migrating away from them in favor of cached controller-runtime clients (see &lt;a href="https://github.com/gardener/gardener/issues/2414">gardener/gardener#2414&lt;/a>, &lt;a href="https://github.com/gardener/gardener/issues/2822">gardener/gardener#2822&lt;/a>). Thus, they are described only briefly here.&lt;/p>
&lt;p>&lt;em>Important characteristics of Listers:&lt;/em>&lt;/p>
&lt;ul>
&lt;li>Objects read from Informers and Listers can always be slightly out-out-date (i.e., stale) because the client has to first observe changes to API objects via watch events (which can intermittently lag behind by a second or even more).&lt;/li>
&lt;li>Thus, don&amp;rsquo;t make any decisions based on data read from Listers if the consequences of deciding wrongfully based on stale state might be catastrophic (e.g. leaking infrastructure resources). In such cases, read directly from the API server via a client instead.&lt;/li>
&lt;li>Objects retrieved from Informers or Listers are pointers to the cached objects, so they must not be modified without copying them first, otherwise the objects in the cache are also modified.&lt;/li>
&lt;/ul>
&lt;h3 id="controller-runtime-caches">Controller-Runtime Caches&lt;/h3>
&lt;p>controller-runtime features a cache implementation that can be used equivalently as their clients. In fact, it implements a subset of the &lt;code>client.Client&lt;/code> interface containing the &lt;code>Get&lt;/code> and &lt;code>List&lt;/code> functions.
Under the hood, a &lt;code>cache.Cache&lt;/code> dynamically creates &lt;code>Informers&lt;/code> (i.e., opens watches) for every object GroupVersionKind that is being retrieved from it.&lt;/p>
&lt;p>Note that the underlying Informers of a controller-runtime cache (&lt;code>cache.Cache&lt;/code>) and the ones of a &lt;code>SharedInformerFactory&lt;/code> (client-go) are not related in any way.
Both create &lt;code>Informers&lt;/code> and watch objects on the API server individually.
This means that if you read the same object from different cache implementations, you may receive different versions of the object because the watch connections of the individual Informers are not synced.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ Because of this, controllers/reconcilers should get the object from the same cache in the reconcile loop, where the &lt;code>EventHandler&lt;/code> was also added to set up the controller. For example, if a &lt;code>SharedInformerFactory&lt;/code> is used for setting up the controller then read the object in the reconciler from the &lt;code>Lister&lt;/code> instead of from a cached controller-runtime client.&lt;/p>
&lt;/blockquote>
&lt;p>By default, the &lt;code>client.Client&lt;/code> created by a controller-runtime &lt;code>Manager&lt;/code> is a &lt;code>DelegatingClient&lt;/code>. It delegates &lt;code>Get&lt;/code> and &lt;code>List&lt;/code> calls to a &lt;code>Cache&lt;/code>, and all other calls to a client that talks directly to the API server. Exceptions are requests with &lt;code>*unstructured.Unstructured&lt;/code> objects and object kinds that were configured to be excluded from the cache in the &lt;code>DelegatingClient&lt;/code>.&lt;/p>
&lt;blockquote>
&lt;p>ℹ️
&lt;code>kubernetes.Interface.Client()&lt;/code> returns a &lt;code>DelegatingClient&lt;/code> that uses the cache returned from &lt;code>kubernetes.Interface.Cache()&lt;/code> under the hood. This means that all &lt;code>Client()&lt;/code> usages need to be ready for cached clients and should be able to cater with stale cache reads.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;em>Important characteristics of cached controller-runtime clients:&lt;/em>&lt;/p>
&lt;ul>
&lt;li>Like for Listers, objects read from a controller-runtime cache can always be slightly out of date. Hence, don&amp;rsquo;t base any important decisions on data read from the cache (see above).&lt;/li>
&lt;li>In contrast to Listers, controller-runtime caches fill the passed in-memory object with the state of the object in the cache (i.e., they perform something like a &amp;ldquo;deep copy into&amp;rdquo;). This means that objects read from a controller-runtime cache can safely be modified without unintended side effects.&lt;/li>
&lt;li>Reading from a controller-runtime cache or a cached controller-runtime client implicitly starts a watch for the given object kind under the hood. This has important consequences:
&lt;ul>
&lt;li>Reading a given object kind from the cache for the first time can take up to a few seconds depending on size and amount of objects as well as API server latency. This is because the cache has to do a full list operation and wait for an initial watch sync before returning results.&lt;/li>
&lt;li>⚠️ Controllers need appropriate RBAC permissions for the object kinds they retrieve via cached clients (i.e., &lt;code>list&lt;/code> and &lt;code>watch&lt;/code>).&lt;/li>
&lt;li>⚠️ By default, watches started by a controller-runtime cache are cluster-scoped, meaning it watches and caches objects across all namespaces. Thus, be careful which objects to read from the cache as it might significantly increase the controller&amp;rsquo;s memory footprint.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>There is no interaction with the cache on writing calls (&lt;code>Create&lt;/code>, &lt;code>Update&lt;/code>, &lt;code>Patch&lt;/code> and &lt;code>Delete&lt;/code>), see below.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Uncached objects, filtered caches, &lt;code>APIReader&lt;/code>s:&lt;/strong>&lt;/p>
&lt;p>In order to allow more granular control over which object kinds should be cached and which calls should bypass the cache, controller-runtime offers a few mechanisms to further tweak the client/cache behavior:&lt;/p>
&lt;ul>
&lt;li>When creating a &lt;code>DelegatingClient&lt;/code>, certain object kinds can be configured to always be read directly from the API instead of from the cache. Note that this does not prevent starting a new Informer when retrieving them directly from the cache.&lt;/li>
&lt;li>Watches can be restricted to a given (set of) namespace(s) by setting &lt;code>cache.Options.Namespaces&lt;/code>.&lt;/li>
&lt;li>Watches can be filtered (e.g., by label) per object kind by configuring &lt;code>cache.Options.SelectorsByObject&lt;/code> on creation of the cache.&lt;/li>
&lt;li>Retrieving metadata-only objects or lists from a cache results in a metadata-only watch/cache for that object kind.&lt;/li>
&lt;li>The &lt;code>APIReader&lt;/code> can be used to always talk directly to the API server for a given &lt;code>Get&lt;/code> or &lt;code>List&lt;/code> call (use with care and only as a last resort!).&lt;/li>
&lt;/ul>
&lt;h3 id="to-cache-or-not-to-cache">To Cache or Not to Cache&lt;/h3>
&lt;p>Although watch-based caches are an important factor for the immense scalability of Kubernetes, it definitely comes at a price (mainly in terms of memory consumption).
Thus, developers need to be careful when introducing new API calls and caching new object kinds.
Here are some general guidelines on choosing whether to read from a cache or not:&lt;/p>
&lt;ul>
&lt;li>Always try to use the cache wherever possible and make your controller able to tolerate stale reads.
&lt;ul>
&lt;li>Leverage optimistic locking: use deterministic naming for objects you create (this is what the &lt;code>Deployment&lt;/code> controller does [2]).&lt;/li>
&lt;li>Leverage optimistic locking / concurrency control of the API server: send updates/patches with the last-known &lt;code>resourceVersion&lt;/code> from the cache (see below). This will make the request fail, if there were concurrent updates to the object (conflict error), which indicates that we have operated on stale data and might have made wrong decisions. In this case, let the controller handle the error with exponential backoff. This will make the controller eventually consistent.&lt;/li>
&lt;li>Track the actions you took, e.g., when creating objects with &lt;code>generateName&lt;/code> (this is what the &lt;code>ReplicaSet&lt;/code> controller does [3]). The actions can be tracked in memory and repeated if the expected watch events don&amp;rsquo;t occur after a given amount of time.&lt;/li>
&lt;li>Always try to write controllers with the assumption that data will only be eventually correct and can be slightly out of date (even if read directly from the API server!).&lt;/li>
&lt;li>If there is already some other code that needs a cache (e.g., a controller watch), reuse it instead of doing extra direct reads.&lt;/li>
&lt;li>Don&amp;rsquo;t read an object again if you just sent a write request. Write requests (&lt;code>Create&lt;/code>, &lt;code>Update&lt;/code>, &lt;code>Patch&lt;/code> and &lt;code>Delete&lt;/code>) don&amp;rsquo;t interact with the cache. Hence, use the current state that the API server returned (filled into the passed in-memory object), which is basically a &amp;ldquo;free direct read&amp;rdquo; instead of reading the object again from a cache, because this will probably set back the object to an older &lt;code>resourceVersion&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If you are concerned about the impact of the resulting cache, try to minimize that by using filtered or metadata-only watches.&lt;/li>
&lt;li>If watching and caching an object type is not feasible, for example because there will be a lot of updates, and you are only interested in the object every ~5m, or because it will blow up the controllers memory footprint, fallback to a direct read. This can either be done by disabling caching the object type generally or doing a single request via an &lt;code>APIReader&lt;/code>. In any case, please bear in mind that every direct API call results in a &lt;a href="https://kubernetes.io/docs/reference/using-api/api-concepts/#the-resourceversion-parameter">quorum read from etcd&lt;/a>, which can be costly in a heavily-utilized cluster and impose significant scalability limits. Thus, always try to minimize the impact of direct calls by filtering results by namespace or labels, limiting the number of results and/or using metadata-only calls.&lt;/li>
&lt;/ul>
&lt;p>[2] The &lt;code>Deployment&lt;/code> controller uses the pattern &lt;code>&amp;lt;deployment-name&amp;gt;-&amp;lt;podtemplate-hash&amp;gt;&lt;/code> for naming &lt;code>ReplicaSets&lt;/code>. This means, the name of a &lt;code>ReplicaSet&lt;/code> it tries to create/update/delete at any given time is deterministically calculated based on the &lt;code>Deployment&lt;/code> object. By this, it is insusceptible to stale reads from its &lt;code>ReplicaSets&lt;/code> cache.&lt;/p>
&lt;p>[3] In simple terms, the &lt;code>ReplicaSet&lt;/code> controller tracks its &lt;code>CREATE pod&lt;/code> actions as follows: when creating new &lt;code>Pods&lt;/code>, it increases a counter of expected &lt;code>ADDED&lt;/code> watch events for the corresponding &lt;code>ReplicaSet&lt;/code>. As soon as such events arrive, it decreases the counter accordingly. It only creates new &lt;code>Pods&lt;/code> for a given &lt;code>ReplicaSet&lt;/code> once all expected events occurred (counter is back to zero) or a timeout has occurred. This way, it prevents creating more &lt;code>Pods&lt;/code> than desired because of stale cache reads and makes the controller eventually consistent.&lt;/p>
&lt;h2 id="conflicts-concurrency-control-and-optimistic-locking">Conflicts, Concurrency Control, and Optimistic Locking&lt;/h2>
&lt;p>Every Kubernetes API object contains the &lt;code>metadata.resourceVersion&lt;/code> field, which identifies an object&amp;rsquo;s version in the backing data store, i.e., etcd. Every write to an object in etcd results in a newer &lt;code>resourceVersion&lt;/code>.
This field is mainly used for concurrency control on the API server in an optimistic locking fashion, but also for efficient resumption of interrupted watch connections.&lt;/p>
&lt;p>Optimistic locking in the Kubernetes API sense means that when a client wants to update an API object, then it includes the object&amp;rsquo;s &lt;code>resourceVersion&lt;/code> in the request to indicate the object&amp;rsquo;s version the modifications are based on.
If the &lt;code>resourceVersion&lt;/code> in etcd has not changed in the meantime, the update request is accepted by the API server and the updated object is written to etcd.
If the &lt;code>resourceVersion&lt;/code> sent by the client does not match the one of the object stored in etcd, there were concurrent modifications to the object. Consequently, the request is rejected with a conflict error (status code &lt;code>409&lt;/code>, API reason &lt;code>Conflict&lt;/code>), for example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;kind&amp;#34;: &lt;span style="color:#a31515">&amp;#34;Status&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;apiVersion&amp;#34;: &lt;span style="color:#a31515">&amp;#34;v1&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;metadata&amp;#34;: {},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;status&amp;#34;: &lt;span style="color:#a31515">&amp;#34;Failure&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;message&amp;#34;: &lt;span style="color:#a31515">&amp;#34;Operation cannot be fulfilled on configmaps \&amp;#34;foo\&amp;#34;: the object has been modified; please apply your changes to the latest version and try again&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;reason&amp;#34;: &lt;span style="color:#a31515">&amp;#34;Conflict&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;details&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;name&amp;#34;: &lt;span style="color:#a31515">&amp;#34;foo&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;kind&amp;#34;: &lt;span style="color:#a31515">&amp;#34;configmaps&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;code&amp;#34;: 409
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This concurrency control is an important mechanism in Kubernetes as there are typically multiple clients acting on API objects at the same time (humans, different controllers, etc.). If a client receives a conflict error, it should read the object&amp;rsquo;s latest version from the API server, make the modifications based on the newest changes, and retry the update.
The reasoning behind this is that a client might choose to make different decisions based on the concurrent changes made by other actors compared to the outdated version that it operated on.&lt;/p>
&lt;p>&lt;em>Important points about concurrency control and conflicts:&lt;/em>&lt;/p>
&lt;ul>
&lt;li>The &lt;code>resourceVersion&lt;/code> field carries a string value and clients must not assume numeric values (the type and structure of versions depend on the backing data store). This means clients may compare &lt;code>resourceVersion&lt;/code> values to detect whether objects were changed. But they must not compare &lt;code>resourceVersion&lt;/code>s to figure out which one is newer/older, i.e., no greater/less-than comparisons are allowed.&lt;/li>
&lt;li>By default, update calls (e.g. via client-go and controller-runtime clients) use optimistic locking as the passed in-memory usually object contains the latest &lt;code>resourceVersion&lt;/code> known to the controller, which is then also sent to the API server.&lt;/li>
&lt;li>API servers can also choose to accept update calls without optimistic locking (i.e., without a &lt;code>resourceVersion&lt;/code> in the object&amp;rsquo;s metadata) for any given resource. However, sending update requests without optimistic locking is strongly discouraged, as doing so overwrites the entire object, discarding any concurrent changes made to it.&lt;/li>
&lt;li>On the other side, patch requests can always be executed either with or without optimistic locking, by (not) including the &lt;code>resourceVersion&lt;/code> in the patched object&amp;rsquo;s metadata. Sending patch requests without optimistic locking might be safe and even desirable as a patch typically updates only a specific section of the object. However, there are also situations where patching without optimistic locking is not safe (see below).&lt;/li>
&lt;/ul>
&lt;h3 id="dont-retry-on-conflict">Don’t Retry on Conflict&lt;/h3>
&lt;p>Similar to how a human would typically handle a conflict error, there are helper functions implementing &lt;code>RetryOnConflict&lt;/code>-semantics, i.e., try an update call, then re-read the object if a conflict occurs, apply the modification again and retry the update.
However, controllers should generally &lt;em>not&lt;/em> use &lt;code>RetryOnConflict&lt;/code>-semantics. Instead, controllers should abort their current reconciliation run and let the queue handle the conflict error with exponential backoff.
The reasoning behind this is that a conflict error indicates that the controller has operated on stale data and might have made wrong decisions earlier on in the reconciliation.
When using a helper function that implements &lt;code>RetryOnConflict&lt;/code>-semantics, the controller doesn&amp;rsquo;t check which fields were changed and doesn&amp;rsquo;t revise its previous decisions accordingly.
Instead, retrying on conflict basically just ignores any conflict error and blindly applies the modification.&lt;/p>
&lt;p>To properly solve the conflict situation, controllers should immediately return with the error from the update call. This will cause retries with exponential backoff so that the cache has a chance to observe the latest changes to the object.
In a later run, the controller will then make correct decisions based on the newest version of the object, not run into conflict errors, and will then be able to successfully reconcile the object. This way, the controller becomes eventually consistent.&lt;/p>
&lt;p>The other way to solve the situation is to modify objects without optimistic locking in order to avoid running into a conflict in the first place (only if this is safe).
This can be a preferable solution for controllers with long-running reconciliations (which is actually an anti-pattern but quite unavoidable in some of Gardener&amp;rsquo;s controllers).
Aborting the entire reconciliation run is rather undesirable in such cases, as it will add a lot of unnecessary waiting time for end users and overhead in terms of compute and network usage.&lt;/p>
&lt;p>However, in any case, retrying on conflict is probably not the right option to solve the situation (there are some correct use cases for it, though, they are very rare). Hence, don&amp;rsquo;t retry on conflict.&lt;/p>
&lt;h3 id="to-lock-or-not-to-lock">To Lock or Not to Lock&lt;/h3>
&lt;p>As explained before, conflicts are actually important and prevent clients from doing wrongful concurrent updates. This means that conflicts are not something we generally want to avoid or ignore.
However, in many cases controllers are exclusive owners of the fields they want to update and thus it might be safe to run without optimistic locking.&lt;/p>
&lt;p>For example, the gardenlet is the exclusive owner of the &lt;code>spec&lt;/code> section of the Extension resources it creates on behalf of a Shoot (e.g., the &lt;code>Infrastructure&lt;/code> resource for creating VPC). Meaning, it knows the exact desired state and no other actor is supposed to update the Infrastructure&amp;rsquo;s &lt;code>spec&lt;/code> fields.
When the gardenlet now updates the Infrastructures &lt;code>spec&lt;/code> section as part of the Shoot reconciliation, it can simply issue a &lt;code>PATCH&lt;/code> request that only updates the &lt;code>spec&lt;/code> and runs without optimistic locking.
If another controller concurrently updated the object in the meantime (e.g., the &lt;code>status&lt;/code> section), the &lt;code>resourceVersion&lt;/code> got changed, which would cause a conflict error if running with optimistic locking.
However, concurrent &lt;code>status&lt;/code> updates would not change the gardenlet&amp;rsquo;s mind on the desired &lt;code>spec&lt;/code> of the Infrastructure resource as it is determined only by looking at the Shoot&amp;rsquo;s specification.
If the &lt;code>spec&lt;/code> section was changed concurrently, it&amp;rsquo;s still fine to overwrite it because the gardenlet should reconcile the &lt;code>spec&lt;/code> back to its desired state.&lt;/p>
&lt;p>Generally speaking, if a controller is the exclusive owner of a given set of fields and they are independent of concurrent changes to other fields in that object, it can patch these fields without optimistic locking.
This might ignore concurrent changes to other fields or blindly overwrite changes to the same fields, but this is fine if the mentioned conditions apply.
Obviously, this applies only to patch requests that modify only a specific set of fields but not to update requests that replace the entire object.&lt;/p>
&lt;p>In such cases, it&amp;rsquo;s even desirable to run without optimistic locking as it will be more performant and save retries.
If certain requests are made with high frequency and have a good chance of causing conflicts, retries because of optimistic locking can cause a lot of additional network traffic in a large-scale Gardener installation.&lt;/p>
&lt;h2 id="updates-patches-server-side-apply">Updates, Patches, Server-Side Apply&lt;/h2>
&lt;p>There are different ways of modifying Kubernetes API objects.
The following snippet demonstrates how to do a given modification with the most frequently used options using a controller-runtime client:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx context.Context
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> c client.Client
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shoot *gardencorev1beta1.Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// update
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>shoot.Spec.Kubernetes.Version = &lt;span style="color:#a31515">&amp;#34;1.26&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>err := c.Update(ctx, shoot)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// json merge patch
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>patch := client.MergeFrom(shoot.DeepCopy())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>shoot.Spec.Kubernetes.Version = &lt;span style="color:#a31515">&amp;#34;1.26&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>err = c.Patch(ctx, shoot, patch)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// strategic merge patch
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>patch = client.StrategicMergeFrom(shoot.DeepCopy())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>shoot.Spec.Kubernetes.Version = &lt;span style="color:#a31515">&amp;#34;1.26&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>err = c.Patch(ctx, shoot, patch)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;em>Important characteristics of the shown request types:&lt;/em>&lt;/p>
&lt;ul>
&lt;li>Update requests always send the entire object to the API server and update all fields accordingly. By default, optimistic locking is used (&lt;code>resourceVersion&lt;/code> is included).&lt;/li>
&lt;li>Both patch types run without optimistic locking by default. However, it can be enabled explicitly if needed:
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// json merge patch + optimistic locking
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>patch := client.MergeFromWithOptions(shoot.DeepCopy(), client.MergeFromWithOptimisticLock{})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// strategic merge patch + optimistic locking
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>patch = client.StrategicMergeFrom(shoot.DeepCopy(), client.MergeFromWithOptimisticLock{})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>Patch requests only contain the changes made to the in-memory object between the copy passed to &lt;code>client.*MergeFrom&lt;/code> and the object passed to &lt;code>Client.Patch()&lt;/code>. The diff is calculated on the client-side based on the in-memory objects only. This means that if in the meantime some fields were changed on the API server to a different value than the one on the client-side, the fields will not be changed back as long as they are not changed on the client-side as well (there will be no diff in memory).&lt;/li>
&lt;li>Thus, if you want to ensure a given state using patch requests, always read the object first before patching it, as there will be no diff otherwise, meaning the patch will be empty. For more information, see &lt;a href="https://github.com/gardener/gardener/pull/4057">gardener/gardener#4057&lt;/a> and the comments in &lt;a href="https://github.com/gardener/gardener/pull/4027">gardener/gardener#4027&lt;/a>.&lt;/li>
&lt;li>Also, always send updates and patch requests even if your controller hasn&amp;rsquo;t made any changes to the current state on the API server. I.e., don&amp;rsquo;t make any optimization for preventing empty patches or no-op updates. There might be mutating webhooks in the system that will modify the object and that rely on update/patch requests being sent (even if they are no-op). Gardener&amp;rsquo;s extension concept makes heavy use of mutating webhooks, so it&amp;rsquo;s important to keep this in mind.&lt;/li>
&lt;li>JSON merge patches always replace lists as a whole and don&amp;rsquo;t merge them. Keep this in mind when operating on lists with merge patch requests. If the controller is the exclusive owner of the entire list, it&amp;rsquo;s safe to run without optimistic locking. Though, if you want to prevent overwriting concurrent changes to the list or its items made by other actors (e.g., additions/removals to the &lt;code>metadata.finalizers&lt;/code> list), enable optimistic locking.&lt;/li>
&lt;li>Strategic merge patches are able to make more granular modifications to lists and their elements without replacing the entire list. It uses Golang struct tags of the API types to determine which and how lists should be merged. See &lt;a href="https://kubernetes.io/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/">Update API Objects in Place Using kubectl patch&lt;/a> or the &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/strategic-merge-patch.md">strategic merge patch documentation&lt;/a> for more in-depth explanations and comparison with JSON merge patches.
With this, controllers &lt;em>might&lt;/em> be able to issue patch requests for individual list items without optimistic locking, even if they are not exclusive owners of the entire list. Remember to check the &lt;code>patchStrategy&lt;/code> and &lt;code>patchMergeKey&lt;/code> struct tags of the fields you want to modify before blindly adding patch requests without optimistic locking.&lt;/li>
&lt;li>Strategic merge patches are only supported by built-in Kubernetes resources and custom resources served by Extension API servers. Strategic merge patches are not supported by custom resources defined by &lt;code>CustomResourceDefinition&lt;/code>s (see &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#advanced-features-and-flexibility">this comparison&lt;/a>). In that case, fallback to JSON merge patches.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">Server-side Apply&lt;/a> is yet another mechanism to modify API objects, which is supported by all API resources (in newer Kubernetes versions). However, it has a few problems and more caveats preventing us from using it in Gardener at the time of writing. See &lt;a href="https://github.com/gardener/gardener/issues/4122">gardener/gardener#4122&lt;/a> for more details.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Generally speaking, patches are often the better option compared to update requests because they can save network traffic, encoding/decoding effort, and avoid conflicts under the presented conditions.
If choosing a patch type, consider which type is supported by the resource you&amp;rsquo;re modifying and what will happen in case of a conflict. Consider whether your modification is safe to run without optimistic locking.
However, there is no simple rule of thumb on which patch type to choose.&lt;/p>
&lt;/blockquote>
&lt;h2 id="on-helper-functions">On Helper Functions&lt;/h2>
&lt;p>Here is a note on some helper functions, that should be avoided and why:&lt;/p>
&lt;p>&lt;code>controllerutil.CreateOrUpdate&lt;/code> does a basic get, mutate and create or update call chain, which is often used in controllers. We should avoid using this helper function in Gardener, because it is likely to cause conflicts for cached clients and doesn&amp;rsquo;t send no-op requests if nothing was changed, which can cause problems because of the heavy use of webhooks in Gardener extensions (see above).
That&amp;rsquo;s why usage of this function was completely replaced in &lt;a href="https://github.com/gardener/gardener/pull/4227">gardener/gardener#4227&lt;/a> and similar PRs.&lt;/p>
&lt;p>&lt;code>controllerutil.CreateOrPatch&lt;/code> is similar to &lt;code>CreateOrUpdate&lt;/code> but does a patch request instead of an update request. It has the same drawback as &lt;code>CreateOrUpdate&lt;/code> regarding no-op updates.
Also, controllers can&amp;rsquo;t use optimistic locking or strategic merge patches when using &lt;code>CreateOrPatch&lt;/code>.
Another reason for avoiding use of this function is that it also implicitly patches the status section if it was changed, which is confusing for others reading the code. To accomplish this, the func does some back and forth conversion, comparison and checks, which are unnecessary in most of our cases and simply wasted CPU cycles and complexity we want to avoid.&lt;/p>
&lt;p>There were some &lt;code>Try{Update,UpdateStatus,Patch,PatchStatus}&lt;/code> helper functions in Gardener that were already removed by &lt;a href="https://github.com/gardener/gardener/pull/4378">gardener/gardener#4378&lt;/a> but are still used in some extension code at the time of writing.
The reason for eliminating these functions is that they implement &lt;code>RetryOnConflict&lt;/code>-semantics. Meaning, they first get the object, mutate it, then try to update and retry if a conflict error occurs.
As explained above, retrying on conflict is a controller anti-pattern and should be avoided in almost every situation.
The other problem with these functions is that they read the object first from the API server (always do a direct call), although in most cases we already have a recent version of the object at hand. So, using this function generally does unnecessary API calls and therefore causes unwanted compute and network load.&lt;/p>
&lt;p>For the reasons explained above, there are similar helper functions that accomplish similar things but address the mentioned drawbacks: &lt;code>controllerutils.{GetAndCreateOrMergePatch,GetAndCreateOrStrategicMergePatch}&lt;/code>.
These can be safely used as replacements for the aforementioned helper funcs.
If they are not fitting for your use case, for example because you need to use optimistic locking, just do the appropriate calls in the controller directly.&lt;/p>
&lt;h2 id="related-links">Related Links&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=RPsUo925PUA&amp;amp;t=40s">Kubernetes Client usage in Gardener&lt;/a> (Community Meeting talk, 2020-06-26)&lt;/li>
&lt;/ul>
&lt;p>These resources are only partially related to the topics covered in this doc, but might still be interesting for developer seeking a deeper understanding of Kubernetes API machinery, architecture and foundational concepts.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md">API Conventions&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/design-proposals-archive/blob/main/architecture/resource-management.md">The Kubernetes Resource Model&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: KUBERNETES_SERVICE_HOST Environment Variable Injection</title><link>https://gardener.cloud/docs/gardener/shoot_kubernetes_service_host_injection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/shoot_kubernetes_service_host_injection/</guid><description>
&lt;h1 id="kubernetes_service_host-environment-variable-injection">&lt;code>KUBERNETES_SERVICE_HOST&lt;/code> Environment Variable Injection&lt;/h1>
&lt;p>In each Shoot cluster&amp;rsquo;s &lt;code>kube-system&lt;/code> namespace a &lt;code>DaemonSet&lt;/code> called &lt;code>apiserver-proxy&lt;/code> is deployed. It routes traffic to the upstream Shoot Kube APIServer. See the &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md">APIServer SNI GEP&lt;/a> for more details.&lt;/p>
&lt;p>To skip this extra network hop, a &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook">mutating webhook&lt;/a> called &lt;code>apiserver-proxy.networking.gardener.cloud&lt;/code> is deployed next to the API server in the Seed. It adds a &lt;code>KUBERNETES_SERVICE_HOST&lt;/code> environment variable to each container and init container that do not specify it. See the webhook &lt;a href="https://github.com/gardener/apiserver-proxy/">repository&lt;/a> for more information.&lt;/p>
&lt;h2 id="opt-out-of-pod-injection">Opt-Out of Pod Injection&lt;/h2>
&lt;p>In some cases it&amp;rsquo;s desirable to opt-out of Pod injection:&lt;/p>
&lt;ul>
&lt;li>DNS is disabled on that individual Pod, but it still needs to talk to the kube-apiserver.&lt;/li>
&lt;li>Want to test the &lt;code>kube-proxy&lt;/code> and &lt;code>kubelet&lt;/code> in-cluster discovery.&lt;/li>
&lt;/ul>
&lt;h3 id="opt-out-of-pod-injection-for-specific-pods">Opt-Out of Pod Injection for Specific Pods&lt;/h3>
&lt;p>To opt out of the injection, the Pod should be labeled with &lt;code>apiserver-proxy.networking.gardener.cloud/inject: disable&lt;/code>, e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Deployment
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> replicas: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> selector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiserver-proxy.networking.gardener.cloud/inject: disable
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> image: nginx:1.14.2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ports:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - containerPort: 80
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="opt-out-of-pod-injection-on-namespace-level">Opt-Out of Pod Injection on Namespace Level&lt;/h3>
&lt;p>To opt out of the injection of &lt;strong>all&lt;/strong> Pods in a namespace, you should label your namespace with &lt;code>apiserver-proxy.networking.gardener.cloud/inject: disable&lt;/code>, e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiserver-proxy.networking.gardener.cloud/inject: disable
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-namespace
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>or via &lt;code>kubectl&lt;/code> for existing namespace:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl label namespace my-namespace apiserver-proxy.networking.gardener.cloud/inject=disable
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> Please be aware that it&amp;rsquo;s not possible to disable injection on a namespace level and enable it for individual pods in it.&lt;/p>
&lt;/blockquote>
&lt;h3 id="opt-out-of-pod-injection-for-the-entire-cluster">Opt-Out of Pod Injection for the Entire Cluster&lt;/h3>
&lt;p>If the injection is causing problems for different workloads and ignoring individual pods or namespaces is not possible, then the feature could be disabled for the entire cluster with the &lt;code>alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector&lt;/code> annotation with value &lt;code>disable&lt;/code> on the &lt;code>Shoot&lt;/code> resource itself:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector: &lt;span style="color:#a31515">&amp;#39;disable&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-cluster
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>or via &lt;code>kubectl&lt;/code> for existing shoot cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl label shoot my-cluster alpha.featuregates.shoot.gardener.cloud/apiserver-sni-pod-injector=disable
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> Please be aware that it&amp;rsquo;s not possible to disable injection on a cluster level and enable it for individual pods in it.&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Local Setup</title><link>https://gardener.cloud/docs/gardener/local_setup/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/local_setup/</guid><description>
&lt;h1 id="overview">Overview&lt;/h1>
&lt;p>Conceptually, all Gardener components are designed to run as a Pod inside a Kubernetes cluster.
The Gardener API server extends the Kubernetes API via the user-aggregated API server concepts.
However, if you want to develop it, you may want to work locally with the Gardener without building a Docker image and deploying it to a cluster each and every time.
That means that the Gardener runs outside a Kubernetes cluster which requires providing a &lt;a href="https://kubernetes.io/docs/tasks/access-application-cluster/authenticate-across-clusters-kubeconfig/">Kubeconfig&lt;/a> in your local filesystem and point the Gardener to it when starting it (see below).&lt;/p>
&lt;p>Further details can be found in&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/">Principles of Kubernetes&lt;/a>, and its &lt;a href="https://kubernetes.io/docs/concepts/overview/components/">components&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/tree/master/contributors/devel">Kubernetes Development Guide&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/documentation/wiki/Architecture">Architecture of Gardener&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>This guide is split into two main parts:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/local_setup/#preparing-the-setup">Preparing your setup by installing all dependencies and tools&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/local_setup/#get-the-sources">Getting the Gardener source code locally&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="preparing-the-setup">Preparing the Setup&lt;/h1>
&lt;h2 id="macos-only-installing-homebrew">[macOS only] Installing homebrew&lt;/h2>
&lt;p>The copy-paste instructions in this guide are designed for macOS and use the package manager &lt;a href="https://brew.sh/">Homebrew&lt;/a>.&lt;/p>
&lt;p>On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>/bin/bash -c &lt;span style="color:#a31515">&amp;#34;&lt;/span>&lt;span style="color:#00f">$(&lt;/span>curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh&lt;span style="color:#00f">)&lt;/span>&lt;span style="color:#a31515">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="macos-only-installing-gnu-bash">[macOS only] Installing GNU bash&lt;/h2>
&lt;p>Built-in apple-darwin bash is missing some features that could cause shell scripts to fail locally.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install bash
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="installing-git">Installing git&lt;/h2>
&lt;p>We use &lt;code>git&lt;/code> as VCS which you need to install. On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install git
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For other OS, please check the &lt;a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">Git installation documentation&lt;/a>.&lt;/p>
&lt;h2 id="installing-go">Installing Go&lt;/h2>
&lt;p>Install the latest version of Go. On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install go
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For other OS, please check &lt;a href="https://golang.org/doc/install">Go installation documentation&lt;/a>.&lt;/p>
&lt;h2 id="installing-kubectl">Installing kubectl&lt;/h2>
&lt;p>Install &lt;code>kubectl&lt;/code>. Please make sure that the version of &lt;code>kubectl&lt;/code> is at least &lt;code>v1.25.x&lt;/code>. On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install kubernetes-cli
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For other OS, please check the &lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl installation documentation&lt;/a>.&lt;/p>
&lt;h2 id="installing-docker">Installing Docker&lt;/h2>
&lt;p>You need to have docker installed and running. On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install --cask docker
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For other OS please check the &lt;a href="https://docs.docker.com/get-docker/">docker installation documentation&lt;/a>.&lt;/p>
&lt;h2 id="installing-iproute2">Installing iproute2&lt;/h2>
&lt;p>&lt;code>iproute2&lt;/code> provides a collection of utilities for network administration and configuration. On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install iproute2mac
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="installing-jq">Installing jq&lt;/h2>
&lt;p>&lt;a href="https://jqlang.github.io/jq/">jq&lt;/a> is a lightweight and flexible command-line JSON processor. On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install jq
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="installing-yq">Installing yq&lt;/h2>
&lt;p>&lt;a href="https://mikefarah.gitbook.io/yq">yq&lt;/a> is a lightweight and portable command-line YAML processor. On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install yq
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="installing-gnu-parallel">Installing GNU Parallel&lt;/h2>
&lt;p>&lt;a href="https://www.gnu.org/software/parallel/">GNU Parallel&lt;/a> is a shell tool for executing jobs in parallel, used by the code generation scripts (&lt;code>make generate&lt;/code>). On macOS run&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install parallel
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="macos-only-install-gnu-core-utilities">[macOS only] Install GNU Core Utilities&lt;/h2>
&lt;p>When running on macOS, install the GNU core utilities and friends:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>brew install coreutils gnu-sed gnu-tar grep gzip
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will create symbolic links for the GNU utilities with &lt;code>g&lt;/code> prefix on your &lt;code>PATH&lt;/code>, e.g., &lt;code>gsed&lt;/code> or &lt;code>gbase64&lt;/code>.
To allow using them without the &lt;code>g&lt;/code> prefix, add the &lt;code>gnubin&lt;/code> directories to the beginning of your &lt;code>PATH&lt;/code> environment variable (&lt;code>brew install&lt;/code> and &lt;code>brew info&lt;/code> will print out instructions for each formula):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>export PATH=&lt;span style="color:#00f">$(&lt;/span>brew --prefix&lt;span style="color:#00f">)&lt;/span>/opt/coreutils/libexec/gnubin:$PATH
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export PATH=&lt;span style="color:#00f">$(&lt;/span>brew --prefix&lt;span style="color:#00f">)&lt;/span>/opt/gnu-sed/libexec/gnubin:$PATH
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export PATH=&lt;span style="color:#00f">$(&lt;/span>brew --prefix&lt;span style="color:#00f">)&lt;/span>/opt/gnu-tar/libexec/gnubin:$PATH
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export PATH=&lt;span style="color:#00f">$(&lt;/span>brew --prefix&lt;span style="color:#00f">)&lt;/span>/opt/grep/libexec/gnubin:$PATH
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export PATH=&lt;span style="color:#00f">$(&lt;/span>brew --prefix&lt;span style="color:#00f">)&lt;/span>/opt/gzip/bin:$PATH
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="windows-only-wsl2">[Windows Only] WSL2&lt;/h2>
&lt;p>Apart from Linux distributions and macOS, the local gardener setup can also run on the Windows Subsystem for Linux 2.&lt;/p>
&lt;p>While WSL1, plain docker for Windows and various Linux distributions and local Kubernetes environments may be supported, this setup was verified with:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://docs.microsoft.com/en-us/windows/wsl/wsl2-index">WSL2&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.docker.com/docker-for-windows/wsl/">Docker Desktop WSL2 Engine&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://ubuntu.com/blog/ubuntu-on-wsl-2-is-generally-available">Ubuntu 18.04 LTS on WSL2&lt;/a>&lt;/li>
&lt;li>Nodeless local garden (see below)&lt;/li>
&lt;/ul>
&lt;p>The Gardener repository and all the above-mentioned tools (git, golang, kubectl, &amp;hellip;) should be installed in your WSL2 distro, according to the distribution-specific Linux installation instructions.&lt;/p>
&lt;h1 id="get-the-sources">Get the Sources&lt;/h1>
&lt;p>Clone the repository from GitHub into your &lt;code>$GOPATH&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>mkdir -p &lt;span style="color:#00f">$(&lt;/span>go env GOPATH&lt;span style="color:#00f">)&lt;/span>/src/github.com/gardener
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cd &lt;span style="color:#00f">$(&lt;/span>go env GOPATH&lt;span style="color:#00f">)&lt;/span>/src/github.com/gardener
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>git clone git@github.com:gardener/gardener.git
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cd gardener
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>Note: Gardener is using Go modules and cloning the repository into &lt;code>$GOPATH&lt;/code> is not a hard requirement. However it is still recommended to clone into &lt;code>$GOPATH&lt;/code> because &lt;code>k8s.io/code-generator&lt;/code> does not work yet outside of &lt;code>$GOPATH&lt;/code> - &lt;a href="https://github.com/kubernetes/kubernetes/issues/86753">kubernetes/kubernetes#86753&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;h1 id="start-the-gardener">Start the Gardener&lt;/h1>
&lt;p>Please see &lt;a href="https://gardener.cloud/docs/gardener/deployment/getting_started_locally/">getting_started_locally.md&lt;/a> how to build and deploy Gardener from your local sources.&lt;/p></description></item><item><title>Docs: Log Parsers</title><link>https://gardener.cloud/docs/gardener/log_parsers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/log_parsers/</guid><description>
&lt;h1 id="how-to-create-log-parser-for-container-into-fluent-bit">How to Create Log Parser for Container into fluent-bit&lt;/h1>
&lt;p>If our log message is parsed correctly, it has to be showed in Plutono like this:&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-jsonc" data-lang="jsonc"> {&amp;#34;log&amp;#34;:&amp;#34;OpenAPI AggregationController: Processing item v1beta1.metrics.k8s.io&amp;#34;,&amp;#34;pid&amp;#34;:&amp;#34;1&amp;#34;,&amp;#34;severity&amp;#34;:&amp;#34;INFO&amp;#34;,&amp;#34;source&amp;#34;:&amp;#34;controller.go:107&amp;#34;}
&lt;/code>&lt;/pre>&lt;p>Otherwise it will looks like this:&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-jsonc" data-lang="jsonc">{
&amp;#34;log&amp;#34;:&amp;#34;{
\&amp;#34;level\&amp;#34;:\&amp;#34;info\&amp;#34;,\&amp;#34;ts\&amp;#34;:\&amp;#34;2020-06-01T11:23:26.679Z\&amp;#34;,\&amp;#34;logger\&amp;#34;:\&amp;#34;gardener-resource-manager.health-reconciler\&amp;#34;,\&amp;#34;msg\&amp;#34;:\&amp;#34;Finished ManagedResource health checks\&amp;#34;,\&amp;#34;object\&amp;#34;:\&amp;#34;garden/provider-aws-dsm9r\&amp;#34;
}\n&amp;#34;
}
}
&lt;/code>&lt;/pre>&lt;h2 id="create-a-custom-parser">Create a Custom Parser&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>First of all, we need to know how the log for the specific container looks like (for example, lets take a log from the &lt;code>alertmanager&lt;/code> :
&lt;code>level=info ts=2019-01-28T12:33:49.362015626Z caller=main.go:175 build_context=&amp;quot;(go=go1.11.2, user=root@4ecc17c53d26, date=20181109-15:40:48)&lt;/code>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>We can see that this log contains 4 subfields(severity=info, timestamp=2019-01-28T12:33:49.362015626Z, source=main.go:175 and the actual message).
So we have to write a regex which matches this log in 4 groups(We can use &lt;a href="https://regex101.com/">https://regex101.com/&lt;/a> like helping tool). So, for this purpose our regex looks like this:&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>^level=(?&amp;lt;severity&amp;gt;\w+)\s+ts=(?&amp;lt;time&amp;gt;\d{4}-\d{2}-\d{2}[Tt].*[zZ])\s+caller=(?&amp;lt;source&amp;gt;[^\s]*+)\s+(?&amp;lt;log&amp;gt;.*)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Now we have to create correct time format for the timestamp (We can use this site for this purpose: &lt;a href="http://ruby-doc.org/stdlib-2.4.1/libdoc/time/rdoc/Time.html#method-c-strptime">http://ruby-doc.org/stdlib-2.4.1/libdoc/time/rdoc/Time.html#method-c-strptime&lt;/a>).
So our timestamp matches correctly the following format:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>%Y-%m-%dT%H:%M:%S.%L
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>It&amp;rsquo;s time to apply our new regex into fluent-bit configuration. To achieve that we can just deploy in the cluster where the &lt;code>fluent-operator&lt;/code> is deployed the following custom resources:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: fluentbit.fluent.io/v1alpha2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterFilter
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fluentbit.gardener/type: seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: &amp;lt;&amp;lt; pod-name &amp;gt;&amp;gt;--(&amp;lt;&amp;lt; container-name &amp;gt;&amp;gt;)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> filters:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - parser:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> keyName: log
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> parser: &amp;lt;&amp;lt; container-name &amp;gt;&amp;gt;-parser
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reserveData: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> match: kubernetes.&amp;lt;&amp;lt; pod-name &amp;gt;&amp;gt;*&amp;lt;&amp;lt; container-name &amp;gt;&amp;gt;*
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>EXAMPLE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: fluentbit.fluent.io/v1alpha2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterFilter
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fluentbit.gardener/type: seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: alertmanager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> filters:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - parser:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> keyName: log
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> parser: alertmanager-parser
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reserveData: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> match: &lt;span style="color:#a31515">&amp;#34;kubernetes.alertmanager*alertmanager*&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Now lets check if there already exists &lt;code>ClusterParser&lt;/code> with such a regex and time format that we need. If it doesn&amp;rsquo;t, create one:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: fluentbit.fluent.io/v1alpha2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterParser
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: &amp;lt;&amp;lt; container-name &amp;gt;&amp;gt;-parser
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fluentbit.gardener/type: &lt;span style="color:#a31515">&amp;#34;seed&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> regex:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> timeKey: time
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> timeFormat: &amp;lt;&amp;lt; time-format &amp;gt;&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> regex: &lt;span style="color:#a31515">&amp;#34;&amp;lt;&amp;lt; regex &amp;gt;&amp;gt;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>EXAMPLE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: fluentbit.fluent.io/v1alpha2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterParser
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: alermanager-parser
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> fluentbit.gardener/type: &lt;span style="color:#a31515">&amp;#34;seed&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> regex:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> timeKey: time
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> timeFormat: &lt;span style="color:#a31515">&amp;#34;%Y-%m-%dT%H:%M:%S.%L&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> regex: &lt;span style="color:#a31515">&amp;#34;^level=(?&amp;lt;severity&amp;gt;\\w+)\\s+ts=(?&amp;lt;time&amp;gt;\\d{4}-\\d{2}-\\d{2}[Tt].*[zZ])\\s+caller=(?&amp;lt;source&amp;gt;[^\\s]*+)\\s+(?&amp;lt;log&amp;gt;.*)&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>Follow your development setup to validate that the parsers are working correctly.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Logging</title><link>https://gardener.cloud/docs/gardener/logging/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/logging/</guid><description>
&lt;h1 id="logging-stack">Logging Stack&lt;/h1>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Kubernetes uses the underlying container runtime logging, which does not persist logs for stopped and destroyed containers. This makes it difficult to investigate issues in the very common case of not running containers. Gardener provides a solution to this problem for the managed cluster components by introducing its own logging stack.&lt;/p>
&lt;h2 id="components">Components&lt;/h2>
&lt;ul>
&lt;li>A Fluent-bit daemonset which works like a log collector and custom Golang plugin which spreads log messages to their Vali instances.&lt;/li>
&lt;li>One Vali Statefulset in the &lt;code>garden&lt;/code> namespace which contains logs for the seed cluster and one per shoot namespace which contains logs for shoot&amp;rsquo;s controlplane.&lt;/li>
&lt;li>One Plutono Deployment in &lt;code>garden&lt;/code> namespace and two Deployments per shoot namespace (one exposed to the end users and one for the operators). Plutono is the UI component used in the logging stack.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://gardener.cloud/__resources/logging-architecture_c8dc32.png" alt="">&lt;/p>
&lt;h2 id="container-logs-rotation-and-retention">Container Logs Rotation and Retention&lt;/h2>
&lt;p>Container &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/#log-rotation">log rotation&lt;/a> in Kubernetes describes a subtile but important implementation detail depending on the type of the used high-level container runtime. When the used container runtime is not CRI compliant (such as &lt;code>dockershim&lt;/code>), then the &lt;code>kubelet&lt;/code> does not provide any rotation or retention implementations, hence leaving those aspects to the downstream components. When the used container runtime is CRI compliant (such as &lt;code>containerd&lt;/code>), then the &lt;code>kubelet&lt;/code> provides the necessary implementation with two configuration options:&lt;/p>
&lt;ul>
&lt;li>&lt;code>ContainerLogMaxSize&lt;/code> for rotation&lt;/li>
&lt;li>&lt;code>ContainerLogMaxFiles&lt;/code> for retention&lt;/li>
&lt;/ul>
&lt;h3 id="containerd-runtime">ContainerD Runtime&lt;/h3>
&lt;p>In this case, it is possible to configure the &lt;code>containerLogMaxSize&lt;/code> and &lt;code>containerLogMaxFiles&lt;/code> fields in the Shoot specification. Both fields are optional and if nothing is specified, then the &lt;code>kubelet&lt;/code> rotates on the size &lt;code>100M&lt;/code>. Those fields are part of provider&amp;rsquo;s workers definition. Here is an example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - cri:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: containerd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubelet:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># accepted values are of resource.Quantity&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containerLogMaxSize: 150Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containerLogMaxFiles: 10
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The values of the &lt;code>containerLogMaxSize&lt;/code> and &lt;code>containerLogMaxFiles&lt;/code> fields need to be considered with care since container log files claim disk space from the host. On the opposite side, log rotations on too small sizes may result in frequent rotations which can be missed by other components (log shippers) observing these rotations.&lt;/p>
&lt;p>In the majority of the cases, the defaults should do just fine. Custom configuration might be of use under rare conditions.&lt;/p>
&lt;h2 id="extension-of-the-logging-stack">Extension of the Logging Stack&lt;/h2>
&lt;p>The logging stack is extended to scrape logs from the systemd services of each shoots&amp;rsquo; nodes and from all Gardener components in the shoot &lt;code>kube-system&lt;/code> namespace. These logs are exposed only to the Gardener operators.&lt;/p>
&lt;p>Also, in the shoot control plane an &lt;code>event-logger&lt;/code> pod is deployed, which scrapes events from the shoot &lt;code>kube-system&lt;/code> namespace and shoot &lt;code>control-plane&lt;/code> namespace in the seed. The &lt;code>event-logger&lt;/code> logs the events to the standard output. Then the &lt;code>fluent-bit&lt;/code> gets these events as container logs and sends them to the Vali in the shoot control plane (similar to how it works for any other control plane component).
&lt;img src="https://gardener.cloud/__resources/shoot-node-logging-architecture_23c018.png" alt="">&lt;/p>
&lt;h2 id="how-to-access-the-logs">How to Access the Logs&lt;/h2>
&lt;p>The logs are accessible via Plutono. To access them:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Authenticate via basic auth to gain access to Plutono.
The Plutono URL can be found in the &lt;code>Logging and Monitoring&lt;/code> section of a cluster in the Gardener Dashboard alongside the credentials.
The secret containing the credentials is stored in the project namespace following the naming pattern &lt;code>&amp;lt;shoot-name&amp;gt;.monitoring&lt;/code>.
For Gardener operators, the credentials are also stored in the control-plane (&lt;code>shoot--&amp;lt;project-name&amp;gt;--&amp;lt;shoot-name&amp;gt;&lt;/code>) namespace in the &lt;code>observability-ingress-users-&amp;lt;hash&amp;gt;&lt;/code> secret in the seed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Plutono contains several dashboards that aim to facilitate the work of operators and users.
From the &lt;code>Explore&lt;/code> tab, users and operators have unlimited abilities to extract and manipulate logs.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> Gardener Operators are people part of the Gardener team with operator permissions, not operators of the end-user cluster!&lt;/p>
&lt;/blockquote>
&lt;h3 id="how-to-use-the-explore-tab">How to Use the &lt;code>Explore&lt;/code> Tab&lt;/h3>
&lt;p>If you click on the &lt;code>Log browser &amp;gt;&lt;/code> button, you will see all of the available labels.
Clicking on the label, you can see all of its available values for the given period of time you have specified.
If you are searching for logs for the past one hour, do not expect to see labels or values for which there were no logs for that period of time.
By clicking on a value, Plutono automatically eliminates all other labels and/or values with which no valid log stream can be made.
After choosing the right labels and their values, click on the &lt;code>Show logs&lt;/code> button.
This will build &lt;code>Log query&lt;/code> and execute it.
This approach is convenient when you don&amp;rsquo;t know the labels names or they values.
&lt;img src="https://gardener.cloud/__resources/explore-button-usage_0dfdca.png" alt="">&lt;/p>
&lt;p>Once you feel comfortable, you can start to use the &lt;a href="https://github.com/credativ/plutono">LogQL&lt;/a> language to search for logs.
Next to the &lt;code>Log browser &amp;gt;&lt;/code> button is the place where you can type log queries.&lt;/p>
&lt;p>Examples:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>If you want to get logs for &lt;code>calico-node-&amp;lt;hash&amp;gt;&lt;/code> pod in the cluster &lt;code>kube-system&lt;/code>:
The name of the node on which &lt;code>calico-node&lt;/code> was running is known, but not the hash suffix of the &lt;code>calico-node&lt;/code> pod.
Also we want to search for errors in the logs.&lt;/p>
&lt;p>&lt;code>{pod_name=~&amp;quot;calico-node-.+&amp;quot;, nodename=&amp;quot;ip-10-222-31-182.eu-central-1.compute.internal&amp;quot;} |~ &amp;quot;error&amp;quot;&lt;/code>&lt;/p>
&lt;p>Here, you will get as much help as possible from the Plutono by giving you suggestions and auto-completion.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If you want to get the logs from &lt;code>kubelet&lt;/code> systemd service of a given node and search for a pod name in the logs:&lt;/p>
&lt;p>&lt;code>{unit=&amp;quot;kubelet.service&amp;quot;, nodename=&amp;quot;ip-10-222-31-182.eu-central-1.compute.internal&amp;quot;} |~ &amp;quot;pod name&amp;quot;&lt;/code>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> Under &lt;code>unit&lt;/code> label there is only the &lt;code>docker&lt;/code>, &lt;code>containerd&lt;/code>, &lt;code>kubelet&lt;/code> and &lt;code>kernel&lt;/code> logs.&lt;/p>
&lt;/blockquote>
&lt;ol start="3">
&lt;li>
&lt;p>If you want to get the logs from &lt;code>gardener-node-agent&lt;/code> systemd service of a given node and search for a string in the logs:&lt;/p>
&lt;p>&lt;code>{job=&amp;quot;systemd-combine-journal&amp;quot;,nodename=&amp;quot;ip-10-222-31-182.eu-central-1.compute.internal&amp;quot;} | unpack | unit=&amp;quot;gardener-node-agent.service&amp;quot;&lt;/code>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> &lt;code>{job=&amp;quot;systemd-combine-journal&amp;quot;,nodename=&amp;quot;&amp;lt;node name&amp;gt;&amp;quot;}&lt;/code> stream &lt;a href="https://github.com/credativ/plutono">pack&lt;/a> all logs from systemd services except &lt;code>docker&lt;/code>, &lt;code>containerd&lt;/code>, &lt;code>kubelet&lt;/code>, and &lt;code>kernel&lt;/code>. To filter those log by unit, you have to &lt;a href="https://github.com/credativ/plutono">unpack&lt;/a> them first.&lt;/p>
&lt;/blockquote>
&lt;ol start="4">
&lt;li>Retrieving events:&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>
&lt;p>If you want to get the events from the shoot &lt;code>kube-system&lt;/code> namespace generated by &lt;code>kubelet&lt;/code> and related to the &lt;code>node-problem-detector&lt;/code>:&lt;/p>
&lt;p>&lt;code>{job=&amp;quot;event-logging&amp;quot;} | unpack | origin_extracted=&amp;quot;shoot&amp;quot;,source=&amp;quot;kubelet&amp;quot;,object=~&amp;quot;.*node-problem-detector.*&amp;quot;&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If you want to get the events generated by MCM in the shoot control plane in the seed:&lt;/p>
&lt;p>&lt;code>{job=&amp;quot;event-logging&amp;quot;} | unpack | origin_extracted=&amp;quot;seed&amp;quot;,source=~&amp;quot;.*machine-controller-manager.*&amp;quot;&lt;/code>&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> In order to group events by origin, one has to specify &lt;code>origin_extracted&lt;/code> because the &lt;code>origin&lt;/code> label is reserved for all of the logs from the seed and the &lt;code>event-logger&lt;/code> resides in the seed, so all of its logs are coming as they are only from the seed. The actual origin is embedded in the unpacked event. When unpacked, the embedded &lt;code>origin&lt;/code> becomes &lt;code>origin_extracted&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;h2 id="expose-logs-for-component-to-user-plutono">Expose Logs for Component to User Plutono&lt;/h2>
&lt;p>Exposing logs for a new component to the User&amp;rsquo;s Plutono is described in the &lt;a href="https://gardener.cloud/docs/gardener/extensions/logging-and-monitoring/#how-to-expose-logs-to-the-users">How to Expose Logs to the Users&lt;/a> section.&lt;/p>
&lt;h2 id="configuration">Configuration&lt;/h2>
&lt;h3 id="fluent-bit">Fluent-bit&lt;/h3>
&lt;p>The Fluent-bit configurations can be found on &lt;code>pkg/component/observability/logging/fluentoperator/customresources&lt;/code>
There are six different specifications:&lt;/p>
&lt;ul>
&lt;li>FluentBit: Defines the fluent-bit DaemonSet specifications&lt;/li>
&lt;li>ClusterFluentBitConfig: Defines the labelselectors of the resources which fluent-bit will match&lt;/li>
&lt;li>ClusterInput: Defines the location of the input stream of the logs&lt;/li>
&lt;li>ClusterOutput: Defines the location of the output source (Vali for example)&lt;/li>
&lt;li>ClusterFilter: Defines filters which match specific keys&lt;/li>
&lt;li>ClusterParser: Defines parsers which are used by the filters&lt;/li>
&lt;/ul>
&lt;h3 id="vali">Vali&lt;/h3>
&lt;p>The Vali configurations can be found on &lt;code>charts/seed-bootstrap/charts/vali/templates/vali-configmap.yaml&lt;/code>&lt;/p>
&lt;p>The main specifications there are:&lt;/p>
&lt;ul>
&lt;li>Index configuration: Currently the following one is used:&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code> schema_config:
configs:
- from: 2018-04-15
store: boltdb
object_store: filesystem
schema: v11
index:
prefix: index_
period: 24h
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>&lt;code>from&lt;/code>: Is the date from which logs collection is started. Using a date in the past is okay.&lt;/li>
&lt;li>&lt;code>store&lt;/code>: The DB used for storing the index.&lt;/li>
&lt;li>&lt;code>object_store&lt;/code>: Where the data is stored.&lt;/li>
&lt;li>&lt;code>schema&lt;/code>: Schema version which should be used (v11 is currently recommended).&lt;/li>
&lt;li>&lt;code>index.prefix&lt;/code>: The prefix for the index.&lt;/li>
&lt;li>&lt;code>index.period&lt;/code>: The period for updating the indices.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Adding a new index happens with new config block definition. The &lt;code>from&lt;/code> field should start from the current day + previous &lt;code>index.period&lt;/code> and should not overlap with the current index. The &lt;code>prefix&lt;/code> also should be different.&lt;/strong>&lt;/p>
&lt;pre tabindex="0">&lt;code> schema_config:
configs:
- from: 2018-04-15
store: boltdb
object_store: filesystem
schema: v11
index:
prefix: index_
period: 24h
- from: 2020-06-18
store: boltdb
object_store: filesystem
schema: v11
index:
prefix: index_new_
period: 24h
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>chunk_store_config Configuration&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code> chunk_store_config:
max_look_back_period: 336h
&lt;/code>&lt;/pre>&lt;p>&lt;strong>&lt;code>chunk_store_config.max_look_back_period&lt;/code> should be the same as the &lt;code>retention_period&lt;/code>&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>table_manager Configuration&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code> table_manager:
retention_deletes_enabled: true
retention_period: 336h
&lt;/code>&lt;/pre>&lt;p>&lt;code>table_manager.retention_period&lt;/code> is the living time for each log message. Vali will keep messages for (&lt;code>table_manager.retention_period&lt;/code> - &lt;code>index.period&lt;/code>) time due to specification in the Vali implementation.&lt;/p>
&lt;h3 id="plutono">Plutono&lt;/h3>
&lt;p>This is the Vali configuration that Plutono uses:&lt;/p>
&lt;pre tabindex="0">&lt;code> - name: vali
type: vali
access: proxy
url: http://logging.{{ .Release.Namespace }}.svc:3100
jsonData:
maxLines: 5000
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>&lt;code>name&lt;/code>: Is the name of the datasource.&lt;/li>
&lt;li>&lt;code>type&lt;/code>: Is the type of the datasource.&lt;/li>
&lt;li>&lt;code>access&lt;/code>: Should be set to proxy.&lt;/li>
&lt;li>&lt;code>url&lt;/code>: Vali&amp;rsquo;s url&lt;/li>
&lt;li>&lt;code>svc&lt;/code>: Vali&amp;rsquo;s port&lt;/li>
&lt;li>&lt;code>jsonData.maxLines&lt;/code>: The limit of the log messages which Plutono will show to the users.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Decrease this value if the browser works slowly!&lt;/strong>&lt;/p></description></item><item><title>Docs: Logging Development</title><link>https://gardener.cloud/docs/gardener/logging-development/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/logging-development/</guid><description>
&lt;h1 id="logging-in-gardener-components">Logging in Gardener Components&lt;/h1>
&lt;p>This document aims at providing a general developer guideline on different aspects of logging practices and conventions used in the Gardener codebase.
It contains mostly Gardener-specific points, and references other existing and commonly accepted logging guidelines for general advice.
Developers and reviewers should consult this guide when writing, refactoring, and reviewing Gardener code.
If parts are unclear or new learnings arise, this guide should be adapted accordingly.&lt;/p>
&lt;h2 id="logging-libraries--implementations">Logging Libraries / Implementations&lt;/h2>
&lt;p>Historically, Gardener components have been using &lt;a href="https://github.com/sirupsen/logrus">logrus&lt;/a>.
There is a global logrus logger (&lt;a href="https://github.com/gardener/gardener/blob/626ba7c10e1150819b3905116d3988512c18c9ee/pkg/logger/logrus.go#L28">&lt;code>logger.Logger&lt;/code>&lt;/a>) that is initialized by components on startup and used across the codebase.
In most places, it is used as a &lt;code>printf&lt;/code>-style logger and only in some instances we make use of logrus&amp;rsquo; structured logging functionality.&lt;/p>
&lt;p>In the process of migrating our components to native controller-runtime components (see &lt;a href="https://github.com/gardener/gardener/issues/4251">gardener/gardener#4251&lt;/a>), we also want to make use of controller-runtime&amp;rsquo;s built-in mechanisms for streamlined logging.
controller-runtime uses &lt;a href="https://github.com/go-logr/logr">logr&lt;/a>, a simple structured logging interface, for library-internal logging and logging in controllers.&lt;/p>
&lt;p>logr itself is only an interface and doesn&amp;rsquo;t provide an implementation out of the box.
Instead, it needs to be backed by a logging implementation like &lt;a href="https://github.com/go-logr/zapr">zapr&lt;/a>. Code that uses the logr interface is thereby not tied to a specific logging implementation and makes the implementation easily exchangeable.
controller-runtime already provides a &lt;a href="https://github.com/kubernetes-sigs/controller-runtime/tree/v0.11.0/pkg/log/zap">set of helpers&lt;/a> for constructing zapr loggers, i.e., logr loggers backed by &lt;a href="https://github.com/uber-go/zap">zap&lt;/a>, which is a popular logging library in the go community.
Hence, we are migrating our component logging from logrus to logr (backed by zap) as part of &lt;a href="https://github.com/gardener/gardener/issues/4251">gardener/gardener#4251&lt;/a>.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ &lt;code>logger.Logger&lt;/code> (logrus logger) is deprecated in Gardener and shall not be used in new code – use logr loggers when writing new code! (also see &lt;a href="https://gardener.cloud/docs/gardener/logging-development/#migration-from-logrus-to-logr">Migration from logrus to logr&lt;/a>)&lt;/p>
&lt;p>ℹ️ Don&amp;rsquo;t use zap loggers directly, always use the logr interface in order to avoid tight coupling to a specific logging implementation.&lt;/p>
&lt;/blockquote>
&lt;p>gardener-apiserver differs from the other components as it is based on the &lt;a href="https://github.com/kubernetes/apiserver">apiserver library&lt;/a> and therefore uses &lt;a href="https://github.com/kubernetes/klog">klog&lt;/a> – just like kube-apiserver.
As gardener-apiserver writes (almost) no logs in our coding (outside the apiserver library), there is currently no plan for switching the logging implementation.
Hence, the following sections focus on logging in the controller and admission components only.&lt;/p>
&lt;h2 id="logcheck-tool">&lt;code>logcheck&lt;/code> Tool&lt;/h2>
&lt;p>To ensure a smooth migration to logr and make logging in Gardener components more consistent, the &lt;a href="https://github.com/gardener/gardener/tree/master/hack/tools/logcheck">&lt;code>logcheck&lt;/code> tool&lt;/a> was added.
It enforces (parts of) this guideline and detects programmer-level errors early on in order to prevent bugs.
Please check out the &lt;a href="https://github.com/gardener/gardener/tree/master/hack/tools/logcheck">tool&amp;rsquo;s documentation&lt;/a> for a detailed description.&lt;/p>
&lt;h2 id="structured-logging">Structured Logging&lt;/h2>
&lt;p>Similar to &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md">efforts in the Kubernetes project&lt;/a>, we want to migrate our component logs to structured logging.
As motivated above, we will use the logr interface instead of klog though.&lt;/p>
&lt;p>You can read more about the motivation behind structured logging in &lt;a href="https://github.com/go-logr/logr#background">logr&amp;rsquo;s background and FAQ&lt;/a> (also see &lt;a href="http://dave.cheney.net/2015/11/05/lets-talk-about-logging">this blog post by Dave Cheney&lt;/a>).
Also, make sure to check out controller-runtime&amp;rsquo;s &lt;a href="https://github.com/kubernetes-sigs/controller-runtime/blob/v0.11.0/TMP-LOGGING.md">logging guideline&lt;/a> with specifics for projects using the library.
The following sections will focus on the most important takeaways from those guidelines and give general instructions on how to apply them to Gardener and its controller-runtime components.&lt;/p>
&lt;blockquote>
&lt;p>Note: Some parts in this guideline differ slightly from controller-runtime&amp;rsquo;s document.&lt;/p>
&lt;/blockquote>
&lt;h3 id="tldr-of-structured-logging">TL;DR of Structured Logging&lt;/h3>
&lt;p>❌ Stop using &lt;code>printf&lt;/code>-style logging:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> logger *logrus.Logger
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>logger.Infof(&lt;span style="color:#a31515">&amp;#34;Scaling deployment %s/%s to %d replicas&amp;#34;&lt;/span>, deployment.Namespace, deployment.Name, replicaCount)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>✅ Instead, write static log messages and enrich them with additional structured information in form of key-value pairs:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> logger logr.Logger
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>logger.Info(&lt;span style="color:#a31515">&amp;#34;Scaling deployment&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;deployment&amp;#34;&lt;/span>, client.ObjectKeyFromObject(deployment), &lt;span style="color:#a31515">&amp;#34;replicas&amp;#34;&lt;/span>, replicaCount)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="log-configuration">Log Configuration&lt;/h2>
&lt;p>Gardener components can be configured to either log in &lt;code>json&lt;/code> (default) or &lt;code>text&lt;/code> format:
&lt;code>json&lt;/code> format is supposed to be used in production, while &lt;code>text&lt;/code> format might be nicer for development.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span># json
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:&amp;#34;2021-12-16T08:32:21.059+0100&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Hello botanist&amp;#34;,&amp;#34;garden&amp;#34;:&amp;#34;eden&amp;#34;}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span># text
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>2021-12-16T08:32:21.059+0100 INFO Hello botanist {&amp;#34;garden&amp;#34;: &amp;#34;eden&amp;#34;}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Components can be set to one of the following log levels (with increasing verbosity): &lt;code>error&lt;/code>, &lt;code>info&lt;/code> (default), &lt;code>debug&lt;/code>.&lt;/p>
&lt;h2 id="log-levels">Log Levels&lt;/h2>
&lt;p>logr uses &lt;a href="https://github.com/go-logr/logr#why-v-levels">V-levels&lt;/a> (numbered log levels), higher V-level means higher verbosity.
V-levels are relative (in contrast to &lt;code>klog&lt;/code>&amp;rsquo;s absolute V-levels), i.e., &lt;code>V(1)&lt;/code> creates a logger, that is one level more verbose than its parent logger.&lt;/p>
&lt;p>In Gardener components, the mentioned log levels in the component config (&lt;code>error&lt;/code>, &lt;code>info&lt;/code>, &lt;code>debug&lt;/code>) map to the zap levels with the same names (see &lt;a href="https://github.com/gardener/gardener/blob/770fc01a34b70f6cb77b8cfe929d9daef0026d1c/pkg/logger/zap.go#L43-L55">here&lt;/a>).
Hence, our loggers follow the same mapping from numerical logr levels to named zap levels like described in &lt;a href="https://github.com/go-logr/zapr/tree/v1.1.0#increasing-verbosity">zapr&lt;/a>, i.e.:&lt;/p>
&lt;ul>
&lt;li>component config specifies &lt;code>debug&lt;/code> ➡️ both &lt;code>V(0)&lt;/code> and &lt;code>V(1)&lt;/code> are enabled&lt;/li>
&lt;li>component config specifies &lt;code>info&lt;/code> ➡️ &lt;code>V(0)&lt;/code> is enabled, &lt;code>V(1)&lt;/code> will not be shown&lt;/li>
&lt;li>component config specifies &lt;code>error&lt;/code> ➡️ neither &lt;code>V(0)&lt;/code> nor &lt;code>V(1)&lt;/code> will be shown&lt;/li>
&lt;li>&lt;code>Error()&lt;/code> logs will always be shown&lt;/li>
&lt;/ul>
&lt;p>This mapping applies to the components&amp;rsquo; root loggers (the ones that are not &amp;ldquo;derived&amp;rdquo; from any other logger; constructed on component startup).
If you derive a new logger with e.g. &lt;code>V(1)&lt;/code>, the mapping will shift by one. For example, &lt;code>V(0)&lt;/code> will then log at zap&amp;rsquo;s &lt;code>debug&lt;/code> level.&lt;/p>
&lt;p>There is no &lt;code>warning&lt;/code> level (see &lt;a href="https://dave.cheney.net/2015/11/05/lets-talk-about-logging">Dave Cheney&amp;rsquo;s post&lt;/a>).
If there is an error condition (e.g., unexpected error received from a called function), the error should either be handled or logged at &lt;code>error&lt;/code> if it is neither handled nor returned.
If you have an &lt;code>error&lt;/code> value at hand that doesn&amp;rsquo;t represent an actual error condition, but you still want to log it as an informational message, log it at &lt;code>info&lt;/code> level with key &lt;code>err&lt;/code>.&lt;/p>
&lt;p>We might consider to make use of a broader range of log levels in the future when introducing more logs and common command line flags for our components (comparable to &lt;code>--v&lt;/code> of Kubernetes components).
For now, we stick to the mentioned two log levels like controller-runtime: info (&lt;code>V(0)&lt;/code>) and debug (&lt;code>V(1)&lt;/code>).&lt;/p>
&lt;h2 id="logging-in-controllers">Logging in Controllers&lt;/h2>
&lt;h3 id="named-loggers">Named Loggers&lt;/h3>
&lt;p>Controllers should use named loggers that include their name, e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>controllerLogger := rootLogger.WithName(&lt;span style="color:#a31515">&amp;#34;controller&amp;#34;&lt;/span>).WithName(&lt;span style="color:#a31515">&amp;#34;shoot&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>controllerLogger.Info(&lt;span style="color:#a31515">&amp;#34;Deploying kube-apiserver&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>results in&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>2021-12-16T09:27:56.550+0100 INFO controller.shoot Deploying kube-apiserver
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Logger names are hierarchical. You can make use of it, where controllers are composed of multiple &amp;ldquo;subcontrollers&amp;rdquo;, e.g., &lt;code>controller.shoot.hibernation&lt;/code> or &lt;code>controller.shoot.maintenance&lt;/code>.&lt;/p>
&lt;p>Using the global logger &lt;code>logf.Log&lt;/code> directly is discouraged and should be rather exceptional because it makes correlating logs with code harder.
Preferably, all parts of the code should use some named logger.&lt;/p>
&lt;h3 id="reconciler-loggers">Reconciler Loggers&lt;/h3>
&lt;p>In your &lt;code>Reconcile&lt;/code> function, retrieve a logger from the given &lt;code>context.Context&lt;/code>.
It inherits from the controller&amp;rsquo;s logger (i.e., is already named) and is preconfigured with &lt;code>name&lt;/code> and &lt;code>namespace&lt;/code> values for the reconciliation request:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">func&lt;/span> (r *reconciler) Reconcile(ctx context.Context, request reconcile.Request) (reconcile.Result, &lt;span style="color:#2b91af">error&lt;/span>) {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> log := logf.FromContext(ctx)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> log.Info(&lt;span style="color:#a31515">&amp;#34;Reconciling Shoot&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> &lt;span style="color:#00f">return&lt;/span> reconcile.Result{}, &lt;span style="color:#00f">nil&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>results in&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>2021-12-16T09:35:59.099+0100 INFO controller.shoot Reconciling Shoot {&amp;#34;name&amp;#34;: &amp;#34;sunflower&amp;#34;, &amp;#34;namespace&amp;#34;: &amp;#34;garden-greenhouse&amp;#34;}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The logger is injected by controller-runtime&amp;rsquo;s &lt;code>Controller&lt;/code> implementation. The logger returned by &lt;code>logf.FromContext&lt;/code> is never &lt;code>nil&lt;/code>. If the context doesn&amp;rsquo;t carry a logger, it falls back to the global logger (&lt;code>logf.Log&lt;/code>), which might discard logs if not configured, but is also never &lt;code>nil&lt;/code>.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ Make sure that you don&amp;rsquo;t overwrite the &lt;code>name&lt;/code> or &lt;code>namespace&lt;/code> value keys for such loggers, otherwise you will lose information about the reconciled object.&lt;/p>
&lt;/blockquote>
&lt;p>The controller implementation (controller-runtime) itself takes care of logging the error returned by reconcilers.
Hence, don&amp;rsquo;t log an error that you are returning.
Generally, functions should not return an error, if they already logged it, because that means the error is already handled and not an error anymore.
See &lt;a href="https://dave.cheney.net/2015/11/05/lets-talk-about-logging">Dave Cheney&amp;rsquo;s post&lt;/a> for more on this.&lt;/p>
&lt;h3 id="messages">Messages&lt;/h3>
&lt;ul>
&lt;li>Log messages should be static. Don&amp;rsquo;t put variable content in there, i.e., no &lt;code>fmt.Sprintf&lt;/code> or string concatenation (&lt;code>+&lt;/code>). Use key-value pairs instead.&lt;/li>
&lt;li>Log messages should be capitalized. Note: This contrasts with error messages, that should not be capitalized. However, both should not end with a punctuation mark.&lt;/li>
&lt;/ul>
&lt;h3 id="keys-and-values">Keys and Values&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Use &lt;code>WithValues&lt;/code> instead of repeatedly adding key-value pairs for multiple log statements. &lt;code>WithValues&lt;/code> creates a new logger from the parent, that carries the given key-value pairs. E.g., use it when acting on one object in multiple steps and logging something for each step:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>log := parentLog.WithValues(&lt;span style="color:#a31515">&amp;#34;infrastructure&amp;#34;&lt;/span>, client.ObjectKeyFromObject(infrastrucutre))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>log.Info(&lt;span style="color:#a31515">&amp;#34;Creating Infrastructure&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>log.Info(&lt;span style="color:#a31515">&amp;#34;Waiting for Infrastructure to be reconciled&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Note: &lt;code>WithValues&lt;/code> bypasses controller-runtime&amp;rsquo;s special zap encoder that nicely encodes &lt;code>ObjectKey&lt;/code>/&lt;code>NamespacedName&lt;/code> and &lt;code>runtime.Object&lt;/code> values, see &lt;a href="https://github.com/kubernetes-sigs/controller-runtime/issues/1290">kubernetes-sigs/controller-runtime#1290&lt;/a>.
Thus, the end result might look different depending on the value and its &lt;code>Stringer&lt;/code> implementation.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>
&lt;p>Use &lt;a href="https://en.wiktionary.org/wiki/lowerCamelCase">lowerCamelCase&lt;/a> for keys. Don&amp;rsquo;t put spaces in keys, as it will make log processing with simple tools like &lt;code>jq&lt;/code> harder.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Keys should be constant, human-readable, consistent across the codebase and naturally match parts of the log message, see &lt;a href="https://github.com/go-logr/logr#how-do-i-choose-my-keys">logr guideline&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When logging object keys (name and namespace), use the object&amp;rsquo;s type as the log key and a &lt;code>client.ObjectKey&lt;/code>/&lt;code>types.NamespacedName&lt;/code> value as value, e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> deployment *appsv1.Deployment
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>log.Info(&lt;span style="color:#a31515">&amp;#34;Creating Deployment&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;deployment&amp;#34;&lt;/span>, client.ObjectKeyFromObject(deployment))
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>which results in&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:&amp;#34;2021-12-16T08:32:21.059+0100&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Creating Deployment&amp;#34;,&amp;#34;deployment&amp;#34;:{&amp;#34;name&amp;#34;: &amp;#34;bar&amp;#34;, &amp;#34;namespace&amp;#34;: &amp;#34;foo&amp;#34;}}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>There are cases where you don&amp;rsquo;t have the full object key or the object itself at hand, e.g., if an object references another object (in the same namespace) by name (think &lt;code>secretRef&lt;/code> or similar).
In such a cases, either construct the full object key including the implied namespace or log the object name under a key ending in &lt;code>Name&lt;/code>, e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">var&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">// object to reconcile
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> shoot *gardencorev1beta1.Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">// retrieved via logf.FromContext, preconfigured by controller with namespace and name of reconciliation request
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> log logr.Logger
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// option a: full object key, manually constructed
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>log.Info(&lt;span style="color:#a31515">&amp;#34;Shoot uses SecretBinding&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;secretBinding&amp;#34;&lt;/span>, client.ObjectKey{Namespace: shoot.Namespace, Name: *shoot.Spec.SecretBindingName})
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// option b: only name under respective *Name log key
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span>log.Info(&lt;span style="color:#a31515">&amp;#34;Shoot uses SecretBinding&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;secretBindingName&amp;#34;&lt;/span>, *shoot.Spec.SecretBindingName)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Both options result in well-structured logs, that are easy to interpret and process:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:&amp;#34;2022-01-18T18:00:56.672+0100&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Shoot uses SecretBinding&amp;#34;,&amp;#34;name&amp;#34;:&amp;#34;my-shoot&amp;#34;,&amp;#34;namespace&amp;#34;:&amp;#34;garden-project&amp;#34;,&amp;#34;secretBinding&amp;#34;:{&amp;#34;namespace&amp;#34;:&amp;#34;garden-project&amp;#34;,&amp;#34;name&amp;#34;:&amp;#34;aws&amp;#34;}}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:&amp;#34;2022-01-18T18:00:56.673+0100&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Shoot uses SecretBinding&amp;#34;,&amp;#34;name&amp;#34;:&amp;#34;my-shoot&amp;#34;,&amp;#34;namespace&amp;#34;:&amp;#34;garden-project&amp;#34;,&amp;#34;secretBindingName&amp;#34;:&amp;#34;aws&amp;#34;}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>When handling generic &lt;code>client.Object&lt;/code> values (e.g. in helper funcs), use &lt;code>object&lt;/code> as key.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When adding timestamps to key-value pairs, use &lt;code>time.Time&lt;/code> values. By this, they will be encoded in the same format as the log entry&amp;rsquo;s timestamp.&lt;br>
Don&amp;rsquo;t use &lt;code>metav1.Time&lt;/code> values, as they will be encoded in a different format by their &lt;code>Stringer&lt;/code> implementation. Pass &lt;code>&amp;lt;someTimestamp&amp;gt;.Time&lt;/code> to loggers in case you have a &lt;code>metav1.Time&lt;/code> value at hand.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Same applies to durations. Use &lt;code>time.Duration&lt;/code> values instead of &lt;code>*metav1.Duration&lt;/code>. Durations can be handled specially by zap just like timestamps.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Event recorders not only create &lt;code>Event&lt;/code> objects but also log them.
However, both Gardener&amp;rsquo;s manually instantiated event recorders and the ones that controller-runtime provides log to &lt;code>debug&lt;/code> level and use generic formats, that are not very easy to interpret or process (no structured logs).
Hence, don&amp;rsquo;t use event recorders as replacements for well-structured logs.
If a controller records an event for a completed action or important information, it should probably log it as well, e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>log.Info(&lt;span style="color:#a31515">&amp;#34;Creating ManagedSeed&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;replica&amp;#34;&lt;/span>, r.GetObjectKey())
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>a.recorder.Eventf(managedSeedSet, corev1.EventTypeNormal, EventCreatingManagedSeed, &lt;span style="color:#a31515">&amp;#34;Creating ManagedSeed %s&amp;#34;&lt;/span>, r.GetFullName())
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;h2 id="logging-in-test-code">Logging in Test Code&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>If the tested production code requires a logger, you can pass &lt;code>logr.Discard()&lt;/code> or &lt;code>logf.NullLogger{}&lt;/code> in your test, which simply discards all logs.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>logf.Log&lt;/code> is safe to use in tests and will not cause a nil pointer deref, even if it&amp;rsquo;s not initialized via &lt;code>logf.SetLogger&lt;/code>.
It is initially set to a &lt;code>NullLogger&lt;/code> by default, which means all logs are discarded, unless &lt;code>logf.SetLogger&lt;/code> is called in the first 30 seconds of execution.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Pass &lt;code>zap.WriteTo(GinkgoWriter)&lt;/code> in tests where you want to see the logs on test failure but not on success, for example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>logf.SetLogger(logger.MustNewZapLogger(logger.DebugLevel, logger.FormatJSON, zap.WriteTo(GinkgoWriter)))
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>log := logf.Log.WithName(&lt;span style="color:#a31515">&amp;#34;test&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Managed Seed</title><link>https://gardener.cloud/docs/gardener/managed_seed/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/managed_seed/</guid><description>
&lt;h1 id="managedseeds-register-shoot-as-seed">&lt;code>ManagedSeed&lt;/code>s: Register Shoot as Seed&lt;/h1>
&lt;p>An existing shoot can be registered as a seed by creating a &lt;code>ManagedSeed&lt;/code> resource. This resource contains:&lt;/p>
&lt;ul>
&lt;li>The name of the shoot that should be registered as seed.&lt;/li>
&lt;li>A &lt;code>gardenlet&lt;/code> section that contains:
&lt;ul>
&lt;li>&lt;code>gardenlet&lt;/code> deployment parameters, such as the number of replicas, the image, etc.&lt;/li>
&lt;li>The &lt;code>GardenletConfiguration&lt;/code> resource that contains controllers configuration, feature gates, and a &lt;code>seedConfig&lt;/code> section that contains the &lt;code>Seed&lt;/code> spec and parts of its metadata.&lt;/li>
&lt;li>Additional configuration parameters, such as the garden connection bootstrap mechanism (see &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/#tls-bootstrapping">TLS Bootstrapping&lt;/a>), and whether to merge the provided configuration with the configuration of the parent &lt;code>gardenlet&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;code>gardenlet&lt;/code> is deployed to the shoot, and it registers a new seed upon startup based on the &lt;code>seedConfig&lt;/code> section.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> Earlier Gardener allowed specifying a &lt;code>seedTemplate&lt;/code> directly in the &lt;code>ManagedSeed&lt;/code> resource. This feature is discontinued, any seed configuration must be via the &lt;code>GardenletConfiguration&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;p>Note the following important aspects:&lt;/p>
&lt;ul>
&lt;li>Unlike the &lt;code>Seed&lt;/code> resource, the &lt;code>ManagedSeed&lt;/code> resource is namespaced. Currently, managed seeds are restricted to the &lt;code>garden&lt;/code> namespace.&lt;/li>
&lt;li>The newly created &lt;code>Seed&lt;/code> resource always has the same name as the &lt;code>ManagedSeed&lt;/code> resource. Attempting to specify a different name in the &lt;code>seedConfig&lt;/code> will fail.&lt;/li>
&lt;li>The &lt;code>ManagedSeed&lt;/code> resource must always refer to an existing shoot. Attempting to create a &lt;code>ManagedSeed&lt;/code> referring to a non-existing shoot will fail.&lt;/li>
&lt;li>A shoot that is being referred to by a &lt;code>ManagedSeed&lt;/code> cannot be deleted. Attempting to delete such a shoot will fail.&lt;/li>
&lt;li>You can omit practically everything from the &lt;code>gardenlet&lt;/code> section, including all or most of the &lt;code>Seed&lt;/code> spec fields. Proper defaults will be supplied in all cases, based either on the most common use cases or the information already available in the &lt;code>Shoot&lt;/code> resource.&lt;/li>
&lt;li>Also, if your seed is configured to host HA shoot control planes, then &lt;code>gardenlet&lt;/code> will be deployed with multiple replicas across nodes or availability zones by default.&lt;/li>
&lt;li>Some &lt;code>Seed&lt;/code> spec fields, for example the provider type and region, networking CIDRs for pods, services, and nodes, etc., must be the same as the corresponding &lt;code>Shoot&lt;/code> spec fields of the shoot that is being registered as seed. Attempting to use different values (except empty ones, so that they are supplied by the defaulting mechanims) will fail.&lt;/li>
&lt;/ul>
&lt;h2 id="deploying-gardenlet-to-the-shoot">Deploying gardenlet to the Shoot&lt;/h2>
&lt;p>To register a shoot as a seed and deploy &lt;code>gardenlet&lt;/code> to the shoot using a default configuration, create a &lt;code>ManagedSeed&lt;/code> resource similar to the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: seedmanagement.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ManagedSeed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-managed-seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shoot:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: crazy-botany
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> gardenlet: {}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For an example that uses non-default configuration, see &lt;a href="https://github.com/gardener/gardener/blob/master/example/55-managedseed-gardenlet.yaml">55-managed-seed-gardenlet.yaml&lt;/a>&lt;/p>
&lt;h3 id="renewing-the-gardenlet-kubeconfig-secret">Renewing the Gardenlet Kubeconfig Secret&lt;/h3>
&lt;p>In order to make the &lt;code>ManagedSeed&lt;/code> controller renew the gardenlet&amp;rsquo;s kubeconfig secret, annotate the &lt;code>ManagedSeed&lt;/code> with &lt;code>gardener.cloud/operation=renew-kubeconfig&lt;/code>. This will trigger a reconciliation during which the kubeconfig secret is deleted and the bootstrapping is performed again (during which gardenlet obtains a new client certificate).&lt;/p>
&lt;p>It is also possible to trigger the renewal on the secret directly, see &lt;a href="https://gardener.cloud/docs/gardener/concepts/gardenlet/#rotate-certificates-using-bootstrap-kubeconfig">Rotate Certificates Using Bootstrap kubeconfig&lt;/a>.&lt;/p>
&lt;h3 id="specifying-apiserver-replicas-and-autoscaler-options">Specifying &lt;code>apiServer&lt;/code> &lt;code>replicas&lt;/code> and &lt;code>autoscaler&lt;/code> Options&lt;/h3>
&lt;p>There are few configuration options that are not supported in a &lt;code>Shoot&lt;/code> resource but due to backward compatibility reasons it is possible to specify them for a &lt;code>Shoot&lt;/code> that is referred by a &lt;code>ManagedSeed&lt;/code>. These options are:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Option&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>apiServer.autoscaler.minReplicas&lt;/code>&lt;/td>
&lt;td>Controls the minimum number of &lt;code>kube-apiserver&lt;/code> replicas for the shoot registered as seed cluster.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>apiServer.autoscaler.maxReplicas&lt;/code>&lt;/td>
&lt;td>Controls the maximum number of &lt;code>kube-apiserver&lt;/code> replicas for the shoot registered as seed cluster.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>apiServer.replicas&lt;/code>&lt;/td>
&lt;td>Controls how many &lt;code>kube-apiserver&lt;/code> replicas the shoot registered as seed cluster gets by default.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>It is possible to specify these options via the &lt;code>shoot.gardener.cloud/managed-seed-api-server&lt;/code> annotation on the Shoot resource. Example configuration:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shoot.gardener.cloud/managed-seed-api-server: &lt;span style="color:#a31515">&amp;#34;apiServer.replicas=3,apiServer.autoscaler.minReplicas=3,apiServer.autoscaler.maxReplicas=6&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="enforced-configuration-options">Enforced Configuration Options&lt;/h3>
&lt;p>The following configuration options are enforced by Gardener API server for the ManagedSeed resources:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>The vertical pod autoscaler should be enabled from the Shoot specification.&lt;/p>
&lt;p>The vertical pod autoscaler is a prerequisite for a Seed cluster. It is possible to enable the VPA feature for a Seed &lt;a href="https://gardener.cloud/docs/gardener/seed_settings/#vertical-pod-autoscaler">(using the Seed spec)&lt;/a> and for a Shoot &lt;a href="https://gardener.cloud/docs/gardener/shoot_autoscaling/#vertical-pod-auto-scaling">(using the Shoot spec)&lt;/a>. In context of &lt;code>ManagedSeed&lt;/code>s, enabling the VPA in the Seed spec (instead of the Shoot spec) offers less flexibility and increases the network transfer and cost. Due to these reasons, the Gardener API server enforces the vertical pod autoscaler to be enabled from the Shoot specification.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The nginx-ingress addon should not be enabled for a Shoot referred by a ManagedSeed.&lt;/p>
&lt;p>An Ingress controller is also a prerequisite for a Seed cluster. For a Seed cluster, it is possible to enable Gardener managed Ingress controller or to deploy self-managed Ingress controller. There is also the nginx-ingress addon that can be enabled for a Shoot (using the Shoot spec). However, the Shoot nginx-ingress addon is in deprecated mode and it is not recommended for production clusters. Due to these reasons, the Gardener API server does not allow the Shoot nginx-ingress addon to be enabled for ManagedSeeds.&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Docs: Monitoring Stack</title><link>https://gardener.cloud/docs/gardener/monitoring-stack/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/monitoring-stack/</guid><description>
&lt;h1 id="extending-the-monitoring-stack">Extending the Monitoring Stack&lt;/h1>
&lt;p>This document provides instructions to extend the Shoot cluster monitoring stack by integrating new scrape targets, alerts and dashboards.&lt;/p>
&lt;p>Please ensure that you have understood the basic principles of &lt;a href="https://prometheus.io/docs/introduction/overview/">Prometheus&lt;/a> and its ecosystem before you continue.&lt;/p>
&lt;p>‼️ &lt;strong>The purpose of the monitoring stack is to observe the behaviour of the control plane and the system components deployed by Gardener onto the worker nodes. Monitoring of custom workloads running in the cluster is out of scope.&lt;/strong>&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/monitoring-architecture_cd945d.png" alt="Monitoring Architecture">&lt;/p>
&lt;p>Each Shoot cluster comes with its own monitoring stack. The following components are deployed into the seed and shoot:&lt;/p>
&lt;ul>
&lt;li>Seed
&lt;ul>
&lt;li>&lt;a href="https://github.com/prometheus/prometheus">Prometheus&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/credativ/plutono">Plutono&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/prometheus/blackbox_exporter">blackbox-exporter&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics&lt;/a> (Seed metrics)&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics&lt;/a> (Shoot metrics)&lt;/li>
&lt;li>&lt;a href="https://github.com/prometheus/alertmanager">Alertmanager&lt;/a> (Optional)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Shoot
&lt;ul>
&lt;li>&lt;a href="https://github.com/prometheus/node_exporter">node-exporter(s)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/prometheus/blackbox_exporter">blackbox-exporter&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>In each Seed cluster there is a Prometheus in the &lt;code>garden&lt;/code> namespace responsible for collecting metrics from the Seed kubelets and cAdvisors. These metrics are provided to each Shoot Prometheus via federation.&lt;/p>
&lt;p>The alerts for all Shoot clusters hosted on a Seed are routed to a central Alertmanger running in the &lt;code>garden&lt;/code> namespace of the Seed. The purpose of this central Alertmanager is to forward all important alerts to the operators of the Gardener setup.&lt;/p>
&lt;p>The Alertmanager in the Shoot namespace on the Seed is only responsible for forwarding alerts from its Shoot cluster to a cluster owner/cluster alert receiver via email. The Alertmanager is optional and the conditions for a deployment are already described in &lt;a href="https://gardener.cloud/docs/gardener/monitoring/alerting/">Alerting&lt;/a>.&lt;/p>
&lt;p>The node-exporter&amp;rsquo;s &lt;a href="https://github.com/prometheus/node_exporter#textfile-collector">textfile collector&lt;/a> is enabled and configured to parse all &lt;code>*.prom&lt;/code> files in the &lt;code>/var/lib/node-exporter/textfile-collector&lt;/code> directory on each Shoot node. Scripts and programs which run on Shoot nodes and cannot expose an endpoint to be scraped by prometheus can use this directory to export metrics in files that match the glob &lt;code>*.prom&lt;/code> using the &lt;a href="https://prometheus.io/docs/instrumenting/exposition_formats/">text format&lt;/a>.&lt;/p>
&lt;h2 id="adding-new-monitoring-targets">Adding New Monitoring Targets&lt;/h2>
&lt;p>After exploring the metrics which your component provides or adding new metrics, you should be aware which metrics are required to write the needed alerts and dashboards.&lt;/p>
&lt;p>Prometheus prefers a pull based metrics collection approach and therefore the targets to observe need to be defined upfront. The targets are defined in &lt;code>charts/seed-monitoring/charts/core/charts/prometheus/templates/config.yaml&lt;/code>.
New scrape jobs can be added in the section &lt;code>scrape_configs&lt;/code>. Detailed information how to configure scrape jobs and how to use the kubernetes service discovery are available in the &lt;a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config">Prometheus documentation&lt;/a>.&lt;/p>
&lt;p>The &lt;code>job_name&lt;/code> of a scrape job should be the name of the component e.g. &lt;code>kube-apiserver&lt;/code> or &lt;code>vpn&lt;/code>. The collection interval should be the default of &lt;code>30s&lt;/code>. You do not need to specify this in the configuration.&lt;/p>
&lt;p>Please do not ingest all metrics which are provided by a component. Rather, collect only those metrics which are needed to define the alerts and dashboards (i.e. whitelist). This can be achieved by adding the following &lt;code>metric_relabel_configs&lt;/code> statement to your scrape jobs (replace &lt;code>exampleComponent&lt;/code> with component name).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span> - job_name: example-component
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metric_relabel_configs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{{ include &amp;#34;prometheus.keep-metrics.metric-relabel-config&amp;#34; .Values.allowedMetrics.exampleComponent | indent 6 }}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The whitelist for the metrics of your job can be maintained in &lt;code>charts/seed-monitoring/charts/core/charts/prometheus/values.yaml&lt;/code> in section &lt;code>allowedMetrics.exampleComponent&lt;/code> (replace &lt;code>exampleComponent&lt;/code> with component name). Check the following example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>allowedMetrics:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exampleComponent:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> * metrics_name_1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> * metrics_name_2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="adding-alerts">Adding Alerts&lt;/h2>
&lt;p>The alert definitons are located in &lt;code>charts/seed-monitoring/charts/core/charts/prometheus/rules&lt;/code>. There are two approaches for adding new alerts.&lt;/p>
&lt;ol>
&lt;li>Adding additional alerts for a component which already has a set of alerts. In this case you have to extend the existing rule file for the component.&lt;/li>
&lt;li>Adding alerts for a new component. In this case a new rule file with name scheme &lt;code>example-component.rules.yaml&lt;/code> needs to be added.&lt;/li>
&lt;li>Add the new alert to &lt;code>alertInhibitionGraph.dot&lt;/code>, add any required inhibition flows and render the new graph. To render the graph, run:&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>dot -Tpng ./content/alertInhibitionGraph.dot -o ./content/alertInhibitionGraph.png
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>Create a test for the new alert. See &lt;code>Alert Tests&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>Example alert:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>groups:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>* name: example.rules
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> * alert: ExampleAlert
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expr: absent(up{job=&amp;#34;exampleJob&amp;#34;} == 1)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> for: 20m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service: example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> severity: critical &lt;span style="color:#008000"># How severe is the alert? (blocker|critical|info|warning)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: shoot &lt;span style="color:#008000"># For which topology is the alert relevant? (seed|shoot)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> visibility: all &lt;span style="color:#008000"># Who should receive the alerts? (all|operator|owner)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> description: A longer description of the example alert that should also explain the impact of the alert.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> summary: Short summary of an example alert.
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If the deployment of component is optional then the alert definitions needs to be added to &lt;code>charts/seed-monitoring/charts/core/charts/prometheus/optional-rules&lt;/code> instead. Furthermore the alerts for component need to be activatable in &lt;code>charts/seed-monitoring/charts/core/charts/prometheus/values.yaml&lt;/code> via &lt;code>rules.optional.example-component.enabled&lt;/code>. The default should be &lt;code>true&lt;/code>.&lt;/p>
&lt;p>Basic instruction how to define alert rules can be found in the &lt;a href="https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules">Prometheus documentation&lt;/a>.&lt;/p>
&lt;h3 id="routing-tree">Routing Tree&lt;/h3>
&lt;p>The Alertmanager is grouping incoming alerts based on labels into buckets. Each bucket has its own configuration like alert receivers, initial delaying duration or resending frequency, etc. You can find more information about Alertmanager routing in the &lt;a href="https://prometheus.io/docs/alerting/configuration/#route">Prometheus/Alertmanager documentation&lt;/a>. The routing trees for the Alertmanagers deployed by Gardener are depicted below.&lt;/p>
&lt;p>Central Seed Alertmanager&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>∟ main route (all alerts for all shoots on the seed will enter)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by project and shoot name
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by visibility &amp;#34;all&amp;#34; and &amp;#34;operator&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by severity &amp;#34;blocker&amp;#34;, &amp;#34;critical&amp;#34;, and &amp;#34;info&amp;#34; → route to Garden operators
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by severity &amp;#34;warning&amp;#34; (dropped)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by visibility &amp;#34;owner&amp;#34; (dropped)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Shoot Alertmanager&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>∟ main route (only alerts for one Shoot will enter)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by visibility &amp;#34;all&amp;#34; and &amp;#34;owner&amp;#34;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by severity &amp;#34;blocker&amp;#34;, &amp;#34;critical&amp;#34;, and &amp;#34;info&amp;#34; → route to cluster alert receiver
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by severity &amp;#34;warning&amp;#34; (dropped, will change soon → route to cluster alert receiver)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ∟ group by visibility &amp;#34;operator&amp;#34; (dropped)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="alert-inhibition">Alert Inhibition&lt;/h3>
&lt;p>All alerts related to components running on the Shoot workers are inhibited in case of an issue with the vpn connection, because those components can&amp;rsquo;t be scraped anymore and Prometheus will fire alerts in consequence. The components running on the workers are probably healthy and the alerts are presumably false positives. The inhibition flow is shown in the figure below. If you add a new alert, make sure to add it to the diagram.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/alertInhibitionGraph_ceaef0.png" alt="alertDiagram">&lt;/p>
&lt;h3 id="alert-attributes">Alert Attributes&lt;/h3>
&lt;p>Each alert rule definition has to contain the following annotations:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>summary&lt;/strong>: A short description of the issue.&lt;/li>
&lt;li>&lt;strong>description&lt;/strong>: A detailed explanation of the issue with hints to the possible root causes and the impact assessment of the issue.&lt;/li>
&lt;/ul>
&lt;p>In addition, each alert must contain the following labels:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>type&lt;/strong>
&lt;ul>
&lt;li>&lt;code>shoot&lt;/code>: Components running on the Shoot worker nodes in the &lt;code>kube-system&lt;/code> namespace.&lt;/li>
&lt;li>&lt;code>seed&lt;/code>: Components running on the Seed in the Shoot namespace as part of/next to the control plane.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>service&lt;/strong>
&lt;ul>
&lt;li>Name of the component (in lowercase) e.g. &lt;code>kube-apiserver&lt;/code>, &lt;code>alertmanager&lt;/code> or &lt;code>vpn&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>severity&lt;/strong>
&lt;ul>
&lt;li>&lt;code>blocker&lt;/code>: All issues which make the cluster entirely unusable, e.g. &lt;code>KubeAPIServerDown&lt;/code> or &lt;code>KubeSchedulerDown&lt;/code>&lt;/li>
&lt;li>&lt;code>critical&lt;/code>: All issues which affect single functionalities/components but do not affect the cluster in its core functionality e.g. &lt;code>VPNDown&lt;/code> or &lt;code>KubeletDown&lt;/code>.&lt;/li>
&lt;li>&lt;code>info&lt;/code>: All issues that do not affect the cluster or its core functionality, but if this component is down we cannot determine if a blocker alert is firing. (i.e. A component with an info level severity is a dependency for a component with a blocker severity)&lt;/li>
&lt;li>&lt;code>warning&lt;/code>: No current existing issue, rather a hint for situations which could lead to real issue in the close future e.g. &lt;code>HighLatencyApiServerToWorkers&lt;/code> or &lt;code>ApiServerResponseSlow&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="adding-plutono-dashboards">Adding Plutono Dashboards&lt;/h2>
&lt;p>The dashboard definition files are located in &lt;code>charts/seed-monitoring/charts/plutono/dashboards&lt;/code>. Every dashboard needs its own file.&lt;/p>
&lt;p>If you are adding a new component dashboard please also update the overview dashboard by adding a chart for its current up/down status and with a drill down option to the component dashboard.&lt;/p>
&lt;h3 id="dashboard-structure">Dashboard Structure&lt;/h3>
&lt;p>The dashboards should be structured in the following way. The assignment of the component dashboards to the categories should be handled via dashboard tags.&lt;/p>
&lt;ul>
&lt;li>Kubernetes control plane components (Tag: &lt;code>control-plane&lt;/code>)
&lt;ul>
&lt;li>All components which are part of the Kubernetes control plane e. g. Kube API Server, Kube Controller Manager, Kube Scheduler and Cloud Controller Manager&lt;/li>
&lt;li>ETCD + Backup/Restore&lt;/li>
&lt;li>Kubernetes Addon Manager&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Node/Machine components (Tag: &lt;code>node/machine&lt;/code>)
&lt;ul>
&lt;li>All metrics which are related to the behaviour/control of the Kubernetes nodes and kubelets&lt;/li>
&lt;li>Machine-Controller-Manager + Cluster Autoscaler&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Networking components (Tag: &lt;code>network&lt;/code>)
&lt;ul>
&lt;li>CoreDNS, KubeProxy, Calico, VPN, Nginx Ingress&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Addon components (Tag: &lt;code>addon&lt;/code>)
&lt;ul>
&lt;li>Cert Broker&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Monitoring components (Tag: &lt;code>monitoring&lt;/code>)&lt;/li>
&lt;li>Logging components (Tag: &lt;code>logging&lt;/code>)&lt;/li>
&lt;/ul>
&lt;h4 id="mandatory-charts-for-component-dashboards">Mandatory Charts for Component Dashboards&lt;/h4>
&lt;p>For each new component, its corresponding dashboard should contain the following charts in the first row, before adding custom charts for the component in the subsequent rows.&lt;/p>
&lt;ol>
&lt;li>Pod up/down status &lt;code>up{job=&amp;quot;example-component&amp;quot;}&lt;/code>&lt;/li>
&lt;li>Pod/containers cpu utilization&lt;/li>
&lt;li>Pod/containers memory consumption&lt;/li>
&lt;li>Pod/containers network i/o&lt;/li>
&lt;/ol>
&lt;p>That information is provided by the cAdvisor metrics. These metrics are already integrated. Please check the other dashboards for detailed information on how to query.&lt;/p>
&lt;h5 id="chart-requirements">Chart Requirements&lt;/h5>
&lt;p>Each chart needs to contain:&lt;/p>
&lt;ul>
&lt;li>a meaningful name&lt;/li>
&lt;li>a detailed description (for non trivial charts)&lt;/li>
&lt;li>appropriate x/y axis descriptions&lt;/li>
&lt;li>appropriate scaling levels for the x/y axis&lt;/li>
&lt;li>proper units for the x/y axis&lt;/li>
&lt;/ul>
&lt;h5 id="dashboard-parameters">Dashboard Parameters&lt;/h5>
&lt;p>The following parameters should be added to all dashboards to ensure a homogeneous experience across all dashboards.&lt;/p>
&lt;p>Dashboards have to:&lt;/p>
&lt;ul>
&lt;li>contain a title which refers to the component name(s)&lt;/li>
&lt;li>contain a timezone statement which should be the browser time&lt;/li>
&lt;li>contain tags which express where the component is running (&lt;code>seed&lt;/code> or &lt;code>shoot&lt;/code>) and to which category the component belong (see dashboard structure)&lt;/li>
&lt;li>contain a version statement with a value of 1&lt;/li>
&lt;li>be immutable&lt;/li>
&lt;/ul>
&lt;p>Example dashboard configuration:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;title&amp;#34;: &lt;span style="color:#a31515">&amp;#34;example-component&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;timezone&amp;#34;: &lt;span style="color:#a31515">&amp;#34;utc&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;tags&amp;#34;: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;seed&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;control-plane&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;version&amp;#34;: 1,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;editable&amp;#34;: &lt;span style="color:#a31515">&amp;#34;false&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Furthermore, all dashboards should contain the following time options:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;time&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;from&amp;#34;: &lt;span style="color:#a31515">&amp;#34;now-1h&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;to&amp;#34;: &lt;span style="color:#a31515">&amp;#34;now&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;timepicker&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;refresh_intervals&amp;#34;: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;30s&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;1m&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;5m&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;time_options&amp;#34;: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;5m&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;15m&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;1h&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;6h&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;12h&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;24h&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;2d&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#a31515">&amp;#34;10d&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Necessary Labeling for Custom CSI Components</title><link>https://gardener.cloud/docs/gardener/csi_components/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/csi_components/</guid><description>
&lt;h1 id="necessary-labeling-for-custom-csi-components">Necessary Labeling for Custom CSI Components&lt;/h1>
&lt;p>Some provider extensions for Gardener are using CSI components to manage persistent volumes in the shoot clusters.
Additionally, most of the provider extensions are deploying controllers for taking volume snapshots (CSI snapshotter).&lt;/p>
&lt;p>End-users can deploy their own CSI components and controllers into shoot clusters.
In such situations, there are multiple controllers acting on the &lt;code>VolumeSnapshot&lt;/code> custom resources (each responsible for those instances associated with their respective driver provisioner types).&lt;/p>
&lt;p>However, this might lead to operational conflicts that cannot be overcome by Gardener alone.
Concretely, Gardener cannot know which custom CSI components were installed by end-users which can lead to issues, especially during shoot cluster deletion.
You can add a label to your custom CSI components indicating that Gardener should not try to remove them during shoot cluster deletion. This means you have to take care of the lifecycle for these components yourself!&lt;/p>
&lt;h2 id="recommendations">Recommendations&lt;/h2>
&lt;p>Custom CSI components are typically regular &lt;code>Deployment&lt;/code>s running in the shoot clusters.&lt;/p>
&lt;p>&lt;strong>Please label them with the &lt;code>shoot.gardener.cloud/no-cleanup=true&lt;/code> label.&lt;/strong>&lt;/p>
&lt;h2 id="background-information">Background Information&lt;/h2>
&lt;p>When a shoot cluster is deleted, Gardener deletes most Kubernetes resources (&lt;code>Deployment&lt;/code>s, &lt;code>DaemonSet&lt;/code>s, &lt;code>StatefulSet&lt;/code>s, etc.). Gardener will also try to delete CSI components if they are not marked with the above mentioned label.&lt;/p>
&lt;p>This can result in &lt;code>VolumeSnapshot&lt;/code> resources still having finalizers that will never be cleaned up.
Consequently, manual intervention is required to clean them up before the cluster deletion can continue.&lt;/p></description></item><item><title>Docs: Network Policies</title><link>https://gardener.cloud/docs/gardener/network_policies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/network_policies/</guid><description>
&lt;h1 id="networkpolicys-in-garden-seed-shoot-clusters">&lt;code>NetworkPolicy&lt;/code>s In Garden, Seed, Shoot Clusters&lt;/h1>
&lt;p>This document describes which &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Kubernetes &lt;code>NetworkPolicy&lt;/code>s&lt;/a> deployed by Gardener into the various clusters.&lt;/p>
&lt;h2 id="garden-cluster">Garden Cluster&lt;/h2>
&lt;p>&lt;em>(via &lt;code>gardener-operator&lt;/code> and &lt;code>gardener-resource-manager&lt;/code>)&lt;/em>&lt;/p>
&lt;p>The &lt;code>gardener-operator&lt;/code> runs a &lt;a href="https://gardener.cloud/docs/gardener/concepts/operator/#networkpolicy-controller-registrar">&lt;code>NetworkPolicy&lt;/code> controller&lt;/a> which is responsible for the following namespaces:&lt;/p>
&lt;ul>
&lt;li>&lt;code>garden&lt;/code>&lt;/li>
&lt;li>&lt;code>istio-system&lt;/code>&lt;/li>
&lt;li>&lt;code>*istio-ingress-*&lt;/code>&lt;/li>
&lt;li>&lt;code>shoot-*&lt;/code>&lt;/li>
&lt;li>&lt;code>extension-*&lt;/code> (in case the garden cluster is a seed cluster at the same time)&lt;/li>
&lt;/ul>
&lt;p>It deploys the following so-called &amp;ldquo;general &lt;code>NetworkPolicy&lt;/code>s&amp;rdquo;:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Purpose&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>deny-all&lt;/code>&lt;/td>
&lt;td>&lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-and-all-egress-traffic">Denies all ingress and egress traffic&lt;/a> for all pods in this namespace. Hence, all traffic must be explicitly allowed.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>allow-to-dns&lt;/code>&lt;/td>
&lt;td>Allows egress traffic from pods labeled with &lt;code>networking.gardener.cloud/to-dns=allowed&lt;/code> to DNS pods running in the &lt;code>kube-sytem&lt;/code> namespace. In practice, most of the pods performing network egress traffic need this label.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>allow-to-runtime-apiserver&lt;/code>&lt;/td>
&lt;td>Allows egress traffic from pods labeled with &lt;code>networking.gardener.cloud/to-runtime-apiserver=allowed&lt;/code> to the API server of the runtime cluster.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>allow-to-blocked-cidrs&lt;/code>&lt;/td>
&lt;td>Allows egress traffic from pods labeled with &lt;code>networking.gardener.cloud/to-blocked-cidrs=allowed&lt;/code> to explicitly blocked addresses configured by human operators (configured via &lt;code>.spec.networking.blockedCIDRs&lt;/code> in the &lt;code>Seed&lt;/code>). For instance, this can be used to block the cloud provider&amp;rsquo;s metadata service.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>allow-to-public-networks&lt;/code>&lt;/td>
&lt;td>Allows egress traffic from pods labeled with &lt;code>networking.gardener.cloud/to-public-networks=allowed&lt;/code> to all public network IPs, except for private networks (RFC1918), carrier-grade NAT (RFC6598), and explicitly blocked addresses configured by human operators for all pods labeled with &lt;code>networking.gardener.cloud/to-public-networks=allowed&lt;/code>. In practice, this blocks egress traffic to all networks in the cluster and only allows egress traffic to public IPv4 addresses.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>allow-to-private-networks&lt;/code>&lt;/td>
&lt;td>Allows egress traffic from pods labeled with &lt;code>networking.gardener.cloud/to-private-networks=allowed&lt;/code> to the private networks (RFC1918) and carrier-grade NAT (RFC6598) except for cluster-specific networks (configured via &lt;code>.spec.networks&lt;/code> in the &lt;code>Seed&lt;/code>).&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Apart from those, the &lt;code>gardener-operator&lt;/code> also enables the &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/#networkpolicy-controller">&lt;code>NetworkPolicy&lt;/code> controller of &lt;code>gardener-resource-manager&lt;/code>&lt;/a>.
Please find more information in the linked document.
In summary, most of the pods that initiate connections with other pods will have labels with &lt;code>networking.resources.gardener.cloud/&lt;/code> prefixes.
This way, they leverage the automatically created &lt;code>NetworkPolicy&lt;/code>s by the controller.
As a result, in most cases no special/custom-crafted &lt;code>NetworkPolicy&lt;/code>s must be created anymore.&lt;/p>
&lt;h2 id="seed-cluster">Seed Cluster&lt;/h2>
&lt;p>&lt;em>(via &lt;code>gardenlet&lt;/code> and &lt;code>gardener-resource-manager&lt;/code>)&lt;/em>&lt;/p>
&lt;p>In seed clusters it works the same way as in the garden cluster managed by &lt;code>gardener-operator&lt;/code>.
When a seed cluster is the garden cluster at the same time, &lt;code>gardenlet&lt;/code> does not enable the &lt;code>NetworkPolicy&lt;/code> controller (since &lt;code>gardener-operator&lt;/code> already runs it).
Otherwise, it uses the exact same controller and code like &lt;code>gardener-operator&lt;/code>, resulting in the same behaviour in both garden and seed clusters.&lt;/p>
&lt;h3 id="logging--monitoring">Logging &amp;amp; Monitoring&lt;/h3>
&lt;h4 id="seed-system-namespaces">Seed System Namespaces&lt;/h4>
&lt;p>As part of the seed reconciliation flow, the &lt;code>gardenlet&lt;/code> deploys various Prometheus instances into the &lt;code>garden&lt;/code> namespace.
See also &lt;a href="https://gardener.cloud/docs/gardener/monitoring-stack/">this document&lt;/a> for more information.
Each pod that should be scraped for metrics by these instances must have a &lt;code>Service&lt;/code> which is annotated with&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networking.resources.gardener.cloud/from-all-seed-scrape-targets-allowed-ports: &lt;span style="color:#a31515">&amp;#39;[{&amp;#34;port&amp;#34;:&amp;lt;metrics-port-on-pod&amp;gt;,&amp;#34;protocol&amp;#34;:&amp;#34;&amp;lt;protocol, typically TCP&amp;gt;&amp;#34;}]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If the respective pod is not running in the &lt;code>garden&lt;/code> namespace, the &lt;code>Service&lt;/code> needs these annotations in addition:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networking.resources.gardener.cloud/namespace-selectors: &lt;span style="color:#a31515">&amp;#39;[{&amp;#34;matchLabels&amp;#34;:{&amp;#34;kubernetes.io/metadata.name&amp;#34;:&amp;#34;garden&amp;#34;}}]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If the respective pod is running in an &lt;code>extension-*&lt;/code> namespace, the &lt;code>Service&lt;/code> needs this annotation in addition:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networking.resources.gardener.cloud/pod-label-selector-namespace-alias: extensions
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This automatically allows the needed network traffic from the respective Prometheus pods.&lt;/p>
&lt;h4 id="shoot-namespaces">Shoot Namespaces&lt;/h4>
&lt;p>As part of the shoot reconciliation flow, the &lt;code>gardenlet&lt;/code> deploys a shoot-specific Prometheus into the shoot namespace.
Each pod that should be scraped for metrics must have a &lt;code>Service&lt;/code> which is annotated with&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networking.resources.gardener.cloud/from-all-scrape-targets-allowed-ports: &lt;span style="color:#a31515">&amp;#39;[{&amp;#34;port&amp;#34;:&amp;lt;metrics-port-on-pod&amp;gt;,&amp;#34;protocol&amp;#34;:&amp;#34;&amp;lt;protocol, typically TCP&amp;gt;&amp;#34;}]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This automatically allows the network traffic from the Prometheus pod.&lt;/p>
&lt;h3 id="webhook-servers">Webhook Servers&lt;/h3>
&lt;p>Components serving webhook handlers that must be reached by &lt;code>kube-apiserver&lt;/code>s of the virtual garden cluster or shoot clusters just need to annotate their &lt;code>Service&lt;/code> as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networking.resources.gardener.cloud/from-all-webhook-targets-allowed-ports: &lt;span style="color:#a31515">&amp;#39;[{&amp;#34;port&amp;#34;:&amp;lt;server-port-on-pod&amp;gt;,&amp;#34;protocol&amp;#34;:&amp;#34;&amp;lt;protocol, typically TCP&amp;gt;&amp;#34;}]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This automatically allows the network traffic from the API server pods.&lt;/p>
&lt;p>In case the servers run in a different namespace than the &lt;code>kube-apiserver&lt;/code>s, the following annotations are needed:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networking.resources.gardener.cloud/from-all-webhook-targets-allowed-ports: &lt;span style="color:#a31515">&amp;#39;[{&amp;#34;port&amp;#34;:&amp;lt;server-port-on-pod&amp;gt;,&amp;#34;protocol&amp;#34;:&amp;#34;&amp;lt;protocol, typically TCP&amp;gt;&amp;#34;}]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networking.resources.gardener.cloud/pod-label-selector-namespace-alias: extensions
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># for the virtual garden cluster:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networking.resources.gardener.cloud/namespace-selectors: &lt;span style="color:#a31515">&amp;#39;[{&amp;#34;matchLabels&amp;#34;:{&amp;#34;kubernetes.io/metadata.name&amp;#34;:&amp;#34;garden&amp;#34;}}]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># for shoot clusters:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networking.resources.gardener.cloud/namespace-selectors: &lt;span style="color:#a31515">&amp;#39;[{&amp;#34;matchLabels&amp;#34;:{&amp;#34;gardener.cloud/role&amp;#34;:&amp;#34;shoot&amp;#34;}}]&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="additional-namespace-coverage-in-gardenseed-cluster">Additional Namespace Coverage in Garden/Seed Cluster&lt;/h2>
&lt;p>In some cases, garden or seed clusters might run components in dedicated namespaces which are not covered by the controller by default (see list above).
Still, it might(/should) be desired to also include such &amp;ldquo;custom namespaces&amp;rdquo; into the control of the &lt;code>NetworkPolicy&lt;/code> controllers.&lt;/p>
&lt;p>In order to do so, human operators can adapt the component configs of &lt;code>gardener-operator&lt;/code> or &lt;code>gardenlet&lt;/code> by providing label selectors for additional namespaces:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>controllers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> networkPolicy:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> additionalNamespaceSelectors:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> foo: bar
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="communication-with-kube-apiserver-for-components-in-custom-namespaces">Communication With &lt;code>kube-apiserver&lt;/code> For Components In Custom Namespaces&lt;/h3>
&lt;h3 id="egress-traffic">Egress Traffic&lt;/h3>
&lt;p>Component running in such custom namespaces might need to initiate the communication with the &lt;code>kube-apiserver&lt;/code>s of the virtual garden cluster or a shoot cluster.
In order to achieve this, their custom namespace must be labeled with &lt;code>networking.gardener.cloud/access-target-apiserver=allowed&lt;/code>.
This will make the &lt;code>NetworkPolicy&lt;/code> controllers automatically provisioning the required policies into their namespace.&lt;/p>
&lt;p>As a result, the respective component pods just need to be labeled with&lt;/p>
&lt;ul>
&lt;li>&lt;code>networking.resources.gardener.cloud/to-garden-virtual-garden-kube-apiserver-tcp-443=allowed&lt;/code> (virtual garden cluster)&lt;/li>
&lt;li>&lt;code>networking.resources.gardener.cloud/to-all-shoots-kube-apiserver-tcp-443=allowed&lt;/code> (shoot clusters)&lt;/li>
&lt;/ul>
&lt;h3 id="ingress-traffic">Ingress Traffic&lt;/h3>
&lt;p>Components running in such custom namespaces might serve webhook handlers that must be reached by the &lt;code>kube-apiserver&lt;/code>s of the virtual garden cluster or a shoot cluster.
In order to achieve this, their &lt;code>Service&lt;/code> must be annotated.
Please refer to &lt;a href="https://gardener.cloud/docs/gardener/network_policies/#webhook-servers">this section&lt;/a> for more information.&lt;/p>
&lt;h2 id="shoot-cluster">Shoot Cluster&lt;/h2>
&lt;p>&lt;em>(via &lt;code>gardenlet&lt;/code>)&lt;/em>&lt;/p>
&lt;p>For shoot clusters, the concepts mentioned above don&amp;rsquo;t apply and are not enabled.
Instead, &lt;code>gardenlet&lt;/code> only deploys a few &amp;ldquo;custom&amp;rdquo; &lt;code>NetworkPolicy&lt;/code>s for the shoot system components running in the &lt;code>kube-system&lt;/code> namespace.
All other namespaces in the shoot cluster do not contain network policies deployed by &lt;code>gardenlet&lt;/code>.&lt;/p>
&lt;p>As a best practice, every pod deployed into the &lt;code>kube-system&lt;/code> namespace should use appropriate &lt;code>NetworkPolicy&lt;/code> in order to only allow &lt;strong>required&lt;/strong> network traffic.
Therefore, pods should have labels matching to the selectors of the available network policies.&lt;/p>
&lt;p>&lt;code>gardenlet&lt;/code> deploys the following &lt;code>NetworkPolicy&lt;/code>s:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>NAME POD-SELECTOR
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gardener.cloud--allow-dns k8s-app in (kube-dns)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gardener.cloud--allow-from-seed networking.gardener.cloud/from-seed=allowed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gardener.cloud--allow-to-dns networking.gardener.cloud/to-dns=allowed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gardener.cloud--allow-to-apiserver networking.gardener.cloud/to-apiserver=allowed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gardener.cloud--allow-to-from-nginx app=nginx-ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gardener.cloud--allow-to-kubelet networking.gardener.cloud/to-kubelet=allowed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gardener.cloud--allow-to-public-networks networking.gardener.cloud/to-public-networks=allowed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gardener.cloud--allow-vpn app=vpn-shoot
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that a &lt;code>deny-all&lt;/code> policy will not be created by &lt;code>gardenlet&lt;/code>.
Shoot owners can create it manually if needed/desired.
Above listed &lt;code>NetworkPolicy&lt;/code>s ensure that the traffic for the shoot system components is allowed in case such &lt;code>deny-all&lt;/code> policies is created.&lt;/p>
&lt;h3 id="webhook-servers-in-shoot-clusters">Webhook Servers in Shoot Clusters&lt;/h3>
&lt;p>Shoot components serving webhook handlers must be reached by &lt;code>kube-apiserver&lt;/code>s of the shoot cluster.
However, the control plane components, e.g. &lt;code>kube-apiserver&lt;/code>, run on the seed cluster decoupled by a &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/14-reversed-cluster-vpn.md">VPN connection&lt;/a>.
Therefore, shoot components serving webhook handlers need to allow the VPN endpoints in the shoot cluster as clients to allow &lt;code>kube-apiserver&lt;/code>s to call them.&lt;/p>
&lt;p>For the &lt;code>kube-system&lt;/code> namespace, the network policy &lt;code>gardener.cloud--allow-from-seed&lt;/code> fulfils the purpose to allow pods to mark themselves as targets for such calls, allowing corresponding traffic to pass through.&lt;/p>
&lt;p>For custom namespaces, operators can use the network policy &lt;code>gardener.cloud--allow-from-seed&lt;/code> as a template.
Please note that the label selector may change over time, i.e. with Gardener version updates.
This is why a simpler variant with a reduced label selector like the example below is recommended:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: networking.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: NetworkPolicy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: allow-from-seed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: custom-namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ingress:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - from:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - namespaceSelector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> gardener.cloud/purpose: kube-system
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> podSelector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> app: vpn-shoot
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="implications-for-gardener-extensions">Implications for Gardener Extensions&lt;/h2>
&lt;p>Gardener extensions sometimes need to deploy additional components into the shoot namespace in the seed cluster hosting the control plane.
For example, the &lt;a href="https://github.com/gardener/gardener-extension-provider-aws">&lt;code>gardener-extension-provider-aws&lt;/code>&lt;/a> deploys the &lt;code>cloud-controller-manager&lt;/code> into the shoot namespace.
In most cases, such pods require network policy labels to allow the traffic they are initiating.&lt;/p>
&lt;p>For components deployed in the &lt;code>kube-system&lt;/code> namespace of the shoots (e.g., CNI plugins or CSI drivers, etc.), custom &lt;code>NetworkPolicy&lt;/code>s might be required to ensure the respective components can still communicate in case the user creates a &lt;code>deny-all&lt;/code> policy.&lt;/p></description></item><item><title>Docs: New Cloud Provider</title><link>https://gardener.cloud/docs/gardener/new-cloud-provider/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/new-cloud-provider/</guid><description>
&lt;h1 id="adding-cloud-providers">Adding Cloud Providers&lt;/h1>
&lt;p>This document provides an overview of how to integrate a new cloud provider into Gardener. Each component that requires integration has a detailed description of how to integrate it and the steps required.&lt;/p>
&lt;h2 id="cloud-components">Cloud Components&lt;/h2>
&lt;p>Gardener is composed of 2 or more Kubernetes clusters:&lt;/p>
&lt;ul>
&lt;li>Shoot: These are the end-user clusters, the regular Kubernetes clusters you have seen. They provide places for your workloads to run.&lt;/li>
&lt;li>Seed: This is the &amp;ldquo;management&amp;rdquo; cluster. It manages the control planes of shoots by running them as native Kubernetes workloads.&lt;/li>
&lt;/ul>
&lt;p>These two clusters can run in the same cloud provider, but they do not need to. For example, you could run your Seed in AWS, while having one shoot in Azure, two in Google, two in Alicloud, and three in Equinix Metal.&lt;/p>
&lt;p>The Seed cluster deploys and manages the Shoot clusters. Importantly, for this discussion, the &lt;code>etcd&lt;/code> data store backing each Shoot runs as workloads inside the Seed. Thus, to use the above example, the clusters in Azure, Google, Alicloud and Equinix Metal will have their worker nodes and master nodes running in those clouds, but the &lt;code>etcd&lt;/code> clusters backing them will run as separate &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">deployments&lt;/a> in the Seed Kubernetes cluster on AWS.&lt;/p>
&lt;p>This distinction becomes important when preparing the integration to a new cloud provider.&lt;/p>
&lt;h2 id="gardener-cloud-integration">Gardener Cloud Integration&lt;/h2>
&lt;p>Gardener and its related components integrate with cloud providers at the following key lifecycle elements:&lt;/p>
&lt;ul>
&lt;li>Create/destroy/get/list machines for the Shoot.&lt;/li>
&lt;li>Create/destroy/get/list infrastructure components for the Shoot, e.g. VPCs, subnets, routes, etc.&lt;/li>
&lt;li>Backup/restore etcd for the Seed via writing files to and reading them from object storage.&lt;/li>
&lt;/ul>
&lt;p>Thus, the integrations you need for your cloud provider depend on whether you want to deploy Shoot clusters to the provider, Seed or both.&lt;/p>
&lt;ul>
&lt;li>Shoot Only: machine lifecycle management, infrastructure&lt;/li>
&lt;li>Seed: etcd backup/restore&lt;/li>
&lt;/ul>
&lt;h2 id="gardener-api">Gardener API&lt;/h2>
&lt;p>In addition to the requirements to integrate with the cloud provider, you also need to enable the core Gardener app to receive, validate, and process requests to use that cloud provider.&lt;/p>
&lt;ul>
&lt;li>Expose the cloud provider to the consumers of the Gardener API, so it can be told to use that cloud provider as an option.&lt;/li>
&lt;li>Validate that API as requests come in.&lt;/li>
&lt;li>Write cloud provider specific implementation (called &amp;ldquo;provider extension&amp;rdquo;).&lt;/li>
&lt;/ul>
&lt;h2 id="cloud-provider-api-requirements">Cloud Provider API Requirements&lt;/h2>
&lt;p>In order for a cloud provider to integrate with Gardener, the provider must have an API to perform machine lifecycle events, specifically:&lt;/p>
&lt;ul>
&lt;li>Create a machine&lt;/li>
&lt;li>Destroy a machine&lt;/li>
&lt;li>Get information about a machine and its state&lt;/li>
&lt;li>List machines&lt;/li>
&lt;/ul>
&lt;p>In addition, if the Seed is to run on the given provider, it also must have an API to save files to block storage and retrieve them, for etcd backup/restore.&lt;/p>
&lt;p>The current integration with cloud providers is to add their API calls to Gardener and the Machine Controller Manager. As both Gardener and the Machine Controller Manager are written in &lt;a href="https://golang.org">go&lt;/a>, the cloud provider should have a go SDK. However, if it has an API that is wrappable in go, e.g. a REST API, then you can use that to integrate.&lt;/p>
&lt;p>The Gardener team is working on bringing cloud provider integrations out-of-tree, making them plugable, which should simplify the process and make it possible to use other SDKs.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>To add a new cloud provider, you need some or all of the following. Each repository contains instructions on how to extend it to a new cloud provider.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Type&lt;/th>
&lt;th>Purpose&lt;/th>
&lt;th>Location&lt;/th>
&lt;th>Documentation&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Seed or Shoot&lt;/td>
&lt;td>Machine Lifecycle&lt;/td>
&lt;td>&lt;a href="https://github.com/gardener/machine-controller-manager">machine-controller-manager&lt;/a>&lt;/td>
&lt;td>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/cp_support_new/">MCM new cloud provider&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Seed only&lt;/td>
&lt;td>etcd backup/restore&lt;/td>
&lt;td>&lt;a href="https://github.com/gardener/etcd-backup-restore/">etcd-backup-restore&lt;/a>&lt;/td>
&lt;td>In process&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>All&lt;/td>
&lt;td>Extension implementation&lt;/td>
&lt;td>&lt;a href="https://github.com/gardener/gardener">gardener&lt;/a>&lt;/td>
&lt;td>&lt;a href="https://gardener.cloud/docs/gardener/extensions/">Extension controller&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Docs: New Kubernetes Version</title><link>https://gardener.cloud/docs/gardener/new-kubernetes-version/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/new-kubernetes-version/</guid><description>
&lt;h1 id="adding-support-for-a-new-kubernetes-version">Adding Support For a New Kubernetes Version&lt;/h1>
&lt;p>This document describes the steps needed to perform in order to confidently add support for a new Kubernetes &lt;strong>minor&lt;/strong> version.&lt;/p>
&lt;blockquote>
&lt;p>⚠️ Typically, once a minor Kubernetes version &lt;code>vX.Y&lt;/code> is supported by Gardener, then all patch versions &lt;code>vX.Y.Z&lt;/code> are also automatically supported without any required action.
This is because patch versions do not introduce any new feature or API changes, so there is nothing that needs to be adapted in &lt;code>gardener/gardener&lt;/code> code.&lt;/p>
&lt;/blockquote>
&lt;p>The Kubernetes community release a new minor version roughly every 4 months.
Please refer to the &lt;a href="https://kubernetes.io/releases/release/">official documentation&lt;/a> about their release cycles for any additional information.&lt;/p>
&lt;p>Shortly before a new release, an &amp;ldquo;umbrella&amp;rdquo; issue should be opened which is used to collect the required adaptations and to track the work items.
For example, &lt;a href="https://github.com/gardener/gardener/issues/5102">#5102&lt;/a> can be used as a template for the issue description.
As you can see, the task of supporting a new Kubernetes version also includes the provider extensions maintained in the &lt;code>gardener&lt;/code> GitHub organization and is not restricted to &lt;code>gardener/gardener&lt;/code> only.&lt;/p>
&lt;p>Generally, the work items can be split into two groups:
The first group contains tasks specific to the changes in the given Kubernetes release, the second group contains Kubernetes release-independent tasks.&lt;/p>
&lt;blockquote>
&lt;p>ℹ️ Upgrading the &lt;code>k8s.io/*&lt;/code> and &lt;code>sigs.k8s.io/controller-runtime&lt;/code> Golang dependencies is typically tracked and worked on separately (see e.g. &lt;a href="https://github.com/gardener/gardener/issues/4772">#4772&lt;/a> or &lt;a href="https://github.com/gardener/gardener/issues/5282">#5282&lt;/a>).&lt;/p>
&lt;/blockquote>
&lt;h2 id="deriving-release-specific-tasks">Deriving Release-Specific Tasks&lt;/h2>
&lt;p>Most new minor Kubernetes releases incorporate API changes, deprecations, or new features.
The community announces them via their &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/">change logs&lt;/a>.
In order to derive the release-specific tasks, the respective change log for the new version &lt;code>vX.Y&lt;/code> has to be read and understood (for example, &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md">the changelog&lt;/a> for &lt;code>v1.24&lt;/code>).&lt;/p>
&lt;p>As already mentioned, typical changes to watch out for are:&lt;/p>
&lt;ul>
&lt;li>API version promotions or deprecations&lt;/li>
&lt;li>Feature gate promotions or deprecations&lt;/li>
&lt;li>CLI flag changes for Kubernetes components&lt;/li>
&lt;li>New default values in resources&lt;/li>
&lt;li>New available fields in resources&lt;/li>
&lt;li>New features potentially relevant for the Gardener system&lt;/li>
&lt;li>Changes of labels or annotations Gardener relies on&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>Obviously, this requires a certain experience and understanding of the Gardener project so that all &amp;ldquo;relevant changes&amp;rdquo; can be identified.
While reading the change log, add the tasks (along with the respective PR in &lt;code>kubernetes/kubernetes&lt;/code> to the umbrella issue).&lt;/p>
&lt;blockquote>
&lt;p>ℹ️ Some of the changes might be specific to certain cloud providers. Pay attention to those as well and add related tasks to the issue.&lt;/p>
&lt;/blockquote>
&lt;h2 id="list-of-release-independent-tasks">List Of Release-Independent Tasks&lt;/h2>
&lt;p>The following paragraphs describe recurring tasks that need to be performed for each new release.&lt;/p>
&lt;h3 id="make-sure-a-new-hyperkube-image-is-released">Make Sure a New &lt;code>hyperkube&lt;/code> Image Is Released&lt;/h3>
&lt;p>The &lt;a href="https://github.com/gardener/hyperkube">&lt;code>gardener/hyperkube&lt;/code>&lt;/a> repository is used to release container images consisting of the &lt;code>kubectl&lt;/code> and &lt;code>kubelet&lt;/code> binaries.&lt;/p>
&lt;p>There is a CI/CD job that runs periodically and releases a new &lt;code>hyperkube&lt;/code> image when there is a new Kubernetes release. Before proceeding with the next steps, make sure that a new &lt;code>hyperkube&lt;/code> image is released for the corresponding new Kubernetes minor version. Make sure that container image is present in GCR.&lt;/p>
&lt;h3 id="adapting-gardener">Adapting Gardener&lt;/h3>
&lt;ul>
&lt;li>Allow instantiation of a Kubernetes client for the new minor version and update the &lt;code>README.md&lt;/code>:
&lt;ul>
&lt;li>See &lt;a href="https://github.com/gardener/gardener/pull/5255/commits/63bdae022f1cb1c9cbd1cd49b557545dca2ec32a">this&lt;/a> example commit.&lt;/li>
&lt;li>The list of supported versions is meanwhile maintained &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/utils/validation/kubernetesversion/version.go">here&lt;/a> in the &lt;code>SupportedVersions&lt;/code> variable.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Maintain the Kubernetes feature gates used for validation of &lt;code>Shoot&lt;/code> resources:
&lt;ul>
&lt;li>The feature gates are maintained in &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/utils/validation/features/featuregates.go">this&lt;/a> file.&lt;/li>
&lt;li>To maintain this list for new Kubernetes versions, run &lt;code>hack/compare-k8s-feature-gates.sh &amp;lt;old-version&amp;gt; &amp;lt;new-version&amp;gt;&lt;/code> (e.g. &lt;code>hack/compare-k8s-feature-gates.sh v1.26 v1.27&lt;/code>).&lt;/li>
&lt;li>It will present 3 lists of feature gates: those added and those removed in &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> compared to &lt;code>&amp;lt;old-version&amp;gt;&lt;/code> and feature gates that got locked to default in &lt;code>&amp;lt;new-version&amp;gt;&lt;/code>.&lt;/li>
&lt;li>Add all added feature gates to the map with &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> as &lt;code>AddedInVersion&lt;/code> and no &lt;code>RemovedInVersion&lt;/code>.&lt;/li>
&lt;li>For any removed feature gates, add &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> as &lt;code>RemovedInVersion&lt;/code> to the already existing feature gate in the map.&lt;/li>
&lt;li>For feature gates locked to default, add &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> as &lt;code>LockedToDefaultInVersion&lt;/code> to the already existing feature gate in the map.&lt;/li>
&lt;li>See &lt;a href="https://github.com/gardener/gardener/pull/5255/commits/97923b0604300ff805def8eae981ed388d5e4a83">this&lt;/a> example commit.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Maintain the Kubernetes &lt;code>kube-apiserver&lt;/code> admission plugins used for validation of &lt;code>Shoot&lt;/code> resources:
&lt;ul>
&lt;li>The admission plugins are maintained in &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/utils/validation/admissionplugins/admissionplugins.go">this&lt;/a> file.&lt;/li>
&lt;li>To maintain this list for new Kubernetes versions, run &lt;code>hack/compare-k8s-admission-plugins.sh &amp;lt;old-version&amp;gt; &amp;lt;new-version&amp;gt;&lt;/code> (e.g. &lt;code>hack/compare-k8s-admission-plugins.sh 1.26 1.27&lt;/code>).&lt;/li>
&lt;li>It will present 2 lists of admission plugins: those added and those removed in &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> compared to &lt;code>&amp;lt;old-version&amp;gt;&lt;/code>.&lt;/li>
&lt;li>Add all added admission plugins to the &lt;code>admissionPluginsVersionRanges&lt;/code> map with &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> as &lt;code>AddedInVersion&lt;/code> and no &lt;code>RemovedInVersion&lt;/code>.&lt;/li>
&lt;li>For any removed admission plugins, add &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> as &lt;code>RemovedInVersion&lt;/code> to the already existing admission plugin in the map.&lt;/li>
&lt;li>Flag any admission plugins that are required (plugins that must not be disabled in the &lt;code>Shoot&lt;/code> spec) by setting the &lt;code>Required&lt;/code> boolean variable to true for the admission plugin in the map.&lt;/li>
&lt;li>Flag any admission plugins that are forbidden by setting the &lt;code>Forbidden&lt;/code> boolean variable to true for the admission plugin in the map.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Maintain the Kubernetes &lt;code>kube-apiserver&lt;/code> API groups used for validation of &lt;code>Shoot&lt;/code> resources:
&lt;ul>
&lt;li>The API groups are maintained in &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/utils/validation/apigroups/apigroups.go">this&lt;/a> file.&lt;/li>
&lt;li>To maintain this list for new Kubernetes versions, run &lt;code>hack/compare-k8s-api-groups.sh &amp;lt;old-version&amp;gt; &amp;lt;new-version&amp;gt;&lt;/code> (e.g. &lt;code>hack/compare-k8s-api-groups.sh 1.26 1.27&lt;/code>).&lt;/li>
&lt;li>It will present 2 lists of API GroupVersions and 2 lists of API GroupVersionResources: those added and those removed in &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> compared to &lt;code>&amp;lt;old-version&amp;gt;&lt;/code>.&lt;/li>
&lt;li>Add all added group versions to the &lt;code>apiGroupVersionRanges&lt;/code> map and group version resources to the &lt;code>apiGVRVersionRanges&lt;/code> map with &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> as &lt;code>AddedInVersion&lt;/code> and no &lt;code>RemovedInVersion&lt;/code>.&lt;/li>
&lt;li>For any removed APIs, add &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> as &lt;code>RemovedInVersion&lt;/code> to the already existing API in the corresponding map.&lt;/li>
&lt;li>Flag any APIs that are required (APIs that must not be disabled in the &lt;code>Shoot&lt;/code> spec) by setting the &lt;code>Required&lt;/code> boolean variable to true for the API in the &lt;code>apiGVRVersionRanges&lt;/code> map. If this API also should not be disabled for &lt;a href="https://gardener.cloud/docs/gardener/shoot_workerless/">Workerless Shoots&lt;/a>, then set &lt;code>RequiredForWorkerless&lt;/code> boolean variable also to true. If the API is required for both Shoot types, then both of these booleans need to be set to true. If the whole API Group is required, then mark it correspondingly in the &lt;code>apiGroupVersionRanges&lt;/code> map.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Maintain the Kubernetes &lt;code>kube-controller-manager&lt;/code> controllers for each API group used in deploying required KCM controllers based on active APIs:
&lt;ul>
&lt;li>The API groups are maintained in &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/utils/kubernetes/controllers.go">this&lt;/a> file.&lt;/li>
&lt;li>To maintain this list for new Kubernetes versions, run &lt;code>hack/compute-k8s-controllers.sh &amp;lt;old-version&amp;gt; &amp;lt;new-version&amp;gt;&lt;/code> (e.g. &lt;code>hack/compute-k8s-controllers.sh 1.28 1.29&lt;/code>).&lt;/li>
&lt;li>If it complains that the path for the controller is not present in the map, check the release branch of the new Kubernetes version and find the correct path for the missing/wrong controller. You can do so by checking the file &lt;code>cmd/kube-controller-manager/app/controllermanager.go&lt;/code> and where the controller is initialized from. As of now, there is no straight-forward way to map each controller to its file. If this has improved, please enhance the script.&lt;/li>
&lt;li>If the paths are correct, it will present 2 lists of controllers: those added and those removed for each API group in &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> compared to &lt;code>&amp;lt;old-version&amp;gt;&lt;/code>.&lt;/li>
&lt;li>Add all added controllers to the &lt;code>APIGroupControllerMap&lt;/code> map and under the corresponding API group with &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> as &lt;code>AddedInVersion&lt;/code> and no &lt;code>RemovedInVersion&lt;/code>.&lt;/li>
&lt;li>For any removed controllers, add &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> as &lt;code>RemovedInVersion&lt;/code> to the already existing controller in the corresponding API group map. If you are unable to find the removed controller name, then check for its alias. Either in the &lt;code>staging/src/k8s.io/cloud-provider/names/controller_names.go&lt;/code> file (&lt;a href="https://github.com/kubernetes/kubernetes/blob/9fd8f568fe06a154e15cd4919ad2a7f6c6917b9f/staging/src/k8s.io/cloud-provider/names/controller_names.go#L60">example&lt;/a>) or in the &lt;code>cmd/kube-controller-manager/app/*&lt;/code> files (&lt;a href="https://github.com/kubernetes/kubernetes/blob/b584b87a94d6ff5256624bbf83dd5f758dff6eb2/cmd/kube-controller-manager/app/apps.go#L39">example for apps API group&lt;/a>). This is because for kubernetes versions starting from &lt;code>v1.28&lt;/code>, we don&amp;rsquo;t maintain the aliases in the controller, but the controller names itself since some controllers can be initialized without aliases as well (&lt;a href="https://github.com/kubernetes/kubernetes/blob/b584b87a94d6ff5256624bbf83dd5f758dff6eb2/cmd/kube-controller-manager/app/networking.go#L32-L39">example&lt;/a>). The old alias should still be working since it should be backwards compatible as explained &lt;a href="https://github.com/kubernetes/kubernetes/blob/9fd8f568fe06a154e15cd4919ad2a7f6c6917b9f/staging/src/k8s.io/cloud-provider/names/controller_names.go#L26-L31">here&lt;/a>. Once the support for kubernetes version &amp;lt; &lt;code>v1.28&lt;/code> is droppped, we can drop the usages of these aliases and move completely to controller names.&lt;/li>
&lt;li>Make sure that the API groups in &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/utils/validation/apigroups/apigroups.go">this&lt;/a> file are in sync with the groups in &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/utils/kubernetes/controllers.go">this&lt;/a> file. For example, &lt;code>core/v1&lt;/code> is replaced by the script as &lt;code>v1&lt;/code> and &lt;code>apiserverinternal&lt;/code> as &lt;code>internal&lt;/code>. This is because the API groups registered by the apiserver (&lt;a href="https://github.com/kubernetes/kubernetes/blob/8a9b209cb11943f4d53a0d840b55cf92ebfbe004/staging/src/k8s.io/api/apiserverinternal/v1alpha1/register.go#L26">example&lt;/a>) and the file path imported by the controllers (&lt;a href="https://github.com/kubernetes/kubernetes/blob/8a9b209cb11943f4d53a0d840b55cf92ebfbe004/pkg/controller/storageversiongc/gc_controller.go#L24">example&lt;/a>) might be slightly different in some cases.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Maintain the &lt;code>ServiceAccount&lt;/code> names for the controllers part of &lt;code>kube-controller-manager&lt;/code>:
&lt;ul>
&lt;li>The names are maintained in &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/component/shoot/system/system.go">this&lt;/a> file.&lt;/li>
&lt;li>To maintain this list for new Kubernetes versions, run &lt;code>hack/compare-k8s-controllers.sh &amp;lt;old-version&amp;gt; &amp;lt;new-version&amp;gt;&lt;/code> (e.g. &lt;code>hack/compare-k8s-controllers.sh 1.26 1.27&lt;/code>).&lt;/li>
&lt;li>It will present 2 lists of controllers: those added and those removed in &lt;code>&amp;lt;new-version&amp;gt;&lt;/code> compared to &lt;code>&amp;lt;old-version&amp;gt;&lt;/code>.&lt;/li>
&lt;li>Double check whether such &lt;code>ServiceAccount&lt;/code> indeed appears in the &lt;code>kube-system&lt;/code> namespace when creating a cluster with &lt;code>&amp;lt;new-version&amp;gt;&lt;/code>. Note that it sometimes might be hidden behind a default-off feature gate. You can create a local cluster with the new version using the &lt;a href="https://gardener.cloud/docs/gardener/getting_started_locally/">local provider&lt;/a>. It could so happen that the name of the controller is used in the form of a constant and not a string, see &lt;a href="https://github.com/kubernetes/kubernetes/blob/de506ce7ac9981c8253b2f818478bb4093fb7bb6/cmd/kube-controller-manager/app/validatingadmissionpolicystatus.go#L56">example&lt;/a>, In that case not the value of the constant separetely. You could also cross check the names with the result of the &lt;code>compute-k8s-controllers.sh&lt;/code> script used in the previous step.&lt;/li>
&lt;li>If it appears, add all added controllers to the list based on the Kubernetes version (&lt;a href="https://github.com/gardener/gardener/blob/b0de7db96ad436fe32c25daae5e8cb552dac351f/pkg/component/shootsystem/shootsystem.go#L253-L318">example&lt;/a>).&lt;/li>
&lt;li>For any removed controllers, add them only to the Kubernetes version if it is low enough.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Maintain the names of controllers used for workerless Shoots, &lt;a href="https://github.com/gardener/gardener/blob/6988da80bae6ba827d63535655f28885d91b0a23/pkg/component/kubernetes/controllermanager/controllermanager.go#L744-L766">here&lt;/a> after carefully evaluating whether they are needed if there are no workers.&lt;/li>
&lt;li>Maintain copies of the &lt;code>DaemonSet&lt;/code> controller&amp;rsquo;s scheduling logic:
&lt;ul>
&lt;li>&lt;code>gardener-resource-manager&lt;/code>&amp;rsquo;s &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/#node-controller">&lt;code>Node&lt;/code> controller&lt;/a> uses a copy of parts of the &lt;code>DaemonSet&lt;/code> controller&amp;rsquo;s logic for determining whether a specific &lt;code>Node&lt;/code> should run a daemon pod of a given &lt;code>DaemonSet&lt;/code>: see &lt;a href="https://github.com/gardener/gardener/blob/master/pkg/resourcemanager/controller/node/criticalcomponents/helper/daemon_controller.go">this file&lt;/a>.&lt;/li>
&lt;li>Check the referenced upstream files for changes to the &lt;code>DaemonSet&lt;/code> controller&amp;rsquo;s logic and adapt our copies accordingly. This might include introducing version-specific checks in our codebase to handle different shoot cluster versions.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Maintain version specific defaulting logic in shoot admission plugin:
&lt;ul>
&lt;li>Sometimes default values for shoots are intentionally changed with the introduction of a new Kubernetes version.&lt;/li>
&lt;li>The final Kubernetes version for a shoot is determined in the &lt;a href="https://github.com/gardener/gardener/blob/17dfefaffed6c5e125e35b6614c8dcad801839f1/plugin/pkg/shoot/validator/admission.go">Shoot Validator Admission Plugin&lt;/a>.&lt;/li>
&lt;li>Any defaulting logic that depends on the version should be placed in this admission plugin (&lt;a href="https://github.com/gardener/gardener/blob/f754c071e6cf8e45f7ac7bc5924acaf81b96dc06/plugin/pkg/shoot/validator/admission.go#L782">example&lt;/a>).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Ensure that &lt;a href="https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/shoot/maintenance">maintenance-controller&lt;/a> is able to auto-update shoots to the new Kubernetes version. Changes to the shoot spec required for the Kubernetes update should be enforced in such cases (&lt;a href="https://github.com/gardener/gardener/blob/bdfc06dc5cb4e5764800fd31ba1dd07727ad78bf/pkg/controllermanager/controller/shoot/maintenance/reconciler.go#L146-L162">examples&lt;/a>).&lt;/li>
&lt;li>Bump the used Kubernetes version for local e2e test.
&lt;ul>
&lt;li>See &lt;a href="https://github.com/gardener/gardener/pull/5255/commits/5707c4c7a4fd265b176387178b755cabeea89ffe">this&lt;/a> example commit.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="filing-the-pull-request">Filing the Pull Request&lt;/h4>
&lt;p>Work on all the tasks you have collected and validate them using the &lt;a href="https://gardener.cloud/docs/gardener/getting_started_locally/">local provider&lt;/a>.
Execute the e2e tests and if everything looks good, then go ahead and file the PR (&lt;a href="https://github.com/gardener/gardener/pull/5255">example PR&lt;/a>).
Generally, it is great if you add the PRs also to the umbrella issue so that they can be tracked more easily.&lt;/p>
&lt;h3 id="adapting-provider-extensions">Adapting Provider Extensions&lt;/h3>
&lt;p>After the PR in &lt;code>gardener/gardener&lt;/code> for the support of the new version has been merged, you can go ahead and work on the provider extensions.&lt;/p>
&lt;blockquote>
&lt;p>Actually, you can already start even if the PR is not yet merged and use the branch of your fork.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>Update the &lt;code>github.com/gardener/gardener&lt;/code> dependency in the extension and update the &lt;code>README.md&lt;/code>.&lt;/li>
&lt;li>Work on release-specific tasks related to this provider.&lt;/li>
&lt;/ul>
&lt;h4 id="maintaining-the-cloud-controller-manager-images">Maintaining the &lt;code>cloud-controller-manager&lt;/code> Images&lt;/h4>
&lt;p>Some of the cloud providers are not yet using upstream &lt;code>cloud-controller-manager&lt;/code> images.
Instead, we build and maintain them ourselves:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/gardener/cloud-provider-gcp">cloud-provider-gcp&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Until we switch to upstream images, you need to update the Kubernetes dependencies and release a new image.
The required steps are as follows:&lt;/p>
&lt;ul>
&lt;li>Checkout the &lt;code>legacy-cloud-provider&lt;/code> branch of the respective repository&lt;/li>
&lt;li>Bump the versions in the &lt;code>Dockerfile&lt;/code> (&lt;a href="https://github.com/gardener/cloud-provider-gcp/commit/b7eb3f56b252aaf29adc78406672574b1bc17495">example commit&lt;/a>).&lt;/li>
&lt;li>Update the &lt;code>VERSION&lt;/code> to &lt;code>vX.Y.Z-dev&lt;/code> where &lt;code>Z&lt;/code> is the latest available Kubernetes patch version for the &lt;code>vX.Y&lt;/code> minor version.&lt;/li>
&lt;li>Update the &lt;code>k8s.io/*&lt;/code> dependencies in the &lt;code>go.mod&lt;/code> file to &lt;code>vX.Y.Z&lt;/code> and run &lt;code>go mod tidy&lt;/code> (&lt;a href="https://github.com/gardener/cloud-provider-gcp/commit/d41cc9f035bcc4893b40d90a4f617c4d436c5d62">example commit&lt;/a>).&lt;/li>
&lt;li>Checkout a new &lt;code>release-vX.Y&lt;/code> branch and release it (&lt;a href="https://github.com/gardener/cloud-provider-gcp/commits/release-v1.23">example&lt;/a>)&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>As you are already on it, it is great if you also bump the &lt;code>k8s.io/*&lt;/code> dependencies for the last three minor releases as well.
In this case, you need to checkout the &lt;code>release-vX.{Y-{1,2,3}}&lt;/code> branches and only perform the last three steps (&lt;a href="https://github.com/gardener/cloud-provider-gcp/commits/release-v1.20">example branch&lt;/a>, &lt;a href="https://github.com/gardener/cloud-provider-gcp/commit/372aa43fbacdeb76b3da9f6fad6cfd924d916227">example commit&lt;/a>).&lt;/p>
&lt;/blockquote>
&lt;p>Now you need to update the new releases in the &lt;code>imagevector/images.yaml&lt;/code> of the respective provider extension so that they are used (see this &lt;a href="https://github.com/gardener/gardener-extension-provider-aws/pull/942/commits/7e5c0d95ff95d65459d13ae7f79a030049322c71">example commit&lt;/a> for reference).&lt;/p>
&lt;h4 id="filing-the-pull-request-1">Filing the Pull Request&lt;/h4>
&lt;p>Again, work on all the tasks you have collected.
This time, you cannot use the local provider for validation but should create real clusters on the various infrastructures.
Typically, the following validations should be performed:&lt;/p>
&lt;ul>
&lt;li>Create new clusters with versions &amp;lt; &lt;code>vX.Y&lt;/code>&lt;/li>
&lt;li>Create new clusters with version = &lt;code>vX.Y&lt;/code>&lt;/li>
&lt;li>Upgrade old clusters from version &lt;code>vX.{Y-1}&lt;/code> to version &lt;code>vX.Y&lt;/code>&lt;/li>
&lt;li>Delete clusters with versions &amp;lt; &lt;code>vX.Y&lt;/code>&lt;/li>
&lt;li>Delete clusters with version = &lt;code>vX.Y&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>If everything looks good, then go ahead and file the PR (&lt;a href="https://github.com/gardener/gardener-extension-provider-aws/pull/480">example PR&lt;/a>).
Generally, it is again great if you add the PRs also to the umbrella issue so that they can be tracked more easily.&lt;/p></description></item><item><title>Docs: NodeLocalDNS Configuration</title><link>https://gardener.cloud/docs/gardener/node-local-dns/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/node-local-dns/</guid><description>
&lt;h1 id="nodelocaldns-configuration">NodeLocalDNS Configuration&lt;/h1>
&lt;p>This is a short guide describing how to enable DNS caching on the shoot cluster nodes.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Currently in Gardener we are using CoreDNS as a deployment that is auto-scaled horizontally to cover for QPS-intensive applications. However, doing so does not seem to be enough to completely circumvent DNS bottlenecks such as:&lt;/p>
&lt;ul>
&lt;li>Cloud provider limits for DNS lookups.&lt;/li>
&lt;li>Unreliable UDP connections that forces a period of timeout in case packets are dropped.&lt;/li>
&lt;li>Unnecessary node hopping since CoreDNS is not deployed on all nodes, and as a result DNS queries end-up traversing multiple nodes before reaching the destination server.&lt;/li>
&lt;li>Inefficient load-balancing of services (e.g., round-robin might not be enough when using IPTables mode)&lt;/li>
&lt;li>and more &amp;hellip;&lt;/li>
&lt;/ul>
&lt;p>To workaround the issues described above, &lt;code>node-local-dns&lt;/code> was introduced. The architecture is described below. The idea is simple:&lt;/p>
&lt;ul>
&lt;li>For new queries, the connection is upgraded from UDP to TCP and forwarded towards the cluster IP for the original CoreDNS server.&lt;/li>
&lt;li>For previously resolved queries, an immediate response from the same node where the requester workload / pod resides is provided.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://gardener.cloud/__resources/node-local-dns_6d452a.png" alt="node-local-dns-architecture">&lt;/p>
&lt;h2 id="configuring-nodelocaldns">Configuring NodeLocalDNS&lt;/h2>
&lt;p>All that needs to be done to enable the usage of the &lt;code>node-local-dns&lt;/code> feature is to set the corresponding option (&lt;code>spec.systemComponents.nodeLocalDNS.enabled&lt;/code>) in the &lt;code>Shoot&lt;/code> resource to &lt;code>true&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> systemComponents:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeLocalDNS:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It is worth noting that:&lt;/p>
&lt;ul>
&lt;li>When migrating from IPVS to IPTables, existing pods will continue to leverage the node-local-dns cache.&lt;/li>
&lt;li>When migrating from IPtables to IPVS, only newer pods will be switched to the node-local-dns cache.&lt;/li>
&lt;li>During the reconfiguration of the node-local-dns there might be a short disruption in terms of domain name resolution depending on the setup. Usually, DNS requests are repeated for some time as UDP is an unreliable protocol, but that strictly depends on the application/way the domain name resolution happens. It is recommended to let the shoot be reconciled during the next maintenance period.&lt;/li>
&lt;li>Enabling or disabling node-local-dns triggers a rollout of all shoot worker nodes, see also &lt;a href="https://gardener.cloud/docs/gardener/shoot_updates/#rolling-update-triggers">this document&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>For more information about &lt;code>node-local-dns&lt;/code>, please refer to the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-network/1024-nodelocal-cache-dns/README.md">KEP&lt;/a> or to the &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/">usage documentation&lt;/a>.&lt;/p>
&lt;h2 id="known-issues">Known Issues&lt;/h2>
&lt;p>Custom DNS configuration may not work as expected in conjunction with &lt;code>NodeLocalDNS&lt;/code>.
Please refer to &lt;a href="https://gardener.cloud/docs/gardener/custom-dns-config/#node-local-dns">Custom DNS Configuration&lt;/a>.&lt;/p></description></item><item><title>Docs: OpenIDConnect Presets</title><link>https://gardener.cloud/docs/gardener/openidconnect-presets/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/openidconnect-presets/</guid><description>
&lt;h1 id="clusteropenidconnectpreset-and-openidconnectpreset">ClusterOpenIDConnectPreset and OpenIDConnectPreset&lt;/h1>
&lt;p>This page provides an overview of ClusterOpenIDConnectPresets and OpenIDConnectPresets, which are objects for injecting &lt;a href="https://openid.net/connect/">OpenIDConnect Configuration&lt;/a> into &lt;code>Shoot&lt;/code> at creation time. The injected information contains configuration for the Kube API Server and optionally configuration for kubeconfig generation using said configuration.&lt;/p>
&lt;h2 id="openidconnectpreset">OpenIDConnectPreset&lt;/h2>
&lt;p>An OpenIDConnectPreset is an API resource for injecting additional runtime OIDC requirements into a Shoot at creation time. You use label selectors to specify the &lt;code>Shoot&lt;/code> to which a given OpenIDConnectPreset applies.&lt;/p>
&lt;p>Using a OpenIDConnectPresets allows project owners to not have to explicitly provide the same OIDC configuration for every &lt;code>Shoot&lt;/code> in their &lt;code>Project&lt;/code>.&lt;/p>
&lt;p>For more information about the background, see the &lt;a href="https://github.com/gardener/gardener/issues/1161">issue&lt;/a> for OpenIDConnectPreset.&lt;/p>
&lt;h3 id="how-openidconnectpreset-works">How OpenIDConnectPreset Works&lt;/h3>
&lt;p>Gardener provides an admission controller (OpenIDConnectPreset) which, when enabled, applies OpenIDConnectPresets to incoming &lt;code>Shoot&lt;/code> creation requests. When a &lt;code>Shoot&lt;/code> creation request occurs, the system does the following:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Retrieve all OpenIDConnectPreset available for use in the &lt;code>Shoot&lt;/code> namespace.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check if the shoot label selectors of any OpenIDConnectPreset matches the labels on the Shoot being created.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If multiple presets are matched then only one is chosen and results are sorted based on:&lt;/p>
&lt;ol>
&lt;li>&lt;code>.spec.weight&lt;/code> value.&lt;/li>
&lt;li>lexicographically ordering their names (e.g., &lt;code>002preset&lt;/code> &amp;gt; &lt;code>001preset&lt;/code>)&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>If the &lt;code>Shoot&lt;/code> already has a &lt;code>.spec.kubernetes.kubeAPIServer.oidcConfig&lt;/code>, then no mutation occurs.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="simple-openidconnectpreset-example">Simple OpenIDConnectPreset Example&lt;/h3>
&lt;p>This is a simple example to show how a &lt;code>Shoot&lt;/code> is modified by the OpenIDConnectPreset:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: settings.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: OpenIDConnectPreset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: test-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shootSelector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidc: enabled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientID: test-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerURL: https://foo.bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># caBundle: |&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># -----BEGIN CERTIFICATE-----&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Li4u&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># -----END CERTIFICATE-----&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsClaim: groups-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsPrefix: groups-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernameClaim: username-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernamePrefix: username-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> signingAlgs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - RS256
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requiredClaims:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key: value
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> client:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secret: oidc-client-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extraConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extra-scopes: &lt;span style="color:#a31515">&amp;#34;email,offline_access,profile&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> foo: bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> weight: 90
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create the OpenIDConnectPreset:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl apply -f preset.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Examine the created OpenIDConnectPreset:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl get openidconnectpresets
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME ISSUER SHOOT-SELECTOR AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test-1 https://foo.bar oidc=enabled 1s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Simple &lt;code>Shoot&lt;/code> example:&lt;/p>
&lt;p>This is a sample of a &lt;code>Shoot&lt;/code> with some fields omitted:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: preset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidc: enabled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.20.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create the Shoot:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl apply -f shoot.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Examine the created Shoot:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl get shoot preset -o yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: preset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidc: enabled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeAPIServer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidcConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientAuthentication:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extraConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extra-scopes: email,offline_access,profile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> foo: bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secret: oidc-client-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientID: test-1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsClaim: groups-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsPrefix: groups-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerURL: https://foo.bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requiredClaims:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key: value
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> signingAlgs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - RS256
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernameClaim: username-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernamePrefix: username-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.20.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="disable-openidconnectpreset">Disable OpenIDConnectPreset&lt;/h3>
&lt;p>The OpenIDConnectPreset admission control is enabled by default. To disable it, use the &lt;code>--disable-admission-plugins&lt;/code> flag on the gardener-apiserver.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>--disable-admission-plugins=OpenIDConnectPreset
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="clusteropenidconnectpreset">ClusterOpenIDConnectPreset&lt;/h2>
&lt;p>A ClusterOpenIDConnectPreset is an API resource for injecting additional runtime OIDC requirements into a Shoot at creation time. In contrast to OpenIDConnect, it&amp;rsquo;s a cluster-scoped resource. You use label selectors to specify the &lt;code>Project&lt;/code> and &lt;code>Shoot&lt;/code> to which a given OpenIDCConnectPreset applies.&lt;/p>
&lt;p>Using a OpenIDConnectPresets allows cluster owners to not have to explicitly provide the same OIDC configuration for every &lt;code>Shoot&lt;/code> in specific &lt;code>Project&lt;/code>.&lt;/p>
&lt;p>For more information about the background, see the &lt;a href="https://github.com/gardener/gardener/issues/1161">issue&lt;/a> for ClusterOpenIDConnectPreset.&lt;/p>
&lt;h3 id="how-clusteropenidconnectpreset-works">How ClusterOpenIDConnectPreset Works&lt;/h3>
&lt;p>Gardener provides an admission controller (ClusterOpenIDConnectPreset) which, when enabled, applies ClusterOpenIDConnectPresets to incoming &lt;code>Shoot&lt;/code> creation requests. When a &lt;code>Shoot&lt;/code> creation request occurs, the system does the following:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Retrieve all ClusterOpenIDConnectPresets available.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check if the project label selector of any ClusterOpenIDConnectPreset matches the labels of the &lt;code>Project&lt;/code> in which the &lt;code>Shoot&lt;/code> is being created.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check if the shoot label selectors of any ClusterOpenIDConnectPreset matches the labels on the &lt;code>Shoot&lt;/code> being created.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If multiple presets are matched then only one is chosen and results are sorted based on:&lt;/p>
&lt;ol>
&lt;li>&lt;code>.spec.weight&lt;/code> value.&lt;/li>
&lt;li>lexicographically ordering their names ( e.g. &lt;code>002preset&lt;/code> &amp;gt; &lt;code>001preset&lt;/code> )&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>If the &lt;code>Shoot&lt;/code> already has a &lt;code>.spec.kubernetes.kubeAPIServer.oidcConfig&lt;/code> then no mutation occurs.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> Due to the previous requirement, if a &lt;code>Shoot&lt;/code> is matched by both &lt;code>OpenIDConnectPreset&lt;/code> and &lt;code>ClusterOpenIDConnectPreset&lt;/code>, then &lt;code>OpenIDConnectPreset&lt;/code> takes precedence over &lt;code>ClusterOpenIDConnectPreset&lt;/code>.&lt;/p>
&lt;/blockquote>
&lt;h3 id="simple-clusteropenidconnectpreset-example">Simple ClusterOpenIDConnectPreset Example&lt;/h3>
&lt;p>This is a simple example to show how a &lt;code>Shoot&lt;/code> is modified by the ClusterOpenIDConnectPreset:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: settings.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterOpenIDConnectPreset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: test
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shootSelector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidc: enabled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> projectSelector: {} &lt;span style="color:#008000"># selects all projects.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientID: cluster-preset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerURL: https://foo.bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># caBundle: |&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># -----BEGIN CERTIFICATE-----&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Li4u&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># -----END CERTIFICATE-----&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsClaim: groups-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsPrefix: groups-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernameClaim: username-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernamePrefix: username-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> signingAlgs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - RS256
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requiredClaims:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key: value
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> client:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secret: oidc-client-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extraConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extra-scopes: &lt;span style="color:#a31515">&amp;#34;email,offline_access,profile&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> foo: bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> weight: 90
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create the ClusterOpenIDConnectPreset:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl apply -f preset.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Examine the created ClusterOpenIDConnectPreset:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl get clusteropenidconnectpresets
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME ISSUER PROJECT-SELECTOR SHOOT-SELECTOR AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>test https://foo.bar &amp;lt;none&amp;gt; oidc=enabled 1s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This is a sample of a &lt;code>Shoot&lt;/code>, with some fields omitted:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: preset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidc: enabled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.20.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create the Shoot:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl apply -f shoot.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Examine the created Shoot:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl get shoot preset -o yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: preset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidc: enabled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeAPIServer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidcConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientAuthentication:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extraConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extra-scopes: email,offline_access,profile
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> foo: bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secret: oidc-client-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientID: cluster-preset
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsClaim: groups-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> groupsPrefix: groups-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerURL: https://foo.bar
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requiredClaims:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> key: value
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> signingAlgs:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - RS256
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernameClaim: username-claim
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernamePrefix: username-prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> version: 1.20.2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="disable-clusteropenidconnectpreset">Disable ClusterOpenIDConnectPreset&lt;/h3>
&lt;p>The ClusterOpenIDConnectPreset admission control is enabled by default. To disable it, use the &lt;code>--disable-admission-plugins&lt;/code> flag on the gardener-apiserver.&lt;/p>
&lt;p>For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>--disable-admission-plugins=ClusterOpenIDConnectPreset
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Priority Classes</title><link>https://gardener.cloud/docs/gardener/priority-classes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/priority-classes/</guid><description>
&lt;h1 id="priorityclasses-in-gardener-clusters">&lt;code>PriorityClass&lt;/code>es in Gardener Clusters&lt;/h1>
&lt;p>Gardener makes use of &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/">&lt;code>PriorityClass&lt;/code>es&lt;/a> to improve the overall robustness of the system.
In order to benefit from the full potential of &lt;code>PriorityClass&lt;/code>es, the gardenlet manages a set of well-known &lt;code>PriorityClass&lt;/code>es with fine-granular priority values.&lt;/p>
&lt;p>All components of the system should use these well-known &lt;code>PriorityClass&lt;/code>es instead of creating and using separate ones with arbitrary values, which would compromise the overall goal of using &lt;code>PriorityClass&lt;/code>es in the first place.
The gardenlet manages the well-known &lt;code>PriorityClass&lt;/code>es listed in this document, so that third parties (e.g., Gardener extensions) can rely on them to be present when deploying components to Seed and Shoot clusters.&lt;/p>
&lt;p>The listed well-known &lt;code>PriorityClass&lt;/code>es follow this rough concept:&lt;/p>
&lt;ul>
&lt;li>Values are close to the maximum that can be declared by the user. This is important to ensure that Shoot system components have higher priority than the workload deployed by end-users.&lt;/li>
&lt;li>Values have a bit of headroom in between to ensure flexibility when the need for intermediate priority values arises.&lt;/li>
&lt;li>Values of &lt;code>PriorityClass&lt;/code>es created on Seed clusters are lower than the ones on Shoots to ensure that Shoot system components have higher priority than Seed components, if the Seed is backed by a Shoot (&lt;code>ManagedSeed&lt;/code>), e.g. &lt;code>coredns&lt;/code> should have higher priority than &lt;code>gardenlet&lt;/code>.&lt;/li>
&lt;li>Names simply include the last digits of the value to minimize confusion caused by many (similar) names like &lt;code>critical&lt;/code>, &lt;code>importance-high&lt;/code>, etc.&lt;/li>
&lt;/ul>
&lt;h2 id="garden-clusters">Garden Clusters&lt;/h2>
&lt;p>When using the &lt;code>gardener-operator&lt;/code> for managing the garden runtime and virtual cluster, the following &lt;code>PriorityClass&lt;/code>es are available:&lt;/p>
&lt;h3 id="priorityclasses-for-garden-control-plane-components">&lt;code>PriorityClass&lt;/code>es for Garden Control Plane Components&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Priority&lt;/th>
&lt;th>Associated Components (Examples)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>gardener-garden-system-critical&lt;/code>&lt;/td>
&lt;td>999999550&lt;/td>
&lt;td>&lt;code>gardener-operator&lt;/code>, &lt;code>gardener-resource-manager&lt;/code>, &lt;code>istio&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-garden-system-500&lt;/code>&lt;/td>
&lt;td>999999500&lt;/td>
&lt;td>&lt;code>virtual-garden-etcd-events&lt;/code>, &lt;code>virtual-garden-etcd-main&lt;/code>, &lt;code>virtual-garden-kube-apiserver&lt;/code>, &lt;code>gardener-apiserver&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-garden-system-400&lt;/code>&lt;/td>
&lt;td>999999400&lt;/td>
&lt;td>&lt;code>virtual-garden-gardener-resource-manager&lt;/code>, &lt;code>gardener-admission-controller&lt;/code>, Extension Admission Controllers&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-garden-system-300&lt;/code>&lt;/td>
&lt;td>999999300&lt;/td>
&lt;td>&lt;code>virtual-garden-kube-controller-manager&lt;/code>, &lt;code>vpa-admission-controller&lt;/code>, &lt;code>etcd-druid&lt;/code>, &lt;code>nginx-ingress-controller&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-garden-system-200&lt;/code>&lt;/td>
&lt;td>999999200&lt;/td>
&lt;td>&lt;code>vpa-recommender&lt;/code>, &lt;code>vpa-updater&lt;/code>, &lt;code>hvpa-controller&lt;/code>, &lt;code>gardener-scheduler&lt;/code>, &lt;code>gardener-controller-manager&lt;/code>, &lt;code>gardener-dashboard&lt;/code>, &lt;code>terminal-controller-manager&lt;/code>, &lt;code>gardener-discovery-server&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-garden-system-100&lt;/code>&lt;/td>
&lt;td>999999100&lt;/td>
&lt;td>&lt;code>fluent-operator&lt;/code>, &lt;code>fluent-bit&lt;/code>, &lt;code>gardener-metrics-exporter&lt;/code>, &lt;code>kube-state-metrics&lt;/code>, &lt;code>plutono&lt;/code>, &lt;code>vali&lt;/code>, &lt;code>prometheus-operator&lt;/code>, &lt;code>alertmanager-garden&lt;/code>, &lt;code>prometheus-garden&lt;/code>, &lt;code>blackbox-exporter&lt;/code>, &lt;code>prometheus-longterm&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="seed-clusters">Seed Clusters&lt;/h2>
&lt;h3 id="priorityclasses-for-seed-system-components">&lt;code>PriorityClass&lt;/code>es for Seed System Components&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Priority&lt;/th>
&lt;th>Associated Components (Examples)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>gardener-system-critical&lt;/code>&lt;/td>
&lt;td>999998950&lt;/td>
&lt;td>&lt;code>gardenlet&lt;/code>, &lt;code>gardener-resource-manager&lt;/code>, &lt;code>istio-ingressgateway&lt;/code>, &lt;code>istiod&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-system-900&lt;/code>&lt;/td>
&lt;td>999998900&lt;/td>
&lt;td>Extensions, &lt;code>reversed-vpn-auth-server&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-system-800&lt;/code>&lt;/td>
&lt;td>999998800&lt;/td>
&lt;td>&lt;code>dependency-watchdog-endpoint&lt;/code>, &lt;code>dependency-watchdog-probe&lt;/code>, &lt;code>etcd-druid&lt;/code>, &lt;code>vpa-admission-controller&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-system-700&lt;/code>&lt;/td>
&lt;td>999998700&lt;/td>
&lt;td>&lt;code>hvpa-controller&lt;/code>, &lt;code>vpa-recommender&lt;/code>, &lt;code>vpa-updater&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-system-600&lt;/code>&lt;/td>
&lt;td>999998600&lt;/td>
&lt;td>&lt;code>alertmanager-seed&lt;/code>, &lt;code>fluent-operator&lt;/code>, &lt;code>fluent-bit&lt;/code>, &lt;code>plutono&lt;/code>, &lt;code>kube-state-metrics&lt;/code>, &lt;code>nginx-ingress-controller&lt;/code>, &lt;code>nginx-k8s-backend&lt;/code>, &lt;code>prometheus-operator&lt;/code>, &lt;code>prometheus-aggregate&lt;/code>, &lt;code>prometheus-cache&lt;/code>, &lt;code>prometheus-seed&lt;/code>, &lt;code>vali&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-reserve-excess-capacity&lt;/code>&lt;/td>
&lt;td>-5&lt;/td>
&lt;td>&lt;code>reserve-excess-capacity&lt;/code> (&lt;a href="https://github.com/gardener/gardener/pull/6135">ref&lt;/a>)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="priorityclasses-for-shoot-control-plane-components">&lt;code>PriorityClass&lt;/code>es for Shoot Control Plane Components&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Priority&lt;/th>
&lt;th>Associated Components (Examples)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>gardener-system-500&lt;/code>&lt;/td>
&lt;td>999998500&lt;/td>
&lt;td>&lt;code>etcd-events&lt;/code>, &lt;code>etcd-main&lt;/code>, &lt;code>kube-apiserver&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-system-400&lt;/code>&lt;/td>
&lt;td>999998400&lt;/td>
&lt;td>&lt;code>gardener-resource-manager&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-system-300&lt;/code>&lt;/td>
&lt;td>999998300&lt;/td>
&lt;td>&lt;code>cloud-controller-manager&lt;/code>, &lt;code>cluster-autoscaler&lt;/code>, &lt;code>csi-driver-controller&lt;/code>, &lt;code>kube-controller-manager&lt;/code>, &lt;code>kube-scheduler&lt;/code>, &lt;code>machine-controller-manager&lt;/code>, &lt;code>terraformer&lt;/code>, &lt;code>vpn-seed-server&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-system-200&lt;/code>&lt;/td>
&lt;td>999998200&lt;/td>
&lt;td>&lt;code>csi-snapshot-controller&lt;/code>, &lt;code>csi-snapshot-validation&lt;/code>, &lt;code>cert-controller-manager&lt;/code>, &lt;code>shoot-dns-service&lt;/code>, &lt;code>vpa-admission-controller&lt;/code>, &lt;code>vpa-recommender&lt;/code>, &lt;code>vpa-updater&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-system-100&lt;/code>&lt;/td>
&lt;td>999998100&lt;/td>
&lt;td>&lt;code>alertmanager-shoot&lt;/code>, &lt;code>plutono&lt;/code>, &lt;code>kube-state-metrics&lt;/code>, &lt;code>prometheus-shoot&lt;/code>, &lt;code>blackbox-exporter&lt;/code>, &lt;code>vali&lt;/code>, &lt;code>event-logger&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="shoot-clusters">Shoot Clusters&lt;/h2>
&lt;h2 id="priorityclasses-for-shoot-system-components">&lt;code>PriorityClass&lt;/code>es for Shoot System Components&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Priority&lt;/th>
&lt;th>Associated Components (Examples)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>system-node-critical&lt;/code> (created by Kubernetes)&lt;/td>
&lt;td>2000001000&lt;/td>
&lt;td>&lt;code>calico-node&lt;/code>, &lt;code>kube-proxy&lt;/code>, &lt;code>apiserver-proxy&lt;/code>, &lt;code>csi-driver&lt;/code>, &lt;code>egress-filter-applier&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>system-cluster-critical&lt;/code> (created by Kubernetes)&lt;/td>
&lt;td>2000000000&lt;/td>
&lt;td>&lt;code>calico-typha&lt;/code>, &lt;code>calico-kube-controllers&lt;/code>, &lt;code>coredns&lt;/code>, &lt;code>vpn-shoot&lt;/code>, &lt;code>registry-cache&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-shoot-system-900&lt;/code>&lt;/td>
&lt;td>999999900&lt;/td>
&lt;td>&lt;code>node-problem-detector&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-shoot-system-800&lt;/code>&lt;/td>
&lt;td>999999800&lt;/td>
&lt;td>&lt;code>calico-typha-horizontal-autoscaler&lt;/code>, &lt;code>calico-typha-vertical-autoscaler&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-shoot-system-700&lt;/code>&lt;/td>
&lt;td>999999700&lt;/td>
&lt;td>&lt;code>blackbox-exporter&lt;/code>, &lt;code>node-exporter&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>gardener-shoot-system-600&lt;/code>&lt;/td>
&lt;td>999999600&lt;/td>
&lt;td>&lt;code>addons-nginx-ingress-controller&lt;/code>, &lt;code>addons-nginx-ingress-k8s-backend&lt;/code>, &lt;code>kubernetes-dashboard&lt;/code>, &lt;code>kubernetes-metrics-scraper&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Docs: Process</title><link>https://gardener.cloud/docs/gardener/process/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/process/</guid><description>
&lt;h1 id="releases-features-hotfixes">Releases, Features, Hotfixes&lt;/h1>
&lt;p>This document describes how to contribute features or hotfixes, and how new Gardener releases are usually scheduled, validated, etc.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/process/#releases-features-hotfixes">Releases, Features, Hotfixes&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/process/#releases">Releases&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/process/#release-responsible-plan">Release Responsible Plan&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/process/#release-validation">Release Validation&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/process/#contributing-new-features-or-fixes">Contributing New Features or Fixes&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/process/#todo-statements">TODO Statements&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/process/#cherry-picks">Cherry Picks&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/process/#prerequisites">Prerequisites&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/process/#initiate-a-cherry-pick">Initiate a Cherry Pick&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="releases">Releases&lt;/h2>
&lt;p>The &lt;a href="https://github.com/orgs/gardener/teams/gardener-maintainers">@gardener-maintainers&lt;/a> are trying to provide a new release roughly every other week (depending on their capacity and the stability/robustness of the &lt;code>master&lt;/code> branch).&lt;/p>
&lt;p>Hotfixes are usually maintained for the latest three minor releases, though, there are no fixed release dates.&lt;/p>
&lt;h3 id="release-responsible-plan">Release Responsible Plan&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Version&lt;/th>
&lt;th>Week No&lt;/th>
&lt;th>Begin Validation Phase&lt;/th>
&lt;th>Due Date&lt;/th>
&lt;th>Release Responsible&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>v1.101&lt;/td>
&lt;td>Week 31-32&lt;/td>
&lt;td>July 29, 2024&lt;/td>
&lt;td>August 11, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.102&lt;/td>
&lt;td>Week 33-34&lt;/td>
&lt;td>August 12, 2024&lt;/td>
&lt;td>August 25, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/plkokanov">@plkokanov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.103&lt;/td>
&lt;td>Week 35-36&lt;/td>
&lt;td>August 26, 2024&lt;/td>
&lt;td>September 8, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/oliver-goetz">@oliver-goetz&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.104&lt;/td>
&lt;td>Week 37-38&lt;/td>
&lt;td>September 9, 2024&lt;/td>
&lt;td>September 22, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.105&lt;/td>
&lt;td>Week 39-40&lt;/td>
&lt;td>September 23, 2024&lt;/td>
&lt;td>October 6, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/acumino">@acumino&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.106&lt;/td>
&lt;td>Week 41-42&lt;/td>
&lt;td>October 7, 2024&lt;/td>
&lt;td>October 20, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/timuthy">@timuthy&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.107&lt;/td>
&lt;td>Week 43-44&lt;/td>
&lt;td>October 21, 2024&lt;/td>
&lt;td>November 3, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/LucaBernstein">@LucaBernstein&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.108&lt;/td>
&lt;td>Week 45-46&lt;/td>
&lt;td>November 4, 2024&lt;/td>
&lt;td>November 17, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/ScheererJ">@ScheererJ&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.109&lt;/td>
&lt;td>Week 47-48&lt;/td>
&lt;td>November 18, 2024&lt;/td>
&lt;td>December 1, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/ary1992">@ary1992&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.110&lt;/td>
&lt;td>Week 48-49&lt;/td>
&lt;td>November 25, 2024&lt;/td>
&lt;td>December 8, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/shafeeqes">@shafeeqes&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.111&lt;/td>
&lt;td>Week 50-51&lt;/td>
&lt;td>December 9, 2024&lt;/td>
&lt;td>December 22, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/oliver-goetz">@oliver-goetz&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.112&lt;/td>
&lt;td>Week 01-04&lt;/td>
&lt;td>December 30, 2024&lt;/td>
&lt;td>January 26, 2025&lt;/td>
&lt;td>&lt;a href="https://github.com/tobschli">@tobschli&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.113&lt;/td>
&lt;td>Week 05-06&lt;/td>
&lt;td>January 27, 2025&lt;/td>
&lt;td>February 9, 2025&lt;/td>
&lt;td>&lt;a href="https://github.com/plkokanov">@plkokanov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.114&lt;/td>
&lt;td>Week 07-08&lt;/td>
&lt;td>February 10, 2025&lt;/td>
&lt;td>February 23, 2025&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.115&lt;/td>
&lt;td>Week 09-10&lt;/td>
&lt;td>February 24, 2025&lt;/td>
&lt;td>March 9, 2025&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Apart from the release of the next version, the release responsible is also taking care of potential hotfix releases of the last three minor versions.
The release responsible is the main contact person for coordinating new feature PRs for the next minor versions or cherry-pick PRs for the last three minor versions.&lt;/p>
&lt;details>
&lt;summary>Click to expand the archived release responsible associations!&lt;/summary>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Version&lt;/th>
&lt;th>Week No&lt;/th>
&lt;th>Begin Validation Phase&lt;/th>
&lt;th>Due Date&lt;/th>
&lt;th>Release Responsible&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>v1.17&lt;/td>
&lt;td>Week 07-08&lt;/td>
&lt;td>February 15, 2021&lt;/td>
&lt;td>February 28, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.18&lt;/td>
&lt;td>Week 09-10&lt;/td>
&lt;td>March 1, 2021&lt;/td>
&lt;td>March 14, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/danielfoehrKn">@danielfoehrKn&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.19&lt;/td>
&lt;td>Week 11-12&lt;/td>
&lt;td>March 15, 2021&lt;/td>
&lt;td>March 28, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/timebertt">@timebertt&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.20&lt;/td>
&lt;td>Week 13-14&lt;/td>
&lt;td>March 29, 2021&lt;/td>
&lt;td>April 11, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/vpnachev">@vpnachev&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.21&lt;/td>
&lt;td>Week 15-16&lt;/td>
&lt;td>April 12, 2021&lt;/td>
&lt;td>April 25, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/timuthy">@timuthy&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.22&lt;/td>
&lt;td>Week 17-18&lt;/td>
&lt;td>April 26, 2021&lt;/td>
&lt;td>May 9, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/BeckerMax">@BeckerMax&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.23&lt;/td>
&lt;td>Week 19-20&lt;/td>
&lt;td>May 10, 2021&lt;/td>
&lt;td>May 23, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.24&lt;/td>
&lt;td>Week 21-22&lt;/td>
&lt;td>May 24, 2021&lt;/td>
&lt;td>June 5, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/stoyanr">@stoyanr&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.25&lt;/td>
&lt;td>Week 23-24&lt;/td>
&lt;td>June 7, 2021&lt;/td>
&lt;td>June 20, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.26&lt;/td>
&lt;td>Week 25-26&lt;/td>
&lt;td>June 21, 2021&lt;/td>
&lt;td>July 4, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/danielfoehrKn">@danielfoehrKn&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.27&lt;/td>
&lt;td>Week 27-28&lt;/td>
&lt;td>July 5, 2021&lt;/td>
&lt;td>July 18, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/timebertt">@timebertt&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.28&lt;/td>
&lt;td>Week 29-30&lt;/td>
&lt;td>July 19, 2021&lt;/td>
&lt;td>August 1, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.29&lt;/td>
&lt;td>Week 31-32&lt;/td>
&lt;td>August 2, 2021&lt;/td>
&lt;td>August 15, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/timuthy">@timuthy&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.30&lt;/td>
&lt;td>Week 33-34&lt;/td>
&lt;td>August 16, 2021&lt;/td>
&lt;td>August 29, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/BeckerMax">@BeckerMax&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.31&lt;/td>
&lt;td>Week 35-36&lt;/td>
&lt;td>August 30, 2021&lt;/td>
&lt;td>September 12, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/stoyanr">@stoyanr&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.32&lt;/td>
&lt;td>Week 37-38&lt;/td>
&lt;td>September 13, 2021&lt;/td>
&lt;td>September 26, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/vpnachev">@vpnachev&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.33&lt;/td>
&lt;td>Week 39-40&lt;/td>
&lt;td>September 27, 2021&lt;/td>
&lt;td>October 10, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/voelzmo">@voelzmo&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.34&lt;/td>
&lt;td>Week 41-42&lt;/td>
&lt;td>October 11, 2021&lt;/td>
&lt;td>October 24, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/plkokanov">@plkokanov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.35&lt;/td>
&lt;td>Week 43-44&lt;/td>
&lt;td>October 25, 2021&lt;/td>
&lt;td>November 7, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/kris94">@kris94&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.36&lt;/td>
&lt;td>Week 45-46&lt;/td>
&lt;td>November 8, 2021&lt;/td>
&lt;td>November 21, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/timebertt">@timebertt&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.37&lt;/td>
&lt;td>Week 47-48&lt;/td>
&lt;td>November 22, 2021&lt;/td>
&lt;td>December 5, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/danielfoehrKn">@danielfoehrKn&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.38&lt;/td>
&lt;td>Week 49-50&lt;/td>
&lt;td>December 6, 2021&lt;/td>
&lt;td>December 19, 2021&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.39&lt;/td>
&lt;td>Week 01-04&lt;/td>
&lt;td>January 3, 2022&lt;/td>
&lt;td>January 30, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>, &lt;a href="https://github.com/timuthy">@timuthy&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.40&lt;/td>
&lt;td>Week 05-06&lt;/td>
&lt;td>January 31, 2022&lt;/td>
&lt;td>February 13, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/BeckerMax">@BeckerMax&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.41&lt;/td>
&lt;td>Week 07-08&lt;/td>
&lt;td>February 14, 2022&lt;/td>
&lt;td>February 27, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/plkokanov">@plkokanov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.42&lt;/td>
&lt;td>Week 09-10&lt;/td>
&lt;td>February 28, 2022&lt;/td>
&lt;td>March 13, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/kris94">@kris94&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.43&lt;/td>
&lt;td>Week 11-12&lt;/td>
&lt;td>March 14, 2022&lt;/td>
&lt;td>March 27, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.44&lt;/td>
&lt;td>Week 13-14&lt;/td>
&lt;td>March 28, 2022&lt;/td>
&lt;td>April 10, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/timebertt">@timebertt&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.45&lt;/td>
&lt;td>Week 15-16&lt;/td>
&lt;td>April 11, 2022&lt;/td>
&lt;td>April 24, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/acumino">@acumino&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.46&lt;/td>
&lt;td>Week 17-18&lt;/td>
&lt;td>April 25, 2022&lt;/td>
&lt;td>May 8, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.47&lt;/td>
&lt;td>Week 19-20&lt;/td>
&lt;td>May 9, 2022&lt;/td>
&lt;td>May 22, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/shafeeqes">@shafeeqes&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.48&lt;/td>
&lt;td>Week 21-22&lt;/td>
&lt;td>May 23, 2022&lt;/td>
&lt;td>June 5, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/ary1992">@ary1992&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.49&lt;/td>
&lt;td>Week 23-24&lt;/td>
&lt;td>June 6, 2022&lt;/td>
&lt;td>June 19, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/plkokanov">@plkokanov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.50&lt;/td>
&lt;td>Week 25-26&lt;/td>
&lt;td>June 20, 2022&lt;/td>
&lt;td>July 3, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.51&lt;/td>
&lt;td>Week 27-28&lt;/td>
&lt;td>July 4, 2022&lt;/td>
&lt;td>July 17, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/timebertt">@timebertt&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.52&lt;/td>
&lt;td>Week 29-30&lt;/td>
&lt;td>July 18, 2022&lt;/td>
&lt;td>July 31, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/acumino">@acumino&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.53&lt;/td>
&lt;td>Week 31-32&lt;/td>
&lt;td>August 1, 2022&lt;/td>
&lt;td>August 14, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/kris94">@kris94&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.54&lt;/td>
&lt;td>Week 33-34&lt;/td>
&lt;td>August 15, 2022&lt;/td>
&lt;td>August 28, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.55&lt;/td>
&lt;td>Week 35-36&lt;/td>
&lt;td>August 29, 2022&lt;/td>
&lt;td>September 11, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/oliver-goetz">@oliver-goetz&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.56&lt;/td>
&lt;td>Week 37-38&lt;/td>
&lt;td>September 12, 2022&lt;/td>
&lt;td>September 25, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/shafeeqes">@shafeeqes&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.57&lt;/td>
&lt;td>Week 39-40&lt;/td>
&lt;td>September 26, 2022&lt;/td>
&lt;td>October 9, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/ary1992">@ary1992&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.58&lt;/td>
&lt;td>Week 41-42&lt;/td>
&lt;td>October 10, 2022&lt;/td>
&lt;td>October 23, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/plkokanov">@plkokanov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.59&lt;/td>
&lt;td>Week 43-44&lt;/td>
&lt;td>October 24, 2022&lt;/td>
&lt;td>November 6, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.60&lt;/td>
&lt;td>Week 45-46&lt;/td>
&lt;td>November 7, 2022&lt;/td>
&lt;td>November 20, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/acumino">@acumino&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.61&lt;/td>
&lt;td>Week 47-48&lt;/td>
&lt;td>November 21, 2022&lt;/td>
&lt;td>December 4, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.62&lt;/td>
&lt;td>Week 49-50&lt;/td>
&lt;td>December 5, 2022&lt;/td>
&lt;td>December 18, 2022&lt;/td>
&lt;td>&lt;a href="https://github.com/oliver-goetz">@oliver-goetz&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.63&lt;/td>
&lt;td>Week 01-04&lt;/td>
&lt;td>January 2, 2023&lt;/td>
&lt;td>January 29, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/shafeeqes">@shafeeqes&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.64&lt;/td>
&lt;td>Week 05-06&lt;/td>
&lt;td>January 30, 2023&lt;/td>
&lt;td>February 12, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/ary1992">@ary1992&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.65&lt;/td>
&lt;td>Week 07-08&lt;/td>
&lt;td>February 13, 2023&lt;/td>
&lt;td>February 26, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/timuthy">@timuthy&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.66&lt;/td>
&lt;td>Week 09-10&lt;/td>
&lt;td>February 27, 2023&lt;/td>
&lt;td>March 12, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/plkokanov">@plkokanov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.67&lt;/td>
&lt;td>Week 11-12&lt;/td>
&lt;td>March 13, 2023&lt;/td>
&lt;td>March 26, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.68&lt;/td>
&lt;td>Week 13-14&lt;/td>
&lt;td>March 27, 2023&lt;/td>
&lt;td>April 9, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/acumino">@acumino&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.69&lt;/td>
&lt;td>Week 15-16&lt;/td>
&lt;td>April 10, 2023&lt;/td>
&lt;td>April 23, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/oliver-goetz">@oliver-goetz&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.70&lt;/td>
&lt;td>Week 17-18&lt;/td>
&lt;td>April 24, 2023&lt;/td>
&lt;td>May 7, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.71&lt;/td>
&lt;td>Week 19-20&lt;/td>
&lt;td>May 8, 2023&lt;/td>
&lt;td>May 21, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/shafeeqes">@shafeeqes&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.72&lt;/td>
&lt;td>Week 21-22&lt;/td>
&lt;td>May 22, 2023&lt;/td>
&lt;td>June 4, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/ary1992">@ary1992&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.73&lt;/td>
&lt;td>Week 23-24&lt;/td>
&lt;td>June 5, 2023&lt;/td>
&lt;td>June 18, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/timuthy">@timuthy&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.74&lt;/td>
&lt;td>Week 25-26&lt;/td>
&lt;td>June 19, 2023&lt;/td>
&lt;td>July 2, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/oliver-goetz">@oliver-goetz&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.75&lt;/td>
&lt;td>Week 27-28&lt;/td>
&lt;td>July 3, 2023&lt;/td>
&lt;td>July 16, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.76&lt;/td>
&lt;td>Week 29-30&lt;/td>
&lt;td>July 17, 2023&lt;/td>
&lt;td>July 30, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/plkokanov">@plkokanov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.77&lt;/td>
&lt;td>Week 31-32&lt;/td>
&lt;td>July 31, 2023&lt;/td>
&lt;td>August 13, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.78&lt;/td>
&lt;td>Week 33-34&lt;/td>
&lt;td>August 14, 2023&lt;/td>
&lt;td>August 27, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/acumino">@acumino&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.79&lt;/td>
&lt;td>Week 35-36&lt;/td>
&lt;td>August 28, 2023&lt;/td>
&lt;td>September 10, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/shafeeqes">@shafeeqes&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.80&lt;/td>
&lt;td>Week 37-38&lt;/td>
&lt;td>September 11, 2023&lt;/td>
&lt;td>September 24, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/ScheererJ">@ScheererJ&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.81&lt;/td>
&lt;td>Week 39-40&lt;/td>
&lt;td>September 25, 2023&lt;/td>
&lt;td>October 8, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/ary1992">@ary1992&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.82&lt;/td>
&lt;td>Week 41-42&lt;/td>
&lt;td>October 9, 2023&lt;/td>
&lt;td>October 22, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/timuthy">@timuthy&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.83&lt;/td>
&lt;td>Week 43-44&lt;/td>
&lt;td>October 23, 2023&lt;/td>
&lt;td>November 5, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/oliver-goetz">@oliver-goetz&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.84&lt;/td>
&lt;td>Week 45-46&lt;/td>
&lt;td>November 6, 2023&lt;/td>
&lt;td>November 19, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.85&lt;/td>
&lt;td>Week 47-48&lt;/td>
&lt;td>November 20, 2023&lt;/td>
&lt;td>December 3, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/plkokanov">@plkokanov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.86&lt;/td>
&lt;td>Week 49-50&lt;/td>
&lt;td>December 4, 2023&lt;/td>
&lt;td>December 17, 2023&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.87&lt;/td>
&lt;td>Week 01-04&lt;/td>
&lt;td>January 1, 2024&lt;/td>
&lt;td>January 28, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/acumino">@acumino&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.88&lt;/td>
&lt;td>Week 05-06&lt;/td>
&lt;td>January 29, 2024&lt;/td>
&lt;td>February 11, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/timuthy">@timuthy&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.89&lt;/td>
&lt;td>Week 07-08&lt;/td>
&lt;td>February 12, 2024&lt;/td>
&lt;td>February 25, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/ScheererJ">@ScheererJ&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.90&lt;/td>
&lt;td>Week 09-10&lt;/td>
&lt;td>February 26, 2024&lt;/td>
&lt;td>March 10, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/ary1992">@ary1992&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.91&lt;/td>
&lt;td>Week 11-12&lt;/td>
&lt;td>March 11, 2024&lt;/td>
&lt;td>March 24, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/shafeeqes">@shafeeqes&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.92&lt;/td>
&lt;td>Week 13-14&lt;/td>
&lt;td>March 25, 2024&lt;/td>
&lt;td>April 7, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/oliver-goetz">@oliver-goetz&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.93&lt;/td>
&lt;td>Week 15-16&lt;/td>
&lt;td>April 8, 2024&lt;/td>
&lt;td>April 21, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/rfranzke">@rfranzke&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.94&lt;/td>
&lt;td>Week 17-18&lt;/td>
&lt;td>April 22, 2024&lt;/td>
&lt;td>May 5, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/plkokanov">@plkokanov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.95&lt;/td>
&lt;td>Week 19-20&lt;/td>
&lt;td>May 6, 2024&lt;/td>
&lt;td>May 19, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/ialidzhikov">@ialidzhikov&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.96&lt;/td>
&lt;td>Week 21-22&lt;/td>
&lt;td>May 20, 2024&lt;/td>
&lt;td>June 2, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/acumino">@acumino&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.97&lt;/td>
&lt;td>Week 23-24&lt;/td>
&lt;td>June 3, 2024&lt;/td>
&lt;td>June 16, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/timuthy">@timuthy&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.98&lt;/td>
&lt;td>Week 25-26&lt;/td>
&lt;td>June 17, 2024&lt;/td>
&lt;td>June 30, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/ScheererJ">@ScheererJ&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.99&lt;/td>
&lt;td>Week 27-28&lt;/td>
&lt;td>July 1, 2024&lt;/td>
&lt;td>July 14, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/ary1992">@ary1992&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>v1.100&lt;/td>
&lt;td>Week 29-30&lt;/td>
&lt;td>July 15, 2024&lt;/td>
&lt;td>July 28, 2024&lt;/td>
&lt;td>&lt;a href="https://github.com/shafeeqes">@shafeeqes&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/details>
&lt;h3 id="release-validation">Release Validation&lt;/h3>
&lt;p>The release phase for a new minor version lasts two weeks.
Typically, the first week is used for the validation of the release.
This phase includes the following steps:&lt;/p>
&lt;ol>
&lt;li>&lt;code>master&lt;/code> (or latest &lt;code>release-*&lt;/code> branch) is deployed to a development landscape that already hosts some existing seed and shoot clusters.&lt;/li>
&lt;li>An extended test suite is triggered by the &amp;ldquo;release responsible&amp;rdquo; which:
&lt;ol>
&lt;li>executes the Gardener integration tests for different Kubernetes versions, infrastructures, and &lt;code>Shoot&lt;/code> settings.&lt;/li>
&lt;li>executes the Kubernetes conformance tests.&lt;/li>
&lt;li>executes further tests like Kubernetes/OS patch/minor version upgrades.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Additionally, every four hours (or on demand) more tests (e.g., including the Kubernetes e2e test suite) are executed for different infrastructures.&lt;/li>
&lt;li>The &amp;ldquo;release responsible&amp;rdquo; is verifying new features or other notable changes (derived of the draft release notes) in this development system.&lt;/li>
&lt;/ol>
&lt;p>Usually, the new release is triggered in the beginning of the second week if all tests are green, all checks were successful, and if all of the planned verifications were performed by the release responsible.&lt;/p>
&lt;h2 id="contributing-new-features-or-fixes">Contributing New Features or Fixes&lt;/h2>
&lt;p>Please refer to the &lt;a href="https://gardener.cloud/docs/contribute/">Gardener contributor guide&lt;/a>.
Besides a lot of a general information, it also provides a checklist for newly created pull requests that may help you to prepare your changes for an efficient review process.
If you are contributing a fix or major improvement, please take care to open cherry-pick PRs to all affected and still supported versions once the change is approved and merged in the &lt;code>master&lt;/code> branch.&lt;/p>
&lt;p>⚠️ Please ensure that your modifications pass the verification checks (linting, formatting, static code checks, tests, etc.) by executing&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>make verify
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>before filing your pull request.&lt;/p>
&lt;p>The guide applies for both changes to the &lt;code>master&lt;/code> and to any &lt;code>release-*&lt;/code> branch.
All changes must be submitted via a pull request and be reviewed and approved by at least one code owner.&lt;/p>
&lt;h3 id="todo-statements">TODO Statements&lt;/h3>
&lt;p>Sometimes, TODO statements are being introduced when one cannot follow up immediately with certain tasks or when temporary migration code is required.
In order to properly follow-up with such TODOs and to prevent them from piling up without getting attention, the following rules should be followed:&lt;/p>
&lt;ul>
&lt;li>Each TODO statement should have an associated person and state when it can be removed.
Example:
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-golang" data-lang="golang">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// TODO(&amp;lt;github-username&amp;gt;): Remove this code after v1.75 has been released.
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>When the task depends on a certain implementation, a GitHub issue should be opened and referenced in the statement.
Example:
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-golang" data-lang="golang">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">// TODO(&amp;lt;github-username&amp;gt;): Remove this code after https://github.com/gardener/gardener/issues/&amp;lt;issue-number&amp;gt; has been implemented.
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>The associated person should actively drive the implementation of the referenced issue (unless it cannot be done because of third-party dependencies or conditions) so that the TODO statement does not get stale.&lt;/li>
&lt;li>TODO statements without actionable tasks or those that are unlikely to ever be implemented (maybe because of very low priorities) should not be specified in the first place. If a TODO is specified, the associated person should make sure to actively follow-up.&lt;/li>
&lt;/ul>
&lt;h2 id="cherry-picks">Cherry Picks&lt;/h2>
&lt;p>This section explains how to initiate cherry picks on release branches within the &lt;code>gardener/gardener&lt;/code> repository.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/process/#prerequisites">Prerequisites&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/gardener/process/#initiate-a-cherry-pick">Initiate a Cherry Pick&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="prerequisites">Prerequisites&lt;/h3>
&lt;p>Before you initiate a cherry pick, make sure that the following prerequisites are accomplished.&lt;/p>
&lt;ul>
&lt;li>A pull request merged against the &lt;code>master&lt;/code> branch.&lt;/li>
&lt;li>The release branch exists (check in the &lt;a href="https://github.com/gardener/gardener/branches">branches section&lt;/a>).&lt;/li>
&lt;li>Have the &lt;code>gardener/gardener&lt;/code> repository cloned as follows:
&lt;ul>
&lt;li>the &lt;code>origin&lt;/code> remote should point to your fork (alternatively this can be overwritten by passing &lt;code>FORK_REMOTE=&amp;lt;fork-remote&amp;gt;&lt;/code>).&lt;/li>
&lt;li>the &lt;code>upstream&lt;/code> remote should point to the Gardener GitHub org (alternatively this can be overwritten by passing &lt;code>UPSTREAM_REMOTE=&amp;lt;upstream-remote&amp;gt;&lt;/code>).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Have &lt;code>hub&lt;/code> installed, which is most easily installed via
&lt;code>go get github.com/github/hub&lt;/code> assuming you have a standard golang
development environment.&lt;/li>
&lt;li>A GitHub token which has permissions to create a PR in an upstream branch.&lt;/li>
&lt;/ul>
&lt;h3 id="initiate-a-cherry-pick">Initiate a Cherry Pick&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Run the [cherry pick script][cherry-pick-script].&lt;/p>
&lt;p>This example applies a master branch PR #3632 to the remote branch
&lt;code>upstream/release-v3.14&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>GITHUB_USER=&amp;lt;your-user&amp;gt; hack/cherry-pick-pull.sh upstream/release-v3.14 3632
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;p>Be aware the cherry pick script assumes you have a git remote called
&lt;code>upstream&lt;/code> that points at the Gardener GitHub org.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>You will need to run the cherry pick script separately for each patch
release you want to cherry pick to. Cherry picks should be applied to all
active release branches where the fix is applicable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When asked for your GitHub password, provide the created GitHub token
rather than your actual GitHub password.
Refer &lt;a href="https://github.com/github/hub/issues/2655#issuecomment-735836048">https://github.com/github/hub/issues/2655#issuecomment-735836048&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/gardener/gardener/blob/master/hack/cherry-pick-pull.sh">cherry-pick-script&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: Projects</title><link>https://gardener.cloud/docs/gardener/projects/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/projects/</guid><description>
&lt;h1 id="projects">Projects&lt;/h1>
&lt;p>The Gardener API server supports a cluster-scoped &lt;code>Project&lt;/code> resource which is used for data isolation between individual Gardener consumers. For example, each development team has its own project to manage its own shoot clusters.&lt;/p>
&lt;p>Each &lt;code>Project&lt;/code> is backed by a Kubernetes &lt;code>Namespace&lt;/code> that contains the actual related Kubernetes resources, like &lt;code>Secret&lt;/code>s or &lt;code>Shoot&lt;/code>s.&lt;/p>
&lt;p>&lt;strong>Example resource:&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Project
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden-dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> description: &lt;span style="color:#a31515">&amp;#34;This is my first project&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> purpose: &lt;span style="color:#a31515">&amp;#34;Experimenting with Gardener&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> owner:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: User
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: john.doe@example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> members:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: User
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: alice.doe@example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> role: admin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># roles:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - viewer &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - uam&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - serviceaccountmanager&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># - extension:foo&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: User
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: bob.doe@example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> role: viewer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># tolerations:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># defaults:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># - key: &amp;lt;some-key&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># whitelist:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># - key: &amp;lt;some-key&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>.spec.namespace&lt;/code> field is optional and is initialized if unset.
The name of the resulting namespace will be determined based on the &lt;code>Project&lt;/code> name and UID, e.g., &lt;code>garden-dev-5aef3&lt;/code>.
It&amp;rsquo;s also possible to adopt existing namespaces by labeling them &lt;code>gardener.cloud/role=project&lt;/code> and &lt;code>project.gardener.cloud/name=dev&lt;/code> beforehand (otherwise, they cannot be adopted).&lt;/p>
&lt;p>When deleting a Project resource, the corresponding namespace is also deleted.
To keep a namespace after project deletion, an administrator/operator (not Project members!) can annotate the project-namespace with &lt;code>namespace.gardener.cloud/keep-after-project-deletion&lt;/code>.&lt;/p>
&lt;p>The &lt;code>spec.description&lt;/code> and &lt;code>.spec.purpose&lt;/code> fields can be used to describe to fellow team members and Gardener operators what this project is used for.&lt;/p>
&lt;p>Each project has one dedicated owner, configured in &lt;code>.spec.owner&lt;/code> using the &lt;code>rbac.authorization.k8s.io/v1.Subject&lt;/code> type.
The owner is the main contact person for Gardener operators.
Please note that the &lt;code>.spec.owner&lt;/code> field is deprecated and will be removed in future API versions in favor of the &lt;code>owner&lt;/code> role, see below.&lt;/p>
&lt;p>The list of members (again a list in &lt;code>.spec.members[]&lt;/code> using the &lt;code>rbac.authorization.k8s.io/v1.Subject&lt;/code> type) contains all the people that are associated with the project in any way.
Each project member must have at least one role (currently described in &lt;code>.spec.members[].role&lt;/code>, additional roles can be added to &lt;code>.spec.members[].roles[]&lt;/code>). The following roles exist:&lt;/p>
&lt;ul>
&lt;li>&lt;code>admin&lt;/code>: This allows to fully manage resources inside the project (e.g., secrets, shoots, configmaps, and similar). Mind that the &lt;code>admin&lt;/code> role has read only access to service accounts.&lt;/li>
&lt;li>&lt;code>serviceaccountmanager&lt;/code>: This allows to fully manage service accounts inside the project namespace and request tokens for them. The permissions of the created service accounts are instead managed by the &lt;code>admin&lt;/code> role. Please refer to &lt;a href="https://gardener.cloud/docs/gardener/service-account-manager/">Service Account Manager&lt;/a>.&lt;/li>
&lt;li>&lt;code>uam&lt;/code>: This allows to add/modify/remove human users or groups to/from the project member list.&lt;/li>
&lt;li>&lt;code>viewer&lt;/code>: This allows to read all resources inside the project except secrets.&lt;/li>
&lt;li>&lt;code>owner&lt;/code>: This combines the &lt;code>admin&lt;/code>, &lt;code>uam&lt;/code>, and &lt;code>serviceaccountmanager&lt;/code> roles.&lt;/li>
&lt;li>Extension roles (prefixed with &lt;code>extension:&lt;/code>): Please refer to &lt;a href="https://gardener.cloud/docs/gardener/extensions/project-roles/">Extending Project Roles&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>The &lt;a href="https://gardener.cloud/docs/gardener/concepts/controller-manager/#project-controller">project controller&lt;/a> inside the Gardener Controller Manager is managing RBAC resources that grant the described privileges to the respective members.&lt;/p>
&lt;p>There are three central &lt;code>ClusterRole&lt;/code>s &lt;code>gardener.cloud:system:project-member&lt;/code>, &lt;code>gardener.cloud:system:project-viewer&lt;/code>, and &lt;code>gardener.cloud:system:project-serviceaccountmanager&lt;/code> that grant the permissions for namespaced resources (e.g., &lt;code>Secret&lt;/code>s, &lt;code>Shoot&lt;/code>s, &lt;code>ServiceAccount&lt;/code>s).
Via referring &lt;code>RoleBinding&lt;/code>s created in the respective namespace the project members get bound to these &lt;code>ClusterRole&lt;/code>s and, thus, the needed permissions.
There are also project-specific &lt;code>ClusterRole&lt;/code>s granting the permissions for cluster-scoped resources, e.g., the &lt;code>Namespace&lt;/code> or &lt;code>Project&lt;/code> itself.&lt;br>
For each role, the following &lt;code>ClusterRole&lt;/code>s, &lt;code>ClusterRoleBinding&lt;/code>s, and &lt;code>RoleBinding&lt;/code>s are created:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Role&lt;/th>
&lt;th>&lt;code>ClusterRole&lt;/code>&lt;/th>
&lt;th>&lt;code>ClusterRoleBinding&lt;/code>&lt;/th>
&lt;th>&lt;code>RoleBinding&lt;/code>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>admin&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-member:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-member:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-member&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>serviceaccountmanager&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-serviceaccountmanager&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>uam&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-uam:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-uam:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>viewer&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-viewer:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-viewer:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project-viewer&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>owner&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:system:project:&amp;lt;projectName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>extension:*&lt;/code>&lt;/td>
&lt;td>&lt;code>gardener.cloud:extension:project:&amp;lt;projectName&amp;gt;:&amp;lt;extensionRoleName&amp;gt;&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;code>gardener.cloud:extension:project:&amp;lt;projectName&amp;gt;:&amp;lt;extensionRoleName&amp;gt;&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="user-access-management">User Access Management&lt;/h2>
&lt;p>For &lt;code>Project&lt;/code>s created before Gardener v1.8, all admins were allowed to manage other members.
Beginning with v1.8, the new &lt;code>uam&lt;/code> role is being introduced.
It is backed by the &lt;code>manage-members&lt;/code> custom RBAC verb which allows to add/modify/remove human users or groups to/from the project member list.
Human users are subjects with &lt;code>kind=User&lt;/code> and &lt;code>name!=system:serviceaccount:*&lt;/code>, and groups are subjects with &lt;code>kind=Group&lt;/code>.
The management of service account subjects (&lt;code>kind=ServiceAccount&lt;/code> or &lt;code>name=system:serviceaccount:*&lt;/code>) is not controlled via the &lt;code>uam&lt;/code> custom verb but with the standard &lt;code>update&lt;/code>/&lt;code>patch&lt;/code> verbs for projects.&lt;/p>
&lt;p>All newly created projects will only bind the owner to the &lt;code>uam&lt;/code> role.
The owner can still grant the &lt;code>uam&lt;/code> role to other members if desired.
For projects created before Gardener v1.8, the Gardener Controller Manager will migrate all projects to also assign the &lt;code>uam&lt;/code> role to all &lt;code>admin&lt;/code> members (to not break existing use-cases). The corresponding migration logic is present in Gardener Controller Manager from v1.8 to v1.13.
The project owner can gradually remove these roles if desired.&lt;/p>
&lt;h2 id="stale-projects">Stale Projects&lt;/h2>
&lt;p>When a project is not actively used for some period of time, it is marked as &amp;ldquo;stale&amp;rdquo;. This is done by a controller called &lt;a href="https://gardener.cloud/docs/gardener/concepts/controller-manager/#stale-projects-reconciler">&amp;ldquo;Stale Projects Reconciler&amp;rdquo;&lt;/a>. Once the project is marked as stale, there is a time frame in which if not used it will be deleted by that controller.&lt;/p>
&lt;h2 id="four-eyes-principle-for-resource-deletion">Four-Eyes-Principle For Resource Deletion&lt;/h2>
&lt;p>In order to delete a &lt;code>Shoot&lt;/code>, the deletion must be confirmed upfront with the &lt;code>confirmation.gardener.cloud/deletion=true&lt;/code> annotation.
Without this annotation being set, &lt;code>gardener-apiserver&lt;/code> denies any DELETE request.
Still, users sometimes accidentally shot themselves in the foot, meaning that they accidentally deleted a &lt;code>Shoot&lt;/code> despite the confirmation requirement.&lt;/p>
&lt;p>To prevent that (or make it harder, at least), the &lt;code>Project&lt;/code> can be configured to apply the dual approval concept for &lt;code>Shoot&lt;/code> deletion.
This means that the subject confirming the deletion must not be the same as the subject sending the DELETE request.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dualApprovalForDeletion:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - resource: shoots
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> selector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> includeServiceAccounts: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>[!NOTE]
As of today, &lt;code>core.gardener.cloud/v1beta1.Shoot&lt;/code> is the only resource for which this concept is implemented.&lt;/p>
&lt;/blockquote>
&lt;p>As usual, &lt;code>.spec.dualApprovalForDeletion[].selector.matchLabels={}&lt;/code> matches all resources, &lt;code>.spec.dualApprovalForDeletion[].selector.matchLabels=null&lt;/code> matches none at all.
It can also be decided to specify an individual label selector if this concept shall only apply to a subset of the &lt;code>Shoot&lt;/code>s in the project (e.g., CI/development clusters shall be excluded).&lt;/p>
&lt;p>The &lt;code>includeServiceAccounts&lt;/code> (default: &lt;code>true&lt;/code>) controls whether the concept also applies when the &lt;code>Shoot&lt;/code> deletion confirmation and actual deletion is triggered via &lt;code>ServiceAccount&lt;/code>s.
This is to prevent that CI jobs have to follow this concept as well, adding additional complexity/overhead.
Alternatively, you could also use two &lt;code>ServiceAccount&lt;/code>s, one for confirming the deletion, and another one for actually sending the DELETE request, if desired.&lt;/p>
&lt;blockquote>
&lt;p>[!IMPORTANT]
Project members can still change the labels of &lt;code>Shoot&lt;/code>s (or the selector itself) to circumvent the dual approval concept.
This concern is intentionally excluded/ignored for now since the principle is not a &amp;ldquo;security feature&amp;rdquo; but shall just help preventing &lt;em>accidental&lt;/em> deletion.&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Readiness of Shoot Worker Nodes</title><link>https://gardener.cloud/docs/gardener/node-readiness/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/node-readiness/</guid><description>
&lt;h1 id="readiness-of-shoot-worker-nodes">Readiness of Shoot Worker Nodes&lt;/h1>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>When registering new &lt;code>Nodes&lt;/code>, kubelet adds the &lt;code>node.kubernetes.io/not-ready&lt;/code> taint to prevent scheduling workload Pods to the &lt;code>Node&lt;/code> until the &lt;code>Ready&lt;/code> condition gets &lt;code>True&lt;/code>.
However, the kubelet does not consider the readiness of node-critical Pods.
Hence, the &lt;code>Ready&lt;/code> condition might get &lt;code>True&lt;/code> and the &lt;code>node.kubernetes.io/not-ready&lt;/code> taint might get removed, for example, before the CNI daemon Pod (e.g., &lt;code>calico-node&lt;/code>) has successfully placed the CNI binaries on the machine.&lt;/p>
&lt;p>This problem has been discussed extensively in kubernetes, e.g., in &lt;a href="https://github.com/kubernetes/kubernetes/issues/75890">kubernetes/kubernetes#75890&lt;/a>.
However, several proposals have been rejected because the problem can be solved by using the &lt;code>--register-with-taints&lt;/code> kubelet flag and dedicated controllers (&lt;a href="https://github.com/kubernetes/enhancements/pull/1003#issuecomment-619087019">ref&lt;/a>).&lt;/p>
&lt;h2 id="implementation-in-gardener">Implementation in Gardener&lt;/h2>
&lt;p>Gardener makes sure that workload Pods are only scheduled to &lt;code>Nodes&lt;/code> where all node-critical components required for running workload Pods are ready.
For this, Gardener follows the proposed solution by the Kubernetes community and registers new &lt;code>Node&lt;/code> objects with the &lt;code>node.gardener.cloud/critical-components-not-ready&lt;/code> taint (effect &lt;code>NoSchedule&lt;/code>).
gardener-resource-manager&amp;rsquo;s &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/#node-controller">&lt;code>Node&lt;/code> controller&lt;/a> reacts on newly created &lt;code>Node&lt;/code> objects that have this taint.
The controller removes the taint once all node-critical Pods are ready (determined by checking the Pods&amp;rsquo; &lt;code>Ready&lt;/code> conditions).&lt;/p>
&lt;p>The &lt;code>Node&lt;/code> controller considers all &lt;code>DaemonSets&lt;/code> and &lt;code>Pods&lt;/code> with the label &lt;code>node.gardener.cloud/critical-component=true&lt;/code> as node-critical.
If there are &lt;code>DaemonSets&lt;/code> that contain the &lt;code>node.gardener.cloud/critical-component=true&lt;/code> label in their metadata and in their Pod template, the &lt;code>Node&lt;/code> controller waits for corresponding daemon Pods to be scheduled and to get ready before removing the taint.&lt;/p>
&lt;p>Additionally, the &lt;code>Node&lt;/code> controller checks for the readiness of &lt;code>csi-driver-node&lt;/code> components if a respective Pod indicates that it uses such a driver.
This is achieved through a well-defined annotation prefix (&lt;code>node.gardener.cloud/wait-for-csi-node-&lt;/code>).
For example, the &lt;code>csi-driver-node&lt;/code> Pod for Openstack Cinder is annotated with &lt;code>node.gardener.cloud/wait-for-csi-node-cinder=cinder.csi.openstack.org&lt;/code>.
A key prefix is used instead of a &amp;ldquo;regular&amp;rdquo; annotation to allow for multiple CSI drivers being registered by one &lt;code>csi-driver-node&lt;/code> Pod.
The annotation key&amp;rsquo;s suffix can be chosen arbitrarily (in this case &lt;code>cinder&lt;/code>) and the annotation value needs to match the actual driver name as specified in the &lt;code>CSINode&lt;/code> object.
The &lt;code>Node&lt;/code> controller will verify that the used driver is properly registered in this object before removing the &lt;code>node.gardener.cloud/critical-components-not-ready&lt;/code> taint.
Note that the &lt;code>csi-driver-node&lt;/code> Pod still needs to be labelled and tolerate the taint as described above to be considered in this additional check.&lt;/p>
&lt;h2 id="marking-node-critical-components">Marking Node-Critical Components&lt;/h2>
&lt;p>To make use of this feature, node-critical DaemonSets and Pods need to:&lt;/p>
&lt;ul>
&lt;li>Tolerate the &lt;code>node.gardener.cloud/critical-components-not-ready&lt;/code> &lt;code>NoSchedule&lt;/code> taint.&lt;/li>
&lt;li>Be labelled with &lt;code>node.gardener.cloud/critical-component=true&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>&lt;code>csi-driver-node&lt;/code> Pods additionally need to:&lt;/p>
&lt;ul>
&lt;li>Be annotated with &lt;code>node.gardener.cloud/wait-for-csi-node-&amp;lt;name&amp;gt;=&amp;lt;full-driver-name&amp;gt;&lt;/code>.
It&amp;rsquo;s required that these Pods fulfill the above criteria (label and toleration) as well.&lt;/li>
&lt;/ul>
&lt;p>Gardener already marks components like kube-proxy, apiserver-proxy and node-local-dns as node-critical.
Provider extensions mark components like csi-driver-node as node-critical and add the &lt;code>wait-for-csi-node&lt;/code> annotation.
Network extensions mark components responsible for setting up CNI on worker Nodes (e.g., &lt;code>calico-node&lt;/code>) as node-critical.
If shoot owners manage any additional node-critical components, they can make use of this feature as well.&lt;/p></description></item><item><title>Docs: Reversed VPN Tunnel</title><link>https://gardener.cloud/docs/gardener/reversed-vpn-tunnel/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/reversed-vpn-tunnel/</guid><description>
&lt;h1 id="reversed-vpn-tunnel-setup-and-configuration">Reversed VPN Tunnel Setup and Configuration&lt;/h1>
&lt;p>The Reversed VPN Tunnel is enabled by default.
A highly available VPN connection is automatically deployed in all shoots that configure an HA control-plane.&lt;/p>
&lt;h2 id="reversed-vpn-tunnel">Reversed VPN Tunnel&lt;/h2>
&lt;p>In the first VPN solution, connection establishment was initiated by a VPN client in the seed cluster.
Due to several issues with this solution, the tunnel establishment direction has been reverted.
The client is deployed in the shoot and initiates the connection from there. This way, there is no need to deploy a special purpose
loadbalancer for the sake of addressing the data-plane, in addition to saving costs, this is considered the more secure alternative.
For more information on how this is achieved, please have a look at the following &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/14-reversed-cluster-vpn.md">GEP&lt;/a>.&lt;/p>
&lt;p>Connection establishment with a reversed tunnel:&lt;/p>
&lt;p>&lt;code>APIServer --&amp;gt; Envoy-Proxy | VPN-Seed-Server &amp;lt;-- Istio/Envoy-Proxy &amp;lt;-- SNI API Server Endpoint &amp;lt;-- LB (one for all clusters of a seed) &amp;lt;--- internet &amp;lt;--- VPN-Shoot-Client --&amp;gt; Pods | Nodes | Services&lt;/code>&lt;/p>
&lt;h2 id="high-availability-for-reversed-vpn-tunnel">High Availability for Reversed VPN Tunnel&lt;/h2>
&lt;p>Shoots which define &lt;code>spec.controlPlane.highAvailability.failureTolerance: {node, zone}&lt;/code> get an HA control-plane, including a
highly available VPN connection by deploying redundant VPN servers and clients.&lt;/p>
&lt;p>Please note that it is not possible to move an open connection to another VPN tunnel. Especially long-running
commands like &lt;code>kubectl exec -it ...&lt;/code> or &lt;code>kubectl logs -f ...&lt;/code> will still break if the routing path must be switched
because either VPN server or client are not reachable anymore. A new request should be possible within seconds.&lt;/p>
&lt;h3 id="ha-architecture-for-vpn">HA Architecture for VPN&lt;/h3>
&lt;p>Establishing a connection from the VPN client on the shoot to the server in the control plane works nearly the same
way as in the non-HA case. The only difference is that the VPN client targets one of two VPN servers, represented by two services
&lt;code>vpn-seed-server-0&lt;/code> and &lt;code>vpn-seed-server-1&lt;/code> with endpoints in pods with the same name.
The VPN tunnel is used by a &lt;code>kube-apiserver&lt;/code> to reach nodes, services, or pods in the shoot cluster.
In the non-HA case, a kube-apiserver uses an HTTP proxy running as a side-car in the VPN server to address
the shoot networks via the VPN tunnel and the &lt;code>vpn-shoot&lt;/code> acts as a router.
In the HA case, the setup is more complicated. Instead of an HTTP proxy in the VPN server, the kube-apiserver has
additional side-cars, one side-car for each VPN client to connect to the corresponding VPN server.
On the shoot side, there are now two &lt;code>vpn-shoot&lt;/code> pods, each with two VPN clients for each VPN server.
With this setup, there would be four possible routes, but only one can be used. Switching the route kills all
open connections. Therefore, another layer is introduced: link aggregation, also named &lt;a href="https://www.kernel.org/doc/Documentation/networking/bonding.txt">bonding&lt;/a>.
In Linux, you can create a network link by using several other links as slaves. Bonding here is used with
active-backup mode. This means the traffic only goes through the active sublink and is only changed if the active one
becomes unavailable. Switching happens in the bonding network driver without changing any routes. So with this layer,
vpn-seed-server pods can be rolled without disrupting open connections.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/vpn-ha-architecture_2f71aa.png" alt="VPN HA Architecture">&lt;/p>
&lt;p>With bonding, there are 2 possible routing paths, ensuring that there is at least one routing path intact even if
one &lt;code>vpn-seed-server&lt;/code> pod and one &lt;code>vpn-shoot&lt;/code> pod are unavailable at the same time.&lt;/p>
&lt;p>As it is not possible to use multi-path routing, one routing path must be configured explicitly.
For this purpose, the &lt;code>path-controller&lt;/code> script is running in another side-car of the kube-apiserver pod.
It pings all shoot-side VPN clients regularly every few seconds. If the active routing path is not responsive anymore,
the routing is switched to the other responsive routing path.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/vpn-ha-routing-paths_aa0d8d.png" alt="Four possible routing paths">&lt;/p>
&lt;p>For general information about HA control-plane, see &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/20-ha-control-planes.md">GEP-20&lt;/a>.&lt;/p></description></item><item><title>Docs: Secrets Management</title><link>https://gardener.cloud/docs/gardener/secrets_management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardener/secrets_management/</guid><description>
&lt;h1 id="secrets-management-for-seed-and-shoot-cluster">Secrets Management for Seed and Shoot Cluster&lt;/h1>
&lt;p>The gardenlet needs to create quite some amount of credentials (certificates, private keys, passwords) for seed and shoot clusters in order to ensure secure deployments.
Such credentials typically should be renewed automatically when their validity expires, rotated regularly, and they potentially need to be persisted such that they don&amp;rsquo;t get lost in case of a control plane migration or a lost seed cluster.&lt;/p>
&lt;h2 id="secretsmanager-introduction">SecretsManager Introduction&lt;/h2>
&lt;p>These requirements can be covered by using the &lt;code>SecretsManager&lt;/code> package maintained in &lt;a href="https://github.com/gardener/gardener/tree/master/pkg/utils/secrets/manager">&lt;code>pkg/utils/secrets/manager&lt;/code>&lt;/a>.
It is built on top of the &lt;code>ConfigInterface&lt;/code> and &lt;code>DataInterface&lt;/code> interfaces part of &lt;a href="https://github.com/gardener/gardener/tree/master/pkg/utils/secrets">&lt;code>pkg/utils/secrets&lt;/code>&lt;/a> and provides the following functions:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>Generate(context.Context, secrets.ConfigInterface, ...GenerateOption) (*corev1.Secret, error)&lt;/code>&lt;/p>
&lt;p>This method either retrieves the current secret for the given configuration or it (re)generates it in case the configuration changed, the signing CA changed (for certificate secrets), or when proactive rotation was triggered.
If the configuration describes a certificate authority secret then this method automatically generates a bundle secret containing the current and potentially the old certificate.
Available &lt;code>GenerateOption&lt;/code>s:&lt;/p>
&lt;ul>
&lt;li>&lt;code>SignedByCA(string, ...SignedByCAOption)&lt;/code>: This is only valid for certificate secrets and automatically retrieves the correct certificate authority in order to sign the provided server or client certificate.
&lt;ul>
&lt;li>There are two &lt;code>SignedByCAOption&lt;/code>s:
&lt;ul>
&lt;li>&lt;code>UseCurrentCA&lt;/code>. This option will sign server certificates with the new/current CA in case of a CA rotation. For more information, please refer to the &lt;a href="https://gardener.cloud/docs/gardener/secrets_management/#certificate-signing">&amp;ldquo;Certificate Signing&amp;rdquo;&lt;/a> section below.&lt;/li>
&lt;li>&lt;code>UseOldCA&lt;/code>. This option will sign client certificates with the old CA in case of a CA rotation. For more information, please refer to the &lt;a href="https://gardener.cloud/docs/gardener/secrets_management/#certificate-signing">&amp;ldquo;Certificate Signing&amp;rdquo;&lt;/a> section below.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Persist()&lt;/code>: This marks the secret such that it gets persisted in the &lt;code>ShootState&lt;/code> resource in the garden cluster. Consequently, it should only be used for secrets related to a shoot cluster.&lt;/li>
&lt;li>&lt;code>Rotate(rotationStrategy)&lt;/code>: This specifies the strategy in case this secret is to be rotated or regenerated (either &lt;code>InPlace&lt;/code> which immediately forgets about the old secret, or &lt;code>KeepOld&lt;/code> which keeps the old secret in the system).&lt;/li>
&lt;li>&lt;code>IgnoreOldSecrets()&lt;/code>: This specifies that old secrets should not be considered and loaded (contrary to the default behavior). It should be used when old secrets are no longer important and can be &amp;ldquo;forgotten&amp;rdquo; (e.g. in &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/18-shoot-CA-rotation.md#rotation-sequence-for-cluster-and-client-ca">&amp;ldquo;phase 2&amp;rdquo; (&lt;code>t2&lt;/code>) of the CA certificate rotation&lt;/a>). Such old secrets will be deleted on &lt;code>Cleanup()&lt;/code>.&lt;/li>
&lt;li>&lt;code>IgnoreOldSecretsAfter(time.Duration)&lt;/code>: This specifies that old secrets should not be considered and loaded once a given duration after rotation has passed. It can be used to clean up old secrets after automatic rotation (e.g. the Seed cluster CA is automatically rotated when its validity will soon end and the old CA will be cleaned up 24 hours after triggering the rotation).&lt;/li>
&lt;li>&lt;code>Validity(time.Duration)&lt;/code>: This specifies how long the secret should be valid. For certificate secret configurations, the manager will automatically deduce this information from the generated certificate.&lt;/li>
&lt;li>&lt;code>RenewAfterValidityPercentage(int)&lt;/code>: This specifies the percentage of validity for renewal. The secret will be renewed based on whichever comes first: The specified percentage of validity or 10 days before end of validity. If not specified, the default percentage is &lt;code>80&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Get(string, ...GetOption) (*corev1.Secret, bool)&lt;/code>&lt;/p>
&lt;p>This method retrieves the current secret for the given name.
In case the secret in question is a certificate authority secret then it retrieves the bundle secret by default.
It is important that this method only knows about secrets for which there were prior &lt;code>Generate&lt;/code> calls.
Available &lt;code>GetOption&lt;/code>s:&lt;/p>
&lt;ul>
&lt;li>&lt;code>Bundle&lt;/code> (default): This retrieves the bundle secret.&lt;/li>
&lt;li>&lt;code>Current&lt;/code>: This retrieves the current secret.&lt;/li>
&lt;li>&lt;code>Old&lt;/code>: This retrieves the old secret.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Cleanup(context.Context) error&lt;/code>&lt;/p>
&lt;p>This method deletes secrets which are no longer required.
No longer required secrets are those still existing in the system which weren&amp;rsquo;t detected by prior &lt;code>Generate&lt;/code> calls.
Consequently, only call &lt;code>Cleanup&lt;/code> after you have executed &lt;code>Generate&lt;/code> calls for all desired secrets.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Some exemplary usages would look as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>secret, err := k.secretsManager.Generate(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;amp;secrets.CertificateSecretConfig{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Name: &lt;span style="color:#a31515">&amp;#34;my-server-secret&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> CommonName: &lt;span style="color:#a31515">&amp;#34;server-abc&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> DNSNames: []&lt;span style="color:#2b91af">string&lt;/span>{&lt;span style="color:#a31515">&amp;#34;first-name&amp;#34;&lt;/span>, &lt;span style="color:#a31515">&amp;#34;second-name&amp;#34;&lt;/span>},
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> CertType: secrets.ServerCert,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> SkipPublishingCACertificate: &lt;span style="color:#00f">true&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretsmanager.SignedByCA(&lt;span style="color:#a31515">&amp;#34;my-ca&amp;#34;&lt;/span>),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretsmanager.Persist(),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretsmanager.Rotate(secretsmanager.InPlace),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">if&lt;/span> err != &lt;span style="color:#00f">nil&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00f">return&lt;/span> err
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As explained above, the caller does not need to care about the renewal, rotation or the persistence of this secret - all of these concerns are handled by the secrets manager.
Automatic renewal of secrets happens when their validity approaches 80% or less than &lt;code>10d&lt;/code> are left until expiration.&lt;/p>
&lt;p>In case a CA certificate is needed by some component, then it can be retrieved as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>caSecret, found := k.secretsManager.Get(&lt;span style="color:#a31515">&amp;#34;my-ca&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f">if&lt;/span> !found {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#00f">return&lt;/span> fmt.Errorf(&lt;span style="color:#a31515">&amp;#34;secret my-ca not found&amp;#34;&lt;/span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As explained above, this returns the bundle secret for the CA &lt;code>my-ca&lt;/code> which might potentially contain both the current and the old CA (in case of rotation/regeneration).&lt;/p>
&lt;h3 id="certificate-signing">Certificate Signing&lt;/h3>
&lt;h4 id="default-behaviour">Default Behaviour&lt;/h4>
&lt;p>By default, client certificates are signed by the current CA while server certificate are signed by the old CA (if it exists).
This is to ensure a smooth exchange of certificate during a CA rotation (typically has two phases, ref &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/18-shoot-CA-rotation.md#rotation-sequence-for-cluster-and-client-ca">GEP-18&lt;/a>):&lt;/p>
&lt;ul>
&lt;li>Client certificates:
&lt;ul>
&lt;li>In phase 1, clients get new certificates as soon as possible to ensure that all clients have been adapted before phase 2.&lt;/li>
&lt;li>In phase 2, the respective server drops accepting certificates signed by the old CA.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Server certificates:
&lt;ul>
&lt;li>In phase 1, servers still use their old/existing certificates to allow clients to update their CA bundle used for verification of the servers&amp;rsquo; certificates.&lt;/li>
&lt;li>In phase 2, the old CA is dropped, hence servers need to get a certificate signed by the new/current CA. At this point in time, clients have already adapted their CA bundles.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="alternative-sign-server-certificates-with-current-ca">Alternative: Sign Server Certificates with Current CA&lt;/h4>
&lt;p>In case you control all clients and update them at the same time as the server, it is possible to make the secrets manager generate even server certificates with the new/current CA.
This can help to prevent certificate mismatches when the CA bundle is already exchanged while the server still serves with a certificate signed by a CA no longer part of the bundle.&lt;/p>
&lt;p>Let&amp;rsquo;s consider the two following examples:&lt;/p>
&lt;ol>
&lt;li>&lt;code>gardenlet&lt;/code> deploys a webhook server (&lt;code>gardener-resource-manager&lt;/code>) and a corresponding &lt;code>MutatingWebhookConfiguration&lt;/code> at the same time. In this case, the server certificate should be generated with the new/current CA to avoid above mentioned certificate mismatches during a CA rotation.&lt;/li>
&lt;li>&lt;code>gardenlet&lt;/code> deploys a server (&lt;code>etcd&lt;/code>) in one step, and a client (&lt;code>kube-apiserver&lt;/code>) in a subsequent step. In this case, the default behaviour should apply (server certificate should be signed by old/existing CA).&lt;/li>
&lt;/ol>
&lt;h4 id="alternative-sign-client-certificate-with-old-ca">Alternative: Sign Client Certificate with Old CA&lt;/h4>
&lt;p>In the unusual case where the client is deployed before the server, it might be useful to always use the old CA for signing the client&amp;rsquo;s certificate.
This can help to prevent certificate mismatches when the client already gets a new certificate while the server still only accepts certificates signed by the old CA.&lt;/p>
&lt;p>Let&amp;rsquo;s consider the two following examples:&lt;/p>
&lt;ol>
&lt;li>&lt;code>gardenlet&lt;/code> deploys the &lt;code>kube-apiserver&lt;/code> before the &lt;code>kubelet&lt;/code>. However, the &lt;code>kube-apiserver&lt;/code> has a client certificate signed by the &lt;code>ca-kubelet&lt;/code> in order to communicate with it (e.g., when retrieving logs or forwarding ports). In this case, the client certificate should be generated with the old CA to avoid above mentioned certificate mismatches during a CA rotation.&lt;/li>
&lt;li>&lt;code>gardenlet&lt;/code> deploys a server (&lt;code>etcd&lt;/code>) in one step, and a client (&lt;code>kube-apiserver&lt;/code>) in a subsequent step. In this case, the default behaviour should apply (client certificate should be signed by new/current CA).&lt;/li>
&lt;/ol>
&lt;h2 id="reusing-the-secretsmanager-in-other-components">Reusing the SecretsManager in Other Components&lt;/h2>
&lt;p>While the &lt;code>SecretsManager&lt;/code> is primarily used by gardenlet, it can be reused by other components (e.g. extensions) as well for managing secrets that are specific to the component or extension. For example, provider extensions might use their own &lt;code>SecretsManager&lt;/code> instance for managing the serving certificate of &lt;code>cloud-controller-manager&lt;/code>.&lt;/p>
&lt;p>External components that want to reuse the &lt;code>SecretsManager&lt;/code> should consider the following aspects:&lt;/p>
&lt;ul>
&lt;li>On initialization of a &lt;code>SecretsManager&lt;/code>, pass an &lt;code>identity&lt;/code> specific to the component, controller and purpose. For example, gardenlet&amp;rsquo;s shoot controller uses &lt;code>gardenlet&lt;/code> as the &lt;code>SecretsManager&lt;/code>&amp;rsquo;s identity, the &lt;code>Worker&lt;/code> controller in &lt;code>provider-foo&lt;/code> should use &lt;code>provider-foo-worker&lt;/code>, and the &lt;code>ControlPlane&lt;/code> controller should use &lt;code>provider-foo-controlplane-exposure&lt;/code> for &lt;code>ControlPlane&lt;/code> objects of purpose &lt;code>exposure&lt;/code>.
The given identity is added as a value for the &lt;code>manager-identity&lt;/code> label on managed &lt;code>Secret&lt;/code>s.
This label is used by the &lt;code>Cleanup&lt;/code> function to select only those &lt;code>Secret&lt;/code>s that are actually managed by the particular &lt;code>SecretManager&lt;/code> instance. This is done to prevent removing still needed &lt;code>Secret&lt;/code>s that are managed by other instances.&lt;/li>
&lt;li>Generate dedicated CAs for signing certificates instead of depending on CAs managed by gardenlet.&lt;/li>
&lt;li>Names of &lt;code>Secret&lt;/code>s managed by external &lt;code>SecretsManager&lt;/code> instances must not conflict with &lt;code>Secret&lt;/code> names from other instances (e.g. gardenlet).&lt;/li>
&lt;li>For CAs that should be rotated in lock-step with the Shoot CAs managed by gardenlet, components need to pass information about the last rotation initiation time and the current rotation phase to the &lt;code>SecretsManager&lt;/code> upon initialization.
The relevant information can be retrieved from the &lt;code>Cluster&lt;/code> resource under &lt;code>.spec.shoot.status.credentials.rotation.certificateAuthorities&lt;/code>.&lt;/li>
&lt;li>Independent of the specific identity, secrets marked with the &lt;code>Persist&lt;/code> option are automatically saved in the &lt;code>ShootState&lt;/code> resource by the gardenlet and are also restored by the gardenlet on Control Plane Migration to the new Seed.&lt;/li>
&lt;/ul>
&lt;h2 id="migrating-existing-secrets-to-secretsmanager">Migrating Existing Secrets To SecretsManager&lt;/h2>
&lt;p>If you already have existing secrets which were not created with &lt;code>SecretsManager&lt;/code>, then you can (optionally) migrate them by labeling them with &lt;code>secrets-manager-use-data-for-name=&amp;lt;config-name&amp;gt;&lt;/code>.
For example, if your &lt;code>SecretsManager&lt;/code> generates a &lt;code>CertificateConfigSecret&lt;/code> with name &lt;code>foo&lt;/code> like this&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>secret, err := k.secretsManager.Generate(
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ctx,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;amp;secrets.CertificateSecretConfig{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Name: &lt;span style="color:#a31515">&amp;#34;foo&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">// ...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">&lt;/span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>and you already have an existing secret in your system whose data should be kept instead of regenerated, then labeling it with &lt;code>secrets-manager-use-data-for-name=foo&lt;/code> will instruct &lt;code>SecretsManager&lt;/code> accordingly.&lt;/p>
&lt;p>&lt;strong>⚠️ Caveat: You have to make sure that the existing &lt;code>data&lt;/code> keys match with what &lt;code>SecretsManager&lt;/code> uses:&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Secret Type&lt;/th>
&lt;th>Data Keys&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Basic Auth&lt;/td>
&lt;td>&lt;code>username&lt;/code>, &lt;code>password&lt;/code>, &lt;code>auth&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CA Certificate&lt;/td>
&lt;td>&lt;code>ca.crt&lt;/code>, &lt;code>ca.key&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Non-CA Certificate&lt;/td>
&lt;td>&lt;code>tls.crt&lt;/code>, &lt;code>tls.key&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Control Plane Secret&lt;/td>
&lt;td>&lt;code>ca.crt&lt;/code>, &lt;code>username&lt;/code>, &lt;code>password&lt;/code>, &lt;code>token&lt;/code>, &lt;code>kubeconfig&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ETCD Encryption Key&lt;/td>
&lt;td>&lt;code>key&lt;/code>, &lt;code>secret&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kubeconfig&lt;/td>
&lt;td>&lt;code>kubeconfig&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>RSA Private Key&lt;/td>
&lt;td>&lt;code>id_rsa&lt;/code>, &lt;code>id_rsa.pub&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Static Token&lt;/td>
&lt;td>&lt;code>static_tokens.csv&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>VPN TLS Auth&lt;/td>
&lt;td>&lt;code>vpn.tlsauth&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="implementation-details">Implementation Details&lt;/h2>
&lt;p>The source of truth for the secrets manager is the list of &lt;code>Secret&lt;/code>s in the Kubernetes cluster it acts upon (typically, the seed cluster).
The persisted secrets in the &lt;code>ShootState&lt;/code> are only used if and only if the shoot is in the &lt;code>Restore&lt;/code> phase - in this case all secrets are just synced to the seed cluster so that they can be picked up by the secrets manager.&lt;/p>
&lt;p>In order to prevent kubelets from unneeded watches (thus, causing some significant traffic against the &lt;code>kube-apiserver&lt;/code>), the &lt;code>Secret&lt;/code>s are marked as immutable.
Consequently, they have a unique, deterministic name which is computed as follows:&lt;/p>
&lt;ul>
&lt;li>For CA secrets, the name is just exactly the name specified in the configuration (e.g., &lt;code>ca&lt;/code>). This is for backwards-compatibility and will be dropped in a future release once all components depending on the static name have been adapted.&lt;/li>
&lt;li>For all other secrets, the name specified in the configuration is used as prefix followed by an 8-digit hash. This hash is computed out of the checksum of the secret configuration and the checksum of the certificate of the signing CA (only for certificate configurations).&lt;/li>
&lt;/ul>
&lt;p>In all cases, the name of the secrets is suffixed with a 5-digit hash computed out of the time when the rotation for this secret was last started.&lt;/p></description></item></channel></rss>