<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener – Docs</title><link>https://gardener.cloud/docs/</link><description>Recent content in Docs on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 22 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://gardener.cloud/docs/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: API Reference</title><link>https://gardener.cloud/docs/other-components/etcd-druid/api-reference/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-druid/api-reference/</guid><description>
&lt;p>Packages:&lt;/p>
&lt;ul>
&lt;li>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1">druid.gardener.cloud/v1alpha1&lt;/a>
&lt;/li>
&lt;/ul>
&lt;h2 id="druid.gardener.cloud/v1alpha1">druid.gardener.cloud/v1alpha1&lt;/h2>
&lt;p>
&lt;p>Package v1alpha1 is the v1alpha1 version of the etcd-druid API.&lt;/p>
&lt;/p>
Resource Types:
&lt;ul>&lt;/ul>
&lt;h3 id="druid.gardener.cloud/v1alpha1.BackupSpec">BackupSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec">EtcdSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>BackupSpec defines parameters associated with the full and delta snapshots of etcd.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>port&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Port define the port on which etcd-backup-restore server will be exposed.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>tls&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig">
TLSConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>image&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Image defines the etcd container image and tag&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>store&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec">
StoreSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Store defines the specification of object store provider for storing backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>resources&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core">
Kubernetes core/v1.ResourceRequirements
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Resources defines compute Resources required by backup-restore container.
More info: &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/">https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>compactionResources&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core">
Kubernetes core/v1.ResourceRequirements
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>CompactionResources defines compute Resources required by compaction job.
More info: &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/">https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>fullSnapshotSchedule&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>FullSnapshotSchedule defines the cron standard schedule for full snapshots.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>garbageCollectionPolicy&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.GarbageCollectionPolicy">
GarbageCollectionPolicy
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>GarbageCollectionPolicy defines the policy for garbage collecting old backups&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>garbageCollectionPeriod&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>GarbageCollectionPeriod defines the period for garbage collecting old backups&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>deltaSnapshotPeriod&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>DeltaSnapshotPeriod defines the period after which delta snapshots will be taken&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>deltaSnapshotMemoryLimit&lt;/code>&lt;/br>
&lt;em>
k8s.io/apimachinery/pkg/api/resource.Quantity
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>DeltaSnapshotMemoryLimit defines the memory limit after which delta snapshots will be taken&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>compression&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompressionSpec">
CompressionSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>SnapshotCompression defines the specification for compression of Snapshots.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>enableProfiling&lt;/code>&lt;/br>
&lt;em>
bool
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>EnableProfiling defines if profiling should be enabled for the etcd-backup-restore-sidecar&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>etcdSnapshotTimeout&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>EtcdSnapshotTimeout defines the timeout duration for etcd FullSnapshot operation&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>leaderElection&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.LeaderElectionSpec">
LeaderElectionSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>LeaderElection defines parameters related to the LeaderElection configuration.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.ClientService">ClientService
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig">EtcdConfig&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>ClientService defines the parameters of the client service that a user can specify&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>annotations&lt;/code>&lt;/br>
&lt;em>
map[string]string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Annotations specify the annotations that should be added to the client service&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>labels&lt;/code>&lt;/br>
&lt;em>
map[string]string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Labels specify the labels that should be added to the client service&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.CompactionMode">CompactionMode
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SharedConfig">SharedConfig&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>CompactionMode defines the auto-compaction-mode: ‘periodic’ or ‘revision’.
‘periodic’ for duration based retention and ‘revision’ for revision number based retention.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.CompressionPolicy">CompressionPolicy
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompressionSpec">CompressionSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>CompressionPolicy defines the type of policy for compression of snapshots.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.CompressionSpec">CompressionSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec">BackupSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>CompressionSpec defines parameters related to compression of Snapshots(full as well as delta).&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>enabled&lt;/code>&lt;/br>
&lt;em>
bool
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>policy&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompressionPolicy">
CompressionPolicy
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.Condition">Condition
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus">EtcdCopyBackupsTaskStatus&lt;/a>,
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus">EtcdStatus&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>Condition holds the information about the state of a resource.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>type&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.ConditionType">
ConditionType
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Type of the Etcd condition.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.ConditionStatus">
ConditionStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Status of the condition, one of True, False, Unknown.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastTransitionTime&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta">
Kubernetes meta/v1.Time
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Last time the condition transitioned from one status to another.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastUpdateTime&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta">
Kubernetes meta/v1.Time
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Last time the condition was updated.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>reason&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>The reason for the condition’s last transition.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>message&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>A human-readable message indicating details about the transition.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.ConditionStatus">ConditionStatus
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition">Condition&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>ConditionStatus is the status of a condition.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.ConditionType">ConditionType
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition">Condition&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>ConditionType is the type of condition.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.CrossVersionObjectReference">CrossVersionObjectReference
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus">EtcdStatus&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>CrossVersionObjectReference contains enough information to let you identify the referred resource.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>kind&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Kind of the referent&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>name&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Name of the referent&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>apiVersion&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>API version of the referent&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.Etcd">Etcd
&lt;/h3>
&lt;p>
&lt;p>Etcd is the Schema for the etcds API&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>metadata&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
Refer to the Kubernetes API documentation for the fields of the
&lt;code>metadata&lt;/code> field.
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>spec&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec">
EtcdSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;br/>
&lt;br/>
&lt;table>
&lt;tr>
&lt;td>
&lt;code>selector&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>selector is a label query over pods that should match the replica count.
It must match the pod template’s labels.
More info: &lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>labels&lt;/code>&lt;/br>
&lt;em>
map[string]string
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>annotations&lt;/code>&lt;/br>
&lt;em>
map[string]string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>etcd&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig">
EtcdConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>backup&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec">
BackupSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>sharedConfig&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SharedConfig">
SharedConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>schedulingConstraints&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SchedulingConstraints">
SchedulingConstraints
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>replicas&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>priorityClassName&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>PriorityClassName is the name of a priority class that shall be used for the etcd pods.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>storageClass&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>StorageClass defines the name of the StorageClass required by the claim.
More info: &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1">https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>storageCapacity&lt;/code>&lt;/br>
&lt;em>
k8s.io/apimachinery/pkg/api/resource.Quantity
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>StorageCapacity defines the size of persistent volume.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>volumeClaimTemplate&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>VolumeClaimTemplate defines the volume claim template to be created&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus">
EtcdStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdConfig">EtcdConfig
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec">EtcdSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdConfig defines parameters associated etcd deployed&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>quota&lt;/code>&lt;/br>
&lt;em>
k8s.io/apimachinery/pkg/api/resource.Quantity
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Quota defines the etcd DB quota.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>defragmentationSchedule&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>DefragmentationSchedule defines the cron standard schedule for defragmentation of etcd.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>serverPort&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>clientPort&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>image&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Image defines the etcd container image and tag&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>authSecretRef&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core">
Kubernetes core/v1.SecretReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>metrics&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.MetricsLevel">
MetricsLevel
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Metrics defines the level of detail for exported metrics of etcd, specify ‘extensive’ to include histogram metrics.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>resources&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core">
Kubernetes core/v1.ResourceRequirements
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Resources defines the compute Resources required by etcd container.
More info: &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/">https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>clientUrlTls&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig">
TLSConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ClientUrlTLS contains the ca, server TLS and client TLS secrets for client communication to ETCD cluster&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>peerUrlTls&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig">
TLSConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>PeerUrlTLS contains the ca and server TLS secrets for peer communication within ETCD cluster
Currently, PeerUrlTLS does not require client TLS secrets for gardener implementation of ETCD cluster.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>etcdDefragTimeout&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>EtcdDefragTimeout defines the timeout duration for etcd defrag call&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>heartbeatDuration&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>HeartbeatDuration defines the duration for members to send heartbeats. The default value is 10s.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>clientService&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.ClientService">
ClientService
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ClientService defines the parameters of the client service that a user can specify&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask">EtcdCopyBackupsTask
&lt;/h3>
&lt;p>
&lt;p>EtcdCopyBackupsTask is a task for copying etcd backups from a source to a target store.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>metadata&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#objectmeta-v1-meta">
Kubernetes meta/v1.ObjectMeta
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
Refer to the Kubernetes API documentation for the fields of the
&lt;code>metadata&lt;/code> field.
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>spec&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec">
EtcdCopyBackupsTaskSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;br/>
&lt;br/>
&lt;table>
&lt;tr>
&lt;td>
&lt;code>sourceStore&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec">
StoreSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>SourceStore defines the specification of the source object store provider for storing backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>targetStore&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec">
StoreSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>TargetStore defines the specification of the target object store provider for storing backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>maxBackupAge&lt;/code>&lt;/br>
&lt;em>
uint32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>MaxBackupAge is the maximum age in days that a backup must have in order to be copied.
By default all backups will be copied.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>maxBackups&lt;/code>&lt;/br>
&lt;em>
uint32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>MaxBackups is the maximum number of backups that will be copied starting with the most recent ones.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>waitForFinalSnapshot&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec">
WaitForFinalSnapshotSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>WaitForFinalSnapshot defines the parameters for waiting for a final full snapshot before copying backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus">
EtcdCopyBackupsTaskStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec">EtcdCopyBackupsTaskSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask">EtcdCopyBackupsTask&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdCopyBackupsTaskSpec defines the parameters for the copy backups task.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>sourceStore&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec">
StoreSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>SourceStore defines the specification of the source object store provider for storing backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>targetStore&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec">
StoreSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>TargetStore defines the specification of the target object store provider for storing backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>maxBackupAge&lt;/code>&lt;/br>
&lt;em>
uint32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>MaxBackupAge is the maximum age in days that a backup must have in order to be copied.
By default all backups will be copied.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>maxBackups&lt;/code>&lt;/br>
&lt;em>
uint32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>MaxBackups is the maximum number of backups that will be copied starting with the most recent ones.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>waitForFinalSnapshot&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec">
WaitForFinalSnapshotSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>WaitForFinalSnapshot defines the parameters for waiting for a final full snapshot before copying backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskStatus">EtcdCopyBackupsTaskStatus
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTask">EtcdCopyBackupsTask&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdCopyBackupsTaskStatus defines the observed state of the copy backups task.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>conditions&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition">
[]Condition
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Conditions represents the latest available observations of an object’s current state.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>observedGeneration&lt;/code>&lt;/br>
&lt;em>
int64
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ObservedGeneration is the most recent generation observed for this resource.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastError&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>LastError represents the last occurred error.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdMemberConditionStatus">EtcdMemberConditionStatus
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberStatus">EtcdMemberStatus&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdMemberConditionStatus is the status of an etcd cluster member.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdMemberStatus">EtcdMemberStatus
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdStatus">EtcdStatus&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdMemberStatus holds information about a etcd cluster membership.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>name&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Name is the name of the etcd member. It is the name of the backing &lt;code>Pod&lt;/code>.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>id&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ID is the ID of the etcd member.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>role&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdRole">
EtcdRole
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Role is the role in the etcd cluster, either &lt;code>Leader&lt;/code> or &lt;code>Member&lt;/code>.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>status&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberConditionStatus">
EtcdMemberConditionStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Status of the condition, one of True, False, Unknown.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>reason&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>The reason for the condition’s last transition.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastTransitionTime&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#time-v1-meta">
Kubernetes meta/v1.Time
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>LastTransitionTime is the last time the condition’s status changed.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdRole">EtcdRole
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberStatus">EtcdMemberStatus&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdRole is the role of an etcd cluster member.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdSpec">EtcdSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Etcd">Etcd&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdSpec defines the desired state of Etcd&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>selector&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>selector is a label query over pods that should match the replica count.
It must match the pod template’s labels.
More info: &lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>labels&lt;/code>&lt;/br>
&lt;em>
map[string]string
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>annotations&lt;/code>&lt;/br>
&lt;em>
map[string]string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>etcd&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig">
EtcdConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>backup&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec">
BackupSpec
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>sharedConfig&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SharedConfig">
SharedConfig
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>schedulingConstraints&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SchedulingConstraints">
SchedulingConstraints
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>replicas&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>priorityClassName&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>PriorityClassName is the name of a priority class that shall be used for the etcd pods.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>storageClass&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>StorageClass defines the name of the StorageClass required by the claim.
More info: &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1">https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1&lt;/a>&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>storageCapacity&lt;/code>&lt;/br>
&lt;em>
k8s.io/apimachinery/pkg/api/resource.Quantity
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>StorageCapacity defines the size of persistent volume.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>volumeClaimTemplate&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>VolumeClaimTemplate defines the volume claim template to be created&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.EtcdStatus">EtcdStatus
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Etcd">Etcd&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>EtcdStatus defines the observed state of Etcd.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>observedGeneration&lt;/code>&lt;/br>
&lt;em>
int64
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ObservedGeneration is the most recent generation observed for this resource.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>etcd&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CrossVersionObjectReference">
CrossVersionObjectReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>conditions&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.Condition">
[]Condition
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Conditions represents the latest available observations of an etcd’s current state.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>serviceName&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ServiceName is the name of the etcd service.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>lastError&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>LastError represents the last occurred error.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>clusterSize&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Cluster size is the size of the etcd cluster.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>currentReplicas&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>CurrentReplicas is the current replica count for the etcd cluster.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>replicas&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Replicas is the replica count of the etcd resource.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>readyReplicas&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ReadyReplicas is the count of replicas being ready in the etcd cluster.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>ready&lt;/code>&lt;/br>
&lt;em>
bool
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Ready is &lt;code>true&lt;/code> if all etcd replicas are ready.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>updatedReplicas&lt;/code>&lt;/br>
&lt;em>
int32
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>UpdatedReplicas is the count of updated replicas in the etcd cluster.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>labelSelector&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#labelselector-v1-meta">
Kubernetes meta/v1.LabelSelector
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>LabelSelector is a label query over pods that should match the replica count.
It must match the pod template’s labels.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>members&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdMemberStatus">
[]EtcdMemberStatus
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Members represents the members of the etcd cluster&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>peerUrlTLSEnabled&lt;/code>&lt;/br>
&lt;em>
bool
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>PeerUrlTLSEnabled captures the state of peer url TLS being enabled for the etcd member(s)&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.GarbageCollectionPolicy">GarbageCollectionPolicy
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec">BackupSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>GarbageCollectionPolicy defines the type of policy for snapshot garbage collection.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.LeaderElectionSpec">LeaderElectionSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec">BackupSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>LeaderElectionSpec defines parameters related to the LeaderElection configuration.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>reelectionPeriod&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>ReelectionPeriod defines the Period after which leadership status of corresponding etcd is checked.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>etcdConnectionTimeout&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>EtcdConnectionTimeout defines the timeout duration for etcd client connection during leader election.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.MetricsLevel">MetricsLevel
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig">EtcdConfig&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>MetricsLevel defines the level ‘basic’ or ‘extensive’.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.SchedulingConstraints">SchedulingConstraints
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec">EtcdSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>SchedulingConstraints defines the different scheduling constraints that must be applied to the
pod spec in the etcd statefulset.
Currently supported constraints are Affinity and TopologySpreadConstraints.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>affinity&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#affinity-v1-core">
Kubernetes core/v1.Affinity
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Affinity defines the various affinity and anti-affinity rules for a pod
that are honoured by the kube-scheduler.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>topologySpreadConstraints&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#topologyspreadconstraint-v1-core">
[]Kubernetes core/v1.TopologySpreadConstraint
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>TopologySpreadConstraints describes how a group of pods ought to spread across topology domains,
that are honoured by the kube-scheduler.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.SecretReference">SecretReference
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.TLSConfig">TLSConfig&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>SecretReference defines a reference to a secret.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>SecretReference&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core">
Kubernetes core/v1.SecretReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>
(Members of &lt;code>SecretReference&lt;/code> are embedded into this type.)
&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>dataKey&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>DataKey is the name of the key in the data map containing the credentials.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.SharedConfig">SharedConfig
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdSpec">EtcdSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>SharedConfig defines parameters shared and used by Etcd as well as backup-restore sidecar.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>autoCompactionMode&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.CompactionMode">
CompactionMode
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>AutoCompactionMode defines the auto-compaction-mode:‘periodic’ mode or ‘revision’ mode for etcd and embedded-Etcd of backup-restore sidecar.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>autoCompactionRetention&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>AutoCompactionRetention defines the auto-compaction-retention length for etcd as well as for embedded-Etcd of backup-restore sidecar.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.StorageProvider">StorageProvider
(&lt;code>string&lt;/code> alias)&lt;/p>&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StoreSpec">StoreSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>StorageProvider defines the type of object store provider for storing backups.&lt;/p>
&lt;/p>
&lt;h3 id="druid.gardener.cloud/v1alpha1.StoreSpec">StoreSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec">BackupSpec&lt;/a>,
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec">EtcdCopyBackupsTaskSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>StoreSpec defines parameters related to ObjectStore persisting backups&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>container&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Container is the name of the container the backup is stored at.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>prefix&lt;/code>&lt;/br>
&lt;em>
string
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Prefix is the prefix used for the store.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>provider&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.StorageProvider">
StorageProvider
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Provider is the name of the backup provider.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>secretRef&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core">
Kubernetes core/v1.SecretReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>SecretRef is the reference to the secret which used to connect to the backup store.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.TLSConfig">TLSConfig
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.BackupSpec">BackupSpec&lt;/a>,
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdConfig">EtcdConfig&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>TLSConfig hold the TLS configuration details.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>tlsCASecretRef&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.SecretReference">
SecretReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>serverTLSSecretRef&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core">
Kubernetes core/v1.SecretReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>clientTLSSecretRef&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#secretreference-v1-core">
Kubernetes core/v1.SecretReference
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="druid.gardener.cloud/v1alpha1.WaitForFinalSnapshotSpec">WaitForFinalSnapshotSpec
&lt;/h3>
&lt;p>
(&lt;em>Appears on:&lt;/em>
&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/api-reference/#druid.gardener.cloud/v1alpha1.EtcdCopyBackupsTaskSpec">EtcdCopyBackupsTaskSpec&lt;/a>)
&lt;/p>
&lt;p>
&lt;p>WaitForFinalSnapshotSpec defines the parameters for waiting for a final full snapshot before copying backups.&lt;/p>
&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Field&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>
&lt;code>enabled&lt;/code>&lt;/br>
&lt;em>
bool
&lt;/em>
&lt;/td>
&lt;td>
&lt;p>Enabled specifies whether to wait for a final full snapshot before copying backups.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;code>timeout&lt;/code>&lt;/br>
&lt;em>
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.25/#duration-v1-meta">
Kubernetes meta/v1.Duration
&lt;/a>
&lt;/em>
&lt;/td>
&lt;td>
&lt;em>(Optional)&lt;/em>
&lt;p>Timeout is the timeout for waiting for a final full snapshot. When this timeout expires, the copying of backups
will be performed anyway. No timeout or 0 means wait forever.&lt;/p>
&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;hr/>
&lt;p>&lt;em>
Generated with &lt;a href="https://github.com/ahmetb/gen-crd-api-reference-docs">gen-crd-api-reference-docs&lt;/a>
&lt;/em>&lt;/p></description></item><item><title>Docs: Architecture</title><link>https://gardener.cloud/docs/dashboard/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/dashboard/architecture/</guid><description>
&lt;h1 id="dashboard-architecture-overview">Dashboard Architecture Overview&lt;/h1>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>The dashboard &lt;code>frontend&lt;/code> is a Single Page Application (SPA) built with &lt;a href="https://vuejs.org/">Vue.js&lt;/a>. The dashboard &lt;code>backend&lt;/code> is a web server built with &lt;a href="http://expressjs.com">Express&lt;/a> and &lt;a href="https://nodejs.org/">Node.js&lt;/a>. The &lt;code>backend&lt;/code> serves the bundled &lt;code>frontend&lt;/code> as static content. The dashboard uses &lt;a href="https://socket.io/">Socket.IO&lt;/a> to enable real-time, bidirectional and event-based communication between the &lt;code>frontend&lt;/code> and the &lt;code>backend&lt;/code>. For the communication from the &lt;code>backend&lt;/code> to different &lt;code>kube-apiservers&lt;/code> the http/2 network protocol is used. Authentication at the &lt;code>apiserver&lt;/code> of the garden cluster is done via JWT tokens. These can either be an ID Token issued by an OpenID Connect Provider or the token of a Kubernetes Service Account.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/architecture-1_09ad4f.png">
&lt;h2 id="frontend">Frontend&lt;/h2>
&lt;p>The dashboard &lt;code>frontend&lt;/code> consists of many Vue.js single file components that manage their state via a &lt;a href="https://vuex.vuejs.org/">centralized store&lt;/a>. The store defines mutations to modify the state synchronously. If several mutations have to be combined or the state in the &lt;code>backend&lt;/code> has to be modified at the same time, the store provides asynchronous actions to do this job. The synchronization of the data with the &lt;code>backend&lt;/code> is done by plugins that also use actions.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/architecture-2_718ce1.png">
&lt;h2 id="backend">Backend&lt;/h2>
&lt;p>The &lt;code>backend&lt;/code> is currently a monolithic Node.js application, but it performs several tasks that are actually independent.&lt;/p>
&lt;ul>
&lt;li>Static web server for the &lt;code>frontend&lt;/code> single page application&lt;/li>
&lt;li>Forward real time events of the &lt;code>apiserver&lt;/code> to the &lt;code>frontend&lt;/code>&lt;/li>
&lt;li>Provide an HTTP API&lt;/li>
&lt;li>Initiate and manage the end user login flow in order to obtain an ID Token&lt;/li>
&lt;li>Bidirectional integration with the GitHub issue management&lt;/li>
&lt;/ul>
&lt;img src="https://gardener.cloud/__resources/architecture-3_c848c2.png">
&lt;p>It is planned to split the &lt;code>backend&lt;/code> into several independent containers to increase stability and performance.&lt;/p>
&lt;h2 id="authentication">Authentication&lt;/h2>
&lt;p>The following diagram shows the authorization code flow in the Gardener dashboard. When the user clicks the login button, he is redirected to the authorization endpoint of the openid connect provider. In the case of &lt;a href="https://dexidp.io/">Dex IDP&lt;/a>, authentication is delegated to the connected IDP. After a successful login, the OIDC provider redirects back to the dashboard &lt;code>backend&lt;/code> with a one time authorization code. With this code, the dashboard &lt;code>backend&lt;/code> can now request an ID token for the logged in user. The ID token is encrypted and stored as a secure &lt;code>httpOnly&lt;/code> session cookie.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/architecture-4_371bde.png"></description></item><item><title>Docs: Components</title><link>https://gardener.cloud/docs/getting-started/observability/components/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/observability/components/</guid><description>
&lt;h2 id="core-components">Core Components&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/core-components_287ae2.png" alt="core-components">&lt;/p>
&lt;p>The core Observability components which Gardener offers out-of-the-box are:&lt;/p>
&lt;ul>
&lt;li>Prometheus - for Metrics and Alerting&lt;/li>
&lt;li>Vali - a Loki fork for Logging&lt;/li>
&lt;li>Plutono - a Grafana fork for Dashboard visualization&lt;/li>
&lt;/ul>
&lt;p>Both forks are done from the last version with an Apache license.&lt;/p>
&lt;h3 id="control-plane-components-on-the-seed">Control Plane Components on the Seed&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/control-plane-components_1b8262.png" alt="control-plane-components">&lt;/p>
&lt;p>Prometheus, Plutono, and Vali are all located in the seed cluster. They run next to the control plane of your cluster.&lt;/p>
&lt;p>The next sections will explore those components in detail.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Gardener only provides monitoring for Gardener-deployed components. If you need logging or monitoring for your workload, then you need to deploy your own monitoring stack into your shoot cluster.
&lt;/div>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Gardener only provides a monitoring stack if the cluster is not of &lt;code>purpose: testing&lt;/code>. For more information, see &lt;a href="https://gardener.cloud/docs/gardener/shoot_purposes/">Shoot Cluster Purpose&lt;/a>.
&lt;/div>
&lt;h3 id="logging-into-plutono">Logging into Plutono&lt;/h3>
&lt;p>Let us start by giving some visual hints on how to access Plutono. &lt;a href="https://github.com/credativ/plutono#plutono">Plutono&lt;/a> allows us to query logs and metrics and visualise those in form of dashboards. Plutono is shipped ready-to-use with a Gardener shoot cluster.&lt;/p>
&lt;p>In order to access the Gardener provided dashboards, open the &lt;code>Plutono&lt;/code> link provided in the Gardener dashboard and use the username and password provided next to it.&lt;/p>
&lt;p>The password you can use to log in can be retrieved as shown below:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/access-plutono_07ba1b.png" alt="access-plutono">&lt;/p>
&lt;h3 id="accessing-the-dashboards">Accessing the Dashboards&lt;/h3>
&lt;p>After logging in, you will be greeted with a Plutono welcome screen. Navigate to &lt;code>General/Home&lt;/code>, as depicted with the red arrow in the next picture:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/welcome-plutono_6d6c00.png" alt="welcome-plutono">&lt;/p>
&lt;p>Then you will be able to select the dashboards. Some interesting ones to look at are:&lt;/p>
&lt;ul>
&lt;li>The &lt;code>Kubernetes Control Plane Status&lt;/code> dashboard allows you to check control plane availability during a certain time frame.&lt;/li>
&lt;li>The &lt;code>API Server&lt;/code> dashboard gives you an overview on which requests are done towards your apiserver and how long they take.&lt;/li>
&lt;li>With the &lt;code>Node Details&lt;/code> dashboard you can analyze CPU/Network pressure or memory usage for nodes.&lt;/li>
&lt;li>The &lt;code>Network Problem Detector&lt;/code> dashboard illustrates the results of periodic networking checks between nodes and to the APIServer.&lt;/li>
&lt;/ul>
&lt;p>Here is a picture with the &lt;code>Kubernetes Control Plane Status&lt;/code> dashboard.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/plutono_855b74.png" alt="plutono">&lt;/p>
&lt;h3 id="prometheus">Prometheus&lt;/h3>
&lt;p>&lt;a href="https://prometheus.io/">Prometheus&lt;/a> is a monitoring system and a time series database. It can be queried using PromQL, the so called Prometheus Querying Language.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/prometheus_707368.png" alt="prometheus">&lt;/p>
&lt;p>This example query describes the current uptime status of the kube apiserver.&lt;/p>
&lt;h4 id="prometheus-and-plutono">Prometheus and Plutono&lt;/h4>
&lt;p>Time series data from Prometheus can be made visible with Plutono. Here we see how the query above which describes the uptime of a Kubernetes cluster is visualized with a Plutono dashboard.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/prometheus-plutono_662ad5.png" alt="prometheus-plutono">&lt;/p>
&lt;h3 id="vali-logs-via-plutono">Vali Logs via Plutono&lt;/h3>
&lt;p>Vali is our logging solution. In order to access the logs provided by Vali, you need to:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;a href="https://gardener.cloud/docs/getting-started/observability/components/#logging-into-plutono">Log into Plutono&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Choose &lt;code>Explore&lt;/code>, which is depicted as the little compass symbol:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://gardener.cloud/__resources/explore-loki_6b28b5.png" alt="explore-loki">&lt;/p>
&lt;ol>
&lt;li>Select &lt;code>Vali&lt;/code> at the top left, as shown here:&lt;/li>
&lt;/ol>
&lt;p>&lt;img src="https://gardener.cloud/__resources/select-vali_c74f45.png" alt="select-vali">&lt;/p>
&lt;p>There you can browse logs or events of the control plane components.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/vali-logs_eb9bd0.png" alt="vali-logs">&lt;/p>
&lt;p>Here are some examples of helpful queries:&lt;/p>
&lt;ul>
&lt;li>&lt;code>{container_name=&amp;quot;cluster-autoscaler&amp;quot; }&lt;/code> to get cluster-autoscaler logs and see why certain node groups were scaled up.&lt;/li>
&lt;li>&lt;code>{container_name=&amp;quot;kube-apiserver&amp;quot;} |~ &amp;quot;error&amp;quot;&lt;/code> to get the logs of the kube-apiserver container and filter for errors.&lt;/li>
&lt;li>&lt;code>{unit=&amp;quot;kubelet.service&amp;quot;, nodename=&amp;quot;ip-123&amp;quot;}&lt;/code> to get the kubelet logs of a specific node.&lt;/li>
&lt;li>&lt;code>{unit=&amp;quot;containerd.service&amp;quot;, nodename=&amp;quot;ip-123&amp;quot;}&lt;/code> to retrieve the containerd logs for a specific node.&lt;/li>
&lt;/ul>
&lt;p>Choose &lt;code>Help &amp;gt;&lt;/code> in order to see what options exist to filter the results.&lt;/p>
&lt;p>For more information on how to retrieve K8s events from the past, see &lt;a href="https://gardener.cloud/docs/gardener/logging-usage/#how-to-access-the-logs">How to Access Logs&lt;/a>.&lt;/p>
&lt;h2 id="detailed-view">Detailed View&lt;/h2>
&lt;h3 id="data-flow">Data Flow&lt;/h3>
&lt;p>Our monitoring and logging solutions Vali and Prometheus both run next to the control plane of the shoot cluster.&lt;/p>
&lt;h4 id="data-flow---logging">Data Flow - Logging&lt;/h4>
&lt;p>The following diagram allows a more detailed look at Vali and the data flow.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/data-flow-logging_e097b8.png" alt="data-flow-logging">&lt;/p>
&lt;p>On the very left, we see Plutono as it displays the logs. Vali is aggregating the logs from different sources.&lt;/p>
&lt;p>Valitail and Fluentbit send the logs to Vali, which in turn stores them.&lt;/p>
&lt;p>&lt;em>Valitail&lt;/em>&lt;/p>
&lt;p>Valitail is a systemd service that runs on each node. It scrapes kubelet, containerd, kernel logs, and the logs of the pods in the kube-system namespace.&lt;/p>
&lt;p>&lt;em>Fluentbit&lt;/em>&lt;/p>
&lt;p>Fluentbit runs as a daemonset on each seed node. It scrapes logs of the kubernetes control plane components, like apiserver or etcd.&lt;/p>
&lt;p>It also scrapes logs of the Gardener deployed components which run next to the control plane of the cluster, like the machine-controller-manager or the cluster autoscaler. Debugging those components, for example, would be helpful when finding out why certain worker groups got scaled up or why nodes were replaced.&lt;/p>
&lt;h4 id="data-flow---monitoring">Data Flow - Monitoring&lt;/h4>
&lt;p>Next to each shoot&amp;rsquo;s control plane, we deploy an instance of Prometheus in the seed.&lt;/p>
&lt;p>Gardener uses &lt;a href="https://prometheus.io/">Prometheus&lt;/a> for storing and accessing shoot-related metrics and alerting.&lt;/p>
&lt;p>The diagram below shows the data flow of metrics.
Plutono uses PromQL queries to query data from Prometheus. It then visualises those metrics in dashboards.
Prometheus itself scrapes various targets for metrics, as seen in the diagram below by the arrows pointing to the Prometheus instance.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/data-flow-monitoring-1_a9b4fd.png" alt="data-flow-monitoring-1">&lt;/p>
&lt;p>Let us have a look what metrics we scrape for debugging purposes:&lt;/p>
&lt;p>&lt;strong>Container performance metrics&lt;/strong>&lt;/p>
&lt;p>cAdvisor is an open-source agent integrated into the kubelet binary that monitors resource usage and analyzes the performance of containers. It collects statistics about the CPU, memory, file, and network usage for all containers running on a given node. We use it to scrape data for all pods running in the kube-system namespace in the shoot cluster.&lt;/p>
&lt;p>&lt;strong>Hardware and kernel-related metrics&lt;/strong>&lt;/p>
&lt;p>The &lt;a href="https://prometheus.io/docs/guides/node-exporter/">Prometheus Node Exporter&lt;/a> runs as a daemonset in the kube-system namespace of your shoot cluster. It exposes a wide variety of hardware and kernel-related metrics. Some of the metrics we scrape are, for example, the current usage of the filesystem (&lt;code>node_filesystem_free_bytes&lt;/code>) or current CPU usage (&lt;code>node_cpu_seconds_total&lt;/code>). Both can help you identify if nodes are running out of hardware resources, which could lead to your workload experiencing downtimes.&lt;/p>
&lt;p>&lt;strong>Control plane component specific metrics&lt;/strong>&lt;/p>
&lt;p>The different control plane pods (for example, etcd, API server, and kube-controller-manager) emit metrics over the &lt;code>/metrics&lt;/code> endpoint. This includes metrics like how long webhooks take, the request count of the apiserver and storage information, like how many and what kind of objects are stored in etcd.&lt;/p>
&lt;p>&lt;strong>Metrics about the state of Kubernetes objects&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics&lt;/a> is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects. It is not concerned with metrics about the Kubernetes components, but rather it exposes metrics calculated from the status of Kubernetes objects (for example, resource requests or health of pods).&lt;/p>
&lt;p>In the following image a few example metrics, which are exposed by the various components, are listed:
&lt;img src="https://gardener.cloud/__resources/data-flow-monitoring-2_4e4e76.png" alt="data-flow-monitoring-2">&lt;/p>
&lt;p>We only store metrics for Gardener deployed components. Those include the Kubernetes control plane, Gardener managed system components (e.g., pods) in the kube-system namespace of the shoot cluster or systemd units on the nodes. We do not gather metrics for workload deployed in the shoot cluster. This is also shown in the picture below.&lt;/p>
&lt;p>This means that for any workload you deploy into your shoot cluster, you need to deploy monitoring and logging yourself.&lt;/p>
&lt;p>Logs or metrics are kept up to 14 days or when a configured space limit is reached.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/data-flow-monitoring-3_38a9ab.png" alt="data-flow-monitoring-3">&lt;/p></description></item><item><title>Docs: Hibernation</title><link>https://gardener.cloud/docs/getting-started/features/hibernation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/hibernation/</guid><description>
&lt;h2 id="hibernation">Hibernation&lt;/h2>
&lt;p>Some clusters need to be up all the time - typically, they would be hosting some kind of production workload. Others might be used for development purposes or testing during business hours only. Keeping them up and running all the time is a waste of money. Gardener can help you here with its &amp;ldquo;hibernation&amp;rdquo; feature. Essentially, hibernation means to shut down all components of a cluster.&lt;/p>
&lt;h2 id="how-hibernation-works">How Hibernation Works&lt;/h2>
&lt;p>The hibernation flow for a shoot attempts to reduce the resources consumed as much as possible. Hence everything not state-related is being decommissioned.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/hibernation_21d43d.gif" alt="hibernation">&lt;/p>
&lt;h3 id="data-plane">Data Plane&lt;/h3>
&lt;p>All nodes will be drained and the VMs will be deleted. As a result, all pods will be &amp;ldquo;stuck&amp;rdquo; in a &lt;code>Pending&lt;/code> state since no new nodes are added. Of course, PVC / PV holding data is not deleted.&lt;/p>
&lt;p>Services of type &lt;code>LoadBalancer&lt;/code> will keep their external IP addresses.&lt;/p>
&lt;h3 id="control-plane">Control Plane&lt;/h3>
&lt;p>All components will be scaled down and no pods will remain running. ETCD data is kept safe on the disk.&lt;/p>
&lt;p>The DNS records routing traffic for the API server are also destroyed. Trying to connect to a hibernated cluster via kubectl will result in a DNS lookup failure / no-such-host message.&lt;/p>
&lt;p>When waking up a cluster, all control plane components will be scaled up again and the DNS records will be re-created. Nodes will be created again and pods scheduled to run on them.&lt;/p>
&lt;h2 id="how-to-configure--trigger-hibernation">How to Configure / Trigger Hibernation&lt;/h2>
&lt;p>The easiest way to configure hibernation schedules is via the dashboard. Of course, this is reflected in the shoot&amp;rsquo;s spec and can also be maintained there. Before a cluster is hibernated, constraints in the shoot&amp;rsquo;s status will be evaluated. There might be conditions (mostly revolving around mutating / validating webhooks) that would block a successful wake-up. In such a case, the constraint will block hibernation in the first place.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/trigger-hibernation_eef81a.png" alt="trigger-hibernation">&lt;/p>
&lt;p>To wake-up or hibernate a shoot immediately, the dashboard can be used or a patch to the shoot&amp;rsquo;s spec can be applied directly.&lt;/p></description></item><item><title>Docs: Introduction to Gardener</title><link>https://gardener.cloud/docs/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/introduction/</guid><description>
&lt;h2 id="problem-space">Problem Space&lt;/h2>
&lt;p>Let&amp;rsquo;s discuss the problem space first. Why does anyone need something like Gardener?&lt;/p>
&lt;h3 id="running-software">Running Software&lt;/h3>
&lt;p>The starting point is this rather simple question: Why would you want to run some software?&lt;/p>
&lt;p>Typically, software is run with a purpose and not just for the sake of running it. Whether it is a digital ledger, a company&amp;rsquo;s inventory or a blog - software provides a service to its user.&lt;/p>
&lt;p>Which brings us to the way this software is being consumed. Traditionally, software has been shipped on physical / digital media to the customer or end user. There, someone had to install, configure, and operate it. In recent times, the pattern has shifted. More and more solutions are operated by the vendor or a hosting partner and sold as a service ready to be used.&lt;/p>
&lt;p>But still, someone needs to install, configure, and maintain it - regardless of where it is installed. And of course, it will run forever once started and is generally resilient to any kind of failures.&lt;/p>
&lt;p>For smaller installations things like maintenance, scaling, debugging or configuration can be done in a semi-automatic way. It&amp;rsquo;s probably no fun and most importantly, only a limited amount of instances can be taken care of - similar to how one would take care of a pet.&lt;/p>
&lt;p>But when hosting services at scale, there is no way someone can do all this manually at acceptable costs. So we need some vehicle to easily spin up new instances, do lifecycle operations, get some basic failure resilience, and more. How can we achieve that?&lt;/p>
&lt;h2 id="solution-space-1---kubernetes">Solution Space 1 - Kubernetes&lt;/h2>
&lt;p>Let&amp;rsquo;s start solving some of the problems described earlier with Container technology and Kubernetes.&lt;/p>
&lt;h3 id="containers">Containers&lt;/h3>
&lt;p>Container technology is at the core of the solution space. A container forms a vehicle that is shippable, can easily run in any supported environment and generally adds a powerful abstraction layer to the infrastructure.&lt;/p>
&lt;p>However, plain containers do not help with resilience or scaling. Therefore, we need another system for orchestration.&lt;/p>
&lt;h3 id="orchestration">Orchestration&lt;/h3>
&lt;p>&amp;ldquo;Classical&amp;rdquo; orchestration that just follows the &amp;ldquo;notes&amp;rdquo; and moves from &lt;code>state A&lt;/code> to &lt;code>state B&lt;/code> doesn&amp;rsquo;t solve all of our problems. We need something else.&lt;/p>
&lt;p>Kubernetes operates on the principle of &amp;ldquo;desired state&amp;rdquo;. With it, you write a construction plan, then have controllers cycle through &amp;ldquo;observe -&amp;gt; analyze -&amp;gt; act&amp;rdquo; and transition the actual to the desired state. Those reconciliations ensure that whatever breaks there is a path back to a healthy state.&lt;/p>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>Containers (famously brought to the mainstream as &amp;ldquo;Docker&amp;rdquo;) and Kubernetes are the ingredients of a fundamental shift in IT. Similar to how the Operating System layer enabled the decoupling of software and hardware, container-related technologies provide an abstract interface to any kind of infrastructure platform for the next-generation of applications.&lt;/p>
&lt;h2 id="solution-space-2---gardener">Solution Space 2 - Gardener&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/operating-apps_5e49bc.png" alt="operating-apps">&lt;/p>
&lt;p>So, Kubernetes solves a lot of problems. But how do you get a Kubernetes cluster?&lt;/p>
&lt;p>Either:&lt;/p>
&lt;ul>
&lt;li>Buy a cluster as a service from an external vendor&lt;/li>
&lt;li>Run a Gardener instance and host yourself a cluster with its help&lt;/li>
&lt;/ul>
&lt;p>Essentially, it was a &amp;ldquo;make or buy&amp;rdquo; decision that led to the founding of Gardener.&lt;/p>
&lt;h3 id="the-reason-why-we-choose-to-make-it">The Reason Why We Choose to &amp;ldquo;Make It&amp;rdquo;&lt;/h3>
&lt;p>Gardener allows to run Kubernetes clusters on various hyperscalers. It offers the same set of basic configuration options independent of the chosen infrastructure. This kind of harmonization supports any multi-vendor strategy while reducing adoption costs for the individual teams. Just imagine having to deal with multiple vendors all offering vastly different Kubernetes clusters.&lt;/p>
&lt;p>Of course, there are plenty more reasons - from acquiring operational knowledge to having influence on the developed features - that made the pendulum swing towards &amp;ldquo;make it&amp;rdquo;.&lt;/p>
&lt;h2 id="what-exactly-is-gardener">What exactly is Gardener?&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/universal-kubernetes_194ebc.png" alt="universal-kubernetes">&lt;/p>
&lt;p>Gardener is a system to manage Kubernetes clusters. It is driven by the same &amp;ldquo;desired state&amp;rdquo; pattern as Kubernetes itself. In fact, it is using Kubernetes to run Kubernetes.&lt;/p>
&lt;p>A user may &amp;ldquo;desire&amp;rdquo; clusters with specific configuration on infrastructures such as GCP, AWS, Azure, Alicloud, Openstack, vsphere, &amp;hellip; and Gardener will make sure to create such a cluster and keep it running.&lt;/p>
&lt;p>If you take this rather simplistic principle of reconciliation and add the feature-richness of Gardener to it, you end up with universal Kubernetes at scale.&lt;/p>
&lt;p>Whether you need fleet management at minimal TCO or to look for a highly customizable control plane - we have it all.&lt;/p>
&lt;p>On top of that, Gardener-managed Kubernetes clusters fulfill the conformance standard set out by the CNCF and we submit our test results for certification.&lt;/p>
&lt;p>Have a look at the &lt;a href="https://cncf.landscape2.io/?item=platform--certified-kubernetes--installer--gardener">CNCF map&lt;/a> for more information or dive into the &lt;a href="https://testgrid.k8s.io/conformance-gardener">testgrid&lt;/a> directly.&lt;/p>
&lt;p>Gardener itself is open-source. Under the umbrella of &lt;a href="https://github.com/gardener">github.com/gardener&lt;/a> we develop the core functionalities as well as the extensions and you are welcome to contribute (by opening issues, feature requests or submitting code).&lt;/p>
&lt;p>Last time we counted, there were already 131 projects. That&amp;rsquo;s actually more projects than members of the organization.&lt;/p>
&lt;p>As of today, Gardener is mainly developed by SAP employees and SAP is an &amp;ldquo;adopter&amp;rdquo; as well, among StackIT, Telekom, Finanz Informatik Technologie Services GmbH and others. For a full list of adopters, see the &lt;a href="https://gardener.cloud/adopter/">Adopters page&lt;/a>.&lt;/p></description></item><item><title>Docs: Tutorials</title><link>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/tutorials/kubernetes-cluster-on-aws-with-gardener/kubernetes-cluster-on-aws-with-gardener/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/tutorials/kubernetes-cluster-on-aws-with-gardener/kubernetes-cluster-on-aws-with-gardener/</guid><description>
&lt;h3 id="overview">Overview&lt;/h3>
&lt;p>Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on AWS.&lt;/p>
&lt;h3 id="prerequisites">Prerequisites&lt;/h3>
&lt;ul>
&lt;li>You have created an &lt;a href="https://aws.amazon.com/">AWS account&lt;/a>.&lt;/li>
&lt;li>You have access to the Gardener dashboard and have permissions to create projects.&lt;/li>
&lt;/ul>
&lt;h3 id="steps">Steps&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>Go to the Gardener dashboard and create a &lt;em>Project&lt;/em>.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/new-gardener-project_ad03bc.png">
&lt;/li>
&lt;li>
&lt;p>Choose &lt;em>Secrets&lt;/em>, then the plus icon &lt;img src="https://gardener.cloud/__resources/plus-icon_3b1f20.png"> and select &lt;em>AWS&lt;/em>.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/create-secret-aws_79dc1a.png">
&lt;/li>
&lt;li>
&lt;p>To copy the policy for AWS from the Gardener dashboard, click on the help icon &lt;img src="https://gardener.cloud/__resources/help-icon_01486c.png"> for AWS secrets, and choose copy &lt;img src="https://gardener.cloud/__resources/copy-icon_0f5ab8.png">.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/gardener-copy-policy_a52965.png">
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://console.aws.amazon.com/iam/home?#/policies">Create a new policy&lt;/a> in AWS:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Choose &lt;em>Create policy&lt;/em>.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/amazon-create-policy_5ef114.png">
&lt;/li>
&lt;li>
&lt;p>Paste the policy that you copied from the Gardener dashboard to this custom policy.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/amazon-create-policy-json_7d6327.png">
&lt;/li>
&lt;li>
&lt;p>Choose &lt;em>Next&lt;/em> until you reach the Review section.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Fill in the name and description, then choose &lt;em>Create policy&lt;/em>.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/amazon-review-policy_6fba71.png">
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://console.aws.amazon.com/iam/home?#/users$new?step=details">Create a new technical user&lt;/a> in AWS:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Type in a username and select the access key credential type.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/add-user_775731.png">
&lt;/li>
&lt;li>
&lt;p>Choose &lt;em>Attach an existing policy&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Select &lt;em>GardenerAccess&lt;/em> from the policy list.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Choose &lt;em>Next&lt;/em> until you reach the Review section.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;img src="https://gardener.cloud/__resources/attach-policy_a6a81f.png">
&lt;img src="https://gardener.cloud/__resources/finish-user_a9e956.png">
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Note: After the user is created, &lt;code>Access key ID&lt;/code> and &lt;code>Secret access key&lt;/code> are generated and displayed. Remember to save them. The &lt;code>Access key ID&lt;/code> is used later to create secrets for Gardener.
&lt;/div>
&lt;img src="https://gardener.cloud/__resources/save-keys_f23816.png">
&lt;/li>
&lt;li>
&lt;p>On the Gardener dashboard, choose &lt;em>Secrets&lt;/em> and then the plus sign &lt;img src="https://gardener.cloud/__resources/plus-icon_3b1f20.png">. Select &lt;em>AWS&lt;/em> from the drop down menu to add a new AWS secret.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create your secret.&lt;/p>
&lt;ol>
&lt;li>Type the name of your secret.&lt;/li>
&lt;li>Copy and paste the &lt;code>Access Key ID&lt;/code> and &lt;code>Secret Access Key&lt;/code> you saved when you created the technical user on AWS.&lt;/li>
&lt;li>Choose &lt;em>Add secret&lt;/em>.
&lt;img src="https://gardener.cloud/__resources/add-aws-secret_ed47ad.png">&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>After completing these steps, you should see your newly created secret in the &lt;em>Infrastructure Secrets&lt;/em> section.&lt;/p>
&lt;/blockquote>
&lt;img src="https://gardener.cloud/__resources/secret-stored_a4c7f9.png">
&lt;/li>
&lt;li>
&lt;p>To create a new cluster, choose &lt;em>Clusters&lt;/em> and then the plus sign in the upper right corner.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/new-cluster_353d7b.png">
&lt;/li>
&lt;li>
&lt;p>In the &lt;em>Create Cluster&lt;/em> section:&lt;/p>
&lt;ol>
&lt;li>Select &lt;em>AWS&lt;/em> in the &lt;em>Infrastructure&lt;/em> tab.&lt;/li>
&lt;li>Type the name of your cluster in the &lt;em>Cluster Details&lt;/em> tab.&lt;/li>
&lt;li>Choose the secret you created before in the &lt;em>Infrastructure Details&lt;/em> tab.&lt;/li>
&lt;li>Choose &lt;em>Create&lt;/em>.&lt;/li>
&lt;/ol>
&lt;img src="https://gardener.cloud/__resources/create-cluster_7a45a2.png">
&lt;/li>
&lt;li>
&lt;p>Wait for your cluster to get created.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/processing-cluster_522005.png">
&lt;/li>
&lt;/ol>
&lt;h3 id="result">Result&lt;/h3>
&lt;p>After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster.&lt;/p>
&lt;img src="https://gardener.cloud/__resources/copy-kubeconfig_752d59.png"></description></item><item><title>Docs: Alerts</title><link>https://gardener.cloud/docs/getting-started/observability/alerts/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/observability/alerts/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>In this overview, we want to present two ways to receive alerts for control plane and Gardener managed system-components:&lt;/p>
&lt;ul>
&lt;li>Predefined Gardener alerts&lt;/li>
&lt;li>Custom alerts&lt;/li>
&lt;/ul>
&lt;h3 id="predefined-control-plane-alerts">Predefined Control Plane Alerts&lt;/h3>
&lt;p>In the shoot spec it is possible to configure &lt;code>emailReceivers&lt;/code>. On this email address you will automatically receive email notifications for predefined alerts of your control plane. Such alerts are deployed in the shoot Prometheus and have visibility &lt;code>owner&lt;/code> or &lt;code>all&lt;/code>. For more alert details, shoot owners can use this visibility to find these alerts in their shoot Prometheus UI.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> monitoring:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> alerting:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> emailReceivers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - john.doe@example.com
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/monitoring/alerting/">Alerting&lt;/a>.&lt;/p>
&lt;h3 id="custom-alerts---federation">Custom Alerts - Federation&lt;/h3>
&lt;p>If you need more customization for alerts for control plane metrics, you have the option to deploy your own Prometheus into your shoot control plane.&lt;/p>
&lt;p>Then you can use federation, which is a Prometheus feature, to forward the metrics from the Gardener managed Prometheus to your custom deployed Prometheus. Since as a shoot owner you do not have access to the control plane pods, this is the only way to get those metrics.&lt;/p>
&lt;p>The credentials and endpoint for the Gardener managed Prometheus are exposed over the Gardener dashboard or programmatically in the garden project as a secret (&lt;code>&amp;lt;shoot-name&amp;gt;.monitoring&lt;/code>).&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/custom-alerts_653ef2.png" alt="custom-alerts">&lt;/p></description></item><item><title>Docs: Architecture</title><link>https://gardener.cloud/docs/getting-started/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/architecture/</guid><description>
&lt;h2 id="kubeception">Kubeception&lt;/h2>
&lt;p>Kubeception - Kubernetes in Kubernetes in Kubernetes&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kubeception_169129.gif" alt="kubeception">&lt;/p>
&lt;p>In the classic setup, there is a dedicated host / VM to host the master components / control plane of a Kubernetes cluster. However, these are just normal programs that can easily be put into containers. Once in containers, Kubernetes Deployments and StatefulSets (for the etcd) can be made to watch over them. And by putting all that into a separate, dedicated Kubernetes cluster you get Kubernetes on Kubernetes, aka Kubeception (named after the famous movie Inception with Leonardo DiCaprio).&lt;/p>
&lt;p>But what are the advantages of running Kubernetes on Kubernetes? For one, it makes use of resources more reasonably. Instead of providing a dedicated computer or virtual machine for the control plane of a Kubernetes cluster - which will probably never be the right size but either too small or too big - you can dynamically scale the individual control plane components based on demand and maximize resource usage by combining the control planes of multiple Kubernetes clusters.&lt;/p>
&lt;p>In addition to that, it helps introducing a first layer of high availability. What happens if the API server suddenly stops responding to requests? In a traditional setup, someone would have to find out and manually restart the API server. In the Kubeception model, the API server is a Kubernetes Deployment and of course, it has sophisticated liveness- and readiness-probes. Should the API server fail, its liveness-probe will fail too and the pod in question simply gets restarted automatically - sometimes even before anybody would have noticed about the API server being unresponsive.&lt;/p>
&lt;p>In Gardener&amp;rsquo;s terminology, the cluster hosting the control plane components is called a seed cluster. The cluster that end users actually use (and whose control plane is hosted in the seed) is called a shoot cluster.&lt;/p>
&lt;p>The worker nodes of a shoot cluster are plain, simple virtual machines in a hyperscaler (EC2 instances in AWS, GCE instances in GCP or ECS instances in Alibaba Cloud). They run an operating system, a container runtime (e.g., containerd), and the kubelet that gets configured during node bootstrap to connect to the shoot&amp;rsquo;s API server. The API server in turn runs in the seed cluster and is exposed through an ingress. This connection happens over public internet and is - of course - TLS encrypted.&lt;/p>
&lt;p>In other terms: you use Kubernetes to run Kubernetes.&lt;/p>
&lt;h2 id="cluster-hierarchy-in-gardener">Cluster Hierarchy in Gardener&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/cluster-hierarchy_5a87a9.png" alt="cluster-hierarchy">&lt;/p>
&lt;p>Gardener uses many Kubernetes clusters to eventually provide you with your very own shoot cluster.&lt;/p>
&lt;p>At the heart of Gardener&amp;rsquo;s cluster hierarchy is the garden cluster. Since Gardener is 100% Kubernetes native, a Kubernetes cluster is needed to store all Gardener related resources. The garden cluster is actually nodeless - it only consists of a control plane, an API server (actually two), an etcd, and a bunch of controllers. The garden cluster is the central brain of a Gardener landscape and the one you connect to in order to create, modify or delete shoot clusters - either with kubectl and a dedicated kubeconfig or through the Gardener dashboard.&lt;/p>
&lt;p>The seed clusters are next in the hierarchy - they are the clusters which will host the &amp;ldquo;kubeceptioned&amp;rdquo; control planes of the shoot clusters. For every hyperscaler supported in a Gardener landscape, there would be at least one seed cluster. However, to reduce latencies as well as for scaling, Gardener landscapes have several different seeds in different regions across the globe to keep the distance between control planes and actual worker nodes small.&lt;/p>
&lt;p>Finally, there are the shoot clusters - what Gardener is all about. Shoot clusters are the clusters which you create through Gardener and which your workload gets deployed to.&lt;/p>
&lt;h2 id="gardener-components-overview">Gardener Components Overview&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/components_1de922.png" alt="components">&lt;/p>
&lt;p>From a very high level point of view, the important components of Gardener are:&lt;/p>
&lt;h3 id="the-gardener-api-endpoint">The Gardener API Endpoint&lt;/h3>
&lt;p>You can connect to the Gardener API Endpoint (i.e., the API server in the garden cluster) either through the dashboard or with kubectl, given that you have a proper kubeconfig for it.&lt;/p>
&lt;h3 id="the-seeds-running-the-shoot-cluster-control-planes">The Seeds Running the Shoot Cluster Control Planes&lt;/h3>
&lt;p>Inside each seed is one of the most important controllers in Gardener - the gardenlet. It spawns many other controllers, which will eventually create all resources for a shoot cluster, including all resources on the cloud providers such as virtual networks, security groups, and virtual machines.&lt;/p>
&lt;h2 id="gardeners-api-endpoint">Gardener&amp;rsquo;s API Endpoint&lt;/h2>
&lt;p>Kubernetes&amp;rsquo; API can be extended - either by CRDs or by API aggregation.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/api-endpoint-1_539e93.png" alt="api-endpoint-1">&lt;/p>
&lt;p>API aggregation involves setting up a so called extension-API-server and registering it with the main Kubernetes API server. The extension API server will then serve resources of custom-defined API groups on its own. While the main Kubernetes API server is still used to handle RBAC, authorization, namespacing, quotas, limits, etc., all custom resources will be delegated to the extension-API-server. This is done through an APIService resource in the main API server - it specifies that, e.g., the API group &lt;code>core.gardener.cloud&lt;/code> is served by a dedicated extension-API-server and all requests concerning this API group should be forwarded the specified IP address or Kubernetes service name. Extension API servers can persist their resources in their very own etcd but they do not have to - instead, they can use the main API servers etcd as well.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/api-endpoint-2_93499e.png" alt="api-endpoint-2">&lt;/p>
&lt;p>Gardener uses its very own extension API server for its resources like Shoot, Seed, CloudProfile, SecretBinding, etc&amp;hellip; However, Gardener does not set up a dedicated etcd for its own extension API server - instead, it reuses the existing etcd of the main Kubernetes API server. This is absolutely possible since the resources of Gardener&amp;rsquo;s API are part of the API group &lt;code>gardener.cloud&lt;/code> and thus will not interfere with any resources of the main Kubernetes API in etcd.&lt;/p>
&lt;p>In case you are interested, you can read more on:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/setup-extension-api-server/">API Extension&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation">API Aggregation &lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/#register-apiservice-objects">APIService Resource&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="gardener-api-resources">Gardener API Resources&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/api-resources_698838.png" alt="api-resources">&lt;/p>
&lt;p>Since Gardener&amp;rsquo;s API endpoint is a regular Kubernetes cluster, it would theoretically serve all resources from the Kubernetes core API, including Pods, Deployments, etc. However, Gardener implements RBAC rules and disables certain controllers that make these resources inaccessible. Objects like Secrets, Namespaces, and ResourceQuotas are still available, though, as they play a vital role in Gardener.&lt;/p>
&lt;p>In addition, through Gardener&amp;rsquo;s extension API server, the API endpoint also serves Gardener&amp;rsquo;s custom resources like Projects, Shoots, CloudProfiles, Seeds, SecretBindings (those are relevant for users), ControllerRegistrations, ControllerDeployments, BackupBuckets, BackupEntries (those are relevant to an operator), etc.&lt;/p></description></item><item><title>Docs: Workerless Shoots</title><link>https://gardener.cloud/docs/getting-started/features/workerless-shoots/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/workerless-shoots/</guid><description>
&lt;h2 id="controlplane-as-a-service">Controlplane as a Service&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/workerless-shoots_0af834.png" alt="workerless-shoots">&lt;/p>
&lt;p>Sometimes, there may be use cases for Kubernetes clusters that don&amp;rsquo;t require pods but only features of the control plane. Gardener can create the so-called &amp;ldquo;workerless&amp;rdquo; shoots, which are exactly that. A Kubernetes cluster without nodes (and without any controller related to them).&lt;/p>
&lt;p>In a scenario where you already have multiple clusters, you can use it for orchestration (leases) or factor out components that require many CRDs.&lt;/p>
&lt;p>As part of the control plane, the following components are deployed in the seed cluster for workerless shoot:&lt;/p>
&lt;ul>
&lt;li>etcds&lt;/li>
&lt;li>kube-apiserver&lt;/li>
&lt;li>kube-controller-manager&lt;/li>
&lt;li>gardener-resource-manager&lt;/li>
&lt;li>Logging and monitoring components&lt;/li>
&lt;li>Extension components (to find out if they support workerless shoots, see the &lt;a href="https://gardener.cloud/docs/gardener/extensions/extension/#what-is-required-to-register-and-support-an-extension-type">Extensions&lt;/a> documentation)&lt;/li>
&lt;/ul></description></item><item><title>Docs: Credential Rotation</title><link>https://gardener.cloud/docs/getting-started/features/credential-rotation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/credential-rotation/</guid><description>
&lt;h2 id="keys">Keys&lt;/h2>
&lt;p>There are plenty of keys in Gardener. The ETCD needs one to store resources like secrets encrypted at rest. Gardener generates certificate authorities (CAs) to ensure secured communication between the various components and actors and service account tokens are signed with a dedicated key. There is also an SSH key pair to allow debugging of nodes and the observability stack has its own passwords too.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/keys_ef3249.png" alt="keys">&lt;/p>
&lt;p>All of these keys share a common property: they are managed by Gardener. Rotating them, however, is potentially very disruptive. Hence, Gardener does not do it automatically, but offers you means to perform these tasks easily. For a single cluster, you may conveniently use the dashboard. Of course, it is also possible to do the same by annotating the shoot resource accordingly:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-credentials-start
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n &amp;lt;shoot-namespace&amp;gt; annotate shoot &amp;lt;shoot-name&amp;gt; gardener.cloud/operation=rotate-credentials-complete​
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Where possible, the rotation happens in two phases. Phase 1 introduces new keys while the old ones are still valid. Users can safely exchange keys / CA bundles wherever they are used. Afterwards, phase 2 will invalidate the old keys / CA bundles.&lt;/p>
&lt;h2 id="rotation-phases">Rotation Phases&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/rotation-phases_317f9b.png" alt="rotation-phases">&lt;/p>
&lt;p>At the beginning, only the old set of credentials exists. By triggering the rotation, new credentials are created in phase 1 and both sets are valid. Now, all clients have to update and start using the new credentials. Only afterwards it is safe to trigger phase 2, which invalidates the old credentials.&lt;/p>
&lt;p>The shoot&amp;rsquo;s status will always show the current status / phase of the rotation.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/shoot_credentials_rotation/">Credentials Rotation for Shoot Clusters&lt;/a>.&lt;/p>
&lt;h2 id="user-provided-credentials">User-Provided Credentials&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/user-provided-keys_976909.png" alt="user-provided-keys">&lt;/p>
&lt;p>You grant Gardener permissions to create resources by handing over cloud provider keys. These keys are stored in a secret and referenced to a shoot via a SecretBinding. Gardener uses the keys to create the network for the cluster resources, routes, VMs, disks, and IP addresses.&lt;/p>
&lt;p>When you rotate credentials, the new keys have to be stored in the same secret and the shoot needs to reconcile successfully to ensure the replication to every controller. Afterwards, the old keys can be deleted safely from Gardener&amp;rsquo;s perspective.&lt;/p>
&lt;p>While the reconciliation can be triggered manually, there is no need for it (if you&amp;rsquo;re not in a hurry). Each shoot reconciles once within 24h and the new keys will be picked up during the next maintenance window.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
It is not possible to move a shoot to a different infrastructure account (at all!).
&lt;/div></description></item><item><title>Docs: Gardener Projects</title><link>https://gardener.cloud/docs/getting-started/project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/project/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/overview_b8990e.png" alt="overview">&lt;/p>
&lt;p>Gardener is all about Kubernetes clusters, which we call shoots. However, Gardener also does user management, delicate permission management and offers technical accounts to integrate its services into other infrastructures. It allows you to create several quotas and it needs credentials to connect to cloud providers. All of these are arranged in multiple fully contained projects, each of which belongs to a dedicated user and / or group.&lt;/p>
&lt;h2 id="projects-on-yaml-level">Projects on YAML Level&lt;/h2>
&lt;p>Projects are a Kubernetes resource which can be expressed by YAML. The resource specification can be found in the &lt;a href="https://github.com/gardener/gardener/blob/master/docs/api-reference/core.md/#core.gardener.cloud/v1beta1.Project">API reference documentation&lt;/a>.&lt;/p>
&lt;p>A project&amp;rsquo;s specification defines a name, a description (which is a free-text field), a purpose (again, a free-text field), an owner, and members. In Gardener, user management is done on a project level. Therefore, projects can have different members with certain roles.&lt;/p>
&lt;p>In Gardener, a user can have one of five different roles: &lt;code>owner&lt;/code>, &lt;code>admin&lt;/code>, &lt;code>viewer&lt;/code>, &lt;code>UAM&lt;/code>, and &lt;code>service account manager&lt;/code>. A member with the &lt;code>viewer&lt;/code> role can see and list all clusters but cannot create, delete or modify them. For that, a member would need the &lt;code>admin&lt;/code> role. Another important role would be the &lt;code>uam&lt;/code> role - members with that role are allowed to manage members and technical users for a project. The &lt;code>owner&lt;/code> of a project is allowed to do all of that, regardless of what other roles might be assigned to him.&lt;/p>
&lt;p>Projects are getting reconciled by Gardener&amp;rsquo;s project-controller, a component of Gardener&amp;rsquo;s controller manager. The status of the last reconcilation, along with any potential failures, will be recorded in the project&amp;rsquo;s &lt;code>status&lt;/code> field.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/projects/">Projects&lt;/a>.&lt;/p>
&lt;p>In case you are interested, you can also view the source code for:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/gardener/gardener/blob/master/pkg/apis/core/types_project.go">The structure of a project API object&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener/blob/master/pkg/controllermanager/controller/project/project/reconciler.go">Reconciling a project&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="gardener-projects-and-kubernetes-namespaces">Gardener Projects and Kubernetes Namespaces&lt;/h2>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Each Gardener project corresponds to a Kubernetes namespace and all project specific resources are placed into it.
&lt;/div>
&lt;p>Even though projects are a dedicated Kubernetes resource, every project also corresponds to a dedicated namespace in the garden cluster. All project resources - including shoots - are placed into this namespace.&lt;/p>
&lt;p>You can ask Gardener to use a specific namespace name in the project manifest but usually, this field should be left empty. The namespace then gets created automatically by Gardener&amp;rsquo;s project-controller, with its name getting generated from the project&amp;rsquo;s name, prefixed by &amp;ldquo;garden-&amp;rdquo;.&lt;/p>
&lt;p>ResourceQuotas - if any - will be enforced on the project namespace.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Quotas&lt;/h4>
Since all Gardener resources are custom Kubernetes resources, the usual and well established concept of &lt;code>resourceQuotas&lt;/code> in Kubernetes can also be applied to Gardener resources. With a &lt;code>resourceQuota&lt;/code> that sets a hard limit on, e.g., &lt;code>count/shoots.core.gardener.cloud&lt;/code>, you can restrict the number of shoot clusters that can be created in a project.
&lt;/div>
&lt;h2 id="infrastructure-secrets">Infrastructure Secrets&lt;/h2>
&lt;p>For Gardener to create all relevant infrastructure that a shoot cluster needs inside a cloud provider, it needs to know how to authenticate to the cloud provider&amp;rsquo;s API. This is done through regular secrets.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/secret_e359cd.png" alt="secret">&lt;/p>
&lt;p>Through the Gardener dashboard, secrets can be created for each supported cloud provider (using the dashboard is the preferred way, as it provides interactive help on what information needs to be placed into the secret and how the corresponding user account on the cloud provider should be configured). All of that is stored in a standard, opaque Kubernetes secret.&lt;/p>
&lt;p>Inside of a shoot manifest, a reference to that secret is given so that Gardener knows which secret to use for a given shoot. Consequently, different shoots, even though they are in the same project, can be created on multiple different cloud provider accounts. However, instead of referring to the secret directly, Gardener introduces another layer of indirection called a SecretBinding.&lt;/p>
&lt;p>In the shoot manifest, we refer to a SecretBinding and the SecretBinding in turn refers to the actual secret.&lt;/p>
&lt;h2 id="secretbindings">SecretBindings&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/secretbindings_5131d3.png" alt="secretbindings">&lt;/p>
&lt;p>With SecretBindings, it is possible to reference the same infrastructure secret in different projects across namespaces. This has the following advantages:​&lt;/p>
&lt;ul>
&lt;li>Infrastructure secrets can be kept in one project (and thus namespace) with limited access. Through SecretsBindings, the secrets can be used in other projects (and thus namespaces) without being able to read their contents.​&lt;/li>
&lt;li>Infrastructure secrets can be kept at one central place (a dedicated project) and be used by many other projects. This way, if a credential rotation is required, they only need to be changed in the secrets at that central place and not in all projects that reference them.&lt;/li>
&lt;/ul>
&lt;h2 id="service-accounts">Service Accounts&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/service-account_ad42b7.png" alt="service-account">&lt;/p>
&lt;p>Since Gardener is 100% Kubernetes, it can be easily used in a programmatic way - by just sending the resource manifest of a Gardener resource to its API server. To do so, a kubeconfig file and a (technical) user that the kubeconfig maps to are required.&lt;/p>
&lt;p>Next to project members, a project can have several service accounts - simple Kubernetes service accounts that are created in a project&amp;rsquo;s namespace. Consequently, every service account will also have its own, dedicated kubeconfig and they can be granted different roles through RoleBindings.&lt;/p>
&lt;p>To integrate Gardener with other infrastructure or CI/CD platforms, one can create a service account, obtain its kubeconfig and then automatically send shoot manifests to the Gardener API server. With that, Kubernetes clusters can be created, modified or deleted on the fly whenever they are needed.&lt;/p></description></item><item><title>Docs: Shoot Status</title><link>https://gardener.cloud/docs/getting-started/observability/shoot-status/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/observability/shoot-status/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>In this topic you can see various shoot statuses and how you can use them to monitor your shoot cluster.&lt;/p>
&lt;h2 id="shoot-status---conditions">Shoot Status - Conditions&lt;/h2>
&lt;p>You can retrieve the shoot status by using &lt;code>kubectl get shoot -oyaml&lt;/code>&lt;/p>
&lt;p>It contains conditions, which give you information about the healthiness of your cluster. Those conditions are also forwarded to the Gardener dashboard and show your cluster as healthy or unhealthy.&lt;/p>
&lt;img style="width: 60%; height: auto; margin: 0, auto" alt="shoot-status-1" src="https://gardener.cloud/__resources/shoot-status-1_81dd83.png"/>
&lt;h2 id="shoot-status---constraints">Shoot Status - Constraints&lt;/h2>
&lt;p>The shoot status also contains constraints. If these constraints are met, your cluster operations are impaired and the cluster is likely to fail at some point. Please watch them and act accordingly.&lt;/p>
&lt;img style="width: 60%; height: auto; margin: 0, auto" alt="shoot-status-2" src="https://gardener.cloud/__resources/shoot-status-2_afb3ee.png"/>
&lt;h2 id="shoot-status---last-operation">Shoot Status - Last Operation&lt;/h2>
&lt;p>The &lt;code>lastOperation&lt;/code>, &lt;code>lastErrors&lt;/code>, and &lt;code>lastMaintenance&lt;/code> give you information on what was last happening in your clusters. This is especially useful when you are facing an error.&lt;/p>
&lt;p>In this example, nodes are being recreated and not all machines have reached the desired state yet.&lt;/p>
&lt;img style="width: 60%; height: auto; margin: 0, auto" alt="shoot-status-3" src="https://gardener.cloud/__resources/shoot-status-3_61cd5c.png"/>
&lt;h2 id="shoot-status---credentials-rotation">Shoot Status - Credentials Rotation&lt;/h2>
&lt;p>You can also see the status of the last credentials rotation. Here you can also programmatically derive when the last rotation was down in order to trigger the next rotation.&lt;/p>
&lt;img style="width: 60%; height: auto; margin: 0, auto" alt="shoot-status-4" src="https://gardener.cloud/__resources/shoot-status-4_5f74e6.png"/></description></item><item><title>Docs: External DNS Management</title><link>https://gardener.cloud/docs/getting-started/features/dns-management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/dns-management/</guid><description>
&lt;h2 id="external-dns-management">External DNS Management&lt;/h2>
&lt;p>When you deploy to Kubernetes, there is no native management of external DNS. Instead, the cloud-controller-manager requests (mostly IPv4) addresses for every service of type LoadBalancer. Of course, the Ingress resource helps here, but how is the external DNS entry for the ingress controller managed?&lt;/p>
&lt;p>Essentially, some sort of automation for DNS management is missing.&lt;/p>
&lt;h2 id="automating-dns-management">Automating DNS Management&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/automate-dns-management_f9812b.png" alt="automate-dns-management">&lt;/p>
&lt;p>From a user&amp;rsquo;s perspective, it is desirable to work with already known resources and concepts. Hence, the DNS management offered by Gardener plugs seamlessly into Kubernetes resources and you do not need to &amp;ldquo;leave&amp;rdquo; the context of the shoot cluster.&lt;/p>
&lt;p>To request a DNS record creation / update, a Service or Ingress resource is annotated accordingly. The shoot-dns-service extension will (if configured) will pick up the request and create a DNSEntry resource + reconcile it to have an actual DNS record created at a configured DNS provider. Gardener supports the following providers:&lt;/p>
&lt;ul>
&lt;li>aws-route53&lt;/li>
&lt;li>azure-dns&lt;/li>
&lt;li>azure-private-dns&lt;/li>
&lt;li>google-clouddns&lt;/li>
&lt;li>openstack-designate&lt;/li>
&lt;li>alicloud-dns&lt;/li>
&lt;li>cloudflare-dns&lt;/li>
&lt;/ul>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/guides/networking/dns-extension/">DNS Names&lt;/a>.&lt;/p>
&lt;h2 id="dns-provider">DNS Provider&lt;/h2>
&lt;p>For the above to work, we need some ingredients. Primarily, this is implemented via a so-called DNSProvider. Every shoot has a default provider that is used to set up the API server&amp;rsquo;s public DNS record. It can be used to request sub-domains as well.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/dns-provider_2d18ba.png" alt="">&lt;/p>
&lt;p>In addition, a shoot can reference credentials to a DNS provider. Those can be used to manage custom domains.&lt;/p>
&lt;p>Please have a look at the &lt;a href="https://gardener.cloud/docs/guides/networking/dns-extension/">documentation&lt;/a> for further details.&lt;/p></description></item><item><title>Docs: Gardener Shoots</title><link>https://gardener.cloud/docs/getting-started/shoots/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/shoots/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/k8s-cluster_4daab7.png" alt="k8s-cluster">&lt;/p>
&lt;p>A Kubernetes cluster consists of a control plane and a data plane. The data plane runs the actual containers on worker nodes (which translate to physical or virtual machines). For the control and data plane to work together properly, lots of components need matching configuration.&lt;/p>
&lt;p>Some configurations are standardized but some are also very specific to the needs of a cluster&amp;rsquo;s user / workload. Ideally, you want a properly configured cluster with the possibility to fine-tune some settings.&lt;/p>
&lt;h2 id="concept-of-a-shoot">Concept of a &amp;ldquo;Shoot&amp;rdquo;&lt;/h2>
&lt;p>In Gardener, Kubernetes clusters (with their control plane and their data plane) are called shoot clusters or simply shoots.
For Gardener, a shoot is just another Kubernetes resource. Gardener components watch it and act upon changes (e.g., creation). It comes with reasonable default settings but also allows fine-tuned configuration. And on top of it, you get a status providing health information, information about ongoing operations, and so on.&lt;/p>
&lt;p>Luckily there is a dashboard to get started.&lt;/p>
&lt;h2 id="basic-configuration-options">Basic Configuration Options&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/basic-configurations-1_073f6f.png" alt="basic-configurations-1">&lt;/p>
&lt;p>Every cluster needs a name - after all, it is a Kubernetes resource and therefore unique within a namespace.&lt;/p>
&lt;p>The Kubernetes version will be used as a starting point. Once a newer version is available, you can always update your existing clusters (but not downgrade, as this is not supported by Kubernetes in general).&lt;/p>
&lt;p>The &amp;ldquo;purpose&amp;rdquo; affects some configuration (like automatic deployment of a monitoring stack or setting up certain alerting rules) and generally indicates the importance of a cluster.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/basic-configurations-2_c00454.png" alt="basic-configurations-2">&lt;/p>
&lt;p>Start by selecting the infrastructure you want to use. The choice will be mapped to a cloud profile that contains provider specific information like the available (actual) OS images, zones and regions or machine types.&lt;/p>
&lt;p>Each data plane runs in an infrastructure account owned by the end user. By selecting the infrastructure secret containing the accounts credentials, you are granting Gardener access to the respective account to create / manage resources.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
&lt;p>Changing the account after the creation of a cluster is not possible. The credentials can be updated with a new key or even user but have to stay within the same account.&lt;/p>
&lt;p>Currently, there is no way to move a single cluster to a different account. You would rather have to re-create a cluster and migrate workloads by different means.&lt;/p>
&lt;/div>
&lt;p>As part of the infrastructure you chose, the region for data plane has to be chosen as well. The Gardener scheduler will try to place the control plane on a seed cluster based on a minimal distance strategy. See &lt;a href="https://gardener.cloud/docs/gardener/concepts/scheduler/">Gardener Scheduler&lt;/a> for more details.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/basic-configurations-3_184cb7.png" alt="basic-configurations-3">&lt;/p>
&lt;p>Up next, the networking provider (CNI) for the cluster has to be selected. At the point of writing, it is possible to choose between Calico and Cilium. If not specified in the shoot&amp;rsquo;s manifest, default CIDR ranges for nodes, services, and pods will be used.&lt;/p>
&lt;p>In order to run any workloads in your cluster, you need nodes. The worker section lets you specify the most important configuration options. For beginners, the machine type is probably the most relevant field, together with the machine image (operating system).&lt;/p>
&lt;p>The machine type is provider-specific and configured in the cloud profile. Check your respective cloud profile if you&amp;rsquo;re missing a machine type. Maybe it is available in general but unavailable in your selected region.&lt;/p>
&lt;p>The operating system your machines will run is the next thing to choose. Debian-based &lt;a href="https://github.com/gardenlinux/gardenlinux">GardenLinux&lt;/a> is the best choice for most use cases.&lt;/p>
&lt;p>Other specifications for the workers include the volume type and size. These settings affect the root disk of each node. Therefore we would always recommend to use an SSD-based type to avoid i/o issues.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Caveat&lt;/h4>
Some machine types (e.g., bare-metal machine types on OpenStack) require you to omit the volume type and volume size settings.
&lt;/div>
&lt;p>The autoscaler parameter defines the initial elasticity / scalability of your cluster. The cluster-autoscaler will add more nodes up to the maximum defined here when your workload grows and remove nodes in case your workload shrinks. The minimum number of nodes should be equal to or higher than the number of zones. You can distribute the nodes of a worker pool among all zones available to your cluster. This is the first step in running HA workloads.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/basic-configurations-4_5b9a86.png" alt="basic-configurations-4">&lt;/p>
&lt;p>Once per day, all clusters reconcile. This means all controllers will check if there are any updates they have to apply (e.g., new image version for ETCD). The maintenance window defines when this daily operation will be triggered. It is important to understand that there is no opt-out for reconciliation.&lt;/p>
&lt;p>It is also possible to confine updates to the shoot spec to be applied only during this time. This can come in handy when you want to bundle changes or prevent changes to be applied outside a well-known time window.&lt;/p>
&lt;p>You can allow Gardener to automatically update your cluster&amp;rsquo;s Kubernetes patch version and/or OS version (of the nodes). Take this decision consciously! Whenever a new Kubernetes patch version or OS version is set to &lt;code>supported&lt;/code> in the respective cloud profile, auto update will upgrade your cluster during the next maintenance window. If you fail to (manually) upgrade the Kubernetes or OS version before they expire, force-upgrades will take place during the maintenance window.&lt;/p>
&lt;h3 id="result">Result&lt;/h3>
&lt;p>The result of your provided inputs and a set of conscious default values is a shoot resource that, once applied, will be acted upon by various Gardener components. The status section represents the intermediate steps / results of these operations. A typical shoot creation flow would look like this:&lt;/p>
&lt;ol>
&lt;li>Assign control plane to a seed.&lt;/li>
&lt;li>Create infrastructure resources in the data plane account (e.g., VPC, gateways, &amp;hellip;)&lt;/li>
&lt;li>Deploy control plane incl. DNS records.&lt;/li>
&lt;li>Create nodes (VMs) and bootstrap kubelets.&lt;/li>
&lt;li>Deploy kube-system components to nodes.&lt;/li>
&lt;/ol>
&lt;h2 id="how-to-access-a-shoot">How to Access a Shoot&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/access-shoot-1_a6af12.png" alt="access-shoot-1">&lt;/p>
&lt;p>Static credentials for shoots were discontinued in Gardener with Kubernetes v1.27. Short lived credentials need to be used instead. You can create/request tokens directly via Gardener or delegate authentication to an identity provider.&lt;/p>
&lt;p>A short-lived admin kubeconfig can be requested by using kubectl. If this is something you do frequently, consider switching to &lt;a href="https://github.com/gardener/gardenlogin">gardenlogin&lt;/a>, which helps you with it.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/access-shoot-2_c89c9e.png" alt="access-shoot-2">&lt;/p>
&lt;p>An alternative is to use an identity provider and issue OIDC tokens.&lt;/p>
&lt;h2 id="what-can-you-configure">What can you configure?&lt;/h2>
&lt;p>With the basic configuration options having been introduced, it is time to discuss more possibilities. Gardener offers a variety of options to tweak the control plane&amp;rsquo;s behavior - like defining an event TTL (default 1h), adding an OIDC configuration or activating some feature gates. You could alter the scheduling profile and define an audit logging policy. In addition, the control plane can be configured to run in HA mode (applied on a node or zone level), but keep in mind that once you enable HA, you cannot go back.&lt;/p>
&lt;p>In case you have specific requirements for the cluster internal DNS, Gardener offers a plugin mechanism for custom core DNS rules or optimization with node-local DNS. For more information, see &lt;a href="https://gardener.cloud/docs/gardener/custom-dns-config/">Custom DNS Configuration&lt;/a> and &lt;a href="https://gardener.cloud/docs/gardener/node-local-dns/">NodeLocalDNS Configuration&lt;/a>.&lt;/p>
&lt;p>Another category of configuration options is dedicated to the nodes and the infrastructure they are running on. Every provider has their own perks and some of them are exposed. Check the detailed documentation of the relevant extension for your infrastructure provider.&lt;/p>
&lt;p>You can fine-tune the cluster-autoscaler or help the kubelet to cope better with your workload.&lt;/p>
&lt;h2 id="worker-pools">Worker Pools&lt;/h2>
&lt;p>There are a couple of ways to configure a worker pool. One of them is to set everything in the Gardener dashboard. However, only a subset of options is presented there.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/worker-pools-1_b5acc0.png" alt="worker-pools-1">&lt;/p>
&lt;p>A slightly more complex way is to set the configuration through the yaml file itself.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/worker-pools-2_5dce7d.png" alt="worker-pools-2">&lt;/p>
&lt;p>This allows you to configure much more properties of a worker pool, like the timeout after which an unhealthy machine is getting replaced. For more options, see the &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#worker">Worker&lt;/a> API reference.&lt;/p>
&lt;h2 id="how-to-change-things">How to Change Things&lt;/h2>
&lt;p>Since a shoot is just another Kubernetes resource, changes can be applied via kubectl. For convenience, the basic settings are configurable via the dashboard&amp;rsquo;s UI. It also has a &amp;ldquo;yaml&amp;rdquo; tab where you can alter all of the shoot&amp;rsquo;s specification in your browser. Once applied, the cluster will reconcile eventually and your changes become active (or cause an error).&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/change-things_7c7ba7.png" alt="change-things">&lt;/p>
&lt;h2 id="immutability-in-a-shoot">Immutability in a Shoot&lt;/h2>
&lt;p>While Gardener allows you to modify existing shoot clusters, it is important to remember that not all properties of a shoot can be changed after it is created.&lt;/p>
&lt;p>For example, it is not possible to move a shoot to a different infrastructure account. This is mainly rooted in the fact that discs and network resources are bound to your account.&lt;/p>
&lt;p>Another set of options that become immutable are most of the network aspects of a cluster. On an infrastructure level the VPC cannot be changed and on a cluster level things like the pod / service cidr ranges, together with the nodeCIDRmask, are set for the lifetime of the cluster.&lt;/p>
&lt;p>Some other things can be changed, but not reverted. While it is possible to add more zones to a cluster on an infrastructure level (assuming that an appropriate CIDR range is available), removing zones is not supported. Similarly, upgrading Kubernetes versions is comparable to a one-way ticket. As of now, Kubernetes does not support downgrading. Lastly, the HA setting of the control plane is immutable once specified.&lt;/p>
&lt;h2 id="crazy-botany">Crazy Botany&lt;/h2>
&lt;p>Since remembering all these options can be quite challenging, here is very helpful resource - an &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml">example shoot&lt;/a> with all the latest options 🎉&lt;/p></description></item><item><title>Docs: Certificate Management</title><link>https://gardener.cloud/docs/getting-started/features/certificate-management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/certificate-management/</guid><description>
&lt;h2 id="certificate-management">Certificate Management&lt;/h2>
&lt;p>For proper consumption, any service should present a TLS certificate to its consumers. However, self-signed certificates are not fit for this purpose - the certificate should be signed by a CA trusted by an application&amp;rsquo;s userbase. Luckily, Issuers like Let&amp;rsquo;s Encrypt and others help here by offering a signing service that issues certificates based on the ACME challenge (Automatic Certificate Management Environment).&lt;/p>
&lt;p>There are plenty of tools you can use to perform the challenge. For Kubernetes, cert-manager certainly is the most common, however its configuration is rather cumbersome and error prone. So let&amp;rsquo;s see how a Gardener extension can help here.&lt;/p>
&lt;h2 id="manage-certificates-with-gardener">Manage Certificates with Gardener&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/manage-certificates_b8392b.png" alt="manage-certificates">&lt;/p>
&lt;p>You may annotate a Service or Ingress resource to trigger the cert-manager to request a certificate from the any configured issuer (e.g. Let&amp;rsquo;s Encrypt) and perform the challenge. A Gardener operator can add a default issuer for convenience.
With the DNS extension discussed previously, setting up the DNS TXT record for the ACME challenge is fairly easy. The requested certificate can be customized by the means of several other annotations known to the controller. Most notably, it is possible to specify SANs via &lt;code>cert.gardener.cloud/dnsnames&lt;/code> to accommodate domain names that have more than 64 characters (the limit for the CN field).&lt;/p>
&lt;p>The user&amp;rsquo;s request for a certificate manifests as a &lt;code>certificate&lt;/code> resource. The status, issuer, and other properties can be checked there.&lt;/p>
&lt;p>Once successful, the resulting certificate will be stored in a secret and is ready for usage.&lt;/p>
&lt;p>With additional configuration, it is also possible to define custom issuers of certificates.&lt;/p>
&lt;p>For more information, see the &lt;a href="https://gardener.cloud/docs/guides/networking/certificate-extension/">Manage certificates with Gardener for public domain&lt;/a> topic and the &lt;a href="https://github.com/gardener/cert-management#follow-cname">cert-management repository&lt;/a>.&lt;/p></description></item><item><title>Docs: Control Plane Components</title><link>https://gardener.cloud/docs/getting-started/ca-components/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/ca-components/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>A cluster has a data plane and a control plane. The data plane is like a space station. It has certain components which keep everyone / everything alive and can operate autonomously to a certain extent. However, without mission control (and the occasional delivery of supplies) it cannot share information or receive new instructions.&lt;/p>
&lt;p>So let&amp;rsquo;s see what the mission control (control plane) of a Kubernetes cluster looks like.&lt;/p>
&lt;h2 id="kubeception">Kubeception&lt;/h2>
&lt;p>&lt;a href="https://gardener.cloud/docs/getting-started/architecture/#kubeception">Kubeception - Kubernetes in Kubernetes in Kubernetes&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kubeception_661793.png" alt="kubeception">&lt;/p>
&lt;p>In the classic setup, there is a dedicated host / VM to host the master components / control plane of a Kubernetes cluster. However, these are just normal programs that can easily be put into containers. Once in containers, we can make Kubernetes Deployments and StatefulSets (for the etcd) watch over them. And now we put all that into a separate, dedicated Kubernetes cluster - et voilà, we have Kubernetes in Kubernetes, aka Kubeception (named after the famous movie Inception with Leonardo DiCaprio).&lt;/p>
&lt;p>In Gardener&amp;rsquo;s terminology, the cluster hosting the control plane components is called a seed cluster. The cluster that end users actually use (and whose control plane is hosted in the seed) is called a shoot cluster.&lt;/p>
&lt;h2 id="control-plane-components-on-the-seed">Control Plane Components on the Seed&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/control-plane-components-1_396730.png" alt="control-plane-components-1">&lt;/p>
&lt;p>All control-plane components of a shoot cluster run in a dedicated namespace on the seed.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/control-plane-components-2_d89d1c.png" alt="control-plane-components-2">&lt;/p>
&lt;p>A control plane has lots of components:&lt;/p>
&lt;ul>
&lt;li>Everything needed to run vanilla Kubernetes&lt;/li>
&lt;li>etcd main &amp;amp; events (split for performance reasons)&lt;/li>
&lt;li>Kube-.*-manager&lt;/li>
&lt;li>CSI driver&lt;/li>
&lt;/ul>
&lt;p>Additionally, we deploy components needed to manage the cluster:&lt;/p>
&lt;ul>
&lt;li>Gardener Resource Manager (GRM)&lt;/li>
&lt;li>Machine Controller Manager (MCM)&lt;/li>
&lt;li>DNS Management&lt;/li>
&lt;li>VPN&lt;/li>
&lt;/ul>
&lt;p>There is also a set of components making our life easier (logging, monitoring) or adding additional features (cert manager).&lt;/p>
&lt;h2 id="core-components">Core Components&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/core-components-1_19b405.png" alt="core-components-1">&lt;/p>
&lt;p>Let&amp;rsquo;s take a close look at the API server as well as etcd.&lt;/p>
&lt;p>Secrets are encrypted at rest. When asking etcd for the data, the reply is still encrypted. Decryption is done by the API server which knows the necessary key.&lt;/p>
&lt;p>For non-HA clusters etcd has only 1 replica, while for HA clusters there are 3 replicas.&lt;/p>
&lt;p>One special remark is needed for Gardener&amp;rsquo;s deployment of etcd. The pods coming from the etcd-main StatefulSet contain two containers - one runs etcd, the other runs a program that periodically backs up etcd&amp;rsquo;s contents to an object store that is set up per seed cluster to make sure no data is lost. After all, etcd is the Achilles heel of all Kubernetes clusters. The backup container is also capable of performing a restore from the object store as well as defragment and compact the etcd datastore. For performance reasons, Gardener stores Kubernetes events in a separate etcd instance. By default, events are retained for 1h but can be kept longer if defined in the &lt;code>shoot.spec&lt;/code>.&lt;/p>
&lt;p>The kube API server (often called &amp;ldquo;kapi&amp;rdquo;) scales both horizontally and vertically.&lt;/p>
&lt;p>The kube API server is not directly exposed / reachable via its public hostname. Instead, Gardener runs a single LoadBalancer service backed by an istio gateway / envoy, which uses SNI to forward traffic.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/core-components-2_527c87.png" alt="core-components-2">&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/overview/components/#kube-controller-manager">kube-controller-manager&lt;/a> (aka &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/">KCM&lt;/a>) is the component that contains all the controllers for the core Kubernetes objects such as Deployments, Services, PVCs, etc.&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/">Kubernetes scheduler&lt;/a> will assign pods to nodes.&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/">Cloud Controller Manager&lt;/a> (aka CCM) is the component that contains all functionality to talk to Cloud environments (e.g., create LoadBalancer services).&lt;/p>
&lt;p>The CSI driver is the storage subsystem of Kubernetes. It provisions and manages anything related to persistence.&lt;/p>
&lt;p>Without the cluster autoscaler, nodes could not be added or removed based on current pressure on the cluster resources. Without the VPA, pods would have fixed resource limits that could not change on demand.&lt;/p>
&lt;h2 id="gardener-specific-components">Gardener-Specific Components&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardener-specific-components_7b4581.png" alt="gardener-specific-components">&lt;/p>
&lt;p>&lt;strong>Shoot DNS service:&lt;/strong> External DNS management for resources within the cluster.&lt;/p>
&lt;p>&lt;strong>Machine Controller Manager:&lt;/strong> Responsible for managing VMs which will become nodes in the cluster.&lt;/p>
&lt;p>&lt;strong>Virtual Private Network deployments&lt;/strong> (aka &lt;a href="https://github.com/gardener/vpn">VPN&lt;/a>): Almost every communication between Kubernetes controllers and the API server is unidirectional - the controllers are given a kubeconfig and will establish a connection to the API server, which is exposed to all nodes of the cluster through a LoadBalancer. However, there are a few operations that require the API server to connect to the kubelet instead (e.g., for every webhook, when using kubectl exec or kubectl logs). Since every good Kubernetes cluster will have its worker nodes shielded behind firewalls to reduce the attack surface, Gardener establishes a VPN connection from the shoot&amp;rsquo;s internal network to the API server in the seed. For that, every shoot, as well as every control plane namespace in the seed, have openVPN pods in them that connect to each other (with the connection being established from the shoot to the seed).&lt;/p>
&lt;p>&lt;strong>Gardener Resource Manager:&lt;/strong> Tooling to deploy and manage Kubernetes resources required for cluster functionality.&lt;/p>
&lt;h2 id="machines">Machines&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/machines_3a36cd.png" alt="machines">&lt;/p>
&lt;p>&lt;strong>Machine Controller Manager&lt;/strong> (aka &lt;a href="https://github.com/gardener/machine-controller-manager">MCM&lt;/a>):&lt;/p>
&lt;p>The machine controller manager, which lives on the seed in a shoot&amp;rsquo;s control plane namespace, is the key component responsible for provisioning and removing worker nodes for a Kubernetes cluster. It acts on MachineClass, MachineDeployment, and MachineSet resources in the seed (think of them as the equivalent of Deployments and ReplicaSets) and controls the lifecycle of machine objects. Through a system of plugins, the MCM is the component that phones to the cloud provider&amp;rsquo;s API and bootstraps virtual machines.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/">MCM&lt;/a> and &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md">Cluster-autoscaler&lt;/a>.&lt;/p>
&lt;h2 id="managedresources">ManagedResources&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/managed-resources_5f4933.png" alt="managed-resources">&lt;/p>
&lt;p>&lt;strong>Gardener Resource Manager&lt;/strong> (aka &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/">GRM&lt;/a>):&lt;/p>
&lt;p>Gardener not only deploys components into the control plane namespace of the seed but also to the shoot (e.g., the counterpart of the VPN). Together with the components in the seed, Gardener needs to have a way to reconcile them.&lt;/p>
&lt;p>Enter the GRM - it reconciles on ManagedResources objects, which are descriptions of Kubernetes resources which are deployed into the seed or shoot by GRM. If any of these resources are modified or deleted by accident, the usual observe-analyze-act cycle will revert these potentially malicious changes back to the values that Gardener envisioned. In fact, all the components found in a shoot&amp;rsquo;s kube-system namespace are ManagedResources governed by the GRM. The actual resource definition is contained in secrets (as they may contain &amp;ldquo;secret&amp;rdquo; data), while the ManagedResources contain a reference to the secret containing the actual resource to be deployed and reconciled.&lt;/p>
&lt;h2 id="dns-records---internal-and-external">DNS Records - &amp;ldquo;Internal&amp;rdquo; and &amp;ldquo;External&amp;rdquo;&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/dns-records_cc6d7e.png" alt="dns-records">&lt;/p>
&lt;p>The internal domain name is used by all Gardener components to talk to the API server. Even though it is called &amp;ldquo;internal&amp;rdquo;, it is still publicly routable.&lt;/p>
&lt;p>But most importantly, it is pre-defined and not configurable by the end user.&lt;/p>
&lt;p>Therefore, the &amp;ldquo;external&amp;rdquo; domain name exists. It is either a user owned domain or can be pre-defined for a Gardener landscape. It is used by any end user accessing the cluster&amp;rsquo;s API server.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/extensions/dnsrecord/">Contract: DNSRecord Resources&lt;/a>.&lt;/p>
&lt;h2 id="features-and-observability">Features and Observability&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/features-observability_39ead9.png" alt="features-observability">&lt;/p>
&lt;p>Gardener runs various health checks to ensure that the cluster works properly. The Network Problem Detector gives information about connectivity within the cluster and to the API server.&lt;/p>
&lt;p>&lt;strong>Certificate Management:&lt;/strong> allows to request certificates via the ACME protocol (e.g., issued by Let&amp;rsquo;s Encrypt) from within the cluster. For detailed information, have a look at the &lt;a href="https://github.com/gardener/cert-management#certificate-management">cert-manager project&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Observability stack:&lt;/strong> Gardener deploys observability components and gathers logs and metrics for the control-plane &amp;amp; kube-system namespace. Also provided out-of-the-box is a UI based on Plutono (fork of Grafana) with pre-defined dashboards to access and query the monitoring data. For more information, see &lt;a href="https://gardener.cloud/docs/getting-started/observability/">Observability&lt;/a>.&lt;/p>
&lt;h2 id="ha-control-plane">HA Control Plane&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/ha-control-plane_f02ea3.png" alt="ha-control-plane">&lt;/p>
&lt;p>As the title indicates, the HA control plane feature is only about the control plane. Setting up the data plane to span multiple zones is part of the worker spec of a shoot.&lt;/p>
&lt;p>HA control planes can be configured as part of the shoot&amp;rsquo;s spec. The available types are:&lt;/p>
&lt;ul>
&lt;li>Node&lt;/li>
&lt;li>Zone&lt;/li>
&lt;/ul>
&lt;p>Both work similarly and just differ in the failure domain the concepts are applied to.&lt;/p>
&lt;p>For detailed guidance and more information, see the &lt;a href="https://gardener.cloud/docs/guides/high-availability/">High Availability Guides&lt;/a>.&lt;/p>
&lt;h2 id="zonal-ha-control-planes">Zonal HA Control Planes&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/zonal-ha-control-planes_36cef6.png" alt="zonal-ha-control-planes">&lt;/p>
&lt;p>Zonal HA is the most likely setup for shoots with &lt;code>purpose: production&lt;/code>.&lt;/p>
&lt;p>The starting point is a regular (non-HA) control plane. etcd and most controllers are singletons and the kube-apiserver might have been scaled up to several replicas.&lt;/p>
&lt;p>To get to an HA setup we need:&lt;/p>
&lt;ul>
&lt;li>A minimum of 3 replicas of the API server&lt;/li>
&lt;li>3 replicas for etcd (both main and events)&lt;/li>
&lt;li>A second instance for each controller (e.g., controller manager, csi-driver, scheduler, etc.) that can take over in case of failure (active / passive).&lt;/li>
&lt;/ul>
&lt;p>To distribute those pods across zones, well-known concepts like PodTopologySpreadConstraints or Affinities are applied.&lt;/p>
&lt;h2 id="kube-system-namespace">kube-system Namespace&lt;/h2>
&lt;img src="https://gardener.cloud/__resources/kube-system-namespace-1_c41f6e.png" alt="kube-system-namespace-1" width="50%"/>
&lt;p>For a fully functional cluster, a few components need to run on the data plane side of the diagram. They all exist in the kube-system namespace. Let&amp;rsquo;s have a closer look at them.&lt;/p>
&lt;h3 id="networking">Networking&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kube-system-namespace-2_5fb502.png" alt="kube-system-namespace-2">&lt;/p>
&lt;p>On each node we need a CNI (container network interface) plugin. Gardener offers Calico or Cilium as network provider for a shoot. When using Calico, a kube-proxy is deployed. Cilium does not need a kube-proxy, as it takes care of its tasks as well.&lt;/p>
&lt;p>The CNI plugin ensures pod-to-pod communication within the cluster. As part of it, it assigns cluster-internal IP addresses to the pods and manages the network devices associated with them. When an overlay network is enabled, calico will also manage the routing of pod traffic between different nodes.&lt;/p>
&lt;p>On the other hand, kube-proxy implements the actual service routing (cilium can do this as well and no kube-proxy is needed). Whenever packets go to a service&amp;rsquo;s IP address, they are re-routed based on IPtables rules maintained by kube-proxy to reach the actual pods backing the service. kube-proxy operates on endpoint-slices and manages IPtables on EVERY node. In addition, kube-proxy provides a health check endpoint for services with &lt;code>externalTrafficPolicy=local&lt;/code>, where traffic only gets to nodes that run a pod matching the selector of the service.&lt;/p>
&lt;p>The egress filter implements basic filtering of outgoing traffic to be compliant with SAP&amp;rsquo;s policies.&lt;/p>
&lt;p>And what happens if the pods crashloop, are missing or otherwise broken?&lt;/p>
&lt;p>Well, in case kube-proxy is broken, service traffic will degrade over time (depending on the pod churn rate and how many kube-proxy pods are broken).&lt;/p>
&lt;p>When calico is failing on a node, no new pods can start there as they don&amp;rsquo;t get any IP address assigned. It might also fail to add routes to newly added nodes. Depending on the error, deleting the pod might help.&lt;/p>
&lt;h3 id="dns-system">DNS System&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kube-system-namespace-3_4f3735.png" alt="kube-system-namespace-3">&lt;/p>
&lt;p>For a normal service in Kubernetes, a cluster-internal DNS record that resolves to the service&amp;rsquo;s ClusterIP address is being created. In Gardener (similar to most other Kubernetes offerings) CoreDNS takes care of this aspect. To reduce the load when it comes to upstream DNS queries, Gardener deploys a DNS cache to each node by default. It will also forward queries outside the cluster&amp;rsquo;s search domain directly to the upstream DNS server. For more information, see &lt;a href="https://gardener.cloud/docs/gardener/node-local-dns/">NodeLocalDNS Configuration&lt;/a> and &lt;a href="https://gardener.cloud/docs/gardener/dns-autoscaling/">DNS autoscaling&lt;/a>.&lt;/p>
&lt;p>In addition to this optimization, Gardener allows &lt;a href="https://gardener.cloud/docs/gardener/custom-dns-config/">custom DNS configuration to be added to CoreDNS&lt;/a> via a dedicated ConfigMap.&lt;/p>
&lt;p>In case this customization is related to non-Kubernetes entities, you may configure the shoot&amp;rsquo;s NodeLocalDNS to forward to CoreDNS instead of upstream (&lt;code>disableForwardToUpstreamDNS: true&lt;/code>).&lt;/p>
&lt;p>A broken DNS system on any level will cause disruption / service degradation for applications within the cluster.&lt;/p>
&lt;h3 id="health-checks-and-metrics">Health Checks and Metrics&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kube-system-namespace-4_a5292e.png" alt="kube-system-namespace-4">&lt;/p>
&lt;p>Gardener deploys probes checking the health of individual nodes. In a similar fashion, a network health check probes connectivity within the cluster (node to node, pod to pod, pod to api-server, &amp;hellip;).&lt;/p>
&lt;p>They provide the data foundation for Gardener&amp;rsquo;s monitoring stack together with the metrics collecting / exporting components.&lt;/p>
&lt;h3 id="connectivity-components">Connectivity Components&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kube-system-namespace-5_fe8073.png" alt="kube-system-namespace-5">&lt;/p>
&lt;p>From the perspective of the data plane, the shoot&amp;rsquo;s API server is reachable via the cluster-internal service &lt;code>kubernetes.default.svc.cluster.local&lt;/code>. The apiserver-proxy intercepts connections to this destination and changes it so that the traffic is forwarded to the kube-apiserver service in the seed cluster. For more information, see &lt;a href="https://github.com/gardener/gardener/blob/764df0ee5ebc13b2634eba98169b409244f19bfe/docs/usage/control-plane-endpoints-and-ports.md#kube-apiserver-via-apiserver-proxy">kube-apiserver via apiserver-proxy&lt;/a>.&lt;/p>
&lt;p>The second component here is the VPN shoot. It initiates a VPN connection to its counterpart in the seed. This way, there is no open port / Loadbalancer needed on the data plane. The VPN connection is used for any traffic flowing from the control plane to the data plane. If the VPN connection is broken, port-forwarding or log querying with kubectl will not work. In addition, webhooks will stop functioning properly.&lt;/p>
&lt;h3 id="csi-driver">csi-driver&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kube-system-namespace-6_d4749d.png" alt="kube-system-namespace-6">&lt;/p>
&lt;p>The last component to mention here is the csi-driver that is deployed as a Daemonset to all nodes. It registers with the kubelet and takes care of the mounting of volume types it is responsible for.&lt;/p></description></item><item><title>Docs: FAQ</title><link>https://gardener.cloud/docs/other-components/machine-controller-manager/faq/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/machine-controller-manager/faq/</guid><description>
&lt;h1 id="frequently-asked-questions">Frequently Asked Questions&lt;/h1>
&lt;p>The answers in this FAQ apply to the newest (HEAD) version of Machine Controller Manager. If
you&amp;rsquo;re using an older version of MCM please refer to corresponding version of
this document. Few of the answers assume that the MCM being used is in conjuction with &lt;a href="https://github.com/gardener/autoscaler">cluster-autoscaler&lt;/a>:&lt;/p>
&lt;h1 id="table-of-contents">Table of Contents:&lt;/h1>
&lt;!--- TOC BEGIN -->
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#basics">Basics&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#what-is-machine-controller-manager">What is Machine Controller Manager?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#Why-is-my-machine-deleted">Why is my machine deleted?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#What-are-the-different-sub-controllers-in-MCM">What are the different sub-controllers in MCM?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#What-is-safety-controller-in-MCM">What is Safety Controller in MCM?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-to">How to?&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-to-install-MCM-in-a-kubernetes-cluster">How to install MCM in a Kubernetes cluster?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-to-better-control-the-rollout-process-of-the-worker-nodes">How to better control the rollout process of the worker nodes?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-to-scale-down-machinedeployment-by-selective-deletion-of-machines">How to scale down MachineDeployment by selective deletion of machines?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-to-force-delete-a-machine">How to force delete a machine?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-to-pause-the-ongoing-rolling-update-of-the-machinedeployment">How to pause the ongoing rolling-update of the machinedeployment?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-to-delete-machine-object-immedietly-if-i-dont-have-access-to-it">How to delete machine object immedietly if I don&amp;rsquo;t have access to it?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-to-avoid-garbage-collection-of-your-node">How to avoid garbage collection of your node?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-to-trigger-rolling-update-of-a-machinedeployment">How to trigger rolling update of a machinedeployment?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#internals">Internals&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#What-is-the-high-level-design-of-MCM">What is the high level design of MCM?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#What-are-the-different-configuration-options-in-MCM">What are the different configuration options in MCM?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#What-are-the-different-timeoutsconfigurations-in-a-machines-lifecycle">What are the different timeouts/configurations in a machine&amp;rsquo;s lifecycle?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-is-the-drain-of-a-machine-implemented">How is the drain of a machine implemented?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-are-the-stateful-applications-drained-during-machine-deletion">How are the stateful applications drained during machine deletion?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-does-maxEvictRetries-configuration-work-with-drainTimeout-configuration">How does maxEvictRetries configuration work with drainTimeout configuration?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#What-are-the-different-phases-of-a-machine">What are the different phases of a machine?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#what-health-checks-are-performed-on-a-machine">What health checks are performed on a machine?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection">How does rate limiting replacement of machine work in MCM ? How is it related to meltdown protection?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-mcm-responds-when-scale-outscale-in-is-done-during-rolling-update-of-a-machinedeployment">How MCM responds when scale-out/scale-in is done during rolling update of a machinedeployment?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-some-unhealthy-machines-are-drained-quickly-">How some unhealthy machines are drained quickly?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-does-mcm-prioritize-the-machines-for-deletion-on-scale-down-of-machinedeployment">How does MCM prioritize the machines for deletion on scale-down of machinedeployment?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#troubleshooting">Troubleshooting&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#My-machine-is-stuck-in-deletion-for-1-hr-why">My machine is stuck in deletion for 1 hr, why?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#My-machine-is-not-joining-the-cluster-why">My machine is not joining the cluster, why?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#developer">Developer&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-should-I-test-my-code-before-submitting-a-PR">How should I test my code before submitting a PR?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#I-need-to-change-the-APIs-what-are-the-recommended-steps">I need to change the APIs, what are the recommended steps?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-can-I-update-the-dependencies-of-MCM">How can I update the dependencies of MCM?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#in-the-context-of-gardener">In the context of Gardener&lt;/a>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-can-I-configure-MCM-using-Shoot-resource">How can I configure MCM using Shoot resource?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#How-is-my-worker-pool-spread-across-zones">How is my worker-pool spread across zones?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--- TOC END -->
&lt;h1 id="basics">Basics&lt;/h1>
&lt;h3 id="what-is-machine-controller-manager">What is Machine Controller Manager?&lt;/h3>
&lt;p>Machine Controller Manager aka MCM is a bunch of controllers used for the lifecycle management of the worker machines. It reconciles a set of CRDs such as &lt;code>Machine&lt;/code>, &lt;code>MachineSet&lt;/code>, &lt;code>MachineDeployment&lt;/code> which depicts the functionality of &lt;code>Pod&lt;/code>, &lt;code>Replicaset&lt;/code>, &lt;code>Deployment&lt;/code> of the core Kubernetes respectively. Read more about it at &lt;a href="https://github.com/gardener/machine-controller-manager/tree/master/docs">README&lt;/a>.&lt;/p>
&lt;ul>
&lt;li>Gardener uses MCM to manage its Kubernetes nodes of the shoot cluster. However, by design, MCM can be used independent of Gardener.&lt;/li>
&lt;/ul>
&lt;h3 id="why-is-my-machine-deleted">Why is my machine deleted?&lt;/h3>
&lt;p>A machine is deleted by MCM generally for 2 reasons-&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Machine is unhealthy for at least &lt;code>MachineHealthTimeout&lt;/code> period. The default &lt;code>MachineHealthTimeout&lt;/code> is 10 minutes.&lt;/p>
&lt;ul>
&lt;li>By default, a machine is considered unhealthy if any of the following node conditions - &lt;code>DiskPressure&lt;/code>, &lt;code>KernelDeadlock&lt;/code>, &lt;code>FileSystem&lt;/code>, &lt;code>Readonly&lt;/code> is set to &lt;code>true&lt;/code>, or &lt;code>KubeletReady&lt;/code> is set to &lt;code>false&lt;/code>. However, this is something that is configurable using the following &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/deployment/out-of-tree/deployment.yaml#L30">flag&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Machine is scaled down by the &lt;code>MachineDeployment&lt;/code> resource.&lt;/p>
&lt;ul>
&lt;li>This is very usual when an external controller cluster-autoscaler (aka CA) is used with MCM. CA deletes the under-utilized machines by scaling down the &lt;code>MachineDeployment&lt;/code>. Read more about cluster-autoscaler&amp;rsquo;s scale down behavior &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#how-does-scale-down-work">here&lt;/a>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="what-are-the-different-sub-controllers-in-mcm">What are the different sub-controllers in MCM?&lt;/h3>
&lt;p>MCM mainly contains the following sub-controllers:&lt;/p>
&lt;ul>
&lt;li>&lt;code>MachineDeployment Controller&lt;/code>: Responsible for reconciling the &lt;code>MachineDeployment&lt;/code> objects. It manages the lifecycle of the &lt;code>MachineSet&lt;/code> objects.&lt;/li>
&lt;li>&lt;code>MachineSet Controller&lt;/code>: Responsible for reconciling the &lt;code>MachineSet&lt;/code> objects. It manages the lifecycle of the &lt;code>Machine&lt;/code> objects.&lt;/li>
&lt;li>&lt;code>Machine Controller&lt;/code>: responsible for reconciling the &lt;code>Machine&lt;/code> objects. It manages the lifecycle of the actual VMs/machines created in cloud/on-prem. This controller has been moved out of tree. Please refer an AWS machine controller for more info - &lt;a href="https://github.com/gardener/machine-controller-manager-provider-gcp">link&lt;/a>.&lt;/li>
&lt;li>Safety-controller: Responsible for handling the unidentified/unknown behaviors from the cloud providers. Please read more about its functionality &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#what-is-safety-controller">below&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="what-is-safety-controller-in-mcm">What is Safety Controller in MCM?&lt;/h3>
&lt;p>&lt;code>Safety Controller&lt;/code> contains following functions:&lt;/p>
&lt;ul>
&lt;li>Orphan VM handler:
&lt;ul>
&lt;li>It lists all the VMs in the cloud matching the &lt;code>tag&lt;/code> of given cluster name and maps the VMs with the &lt;code>machine&lt;/code> objects using the &lt;code>ProviderID&lt;/code> field. VMs without any backing &lt;code>machine&lt;/code> objects are logged and deleted after confirmation.&lt;/li>
&lt;li>This handler runs every 30 minutes and is configurable via &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L112">machine-safety-orphan-vms-period&lt;/a> flag.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Freeze mechanism:
&lt;ul>
&lt;li>&lt;code>Safety Controller&lt;/code> freezes the &lt;code>MachineDeployment&lt;/code> and &lt;code>MachineSet&lt;/code> controller if the number of &lt;code>machine&lt;/code> objects goes beyond a certain threshold on top of &lt;code>Spec.Replicas&lt;/code>. It can be configured by the flag &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L102-L103">&amp;ndash;safety-up or &amp;ndash;safety-down&lt;/a> and also &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go#L113">machine-safety-overshooting-period&lt;/a>.&lt;/li>
&lt;li>&lt;code>Safety Controller&lt;/code> freezes the functionality of the MCM if either of the &lt;code>target-apiserver&lt;/code> or the &lt;code>control-apiserver&lt;/code> is not reachable.&lt;/li>
&lt;li>&lt;code>Safety Controller&lt;/code> unfreezes the MCM automatically once situation is resolved to normal. A &lt;code>freeze&lt;/code> label is applied on &lt;code>MachineDeployment&lt;/code>/&lt;code>MachineSet&lt;/code> to enforce the freeze condition.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h1 id="how-to">How to?&lt;/h1>
&lt;h3 id="how-to-install-mcm-in-a-kubernetes-cluster">How to install MCM in a Kubernetes cluster?&lt;/h3>
&lt;p>MCM can be installed in a cluster with following steps:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Apply all the CRDs from &lt;a href="https://github.com/gardener/machine-controller-manager/tree/master/kubernetes/crds">here&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Apply all the deployment, role-related objects from &lt;a href="https://github.com/gardener/machine-controller-manager/tree/master/kubernetes/deployment/out-of-tree">here&lt;/a>.&lt;/p>
&lt;ul>
&lt;li>Control cluster is the one where the &lt;code>machine-*&lt;/code> objects are stored. Target cluster is where all the node objects are registered.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="how-to-better-control-the-rollout-process-of-the-worker-nodes">How to better control the rollout process of the worker nodes?&lt;/h3>
&lt;p>MCM allows configuring the rollout of the worker machines using &lt;code>maxSurge&lt;/code> and &lt;code>maxUnavailable&lt;/code> fields. These fields are applicable only during the rollout process and means nothing in general scale up/down scenarios.
The overall process is very similar to how the &lt;code>Deployment Controller&lt;/code> manages pods during &lt;code>RollingUpdate&lt;/code>.&lt;/p>
&lt;ul>
&lt;li>&lt;code>maxSurge&lt;/code> refers to the number of additional machines that can be added on top of the &lt;code>Spec.Replicas&lt;/code> of MachineDeployment &lt;em>during rollout process&lt;/em>.&lt;/li>
&lt;li>&lt;code>maxUnavailable&lt;/code> refers to the number of machines that can be deleted from &lt;code>Spec.Replicas&lt;/code> field of the MachineDeployment &lt;em>during rollout process&lt;/em>.&lt;/li>
&lt;/ul>
&lt;h3 id="how-to-scale-down-machinedeployment-by-selective-deletion-of-machines">How to scale down MachineDeployment by selective deletion of machines?&lt;/h3>
&lt;p>During scale down, triggered via &lt;code>MachineDeployment&lt;/code>/&lt;code>MachineSet&lt;/code>, MCM prefers to delete the &lt;code>machine/s&lt;/code> which have the least priority set.
Each &lt;code>machine&lt;/code> object has an annotation &lt;code>machinepriority.machine.sapcloud.io&lt;/code> set to &lt;code>3&lt;/code> by default. Admin can reduce the priority of the given machines by changing the annotation value to &lt;code>1&lt;/code>. The next scale down by &lt;code>MachineDeployment&lt;/code> shall delete the machines with the least priority first.&lt;/p>
&lt;h3 id="how-to-force-delete-a-machine">How to force delete a machine?&lt;/h3>
&lt;p>A machine can be force deleted by adding the label &lt;code>force-deletion: &amp;quot;True&amp;quot;&lt;/code> on the &lt;code>machine&lt;/code> object before executing the actual delete command. During force deletion, MCM skips the drain function and simply triggers the deletion of the machine. This label should be used with caution as it can violate the PDBs for pods running on the machine.&lt;/p>
&lt;h3 id="how-to-pause-the-ongoing-rolling-update-of-the-machinedeployment">How to pause the ongoing rolling-update of the machinedeployment?&lt;/h3>
&lt;p>An ongoing rolling-update of the machine-deployment can be paused by using &lt;code>spec.paused&lt;/code> field. See the example below:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: machine.sapcloud.io/v1alpha1
kind: MachineDeployment
metadata:
name: test-machine-deployment
spec:
paused: true
&lt;/code>&lt;/pre>&lt;p>It can be unpaused again by removing the &lt;code>Paused&lt;/code> field from the machine-deployment.&lt;/p>
&lt;h3 id="how-to-delete-machine-object-immedietly-if-i-dont-have-access-to-it">How to delete machine object immedietly if I don&amp;rsquo;t have access to it?&lt;/h3>
&lt;p>If the user doesn&amp;rsquo;t have access to the machine objects (like in case of Gardener clusters) and they would like to replace a node immedietly then they can place the annotation &lt;code>node.machine.sapcloud.io/trigger-deletion-by-mcm: &amp;quot;true&amp;quot;&lt;/code> on their node. This will start the replacement of the machine with a new node.&lt;/p>
&lt;p>On the other hand if the user deletes the node object immedietly then replacement will start only after &lt;code>MachineHealthTimeout&lt;/code>.&lt;/p>
&lt;p>This annotation can also be used if the user wants to expedite the &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection">replacement of unhealthy nodes&lt;/a>&lt;/p>
&lt;p>&lt;code>NOTE&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;code>node.machine.sapcloud.io/trigger-deletion-by-mcm: &amp;quot;false&amp;quot;&lt;/code> annotation is NOT acted upon by MCM , neither does it mean that MCM will not replace this machine.&lt;/li>
&lt;li>this annotation would delete the desired machine but another machine would be created to maintain &lt;code>desired replicas&lt;/code> specified for the machineDeployment/machineSet. Currently if the user doesn&amp;rsquo;t have access to machineDeployment/machineSet then they cannot remove a machine without replacement.&lt;/li>
&lt;/ul>
&lt;h3 id="how-to-avoid-garbage-collection-of-your-node">How to avoid garbage collection of your node?&lt;/h3>
&lt;p>MCM provides an in-built safety mechanism to garbage collect VMs which have no corresponding machine object. This is done to save costs and is one of the key features of MCM.
However, sometimes users might like to add nodes directly to the cluster without the help of MCM and would prefer MCM to not garbage collect such VMs.
To do so they should remove/not-use tags on their VMs containing the following strings:&lt;/p>
&lt;ol>
&lt;li>&lt;code>kubernetes.io/cluster/&lt;/code>&lt;/li>
&lt;li>&lt;code>kubernetes.io/role/&lt;/code>&lt;/li>
&lt;li>&lt;code>kubernetes-io-cluster-&lt;/code>&lt;/li>
&lt;li>&lt;code>kubernetes-io-role-&lt;/code>&lt;/li>
&lt;/ol>
&lt;h3 id="how-to-trigger-rolling-update-of-a-machinedeployment">How to trigger rolling update of a machinedeployment?&lt;/h3>
&lt;p>Rolling update can be triggered for a machineDeployment by updating one of the following:&lt;/p>
&lt;ul>
&lt;li>&lt;code>.spec.template.annotations&lt;/code>&lt;/li>
&lt;li>&lt;code>.spec.template.spec.class.name&lt;/code>&lt;/li>
&lt;/ul>
&lt;h1 id="internals">Internals&lt;/h1>
&lt;h3 id="what-is-the-high-level-design-of-mcm">What is the high level design of MCM?&lt;/h3>
&lt;p>Please refer the following &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/#design-of-machine-controller-manager">document&lt;/a>.&lt;/p>
&lt;h3 id="what-are-the-different-configuration-options-in-mcm">What are the different configuration options in MCM?&lt;/h3>
&lt;p>MCM allows configuring many knobs to fine-tune its behavior according to the user&amp;rsquo;s need.
Please refer to the &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/cmd/machine-controller-manager/app/options/options.go">link&lt;/a> to check the exact configuration options.&lt;/p>
&lt;h3 id="what-are-the-different-timeoutsconfigurations-in-a-machines-lifecycle">What are the different timeouts/configurations in a machine&amp;rsquo;s lifecycle?&lt;/h3>
&lt;p>A machine&amp;rsquo;s lifecycle is governed by mainly following timeouts, which can be configured &lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/kubernetes/machine_objects/machine-deployment.yaml#L30-L34">here&lt;/a>.&lt;/p>
&lt;ul>
&lt;li>&lt;code>MachineDrainTimeout&lt;/code>: Amount of time after which drain times out and the machine is force deleted. Default ~2 hours.&lt;/li>
&lt;li>&lt;code>MachineHealthTimeout&lt;/code>: Amount of time after which an unhealthy machine is declared &lt;code>Failed&lt;/code> and the machine is replaced by &lt;code>MachineSet&lt;/code> controller.&lt;/li>
&lt;li>&lt;code>MachineCreationTimeout&lt;/code>: Amount of time after which a machine creation is declared &lt;code>Failed&lt;/code> and the machine is replaced by the &lt;code>MachineSet&lt;/code> controller.&lt;/li>
&lt;li>&lt;code>NodeConditions&lt;/code>: List of node conditions which if set to true for &lt;code>MachineHealthTimeout&lt;/code> period, the machine is declared &lt;code>Failed&lt;/code> and replaced by &lt;code>MachineSet&lt;/code> controller.&lt;/li>
&lt;li>&lt;code>MaxEvictRetries&lt;/code>: An integer number depicting the number of times a failed &lt;em>eviction&lt;/em> should be retried on a pod during drain process. A pod is &lt;em>deleted&lt;/em> after &lt;code>max-retries&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h3 id="how-is-the-drain-of-a-machine-implemented">How is the drain of a machine implemented?&lt;/h3>
&lt;p>MCM imports the functionality from the upstream Kubernetes-drain library. Although, few parts have been modified to make it work best in the context of MCM. Drain is executed before machine deletion for graceful migration of the applications.
Drain internally uses the &lt;code>EvictionAPI&lt;/code> to evict the pods and triggers the &lt;code>Deletion&lt;/code> of pods after &lt;code>MachineDrainTimeout&lt;/code>. Please note:&lt;/p>
&lt;ul>
&lt;li>Stateless pods are evicted in parallel.&lt;/li>
&lt;li>Stateful applications (with PVCs) are serially evicted. Please find more info in this &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-are-the-stateful-applications-drained-during-machine-deletion">answer below&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="how-are-the-stateful-applications-drained-during-machine-deletion">How are the stateful applications drained during machine deletion?&lt;/h3>
&lt;p>Drain function serially evicts the stateful-pods. It is observed that serial eviction of stateful pods yields better overall availability of pods as the underlying cloud in most cases detaches and reattaches disks serially anyways.
It is implemented in the following manner:&lt;/p>
&lt;ul>
&lt;li>Drain lists all the pods with attached volumes. It evicts very first stateful-pod and waits for its related entry in Node object&amp;rsquo;s &lt;code>.status.volumesAttached&lt;/code> to be removed by KCM. It does the same for all the stateful-pods.&lt;/li>
&lt;li>It waits for &lt;code>PvDetachTimeout&lt;/code> (default 2 minutes) for a given pod&amp;rsquo;s PVC to be removed, else moves forward.&lt;/li>
&lt;/ul>
&lt;h3 id="how-does-maxevictretries-configuration-work-with-draintimeout-configuration">How does &lt;code>maxEvictRetries&lt;/code> configuration work with &lt;code>drainTimeout&lt;/code> configuration?&lt;/h3>
&lt;p>It is recommended to only set &lt;code>MachineDrainTimeout&lt;/code>. It satisfies the related requirements. &lt;code>MaxEvictRetries&lt;/code> is auto-calculated based on &lt;code>MachineDrainTimeout&lt;/code>, if &lt;code>maxEvictRetries&lt;/code> is not provided. Following will be the overall behavior of both configurations together:&lt;/p>
&lt;ul>
&lt;li>If &lt;code>maxEvictRetries&lt;/code> isn&amp;rsquo;t set and only &lt;code>maxDrainTimeout&lt;/code> is set:
&lt;ul>
&lt;li>MCM auto calculates the &lt;code>maxEvictRetries&lt;/code> based on the &lt;code>drainTimeout&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If &lt;code>drainTimeout&lt;/code> isn&amp;rsquo;t set and only &lt;code>maxEvictRetries&lt;/code> is set:
&lt;ul>
&lt;li>Default &lt;code>drainTimeout&lt;/code> and user provided &lt;code>maxEvictRetries&lt;/code> for each pod is considered.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If both &lt;code>maxEvictRetries&lt;/code> and &lt;code>drainTimoeut&lt;/code> are set:
&lt;ul>
&lt;li>Then both will be respected.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If none are set:
&lt;ul>
&lt;li>Defaults are respected.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="what-are-the-different-phases-of-a-machine">What are the different phases of a machine?&lt;/h3>
&lt;p>A phase of a &lt;code>machine&lt;/code> can be identified with &lt;code>Machine.Status.CurrentStatus.Phase&lt;/code>. Following are the possible phases of a &lt;code>machine&lt;/code> object:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>Pending&lt;/code>: Machine creation call has succeeded. MCM is waiting for machine to join the cluster.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>CrashLoopBackOff&lt;/code>: Machine creation call has failed. MCM will retry the operation after a minor delay.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Running&lt;/code>: Machine creation call has succeeded. Machine has joined the cluster successfully and corresponding node doesn&amp;rsquo;t have &lt;code>node.gardener.cloud/critical-components-not-ready&lt;/code> taint.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Unknown&lt;/code>: Machine &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#what-health-checks-are-performed-on-a-machine">health checks&lt;/a> are failing, eg &lt;code>kubelet&lt;/code> has stopped posting the status.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Failed&lt;/code>: Machine health checks have failed for a prolonged time. Hence it is declared failed by &lt;code>Machine&lt;/code> controller in a &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection">rate limited fashion&lt;/a>. &lt;code>Failed&lt;/code> machines get replaced immediately.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Terminating&lt;/code>: Machine is being terminated. Terminating state is set immediately when the deletion is triggered for the &lt;code>machine&lt;/code> object. It also includes time when it&amp;rsquo;s being drained.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;code>NOTE&lt;/code>: No phase means the machine is being created on the cloud-provider.&lt;/p>
&lt;p>Below is a simple phase transition diagram:
&lt;img src="https://gardener.cloud/__resources/machine_phase_transition_1435f5.png" alt="image">&lt;/p>
&lt;h3 id="what-health-checks-are-performed-on-a-machine">What health checks are performed on a machine?&lt;/h3>
&lt;p>Health check performed on a machine are:&lt;/p>
&lt;ul>
&lt;li>Existense of corresponding node obj&lt;/li>
&lt;li>Status of certain user-configurable node conditions.
&lt;ul>
&lt;li>These conditions can be specified using the flag &lt;code>--node-conditions&lt;/code> for OOT MCM provider or can be specified per machine object.&lt;/li>
&lt;li>The default user configurable node conditions can be found &lt;a href="https://github.com/gardener/machine-controller-manager/blob/91eec24516b8339767db5a40e82698f9fe0daacd/pkg/util/provider/app/options/options.go#L60">here&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>True&lt;/code> status of &lt;code>NodeReady&lt;/code> condition . This condition shows kubelet&amp;rsquo;s status&lt;/li>
&lt;/ul>
&lt;p>If any of the above checks fails , the machine turns to &lt;code>Unknown&lt;/code> phase.&lt;/p>
&lt;h3 id="how-does-rate-limiting-replacement-of-machine-work-in-mcm-how-is-it-related-to-meltdown-protection">How does rate limiting replacement of machine work in MCM? How is it related to meltdown protection?&lt;/h3>
&lt;p>Currently MCM replaces only &lt;code>1&lt;/code> &lt;code>Unkown&lt;/code> machine at a time per machinedeployment. This means until the particular &lt;code>Unknown&lt;/code> machine get terminated and its replacement joins, no other &lt;code>Unknown&lt;/code> machine would be removed.&lt;/p>
&lt;p>The above is achieved by enabling &lt;code>Machine&lt;/code> controller to turn machine from &lt;code>Unknown&lt;/code> -&amp;gt; &lt;code>Failed&lt;/code> only if the above condition is met. &lt;code>MachineSet&lt;/code> controller on the other hand marks &lt;code>Failed&lt;/code> machine as &lt;code>Terminating&lt;/code> immediately.&lt;/p>
&lt;p>One reason for this rate limited replacement was to ensure that in case of network failures , where node&amp;rsquo;s kubelet can&amp;rsquo;t reach out to kube-apiserver , all nodes are not removed together i.e. &lt;code>meltdown protection&lt;/code>.
In gardener context however, &lt;a href="https://gardener.cloud/docs/other-components/dependency-watchdog/concepts/prober/#origin">DWD&lt;/a> is deployed to deal with this scenario, but to stay protected from corner cases , this mechanism has been introduced in MCM.&lt;/p>
&lt;p>&lt;code>NOTE&lt;/code>: Rate limiting replacement is not yet configurable&lt;/p>
&lt;h3 id="how-mcm-responds-when-scale-outscale-in-is-done-during-rolling-update-of-a-machinedeployment">How MCM responds when scale-out/scale-in is done during rolling update of a machinedeployment?&lt;/h3>
&lt;p>&lt;code>Machinedeployment&lt;/code> controller executes the logic of &lt;code>scaling&lt;/code> BEFORE logic of &lt;code>rollout&lt;/code>. It identifies &lt;code>scaling&lt;/code> by comparing the &lt;code>deployment.kubernetes.io/desired-replicas&lt;/code> of each machineset under the machinedeployment with machinedeployment&amp;rsquo;s &lt;code>.spec.replicas&lt;/code>. If the difference is found for any machineSet, a scaling event is detected.&lt;/p>
&lt;p>Case &lt;code>scale-out&lt;/code> -&amp;gt; ONLY New machineSet is scaled out &lt;br>
Case &lt;code>scale-in&lt;/code> -&amp;gt; ALL machineSets(new or old) are scaled in , in proportion to their replica count , any leftover is adjusted in the largest machineSet.&lt;/p>
&lt;p>During update for scaling event, a machineSet is updated if any of the below is true for it:&lt;/p>
&lt;ul>
&lt;li>&lt;code>.spec.Replicas&lt;/code> needs update&lt;/li>
&lt;li>&lt;code>deployment.kubernetes.io/desired-replicas&lt;/code> needs update&lt;/li>
&lt;/ul>
&lt;p>Once scaling is achieved, rollout continues.&lt;/p>
&lt;h3 id="how-does-mcm-prioritize-the-machines-for-deletion-on-scale-down-of-machinedeployment">How does MCM prioritize the machines for deletion on scale-down of machinedeployment?&lt;/h3>
&lt;p>There could be many machines under a machinedeployment with different phases, creationTimestamp. When a scale down is triggered, MCM decides to remove the machine using the following logic:&lt;/p>
&lt;ul>
&lt;li>Machine with least value of &lt;code>machinepriority.machine.sapcloud.io&lt;/code> annotation is picked up.&lt;/li>
&lt;li>If all machines have equal priorities, then following precedence is followed:
&lt;ul>
&lt;li>Terminating &amp;gt; Failed &amp;gt; CrashloopBackoff &amp;gt; Unknown &amp;gt; Pending &amp;gt; Available &amp;gt; Running&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>If still there is no match, the machine with oldest creation time (.i.e. creationTimestamp) is picked up.&lt;/li>
&lt;/ul>
&lt;h2 id="how-some-unhealthy-machines-are-drained-quickly-">How some unhealthy machines are drained quickly ?&lt;/h2>
&lt;p>If a node is unhealthy for more than the &lt;code>machine-health-timeout&lt;/code> specified for the &lt;code>machine-controller&lt;/code>, the controller
health-check moves the machine phase to &lt;code>Failed&lt;/code>. By default, the &lt;code>machine-health-timeout&lt;/code> is 10` minutes.&lt;/p>
&lt;p>&lt;code>Failed&lt;/code> machines have their deletion timestamp set and the machine then moves to the &lt;code>Terminating&lt;/code> phase. The node
drain process is initiated. The drain process is invoked either &lt;em>gracefully&lt;/em> or &lt;em>forcefully&lt;/em>.&lt;/p>
&lt;p>The usual drain process is graceful. Pods are evicted from the node and the drain process waits until any existing
attached volumes are mounted on new node. However, if the node &lt;code>Ready&lt;/code> is &lt;code>False&lt;/code> or the &lt;code>ReadonlyFilesystem&lt;/code> is &lt;code>True&lt;/code>
for greater than &lt;code>5&lt;/code> minutes (non-configurable), then a forceful drain is initiated. In a forceful drain, pods are deleted
and &lt;code>VolumeAttachment&lt;/code> objects associated with the old node are also marked for deletion. This is followed by the deletion of the
cloud provider VM associated with the &lt;code>Machine&lt;/code> and then finally ending with the &lt;code>Node&lt;/code> object deletion.&lt;/p>
&lt;p>During the deletion of the VM we only delete the local data disks and boot disks associated with the VM. The disks associated
with persistent volumes are left un-touched as their attach/de-detach, mount/unmount processes are handled by k8s
attach-detach controller in conjunction with the CSI driver.&lt;/p>
&lt;h1 id="troubleshooting">Troubleshooting&lt;/h1>
&lt;h3 id="my-machine-is-stuck-in-deletion-for-1-hr-why">My machine is stuck in deletion for 1 hr, why?&lt;/h3>
&lt;p>In most cases, the &lt;code>Machine.Status.LastOperation&lt;/code> provides information around why a machine can&amp;rsquo;t be deleted.
Though following could be the reasons but not limited to:&lt;/p>
&lt;ul>
&lt;li>Pod/s with misconfigured PDBs block the drain operation. PDBs with &lt;code>maxUnavailable&lt;/code> set to 0, doesn&amp;rsquo;t allow the eviction of the pods. Hence, drain/eviction is retried till &lt;code>MachineDrainTimeout&lt;/code>. Default &lt;code>MachineDrainTimeout&lt;/code> could be as large as ~2hours. Hence, blocking the machine deletion.
&lt;ul>
&lt;li>Short term: User can manually delete the pod in the question, &lt;em>with caution&lt;/em>.&lt;/li>
&lt;li>Long term: Please set more appropriate PDBs which allow disruption of at least one pod.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Expired cloud credentials can block the deletion of the machine from infrastructure.&lt;/li>
&lt;li>Cloud provider can&amp;rsquo;t delete the machine due to internal errors. Such situations are best debugged by using cloud provider specific CLI or cloud console.&lt;/li>
&lt;/ul>
&lt;h3 id="my-machine-is-not-joining-the-cluster-why">My machine is not joining the cluster, why?&lt;/h3>
&lt;p>In most cases, the &lt;code>Machine.Status.LastOperation&lt;/code> provides information around why a machine can&amp;rsquo;t be created.
It could possibly be debugged with following steps:&lt;/p>
&lt;ul>
&lt;li>Firstly make sure all the relevant controllers like &lt;code>kube-controller-manager&lt;/code> , &lt;code>cloud-controller-manager&lt;/code> are running.&lt;/li>
&lt;li>Verify if the machine is actually created in the cloud. User can use the &lt;code>Machine.Spec.ProviderId&lt;/code> to query the machine in cloud.&lt;/li>
&lt;li>A Kubernetes node is generally bootstrapped with the cloud-config. Please verify, if &lt;code>MachineDeployment&lt;/code> is pointing the correct &lt;code>MachineClass&lt;/code>, and &lt;code>MachineClass&lt;/code> is pointing to the correct &lt;code>Secret&lt;/code>. The secret object contains the actual cloud-config in &lt;code>base64&lt;/code> format which will be used to boot the machine.&lt;/li>
&lt;li>User must also check the logs of the MCM pod to understand any broken logical flow of reconciliation.&lt;/li>
&lt;/ul>
&lt;h3 id="my-rolling-update-is-stuck--why">My rolling update is stuck , why?&lt;/h3>
&lt;p>The following can be the reason:&lt;/p>
&lt;ul>
&lt;li>Insufficient capacity for the new instance type the machineClass mentions.&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/#my-machine-is-stuck-in-deletion-for-1-hr-why">Old machines are stuck in deletion&lt;/a>&lt;/li>
&lt;li>If you are using Gardener for setting up kubernetes cluster, then machine object won&amp;rsquo;t turn to &lt;code>Running&lt;/code> state until &lt;code>node-critical-components&lt;/code> are ready. Refer &lt;a href="https://gardener.cloud/docs/gardener/node-readiness/">this&lt;/a> for more details.&lt;/li>
&lt;/ul>
&lt;h1 id="developer">Developer&lt;/h1>
&lt;h3 id="how-should-i-test-my-code-before-submitting-a-pr">How should I test my code before submitting a PR?&lt;/h3>
&lt;ul>
&lt;li>Developer can locally setup the MCM using following &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/local_setup/">guide&lt;/a>&lt;/li>
&lt;li>Developer must also enhance the unit tests related to the incoming changes.&lt;/li>
&lt;li>Developer can locally run the unit test by executing:&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>make test-unit
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Developer can locally run &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/integration_tests/">integration tests&lt;/a> to ensure basic functionality of MCM is not altered.&lt;/li>
&lt;/ul>
&lt;h3 id="i-need-to-change-the-apis-what-are-the-recommended-steps">I need to change the APIs, what are the recommended steps?&lt;/h3>
&lt;p>Developer should add/update the API fields at both of the following places:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/types.go">https://github.com/gardener/machine-controller-manager/blob/master/pkg/apis/machine/types.go&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/machine-controller-manager/tree/master/pkg/apis/machine/v1alpha1">https://github.com/gardener/machine-controller-manager/tree/master/pkg/apis/machine/v1alpha1&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Once API changes are done, auto-generate the code using following command:&lt;/p>
&lt;pre tabindex="0">&lt;code>make generate
&lt;/code>&lt;/pre>&lt;p>Please ignore the API-violation errors for now.&lt;/p>
&lt;h3 id="how-can-i-update-the-dependencies-of-mcm">How can I update the dependencies of MCM?&lt;/h3>
&lt;p>MCM uses &lt;code>gomod&lt;/code> for depedency management.
Developer should add/udpate depedency in the go.mod file. Please run following command to automatically tidy the dependencies.&lt;/p>
&lt;pre tabindex="0">&lt;code>make tidy
&lt;/code>&lt;/pre>&lt;h1 id="in-the-context-of-gardener">In the context of Gardener&lt;/h1>
&lt;h3 id="how-can-i-configure-mcm-using-shoot-resource">How can I configure MCM using Shoot resource?&lt;/h3>
&lt;p>All of the knobs of MCM can be configured by the &lt;code>workers&lt;/code> &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L29-L126">section&lt;/a> of the shoot resource.&lt;/p>
&lt;ul>
&lt;li>Gardener creates a &lt;code>MachineDeployment&lt;/code> per zone for each worker-pool under &lt;code>workers&lt;/code> section.&lt;/li>
&lt;li>&lt;code>workers.dataVolumes&lt;/code> allows to attach multiple disks to a machine during creation. Refer the &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L29-L126">link&lt;/a>.&lt;/li>
&lt;li>&lt;code>workers.machineControllerManager&lt;/code> allows configuration of multiple knobs of the &lt;code>MachineDeployment&lt;/code> from the shoot resource.&lt;/li>
&lt;/ul>
&lt;h3 id="how-is-my-worker-pool-spread-across-zones">How is my worker-pool spread across zones?&lt;/h3>
&lt;p>Shoot resource allows the worker-pool to spread across multiple zones using the field &lt;code>workers.zones&lt;/code>. Refer &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml#L115">link&lt;/a>.&lt;/p>
&lt;ul>
&lt;li>Gardener creates one &lt;code>MachineDeployment&lt;/code> per zone. Each &lt;code>MachineDeployment&lt;/code> is initiated with the following replica:&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>MachineDeployment.Spec.Replicas = (Workers.Minimum)/(Number of availibility zones)
&lt;/code>&lt;/pre></description></item><item><title>Docs: Shoot Lifecycle</title><link>https://gardener.cloud/docs/getting-started/lifecycle/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/lifecycle/</guid><description>
&lt;h2 id="reconciliation-in-kubernetes-and-gardener">Reconciliation in Kubernetes and Gardener&lt;/h2>
&lt;p>The starting point of all reconciliation cycles is the constant observation of both the desired and actual state. A component would analyze any differences between the two states and try to converge the actual towards the desired state using appropriate actions. Typically, a component is responsible for a single resource type but it also watches others that have an implication on it.&lt;/p>
&lt;p>As an example, the Kubernetes controller for ReplicaSets will watch pods belonging to it in order to ensure that the specified replica count is fulfilled. If one pod gets deleted, the controller will create a new pod to enforce the desired over the actual state.&lt;/p>
&lt;p>This is all standard behaviour, as Gardener is following the native Kubernetes approach. All elements of a shoot cluster have a representation in Kubernetes resources and controllers are watching / acting upon them.&lt;/p>
&lt;p>If we pick up the example of the ReplicaSet - a user typically creates a &lt;code>deployment&lt;/code> resource and the ReplicaSet is implicitly generated on the way to create the pods. Similarly, Gardener takes the user&amp;rsquo;s intent (shoot) and creates lots of domain specific resources on the way. They all reconcile and make sure their actual and desired states match.&lt;/p>
&lt;h2 id="updating-the-desired-state-of-a-shoot">Updating the Desired State of a Shoot&lt;/h2>
&lt;p>Based on the shoot&amp;rsquo;s specifications, Gardener will create network resources on a hyperscaler, backup resources for the ETCD, credentials, and other resources, but also representations of the worker pools. Eventually, this process will result in a fully functional Kubernetes cluster.&lt;/p>
&lt;p>If you change the desired state, Gardener will reconcile the shoot and run through the same cycle to ensure the actual state matches the desired state.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/update-shoot-state_0608f2.png" alt="update-shoot-state">&lt;/p>
&lt;p>For example, the (infrastructure-specific) machine type can be changed within the shoot resource. The following reconciliation will pick up the change and initiate the creation of new nodes with a different machine type and the removal of the old nodes.&lt;/p>
&lt;h2 id="maintenance-window-and-daily-reconciliation">Maintenance Window and Daily Reconciliation&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/maintenance-window_e7495f.png" alt="maintenance-window">&lt;/p>
&lt;p>EVERY shoot cluster reconciles once per day during the so-called &amp;ldquo;maintenance window&amp;rdquo;. You can confine the rollout of spec changes to this window.&lt;/p>
&lt;p>Additionally, the daily reconciliation will help pick up all kind of version changes. When a new Gardener version was rolled out to the landscape, shoot clusters will pick up any changes during their next reconciliation. For example, if a new Calico version is introduced to fix some bug, it will automatically reach all shoots.&lt;/p>
&lt;h2 id="impact-of-a-change">Impact of a Change&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/change-impact_6effbf.png" alt="change-impact">&lt;/p>
&lt;p>It is important to be aware of the impacts that a change can have on a cluster and the workloads within it.&lt;/p>
&lt;p>An operator pushing a new Gardener version with a new calico image to a landscape will cause all calico pods to be re-created. Another example would be the rollout of a new etcd backup-restore image. This would cause etcd pods to be re-created, rendering a non-HA control plane unavailable until etcd is up and running again.&lt;/p>
&lt;p>When you change the shoot spec, it can also have significant impact on the cluster. Imagine that you have changes the machine type of a worker pool. This will cause new machines to be created and old machines to be deleted. Or in other words: all nodes will be drained, the pods will be evicted and then re-created on newly created nodes.&lt;/p>
&lt;h2 id="kubernetes-version-update-minor--patch">Kubernetes Version Update (Minor + Patch)&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/k8s-version-update_d3f6f2.png" alt="k8s-version-update">&lt;/p>
&lt;p>Some operations are rather common and have to be performed on a regular basis. Updating the Kubernetes version is one them. Patch updates cause relatively little disruption, as only the control-plane pods will be re-created with new images and the kubelets on all nodes will restart.&lt;/p>
&lt;p>A minor version update is more impactful - it will cause all nodes to be recreated and rolls components of the control plane.&lt;/p>
&lt;h2 id="os-version-update">OS Version Update&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/os-update_06ee1f.png" alt="os-update">&lt;/p>
&lt;p>The OS version is defined for each worker pool and can be changed per worker pool. You can freely switch back and forth. However, as there is no in-place update, each change will cause the entire worker pool to roll and nodes will be replaced.
For OS versions different update strategies can be configured. Please check the &lt;a href="https://github.com/gardener/gardener/blob/master/docs/usage/shoot_versions.md/#update-path-for-machine-image-versions">documentation&lt;/a> for details.&lt;/p>
&lt;h2 id="available-versions">Available Versions​&lt;/h2>
&lt;img style="width: 80%; height: auto; margin: 0, auto" alt="available-versions" src="https://gardener.cloud/__resources/available-versions_71eae0.png"/>
&lt;p>Gardener has a dedicated resource to maintain a list of available versions – the so-called &lt;code>cloudProfile&lt;/code>.&lt;/p>
&lt;p>A cloudProfile provides information about supported​:&lt;/p>
&lt;ul>
&lt;li>Kubernetes versions​&lt;/li>
&lt;li>OS versions (and where to find those images)​&lt;/li>
&lt;li>Regions (and their zones)​&lt;/li>
&lt;li>Machine types​&lt;/li>
&lt;/ul>
&lt;p>Each shoot references a cloudProfile in order to obtain information about available / possible versions and configurations.&lt;/p>
&lt;h2 id="version-classifications">Version Classifications&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/version-classifications_35a8c0.png" alt="version-classifications">&lt;/p>
&lt;p>Gardener has the following classifications for Kubernetes and OS image versions:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>preview&lt;/code>: still in testing phase (several versions can be in preview at the same time)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>supported&lt;/code>: recommended version&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>deprecated&lt;/code>: a new version has been set to &amp;ldquo;supported&amp;rdquo;, updating is recommended (might have an expiration date)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>expired&lt;/code>: cannot be used anymore, clusters using this version will be force-upgraded&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Version information is maintained in the relevant cloud profile resource. There might be circumstances where a version will never become &lt;code>supported&lt;/code> but instead move to &lt;code>deprecated&lt;/code> directly. Similarly, a version might be directly introduced as &lt;code>supported&lt;/code>.&lt;/p>
&lt;h2 id="autoupdate--forced-updates">AutoUpdate / Forced Updates&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/auto-update_2628a4.png" alt="auto-update">&lt;/p>
&lt;p>AutoUpdate for a machine image version will update all node pools to the latest supported version based on the defined update strategy. Whenever a new version is set to &lt;code>supported&lt;/code>, the cluster will pick it up during its next maintenance window.&lt;/p>
&lt;p>For Kubernetes versions the mechanism is the same, but only applied to patch version. This means that the cluster will be kept on the latest supported patch version of a specific minor version.&lt;/p>
&lt;p>In case a version used in a cluster expires, there is a force update during the next maintenance window. In a worst case scenario, 2 minor versions expire simultaneously. Then there will be two consecutive minor updates enforced.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/shoot_versions/">Shoot Kubernetes and Operating System Versioning in Gardener&lt;/a>.&lt;/p>
&lt;h2 id="applying-changes-to-a-seed">Applying Changes to a Seed&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/seeds-change_b8d4ae.png" alt="seeds-change">&lt;/p>
&lt;p>It is important to keep in mind that a seed is just another Kubernetes cluster. As such, it has its own lifecycle (daily reconciliation, maintenance, etc.) and is also a subject to change.&lt;/p>
&lt;p>From time to time changes need to be applied to the seed as well. Some (like updating the OS version) cause the node pool to roll. In turn, this will cause the eviction of ALL pods running on the affected node. If your etcd is evicted and you don&amp;rsquo;t have a highly available control plane, it will cause downtime for your cluster. Your workloads will continue to run ,of course, but your cluster&amp;rsquo;s API server will not function until the etcd is up and running again.&lt;/p></description></item><item><title>Docs: Vertical Pod Autoscaler</title><link>https://gardener.cloud/docs/getting-started/features/vpa/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/vpa/</guid><description>
&lt;h2 id="vertical-pod-autoscaler">Vertical Pod Autoscaler&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/vpa_43b432.gif" alt="vpa">&lt;/p>
&lt;p>When a pod&amp;rsquo;s resource CPU or memory grows, it will hit a limit eventually. Either the pod has resource limits specified or the node will run short of resources. In both cases, the workload might be throttled or even terminated. When this happens, it is often desirable to increase the request or limits. To do this autonomously within certain boundaries is the goal of the Vertical Pod Autoscaler project.&lt;/p>
&lt;p>Since it is not part of the standard Kubernetes API, you have to install the CRDs and controller manually. With Gardener, you can simply flip the switch in the shoot&amp;rsquo;s spec and start creating your VPA objects.&lt;/p>
&lt;p>Please be aware that VPA and HPA operate in similar domains and might interfere.&lt;/p>
&lt;p>A controller &amp;amp; CRDs for vertical pod auto-scaling can be activated via the shoot&amp;rsquo;s spec.&lt;/p></description></item><item><title>Docs: Cluster Autoscaler</title><link>https://gardener.cloud/docs/getting-started/features/cluster-autoscaler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/cluster-autoscaler/</guid><description>
&lt;h2 id="obtaining-aditional-nodes">Obtaining Aditional Nodes&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/additional-nodes_4a5ea7.gif" alt="additional-nodes">&lt;/p>
&lt;p>The scheduler will assign pods to nodes, as long as they have capacity (CPU, memory, Pod limit, # attachable disks, &amp;hellip;). But what happens when all nodes are fully utilized and the scheduler does not find any suitable target?&lt;/p>
&lt;p>&lt;strong>Option 1:&lt;/strong> Evict other pods based on priority. However, this has the downside that other workloads with lower priority might become unschedulable.&lt;/p>
&lt;p>&lt;strong>Option 2:&lt;/strong> Add more nodes. There is an upstream Cluster Autoscaler project that does exactly this. It simulates the scheduling and reacts to pods not being schedulable events. Gardener has forked it to make it work with machine-controller-manager abstraction of how node (groups) are defined in Gardener.
The cluster autoscaler respects the limits (min / max) of any worker pool in a shoot&amp;rsquo;s spec. It can also scale down nodes based on utilization thresholds. For more details, see the &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md">autoscaler documentation&lt;/a>.&lt;/p>
&lt;h2 id="scaling-by-priority">Scaling by Priority&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/priority-scaling_a4bb49.gif" alt="priority-scaling">&lt;/p>
&lt;p>For clusters with more than one node pool, the cluster autoscaler has to decide which group to scale up. By default, it randomly picks from the available / applicable. However, this behavior is customizable by the use of so-called expanders.&lt;/p>
&lt;p>This section will focus on the priority based expander.&lt;/p>
&lt;p>Each worker pool gets a priority and the cluster autoscaler will scale up the one with the highest priority until it reaches its limit.&lt;/p>
&lt;p>To get more information on the current status of the autoscaler, you can check a &amp;ldquo;status&amp;rdquo; configmap in the &lt;code>kube-system&lt;/code> namespace with the following command:&lt;/p>
&lt;p>&lt;code>kubectl get cm -n kube-system cluster-autoscaler-status -oyaml&lt;/code>&lt;/p>
&lt;p>To obtain information about the decision making, you can check the logs of the cluster-autoscaler pod by using the shoot&amp;rsquo;s monitoring stack.&lt;/p>
&lt;p>For more information, see the &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders">cluster-autoscaler FAQ&lt;/a> and the &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/expander/priority/readme.md">Priority based expander for cluster-autoscaler&lt;/a> topic.&lt;/p></description></item><item><title>Docs: Gardenctl V2</title><link>https://gardener.cloud/docs/gardenctl-v2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/gardenctl-v2/</guid><description>
&lt;h1 id="gardenctl-v2">gardenctl-v2&lt;/h1>
&lt;p>&lt;img src="https://gardener.cloud/__resources/logo_gardener_cli_large_6e11b6.png" alt="">&lt;/p>
&lt;p>&lt;a href="https://api.reuse.software/info/github.com/gardener/gardenctl-v2">&lt;img src="https://api.reuse.software/badge/github.com/gardener/gardenctl-v2" alt="REUSE status">&lt;/a>
&lt;a href="https://goreportcard.com/report/github.com/gardener/gardenctl-v2">&lt;img src="https://goreportcard.com/badge/github.com/gardener/gardenctl-v2" alt="Go Report Card">&lt;/a>
&lt;a href="https://badge.fury.io/gh/gardener%2Fgardenctl-v2">&lt;img src="https://badge.fury.io/gh/gardener%2Fgardenctl-v2.svg" alt="release">&lt;/a>&lt;/p>
&lt;h2 id="what-is-gardenctl">What is &lt;code>gardenctl&lt;/code>?&lt;/h2>
&lt;p>gardenctl is a command-line client for the Gardener. It facilitates the administration of one or many garden, seed and shoot clusters. Use this tool to configure access to clusters and configure cloud provider CLI tools. It also provides support for accessing cluster nodes via ssh.&lt;/p>
&lt;h2 id="installation">Installation&lt;/h2>
&lt;p>Install the latest release from &lt;a href="https://brew.sh/">Homebrew&lt;/a>, &lt;a href="https://chocolatey.org/packages/gardenctl-v2">Chocolatey&lt;/a> or &lt;a href="https://github.com/gardener/gardenctl-v2/releases">GitHub Releases&lt;/a>.&lt;/p>
&lt;h3 id="install-using-package-managers">Install using Package Managers&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Homebrew (macOS and Linux)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>brew install gardener/tap/gardenctl-v2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Chocolatey (Windows)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># default location C:\ProgramData\chocolatey\bin\gardenctl-v2.exe&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>choco install gardenctl-v2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Attention &lt;code>brew&lt;/code> users: &lt;code>gardenctl-v2&lt;/code> uses the same binary name as the legacy &lt;code>gardenctl&lt;/code> (&lt;code>gardener/gardenctl&lt;/code>) CLI. If you have an existing installation you should remove it with &lt;code>brew uninstall gardenctl&lt;/code> before attempting to install &lt;code>gardenctl-v2&lt;/code>. Alternatively, you can choose to link the binary using a different name. If you try to install without removing or relinking the old installation, brew will run into an error and provide instructions how to resolve it.&lt;/p>
&lt;h3 id="install-from-github-release">Install from Github Release&lt;/h3>
&lt;p>If you install via GitHub releases, you need to&lt;/p>
&lt;ul>
&lt;li>put the &lt;code>gardenctl&lt;/code> binary on your path&lt;/li>
&lt;li>and &lt;a href="https://github.com/gardener/gardenlogin#installation">install gardenlogin&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>The other install methods do this for you.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Example for macOS&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># set operating system and architecture&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>os=darwin &lt;span style="color:#008000"># choose between darwin, linux, windows&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>arch=amd64 &lt;span style="color:#008000"># choose between amd64, arm64&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Get latest version. Alternatively set your desired version&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>version=&lt;span style="color:#00f">$(&lt;/span>curl -s https://raw.githubusercontent.com/gardener/gardenctl-v2/master/LATEST&lt;span style="color:#00f">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Download gardenctl&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>curl -LO &lt;span style="color:#a31515">&amp;#34;https://github.com/gardener/gardenctl-v2/releases/download/&lt;/span>&lt;span style="color:#a31515">${&lt;/span>version&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">/gardenctl_v2_&lt;/span>&lt;span style="color:#a31515">${&lt;/span>os&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">_&lt;/span>&lt;span style="color:#a31515">${&lt;/span>arch&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Make the gardenctl binary executable&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>chmod +x &lt;span style="color:#a31515">&amp;#34;./gardenctl_v2_&lt;/span>&lt;span style="color:#a31515">${&lt;/span>os&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">_&lt;/span>&lt;span style="color:#a31515">${&lt;/span>arch&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Move the binary in to your PATH&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>sudo mv &lt;span style="color:#a31515">&amp;#34;./gardenctl_v2_&lt;/span>&lt;span style="color:#a31515">${&lt;/span>os&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">_&lt;/span>&lt;span style="color:#a31515">${&lt;/span>arch&lt;span style="color:#a31515">}&lt;/span>&lt;span style="color:#a31515">&amp;#34;&lt;/span> /usr/local/bin/gardenctl
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="configuration">Configuration&lt;/h2>
&lt;p>&lt;code>gardenctl&lt;/code> requires a configuration file. The default location is in &lt;code>~/.garden/gardenctl-v2.yaml&lt;/code>.&lt;/p>
&lt;p>You can modify this file directly using the &lt;code>gardenctl config&lt;/code> command. It allows adding, modifying and deleting gardens.&lt;/p>
&lt;p>Example &lt;code>config&lt;/code> command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Adapt the path to your kubeconfig file for the garden cluster (not to be mistaken with your shoot cluster)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export KUBECONFIG=~/relative/path/to/kubeconfig.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Fetch cluster-identity of garden cluster from the configmap&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>cluster_identity=&lt;span style="color:#00f">$(&lt;/span>kubectl -n kube-system get configmap cluster-identity -ojsonpath={.data.cluster-identity}&lt;span style="color:#00f">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Configure garden cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gardenctl config set-garden $cluster_identity --kubeconfig $KUBECONFIG
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This command will create or update a garden with the provided identity and kubeconfig path of your garden cluster.&lt;/p>
&lt;h3 id="example-config">Example Config&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>gardens:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - identity: landscape-dev &lt;span style="color:#008000"># Unique identity of the garden cluster. See cluster-identity ConfigMap in kube-system namespace of the garden cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeconfig: ~/relative/path/to/kubeconfig.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># name: my-name # An alternative, unique garden name for targeting&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># context: different-context # Overrides the current-context of the garden cluster kubeconfig&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># patterns: ~ # List of regex patterns for pattern targeting&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note: You need to have &lt;a href="https://github.com/gardener/gardenlogin">gardenlogin&lt;/a> installed as &lt;code>kubectl&lt;/code> plugin in order to use the &lt;code>kubeconfig&lt;/code>s for &lt;code>Shoot&lt;/code> clusters provided by &lt;code>gardenctl&lt;/code>.&lt;/p>
&lt;h3 id="config-path-overwrite">Config Path Overwrite&lt;/h3>
&lt;ul>
&lt;li>The &lt;code>gardenctl&lt;/code> config path can be overwritten with the environment variable &lt;code>GCTL_HOME&lt;/code>.&lt;/li>
&lt;li>The &lt;code>gardenctl&lt;/code> config name can be overwritten with the environment variable &lt;code>GCTL_CONFIG_NAME&lt;/code>.&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>export GCTL_HOME=/alternate/garden/config/dir
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export GCTL_CONFIG_NAME=myconfig &lt;span style="color:#008000"># without extension!&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># config is expected to be under /alternate/garden/config/dir/myconfig.yaml&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="shell-session">Shell Session&lt;/h3>
&lt;p>The state of gardenctl is bound to a shell session and is not shared across windows, tabs or panes.
A shell session is defined by the environment variable &lt;code>GCTL_SESSION_ID&lt;/code>. If this is not defined,
the value of the &lt;code>TERM_SESSION_ID&lt;/code> environment variable is used instead. If both are not defined,
this leads to an error and gardenctl cannot be executed. The &lt;code>target.yaml&lt;/code> and temporary
&lt;code>kubeconfig.*.yaml&lt;/code> files are store in the following directory &lt;code>${TMPDIR}/garden/${GCTL_SESSION_ID}&lt;/code>.&lt;/p>
&lt;p>You can make sure that &lt;code>GCTL_SESSION_ID&lt;/code> or &lt;code>TERM_SESSION_ID&lt;/code> is always present by adding
the following code to your terminal profile &lt;code>~/.profile&lt;/code>, &lt;code>~/.bashrc&lt;/code> or comparable file.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>bash and zsh: [ -n &lt;span style="color:#a31515">&amp;#34;&lt;/span>$GCTL_SESSION_ID&lt;span style="color:#a31515">&amp;#34;&lt;/span> ] || [ -n &lt;span style="color:#a31515">&amp;#34;&lt;/span>$TERM_SESSION_ID&lt;span style="color:#a31515">&amp;#34;&lt;/span> ] || export GCTL_SESSION_ID=&lt;span style="color:#00f">$(&lt;/span>uuidgen&lt;span style="color:#00f">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>fish: [ -n &lt;span style="color:#a31515">&amp;#34;&lt;/span>$GCTL_SESSION_ID&lt;span style="color:#a31515">&amp;#34;&lt;/span> ] || [ -n &lt;span style="color:#a31515">&amp;#34;&lt;/span>$TERM_SESSION_ID&lt;span style="color:#a31515">&amp;#34;&lt;/span> ] || set -gx GCTL_SESSION_ID (uuidgen)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-ps" data-lang="ps">&lt;span style="display:flex;">&lt;span>powershell: if &lt;span style="color:#a31515">( !(Test-Path Env:GCTL_SESSION_ID) -and !(Test-Path Env:TERM_SESSION_ID) )&lt;/span> { $Env:GCTL_SESSION_ID = [guid]::NewGuid&lt;span style="color:#a31515">()&lt;/span>.ToString&lt;span style="color:#a31515">()&lt;/span> }
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="completion">Completion&lt;/h3>
&lt;p>Gardenctl supports completion that will help you working with the CLI and save you typing effort.
It will also help you find clusters by providing suggestions for gardener resources such as shoots or projects.
Completion is supported for &lt;code>bash&lt;/code>, &lt;code>zsh&lt;/code>, &lt;code>fish&lt;/code> and &lt;code>powershell&lt;/code>.
You will find more information on how to configure your shell completion for gardenctl by executing the help for
your shell completion command. Example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>gardenctl completion bash --help
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="usage">Usage&lt;/h2>
&lt;h3 id="targeting">Targeting&lt;/h3>
&lt;p>You can set a target to use it in subsequent commands. You can also overwrite the target for each command individually.&lt;/p>
&lt;p>Note that this will not affect your KUBECONFIG env variable. To update the KUBECONFIG env for your current target see &lt;a href="https://gardener.cloud/docs/gardenctl-v2/#configure-kubeconfig-for-shoot-clusters">Configure KUBECONFIG&lt;/a> section&lt;/p>
&lt;p>Example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># target control plane&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>gardenctl target --garden landscape-dev --project my-project --shoot my-shoot --control-plane
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Find more information in the &lt;a href="https://github.com/gardener/gardenctl-v2/blob/master/docs/usage/targeting.md">documentation&lt;/a>.&lt;/p>
&lt;h3 id="configure-kubeconfig-for-shoot-clusters">Configure KUBECONFIG for Shoot Clusters&lt;/h3>
&lt;p>Generate a script that points KUBECONFIG to the targeted cluster for the specified shell. Use together with &lt;code>eval&lt;/code> to configure your shell. Example for &lt;code>bash&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>eval &lt;span style="color:#00f">$(&lt;/span>gardenctl kubectl-env bash&lt;span style="color:#00f">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="configure-cloud-provider-clis">Configure Cloud Provider CLIs&lt;/h3>
&lt;p>Generate the cloud provider CLI configuration script for the specified shell. Use together with &lt;code>eval&lt;/code> to configure your shell. Example for &lt;code>bash&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>eval &lt;span style="color:#00f">$(&lt;/span>gardenctl provider-env bash&lt;span style="color:#00f">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="ssh">SSH&lt;/h3>
&lt;p>Establish an SSH connection to a Shoot cluster&amp;rsquo;s node.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>gardenctl ssh my-node
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Common Pitfalls</title><link>https://gardener.cloud/docs/getting-started/common-pitfalls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/common-pitfalls/</guid><description>
&lt;h2 id="architecture">Architecture&lt;/h2>
&lt;h3 id="containers-will-not-fix-a-broken-architecture">Containers will NOT fix a broken architecture!&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/microservices_0c3efe.png" alt="microservices">&lt;/p>
&lt;p>Running a highly distributed system has advantages, but of course, those come at a cost. In order to succeed, one would need:&lt;/p>
&lt;ul>
&lt;li>Logging&lt;/li>
&lt;li>Tracing&lt;/li>
&lt;li>No singleton&lt;/li>
&lt;li>Tolerance to failure of individual instances&lt;/li>
&lt;li>Automated config / change management&lt;/li>
&lt;li>Kubernetes knowledge&lt;/li>
&lt;/ul>
&lt;h2 id="scalability">Scalability&lt;/h2>
&lt;p>Most scalability dimensions are interconnected with others. If a cluster grows beyond reasonable defaults, it can still function very well. But tuning it comes at the cost of time and can influence stability negatively.&lt;/p>
&lt;p>Take the number of nodes and pods, for example. Both are connected and you cannot grow both towards their individual limits, as you would face issues way before reaching any theoretical limits.&lt;/p>
&lt;p>Reading the &lt;a href="https://gardener.cloud/docs/guides/administer-shoots/scalability/">Scalability of Gardener Managed Kubernetes Clusters&lt;/a> guide is strongly recommended in order to understand the topic of scalability within Kubernetes and Gardener.&lt;/p>
&lt;h3 id="a-small-sample-of-things-that-can-grow-beyond-reasonable-limits">A Small Sample of Things That Can Grow Beyond Reasonable Limits&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/hibernation-1_fc2e4c.png" alt="hibernation-1">&lt;/p>
&lt;p>When scaling a cluster, there are plenty of resources that can be exhausted or reach a limit:&lt;/p>
&lt;ul>
&lt;li>The API server will be scaled horizontally and vertically by Gardener. However, it can still consume too much resources to fit onto a single node on the seed. In this case, you can only reduce the load on the API server. This should not happen with regular usage patterns though.&lt;/li>
&lt;li>ETCD disk space: 8GB is the limit. If you have too many resources or a high churn rate, a cluster can run out of ETCD capacity. In such a scenario it will stop working until defragmented, compacted, and cleaned up.&lt;/li>
&lt;li>The number of nodes is limited by the network configuration (pod cidr range &amp;amp; node cidr mask). Also, there is a reasonable number of nodes (300) that most workloads should not exceed. It is possible to go beyond but doing so requires careful tuning and consideration of connected scaling dimensions (like the number of pods per node).&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>The availability of your cluster is directly impacted by the way you use it.&lt;/strong>&lt;/p>
&lt;h3 id="infrastructure-capacity-and-quotas">Infrastructure Capacity and Quotas&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/capacity_85f843.png" alt="capacity">&lt;/p>
&lt;p>Sometimes requests cannot be fulfilled due to shortages on the infrastructure side. For example, a certain instance type might not be available and new Kubernetes nodes of this type cannot be added. It is a good practice to use the &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/expander/priority/readme.md">cluster-autoscaler&amp;rsquo;s priority expander&lt;/a> and have a secondary node pool.&lt;/p>
&lt;p>Sometimes, it is not the physical capacity but exhausted quotas within an infrastructure account that result in limits. Obviously, there should be sufficient quota to create as many VMs as needed. But there are also other resources that are created in the infrastructure that need proper quotas:&lt;/p>
&lt;ul>
&lt;li>Loadbalancers&lt;/li>
&lt;li>VPC&lt;/li>
&lt;li>Disks&lt;/li>
&lt;li>Routes (often forgotten, but very important for clusters without overlay network; typically defaults to around 50 routes, meaning that 50 nodes is the maximum a cluster can have)&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;h3 id="nodecidrmasksize">NodeCIDRMaskSize&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/mask-size_e0ca8c.png" alt="mask-size">&lt;/p>
&lt;p>Upon cluster creation, there are several settings that are network related. For example, the address space for Pods has to be defined. In this case, it is a &lt;code>/16&lt;/code> subnet that includes a total of 65.536 hosts. However, that does not imply that you can easily use all addresses at the same point in time.&lt;/p>
&lt;p>As part of the Kubernetes network setup, the &lt;code>/16&lt;/code> network is divided into smaller subnets and each node gets a distinct subnet. The size of this subnet defaults to &lt;code>/24&lt;/code>. It can also be specified (but not changed later).&lt;/p>
&lt;p>Now, as you create more nodes, you have a total of 256 subnets that can be assigned to nodes, thus limiting the total number of nodes of this cluster to 256.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/shoot_networking/">Shoot Networking&lt;/a>.&lt;/p>
&lt;h2 id="overlapping-vpcs">Overlapping VPCs&lt;/h2>
&lt;h3 id="avoid-overlapping-cidr-ranges-in-vpcs">Avoid Overlapping CIDR Ranges in VPCs&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/cidr-ranges_ba59aa.png" alt="cidr-ranges">&lt;/p>
&lt;p>Gardener can create shoot cluster resources in an existing / user-created VPC. However, you have to make sure that the CIDR ranges used by the shoots nodes or subnets for zones do not overlap with other shoots deployed to the same VPC.&lt;/p>
&lt;p>In case of an overlap, there might be strange routing effects, and packets ending up at a wrong location.&lt;/p>
&lt;h2 id="expired-credentials">Expired Credentials&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/expired-credentials_811acc.png" alt="expired-credentials">&lt;/p>
&lt;p>Credentials expire or get revoked. When this happens to the actively used infrastructure credentials of a shoot, the cluster will stop working after a while. New nodes cannot be added, LoadBalancers cannot be created, and so on.&lt;/p>
&lt;p>You can update the credentials stored in the project namespace and reconcile the cluster to replicate the new keys to all relevant controllers. Similarly, when doing a planned rotation one should wait until the shoot reconciled successfully before invalidating the old credentials.&lt;/p>
&lt;h2 id="autoupdate-breaking-clusters">AutoUpdate Breaking Clusters&lt;/h2>
&lt;p>Gardener can automatically update a shoot&amp;rsquo;s Kubernetes patch version, when a new patch version is labeled as &amp;ldquo;supported&amp;rdquo;. Automatically updating of the OS images works in a similar way. Both are triggered by the &amp;ldquo;supported&amp;rdquo; classification in the respective cloud profile and can be enabled / disabled as part a shoot&amp;rsquo;s spec.&lt;/p>
&lt;p>Additionally, when a minor Kubernetes / OS version expires, Gardener will force-update the shoot to the next supported version.&lt;/p>
&lt;p>Turning on AutoUpdate for a shoot may be convenient but comes at the risk of potentially unwanted changes. While it is possible to switch to another OS version, updates to the Kubernetes version are a one way operation and cannot be reverted.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Recommendation&lt;/h4>
Control the version lifecycle separately for any cluster that hosts important workload.
&lt;/div>
&lt;h2 id="node-draining">Node Draining&lt;/h2>
&lt;h3 id="node-draining-and-pod-disruption-budget">Node Draining and Pod Disruption Budget&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/node-draining_18cb25.gif" alt="node-draining">&lt;/p>
&lt;p>Typically, nodes are drained when:&lt;/p>
&lt;ul>
&lt;li>There is a update of the OS / Kubernetes minor version&lt;/li>
&lt;li>An Operator cordons &amp;amp; drains a node&lt;/li>
&lt;li>The cluster-autoscaler wants to scale down&lt;/li>
&lt;/ul>
&lt;p>Without a PodDistruptionBudget, pods will be terminated as fast as possible. If an application has 2 out of 2 replicas running on the drained node, this will probably cause availability issues.&lt;/p>
&lt;h3 id="node-draining-with-pdb">Node Draining with PDB&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/node-draining-pdb_f95465.gif" alt="node-draining-pdb">&lt;/p>
&lt;p>PodDisruptionBudgets can help to manage a graceful node drain. However, if no disruptions are allowed there, the node drain will be blocked until it reaches a timeout. Only then will the nodes be terminated but without respecting PDB thresholds.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Recommendation&lt;/h4>
Configure PDBs and allow disruptions.
&lt;/div>
&lt;h2 id="pod-resource-requests-and-limits">Pod Resource Requests and Limits&lt;/h2>
&lt;h3 id="resource-consumption">Resource Consumption&lt;/h3>
&lt;p>Pods consume resources and, of course, there are only so many resources available on a single node. Setting requests will make the scheduling much better, as the scheduler has more information available.&lt;/p>
&lt;p>Specifying limits can help, but can also limit an application in unintended ways. A recommendation to start with:&lt;/p>
&lt;ul>
&lt;li>Do not set CPU limits (CPU is compressible and throttling is really hard to detect)&lt;/li>
&lt;li>Set memory limits and monitor OOM kills / restarts of workload (typically detectable by container status exit code 137 and corresponding events). This will decrease the likelihood of OOM situations on the node itself. However, for critical workloads it might be better to have uncapped growth and rather risk a node going OOM.&lt;/li>
&lt;/ul>
&lt;p>Next, consider if assigning the workload to quality of service class &lt;code>guaranteed&lt;/code> is needed. Again - this can help or be counterproductive. It is important to be aware of its implications. For more information, see &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/">Pod Quality of Service Classes&lt;/a>.&lt;/p>
&lt;p>Tune &lt;code>shoot.spec.Kubernetes.kubeReserved&lt;/code> to protect the node (kubelet) in case of a workload pod consuming too much resources. It is very helpful to ensure a high level of stability.&lt;/p>
&lt;p>If the usage profile changes over time, the VPA can help a lot to adapt the resource requests / limits automatically.&lt;/p>
&lt;h2 id="webhooks">Webhooks&lt;/h2>
&lt;h3 id="user-deployed-webhooks-in-kubernetes">User-Deployed Webhooks in Kubernetes&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/user-webhook_aed032.gif" alt="user-webhook">&lt;/p>
&lt;p>By default, any request to the API server will go through a chain of checks. Let&amp;rsquo;s take the example of creating a pod.&lt;/p>
&lt;p>When the resource is submitted to the API server, it will be checked against the following validations:&lt;/p>
&lt;ul>
&lt;li>Is the user authorized to perform this action?&lt;/li>
&lt;li>Is the pod definitionactually valid?&lt;/li>
&lt;li>Are the specified values allowed?&lt;/li>
&lt;/ul>
&lt;p>Additionally, there is the defaulting - like the injection of the &lt;code>default&lt;/code> service account&amp;rsquo;s name, if nothing else is specified.&lt;/p>
&lt;p>This chain of admission control and mutation can be enhanced by the user. Read about &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">dynamic admission control&lt;/a> for more details.&lt;/p>
&lt;p>&lt;code>ValidatingWebhookConfiguration&lt;/code>: allow or deny requests based on custom rules&lt;/p>
&lt;p>&lt;code>MutatingWebhookConfiguration&lt;/code>: change а resource before it is actually stored in etcd (that is, before any other controller acts upon)&lt;/p>
&lt;p>Both &lt;code>ValidatingWebhookConfiguration&lt;/code> as well as &lt;code>MutatingWebhookConfiguration&lt;/code> resources:&lt;/p>
&lt;ul>
&lt;li>specify for which resources and operations these checks should be executed.&lt;/li>
&lt;li>specify how to reach the webhook server (typically a service running on the data plane of a cluster)&lt;/li>
&lt;li>rely on a webhook server performing a review and reply to the &lt;code>admissionReview&lt;/code> request&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://gardener.cloud/__resources/user-webhook-fail_a09a80.gif" alt="user-webhook-fail">&lt;/p>
&lt;p>What could possibly go wrong?
Due to the separation of control plane and data plane in Gardener&amp;rsquo;s architecture, webhooks have the potential to break a cluster.
If the webhook server is not responding in time with a valid answer, the request should timeout and the failure policy is invoked. Depending on the scope of the webhook, frequent failures may cause downtime for applications.
Common causes for failure are:&lt;/p>
&lt;ul>
&lt;li>The call to the webhook is made through the VPN tunnel. VPN / connection issues can happen both on the side of the seed as well as the shoot and would render the webhook unavailable from the perspective of the control plane.&lt;/li>
&lt;li>The traffic cannot reach the pod (network issue, pod not available)&lt;/li>
&lt;li>The pod is processing too slow (e.g., because there are too many requests)&lt;/li>
&lt;/ul>
&lt;h3 id="timeout">Timeout&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/timeout_fba80a.png" alt="timeout">&lt;/p>
&lt;p>Webhooks are a very helpful feature of Kubernetes. However, they can easily be configured to break a shoot cluster. Take the timeout, for example. High timeouts (&amp;gt;15s) can lead to blocking requests of control plane components. That&amp;rsquo;s because most control-plane API calls are made with a client-side timeout of 30s, so if a webhook has &lt;code>timeoutSeconds=30&lt;/code>, the overall request might still fail as there is overhead in communication with the API server and other potential webhooks.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Recommendation&lt;/h4>
Webhooks (esp. mutating) may be called sequentially and thus adding up their individual timeouts. Even with a &lt;code>faliurePolicy=ignore&lt;/code> the timeout will stop the request.
&lt;/div>
&lt;h3 id="recommendations">Recommendations&lt;/h3>
&lt;p>Problematic webhooks are reported as part of a shoot&amp;rsquo;s status. In addition to timeouts, it is crucial to exclude the &lt;code>kube-system&lt;/code> namespace and (potentially non-namespaced) resources that are necessary for the cluster to function properly. Those should not be subject to a user-defined webhook.&lt;/p>
&lt;p>In particular, a webhook should not operate on:&lt;/p>
&lt;ul>
&lt;li>the &lt;code>kube-system&lt;/code> namespace&lt;/li>
&lt;li>&lt;code>Endpoints&lt;/code> or &lt;code>EndpointSlices&lt;/code>&lt;/li>
&lt;li>&lt;code>Nodes&lt;/code>&lt;/li>
&lt;li>&lt;code>PodSecurityPolicies&lt;/code>&lt;/li>
&lt;li>&lt;code>ClusterRoles&lt;/code>&lt;/li>
&lt;li>&lt;code>ClusterRoleBindings&lt;/code>&lt;/li>
&lt;li>&lt;code>CustomResourceDefinitions&lt;/code>&lt;/li>
&lt;li>&lt;code>ApiServices&lt;/code>&lt;/li>
&lt;li>&lt;code>CertificateSigningRequests&lt;/code>&lt;/li>
&lt;li>&lt;code>PriorityClasses&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Example:&lt;/strong>&lt;/p>
&lt;p>A webhook checks node objects upon creation and has a &lt;code>failurePolicy: fail&lt;/code>. If the webhook does not answer in time (either due to latency or because there is no pod serving it), new nodes cannot join the cluster.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/shoot_status/#constraints">Shoot Status&lt;/a>.&lt;/p>
&lt;h2 id="conversion-webhooks">Conversion Webhooks&lt;/h2>
&lt;h3 id="who-installs-a-conversion-webhook">Who installs a conversion webhook?&lt;/h3>
&lt;p>If you have written your own &lt;code>CustomResourceDefinition&lt;/code> (CRD) and made a version upgrade, you will also have consciously written &amp;amp; deployed the conversion webhook.&lt;/p>
&lt;p>However, sometimes, you simply use helm or kustomize to install a (third-party) dependency that contains CRDs. Of course, those can contain conversion webhooks as well. As a user of a cluster, please make sure to be aware what you deploy.&lt;/p>
&lt;h3 id="crd-with-a-conversion-webhook">CRD with a Conversion Webhook&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/conversion-webhook-crd_42b789.png" alt="conversion-webhook-crd">&lt;/p>
&lt;p>Conversion webhooks are tricky. Similarly to regular webhooks, they should have a low timeout. However, they cannot be remediated automatically and can cause errors in the control plane. For example, if a webhook is invoked but not available, it can block the garbage collection run by the kube-controller-manager.&lt;/p>
&lt;p>In turn, when deleting something like a &lt;code>deployment&lt;/code>, dependent resources like &lt;code>pods&lt;/code> will not be deleted automatically.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Recommendation&lt;/h4>
Try to avoid conversion webhooks. They are valid and can be used, but should not stay in place forever. Complete the upgrade to a new version of the CRD as soon as possible.
&lt;/div>
&lt;p>For more information, see the &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#webhook-conversion">Webhook Conversion&lt;/a>, &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#upgrade-existing-objects-to-a-new-stored-version">Upgrade Existing Objects to a New Stored Version&lt;/a>, and &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#version-priority">Version Priority&lt;/a> topics in the Kubernetes documentation.&lt;/p></description></item><item><title>Docs: Kubernetes Cluster Hardening Procedure</title><link>https://gardener.cloud/docs/security-and-compliance/kubernetes-hardening/</link><pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/security-and-compliance/kubernetes-hardening/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>The Gardener team takes security seriously, which is why we mandate the Security Technical Implementation Guide (STIG) for Kubernetes as published by the Defense Information Systems Agency (DISA) &lt;a href="https://public.cyber.mil/stigs/downloads/">here&lt;/a>. We offer Gardener adopters the opportunity to show compliance with DISA Kubernetes STIG via the compliance checker tool &lt;a href="https://github.com/gardener/diki">diki&lt;/a>. The latest release in machine readable format can be found in the &lt;a href="https://public.cyber.mil/stigs/downloads/?_dl_facet_stigs=container-platform">STIGs Document Library&lt;/a> by searching for Kubernetes.&lt;/p>
&lt;h2 id="kubernetes-clusters-security-requirements">Kubernetes Clusters Security Requirements&lt;/h2>
&lt;p>&lt;a href="https://cyber.trackr.live/stig/Kubernetes/1/11">DISA Kubernetes STIG version 1 release 11&lt;/a> contains 91 rules overall. &lt;strong>Only the following rules, however, apply to you&lt;/strong>. Some of them are secure-by-default, so your responsibility is to make sure that they are not changed. For your convenience, the requirements are grouped logically and per role:&lt;/p>
&lt;h2 id="rules-relevant-for-cluster-admins">Rules Relevant for Cluster Admins&lt;/h2>
&lt;h3 id="control-plane-configuration">Control Plane Configuration&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>ID&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Secure By Default&lt;/th>
&lt;th>Comments&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>242390&lt;/td>
&lt;td>Kubernetes API server must have anonymous authentication disabled&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>Disabled unless you enable it via &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#kubeapiserverconfig">enableAnnonymousAuthentication&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>245543&lt;/td>
&lt;td>Kubernetes API Server must disable token authentication to protect information in transit&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>Disabled unless you enable it via &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#kubernetes">enableStaticTokenKubeconfig&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242400&lt;/td>
&lt;td>Kubernetes API server must have Alpha APIs disabled&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>Disabled unless you enable it via &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#kubernetesconfig">featureGates&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242436&lt;/td>
&lt;td>Kubernetes API server must have the ValidatingAdmissionWebhook enabled&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>Enabled unless you disable it explicitly via &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#kubeapiserverconfig">admissionPlugins&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242393&lt;/td>
&lt;td>Kubernetes Worker Nodes must not have sshd service running&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>Active to allow debugging of network issues, but it is possible to deactivate via the &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.SSHAccess">sshAccess&lt;/a> setting&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242394&lt;/td>
&lt;td>Kubernetes Worker Nodes must not have the sshd service enabled&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>Enabled to allow debugging of network issues, but it is possible to deactivate via the &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.SSHAccess">sshAccess&lt;/a> setting&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242434&lt;/td>
&lt;td>Kubernetes Kubelet must enable kernel protection&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>Enabled for Kubernetes v1.26 or later unless disabled explicitly via &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#kubeletconfig">protectKernalDefaults&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>245541&lt;/td>
&lt;td>Kubernetes Kubelet must not disable timeouts&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>Enabled for Kubernetes v1.26 or later unless disabled explicitly via &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#kubeletconfig">streamingConnectionIdleTimeout&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="audit-configuration">Audit Configuration&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>ID&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Secure By Default&lt;/th>
&lt;th>Comments&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>242402&lt;/td>
&lt;td>The Kubernetes API Server must have an audit log path set&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>It is the user&amp;rsquo;s responsibility to configure an audit extension that meets the requirements of their organization. Depending on the audit extension implementation the audit logs do not always need to be written on the filesystem, i.e. when &lt;code>--audit-webhook-config-file&lt;/code> is set and logs are sent to an audit backend.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242403&lt;/td>
&lt;td>Kubernetes API Server must generate audit records that identify what type of event has occurred, identify the source of the event, contain the event results, identify any users, and identify any containers associated with the event&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>Users should set an audit policy that meets the requirements of their organization. Please consult the &lt;a href="https://gardener.cloud/docs/gardener/shoot_auditpolicy/">Shoot Audit Policy documentation&lt;/a>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242461&lt;/td>
&lt;td>Kubernetes API Server audit logs must be enabled&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>Users should set an audit policy that meets the requirements of their organization. Please consult the &lt;a href="https://gardener.cloud/docs/gardener/shoot_auditpolicy/">Shoot Audit Policy documentation&lt;/a>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242462&lt;/td>
&lt;td>The Kubernetes API Server must be set to audit log max size&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>It is the user&amp;rsquo;s responsibility to configure an audit extension that meets the requirements of their organization. Depending on the audit extension implementation the audit logs do not always need to be written on the filesystem, i.e. when &lt;code>--audit-webhook-config-file&lt;/code> is set and logs are sent to an audit backend.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242463&lt;/td>
&lt;td>The Kubernetes API Server must be set to audit log maximum backup&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>It is the user&amp;rsquo;s responsibility to configure an audit extension that meets the requirements of their organization. Depending on the audit extension implementation the audit logs do not always need to be written on the filesystem, i.e. when &lt;code>--audit-webhook-config-file&lt;/code> is set and logs are sent to an audit backend.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242464&lt;/td>
&lt;td>The Kubernetes API Server audit log retention must be set&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>It is the user&amp;rsquo;s responsibility to configure an audit extension that meets the requirements of their organization. Depending on the audit extension implementation the audit logs do not always need to be written on the filesystem, i.e. when &lt;code>--audit-webhook-config-file&lt;/code> is set and logs are sent to an audit backend.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242465&lt;/td>
&lt;td>The Kubernetes API Server audit log path must be set&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>It is the user&amp;rsquo;s responsibility to configure an audit extension that meets the requirements of their organization. Depending on the audit extension implementation the audit logs do not always need to be written on the filesystem, i.e. when &lt;code>--audit-webhook-config-file&lt;/code> is set and logs are sent to an audit backend.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="end-user-workload">End User Workload&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>ID&lt;/th>
&lt;th>Description&lt;/th>
&lt;th>Secure By Default&lt;/th>
&lt;th>Comments&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>242395&lt;/td>
&lt;td>Kubernetes dashboard must not be enabled&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>Not installed unless you install it via &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#addon">kubernetesDashboard&lt;/a>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242414&lt;/td>
&lt;td>Kubernetes cluster must use non-privileged host ports for user pods&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>Do not use any ports below 1024 for your own workload.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242415&lt;/td>
&lt;td>Secrets in Kubernetes must not be stored as environment variables&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>Always mount secrets as volumes and never as environment variables.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242383&lt;/td>
&lt;td>User-managed resources must be created in dedicated namespaces&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>Create and use your own/dedicated namespaces and never place anything into the default, kube-system, kube-public, or kube-node-lease namespace. The default namespace is never to be used while the other above listed namespaces are only to be used by the Kubernetes provider (here Gardener).&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242417&lt;/td>
&lt;td>Kubernetes must separate user functionality&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>While 242383 is about all resources, this rule is specifically about pods. Create and use your own/dedicated namespaces and never place pods into the default, kube-system, kube-public, or kube-node-lease namespace. The default namespace is never to be used while the other above listed namespaces are only to be used by the Kubernetes provider (here Gardener).&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242437&lt;/td>
&lt;td>Kubernetes must have a pod security policy set&lt;/td>
&lt;td>✅&lt;/td>
&lt;td>Set, but Gardener can only set default pod security policies (PSP) and does so only until v1.24 as with v1.25 PSPs were removed (deprecated since v1.21) and replaced with &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards">Pod Security Standards&lt;/a> (see &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future">this blog&lt;/a> for more information). Whatever the technology, you are responsible to configure custom-tailured appropriate PSPs respectively use them or PSSs, depending on your own workload and security needs (only you know what a pod should be allowed to do).&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242442&lt;/td>
&lt;td>Kubernetes must remove old components after updated versions have been installed&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>While Gardener manages all its components in its system namespaces (automated), you are naturally responsible for your own workload.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>254800&lt;/td>
&lt;td>Kubernetes must have a Pod Security Admission control file configured&lt;/td>
&lt;td>❌&lt;/td>
&lt;td>Gardener ensures that the pod security configuration allows system components to be deployed in the kube-system namespace but does not set configurations that can affect user namespaces. It is recommended that users enforce a minimum of &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/#pod-security-levels">baseline pod security level for their workload&lt;/a> via &lt;a href="https://gardener.cloud/docs/gardener/pod-security/#admission-configuration-for-the-podsecurity-admission-plugin">PodSecurity admission plugin&lt;/a>.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="rules-relevant-for-service-providers">Rules Relevant for Service Providers&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>ID&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>242376&lt;/td>
&lt;td>The Kubernetes Controller Manager must use TLS 1.2, at a minimum, to protect the confidentiality of sensitive data during electronic dissemination.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242377&lt;/td>
&lt;td>The Kubernetes Scheduler must use TLS 1.2, at a minimum, to protect the confidentiality of sensitive data during electronic dissemination.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242378&lt;/td>
&lt;td>The Kubernetes API Server must use TLS 1.2, at a minimum, to protect the confidentiality of sensitive data during electronic dissemination.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242379&lt;/td>
&lt;td>The Kubernetes etcd must use TLS to protect the confidentiality of sensitive data during electronic dissemination.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242380&lt;/td>
&lt;td>The Kubernetes etcd must use TLS to protect the confidentiality of sensitive data during electronic dissemination.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242381&lt;/td>
&lt;td>The Kubernetes Controller Manager must create unique service accounts for each work payload.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242382&lt;/td>
&lt;td>The Kubernetes API Server must enable Node,RBAC as the authorization mode.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242384&lt;/td>
&lt;td>The Kubernetes Scheduler must have secure binding.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242385&lt;/td>
&lt;td>The Kubernetes Controller Manager must have secure binding.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242386&lt;/td>
&lt;td>The Kubernetes API server must have the insecure port flag disabled.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242387&lt;/td>
&lt;td>The Kubernetes Kubelet must have the &amp;ldquo;readOnlyPort&amp;rdquo; flag disabled.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242388&lt;/td>
&lt;td>The Kubernetes API server must have the insecure bind address not set.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242389&lt;/td>
&lt;td>The Kubernetes API server must have the secure port set.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242391&lt;/td>
&lt;td>The Kubernetes Kubelet must have anonymous authentication disabled.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242392&lt;/td>
&lt;td>The Kubernetes kubelet must enable explicit authorization.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242396&lt;/td>
&lt;td>Kubernetes Kubectl cp command must give expected access and results.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242397&lt;/td>
&lt;td>The Kubernetes kubelet staticPodPath must not enable static pods.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242398&lt;/td>
&lt;td>Kubernetes DynamicAuditing must not be enabled.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242399&lt;/td>
&lt;td>Kubernetes DynamicKubeletConfig must not be enabled.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242404&lt;/td>
&lt;td>Kubernetes Kubelet must deny hostname override.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242405&lt;/td>
&lt;td>The Kubernetes manifests must be owned by root.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242406&lt;/td>
&lt;td>The Kubernetes KubeletConfiguration file must be owned by root.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242407&lt;/td>
&lt;td>The Kubernetes KubeletConfiguration files must have file permissions set to 644 or more restrictive.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242408&lt;/td>
&lt;td>The Kubernetes manifest files must have least privileges.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242409&lt;/td>
&lt;td>Kubernetes Controller Manager must disable profiling.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242410&lt;/td>
&lt;td>The Kubernetes API Server must enforce ports, protocols, and services (PPS) that adhere to the Ports, Protocols, and Services Management Category Assurance List (PPSM CAL).&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242411&lt;/td>
&lt;td>The Kubernetes Scheduler must enforce ports, protocols, and services (PPS) that adhere to the Ports, Protocols, and Services Management Category Assurance List (PPSM CAL).&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242412&lt;/td>
&lt;td>The Kubernetes Controllers must enforce ports, protocols, and services (PPS) that adhere to the Ports, Protocols, and Services Management Category Assurance List (PPSM CAL).&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242413&lt;/td>
&lt;td>The Kubernetes etcd must enforce ports, protocols, and services (PPS) that adhere to the Ports, Protocols, and Services Management Category Assurance List (PPSM CAL).&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242418&lt;/td>
&lt;td>The Kubernetes API server must use approved cipher suites.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242419&lt;/td>
&lt;td>Kubernetes API Server must have the SSL Certificate Authority set.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242420&lt;/td>
&lt;td>Kubernetes Kubelet must have the SSL Certificate Authority set.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242421&lt;/td>
&lt;td>Kubernetes Controller Manager must have the SSL Certificate Authority set.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242422&lt;/td>
&lt;td>Kubernetes API Server must have a certificate for communication.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242423&lt;/td>
&lt;td>Kubernetes etcd must enable client authentication to secure service.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242424&lt;/td>
&lt;td>Kubernetes Kubelet must enable tlsPrivateKeyFile for client authentication to secure service.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242425&lt;/td>
&lt;td>Kubernetes Kubelet must enable tlsCertFile for client authentication to secure service.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242426&lt;/td>
&lt;td>Kubernetes etcd must enable client authentication to secure service.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242427&lt;/td>
&lt;td>Kubernetes etcd must have a key file for secure communication.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242428&lt;/td>
&lt;td>Kubernetes etcd must have a certificate for communication.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242429&lt;/td>
&lt;td>Kubernetes etcd must have the SSL Certificate Authority set.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242430&lt;/td>
&lt;td>Kubernetes etcd must have a certificate for communication.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242431&lt;/td>
&lt;td>Kubernetes etcd must have a key file for secure communication.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242432&lt;/td>
&lt;td>Kubernetes etcd must have peer-cert-file set for secure communication.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242433&lt;/td>
&lt;td>Kubernetes etcd must have a peer-key-file set for secure communication.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242438&lt;/td>
&lt;td>Kubernetes API Server must configure timeouts to limit attack surface.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242443&lt;/td>
&lt;td>Kubernetes must contain the latest updates as authorized by IAVMs, CTOs, DTMs, and STIGs.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242444&lt;/td>
&lt;td>The Kubernetes component manifests must be owned by root.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242445&lt;/td>
&lt;td>The Kubernetes component etcd must be owned by etcd.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242446&lt;/td>
&lt;td>The Kubernetes conf files must be owned by root.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242447&lt;/td>
&lt;td>The Kubernetes Kube Proxy must have file permissions set to 644 or more restrictive.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242448&lt;/td>
&lt;td>The Kubernetes Kube Proxy must be owned by root.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242449&lt;/td>
&lt;td>The Kubernetes Kubelet certificate authority file must have file permissions set to 644 or more restrictive.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242450&lt;/td>
&lt;td>The Kubernetes Kubelet certificate authority must be owned by root.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242451&lt;/td>
&lt;td>The Kubernetes component PKI must be owned by root.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242452&lt;/td>
&lt;td>The Kubernetes kubelet KubeConfig must have file permissions set to 644 or more restrictive.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242453&lt;/td>
&lt;td>The Kubernetes kubelet KubeConfig file must be owned by root.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242454&lt;/td>
&lt;td>The Kubernetes kubeadm.conf must be owned by root.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242455&lt;/td>
&lt;td>The Kubernetes kubeadm.conf must have file permissions set to 644 or more restrictive.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242456&lt;/td>
&lt;td>The Kubernetes kubelet config must have file permissions set to 644 or more restrictive.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242457&lt;/td>
&lt;td>The Kubernetes kubelet config must be owned by root.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242459&lt;/td>
&lt;td>The Kubernetes etcd must have file permissions set to 644 or more restrictive.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242460&lt;/td>
&lt;td>The Kubernetes admin.conf must have file permissions set to 644 or more restrictive.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242466&lt;/td>
&lt;td>The Kubernetes PKI CRT must have file permissions set to 644 or more restrictive.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>242467&lt;/td>
&lt;td>The Kubernetes PKI keys must have file permissions set to 600 or more restrictive.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>245542&lt;/td>
&lt;td>Kubernetes API Server must disable basic authentication to protect information in transit.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>245544&lt;/td>
&lt;td>Kubernetes endpoints must use approved organizational certificate and key pair to protect information in transit.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>254801&lt;/td>
&lt;td>Kubernetes must enable PodSecurity admission controller on static pods and Kubelets.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>Docs: Curated Links</title><link>https://gardener.cloud/curated-links/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/curated-links/</guid><description>
&lt;p>A curated list of awesome Kubernetes sources.
Inspired by &lt;a href="https://github.com/sindresorhus/awesome">@sindresorhus&amp;rsquo; awesome&lt;/a>&lt;/p>
&lt;h3 id="setup">Setup&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://docs.docker.com/docker-for-mac/">Install Docker for Mac&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.docker.com/docker-for-windows/install/">Install Docker for Windows&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">Run a Kubernetes Cluster on your local machine&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="a-place-that-marks-the-beginning-of-a-journey">A Place That Marks the Beginning of a Journey&lt;/h3>
&lt;ul>
&lt;li>Read the &lt;a href="https://kubernetes.io/docs/home">kubernetes.io&lt;/a> documentation&lt;/li>
&lt;li>Take an online &lt;a href="https://www.udemy.com/course/learn-kubernetes">Udemy course&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/presentation/d/1JqcALpsg07eH665ZXQrIvOcin6SzzsIUjMRRVivrZMg/edit?usp=sharing">Kubernetes Community Overview and Contributions Guide&lt;/a> by &lt;a href="https://twitter.com/idvoretskyi/">Ihor Dvoretskyi&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/meteorhacks/meteorhacks.github.io/blob/master/_posts/2015-04-22-learn-kubernetes-the-future-of-the-cloud.md">Kubernetes: The Future of Cloud Hosting&lt;/a> by &lt;a href="https://twitter.com/meteorhacks">Meteorhacks&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://thevirtualizationguy.wordpress.com/tag/kubernetes/">Kubernetes by Google&lt;/a> by &lt;a href="https://twitter.com/GastonPantana">Gaston Pantana&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://keithtenzer.com/containers/application-containers-kubernetes-and-docker-from-scratch/">Application Containers: Kubernetes and Docker from Scratch&lt;/a> by &lt;a href="https://twitter.com/keithtenzer">Keith Tenzer&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://omerio.com/2015/12/18/learn-the-kubernetes-key-concepts-in-10-minutes/">Learn the Kubernetes Key Concepts in 10 Minutes&lt;/a> by &lt;a href="https://twitter.com/omerio">Omer Dawelbeit&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://deis.com/blog/2016/kubernetes-illustrated-guide/">The Children&amp;rsquo;s Illustrated Guide to Kubernetes&lt;/a> by &lt;a href="https://github.com/deis">Deis&lt;/a> :-)&lt;/li>
&lt;li>&lt;a href="https://github.com/xiaopeng163/docker-k8s-lab">Docker Kubernetes Lab Handbook&lt;/a> by &lt;a href="https://twitter.com/xiaopeng163">Peng Xiao&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="interactive-learning-environments">Interactive Learning Environments&lt;/h2>
&lt;p>&lt;em>Learn Kubernetes using an interactive environment without requiring downloads or configuration&lt;/em>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/">Interactive Kubernetes Tutorials&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.oreilly.com/playlists/330a1112-13ee-4e72-8b2a-6fd8766fddae/">Kubernetes: From Basics to Guru&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://kubernetesbootcamp.github.io/kubernetes-bootcamp/">Kubernetes Bootcamp&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="massive-open-online-courses--tutorials">Massive Open Online Courses / Tutorials&lt;/h2>
&lt;p>&lt;em>List of available free online courses(&lt;a href="https://en.wikipedia.org/wiki/Massive_open_online_course">MOOC&lt;/a>) and tutorials&lt;/em>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://devopswithkubernetes.com/">DevOps with Kubernetes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.my-mooc.com/en/mooc/introduction-to-kubernetes/">Introduction to Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="courses">Courses&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://www.udacity.com/course/scalable-microservices-with-kubernetes--ud615">Scalable Microservices with Kubernetes at Udacity&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.edx.org/course/introduction-kubernetes-linuxfoundationx-lfs158x">Introduction to Kubernetes at edX&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="tutorials">Tutorials&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="http://kubernetes.io/docs/tutorials/">Kubernetes Tutorials by Kubernetes Team&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubebyexample.com/">Kubernetes By Example by OpenShift Team&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.tutorialspoint.com/kubernetes/">Kubernetes Tutorial by Tutorialspoint&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="package-managers">Package Managers&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="http://helm.sh">Helm&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/coreos/kpm">KPM&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="rpc">RPC&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="http://grpc.io">gRPC&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="rbac">RBAC&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://rad.security/blog/what-is-kubernetes-rbac?utm_source=tldrinfosec">Kubernetes RBAC: Role-Based Access Control&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="secret-generation-and-management">Secret Generation and Management&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="http://www.vaultproject.io/docs/auth/kubernetes.html">Vault auth plugin backend: Kubernetes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kelseyhightower/vault-controller">Vault controller&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/jetstack/kube-lego">kube-lego&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/dtan4/k8sec">k8sec&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Boostport/kubernetes-vault">kubernetes-vault&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/shyiko/kubesec">kubesec&lt;/a> - Secure Secret management&lt;/li>
&lt;/ul>
&lt;h2 id="machine-learning">Machine Learning&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/tensorflow/k8s">TensorFlow k8s&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/deepinsight/mxnet-operator">mxnet-operator&lt;/a> - Tools for ML/MXNet on Kubernetes.&lt;/li>
&lt;li>&lt;a href="https://github.com/google/kubeflow">kubeflow&lt;/a> - Machine Learning Toolkit for Kubernetes.&lt;/li>
&lt;li>&lt;a href="https://github.com/SeldonIO/seldon-core">seldon-core&lt;/a> - Open source framework for deploying machine learning models on Kubernetes&lt;/li>
&lt;/ul>
&lt;h2 id="raspberry-pi">Raspberry Pi&lt;/h2>
&lt;p>&lt;em>Some of the awesome findings and experiments on using Kubernetes with Raspberry Pi.&lt;/em>&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://kubecloud.io">Kubecloud&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubecloud.io/setting-up-a-kubernetes-on-arm-cluster-on-raspberry-pis-f7f64065138c">Setting up a Kubernetes on ARM cluster&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://blog.hypriot.com/post/setup-kubernetes-raspberry-pi-cluster/">Setup Kubernetes on a Raspberry Pi Cluster easily the official way!&lt;/a> by &lt;a href="https://blog.hypriot.com/crew/">Mathias Renner and Lucas Käldström&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.hanselman.com/blog/HowToBuildAKubernetesClusterWithARMRaspberryPiThenRunNETCoreOnOpenFaas.aspx">How to Build a Kubernetes Cluster with ARM Raspberry Pi then run .NET Core on OpenFaas&lt;/a> by &lt;a href="https://twitter.com/shanselman">Scott Hanselman&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="contributing">Contributing&lt;/h2>
&lt;p>Contributions are most welcome!&lt;/p>
&lt;p>This list is just getting started, please contribute to make it super awesome.&lt;/p></description></item><item><title>Docs: Gardener Teaser</title><link>https://gardener.cloud/docs/resources/videos/gardener-teaser/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/gardener-teaser/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/YI-RyfdQNhw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="Gardener Teaser">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Working with Images</title><link>https://gardener.cloud/docs/contribute/documentation/images/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/contribute/documentation/images/</guid><description>
&lt;p>Using images on the website has to contribute to the aesthetics and comprehensibility of the materials, with uncompromised experience when loading and browsing pages. That concerns crisp clear images, their consistent layout and color scheme, dimensions and aspect ratios, flicker-free and fast loading or the feeling of it, even on unreliable mobile networks and devices.&lt;/p>
&lt;h2 id="image-production-guidelines">Image Production Guidelines&lt;/h2>
&lt;p>A good, detailed reference for optimal use of images for the web can be found at web.dev&amp;rsquo;s &lt;a href="https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/image-optimization?hl=en">Fast Load Times&lt;/a> topic. The following summarizes some key points plus suggestions for tools support.&lt;/p>
&lt;p>You are strongly encouraged to use &lt;strong>vector images&lt;/strong> (SVG) as much as possible. They scale seamlessly without compromising the quality and are easier to maintain.&lt;/p>
&lt;p>If you are just now starting with SVG authoring, here are some tools suggestions: &lt;a href="https://www.figma.com/">Figma&lt;/a> (online/Win/Mac), &lt;a href="https://www.sketch.com/">Sketch&lt;/a> (Mac only).&lt;/p>
&lt;p>For &lt;strong>raster images&lt;/strong> (JPG, PNG, GIF), consider the following requirements and choose a tool that enables you to conform to them:&lt;/p>
&lt;ul>
&lt;li>Be mindful about image size, the total page size and loading times.&lt;/li>
&lt;li>Larger images (&amp;gt;10K) need to support &lt;em>progressive rendering&lt;/em>. Consult with your favorite authoring tool&amp;rsquo;s documentation to find out if and how it supports that.&lt;/li>
&lt;li>The site delivers the optimal media content format and size depending on the device screen size. You need to provide several variants (large screen, laptop, tablet, phone). Your authoring tool should be able to resize and resample images. Always save the largest size first and then downscale from it to avoid image quality loss.&lt;/li>
&lt;/ul>
&lt;p>If you are looking for a tool that conforms to those guidelines, &lt;a href="https://www.irfanview.com/">IrfanView&lt;/a> is a very good option.&lt;/p>
&lt;p>&lt;strong>Screenshots&lt;/strong> can be taken with whatever tool you have available. A simple Alt+PrtSc (Win) and paste into an image processing tool to save it does the job. If you need to add emphasized steps (1,2,3) when you describe a process on a screeshot, you can use &lt;a href="https://www.techsmith.com/screen-capture.html">Snaggit&lt;/a>. Use red color and numbers. Mind the requirements for raster images laid out above.&lt;/p>
&lt;p>&lt;strong>Diagrams&lt;/strong> can be exported as PNG/JPG from a diagraming tool such as Visio or even PowerPoint. Pick whichever you are comfortable with to design the diagram and make sure you comply with the requirements for the raster images production above. Diagrams produced as SVG are welcome too if your authoring tool supports exporting in that format. In any case, ensure that your diagrams &amp;ldquo;blend&amp;rdquo; with the content on the site - use the same color scheme and geometry style. Do not complicate diagrams too much. The site also supports &lt;a href="https://mermaid-js.github.io/mermaid/#/">Mermaid&lt;/a> diagrams produced with markdown and rendered as SVG. You don&amp;rsquo;t need special tools for them, but for more complex ones you might want to prototype your diagram wth Mermaid&amp;rsquo;s &lt;a href="https://mermaidjs.github.io/mermaid-live-editor">online live editor&lt;/a>, before encoding it in your markdown. More tips on using Mermaid can be found in the &lt;a href="https://gardener.cloud/docs/contribute/documentation/shortcodes/#mermaid">Shortcodes&lt;/a> documentation.&lt;/p>
&lt;h2 id="using-images-in-markdown">Using Images in Markdown&lt;/h2>
&lt;p>The standard for adding images to a topic is to use markdown&amp;rsquo;s &lt;code>![caption](image-path)&lt;/code>. If the image is not showing properly, or if you wish to serve images close to their natural size and avoid scaling, then you can use HTML5&amp;rsquo;s &lt;code>&amp;lt;picture&amp;gt;&lt;/code> tag.&lt;/p>
&lt;p>Example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-html" data-lang="html">&lt;span style="display:flex;">&lt;span>&amp;lt;picture&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">&amp;lt;!-- default, laptop-width-L max 1200px --&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;source srcset=&lt;span style="color:#a31515">&amp;#34;https://github.tools.sap/kubernetes/documentation/tree/master/website/documentation/015-tutorials/my-guide/images/overview-XL.png&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> media=&lt;span style="color:#a31515">&amp;#34;(min-width: 1000px)&amp;#34;&lt;/span>&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">&amp;lt;!-- default, laptop-width max 1000px --&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;source srcset=&lt;span style="color:#a31515">&amp;#34;https://github.tools.sap/kubernetes/documentation/tree/master/website/documentation/015-tutorials/my-guide/images/overview-L.png&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> media=&lt;span style="color:#a31515">&amp;#34;(min-width: 1400px)&amp;#34;&lt;/span>&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">&amp;lt;!-- default, tablets-width max 750px --&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;source srcset=&lt;span style="color:#a31515">&amp;#34;https://github.tools.sap/kubernetes/documentation/tree/master/website/documentation/015-tutorials/my-guide/images/overview-M.png&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> media=&lt;span style="color:#a31515">&amp;#34;(min-width: 750px)&amp;#34;&lt;/span>&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">&amp;lt;!-- default, phones-width max 450px --&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;lt;img src=&lt;span style="color:#a31515">&amp;#34;https://github.tools.sap/kubernetes/documentation/tree/master/website/documentation/015-tutorials/my-guide/images/overview.png&amp;#34;&lt;/span> /&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&amp;lt;/picture&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When deciding on image sizes, consider the breakpoints in the example above as maximum widths for each image variant you provide. Note that the site is designed for maximum width 1200px. There is no point to create images larger than that, since they will be scaled down.&lt;/p>
&lt;p>For a nice overview on making the best use of responsive images with HTML5, please refer to the &lt;a href="https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images">Responsive Images&lt;/a> guide.&lt;/p></description></item><item><title>Docs: Run DISA K8s STIGs Ruleset</title><link>https://gardener.cloud/docs/security-and-compliance/partial-disa-k8s-stig-shoot/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/security-and-compliance/partial-disa-k8s-stig-shoot/</guid><description>
&lt;h2 id="run-partial-disa-k8s-stigs-ruleset-against-a-gardener-shoot-cluster">Run Partial DISA K8s STIGs Ruleset Against a Gardener Shoot Cluster&lt;/h2>
&lt;h3 id="introduction">Introduction&lt;/h3>
&lt;p>This part shows how to run the DISA K8s STIGs ruleset against a Gardener shoot cluster. The guide features the &lt;code>managedk8s&lt;/code> provider which does not implement all of the DISA K8s STIG rules since it assumes that the user running the ruleset does not have access to the environment (the seed in this particular case) in which the control plane components reside.&lt;/p>
&lt;h3 id="prerequisites">Prerequisites&lt;/h3>
&lt;p>Make sure you have &lt;a href="https://github.com/gardener/diki/blob/main/README.md#Installation">diki installed&lt;/a> and have a running Gardener shoot cluster.&lt;/p>
&lt;h3 id="configuration">Configuration&lt;/h3>
&lt;p>We will be using the sample &lt;a href="https://github.com/gardener/diki/blob/main/example/guides/partial-disa-k8s-stig-shoot.yaml">Partial DISA K8s STIG for Shoots configuration file&lt;/a> for this run. You will need to set the &lt;code>provider.args.kubeconfigPath&lt;/code> field pointing to a shoot admin kubeconfig.&lt;/p>
&lt;p>In case you need instructions on how to generate such a kubeconfig, please read &lt;a href="https://gardener.cloud/docs/gardener/shoot_access/">Accessing Shoot Clusters&lt;/a>.&lt;/p>
&lt;p>Additional metadata such as the shoot&amp;rsquo;s name can also be included in the &lt;code>provider.metadata&lt;/code> section. The metadata section can be used to add additional context to different diki runs.&lt;/p>
&lt;p>The provided configuration contains the recommended rule options for running the &lt;code>managedk8s&lt;/code> provider ruleset against a shoot cluster, but you can modify rule options parameters according to requirements. All available options can be found in the &lt;a href="https://github.com/gardener/diki/blob/main/example/config/managedk8s.yaml">managedk8s example configuration&lt;/a>.&lt;/p>
&lt;h3 id="running-the-disa-k8s-stigs-ruleset">Running the DISA K8s STIGs Ruleset&lt;/h3>
&lt;p>To run diki against a Gardener shoot cluster, run the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>diki run &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --config=./example/guides/partial-disa-k8s-stig-shoot.yaml &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --provider=managedk8s &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --ruleset-id=disa-kubernetes-stig &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --ruleset-version=v2r1 &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --output=disa-k8s-stigs-report.json
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="generating-a-report">Generating a Report&lt;/h3>
&lt;p>We can use the file generated in the previous step to create an html report by using the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>diki report generate &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --output=disa-k8s-stigs-report.html &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> disa-k8s-stigs-report.json
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: The Illustrated Guide to Kubernetes</title><link>https://gardener.cloud/docs/resources/videos/fairy-tail/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/fairy-tail/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/4ht22ReBjno" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="The Illustrated Guide to Kubernetes">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Gardener Compliance Report</title><link>https://gardener.cloud/docs/security-and-compliance/report/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/security-and-compliance/report/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Gardener aims to comply with public security standards and guidelines, such as the &lt;a href="https://public.cyber.mil/stigs/">Security Technical Implementation Guide (STIG) for Kubernetes from Defense Information Systems Agency (DISA)&lt;/a>. The DISA Kubernetes STIG is a set of rules that provide recommendations for secure deployment and operation of Kubernetes. It covers various aspects of Kubernetes security, including the configurations of the Kubernetes API server and other components, cluster management, certificate management, handling of updates and patches.&lt;/p>
&lt;p>While Gardener aims to follow this guideline, we also recognize that not all of the rules may be directly applicable or optimal for Gardener specific environment. Therefore, some of the requirements are adjusted. Rules that are not applicable to Gardener are skipped given an appropriate justification.&lt;/p>
&lt;p>For every release, we check that Gardener is able of creating security hardened shoot clusters, reconfirming that the configurations which are not secure by default (as per &lt;a href="https://gardener.cloud/docs/guides/security-and-compliance/kubernetes-hardening/">Gardener Kubernetes Cluster Hardening Procedure&lt;/a>) are still possible and work as expected.&lt;/p>
&lt;p>In order to automate and ease this process, Gardener uses a tool called &lt;a href="https://github.com/gardener/diki">diki&lt;/a>.&lt;/p>
&lt;h2 id="security-hardened-shoot-configurations">Security Hardened Shoot Configurations&lt;/h2>
&lt;p>The following security hardened shoot configurations were used in order to generate the compliance report.&lt;/p>
&lt;details>
&lt;summary>AWS&lt;/summary>
&lt;pre>&lt;code>
kind: Shoot
apiVersion: core.gardener.cloud/v1beta1
metadata:
name: aws
spec:
cloudProfileName: aws
kubernetes:
kubeAPIServer:
admissionPlugins:
- name: PodSecurity
config:
apiVersion: pod-security.admission.config.k8s.io/v1beta1
kind: PodSecurityConfiguration
defaults:
enforce: baseline
audit: baseline
warn: baseline
disabled: false
auditConfig:
auditPolicy:
configMapRef:
name: audit-policy
version: "1.28"
enableStaticTokenKubeconfig: false
networking:
type: calico
pods: 100.64.0.0/12
nodes: 10.180.0.0/16
services: 100.104.0.0/13
ipFamilies:
- IPv4
provider:
type: aws
controlPlaneConfig:
apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
kind: ControlPlaneConfig
infrastructureConfig:
apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
kind: InfrastructureConfig
networks:
vpc:
cidr: 10.180.0.0/16
zones:
- internal: 10.180.48.0/20
name: eu-west-1c
public: 10.180.32.0/20
workers: 10.180.0.0/19
workers:
- cri:
name: containerd
name: worker-kkfk1
machine:
type: m5.large
image:
name: gardenlinux
architecture: amd64
maximum: 2
minimum: 2
maxSurge: 1
maxUnavailable: 0
volume:
type: gp3
size: 50Gi
zones:
- eu-west-1c
workersSettings:
sshAccess:
enabled: false
purpose: evaluation
region: eu-west-1
secretBindingName: secretBindingName
&lt;/code>&lt;/pre>
&lt;/details>
&lt;details>
&lt;summary>Azure&lt;/summary>
&lt;pre>&lt;code>
kind: Shoot
apiVersion: core.gardener.cloud/v1beta1
metadata:
name: azure
spec:
cloudProfileName: az
kubernetes:
kubeAPIServer:
admissionPlugins:
- name: PodSecurity
config:
apiVersion: pod-security.admission.config.k8s.io/v1beta1
kind: PodSecurityConfiguration
defaults:
enforce: baseline
audit: baseline
warn: baseline
disabled: false
auditConfig:
auditPolicy:
configMapRef:
name: audit-policy
version: "1.28"
enableStaticTokenKubeconfig: false
networking:
type: calico
pods: 100.64.0.0/12
nodes: 10.180.0.0/16
services: 100.104.0.0/13
ipFamilies:
- IPv4
provider:
type: azure
controlPlaneConfig:
apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
kind: ControlPlaneConfig
infrastructureConfig:
apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
kind: InfrastructureConfig
networks:
vnet:
cidr: 10.180.0.0/16
workers: 10.180.0.0/16
zoned: true
workers:
- cri:
name: containerd
name: worker-g7p4p
machine:
type: Standard_A4_v2
image:
name: gardenlinux
architecture: amd64
maximum: 2
minimum: 2
maxSurge: 1
maxUnavailable: 0
volume:
type: StandardSSD_LRS
size: 50Gi
zones:
- '3'
workersSettings:
sshAccess:
enabled: false
purpose: evaluation
region: westeurope
secretBindingName: secretBindingName
&lt;/code>&lt;/pre>
&lt;/details>
&lt;details>
&lt;summary>GCP&lt;/summary>
&lt;pre>&lt;code>
kind: Shoot
apiVersion: core.gardener.cloud/v1beta1
metadata:
name: gcp
spec:
cloudProfileName: gcp
kubernetes:
kubeAPIServer:
admissionPlugins:
- name: PodSecurity
config:
apiVersion: pod-security.admission.config.k8s.io/v1beta1
kind: PodSecurityConfiguration
defaults:
enforce: baseline
audit: baseline
warn: baseline
disabled: false
auditConfig:
auditPolicy:
configMapRef:
name: audit-policy
version: "1.28"
enableStaticTokenKubeconfig: false
networking:
type: calico
pods: 100.64.0.0/12
nodes: 10.180.0.0/16
services: 100.104.0.0/13
ipFamilies:
- IPv4
provider:
type: gcp
controlPlaneConfig:
apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
kind: ControlPlaneConfig
zone: europe-west1-b
infrastructureConfig:
apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
kind: InfrastructureConfig
networks:
workers: 10.180.0.0/16
workers:
- cri:
name: containerd
name: worker-bex82
machine:
type: n1-standard-2
image:
name: gardenlinux
architecture: amd64
maximum: 2
minimum: 2
maxSurge: 1
maxUnavailable: 0
volume:
type: pd-balanced
size: 50Gi
zones:
- europe-west1-b
workersSettings:
sshAccess:
enabled: false
purpose: evaluation
region: europe-west1
secretBindingName: secretBindingName
&lt;/code>&lt;/pre>
&lt;/details>
&lt;details>
&lt;summary>OpenStack&lt;/summary>
&lt;pre>&lt;code>
kind: Shoot
apiVersion: core.gardener.cloud/v1beta1
metadata:
name: openstack
spec:
cloudProfileName: converged-cloud-cp
kubernetes:
kubeAPIServer:
admissionPlugins:
- name: PodSecurity
config:
apiVersion: pod-security.admission.config.k8s.io/v1beta1
kind: PodSecurityConfiguration
defaults:
enforce: baseline
audit: baseline
warn: baseline
disabled: false
auditConfig:
auditPolicy:
configMapRef:
name: audit-policy
version: "1.28"
enableStaticTokenKubeconfig: false
networking:
type: calico
pods: 100.64.0.0/12
nodes: 10.180.0.0/16
services: 100.104.0.0/13
ipFamilies:
- IPv4
provider:
type: openstack
controlPlaneConfig:
apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
kind: ControlPlaneConfig
loadBalancerProvider: f5
infrastructureConfig:
apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
kind: InfrastructureConfig
networks:
workers: 10.180.0.0/16
floatingPoolName: FloatingIP-external-cp
workers:
- cri:
name: containerd
name: worker-dqty2
machine:
type: g_c2_m4
image:
name: gardenlinux
architecture: amd64
maximum: 2
minimum: 2
maxSurge: 1
maxUnavailable: 0
zones:
- eu-de-1b
workersSettings:
sshAccess:
enabled: false
purpose: evaluation
region: eu-de-1
secretBindingName: secretBindingName
&lt;/code>&lt;/pre>
&lt;/details>
&lt;h2 id="diki-configuration">Diki Configuration&lt;/h2>
&lt;p>The following &lt;a href="https://github.com/gardener/diki">diki&lt;/a> configuration was used in order to test each of the shoot clusters described above. Mind that the rules regarding audit logging are skipped because organizations have different requirements and Gardener can integrate with different audit logging solutions.&lt;/p>
&lt;details>
&lt;summary>Configuration&lt;/summary>
&lt;pre>&lt;code>
metadata: ...
providers:
- id: gardener
name: Gardener
metadata: ...
args: ...
rulesets:
- id: disa-kubernetes-stig
name: DISA Kubernetes Security Technical Implementation Guide
version: v1r11
ruleOptions:
- ruleID: "242402"
skip:
enabled: true
justification: "Gardener can integrate with different audit logging solutions"
- ruleID: "242403"
skip:
enabled: true
justification: "Gardener can integrate with different audit logging solutions"
- ruleID: "242414"
args:
acceptedPods:
- podMatchLabels:
k8s-app: node-local-dns
namespaceMatchLabels:
kubernetes.io/metadata.name: kube-system
justification: "node local dns requires port 53 in order to operate properly"
ports:
- 53
- ruleID: "242445"
args:
expectedFileOwner:
users: ["0", "65532"]
groups: ["0", "65532"]
- ruleID: "242446"
args:
expectedFileOwner:
users: ["0", "65532"]
groups: ["0", "65532"]
- ruleID: "242451"
args:
expectedFileOwner:
users: ["0", "65532"]
groups: ["0", "65532"]
- ruleID: "242462"
skip:
enabled: true
justification: "Gardener can integrate with different audit logging solutions"
- ruleID: "242463"
skip:
enabled: true
justification: "Gardener can integrate with different audit logging solutions"
- ruleID: "242464"
skip:
enabled: true
justification: "Gardener can integrate with different audit logging solutions"
- ruleID: "245543"
args:
acceptedTokens:
- user: "health-check"
uid: "health-check"
- ruleID: "254800"
args:
minPodSecurityLevel: "baseline"
output:
minStatus: Passed
&lt;/code>&lt;/pre>
&lt;/details>
&lt;h2 id="security-compliance-report-for-hardened-shoot-clusters">Security Compliance Report for Hardened Shoot Clusters&lt;/h2>
&lt;p>The report can be reviewed directly or downloaded by &lt;a href="https://gardener.cloud/html/hardened_shoots_report.html" download="">clicking here&lt;/a>.&lt;/p>
&lt;html>
&lt;head>
&lt;meta charset="UTF-8">
&lt;meta name="viewport" content="width=device-width, initial-scale=1.0">
&lt;style>
*,:after,:before{border:0 solid #e5e7eb;box-sizing:border-box}:after,:before{--tw-content:""}html{-webkit-text-size-adjust:100%;font-feature-settings:normal;font-family:ui-sans-serif,system-ui,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;font-variation-settings:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4}body{line-height:inherit;margin:0}hr{border-top-width:1px;color:inherit;height:0}abbr:where([title]){-webkit-text-decoration:underline dotted;text-decoration:underline dotted}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}b,strong{font-weight:bolder}code,kbd,pre,samp{font-family:ui-monospace,SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:initial}sub{bottom:-.25em}sup{top:-.5em}table{border-collapse:collapse;border-color:inherit;text-indent:0}button,input,optgroup,select,textarea{font-feature-settings:inherit;color:inherit;font-family:inherit;font-size:100%;font-variation-settings:inherit;font-weight:inherit;line-height:inherit;margin:0;padding:0}button,select{text-transform:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button;background-color:initial;background-image:none}:-moz-focusring{outline:auto}:-moz-ui-invalid{box-shadow:none}progress{vertical-align:initial}::-webkit-inner-spin-button,::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}summary{display:list-item}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}fieldset{margin:0}fieldset,legend{padding:0}menu,ol,ul{list-style:none;margin:0;padding:0}dialog{padding:0}textarea{resize:vertical}input::-moz-placeholder,textarea::-moz-placeholder{color:#9ca3af;opacity:1}input::placeholder,textarea::placeholder{color:#9ca3af;opacity:1}[role=button],button{cursor:pointer}:disabled{cursor:default}img,video{height:auto;max-width:100%}[hidden]{display:none}*,::backdrop,:after,:before{--tw-border-spacing-x:0;--tw-border-spacing-y:0;--tw-translate-x:0;--tw-translate-y:0;--tw-rotate:0;--tw-skew-x:0;--tw-skew-y:0;--tw-scale-x:1;--tw-scale-y:1;--tw-pan-x: ;--tw-pan-y: ;--tw-pinch-zoom: ;--tw-scroll-snap-strictness:proximity;--tw-gradient-from-position: ;--tw-gradient-via-position: ;--tw-gradient-to-position: ;--tw-ordinal: ;--tw-slashed-zero: ;--tw-numeric-figure: ;--tw-numeric-spacing: ;--tw-numeric-fraction: ;--tw-ring-inset: ;--tw-ring-offset-width:0px;--tw-ring-offset-color:#fff;--tw-ring-color:#3b82f680;--tw-ring-offset-shadow:0 0 #0000;--tw-ring-shadow:0 0 #0000;--tw-shadow:0 0 #0000;--tw-shadow-colored:0 0 #0000;--tw-blur: ;--tw-brightness: ;--tw-contrast: ;--tw-grayscale: ;--tw-hue-rotate: ;--tw-invert: ;--tw-saturate: ;--tw-sepia: ;--tw-drop-shadow: ;--tw-backdrop-blur: ;--tw-backdrop-brightness: ;--tw-backdrop-contrast: ;--tw-backdrop-grayscale: ;--tw-backdrop-hue-rotate: ;--tw-backdrop-invert: ;--tw-backdrop-opacity: ;--tw-backdrop-saturate: ;--tw-backdrop-sepia: }.tw-absolute{position:absolute}.tw-relative{position:relative}.tw-right-3{right:.75rem}.tw-top-3{top:.75rem}.tw-flex{display:flex}.tw-hidden{display:none}.tw-list-inside{list-style-position:inside}.tw-list-disc{list-style-type:disc}.tw-list-none{list-style-type:none}.tw-flex-col{flex-direction:column}.tw-justify-center{justify-content:center}.tw-overflow-x-auto{overflow-x:auto}.tw-rounded{border-radius:.25rem}.tw-rounded-lg{border-radius:.5rem}.tw-bg-gray-200{--tw-bg-opacity:1;background-color:rgb(229 231 235/var(--tw-bg-opacity))}.tw-p-1{padding:.25rem}.tw-p-4{padding:1rem}.tw-px-6{padding-left:1.5rem;padding-right:1.5rem}.tw-pb-5{padding-bottom:1.25rem}.tw-pl-2{padding-left:.5rem}.tw-pl-5{padding-left:1.25rem}.tw-pr-2{padding-right:.5rem}.tw-pt-2{padding-top:.5rem}.tw-text-2xl{font-size:1.5rem;line-height:2rem}.tw-text-3xl{font-size:1.875rem;line-height:2.25rem}.tw-text-lg{font-size:1.125rem;line-height:1.75rem}.tw-text-xl{font-size:1.25rem;line-height:1.75rem}.tw-font-bold{font-weight:700}.tw-font-medium{font-weight:500}.tw-font-semibold{font-weight:600}.hover\:tw-bg-gray-100:hover{--tw-bg-opacity:1;background-color:rgb(243 244 246/var(--tw-bg-opacity))}
&lt;/style>
&lt;style>
.arrow {
border: solid black;
border-width: 0px 3px 3px 0px;
display: inline-block;
padding: 4px;
}
.right {
transform: rotate(-45deg);
-webkit-transform: rotate(-45deg);
}
.left {
transform: rotate(135deg);
-webkit-transform: rotate(135deg);
}
.up {
transform: rotate(-135deg);
-webkit-transform: rotate(-135deg);
}
.down {
transform: rotate(45deg);
-webkit-transform: rotate(45deg);
}
&lt;/style>
&lt;script>
function collapse(event) {
const parent = event.currentTarget.parentElement
const list = parent.getElementsByTagName('ul')[0]
const arrow = event.currentTarget.getElementsByTagName('i')[0]
if (list.classList.contains('tw-hidden') === true) {
list.classList.remove('tw-hidden')
arrow.classList.replace('right', 'down')
return
}
list.classList.add('tw-hidden')
arrow.classList.replace('down', 'right')
}
function cpCode(event) {
const parent = event.currentTarget.parentElement
const code = parent.getElementsByTagName('pre')[0].innerText
navigator.clipboard.writeText(code);
}
&lt;/script>
&lt;/head>
&lt;body>
&lt;div class="tw-flex-col">
&lt;h1 class="tw-text-3xl tw-font-bold tw-pb-5 tw-pt-2 tw-flex tw-justify-center">Compliance Run (07-03-2024)&lt;/h1>
&lt;div class="tw-content">
&lt;span class="tw-text-2xl">&lt;span class="tw-font-bold">Diki Version: &lt;/span>v0.9.0&lt;/span>&lt;br>
&lt;div>
&lt;label class="tw-font-bold tw-text-xl">Provider Gardener&lt;/label>&lt;br>
&lt;button onclick="collapse(event)" class="tw-text-lg tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-text-lg">Evaluated targets&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5 tw-hidden">
&lt;li>&lt;span class="tw-font-bold">aws&lt;/span> (gardenVirtualCloudProvider: gcp, gardenerVersion: v1.98.0, projectName: diki-comp, seedCloudProvider: aws, seedKubernetesVersion: v1.29.4, shootCloudProvider: aws, shootKubernetesVersion: v1.28.9, time: 07-03-2024 08:33:31)&lt;/li>
&lt;li>&lt;span class="tw-font-bold">azure&lt;/span> (gardenVirtualCloudProvider: gcp, gardenerVersion: v1.98.0, projectName: diki-comp, seedCloudProvider: azure, seedKubernetesVersion: v1.29.4, shootCloudProvider: azure, shootKubernetesVersion: v1.28.9, time: 07-03-2024 08:34:49)&lt;/li>
&lt;li>&lt;span class="tw-font-bold">gcp&lt;/span> (gardenVirtualCloudProvider: gcp, gardenerVersion: v1.98.0, projectName: diki-comp, seedCloudProvider: gcp, seedKubernetesVersion: v1.29.4, shootCloudProvider: gcp, shootKubernetesVersion: v1.28.9, time: 07-03-2024 08:36:55)&lt;/li>
&lt;li>&lt;span class="tw-font-bold">openstack&lt;/span> (gardenVirtualCloudProvider: gcp, gardenerVersion: v1.98.0, projectName: diki-comp, seedCloudProvider: openstack, seedKubernetesVersion: v1.29.4, shootCloudProvider: openstack, shootKubernetesVersion: v1.28.9, time: 07-03-2024 08:38:57)&lt;/li>
&lt;/ul>
&lt;ul class="tw-list-none tw-list-inside">
&lt;li>
&lt;span class="tw-text-lg">&lt;span class="tw-font-semibold">v1r11 DISA Kubernetes Security Technical Implementation Guide&lt;/span> (61x Passed 🟢, 24x Skipped 🔵, 7x Accepted 🔵, 6x Warning 🟠, 3x Failed 🔴)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-2">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-text-lg tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-text-lg">&amp;#128994 Passed&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes Controller Manager must use TLS 1.2, at a minimum, to protect the confidentiality of sensitive data during electronic dissemination (MEDIUM 242376)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option tls-min-version has not been set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes Scheduler must use TLS 1.2, at a minimum, to protect the confidentiality of sensitive data during electronic dissemination (MEDIUM 242377)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option tls-min-version has not been set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: deployment name: kube-scheduler namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: deployment name: kube-scheduler namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: deployment name: kube-scheduler namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: deployment name: kube-scheduler namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API Server must use TLS 1.2, at a minimum, to protect the confidentiality of sensitive data during electronic dissemination (MEDIUM 242378)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option tls-min-version has not been set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes etcd must use TLS to protect the confidentiality of sensitive data during electronic dissemination (MEDIUM 242379)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option client-transport-security.auto-tls set to allowed value.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes Controller Manager must create unique service accounts for each work payload(HIGH 242381)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option use-service-account-credentials set to allowed value.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API Server must enable Node,RBAC as the authorization mode (MEDIUM 242382)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option authorization-mode set to expected value.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes must separate user functionality (MEDIUM 242383)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">System resource in system namespaces.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: Service name: kubernetes namespace: default &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: Service name: kubernetes namespace: default &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: Service name: kubernetes namespace: default &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: Service name: kubernetes namespace: default &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API server must have the insecure port flag disabled (HIGH 242386)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option insecure-port not set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes Kubelet must have the &amp;#34;readOnlyPort&amp;#34; flag disabled (HIGH 242387)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option readOnlyPort not set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-rdpvb &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-klgpz &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-xzs4w &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API server must have the insecure bind address not set (HIGH 242388)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option insecure-bind-address not set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API server must have the secure port set (MEDIUM 242389)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option secure-port set to allowed value.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API server must have anonymous authentication disabled (HIGH 242390)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option anonymous-auth set to allowed value.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes Kubelet must have anonymous authentication disabled (HIGH 242391)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option authentication.anonymous.enabled set to allowed value.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-rdpvb &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-klgpz &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-xzs4w &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes kubelet must enable explicit authorization (HIGH 242392)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option authorization.mode set to allowed value.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-rdpvb &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-klgpz &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-xzs4w &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes Worker Nodes must not have sshd service running (MEDIUM 242393)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">SSH daemon service not installed&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes Worker Nodes must not have the sshd service enabled (MEDIUM 242394)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">SSH daemon disabled (or could not be probed)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes dashboard must not be enabled (MEDIUM 242395)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Kubernetes dashboard not installed&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes kubelet staticPodPath must not enable static pods (HIGH 242397)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option staticPodPath not set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-rdpvb &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-klgpz &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-xzs4w &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API server must have Alpha APIs disabled (MEDIUM 242400)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option featureGates.AllAlpha not set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: deployment name: kube-scheduler namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>cluster: shoot kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: deployment name: kube-scheduler namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>cluster: shoot kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-rdpvb &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: deployment name: kube-scheduler namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>cluster: shoot kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-klgpz &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: deployment name: kube-scheduler namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>cluster: shoot kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-xzs4w &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes Kubelet must deny hostname override (MEDIUM 242404)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Flag hostname-override not set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes kubelet configuration file must be owned by root (MEDIUM 242406)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected owners&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /etc/systemd/system/kubelet.service, ownerUser: 0, ownerGroup: 0 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /etc/systemd/system/kubelet.service, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /etc/systemd/system/kubelet.service, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /etc/systemd/system/kubelet.service, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes kubelet configuration files must have file permissions set to 644 or more restrictive (MEDIUM 242407)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected permissions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /etc/systemd/system/kubelet.service, permissions: 600 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /etc/systemd/system/kubelet.service, permissions: 600 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /etc/systemd/system/kubelet.service, permissions: 600 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /etc/systemd/system/kubelet.service, permissions: 600 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes Controller Manager must disable profiling (MEDIUM 242409)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option profiling set to allowed value.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes cluster must use non-privileged host ports for user pods (MEDIUM 242414)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Container does not use hostPort &amp;lt; 1024.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: pod name: aws-custom-route-controller-7f6d6899b8-x9r5g namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: blackbox-exporter-5b995b7bb-sx94r namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: blackbox-exporter-5b995b7bb-w8sz5 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: cert-controller-manager-7d96bc97db-m6tlw namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: cloud-controller-manager-f97ffb5f4-dcc2m namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-5b998b8444-qq6zq namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-5b998b8444-qq6zq namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-5b998b8444-qq6zq namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-5b998b8444-qq6zq namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-5b998b8444-qq6zq namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-5b998b8444-qq6zq namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-5b998b8444-qq6zq namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-controller-8fb86dbf-f4gsm namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-685c9849cc-56gcr namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-685c9849cc-cfnjr namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: event-logger-89cbdcfc7-5wqcc namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-855f6f7b46-85mmk namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-855f6f7b46-t9bc2 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-685b8cc8b4-pkpbs namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-685b8cc8b4-xw7f4 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-5fcbcf7dc7-l7j4z namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-state-metrics-75c4fc5587-6z9zd namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: machine-controller-manager-85657f4474-bc7m4 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: machine-controller-manager-85657f4474-bc7m4 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: network-problem-detector-controller-64df7f7896-v9vhd namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-75c459cdcb-fvkt6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-75c459cdcb-fvkt6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-75c459cdcb-fvkt6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-75c459cdcb-fvkt6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: shoot-dns-service-5d89988fd6-q2zmg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-85f9c6c76-8wct7 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-85f9c6c76-jw6gv namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-recommender-d88749cb7-vcnjm namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-updater-bf55d6cc5-28zb8 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vpn-seed-server-64b7f7c57-7zhm6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vpn-seed-server-64b7f7c57-7zhm6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-4jx6j namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-4jx6j namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-74rpc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-74rpc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-g7djn namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-zfzsp namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-bx8pm namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-bx8pm namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-hsvgv namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-hsvgv namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-vertical-autoscaler-5477bf8d8b-59qg4 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-xk2zz namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-ztc25 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-horizontal-autoscaler-586ff75c6b-d5nv5 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-vertical-autoscaler-b95cbbd-6xvng namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-5cc8785ccd-qpxgd namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-5cc8785ccd-wgl9h namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-ghzlq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-ghzlq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-ghzlq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-vnzw8 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-vnzw8 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-vnzw8 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: diki-242466-ql1zypq3o4 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-rl6gz namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-zwpn7 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-kkfk1-vIP-Addressljzq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-kkfk1-vIP-Addressljzq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-5dcc94c494-42hc2 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-5dcc94c494-mcbfd namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-9bzkl namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-vkmqw namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-bn6xb namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-bs5x2 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-bcfvm namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-hvgzr namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-jmg2p namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-wk7sf namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: vpn-shoot-7d6ccc788d-2h7cx namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: pod name: blackbox-exporter-c68fc79dd-l5xbp namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: blackbox-exporter-c68fc79dd-rjt6b namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: cert-controller-manager-8655956c7b-ms58m namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: cloud-controller-manager-85d8cf66fd-zk5fp namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-disk-c97765999-2h2nm namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-disk-c97765999-2h2nm namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-disk-c97765999-2h2nm namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-disk-c97765999-2h2nm namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-disk-c97765999-2h2nm namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-disk-c97765999-2h2nm namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-file-65d6bbdbff-h2t8z namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-file-65d6bbdbff-h2t8z namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-file-65d6bbdbff-h2t8z namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-file-65d6bbdbff-h2t8z namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-file-65d6bbdbff-h2t8z namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-file-65d6bbdbff-h2t8z namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-controller-564cf4d7bf-nbfkg namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-6f675766dd-hkmp5 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-6f675766dd-k68sj namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-events-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-events-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: event-logger-5599466957-j62xd namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-74b5bb687c-mhfrb namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-74b5bb687c-njjwl namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-7ff99ff96d-6drt5 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-7ff99ff96d-tpnst namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-66fc77c89f-ttgbf namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-controller-manager-74b87bf4b8-prbhk namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-scheduler-7c8bb6bb7b-l92xf namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-state-metrics-5b84bff8d5-wl7wf namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: machine-controller-manager-858bcf9cf7-bs5sq namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: machine-controller-manager-858bcf9cf7-bs5sq namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: network-problem-detector-controller-5f9ff48494-ks4z2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-6c69cc68d6-khz2x namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-6c69cc68d6-khz2x namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-6c69cc68d6-khz2x namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-6c69cc68d6-khz2x namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: remedy-controller-azure-758fbbbd74-9r2rp namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: shoot-dns-service-cbd996dbd-2pwtz namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-5596d8dfb7-4qp5q namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-5596d8dfb7-nf4bt namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-recommender-856c64b67c-tx6b9 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-updater-6898498449-zlvm9 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vpn-seed-server-6f546b7fcc-w7h5v namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vpn-seed-server-6f546b7fcc-w7h5v namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-82w6x namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-82w6x namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-q8txn namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-q8txn namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-fjpqs namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-pflwv namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-ngqx4 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-p2lxd namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-vertical-autoscaler-5477bf8d8b-vs5gq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-khqqq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-wnrxw namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-horizontal-autoscaler-586ff75c6b-gjh5v namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-vertical-autoscaler-b95cbbd-n8mvw namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: cloud-node-manager-tbhz8 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: cloud-node-manager-wqldn namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-58fd58b4f6-2xwmw namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-58fd58b4f6-6lhw8 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-disk-twzlx namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-disk-twzlx namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-disk-twzlx namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-disk-x7kln namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-disk-x7kln namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-disk-x7kln namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-file-nsp5p namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-file-nsp5p namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-file-nsp5p namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-file-zq279 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-file-zq279 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-file-zq279 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: diki-242394-tlttnutsmb namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: diki-242452-k8s4u72bf8 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-22f4b namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-v567r namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-g7p4p-v1.28.9-fvc8h namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-g7p4p-v1.28.9-fvc8h namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-7fcc47fc99-fcwcl namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-7fcc47fc99-nrcr6 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-bnlt5 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-mrkh5 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-vjdkn namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-wkdbs namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-n8z5n namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-sgst9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-m52ph namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-vpfhd namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: vpn-shoot-5ff8486fbb-z24m6 namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: pod name: blackbox-exporter-775dc59576-8szkf namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: blackbox-exporter-775dc59576-frkcz namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: cert-controller-manager-546c867977-vm6c6 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: cloud-controller-manager-74674958fd-ncqcq namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-6cb89f79bd-k82fm namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-6cb89f79bd-k82fm namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-6cb89f79bd-k82fm namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-6cb89f79bd-k82fm namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-6cb89f79bd-k82fm namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-6cb89f79bd-k82fm namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-controller-d7ff6b8d9-9grlc namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-6d49547579-m762f namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-6d49547579-tf78p namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: event-logger-6c7f7fdfbf-z9vgj namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-b77f75fd5-f6fmr namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-b77f75fd5-hwml6 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-7c6bfdf848-jgz89 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-7c6bfdf848-ssjql namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-5dd44867c5-qv2t4 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-state-metrics-6589cb4654-wlsdt namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: machine-controller-manager-64ffbb68d7-26tpj namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: machine-controller-manager-64ffbb68d7-26tpj namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: network-problem-detector-controller-5bf785bd85-tcdck namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-5d88679c8-g8l6t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-5d88679c8-g8l6t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-5d88679c8-g8l6t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-5d88679c8-g8l6t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: shoot-dns-service-59c48bf94d-5pfng namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-689666988f-74svj namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-689666988f-c8kj7 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-recommender-6fb9bbd9f4-6v4mm namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-updater-5497589658-7mlbf namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vpn-seed-server-67b7868d9f-tkjcn namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vpn-seed-server-67b7868d9f-tkjcn namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-4qd2d namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-4qd2d namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-mpp5m namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-mpp5m namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-hw2lq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-k55mx namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-7cslk namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-7cslk namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-cgfjw namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-cgfjw namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-vertical-autoscaler-5477bf8d8b-72mwz namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-qwdr5 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-r4rlz namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-horizontal-autoscaler-586ff75c6b-xr5l9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-vertical-autoscaler-b95cbbd-cmz64 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-679b67f9f7-d5wkn namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-679b67f9f7-r4mcq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-7k5x9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-7k5x9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-7k5x9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-z8wxb namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-z8wxb namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-z8wxb namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: diki-242393-0bb68s1st7 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-tfjd9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-wnv5w namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-bex82-v1.28.9-gqks6 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-bex82-v1.28.9-gqks6 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-78c558b8f9-bvqn9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-78c558b8f9-vr2tp namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-hlwv7 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-ls6dq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-d7cxg namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-zmm2s namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-rtbxz namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-zk7ds namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-tnns6 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-xnn8r namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: vpn-shoot-7964b7c996-fxjqq namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: pod name: blackbox-exporter-86ddbc4fc9-d8fhl namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: blackbox-exporter-86ddbc4fc9-h9z2v namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: cert-controller-manager-7b679d5b8b-njmtd namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: cloud-controller-manager-799ffbdd4-gr5k2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-f7b684577-ndt2l namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-f7b684577-ndt2l namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-f7b684577-ndt2l namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-f7b684577-ndt2l namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-f7b684577-ndt2l namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-f7b684577-ndt2l namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-controller-66744b7bd-9jrk4 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-64647ffc95-6ppgj namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-64647ffc95-q2txg namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: event-logger-7f8d7cf468-hz58t namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-7fc648c6f5-8g287 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-7fc648c6f5-jdxqc namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-95bcd8cf-nl4np namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-95bcd8cf-tn24f namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-74ff95ff84-ctp7t namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-state-metrics-67d488f745-xgt8t namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: machine-controller-manager-7dcfd598d4-dcxg4 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: machine-controller-manager-7dcfd598d4-dcxg4 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: network-problem-detector-controller-9f8685f46-hqqmb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-5d685bc5c4-tc7sd namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-5d685bc5c4-tc7sd namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-5d685bc5c4-tc7sd namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-5d685bc5c4-tc7sd namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: shoot-dns-service-767d7b4d9d-6b225 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-5fb8ddc449-cvnlf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-5fb8ddc449-xn2hq namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-recommender-598d5c9489-cx8kb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-updater-599c96787c-qfpxz namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vpn-seed-server-98f89cb98-jhnxz namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vpn-seed-server-98f89cb98-jhnxz namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-22hcw namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-22hcw namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-zt84j namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-zt84j namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-q2fvs namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-rmq5w namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-kube-controllers-7ddc84b65c-69dqx namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-jz4m5 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-lzm2c namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-vertical-autoscaler-5477bf8d8b-l2hd2 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-kd2bm namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-pmm2k namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-horizontal-autoscaler-586ff75c6b-v627x namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-vertical-autoscaler-b95cbbd-qzmks namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-56d45984c9-7jntr namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-56d45984c9-xc4wx namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-b5h6v namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-b5h6v namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-b5h6v namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-w2796 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-w2796 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-w2796 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: diki-242450-jppkjiyb58 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-clkj6 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-wx9q7 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-dqty2-vIP-Addresstz7z namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-dqty2-vIP-Addresstz7z namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-66c56d6bd9-2tx9q namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-66c56d6bd9-l6zx7 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-mj9pd namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-zqmc4 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-d64ng namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-n5qd9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-8pc6r namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-x2qw5 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-8nsgv namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-zvgs7 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: vpn-shoot-7bb57b7bcc-95zll namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Secrets in Kubernetes must not be stored as environment variables (HIGH 242415)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Pod does not use environment to inject secret.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: pod name: aws-custom-route-controller-7f6d6899b8-x9r5g namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: blackbox-exporter-5b995b7bb-sx94r namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: blackbox-exporter-5b995b7bb-w8sz5 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: cert-controller-manager-7d96bc97db-m6tlw namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: cloud-controller-manager-f97ffb5f4-dcc2m namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-5b998b8444-qq6zq namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-controller-8fb86dbf-f4gsm namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-685c9849cc-56gcr namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-685c9849cc-cfnjr namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: event-logger-89cbdcfc7-5wqcc namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-855f6f7b46-85mmk namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-855f6f7b46-t9bc2 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-685b8cc8b4-pkpbs namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-685b8cc8b4-xw7f4 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-5fcbcf7dc7-l7j4z namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-state-metrics-75c4fc5587-6z9zd namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: machine-controller-manager-85657f4474-bc7m4 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: network-problem-detector-controller-64df7f7896-v9vhd namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-75c459cdcb-fvkt6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: shoot-dns-service-5d89988fd6-q2zmg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-85f9c6c76-8wct7 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-85f9c6c76-jw6gv namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-recommender-d88749cb7-vcnjm namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-updater-bf55d6cc5-28zb8 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed kind: pod name: vpn-seed-server-64b7f7c57-7zhm6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-4jx6j namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-74rpc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-g7djn namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-zfzsp namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-bx8pm namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-hsvgv namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-vertical-autoscaler-5477bf8d8b-59qg4 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-xk2zz namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-ztc25 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-horizontal-autoscaler-586ff75c6b-d5nv5 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-vertical-autoscaler-b95cbbd-6xvng namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-5cc8785ccd-qpxgd namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-5cc8785ccd-wgl9h namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-ghzlq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-vnzw8 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-rl6gz namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-zwpn7 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-kkfk1-vIP-Addressljzq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-5dcc94c494-42hc2 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-5dcc94c494-mcbfd namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-9bzkl namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-vkmqw namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-bn6xb namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-bs5x2 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-bcfvm namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-hvgzr namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-local-dns-5648f namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-local-dns-mwvrg namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-jmg2p namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-wk7sf namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: vpn-shoot-7d6ccc788d-2h7cx namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: pod name: blackbox-exporter-c68fc79dd-l5xbp namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: blackbox-exporter-c68fc79dd-rjt6b namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: cert-controller-manager-8655956c7b-ms58m namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: cloud-controller-manager-85d8cf66fd-zk5fp namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-disk-c97765999-2h2nm namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-file-65d6bbdbff-h2t8z namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-controller-564cf4d7bf-nbfkg namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-6f675766dd-hkmp5 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-6f675766dd-k68sj namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-events-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: event-logger-5599466957-j62xd namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-74b5bb687c-mhfrb namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-74b5bb687c-njjwl namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-7ff99ff96d-6drt5 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-7ff99ff96d-tpnst namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-66fc77c89f-ttgbf namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-controller-manager-74b87bf4b8-prbhk namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-scheduler-7c8bb6bb7b-l92xf namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-state-metrics-5b84bff8d5-wl7wf namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: machine-controller-manager-858bcf9cf7-bs5sq namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: network-problem-detector-controller-5f9ff48494-ks4z2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-6c69cc68d6-khz2x namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: remedy-controller-azure-758fbbbd74-9r2rp namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: shoot-dns-service-cbd996dbd-2pwtz namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-5596d8dfb7-4qp5q namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-5596d8dfb7-nf4bt namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-recommender-856c64b67c-tx6b9 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-updater-6898498449-zlvm9 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed kind: pod name: vpn-seed-server-6f546b7fcc-w7h5v namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-82w6x namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-q8txn namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-fjpqs namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-pflwv namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-ngqx4 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-p2lxd namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-vertical-autoscaler-5477bf8d8b-vs5gq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-khqqq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-wnrxw namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-horizontal-autoscaler-586ff75c6b-gjh5v namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-vertical-autoscaler-b95cbbd-n8mvw namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: cloud-node-manager-tbhz8 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: cloud-node-manager-wqldn namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-58fd58b4f6-2xwmw namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-58fd58b4f6-6lhw8 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-disk-twzlx namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-disk-x7kln namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-file-nsp5p namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-file-zq279 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: diki-242452-k8s4u72bf8 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-22f4b namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-v567r namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-g7p4p-v1.28.9-fvc8h namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-7fcc47fc99-fcwcl namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-7fcc47fc99-nrcr6 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-bnlt5 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-mrkh5 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-vjdkn namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-wkdbs namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-n8z5n namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-sgst9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-local-dns-j5l82 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-local-dns-nczzd namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-m52ph namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-vpfhd namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: vpn-shoot-5ff8486fbb-z24m6 namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: pod name: blackbox-exporter-775dc59576-8szkf namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: blackbox-exporter-775dc59576-frkcz namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: cert-controller-manager-546c867977-vm6c6 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: cloud-controller-manager-74674958fd-ncqcq namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-6cb89f79bd-k82fm namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-controller-d7ff6b8d9-9grlc namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-6d49547579-m762f namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-6d49547579-tf78p namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: event-logger-6c7f7fdfbf-z9vgj namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-b77f75fd5-f6fmr namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-b77f75fd5-hwml6 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-7c6bfdf848-jgz89 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-7c6bfdf848-ssjql namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-5dd44867c5-qv2t4 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-state-metrics-6589cb4654-wlsdt namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: machine-controller-manager-64ffbb68d7-26tpj namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: network-problem-detector-controller-5bf785bd85-tcdck namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-5d88679c8-g8l6t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: shoot-dns-service-59c48bf94d-5pfng namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-689666988f-74svj namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-689666988f-c8kj7 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-recommender-6fb9bbd9f4-6v4mm namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-updater-5497589658-7mlbf namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed kind: pod name: vpn-seed-server-67b7868d9f-tkjcn namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-4qd2d namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-mpp5m namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-hw2lq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-k55mx namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-7cslk namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-cgfjw namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-vertical-autoscaler-5477bf8d8b-72mwz namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-qwdr5 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-r4rlz namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-horizontal-autoscaler-586ff75c6b-xr5l9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-vertical-autoscaler-b95cbbd-cmz64 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-679b67f9f7-d5wkn namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-679b67f9f7-r4mcq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-7k5x9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-z8wxb namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: diki-242449-ujakbucnc7 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-tfjd9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-wnv5w namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-bex82-v1.28.9-gqks6 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-78c558b8f9-bvqn9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-78c558b8f9-vr2tp namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-hlwv7 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-ls6dq namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-d7cxg namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-zmm2s namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-rtbxz namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-zk7ds namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-local-dns-fc8nx namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-local-dns-pm55v namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-tnns6 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-xnn8r namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: vpn-shoot-7964b7c996-fxjqq namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: pod name: blackbox-exporter-86ddbc4fc9-d8fhl namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: blackbox-exporter-86ddbc4fc9-h9z2v namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: cert-controller-manager-7b679d5b8b-njmtd namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: cloud-controller-manager-799ffbdd4-gr5k2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-driver-controller-f7b684577-ndt2l namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-controller-66744b7bd-9jrk4 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-64647ffc95-6ppgj namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: csi-snapshot-validation-64647ffc95-q2txg namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: event-logger-7f8d7cf468-hz58t namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-7fc648c6f5-8g287 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: extension-shoot-lakom-service-7fc648c6f5-jdxqc namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-95bcd8cf-nl4np namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: gardener-resource-manager-95bcd8cf-tn24f namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-74ff95ff84-ctp7t namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: kube-state-metrics-67d488f745-xgt8t namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: machine-controller-manager-7dcfd598d4-dcxg4 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: network-problem-detector-controller-9f8685f46-hqqmb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: plutono-5d685bc5c4-tc7sd namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: prometheus-shoot-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: shoot-dns-service-767d7b4d9d-6b225 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vali-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-5fb8ddc449-cvnlf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-admission-controller-5fb8ddc449-xn2hq namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-recommender-598d5c9489-cx8kb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vpa-updater-599c96787c-qfpxz namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed kind: pod name: vpn-seed-server-98f89cb98-jhnxz namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-22hcw namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: apiserver-proxy-zt84j namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-q2fvs namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: blackbox-exporter-858fbbb8d6-rmq5w namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-kube-controllers-7ddc84b65c-69dqx namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-jz4m5 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-lzm2c namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-node-vertical-autoscaler-5477bf8d8b-l2hd2 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-kd2bm namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-deploy-764557c6bc-pmm2k namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-horizontal-autoscaler-586ff75c6b-v627x namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: calico-typha-vertical-autoscaler-b95cbbd-qzmks namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-56d45984c9-7jntr namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: coredns-56d45984c9-xc4wx namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-b5h6v namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: csi-driver-node-w2796 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-clkj6 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: egress-filter-applier-wx9q7 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-dqty2-vIP-Addresstz7z namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-66c56d6bd9-2tx9q namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: metrics-server-66c56d6bd9-l6zx7 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-mj9pd namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-host-zqmc4 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-d64ng namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: network-problem-detector-pod-n5qd9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-8pc6r namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-exporter-x2qw5 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-local-dns-grlsc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-local-dns-qhngh namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-8nsgv namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: node-problem-detector-zvgs7 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot kind: pod name: vpn-shoot-7bb57b7bcc-95zll namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes must separate user functionality (MEDIUM 242417)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Gardener managed pods are not user pods&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: pod name: apiserver-proxy-4jx6j namespace: kube-system &lt;/li>
&lt;li>kind: pod name: apiserver-proxy-74rpc namespace: kube-system &lt;/li>
&lt;li>kind: pod name: blackbox-exporter-858fbbb8d6-g7djn namespace: kube-system &lt;/li>
&lt;li>kind: pod name: blackbox-exporter-858fbbb8d6-zfzsp namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-node-bx8pm namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-node-hsvgv namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-node-vertical-autoscaler-5477bf8d8b-59qg4 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-deploy-764557c6bc-xk2zz namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-deploy-764557c6bc-ztc25 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-horizontal-autoscaler-586ff75c6b-d5nv5 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-vertical-autoscaler-b95cbbd-6xvng namespace: kube-system &lt;/li>
&lt;li>kind: pod name: coredns-5cc8785ccd-qpxgd namespace: kube-system &lt;/li>
&lt;li>kind: pod name: coredns-5cc8785ccd-wgl9h namespace: kube-system &lt;/li>
&lt;li>kind: pod name: csi-driver-node-ghzlq namespace: kube-system &lt;/li>
&lt;li>kind: pod name: csi-driver-node-vnzw8 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: egress-filter-applier-rl6gz namespace: kube-system &lt;/li>
&lt;li>kind: pod name: egress-filter-applier-zwpn7 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: kube-proxy-worker-kkfk1-vIP-Addressljzq namespace: kube-system &lt;/li>
&lt;li>kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;li>kind: pod name: metrics-server-5dcc94c494-42hc2 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: metrics-server-5dcc94c494-mcbfd namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-host-9bzkl namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-host-vkmqw namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-pod-bn6xb namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-pod-bs5x2 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-exporter-bcfvm namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-exporter-hvgzr namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-local-dns-5648f namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-local-dns-mwvrg namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-problem-detector-jmg2p namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-problem-detector-wk7sf namespace: kube-system &lt;/li>
&lt;li>kind: pod name: vpn-shoot-7d6ccc788d-2h7cx namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: pod name: apiserver-proxy-82w6x namespace: kube-system &lt;/li>
&lt;li>kind: pod name: apiserver-proxy-q8txn namespace: kube-system &lt;/li>
&lt;li>kind: pod name: blackbox-exporter-858fbbb8d6-fjpqs namespace: kube-system &lt;/li>
&lt;li>kind: pod name: blackbox-exporter-858fbbb8d6-pflwv namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-node-ngqx4 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-node-p2lxd namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-node-vertical-autoscaler-5477bf8d8b-vs5gq namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-deploy-764557c6bc-khqqq namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-deploy-764557c6bc-wnrxw namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-horizontal-autoscaler-586ff75c6b-gjh5v namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-vertical-autoscaler-b95cbbd-n8mvw namespace: kube-system &lt;/li>
&lt;li>kind: pod name: cloud-node-manager-tbhz8 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: cloud-node-manager-wqldn namespace: kube-system &lt;/li>
&lt;li>kind: pod name: coredns-58fd58b4f6-2xwmw namespace: kube-system &lt;/li>
&lt;li>kind: pod name: coredns-58fd58b4f6-6lhw8 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: csi-driver-node-disk-twzlx namespace: kube-system &lt;/li>
&lt;li>kind: pod name: csi-driver-node-disk-x7kln namespace: kube-system &lt;/li>
&lt;li>kind: pod name: csi-driver-node-file-nsp5p namespace: kube-system &lt;/li>
&lt;li>kind: pod name: csi-driver-node-file-zq279 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: egress-filter-applier-22f4b namespace: kube-system &lt;/li>
&lt;li>kind: pod name: egress-filter-applier-v567r namespace: kube-system &lt;/li>
&lt;li>kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;li>kind: pod name: kube-proxy-worker-g7p4p-v1.28.9-fvc8h namespace: kube-system &lt;/li>
&lt;li>kind: pod name: metrics-server-7fcc47fc99-fcwcl namespace: kube-system &lt;/li>
&lt;li>kind: pod name: metrics-server-7fcc47fc99-nrcr6 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-host-bnlt5 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-host-mrkh5 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-pod-vjdkn namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-pod-wkdbs namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-exporter-n8z5n namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-exporter-sgst9 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-local-dns-j5l82 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-local-dns-nczzd namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-problem-detector-m52ph namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-problem-detector-vpfhd namespace: kube-system &lt;/li>
&lt;li>kind: pod name: vpn-shoot-5ff8486fbb-z24m6 namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: pod name: apiserver-proxy-4qd2d namespace: kube-system &lt;/li>
&lt;li>kind: pod name: apiserver-proxy-mpp5m namespace: kube-system &lt;/li>
&lt;li>kind: pod name: blackbox-exporter-858fbbb8d6-hw2lq namespace: kube-system &lt;/li>
&lt;li>kind: pod name: blackbox-exporter-858fbbb8d6-k55mx namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-node-7cslk namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-node-cgfjw namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-node-vertical-autoscaler-5477bf8d8b-72mwz namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-deploy-764557c6bc-qwdr5 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-deploy-764557c6bc-r4rlz namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-horizontal-autoscaler-586ff75c6b-xr5l9 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-vertical-autoscaler-b95cbbd-cmz64 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: coredns-679b67f9f7-d5wkn namespace: kube-system &lt;/li>
&lt;li>kind: pod name: coredns-679b67f9f7-r4mcq namespace: kube-system &lt;/li>
&lt;li>kind: pod name: csi-driver-node-7k5x9 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: csi-driver-node-z8wxb namespace: kube-system &lt;/li>
&lt;li>kind: pod name: egress-filter-applier-tfjd9 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: egress-filter-applier-wnv5w namespace: kube-system &lt;/li>
&lt;li>kind: pod name: kube-proxy-worker-bex82-v1.28.9-gqks6 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;li>kind: pod name: metrics-server-78c558b8f9-bvqn9 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: metrics-server-78c558b8f9-vr2tp namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-host-hlwv7 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-host-ls6dq namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-pod-d7cxg namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-pod-zmm2s namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-exporter-rtbxz namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-exporter-zk7ds namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-local-dns-fc8nx namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-local-dns-pm55v namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-problem-detector-tnns6 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-problem-detector-xnn8r namespace: kube-system &lt;/li>
&lt;li>kind: pod name: vpn-shoot-7964b7c996-fxjqq namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: pod name: apiserver-proxy-22hcw namespace: kube-system &lt;/li>
&lt;li>kind: pod name: apiserver-proxy-zt84j namespace: kube-system &lt;/li>
&lt;li>kind: pod name: blackbox-exporter-858fbbb8d6-q2fvs namespace: kube-system &lt;/li>
&lt;li>kind: pod name: blackbox-exporter-858fbbb8d6-rmq5w namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-kube-controllers-7ddc84b65c-69dqx namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-node-jz4m5 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-node-lzm2c namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-node-vertical-autoscaler-5477bf8d8b-l2hd2 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-deploy-764557c6bc-kd2bm namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-deploy-764557c6bc-pmm2k namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-horizontal-autoscaler-586ff75c6b-v627x namespace: kube-system &lt;/li>
&lt;li>kind: pod name: calico-typha-vertical-autoscaler-b95cbbd-qzmks namespace: kube-system &lt;/li>
&lt;li>kind: pod name: coredns-56d45984c9-7jntr namespace: kube-system &lt;/li>
&lt;li>kind: pod name: coredns-56d45984c9-xc4wx namespace: kube-system &lt;/li>
&lt;li>kind: pod name: csi-driver-node-b5h6v namespace: kube-system &lt;/li>
&lt;li>kind: pod name: csi-driver-node-w2796 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: egress-filter-applier-clkj6 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: egress-filter-applier-wx9q7 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: kube-proxy-worker-dqty2-vIP-Addresstz7z namespace: kube-system &lt;/li>
&lt;li>kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: metrics-server-66c56d6bd9-2tx9q namespace: kube-system &lt;/li>
&lt;li>kind: pod name: metrics-server-66c56d6bd9-l6zx7 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-host-mj9pd namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-host-zqmc4 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-pod-d64ng namespace: kube-system &lt;/li>
&lt;li>kind: pod name: network-problem-detector-pod-n5qd9 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-exporter-8pc6r namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-exporter-x2qw5 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-local-dns-grlsc namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-local-dns-qhngh namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-problem-detector-8nsgv namespace: kube-system &lt;/li>
&lt;li>kind: pod name: node-problem-detector-zvgs7 namespace: kube-system &lt;/li>
&lt;li>kind: pod name: vpn-shoot-7bb57b7bcc-95zll namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API server must use approved cipher suites (MEDIUM 242418)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option tls-cipher-suites set to allowed values.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes API Server must have the SSL Certificate Authority set (MEDIUM 242419)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option client-ca-file set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes Kubelet must have the SSL Certificate Authority set (MEDIUM 242420)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option authentication.x509.clientCAFile set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-rdpvb &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-klgpz &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-xzs4w &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes Controller Manager must have the SSL Certificate Authority set (MEDIUM 242421)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option root-ca-file set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-controller-manager namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes API Server must have a certificate for communication (MEDIUM 242422)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option tls-cert-file set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option tls-private-key-file set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes etcd must enable client authentication to secure service (MEDIUM 242423)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option client-transport-security.client-cert-auth set to allowed value.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes Kubelet must enable tlsPrivateKeyFile for client authentication to secure service (MEDIUM 242424)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Kubelet rotates server certificates automatically itself.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-rdpvb &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-klgpz &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-xzs4w &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes Kubelet must enable tlsCertFile for client authentication to secure service (MEDIUM 242425)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Kubelet rotates server certificates automatically itself.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-rdpvb &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-klgpz &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-xzs4w &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes etcd must have a key file for secure communication (MEDIUM 242427)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option client-transport-security.key-file set to allowed value.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes etcd must have a certificate for communication (MEDIUM 242428)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option client-transport-security.cert-file set to allowed value.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes etcd must have the SSL Certificate Authority set (MEDIUM 242429)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option etcd-cafile set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes etcd must have a certificate for communication (MEDIUM 242430)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option etcd-certfile set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes etcd must have a key file for secure communication (MEDIUM 242431)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option etcd-keyfile set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes Kubelet must enable kernel protection (HIGH 242434)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option protectKernelDefaults set to allowed value.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-rdpvb &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-klgpz &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-xzs4w &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API server must have the ValidatingAdmissionWebhook enabled (HIGH 242436)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option enable-admission-plugins set to allowed value.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes API Server must configure timeouts to limit attack surface (MEDIUM 242438)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option request-timeout has not been set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: defaults to 1m0s kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: defaults to 1m0s kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: defaults to 1m0s kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: defaults to 1m0s kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes must remove old components after updated versions have been installed (MEDIUM 242442)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">All found images use current versions.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes component etcd must be owned by etcd (MEDIUM 242445)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected owners&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/new.etcd/member/snap/db, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/new.etcd/member/wal/0.tmp, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/safe_guard, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_08.4131794839/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/token, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/namespace, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_08.1753711665/accessKeyID, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_08.1753711665/bucketName, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_08.1753711665/region, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_08.1753711665/secretAccessKey, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/new.etcd/member/snap/db, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/new.etcd/member/wal/0.tmp, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/safe_guard, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~configmap/etcd-config-file/..2024_07_03_08_13_08.1092397241/etcd.conf.yaml, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_08.4131794839/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/token, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/namespace, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/new.etcd/member/snap/db, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/new.etcd/member/wal/0.tmp, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/safe_guard, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_09.1557393859/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/namespace, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/token, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/new.etcd/member/snap/db, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/new.etcd/member/wal/0.tmp, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/safe_guard, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~configmap/etcd-config-file/..2024_07_03_08_14_09.892466712/etcd.conf.yaml, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_09.1557393859/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/namespace, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/token, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/safe_guard, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/new.etcd/member/wal/0.tmp, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/new.etcd/member/snap/db, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_25.60881411/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/token, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/namespace, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_08_25.694960980/storageKey, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_08_25.694960980/bucketName, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_08_25.694960980/storageAccount, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/safe_guard, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/new.etcd/member/wal/0.tmp, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/new.etcd/member/snap/db, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~configmap/etcd-config-file/..2024_07_03_08_08_25.2056088153/etcd.conf.yaml, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_25.60881411/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/token, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/namespace, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/safe_guard, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/new.etcd/member/wal/0.tmp, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/new.etcd/member/snap/db, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_39.2700028413/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/token, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/namespace, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_08_39.2704609432/bucketName, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_08_39.2704609432/serviceaccount.json, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/safe_guard, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/new.etcd/member/wal/0.tmp, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/new.etcd/member/snap/db, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~configmap/etcd-config-file/..2024_07_03_08_08_39.3391642353/etcd.conf.yaml, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_39.2700028413/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/token, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/namespace, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/safe_guard, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/new.etcd/member/wal/0.tmp, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/new.etcd/member/snap/db, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_38.1180291661/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/token, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/namespace, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/safe_guard, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/new.etcd/member/wal/0.tmp, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/new.etcd/member/snap/db, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~configmap/etcd-config-file/..2024_07_03_08_08_38.340777120/etcd.conf.yaml, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_38.1180291661/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/token, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/namespace, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/safe_guard, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/new.etcd/member/snap/db, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/new.etcd/member/wal/0.tmp, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_39.3622463902/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/namespace, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/token, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/applicationCredentialName, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/applicationCredentialSecret, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/authURL, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/bucketName, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/domainName, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/region, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/tenantName, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/applicationCredentialID, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/safe_guard, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/new.etcd/member/snap/db, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/new.etcd/member/wal/0.tmp, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~configmap/etcd-config-file/..2024_07_03_08_13_39.1118508487/etcd.conf.yaml, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_39.3622463902/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/namespace, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/token, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/new.etcd/member/wal/0.tmp, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/new.etcd/member/snap/db, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/safe_guard, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_40.1257318752/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/namespace, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/token, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/new.etcd/member/wal/0.tmp, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/new.etcd/member/snap/db, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/safe_guard, ownerUser: 65532, ownerGroup: 65532 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~configmap/etcd-config-file/..2024_07_03_08_14_40.1087789113/etcd.conf.yaml, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_40.1257318752/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/namespace, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/token, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes conf files must be owned by root (MEDIUM 242446)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected owners&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_20_09.1326916364/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_20_09.2623235253/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_20_09.2623235253/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~configmap/kube-scheduler-config/..2024_07_03_08_20_09.1466398819/config.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_20_09.314874445/kubeconfig, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_20_09.314874445/token, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_10_40.132223152/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_10_40.160858789/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_10_40.1963177894/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/server/..2024_07_03_08_10_40.3470744456/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/server/..2024_07_03_08_10_40.3470744456/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~configmap/audit-policy-config/..2024_07_03_08_10_40.98956035/audit-policy.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_10_40.247189818/podsecurity.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_10_40.247189818/admission-configuration.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca/..2024_07_03_08_10_40.729577280/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/etcd-encryption-secret/..2024_07_03_08_10_40.4281367363/encryption-configuration.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_10_40.2692570695/id_rsa, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_10_40.621084939/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_10_40.3790343357/bundle.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~configmap/egress-selection-config/..2024_07_03_08_10_40.2649852811/egress-selector-configuration.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_10_40.320446039/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_10_40.320446039/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_10_40.2516322784/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_10_40.2516322784/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/static-token/..2024_07_03_08_10_40.2560086837/static_tokens.csv, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_10_40.3019948839/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_10_40.3019948839/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_10_40.2969198131/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_10_40.2969198131/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_10_40.2969198131/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_10_40.3729800005/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_10_40.3729800005/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_10_40.3422263292/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca/..2024_07_03_08_11_54.3036196029/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_54.4201085789/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_54.4201085789/ca.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_11_54.3862694996/id_rsa, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_54.3440466664/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_54.3440466664/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_54.2354965601/ca.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_54.2354965601/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_12_01.3715611058/kubeconfig, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_12_01.3715611058/token, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_22_12.2221992855/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_22_12.3473324769/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_22_12.2280157390/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/server/..2024_07_03_08_22_12.33202829/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/server/..2024_07_03_08_22_12.33202829/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~configmap/audit-policy-config/..2024_07_03_08_22_12.2179539215/audit-policy.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_22_12.2514111652/podsecurity.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_22_12.2514111652/admission-configuration.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca/..2024_07_03_08_22_12.651057513/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/etcd-encryption-secret/..2024_07_03_08_22_12.3174518015/encryption-configuration.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_22_12.667135894/id_rsa, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_22_12.772465054/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_22_12.2456873837/bundle.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~configmap/egress-selection-config/..2024_07_03_08_22_12.2465776616/egress-selector-configuration.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_22_12.1108572537/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_22_12.1108572537/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_22_12.3056046342/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_22_12.3056046342/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/static-token/..2024_07_03_08_22_12.3720566324/static_tokens.csv, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_22_12.784723719/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_22_12.784723719/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_22_12.2019313851/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_22_12.2019313851/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_22_12.2019313851/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_22_12.3723569969/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_22_12.3723569969/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_22_12.2673815035/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_19_06.1353495674/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_19_06.1317647725/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_19_06.1317647725/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~configmap/kube-scheduler-config/..2024_07_03_08_19_06.953683874/config.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_19_06.4011589525/token, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_19_06.4011589525/kubeconfig, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_17_06.1421105491/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_17_06.1778852592/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_17_06.3754135056/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/server/..2024_07_03_08_17_06.2728615182/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/server/..2024_07_03_08_17_06.2728615182/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~configmap/audit-policy-config/..2024_07_03_08_17_06.3429146079/audit-policy.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_17_06.953443819/podsecurity.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_17_06.953443819/admission-configuration.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca/..2024_07_03_08_17_06.11797511/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/etcd-encryption-secret/..2024_07_03_08_17_06.1577058416/encryption-configuration.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_17_06.3648435760/id_rsa, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_17_06.2297979827/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_17_06.820126748/bundle.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~configmap/egress-selection-config/..2024_07_03_08_17_06.639129834/egress-selector-configuration.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_17_06.1849336632/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_17_06.1849336632/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_17_06.4153675152/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_17_06.4153675152/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/static-token/..2024_07_03_08_17_06.790953682/static_tokens.csv, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_17_06.2975900753/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_17_06.2975900753/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_17_06.2888946878/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_17_06.2888946878/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_17_06.2888946878/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_17_06.3608837884/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_17_06.3608837884/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_17_06.2907287084/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca/..2024_07_03_08_11_01.1416082481/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_01.2822029985/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_01.2822029985/ca.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_11_01.3378265558/id_rsa, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_01.424413895/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_01.424413895/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_01.3157557822/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_01.3157557822/ca.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_11_04.2255727832/token, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_11_04.2255727832/kubeconfig, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca/..2024_07_03_08_15_39.1644055289/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_15_39.4292111706/ca.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_15_39.4292111706/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_15_39.694420208/id_rsa, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/server/..2024_07_03_08_15_39.3129979181/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/server/..2024_07_03_08_15_39.3129979181/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_15_39.4082412295/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_15_39.4082412295/ca.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_15_39.3235024203/token, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_15_39.3235024203/kubeconfig, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_17_40.1915895115/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_17_40.1063558951/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_17_40.1063558951/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~configmap/kube-scheduler-config/..2024_07_03_08_17_40.2246557176/config.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_17_40.4079250913/kubeconfig, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_17_40.4079250913/token, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_28_38.1760636453/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_28_38.212112882/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_28_38.1565717575/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/server/..2024_07_03_08_28_38.3505195174/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/server/..2024_07_03_08_28_38.3505195174/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~configmap/audit-policy-config/..2024_07_03_08_28_38.330514936/audit-policy.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_28_38.901497533/podsecurity.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_28_38.901497533/admission-configuration.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca/..2024_07_03_08_28_38.35631685/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/etcd-encryption-secret/..2024_07_03_08_28_38.1969276561/encryption-configuration.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_28_38.397370216/id_rsa, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_28_38.515039494/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_28_38.1025349512/bundle.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~configmap/egress-selection-config/..2024_07_03_08_28_38.3989665750/egress-selector-configuration.yaml, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_28_38.2197025418/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_28_38.2197025418/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_28_38.1734894870/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_28_38.1734894870/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/static-token/..2024_07_03_08_28_38.1327549647/static_tokens.csv, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_28_38.673160524/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_28_38.673160524/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_28_38.2037804618/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_28_38.2037804618/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_28_38.2037804618/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_28_38.2931148428/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_28_38.2931148428/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_28_38.2236084738/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes Kube Proxy kubeconfig must have file permissions set to 644 or more restrictive (MEDIUM 242447)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected permissions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/pods/53c2c7c8-feb4-4269-9e5e-923d20a955d0/volumes/kubernetes.io~configmap/kube-proxy-config/config.yaml, permissions: 644 kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/pods/53c2c7c8-feb4-4269-9e5e-923d20a955d0/volumes/kubernetes.io~secret/kubeconfig/kubeconfig, permissions: 644 kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/pods/058cd0f1-151a-41c8-b208-938fddcbe984/volumes/kubernetes.io~configmap/kube-proxy-config/config.yaml, permissions: 644 kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/pods/058cd0f1-151a-41c8-b208-938fddcbe984/volumes/kubernetes.io~secret/kubeconfig/kubeconfig, permissions: 644 kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/pods/69199841-8a91-4bed-ac4c-b39e2c0d38fd/volumes/kubernetes.io~configmap/kube-proxy-config/config.yaml, permissions: 644 kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/pods/69199841-8a91-4bed-ac4c-b39e2c0d38fd/volumes/kubernetes.io~secret/kubeconfig/kubeconfig, permissions: 644 kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/pods/aff2acf3-c4c2-45d1-b80f-377077999d72/volumes/kubernetes.io~configmap/kube-proxy-config/config.yaml, permissions: 644 kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/pods/aff2acf3-c4c2-45d1-b80f-377077999d72/volumes/kubernetes.io~secret/kubeconfig/kubeconfig, permissions: 644 kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes Kube Proxy kubeconfig must be owned by root (MEDIUM 242448)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected owners&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/pods/53c2c7c8-feb4-4269-9e5e-923d20a955d0/volumes/kubernetes.io~configmap/kube-proxy-config/config.yaml, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/pods/53c2c7c8-feb4-4269-9e5e-923d20a955d0/volumes/kubernetes.io~secret/kubeconfig/kubeconfig, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/pods/058cd0f1-151a-41c8-b208-938fddcbe984/volumes/kubernetes.io~configmap/kube-proxy-config/config.yaml, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/pods/058cd0f1-151a-41c8-b208-938fddcbe984/volumes/kubernetes.io~secret/kubeconfig/kubeconfig, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/pods/69199841-8a91-4bed-ac4c-b39e2c0d38fd/volumes/kubernetes.io~configmap/kube-proxy-config/config.yaml, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/pods/69199841-8a91-4bed-ac4c-b39e2c0d38fd/volumes/kubernetes.io~secret/kubeconfig/kubeconfig, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/pods/aff2acf3-c4c2-45d1-b80f-377077999d72/volumes/kubernetes.io~configmap/kube-proxy-config/config.yaml, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/pods/aff2acf3-c4c2-45d1-b80f-377077999d72/volumes/kubernetes.io~secret/kubeconfig/kubeconfig, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes Kubelet certificate authority file must have file permissions set to 644 or more restrictive (MEDIUM 242449)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected permissions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/ca.crt, permissions: 644 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/ca.crt, permissions: 644 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/ca.crt, permissions: 644 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/ca.crt, permissions: 644 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes Kubelet certificate authority must be owned by root (MEDIUM 242450)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected owners&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/ca.crt, ownerUser: 0, ownerGroup: 0 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/ca.crt, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/ca.crt, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/ca.crt, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes component PKI must be owned by root (MEDIUM 242451)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected owners&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_10_40.132223152/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_10_40.160858789/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_10_40.1963177894/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/server/..2024_07_03_08_10_40.3470744456/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/server/..2024_07_03_08_10_40.3470744456/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca/..2024_07_03_08_10_40.729577280/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_10_40.621084939/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_10_40.3790343357/bundle.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_10_40.320446039/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_10_40.320446039/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_10_40.2516322784/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_10_40.2516322784/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_10_40.3019948839/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_10_40.3019948839/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_10_40.2969198131/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_10_40.2969198131/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_10_40.2969198131/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_10_40.3729800005/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_10_40.3729800005/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_10_40.3422263292/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_10_40.2969198131, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_10_40.3729800005, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_10_40.132223152, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_10_40.1963177894, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/server/..2024_07_03_08_10_40.3470744456, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_10_40.2516322784, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_10_40.3019948839, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_10_40.3422263292, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_10_40.160858789, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca/..2024_07_03_08_10_40.729577280, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_10_40.621084939, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_10_40.3790343357, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_10_40.320446039, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca/..2024_07_03_08_11_54.3036196029/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_54.4201085789/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_54.4201085789/ca.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_54.3440466664/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_54.3440466664/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_54.2354965601/ca.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_54.2354965601/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_54.4201085789, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_54.3440466664, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_54.2354965601, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca/..2024_07_03_08_11_54.3036196029, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_09.1557393859/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_09.1557393859, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_09.1557393859/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_09.1557393859, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_20_09.1326916364/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_20_09.2623235253/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_20_09.2623235253/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_20_09.1326916364, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_20_09.2623235253, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_08.4131794839/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_08.4131794839, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_08.4131794839/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_08.4131794839, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: shoot containerName: kube-proxy details: fileName: /var/lib/kubelet/pods/53c2c7c8-feb4-4269-9e5e-923d20a955d0/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_20.3132223426/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: kube-proxy details: fileName: /var/lib/kubelet/pods/53c2c7c8-feb4-4269-9e5e-923d20a955d0/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_20.3132223426, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: conntrack-fix details: fileName: /var/lib/kubelet/pods/53c2c7c8-feb4-4269-9e5e-923d20a955d0/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_20.3132223426/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: conntrack-fix details: fileName: /var/lib/kubelet/pods/53c2c7c8-feb4-4269-9e5e-923d20a955d0/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_20.3132223426, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-client-2024-07-03-08-14-06.pem, ownerUser: 0, ownerGroup: 0 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-server-2024-07-03-08-14-07.pem, ownerUser: 0, ownerGroup: 0 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki, ownerUser: 0, ownerGroup: 0 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_25.60881411/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_25.60881411, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_25.60881411/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_25.60881411, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_22_12.2221992855/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_22_12.3473324769/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_22_12.2280157390/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/server/..2024_07_03_08_22_12.33202829/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/server/..2024_07_03_08_22_12.33202829/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca/..2024_07_03_08_22_12.651057513/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_22_12.772465054/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_22_12.2456873837/bundle.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_22_12.1108572537/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_22_12.1108572537/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_22_12.3056046342/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_22_12.3056046342/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_22_12.784723719/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_22_12.784723719/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_22_12.2019313851/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_22_12.2019313851/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_22_12.2019313851/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_22_12.3723569969/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_22_12.3723569969/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_22_12.2673815035/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_22_12.2456873837, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_22_12.2673815035, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_22_12.3473324769, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_22_12.2280157390, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/server/..2024_07_03_08_22_12.33202829, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca/..2024_07_03_08_22_12.651057513, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_22_12.772465054, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_22_12.3723569969, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_22_12.2221992855, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_22_12.1108572537, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_22_12.3056046342, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_22_12.784723719, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_22_12.2019313851, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: shoot containerName: conntrack-fix details: fileName: /var/lib/kubelet/pods/058cd0f1-151a-41c8-b208-938fddcbe984/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_24_30.478337316/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: conntrack-fix details: fileName: /var/lib/kubelet/pods/058cd0f1-151a-41c8-b208-938fddcbe984/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_24_30.478337316, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: kube-proxy details: fileName: /var/lib/kubelet/pods/058cd0f1-151a-41c8-b208-938fddcbe984/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_24_30.478337316/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: kube-proxy details: fileName: /var/lib/kubelet/pods/058cd0f1-151a-41c8-b208-938fddcbe984/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_24_30.478337316, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-server-2024-07-03-08-14-08.pem, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-client-2024-07-03-08-14-05.pem, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca/..2024_07_03_08_11_01.1416082481/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_01.2822029985/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_01.2822029985/ca.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_01.424413895/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_01.424413895/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_01.3157557822/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_01.3157557822/ca.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_01.424413895, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_01.3157557822, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca/..2024_07_03_08_11_01.1416082481, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_01.2822029985, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_19_06.1353495674/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_19_06.1317647725/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_19_06.1317647725/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_19_06.1317647725, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_19_06.1353495674, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_39.2700028413/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_39.2700028413, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_39.2700028413/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_39.2700028413, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_38.1180291661/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_38.1180291661, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_38.1180291661/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_38.1180291661, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_17_06.1421105491/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_17_06.1778852592/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_17_06.3754135056/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/server/..2024_07_03_08_17_06.2728615182/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/server/..2024_07_03_08_17_06.2728615182/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca/..2024_07_03_08_17_06.11797511/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_17_06.2297979827/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_17_06.820126748/bundle.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_17_06.1849336632/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_17_06.1849336632/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_17_06.4153675152/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_17_06.4153675152/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_17_06.2975900753/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_17_06.2975900753/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_17_06.2888946878/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_17_06.2888946878/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_17_06.2888946878/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_17_06.3608837884/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_17_06.3608837884/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_17_06.2907287084/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_17_06.820126748, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_17_06.1849336632, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_17_06.2975900753, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_17_06.1778852592, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_17_06.3754135056, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/server/..2024_07_03_08_17_06.2728615182, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca/..2024_07_03_08_17_06.11797511, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_17_06.3608837884, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_17_06.2907287084, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_17_06.1421105491, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_17_06.2297979827, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_17_06.4153675152, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_17_06.2888946878, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: shoot containerName: kube-proxy details: fileName: /var/lib/kubelet/pods/69199841-8a91-4bed-ac4c-b39e2c0d38fd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_12_54.4164575669/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: kube-proxy details: fileName: /var/lib/kubelet/pods/69199841-8a91-4bed-ac4c-b39e2c0d38fd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_12_54.4164575669, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: conntrack-fix details: fileName: /var/lib/kubelet/pods/69199841-8a91-4bed-ac4c-b39e2c0d38fd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_12_54.4164575669/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: conntrack-fix details: fileName: /var/lib/kubelet/pods/69199841-8a91-4bed-ac4c-b39e2c0d38fd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_12_54.4164575669, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-server-2024-07-03-08-12-53.pem, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-client-2024-07-03-08-12-51.pem, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_28_38.1760636453/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_28_38.212112882/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_28_38.1565717575/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/server/..2024_07_03_08_28_38.3505195174/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/server/..2024_07_03_08_28_38.3505195174/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca/..2024_07_03_08_28_38.35631685/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_28_38.515039494/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_28_38.1025349512/bundle.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_28_38.2197025418/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_28_38.2197025418/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_28_38.1734894870/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_28_38.1734894870/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_28_38.673160524/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_28_38.673160524/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_28_38.2037804618/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_28_38.2037804618/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_28_38.2037804618/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_28_38.2931148428/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_28_38.2931148428/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_28_38.2236084738/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_28_38.1734894870, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_28_38.673160524, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_28_38.2037804618, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_28_38.2931148428, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_28_38.1565717575, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/server/..2024_07_03_08_28_38.3505195174, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca/..2024_07_03_08_28_38.35631685, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_28_38.1025349512, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_28_38.2197025418, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_28_38.2236084738, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_28_38.1760636453, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_28_38.212112882, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_28_38.515039494, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_40.1257318752/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_40.1257318752, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_40.1257318752/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_40.1257318752, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca/..2024_07_03_08_15_39.1644055289/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_15_39.4292111706/ca.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_15_39.4292111706/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/server/..2024_07_03_08_15_39.3129979181/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/server/..2024_07_03_08_15_39.3129979181/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_15_39.4082412295/ca.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_15_39.4082412295/ca.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/server/..2024_07_03_08_15_39.3129979181, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_15_39.4082412295, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca/..2024_07_03_08_15_39.1644055289, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_15_39.4292111706, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_17_40.1915895115/bundle.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_17_40.1063558951/tls.crt, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_17_40.1063558951/tls.key, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_17_40.1915895115, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_17_40.1063558951, ownerUser: 0, ownerGroup: 65532 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_39.3622463902/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_39.3622463902, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_39.3622463902/bundle.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.key, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_39.3622463902, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416, ownerUser: 0, ownerGroup: 0 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: shoot containerName: kube-proxy details: fileName: /var/lib/kubelet/pods/aff2acf3-c4c2-45d1-b80f-377077999d72/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_23_24.1111541912/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: kube-proxy details: fileName: /var/lib/kubelet/pods/aff2acf3-c4c2-45d1-b80f-377077999d72/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_23_24.1111541912, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: conntrack-fix details: fileName: /var/lib/kubelet/pods/aff2acf3-c4c2-45d1-b80f-377077999d72/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_23_24.1111541912/ca.crt, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: conntrack-fix details: fileName: /var/lib/kubelet/pods/aff2acf3-c4c2-45d1-b80f-377077999d72/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_23_24.1111541912, ownerUser: 0, ownerGroup: 0 kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-server-2024-07-03-08-16-12.pem, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-client-2024-07-03-08-16-06.pem, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes kubelet KubeConfig must have file permissions set to 644 or more restrictive (MEDIUM 242452)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected permissions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/kubeconfig-real, permissions: 600 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/config/kubelet, permissions: 644 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/kubeconfig-real, permissions: 600 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/config/kubelet, permissions: 644 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/kubeconfig-real, permissions: 600 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/config/kubelet, permissions: 644 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/kubeconfig-real, permissions: 600 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/config/kubelet, permissions: 644 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes kubelet KubeConfig file must be owned by root (MEDIUM 242453)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected owners&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/kubeconfig-real, ownerUser: 0, ownerGroup: 0 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/config/kubelet, ownerUser: 0, ownerGroup: 0 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/kubeconfig-real, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/config/kubelet, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/kubeconfig-real, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/config/kubelet, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: fileName: /var/lib/kubelet/kubeconfig-real, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>details: fileName: /var/lib/kubelet/config/kubelet, ownerUser: 0, ownerGroup: 0 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes etcd must have file permissions set to 644 or more restrictive (MEDIUM 242459)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected permissions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/new.etcd/member/snap/db, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/new.etcd/member/wal/0.tmp, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/safe_guard, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/new.etcd/member/snap/db, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/new.etcd/member/wal/0.tmp, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-4bb07ef9-2ee4-4dd9-8e60-3770579e2433/mount/safe_guard, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/new.etcd/member/snap/db, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/new.etcd/member/wal/0.tmp, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/safe_guard, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/new.etcd/member/snap/db, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/new.etcd/member/wal/0.tmp, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~csi/pv-shoot--garden--aws-ha-eu2-c841e221-2ca7-4227-829d-09a70a32a70d/mount/safe_guard, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/safe_guard, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/new.etcd/member/wal/0.tmp, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/new.etcd/member/snap/db, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/safe_guard, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/new.etcd/member/wal/0.tmp, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~csi/pv-shoot--garden--az-ha-eu1-77d62819-8fac-4597-a3c1-3b8056c59ad2/mount/new.etcd/member/snap/db, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/safe_guard, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/new.etcd/member/wal/0.tmp, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/new.etcd/member/snap/db, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/safe_guard, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/new.etcd/member/wal/0.tmp, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~csi/pv--688bf1da-60cd-4f1c-8dc7-27a7c514dc07/mount/new.etcd/member/snap/db, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/safe_guard, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/new.etcd/member/wal/0.tmp, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/new.etcd/member/snap/db, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/safe_guard, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/new.etcd/member/wal/0.tmp, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~csi/pv--14d65105-4267-4b28-9dc2-85aebab4b88f/mount/new.etcd/member/snap/db, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/safe_guard, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/new.etcd/member/snap/db, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/new.etcd/member/wal/0.tmp, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/safe_guard, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/new.etcd/member/snap/db, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-064a55af-674a-4a4d-b911-6bde960d1fcd/mount/new.etcd/member/wal/0.tmp, permissions: 600 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/new.etcd/member/wal/0.tmp, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/new.etcd/member/snap/db, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/safe_guard, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/new.etcd/member/wal/0000000000000000-0000000000000000.wal, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/new.etcd/member/wal/0.tmp, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/new.etcd/member/snap/db, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~csi/pv-shoot--garden--cc-ha-eu1-19514def-1185-4deb-892f-9064acabcfdf/mount/safe_guard, permissions: 600 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes admin.conf must have file permissions set to 644 or more restrictive (MEDIUM 242460)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected permissions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_10_40.132223152/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_10_40.160858789/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_10_40.1963177894/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/server/..2024_07_03_08_10_40.3470744456/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/server/..2024_07_03_08_10_40.3470744456/tls.key, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~configmap/audit-policy-config/..2024_07_03_08_10_40.98956035/audit-policy.yaml, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_10_40.247189818/podsecurity.yaml, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_10_40.247189818/admission-configuration.yaml, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca/..2024_07_03_08_10_40.729577280/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/etcd-encryption-secret/..2024_07_03_08_10_40.4281367363/encryption-configuration.yaml, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_10_40.2692570695/id_rsa, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_10_40.621084939/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_10_40.3790343357/bundle.key, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~configmap/egress-selection-config/..2024_07_03_08_10_40.2649852811/egress-selector-configuration.yaml, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_10_40.320446039/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_10_40.320446039/tls.key, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_10_40.2516322784/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_10_40.2516322784/tls.key, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/static-token/..2024_07_03_08_10_40.2560086837/static_tokens.csv, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_10_40.3019948839/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_10_40.3019948839/tls.key, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_10_40.2969198131/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_10_40.2969198131/tls.key, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_10_40.2969198131/ca.crt, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_10_40.3729800005/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_10_40.3729800005/tls.key, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_10_40.3422263292/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca/..2024_07_03_08_11_54.3036196029/bundle.crt, permissions: 644 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_54.4201085789/ca.crt, permissions: 640 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_54.4201085789/ca.key, permissions: 640 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_11_54.3862694996/id_rsa, permissions: 640 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_54.3440466664/tls.key, permissions: 640 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_54.3440466664/tls.crt, permissions: 640 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_54.2354965601/ca.key, permissions: 640 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_54.2354965601/ca.crt, permissions: 640 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_12_01.3715611058/kubeconfig, permissions: 644 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_12_01.3715611058/token, permissions: 644 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_20_09.1326916364/bundle.crt, permissions: 644 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_20_09.2623235253/tls.crt, permissions: 640 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_20_09.2623235253/tls.key, permissions: 640 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~configmap/kube-scheduler-config/..2024_07_03_08_20_09.1466398819/config.yaml, permissions: 644 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_20_09.314874445/kubeconfig, permissions: 644 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_20_09.314874445/token, permissions: 644 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/989c6acf-eb15-4f9f-88b0-02efd3fb9b24/volumes/kubernetes.io~secret/ca/..2024_07_03_08_11_06.1594021858/bundle.crt, permissions: 644 kind: pod name: kube-controller-manager-74b87bf4b8-prbhk namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/989c6acf-eb15-4f9f-88b0-02efd3fb9b24/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_06.1299517339/ca.crt, permissions: 640 kind: pod name: kube-controller-manager-74b87bf4b8-prbhk namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/989c6acf-eb15-4f9f-88b0-02efd3fb9b24/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_06.1299517339/ca.key, permissions: 640 kind: pod name: kube-controller-manager-74b87bf4b8-prbhk namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/989c6acf-eb15-4f9f-88b0-02efd3fb9b24/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_11_06.3969459318/id_rsa, permissions: 640 kind: pod name: kube-controller-manager-74b87bf4b8-prbhk namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/989c6acf-eb15-4f9f-88b0-02efd3fb9b24/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_06.1037796146/tls.crt, permissions: 640 kind: pod name: kube-controller-manager-74b87bf4b8-prbhk namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/989c6acf-eb15-4f9f-88b0-02efd3fb9b24/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_06.1037796146/tls.key, permissions: 640 kind: pod name: kube-controller-manager-74b87bf4b8-prbhk namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/989c6acf-eb15-4f9f-88b0-02efd3fb9b24/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_06.1829169503/ca.crt, permissions: 640 kind: pod name: kube-controller-manager-74b87bf4b8-prbhk namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/989c6acf-eb15-4f9f-88b0-02efd3fb9b24/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_06.1829169503/ca.key, permissions: 640 kind: pod name: kube-controller-manager-74b87bf4b8-prbhk namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/989c6acf-eb15-4f9f-88b0-02efd3fb9b24/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_11_09.708513906/kubeconfig, permissions: 644 kind: pod name: kube-controller-manager-74b87bf4b8-prbhk namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/989c6acf-eb15-4f9f-88b0-02efd3fb9b24/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_11_09.708513906/token, permissions: 644 kind: pod name: kube-controller-manager-74b87bf4b8-prbhk namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/a0732fc0-9689-46eb-8b84-8b76e9d7b28d/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_11_23.4144571353/bundle.crt, permissions: 644 kind: pod name: kube-scheduler-7c8bb6bb7b-l92xf namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/a0732fc0-9689-46eb-8b84-8b76e9d7b28d/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_11_23.794806260/tls.key, permissions: 640 kind: pod name: kube-scheduler-7c8bb6bb7b-l92xf namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/a0732fc0-9689-46eb-8b84-8b76e9d7b28d/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_11_23.794806260/tls.crt, permissions: 640 kind: pod name: kube-scheduler-7c8bb6bb7b-l92xf namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/a0732fc0-9689-46eb-8b84-8b76e9d7b28d/volumes/kubernetes.io~configmap/kube-scheduler-config/..2024_07_03_08_11_23.513054187/config.yaml, permissions: 644 kind: pod name: kube-scheduler-7c8bb6bb7b-l92xf namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/a0732fc0-9689-46eb-8b84-8b76e9d7b28d/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_11_23.1803684866/kubeconfig, permissions: 644 kind: pod name: kube-scheduler-7c8bb6bb7b-l92xf namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/a0732fc0-9689-46eb-8b84-8b76e9d7b28d/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_11_23.1803684866/token, permissions: 644 kind: pod name: kube-scheduler-7c8bb6bb7b-l92xf namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_22_12.2221992855/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_22_12.3473324769/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_22_12.2280157390/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/server/..2024_07_03_08_22_12.33202829/tls.crt, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/server/..2024_07_03_08_22_12.33202829/tls.key, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~configmap/audit-policy-config/..2024_07_03_08_22_12.2179539215/audit-policy.yaml, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_22_12.2514111652/podsecurity.yaml, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_22_12.2514111652/admission-configuration.yaml, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca/..2024_07_03_08_22_12.651057513/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/etcd-encryption-secret/..2024_07_03_08_22_12.3174518015/encryption-configuration.yaml, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_22_12.667135894/id_rsa, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_22_12.772465054/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_22_12.2456873837/bundle.key, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~configmap/egress-selection-config/..2024_07_03_08_22_12.2465776616/egress-selector-configuration.yaml, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_22_12.1108572537/tls.crt, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_22_12.1108572537/tls.key, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_22_12.3056046342/tls.crt, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_22_12.3056046342/tls.key, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/static-token/..2024_07_03_08_22_12.3720566324/static_tokens.csv, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_22_12.784723719/tls.crt, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_22_12.784723719/tls.key, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_22_12.2019313851/ca.crt, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_22_12.2019313851/tls.crt, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_22_12.2019313851/tls.key, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_22_12.3723569969/tls.crt, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_22_12.3723569969/tls.key, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_22_12.2673815035/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_19_06.1353495674/bundle.crt, permissions: 644 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_19_06.1317647725/tls.key, permissions: 640 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_19_06.1317647725/tls.crt, permissions: 640 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~configmap/kube-scheduler-config/..2024_07_03_08_19_06.953683874/config.yaml, permissions: 644 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_19_06.4011589525/token, permissions: 644 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_19_06.4011589525/kubeconfig, permissions: 644 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca/..2024_07_03_08_11_01.1416082481/bundle.crt, permissions: 644 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_01.2822029985/ca.crt, permissions: 640 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_01.2822029985/ca.key, permissions: 640 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_11_01.3378265558/id_rsa, permissions: 640 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_01.424413895/tls.key, permissions: 640 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_01.424413895/tls.crt, permissions: 640 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_01.3157557822/ca.crt, permissions: 640 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_01.3157557822/ca.key, permissions: 640 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_11_04.2255727832/token, permissions: 644 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_11_04.2255727832/kubeconfig, permissions: 644 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_17_06.1421105491/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_17_06.1778852592/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_17_06.3754135056/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/server/..2024_07_03_08_17_06.2728615182/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/server/..2024_07_03_08_17_06.2728615182/tls.key, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~configmap/audit-policy-config/..2024_07_03_08_17_06.3429146079/audit-policy.yaml, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_17_06.953443819/podsecurity.yaml, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_17_06.953443819/admission-configuration.yaml, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca/..2024_07_03_08_17_06.11797511/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/etcd-encryption-secret/..2024_07_03_08_17_06.1577058416/encryption-configuration.yaml, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_17_06.3648435760/id_rsa, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_17_06.2297979827/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_17_06.820126748/bundle.key, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~configmap/egress-selection-config/..2024_07_03_08_17_06.639129834/egress-selector-configuration.yaml, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_17_06.1849336632/tls.key, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_17_06.1849336632/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_17_06.4153675152/tls.key, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_17_06.4153675152/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/static-token/..2024_07_03_08_17_06.790953682/static_tokens.csv, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_17_06.2975900753/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_17_06.2975900753/tls.key, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_17_06.2888946878/ca.crt, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_17_06.2888946878/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_17_06.2888946878/tls.key, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_17_06.3608837884/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_17_06.3608837884/tls.key, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_17_06.2907287084/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca/..2024_07_03_08_15_39.1644055289/bundle.crt, permissions: 644 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_15_39.4292111706/ca.key, permissions: 640 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_15_39.4292111706/ca.crt, permissions: 640 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_15_39.694420208/id_rsa, permissions: 640 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/server/..2024_07_03_08_15_39.3129979181/tls.key, permissions: 640 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/server/..2024_07_03_08_15_39.3129979181/tls.crt, permissions: 640 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_15_39.4082412295/ca.crt, permissions: 640 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_15_39.4082412295/ca.key, permissions: 640 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_15_39.3235024203/token, permissions: 644 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_15_39.3235024203/kubeconfig, permissions: 644 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_17_40.1915895115/bundle.crt, permissions: 644 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_17_40.1063558951/tls.crt, permissions: 640 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_17_40.1063558951/tls.key, permissions: 640 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~configmap/kube-scheduler-config/..2024_07_03_08_17_40.2246557176/config.yaml, permissions: 644 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_17_40.4079250913/kubeconfig, permissions: 644 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~projected/kubeconfig/..2024_07_03_08_17_40.4079250913/token, permissions: 644 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_28_38.1760636453/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_28_38.212112882/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_28_38.1565717575/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/server/..2024_07_03_08_28_38.3505195174/tls.key, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/server/..2024_07_03_08_28_38.3505195174/tls.crt, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~configmap/audit-policy-config/..2024_07_03_08_28_38.330514936/audit-policy.yaml, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_28_38.901497533/podsecurity.yaml, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~configmap/admission-config/..2024_07_03_08_28_38.901497533/admission-configuration.yaml, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca/..2024_07_03_08_28_38.35631685/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/etcd-encryption-secret/..2024_07_03_08_28_38.1969276561/encryption-configuration.yaml, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/service-account-key/..2024_07_03_08_28_38.397370216/id_rsa, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_28_38.515039494/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_28_38.1025349512/bundle.key, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~configmap/egress-selection-config/..2024_07_03_08_28_38.3989665750/egress-selector-configuration.yaml, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_28_38.2197025418/tls.crt, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_28_38.2197025418/tls.key, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_28_38.1734894870/tls.crt, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_28_38.1734894870/tls.key, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/static-token/..2024_07_03_08_28_38.1327549647/static_tokens.csv, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_28_38.673160524/tls.crt, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_28_38.673160524/tls.key, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_28_38.2037804618/ca.crt, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_28_38.2037804618/tls.crt, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_28_38.2037804618/tls.key, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_28_38.2931148428/tls.crt, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_28_38.2931148428/tls.key, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_28_38.2236084738/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API Server audit logs must be enabled (MEDIUM 242461)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option audit-policy-file set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes PKI CRT must have file permissions set to 644 or more restrictive (MEDIUM 242466)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected permissions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_08.4131794839/bundle.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/ca.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_08.4131794839/bundle.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/ca.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_09.1557393859/bundle.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/ca.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_09.1557393859/bundle.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/ca.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_20_09.1326916364/bundle.crt, permissions: 644 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_20_09.2623235253/tls.crt, permissions: 640 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_10_40.132223152/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_10_40.160858789/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_10_40.1963177894/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/server/..2024_07_03_08_10_40.3470744456/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca/..2024_07_03_08_10_40.729577280/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_10_40.621084939/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_10_40.320446039/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_10_40.2516322784/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_10_40.3019948839/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_10_40.2969198131/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_10_40.2969198131/ca.crt, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_10_40.3729800005/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_10_40.3422263292/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca/..2024_07_03_08_11_54.3036196029/bundle.crt, permissions: 644 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_54.4201085789/ca.crt, permissions: 640 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_54.3440466664/tls.crt, permissions: 640 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_54.2354965601/ca.crt, permissions: 640 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: shoot containerName: kube-proxy details: fileName: /var/lib/kubelet/pods/53c2c7c8-feb4-4269-9e5e-923d20a955d0/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_20.3132223426/ca.crt, permissions: 644 kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: conntrack-fix details: fileName: /var/lib/kubelet/pods/53c2c7c8-feb4-4269-9e5e-923d20a955d0/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_20.3132223426/ca.crt, permissions: 644 kind: pod name: kube-proxy-worker-kkfk1-v1.28.9-qpjpc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-client-2024-07-03-08-14-06.pem, permissions: 600 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-server-2024-07-03-08-14-07.pem, permissions: 600 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_22_12.2221992855/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_22_12.3473324769/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_22_12.2280157390/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/server/..2024_07_03_08_22_12.33202829/tls.crt, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca/..2024_07_03_08_22_12.651057513/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_22_12.772465054/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_22_12.1108572537/tls.crt, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_22_12.3056046342/tls.crt, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_22_12.784723719/tls.crt, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_22_12.2019313851/ca.crt, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_22_12.2019313851/tls.crt, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_22_12.3723569969/tls.crt, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_22_12.2673815035/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_25.60881411/bundle.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/ca.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_25.60881411/bundle.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/ca.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: shoot containerName: kube-proxy details: fileName: /var/lib/kubelet/pods/058cd0f1-151a-41c8-b208-938fddcbe984/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_24_30.478337316/ca.crt, permissions: 644 kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: conntrack-fix details: fileName: /var/lib/kubelet/pods/058cd0f1-151a-41c8-b208-938fddcbe984/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_24_30.478337316/ca.crt, permissions: 644 kind: pod name: kube-proxy-worker-g7p4p-vIP-Addressrb5l namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-server-2024-07-03-08-14-08.pem, permissions: 600 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-client-2024-07-03-08-14-05.pem, permissions: 600 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_19_06.1353495674/bundle.crt, permissions: 644 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_19_06.1317647725/tls.crt, permissions: 640 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_39.2700028413/bundle.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/ca.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_39.2700028413/bundle.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/ca.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_38.1180291661/bundle.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/ca.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_38.1180291661/bundle.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/ca.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_17_06.1421105491/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_17_06.1778852592/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_17_06.3754135056/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/server/..2024_07_03_08_17_06.2728615182/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca/..2024_07_03_08_17_06.11797511/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_17_06.2297979827/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_17_06.1849336632/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_17_06.4153675152/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_17_06.2975900753/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_17_06.2888946878/ca.crt, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_17_06.2888946878/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_17_06.3608837884/tls.crt, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_17_06.2907287084/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca/..2024_07_03_08_11_01.1416082481/bundle.crt, permissions: 644 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_01.2822029985/ca.crt, permissions: 640 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_01.424413895/tls.crt, permissions: 640 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_01.3157557822/ca.crt, permissions: 640 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: shoot containerName: conntrack-fix details: fileName: /var/lib/kubelet/pods/69199841-8a91-4bed-ac4c-b39e2c0d38fd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_12_54.4164575669/ca.crt, permissions: 644 kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: kube-proxy details: fileName: /var/lib/kubelet/pods/69199841-8a91-4bed-ac4c-b39e2c0d38fd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_12_54.4164575669/ca.crt, permissions: 644 kind: pod name: kube-proxy-worker-bex82-v1.28.9-kl89k namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-server-2024-07-03-08-12-53.pem, permissions: 600 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-client-2024-07-03-08-12-51.pem, permissions: 600 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_40.1257318752/bundle.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/ca.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_40.1257318752/bundle.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/ca.crt, permissions: 644 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca/..2024_07_03_08_15_39.1644055289/bundle.crt, permissions: 644 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_15_39.4292111706/ca.crt, permissions: 640 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/server/..2024_07_03_08_15_39.3129979181/tls.crt, permissions: 640 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_15_39.4082412295/ca.crt, permissions: 640 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~projected/client-ca/..2024_07_03_08_17_40.1915895115/bundle.crt, permissions: 644 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_17_40.1063558951/tls.crt, permissions: 640 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_39.3622463902/bundle.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/ca.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_39.3622463902/bundle.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/ca.crt, permissions: 644 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_28_38.1760636453/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_28_38.212112882/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-front-proxy/..2024_07_03_08_28_38.1565717575/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/server/..2024_07_03_08_28_38.3505195174/tls.crt, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca/..2024_07_03_08_28_38.35631685/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-vpn/..2024_07_03_08_28_38.515039494/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_28_38.2197025418/tls.crt, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_28_38.1734894870/tls.crt, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_28_38.673160524/tls.crt, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_28_38.2037804618/ca.crt, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_28_38.2037804618/tls.crt, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_28_38.2931148428/tls.crt, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/ca-etcd/..2024_07_03_08_28_38.2236084738/bundle.crt, permissions: 644 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: shoot containerName: kube-proxy details: fileName: /var/lib/kubelet/pods/aff2acf3-c4c2-45d1-b80f-377077999d72/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_23_24.1111541912/ca.crt, permissions: 644 kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot containerName: conntrack-fix details: fileName: /var/lib/kubelet/pods/aff2acf3-c4c2-45d1-b80f-377077999d72/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_23_24.1111541912/ca.crt, permissions: 644 kind: pod name: kube-proxy-worker-dqty2-vIP-Addressjts9 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-server-2024-07-03-08-16-12.pem, permissions: 600 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-client-2024-07-03-08-16-06.pem, permissions: 600 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes PKI keys must have file permissions set to 600 or more restrictive (MEDIUM 242467)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has expected permissions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/f0e239bd-d47c-4b4b-9c00-b0773aac1fc9/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_20_09.2623235253/tls.key, permissions: 640 kind: pod name: kube-scheduler-6d6cb9bccd-9fxx6 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/server/..2024_07_03_08_10_40.3470744456/tls.key, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_10_40.3790343357/bundle.key, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_10_40.320446039/tls.key, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_10_40.2516322784/tls.key, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_10_40.3019948839/tls.key, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_10_40.2969198131/tls.key, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/88a1cac5-35cc-41ad-aa6b-7d3b054e5d5d/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_10_40.3729800005/tls.key, permissions: 640 kind: pod name: kube-apiserver-5fcbcf7dc7-bsl6w namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_54.4201085789/ca.key, permissions: 640 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_54.3440466664/tls.key, permissions: 640 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/16bc0ebb-9186-45c6-8024-5bbb373b6e04/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_54.2354965601/ca.key, permissions: 640 kind: pod name: kube-controller-manager-75dbc5fd68-kndbg namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-client-2024-07-03-08-14-06.pem, permissions: 600 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-server-2024-07-03-08-14-07.pem, permissions: 600 kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/server/..2024_07_03_08_22_12.33202829/tls.key, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_22_12.2456873837/bundle.key, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_22_12.1108572537/tls.key, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_22_12.3056046342/tls.key, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_22_12.784723719/tls.key, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_22_12.2019313851/tls.key, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/1705c009-bdd3-46b2-a2b7-a818f588d694/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_22_12.3723569969/tls.key, permissions: 640 kind: pod name: kube-apiserver-66fc77c89f-tfxg2 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-server-2024-07-03-08-14-08.pem, permissions: 600 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-client-2024-07-03-08-14-05.pem, permissions: 600 kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/server/..2024_07_03_08_17_06.2728615182/tls.key, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_17_06.820126748/bundle.key, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_17_06.1849336632/tls.key, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_17_06.4153675152/tls.key, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_17_06.2975900753/tls.key, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_17_06.2888946878/tls.key, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/182904ae-205c-4d67-ab29-3ea90f65f1dd/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_17_06.3608837884/tls.key, permissions: 640 kind: pod name: kube-apiserver-5dd44867c5-cmnzd namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_11_01.2822029985/ca.key, permissions: 640 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/server/..2024_07_03_08_11_01.424413895/tls.key, permissions: 640 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/24dbbb37-ce87-4b08-afaa-e90c41a0cd45/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_11_01.3157557822/ca.key, permissions: 640 kind: pod name: kube-controller-manager-8ddd5b977-ks92t namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/4965b574-696d-475e-b603-72ba8ac438f5/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_19_06.1317647725/tls.key, permissions: 640 kind: pod name: kube-scheduler-5b6467b48d-drq9z namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-server-2024-07-03-08-12-53.pem, permissions: 600 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-client-2024-07-03-08-12-51.pem, permissions: 600 kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-client/..2024_07_03_08_15_39.4292111706/ca.key, permissions: 640 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/server/..2024_07_03_08_15_39.3129979181/tls.key, permissions: 640 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-controller-manager details: fileName: /var/lib/kubelet/pods/8fa683c6-1744-44aa-a5ce-51fe03e5fe51/volumes/kubernetes.io~secret/ca-kubelet/..2024_07_03_08_15_39.4082412295/ca.key, permissions: 640 kind: pod name: kube-controller-manager-5b69b7b686-28bd2 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-scheduler details: fileName: /var/lib/kubelet/pods/9fcd0f72-9556-4e70-9747-53fad55dc06b/volumes/kubernetes.io~secret/kube-scheduler-server/..2024_07_03_08_17_40.1063558951/tls.key, permissions: 640 kind: pod name: kube-scheduler-545856f7d4-948wf namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/server/..2024_07_03_08_28_38.3505195174/tls.key, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/service-account-key-bundle/..2024_07_03_08_28_38.1025349512/bundle.key, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kube-aggregator/..2024_07_03_08_28_38.2197025418/tls.key, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/kubelet-client/..2024_07_03_08_28_38.1734894870/tls.key, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/http-proxy/..2024_07_03_08_28_38.673160524/tls.key, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/tls-sni-0/..2024_07_03_08_28_38.2037804618/tls.key, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: kube-apiserver details: fileName: /var/lib/kubelet/pods/8d9df1c2-b858-4bfc-b174-1853e9e068e5/volumes/kubernetes.io~secret/etcd-client/..2024_07_03_08_28_38.2931148428/tls.key, permissions: 640 kind: pod name: kube-apiserver-74ff95ff84-kbnvb namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-server-2024-07-03-08-16-12.pem, permissions: 600 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>cluster: shoot details: fileName: /var/lib/kubelet/pki/kubelet-client-2024-07-03-08-16-06.pem, permissions: 600 kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes Kubelet must not disable timeouts (MEDIUM 245541)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option streamingConnectionIdleTimeout set to allowed value.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;li>kind: node name: ip-IP-Address.eu-west-1.compute.internal &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-92nxw &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--azure-worker-g7p4p-z3-85cdb-rdpvb &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-fd7wq &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--gcp-worker-bex82-z1-fcc94-klgpz &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-tth2l &lt;/li>
&lt;li>kind: node name: shoot--diki-comp--openstack-worker-dqty2-z1-57956-xzs4w &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes API Server must disable basic authentication to protect information in transit (HIGH 245542)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option basic-auth-file has not been set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes endpoints must use approved organizational certificate and key pair to protect information in transit (HIGH 245544)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option kubelet-client-certificate set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option kubelet-client-key set.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes must have a Pod Security Admission control file configured (HIGH 254800)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">PodSecurity is properly configured&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: PodSecurityConfiguration &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: PodSecurityConfiguration &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: PodSecurityConfiguration &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: PodSecurityConfiguration &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;ul class="tw-list-inside tw-pl-2">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-text-lg tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-text-lg">&amp;#128309 Skipped&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes etcd must use TLS to protect the confidentiality of sensitive data during electronic dissemination (MEDIUM 242380)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">ETCD runs as a single instance, peer communication options are not used.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes Scheduler must have secure binding (MEDIUM 242384)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">The Kubernetes Scheduler runs in a container which already has limited access to network interfaces. In addition ingress traffic to the Kubernetes Scheduler is restricted via network policies, making an unintended exposure less likely.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes Controller Manager must have secure binding (MEDIUM 242385)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">The Kubernetes Controller Manager runs in a container which already has limited access to network interfaces. In addition ingress traffic to the Kubernetes Controller Manager is restricted via network policies, making an unintended exposure less likely.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes Kubectl cp command must give expected access and results (MEDIUM 242396)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">&amp;#34;kubectl&amp;#34; is not installed into control plane pods or worker nodes and Gardener does not offer Kubernetes v1.12 or older.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes DynamicAuditing must not be enabled (MEDIUM 242398)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option feature-gates.DynamicAuditing removed in Kubernetes v1.19.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes DynamicKubeletConfig must not be enabled (MEDIUM 242399)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option featureGates.DynamicKubeletConfig removed in Kubernetes v1.26.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: Used Kubernetes version 1.28.9. &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: Used Kubernetes version 1.28.9. &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: Used Kubernetes version 1.28.9. &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>details: Used Kubernetes version 1.28.9. &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes manifests must be owned by root (MEDIUM 242405)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Gardener does not deploy any control plane component as systemd processes or static pod.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes manifest files must have least privileges (MEDIUM 242408)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Gardener does not deploy any control plane component as systemd processes or static pod.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API Server must enforce ports, protocols, and services (PPS) that adhere to the Ports, Protocols, and Services Management Category Assurance List (PPSM CAL) (MEDIUM 242410)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Cannot be tested and should be enforced organizationally. Gardener uses a minimum of known and automatically opened/used/created ports/protocols/services (PPSM stands for Ports, Protocols, Service Management).&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes Scheduler must enforce ports, protocols, and services (PPS) that adhere to the Ports, Protocols, and Services Management Category Assurance List (PPSM CAL) (MEDIUM 242411)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Cannot be tested and should be enforced organizationally. Gardener uses a minimum of known and automatically opened/used/created ports/protocols/services (PPSM stands for Ports, Protocols, Service Management).&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes Controllers must enforce ports, protocols, and services (PPS) that adhere to the Ports, Protocols, and Services Management Category Assurance List (PPSM CAL) (MEDIUM 242412)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Cannot be tested and should be enforced organizationally. Gardener uses a minimum of known and automatically opened/used/created ports/protocols/services (PPSM stands for Ports, Protocols, Service Management).&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes etcd must enforce ports, protocols, and services (PPS) that adhere to the Ports, Protocols, and Services Management Category Assurance List (PPSM CAL) (MEDIUM 242413)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Cannot be tested and should be enforced organizationally. Gardener uses a minimum of known and automatically opened/used/created ports/protocols/services (PPSM stands for Ports, Protocols, Service Management).&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes etcd must enable client authentication to secure service (MEDIUM 242426)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">ETCD runs as a single instance, peer communication options are not used.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes etcd must have peer-cert-file set for secure communication (MEDIUM 242432)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">ETCD runs as a single instance, peer communication options are not used.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes etcd must have a peer-key-file set for secure communication (MEDIUM 242433)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">ETCD runs as a single instance, peer communication options are not used.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: statefulSet name: etcd-main namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>kind: statefulSet name: etcd-events namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes must have a pod security policy set (HIGH 242437)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">PSPs are removed in K8s version 1.25.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes must contain the latest updates as authorized by IAVMs, CTOs, DTMs, and STIGs (MEDIUM 242443)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Scanning/patching security vulnerabilities should be enforced organizationally. Security vulnerability scanning should be automated and maintainers should be informed automatically.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes component manifests must be owned by root (MEDIUM 242444)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Rule is duplicate of &amp;#34;242405&amp;#34;&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes kubeadm.conf must be owned by root(MEDIUM 242454)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Gardener does not use &amp;#34;kubeadm&amp;#34; and also does not store any &amp;#34;main config&amp;#34; anywhere in seed or shoot (flow/component logic built-in/in-code).&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes kubeadm.conf must have file permissions set to 644 or more restrictive (MEDIUM 242455)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Gardener does not use &amp;#34;kubeadm&amp;#34; and also does not store any &amp;#34;main config&amp;#34; anywhere in seed or shoot (flow/component logic built-in/in-code).&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes kubelet config must have file permissions set to 644 or more restrictive (MEDIUM 242456)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Rule is duplicate of &amp;#34;242452&amp;#34;.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes kubelet config must be owned by root (MEDIUM 242457)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Rule is duplicate of &amp;#34;242453&amp;#34;.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes API Server audit log path must be set (MEDIUM 242465)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Rule is duplicate of &amp;#34;242402&amp;#34;&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes must enable PodSecurity admission controller on static pods and Kubelets (HIGH 254801)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Option featureGates.PodSecurity was made GA in v1.25 and removed in v1.28.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;ul class="tw-list-inside tw-pl-2">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-text-lg tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-text-lg">&amp;#128309 Accepted&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API Server must have an audit log path set (MEDIUM 242402)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Gardener can integrate with different audit logging solutions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API Server must generate audit records that identify what type of event has occurred, identify the source of the event, contain the event results, identify any users, and identify any containers associated with the event (MEDIUM 242403)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Gardener can integrate with different audit logging solutions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes cluster must use non-privileged host ports for user pods (MEDIUM 242414)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">node local dns requires port 53 in order to operate properly&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-5648f namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-5648f namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-mwvrg namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-mwvrg namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-j5l82 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-j5l82 namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-nczzd namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-nczzd namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-fc8nx namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-fc8nx namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-pm55v namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-pm55v namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-grlsc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-grlsc namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-qhngh namespace: kube-system &lt;/li>
&lt;li>cluster: shoot details: containerName: node-cache, port: 53 kind: pod name: node-local-dns-qhngh namespace: kube-system &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API Server must be set to audit log max size (MEDIUM 242462)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Gardener can integrate with different audit logging solutions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API Server must be set to audit log maximum backup (MEDIUM 242463)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Gardener can integrate with different audit logging solutions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes API Server audit log retention must be set (MEDIUM 242464)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Gardener can integrate with different audit logging solutions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Kubernetes API Server must disable token authentication to protect information in transit (HIGH 245543)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">All defined tokens are accepted.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: deployment name: kube-apiserver namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;ul class="tw-list-inside tw-pl-2">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-text-lg tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-text-lg">&amp;#128992 Warning&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes component etcd must be owned by etcd (MEDIUM 242445)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Reference group cannot be tested since all pods of the group are scheduled on a fully allocated node.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: referenceGroup name: etcd-events uid: c5c370cf-2f8a-4fc5-988c-7a0d54a7ebcf &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes conf files must be owned by root (MEDIUM 242446)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Reference group cannot be tested since all pods of the group are scheduled on a fully allocated node.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: referenceGroup name: kube-controller-manager-74b87bf4b8 uid: b7b065f3-7128-40cb-b902-3f1c47c96c1b &lt;/li>
&lt;li>kind: referenceGroup name: kube-scheduler-7c8bb6bb7b uid: 8cca2ffd-b3e8-4499-93c1-c0ae3570fdd1 &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes component PKI must be owned by root (MEDIUM 242451)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Reference group cannot be tested since all pods of the group are scheduled on a fully allocated node.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: referenceGroup name: etcd-events uid: c5c370cf-2f8a-4fc5-988c-7a0d54a7ebcf &lt;/li>
&lt;li>cluster: seed kind: referenceGroup name: kube-controller-manager-74b87bf4b8 uid: b7b065f3-7128-40cb-b902-3f1c47c96c1b &lt;/li>
&lt;li>cluster: seed kind: referenceGroup name: kube-scheduler-7c8bb6bb7b uid: 8cca2ffd-b3e8-4499-93c1-c0ae3570fdd1 &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes etcd must have file permissions set to 644 or more restrictive (MEDIUM 242459)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Reference group cannot be tested since all pods of the group are scheduled on a fully allocated node.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>kind: referenceGroup name: etcd-events uid: c5c370cf-2f8a-4fc5-988c-7a0d54a7ebcf &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes PKI CRT must have file permissions set to 644 or more restrictive (MEDIUM 242466)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Reference group cannot be tested since all pods of the group are scheduled on a fully allocated node.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: referenceGroup name: etcd-events uid: c5c370cf-2f8a-4fc5-988c-7a0d54a7ebcf &lt;/li>
&lt;li>cluster: seed kind: referenceGroup name: kube-controller-manager-74b87bf4b8 uid: b7b065f3-7128-40cb-b902-3f1c47c96c1b &lt;/li>
&lt;li>cluster: seed kind: referenceGroup name: kube-scheduler-7c8bb6bb7b uid: 8cca2ffd-b3e8-4499-93c1-c0ae3570fdd1 &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes PKI keys must have file permissions set to 600 or more restrictive (MEDIUM 242467)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Reference group cannot be tested since all pods of the group are scheduled on a fully allocated node.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed kind: referenceGroup name: etcd-events uid: c5c370cf-2f8a-4fc5-988c-7a0d54a7ebcf &lt;/li>
&lt;li>cluster: seed kind: referenceGroup name: kube-controller-manager-74b87bf4b8 uid: b7b065f3-7128-40cb-b902-3f1c47c96c1b &lt;/li>
&lt;li>cluster: seed kind: referenceGroup name: kube-scheduler-7c8bb6bb7b uid: 8cca2ffd-b3e8-4499-93c1-c0ae3570fdd1 &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;ul class="tw-list-inside tw-pl-2">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-text-lg tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-text-lg">&amp;#128308 Failed&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">Secrets in Kubernetes must not be stored as environment variables (HIGH 242415)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">Pod uses environment to inject secret.&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed details: containerName: backup-restore, variableName: GOOGLE_STORAGE_API_ENDPOINT, keyRef: storageAPIEndpoint kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes etcd must have file permissions set to 644 or more restrictive (MEDIUM 242459)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has too wide permissions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_08.4131794839/bundle.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/token, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/ca.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/namespace, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_08.1753711665/accessKeyID, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_08.1753711665/bucketName, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_08.1753711665/region, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_08.1753711665/secretAccessKey, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~configmap/etcd-config-file/..2024_07_03_08_13_08.1092397241/etcd.conf.yaml, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_08.4131794839/bundle.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/token, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/ca.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_08.3333908601/namespace, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_09.1557393859/bundle.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/namespace, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/token, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/ca.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~configmap/etcd-config-file/..2024_07_03_08_14_09.892466712/etcd.conf.yaml, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_09.1557393859/bundle.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/namespace, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/token, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_09.2976971247/ca.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_25.60881411/bundle.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/token, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/ca.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/namespace, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_08_25.694960980/storageKey, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_08_25.694960980/bucketName, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_08_25.694960980/storageAccount, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~configmap/etcd-config-file/..2024_07_03_08_08_25.2056088153/etcd.conf.yaml, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_25.60881411/bundle.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/token, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/ca.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_25.1464822221/namespace, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_39.2700028413/bundle.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/token, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/ca.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/namespace, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_08_39.2704609432/bucketName, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_08_39.2704609432/serviceaccount.json, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~configmap/etcd-config-file/..2024_07_03_08_08_39.3391642353/etcd.conf.yaml, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_39.2700028413/bundle.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/token, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/ca.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_39.399790539/namespace, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_38.1180291661/bundle.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/token, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/ca.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/namespace, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~configmap/etcd-config-file/..2024_07_03_08_08_38.340777120/etcd.conf.yaml, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_08_38.1180291661/bundle.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/token, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/ca.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_08_38.1588135457/namespace, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_39.3622463902/bundle.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/ca.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/namespace, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/token, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/applicationCredentialName, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/applicationCredentialSecret, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/authURL, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/bucketName, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/domainName, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/region, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/tenantName, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/etcd-backup/..2024_07_03_08_13_39.3973939140/applicationCredentialID, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~configmap/etcd-config-file/..2024_07_03_08_13_39.1118508487/etcd.conf.yaml, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_13_39.3622463902/bundle.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/ca.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/namespace, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_13_39.340207416/token, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_40.1257318752/bundle.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/namespace, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/token, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/ca.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~configmap/etcd-config-file/..2024_07_03_08_14_40.1087789113/etcd.conf.yaml, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-ca-etcd/..2024_07_03_08_14_40.1257318752/bundle.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/namespace, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/token, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~projected/kube-api-access-gardener/..2024_07_03_08_14_40.3800925646/ca.crt, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-semibold">The Kubernetes PKI keys must have file permissions set to 600 or more restrictive (MEDIUM 242467)&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;button onclick="collapse(event)" class="tw-pr-2">&lt;i
class="arrow right">&lt;/i>&lt;/button>
&lt;span class="tw-font-medium">File has too wide permissions&lt;/span>
&lt;ul class="tw-list-inside tw-pl-5 tw-hidden">
&lt;li>
&lt;span class="tw-font-semibold">aws&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_08.2947585968/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/ae3023ec-2d96-4e5d-85d9-959e3e7016dd/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_08.2392157430/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_09.2031845056/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/49e7dcc5-ee7e-4428-a2c1-3f81831d37eb/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_09.3395855672/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--aws &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">azure&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_25.4248283166/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/8f831ee0-c90d-4e70-9e5b-91eb3fea49c6/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_25.1212701209/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--azure &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">gcp&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_39.1453164624/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/eff146cd-7a58-4d6b-a8f0-d1fcc3d30fad/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_39.1408172618/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_08_38.3152103190/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/2e968378-d2bb-418a-9fd8-04945d95ae1c/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_08_38.1845902704/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--gcp &lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;span class="tw-font-semibold">openstack&lt;/span>
&lt;ul class="tw-list-disc tw-list-inside tw-pl-5">
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_13_39.3087296269/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/1a92a018-cff3-4017-9100-b2ed34fa0bec/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_13_39.1866946220/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-main-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: etcd details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-server-tls/..2024_07_03_08_14_40.1813777235/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;li>cluster: seed containerName: backup-restore details: fileName: /var/lib/kubelet/pods/0d024055-7018-44b3-913a-ef046f5b6a2f/volumes/kubernetes.io~secret/client-url-etcd-client-tls/..2024_07_03_08_14_40.3864464769/tls.key, permissions: 644, expectedPermissionsMax: 640 kind: pod name: etcd-events-0 namespace: shoot--diki-comp--openstack &lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/div>
&lt;/div>
&lt;/div>
&lt;/body>
&lt;/html></description></item><item><title>Docs: Why Kubernetes</title><link>https://gardener.cloud/docs/resources/videos/why-kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/why-kubernetes/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/N6r-9ZzFgzw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="Why Kubernetes">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Regional Restrictions</title><link>https://gardener.cloud/docs/security-and-compliance/regional-restrictions/</link><pubDate>Wed, 22 Nov 2023 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/security-and-compliance/regional-restrictions/</guid><description>
&lt;h2 id="shared-responsibility-model">Shared Responsibility Model&lt;/h2>
&lt;p>Gardener, like most cloud providers&amp;rsquo; Kubernetes offerings, is dedicated for a global setup. And just like how most cloud providers offer means to fulfil regional restrictions, Gardener also has some means built in for this purpose. Similarly, Gardener also follows a shared responsibility model where users are obliged to use the provided Gardener means in a way which results in compliance with regional restrictions.&lt;/p>
&lt;h3 id="regions">Regions&lt;/h3>
&lt;p>Gardener users need to understand that Gardener is a generic tool and has no built-in knowledge about regions as geographical or political conglomerates. For Gardener, regions are only strings. To create regional restrictions is an obligation of all Gardener users who orchestrate existing Gardener functionality to reach evidence which can be audited later on.&lt;/p>
&lt;h3 id="support-for-regional-restrictions">Support for Regional Restrictions&lt;/h3>
&lt;p>Gardener offers functionality to support the most important kind of regional restrictions in its global setup:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>No Restriction:&lt;/strong> All seeds in all regions can be allowed to host the control plane of all shoots.&lt;/li>
&lt;li>&lt;strong>Restriction by Dedication:&lt;/strong> Shoots running in a region can be configured so that only dedicated seeds in dedicated regions are allowed to host the shoot’s control plane. This can be achieved by adding labels to a seed and subsequently restricting shoot control plane placement to appropriately labeled seeds by using the field &lt;code>spec.seedSelector&lt;/code> (&lt;a href="https://github.com/gardener/gardener/blob/v1.84.1/example/90-shoot.yaml#L365-L368">example&lt;/a>).&lt;/li>
&lt;li>&lt;strong>Restriction by Tainting:&lt;/strong> Some seeds running in some dedicated regions are not allowed to host the control plane of any shoots unless explicitly allowed. This can be achieved by tainting seeds appropriately (&lt;a href="https://github.com/gardener/gardener/blob/v1.84.1/example/50-seed.yaml#L86-L88">example&lt;/a>) which in turn requires explicit tolerations if a shoot&amp;rsquo;s control plane should be placed on such tainted seeds (&lt;a href="https://github.com/gardener/gardener/blob/v1.84.1/example/90-shoot.yaml#L360-L361">example&lt;/a>).&lt;/li>
&lt;/ul></description></item><item><title>Docs: High Performance Microservices with Kubernetes, Go, and gRPC</title><link>https://gardener.cloud/docs/resources/videos/microservices-in_kubernetes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/microservices-in_kubernetes/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/YiNt4kUnnIM" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="High Performance Microservices with Kubernetes, Go, and gRPC">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Building Small Containers</title><link>https://gardener.cloud/docs/resources/videos/small-container/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/small-container/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/wGz_cbtCiEA" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="Building Small Containers">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Contributing Bigger Changes</title><link>https://gardener.cloud/docs/contribute/code/contributing-bigger-changes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/contribute/code/contributing-bigger-changes/</guid><description>
&lt;h2 id="contributing-bigger-changes">Contributing Bigger Changes&lt;/h2>
&lt;p>Here are the guidelines you should follow when contributing larger changes to Gardener:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Avoid proposing a big change in one single PR. Instead, split your work into multiple stages which are independently mergeable and create one PR for each stage. For example, if introducing a new API resource and its controller, these stages could be:&lt;/p>
&lt;ul>
&lt;li>API resource types, including defaults and generated code.&lt;/li>
&lt;li>API resource validation.&lt;/li>
&lt;li>API server storage.&lt;/li>
&lt;li>Admission plugin(s), if any.&lt;/li>
&lt;li>Controller(s), including changes to existing controllers. Split this phase further into different functional subsets if appropriate.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>If you realize later that changes to artifacts introduced in a previous stage are required, by all means make them and explain in the PR why they were needed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Consider splitting a big PR further into multiple commits to allow for more focused reviews. For example, you could add unit tests / documentation in separate commits from the rest of the code. If you have to adapt your PR to review feedback, prefer doing that also in a separate commit to make it easier for reviewers to check how their feedback has been addressed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>To make the review process more efficient and avoid too many long discussions in the PR itself, ask for a &amp;ldquo;main reviewer&amp;rdquo; to be assigned to your change, then work with this person to make sure he or she understands it in detail, and agree together on any improvements that may be needed. If you can&amp;rsquo;t reach an agreement on certain topics, comment on the PR and invite other people to join the discussion.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Even if you have a &amp;ldquo;main reviewer&amp;rdquo; assigned, you may still get feedback from other reviewers. In general, these &amp;ldquo;non-main reviewers&amp;rdquo; are advised to focus more on the design and overall approach rather than the implementation details. Make sure that you address any concerns on this level appropriately.&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Docs: Organizing with Namespaces</title><link>https://gardener.cloud/docs/resources/videos/namespace/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/namespace/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/xpnZX3if9Tc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="Organizing with Namespaces">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Readiness != Liveness</title><link>https://gardener.cloud/docs/resources/videos/livecheck-readiness/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/livecheck-readiness/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/mxEvAPQRwhw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="Readiness != Liveness">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: The Ins and Outs of Networking</title><link>https://gardener.cloud/docs/resources/videos/in-out-networking/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/resources/videos/in-out-networking/</guid><description>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/y2bhV81MfKQ" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="The Ins and Outs of Networking">&lt;/iframe>
&lt;/div></description></item><item><title>Docs: Changing alerting settings</title><link>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/alerting/</link><pubDate>Thu, 20 Jul 2023 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/alerting/</guid><description>
&lt;h1 id="changing-alerting-settings">Changing alerting settings&lt;/h1>
&lt;p>Certificates are normally renewed automatically 30 days before they expire.
As a second line of defense, there is an alerting in Prometheus activated if the certificate is a few days
before expiration. By default, the alert is triggered 15 days before expiration.&lt;/p>
&lt;p>You can configure the days in the &lt;code>providerConfig&lt;/code> of the extension.
Setting it to 0 disables the alerting.&lt;/p>
&lt;p>In this example, the days are changed to 3 days before expiration.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extensions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - type: shoot-cert-service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: CertConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> alerting:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> certExpirationAlertDays: 3
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Scalability of Gardener Managed Kubernetes Clusters</title><link>https://gardener.cloud/docs/guides/administer-shoots/scalability/</link><pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/administer-shoots/scalability/</guid><description>
&lt;p>Have you ever wondered how much more your Kubernetes cluster can scale before it breaks down?&lt;/p>
&lt;p>Of course, the answer is heavily dependent on your workloads. But be assured, any cluster will break eventually. Therefore, the best mitigation is to plan for sharding early and run multiple clusters instead of trying to optimize everything hoping to survive with a single cluster.
Still, it is helpful to know when the time has come to scale out. This document aims at giving you the basic knowledge to keep a Gardener-managed Kubernetes cluster up and running while it scales according to your needs.&lt;/p>
&lt;h2 id="welcome-to-planet-scale-please-mind-the-gap">Welcome to Planet Scale, Please Mind the Gap!&lt;/h2>
&lt;p>For a complex, distributed system like Kubernetes it is impossible to give absolute thresholds for its scalability. Instead, the limit of a cluster&amp;rsquo;s scalability is a combination of various, interconnected dimensions.&lt;/p>
&lt;p>Let&amp;rsquo;s take a rather simple example of two dimensions - the number of &lt;code>Pods&lt;/code> per &lt;code>Node&lt;/code> and number of &lt;code>Nodes&lt;/code> in a cluster. According to the &lt;a href="https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md">scalability thresholds documentation&lt;/a>, Kubernetes can scale up to 5000 &lt;code>Nodes&lt;/code> and with default settings accommodate a maximum of 110 &lt;code>Pods&lt;/code> on a single &lt;code>Node&lt;/code>. Pushing only a single dimension towards its limit will likely harm the cluster. But if both are pushed simultaneously, any cluster will break way before reaching one dimension&amp;rsquo;s limit.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/pod-nodes_09aa76.png" alt="Pods and Nodes">&lt;/p>
&lt;p>What sounds rather straightforward in theory can be a bit trickier in reality. While 110 &lt;code>Pods&lt;/code> is the default limit, we successfully pushed beyond that and in certain cases run up to 200 &lt;code>Pods&lt;/code> per &lt;code>Node&lt;/code> without breaking the cluster. This is possible in an environment where one knows and controls all workloads and cluster configurations. It still requires careful testing, though, and comes at the cost of limiting the scalability of other dimensions, like the number of &lt;code>Nodes&lt;/code>.&lt;/p>
&lt;p>Of course, a Kubernetes cluster has a plethora of dimensions. Thus, when looking at a simple questions like &lt;em>&amp;ldquo;How many resources can I store in ETCD?&amp;rdquo;&lt;/em>, the only meaningful answer must be: &lt;em>&amp;ldquo;it depends&amp;rdquo;&lt;/em>&lt;/p>
&lt;p>The following sections will help you to identify relevant dimensions and how they affect a Gardener-managed Kubernetes cluster&amp;rsquo;s scalability.&lt;/p>
&lt;h2 id="official-kubernetes-thresholds-and-scalability-considerations">&amp;ldquo;Official&amp;rdquo; Kubernetes Thresholds and Scalability Considerations&lt;/h2>
&lt;p>To get started with the topic, please check the basic guidance provided by the Kubernetes community (specifically SIG Scalability):&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md#how-we-define-scalability">How we define scalability?&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md">Kubernetes Scalability Thresholds&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Furthermore, the problem space has been discussed in a &lt;a href="https://www.youtube.com/watch?v=t_Ww6ELKl4Q">KubeCon talk&lt;/a>, the slides for which can be found &lt;a href="https://docs.google.com/presentation/d/1aWjxpY4YJ4KJQUTqaVHdR4sbhwqDiW30EF4_hGCc-gI">here&lt;/a>. You should at least read the slides before continuing.&lt;/p>
&lt;p>Essentially, it comes down to this:&lt;/p>
&lt;blockquote>
&lt;p>If you promise to:&lt;/p>
&lt;ul>
&lt;li>correctly configure your cluster&lt;/li>
&lt;li>use extensibility features &amp;ldquo;reasonably&amp;rdquo;&lt;/li>
&lt;li>keep the load in the cluster within recommended limits&lt;/li>
&lt;/ul>
&lt;p>Then we promise that your cluster will function properly.&lt;/p>
&lt;/blockquote>
&lt;p>With that knowledge in mind, let&amp;rsquo;s look at Gardener and eventually pick up the question about the number of objects in ETCD raised above.&lt;/p>
&lt;h2 id="gardener-specific-considerations">Gardener-Specific Considerations&lt;/h2>
&lt;p>The following considerations are based on experience with various large clusters that scaled in different dimensions. Just as explained above, pushing beyond even one of the limits is likely to cause issues at some point in time (but not guaranteed). Depending on the setup of your workloads however, it might work unexpectedly well. Nevertheless, we urge you take conscious decisions and rather think about sharding your workloads. Please keep in mind - your workload affects the overall stability and scalability of a cluster significantly.&lt;/p>
&lt;h3 id="etcd">ETCD&lt;/h3>
&lt;p>&lt;strong>The following section is based on a setup where ETCD &lt;code>Pods&lt;/code> run on a dedicated &lt;code>Node&lt;/code> pool and each &lt;code>Node&lt;/code> has 8 vCPU and 32GB memory at least.&lt;/strong>&lt;/p>
&lt;p>ETCD has a practical space limit of 8 GB. It caps the number of objects one can technically have in a Kubernetes cluster.&lt;/p>
&lt;p>Of course, the number is heavily influenced by each object&amp;rsquo;s size, especially when considering that secrets and configmaps may store up to 1MB of data. Another dimension is a cluster&amp;rsquo;s churn rate. Since ETCD stores a history of the keyspace, a higher churn rate reduces the number of objects. Gardener runs &lt;a href="https://etcd.io/docs/v3.4/op-guide/maintenance/#history-compaction">compaction&lt;/a> every 30min and &lt;a href="https://etcd.io/docs/v3.4/op-guide/maintenance/#defragmentation">defragmentation&lt;/a> once per day during a cluster&amp;rsquo;s maintenance window to ensure proper ETCD operations. However, it is still possible to overload ETCD. If the space limit is reached, ETCD will only accept &lt;code>READ&lt;/code> or &lt;code>DELETE&lt;/code> requests and manual interaction by a Gardener operator is needed to disarm the alarm, once you got below the threshold.&lt;/p>
&lt;p>To avoid such a situation, you can monitor the current ETCD usage via the &amp;ldquo;ETCD&amp;rdquo; dashboard of the monitoring stack. It gives you the current DB size, as well as historical data for the past 2 weeks. While there are improvements planned to trigger compaction and defragmentation based on DB size, an ETCD should not grow up to this threshold. A typical, healthy DB size is less than 3 GB.&lt;/p>
&lt;p>Furthermore, the dashboard has a panel called &amp;ldquo;Memory&amp;rdquo;, which indicates the memory usage of the etcd pod(s). Using more than 16GB memory is a clear red flag, and you should reduce the load on ETCD.&lt;/p>
&lt;p>Another dimension you should be aware of is the object count in ETCD. You can check it via the &amp;ldquo;API Server&amp;rdquo; dashboard, which features a &amp;ldquo;ETCD Object Counts By Resource&amp;rdquo; panel. The overall number of objects (excluding &lt;code>events&lt;/code>, as they are stored in a different etcd instance) should not exceed 100k for most use cases.&lt;/p>
&lt;h3 id="kube-api-server">Kube API Server&lt;/h3>
&lt;p>&lt;strong>The following section is based on a setup where &lt;code>kube-apiserver&lt;/code> run as &lt;code>Pods&lt;/code> and are scheduled to &lt;code>Nodes&lt;/code> with at least 8 vCPU and 32GB memory.&lt;/strong>&lt;/p>
&lt;p>Gardener can scale the &lt;code>Deployment&lt;/code> of a &lt;code>kube-apiserver&lt;/code> horizontally and vertically. Horizontal scaling is limited to a certain number of replicas and should not concern a stakeholder much. However, the CPU / memory consumption of an individual &lt;code>kube-apiserver&lt;/code> pod poses a potential threat to the overall availability of your cluster. The vertical scaling of any &lt;code>kube-apiserver&lt;/code> is limited by the amount of resources available on a single &lt;code>Node&lt;/code>. Outgrowing the resources of a &lt;code>Node&lt;/code> will cause a downtime and render the cluster unavailable.&lt;/p>
&lt;p>In general, continuous CPU usage of up to 3 cores and 16 GB memory per &lt;code>kube-apiserver&lt;/code> pod is considered to be safe. This gives some room to absorb spikes, for example when the caches are initialized. You can check the resource consumption by selecting &lt;code>kube-apiserver&lt;/code> &lt;code>Pods&lt;/code> in the &amp;ldquo;Kubernetes &lt;code>Pods&lt;/code>&amp;rdquo; dashboard. If these boundaries are exceeded constantly, you need to investigate and derive measures to lower the load.&lt;/p>
&lt;p>Further information is also recorded and made available through the monitoring stack. The dashboard &amp;ldquo;API Server Request Duration and Response Size&amp;rdquo; provides insights into the request processing time of &lt;code>kube-apiserver&lt;/code> &lt;code>Pods&lt;/code>. Related information like request rates, dropped requests or termination codes (e.g., &lt;code>429&lt;/code> for too many requests) can be obtained from the dashboards &amp;ldquo;API Server&amp;rdquo; and &amp;ldquo;Kubernetes API Server Details&amp;rdquo;. They provide a good indicator for how well the system is dealing with its current load.&lt;/p>
&lt;p>Reducing the load on the API servers can become a challenge. To get started, you may try to:&lt;/p>
&lt;ul>
&lt;li>Use immutable &lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/#secret-immutable">secrets&lt;/a> and &lt;a href="https://kubernetes.io/docs/concepts/configuration/configmap/#configmap-immutable">configmaps&lt;/a> where possible to save watches. This pays off, especially when you have a high number of &lt;code>Nodes&lt;/code> or just lots of secrets in general.&lt;/li>
&lt;li>Applications interacting with the K8s API: If you know an object by its name, use it. Using label selector queries is expensive, as the filtering happens only within the &lt;code>kube-apiserver&lt;/code> and not &lt;code>etcd&lt;/code>, hence all resources must first pass completely from &lt;code>etcd&lt;/code> to &lt;code>kube-apiserver&lt;/code>.&lt;/li>
&lt;li>Use (single object) caches within your controllers. Check the &lt;a href="https://github.com/gardener/gardener/issues/7593">&amp;ldquo;Use cache for ShootStates in Gardenlet&amp;rdquo; issue&lt;/a> for an example.&lt;/li>
&lt;/ul>
&lt;h3 id="nodes">&lt;code>Nodes&lt;/code>&lt;/h3>
&lt;p>When talking about the scalability of a Kubernetes cluster, &lt;code>Nodes&lt;/code> are probably mentioned in the first place&amp;hellip; well, obviously not in this guide. While vanilla Kubernetes lists 5000 &lt;code>Nodes&lt;/code> as its upper limit, pushing that dimension is not feasible. Most clusters should run with fewer than 300 &lt;code>Nodes&lt;/code>. But of course, the actual limit depends on the workloads deployed and can be lower or higher. As you scale your cluster, be extra careful and closely monitor ETCD and &lt;code>kube-apiserver&lt;/code>.&lt;/p>
&lt;p>The scalability of &lt;code>Nodes&lt;/code> is subject to a range of limiting factors. Some of them can only be defined upon cluster creation and remain immutable during a cluster lifetime. So let&amp;rsquo;s discuss the most important dimensions.&lt;/p>
&lt;p>&lt;strong>CIDR&lt;/strong>:&lt;/p>
&lt;p>Upon cluster creation, you have to specify or use the default values for several network segments. There are dedicated CIDRs for services, &lt;code>Pods&lt;/code>, and &lt;code>Nodes&lt;/code>. Each defines a range of IP addresses available for the individual resource type. Obviously, the maximum of possible &lt;code>Nodes&lt;/code> is capped by the CIDR for &lt;code>Nodes&lt;/code>.
However, there is a second limiting factor, which is the pod CIDR combined with the &lt;code>nodeCIDRMaskSize&lt;/code>. This mask is used to divide the pod CIDR into smaller subnets, where each blocks gets assigned to a node. With a &lt;code>/16&lt;/code> pod network and a &lt;code>/24&lt;/code> nodeCIDRMaskSize, a cluster can scale up to 256 &lt;code>Nodes&lt;/code>. Please check &lt;a href="https://gardener.cloud/docs/gardener/shoot_networking/">Shoot Networking&lt;/a> for details.&lt;/p>
&lt;p>Even though a &lt;code>/24&lt;/code> nodeCIDRMaskSize translates to a theoretical 256 pod IP addresses per &lt;code>Node&lt;/code>, the &lt;code>maxPods&lt;/code> setting should be less than 1/2 of this value. This gives the system some breathing room for churn and minimizes the risk for strange effects like mis-routed packages caused by immediate re-use of IPs.&lt;/p>
&lt;p>&lt;strong>Cloud provider capacity&lt;/strong>:&lt;/p>
&lt;p>Most of the time, &lt;code>Nodes&lt;/code> in Kubernetes translate to virtual machines on a hyperscaler. An attempt to add more &lt;code>Nodes&lt;/code> to a cluster might fail due to capacity issues resulting in an error message like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>Cloud provider message - machine codes error: code = [Internal] message = [InsufficientInstanceCapacity: We currently do not have sufficient &amp;lt;instance type&amp;gt; capacity in the Availability Zone you requested. Our system will be working on provisioning additional capacity.
&lt;/code>&lt;/pre>&lt;p>In heavily utilized regions, individual clusters are competing for scarce resources. So before choosing a region / zone, try to ensure that the hyperscaler supports your anticipated growth. This might be done through quota requests or by contacting the respective support teams.
To mitigate such a situation, you may configure a worker pool with a different &lt;code>Node&lt;/code> type and a corresponding &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/expander/priority/readme.md">priority expander&lt;/a> as part of a &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#clusterautoscaler">shoot&amp;rsquo;s autoscaler section&lt;/a>. Please consult the &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders">Autoscaler FAQ&lt;/a> for more details.&lt;/p>
&lt;p>&lt;strong>Rolling of &lt;code>Node&lt;/code> pools&lt;/strong>:&lt;/p>
&lt;p>The overall number of &lt;code>Nodes&lt;/code> is affecting the duration of a cluster&amp;rsquo;s maintenance. When upgrading a &lt;code>Node&lt;/code> pool to a new OS image or Kubernetes version, all machines will be drained and deleted, and replaced with new ones. The more &lt;code>Nodes&lt;/code> a cluster has, the longer this process will take, given that workloads are typically protected by &lt;code>PodDisruptionBudgets&lt;/code>. Check &lt;a href="https://gardener.cloud/docs/gardener/shoot_updates/">Shoot Updates and Upgrades&lt;/a> for details. Be sure to take this into consideration when planning maintenance.&lt;/p>
&lt;p>&lt;strong>Root disk&lt;/strong>:&lt;/p>
&lt;p>You should be aware that the &lt;code>Node&lt;/code> configuration impacts your workload&amp;rsquo;s performance too. Take the root disk of a &lt;code>Node&lt;/code>, for example. While most hyperscalers offer the usage of HDD and SSD disks, it is strongly recommended to use SSD volumes as root disks. When there are lots of &lt;code>Pods&lt;/code> on a &lt;code>Node&lt;/code> or workloads making extensive use of &lt;code>emptyDir&lt;/code> volumes, disk throttling becomes an issue. When a disk hits its IOPS limits, processes are stuck in IO-wait and slow down significantly. This can lead to a slow-down in the kubelet&amp;rsquo;s heartbeat mechanism and result in &lt;code>Nodes&lt;/code> being replaced automatically, as they appear to be unhealthy. To analyze such a situation, you might have to run tools like &lt;code>iostat&lt;/code>, &lt;code>sar&lt;/code> or &lt;code>top&lt;/code> directly on a &lt;code>Node&lt;/code>.&lt;/p>
&lt;p>Switching to an I/O optimized instance type (if offered for your infrastructure) can help to resolve issue. Please keep in mind that disks used via &lt;code>PersistentVolumeClaims&lt;/code> have I/O limits as well. Sometimes these limits are related to the size and/or can be increased for individual disks.&lt;/p>
&lt;h3 id="cloud-provider-infrastructure-limits">Cloud Provider (Infrastructure) Limits&lt;/h3>
&lt;p>In addition to the already mentioned capacity restrictions, a cloud provider may impose other limitations to a Kubernetes cluster&amp;rsquo;s scalability. One category is the account quota defining the number of resources allowed globally or per region. Make sure to request appropriate values that suit your needs and contain a buffer, for example for having more &lt;code>Nodes&lt;/code> during a rolling update.&lt;/p>
&lt;p>Another dimension is the network throughput per VM or network interface. While you may be able to choose a network-optimized &lt;code>Node&lt;/code> type for your workload to mitigate issues, you cannot influence the available bandwidth for control plane components. Therefore, please ensure that the traffic on the ETCD does not exceed 100MB/s. The ETCD dashboard provides data for monitoring this metric.&lt;/p>
&lt;p>In some environments the upstream DNS might become an issue too and make your workloads subject to rate limiting. Given the heterogeneity of cloud providers incl. private data centers, it is not possible to give any thresholds. Still, the &amp;ldquo;CoreDNS&amp;rdquo; and &amp;ldquo;NodeLocalDNS&amp;rdquo; dashboards can help to derive a workload&amp;rsquo;s usage pattern. Check the &lt;a href="https://gardener.cloud/docs/gardener/dns-autoscaling/">DNS autoscaling&lt;/a> and &lt;a href="https://gardener.cloud/docs/gardener/node-local-dns/">NodeLocalDNS&lt;/a> documentations for available configuration options.&lt;/p>
&lt;h3 id="webhooks">Webhooks&lt;/h3>
&lt;p>While webhooks provide powerful means to manage a cluster, they are equally powerful in breaking a cluster upon a malfunction or unavailability. Imagine using a policy enforcing system like &lt;a href="https://kyverno.io/docs/">Kyverno&lt;/a> or &lt;a href="https://open-policy-agent.github.io/gatekeeper/website/docs/">Open Policy Agent Gatekeeper&lt;/a>. As part of the stack, both will deploy webhooks which are invoked for almost everything that happens in a cluster. Now, if this webhook gets either overloaded or is simply not available, the cluster will stop functioning properly.&lt;/p>
&lt;p>Hence, you have to ensure proper sizing, quick processing time, and availability of the webhook serving &lt;code>Pods&lt;/code> when deploying webhooks. Please consult Dynamic Admission Control (&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#availability">Availability&lt;/a> and &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#timeouts">Timeouts&lt;/a> sections) for details. You should also be aware of the time added to any request that has to go through a webhook, as the &lt;code>kube-apiserver&lt;/code> sends the request for mutation / validation to another pod and waits for the response. The more resources being subject to an external webhook, the more likely this will become a bottleneck when having a high churn rate on resources. Within the Gardener monitoring stack, you can check the extra time per webhook via the &amp;ldquo;API Server (Admission Details)&amp;rdquo; dashboard, which has a panel for &amp;ldquo;Duration per Webhook&amp;rdquo;.&lt;/p>
&lt;p>In Gardener, any webhook timeout should be less than 15 seconds. Due to the separation of Kubernetes data-plane (shoot) and control-plane (seed) in Gardener, the extra hop from &lt;code>kube-apiserver&lt;/code> (control-plane) to webhook (data-plane) is more expensive. Please check &lt;a href="https://gardener.cloud/docs/gardener/shoot_status/">Shoot Status&lt;/a> for more details.&lt;/p>
&lt;h3 id="custom-resource-definitions">Custom Resource Definitions&lt;/h3>
&lt;p>Using Custom Resource Definitions (CRD) to extend a cluster&amp;rsquo;s API is a common Kubernetes pattern and so is writing an operator to act upon custom resources. Writing an efficient controller reduces the load on the &lt;code>kube-apiserver&lt;/code> and allows for better scaling. As a starting point, you might want to read Gardener&amp;rsquo;s &lt;a href="https://gardener.cloud/docs/gardener/kubernetes-clients/">Kubernetes Clients Guide&lt;/a>.&lt;/p>
&lt;p>Another problematic dimension is the usage of &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#webhook-conversion">conversion webhooks&lt;/a> when having resources stored in different versions. Not only do they add latency (see &lt;a href="https://gardener.cloud/docs/guides/administer-shoots/scalability/#webhooks">Webhooks&lt;/a>) but can also block the kube-controllermanager&amp;rsquo;s garbage collection. If a conversion webhook is unavailable, the garbage collector fails to list all resources and does not perform any cleanup. In order to avoid such a situation, it is highly recommended to use conversion webhooks only when necessary and complete the migration to a new version as soon as possible.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>As outlined by SIG Scalability, it is quite impossible to give limits or even recommendations fitting every individual use case. Instead, this guide outlines relevant dimensions and gives rather conservative recommendations based on usage patterns observed. By combining this information, it is possible to operate and scale a cluster in stable manner.&lt;/p>
&lt;p>While going beyond is certainly possible for some dimensions, it significantly increases the risk of instability. Typically, limits on the control-plane are introduced by the availability of resources like CPU or memory on a single machine and can hardly be influenced by any user. Therefore, utilizing the existing resources efficiently is key. Other parameters are controlled by a user. In these cases, careful testing may reveal actual limits for a specific use case.&lt;/p>
&lt;p>Please keep in mind that all aspects of a workload greatly influence the stability and scalability of a Kubernetes cluster.&lt;/p></description></item><item><title>Docs: Best Practices</title><link>https://gardener.cloud/docs/guides/high-availability/best-practices/</link><pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/high-availability/best-practices/</guid><description>
&lt;h1 id="implementing-high-availability-and-tolerating-zone-outages">Implementing High Availability and Tolerating Zone Outages&lt;/h1>
&lt;p>Developing highly available workload that can tolerate a zone outage is no trivial task. You will find here various recommendations to get closer to that goal. While many recommendations are general enough, the examples are specific in how to achieve this in a Gardener-managed cluster and where/how to tweak the different control plane components. If you do not use Gardener, it may be still a worthwhile read.&lt;/p>
&lt;p>First however, what is a zone outage? It sounds like a clear-cut &amp;ldquo;thing&amp;rdquo;, but it isn&amp;rsquo;t. There are many things that can go haywire. Here are some examples:&lt;/p>
&lt;ul>
&lt;li>Elevated cloud provider API error rates for individual or multiple services&lt;/li>
&lt;li>Network bandwidth reduced or latency increased, usually also effecting storage sub systems as they are network attached&lt;/li>
&lt;li>No networking at all, no DNS, machines shutting down or restarting, &amp;hellip;&lt;/li>
&lt;li>Functional issues, of either the entire service (e.g. all block device operations) or only parts of it (e.g. LB listener registration)&lt;/li>
&lt;li>All services down, temporarily or permanently (the proverbial burning down data center 🔥)&lt;/li>
&lt;/ul>
&lt;p>This and everything in between make it hard to prepare for such events, but you can still do a lot. The most important recommendation is to not target specific issues exclusively - tomorrow another service will fail in an unanticipated way. Also, focus more on &lt;a href="https://research.google/pubs/pub50828">meaningful availability&lt;/a> than on internal signals (useful, but not as relevant as the former). Always prefer automation over manual intervention (e.g. leader election is a pretty robust mechanism, auto-scaling may be required as well, etc.).&lt;/p>
&lt;p>Also remember that HA is costly - you need to balance it against the cost of an outage as silly as this may sound, e.g. running all this excess capacity &amp;ldquo;just in case&amp;rdquo; vs. &amp;ldquo;going down&amp;rdquo; vs. a risk-based approach in between where you have means that will kick in, but they are not guaranteed to work (e.g. if the cloud provider is out of resource capacity). Maybe some of your components must run at the highest possible availability level, but others not - that&amp;rsquo;s a decision only you can make.&lt;/p>
&lt;h2 id="control-plane">Control Plane&lt;/h2>
&lt;p>The Kubernetes cluster control plane is managed by Gardener (as pods in separate infrastructure clusters to which you have no direct access) and can be set up with no failure tolerance (control plane pods will be recreated best-effort when resources are available) or one of the &lt;a href="https://gardener.cloud/docs/guides/high-availability/control-plane/">failure tolerance types &lt;code>node&lt;/code> or &lt;code>zone&lt;/code>&lt;/a>.&lt;/p>
&lt;p>Strictly speaking, static workload does not depend on the (high) availability of the control plane, but static workload doesn&amp;rsquo;t rhyme with Cloud and Kubernetes and also means, that when you possibly need it the most, e.g. during a zone outage, critical self-healing or auto-scaling functionality won&amp;rsquo;t be available to you and your workload, if your control plane is down as well. That&amp;rsquo;s why, even though the resource consumption is significantly higher, we generally recommend to use the failure tolerance type &lt;code>zone&lt;/code> for the control planes of productive clusters, at least in all regions that have 3+ zones. Regions that have only 1 or 2 zones don&amp;rsquo;t support the failure tolerance type &lt;code>zone&lt;/code> and then your second best option is the failure tolerance type &lt;code>node&lt;/code>, which means a zone outage can still take down your control plane, but individual node outages won&amp;rsquo;t.&lt;/p>
&lt;p>In the &lt;code>shoot&lt;/code> resource it&amp;rsquo;s merely only this what you need to add:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> controlPlane:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> highAvailability:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> failureTolerance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: zone &lt;span style="color:#008000"># valid values are `node` and `zone` (only available if your control plane resides in a region with 3+ zones)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This setting will scale out all control plane components for a Gardener cluster as necessary, so that no single zone outage can take down the control plane for longer than just a few seconds for the fail-over to take place (e.g. lease expiration and new leader election or readiness probe failure and endpoint removal). Components run highly available in either active-active (servers) or active-passive (controllers) mode at all times, the persistence (ETCD), which is consensus-based, will tolerate the loss of one zone and still maintain quorum and therefore remain operational. These are all patterns that we will revisit down below also for your own workload.&lt;/p>
&lt;h2 id="worker-pools">Worker Pools&lt;/h2>
&lt;p>Now that you have configured your Kubernetes cluster control plane in HA, i.e. spread it across multiple zones, you need to do the same for your own workload, but in order to do so, you need to spread your nodes across multiple zones first.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> minimum: 6
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maximum: 60
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> zones:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Prefer regions with at least 2, better 3+ zones and list the zones in the &lt;code>zones&lt;/code> section for each of your worker pools. Whether you need 2 or 3 zones at a minimum depends on your fail-over concept:&lt;/p>
&lt;ul>
&lt;li>Consensus-based software components (like ETCD) depend on maintaining a quorum of &lt;code>(n/2)+1&lt;/code>, so you need at least 3 zones to tolerate the outage of 1 zone.&lt;/li>
&lt;li>Primary/Secondary-based software components need just 2 zones to tolerate the outage of 1 zone.&lt;/li>
&lt;li>Then there are software components that can scale out horizontally. They are probably fine with 2 zones, but you also need to think about the load-shift and that the remaining zone must then pick up the work of the unhealthy zone. With 2 zones, the remaining zone must cope with an increase of 100% load. With 3 zones, the remaining zones must only cope with an increase of 50% load (per zone).&lt;/li>
&lt;/ul>
&lt;p>In general, the question is also whether you have the fail-over capacity already up and running or not. If not, i.e. you depend on re-scheduling to a healthy zone or auto-scaling, be aware that during a zone outage, you will see a resource crunch in the healthy zones. If you have no automation, i.e. only human operators (a.k.a. &amp;ldquo;red button approach&amp;rdquo;), you probably will not get the machines you need and even with automation, it may be tricky. But holding the capacity available at all times is costly. In the end, that&amp;rsquo;s a decision only you can make. If you made that decision, please adapt the &lt;code>minimum&lt;/code>, &lt;code>maximum&lt;/code>, &lt;code>maxSurge&lt;/code> and &lt;code>maxUnavailable&lt;/code> settings for your worker pools accordingly (visit &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/#on-specproviderworkersminimum-maximum-maxsurge-maxunavailable-zones-and-machinecontrollermanager">this section&lt;/a> for more information).&lt;/p>
&lt;p>Also, consider fall-back worker pools (with different/alternative machine types) and &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders">cluster autoscaler expanders&lt;/a> using a &lt;a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/expander/priority/readme.md">priority-based strategy&lt;/a>.&lt;/p>
&lt;p>Gardener-managed clusters deploy the &lt;a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">cluster autoscaler&lt;/a> or CA for short and you can &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#clusterautoscaler">tweak the general CA knobs&lt;/a> for Gardener-managed clusters like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clusterAutoscaler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expander: &lt;span style="color:#a31515">&amp;#34;least-waste&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scanInterval: 10s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownDelayAfterAdd: 60m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownDelayAfterDelete: 0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownDelayAfterFailure: 3m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownUnneededTime: 30m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownUtilizationThreshold: 0.5
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you want to be ready for a sudden spike or have some buffer in general, &lt;a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-can-i-configure-overprovisioning-with-cluster-autoscaler">over-provision nodes by means of &amp;ldquo;placeholder&amp;rdquo; pods&lt;/a> with &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption">low priority&lt;/a> and appropriate resource requests. This way, they will demand nodes to be provisioned for them, but if any pod comes up with a regular/higher priority, the low priority pods will be evicted to make space for the more important ones. Strictly speaking, this is not related to HA, but it may be important to keep this in mind as you generally want critical components to be rescheduled as fast as possible and if there is no node available, it may take 3 minutes or longer to do so (depending on the cloud provider). Besides, not only zones can fail, but also individual nodes.&lt;/p>
&lt;h2 id="replicas-horizontal-scaling">Replicas (Horizontal Scaling)&lt;/h2>
&lt;p>Now let&amp;rsquo;s talk about your workload. In most cases, this will mean to run multiple replicas. If you cannot do that (a.k.a. you have a singleton), that&amp;rsquo;s a bad situation to be in. Maybe you can run a spare (secondary) as backup? If you cannot, you depend on quick detection and rescheduling of your singleton (more on that below).&lt;/p>
&lt;p>Obviously, things get messier with persistence. If you have persistence, you should ideally replicate your data, i.e. let your spare (secondary) &amp;ldquo;follow&amp;rdquo; your main (primary). If your software doesn&amp;rsquo;t support that, you have to deploy other means, e.g. &lt;a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots">volume snapshotting&lt;/a> or side-backups (specific to the software you deploy; keep the backups regional, so that you can switch to another zone at all times). If you have to do those, your HA scenario becomes more a DR scenario and terms like RPO and RTO become relevant to you:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Recovery Point Objective (RPO)&lt;/strong>: Potential data loss, i.e. how much data will you lose at most (time between backups)&lt;/li>
&lt;li>&lt;strong>Recovery Time Objective (RTO)&lt;/strong>: Time until recovery, i.e. how long does it take you to be operational again (time to restore)&lt;/li>
&lt;/ul>
&lt;p>Also, keep in mind that your persistent volumes are usually zonal, i.e. once you have a volume in one zone, it&amp;rsquo;s bound to that zone and you cannot get up your pod in another zone w/o first recreating the volume yourself (Kubernetes won&amp;rsquo;t help you here directly).&lt;/p>
&lt;p>Anyway, best avoid that, if you can (from technical and cost perspective). The best solution (and also the most costly one) is to run multiple replicas in multiple zones and keep your data replicated at all times, so that your RPO is always 0 (best). That&amp;rsquo;s what we do for Gardener-managed cluster HA control planes (ETCD) as any data loss may be disastrous and lead to orphaned resources (in addition, we deploy side cars that do side-backups for disaster recovery, with full and incremental snapshots with an RPO of 5m).&lt;/p>
&lt;p>So, how to run with multiple replicas? That&amp;rsquo;s the easiest part in Kubernetes and the two most important resources, &lt;code>Deployments&lt;/code> and &lt;code>StatefulSet&lt;/code>, support that out of the box:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Deployment | StatefulSet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> replicas: ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The problem comes with the number of replicas. It&amp;rsquo;s easy only if the number is static, e.g. 2 for active-active/passive or 3 for consensus-based software components, but what with software components that can scale out horizontally? Here you usually do not set the number of replicas statically, but make use of the &lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale">horizontal pod autoscaler&lt;/a> or HPA for short (built-in; part of the kube-controller-manager). There are also other options like the &lt;a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">cluster proportional autoscaler&lt;/a>, but while the former works based on metrics, the latter is more a guestimate approach that derives the number of replicas from the number of nodes/cores in a cluster. Sometimes useful, but often blind to the actual demand.&lt;/p>
&lt;p>So, HPA it is then for most of the cases. However, what is the resource (e.g. CPU or memory) that drives the number of desired replicas? Again, this is up to you, but not always are CPU or memory the best choices. In some cases, &lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#scaling-on-custom-metrics">custom metrics&lt;/a> may be more appropriate, e.g. requests per second (it was also for us).&lt;/p>
&lt;p>You will have to create specific &lt;code>HorizontalPodAutoscaler&lt;/code> resources for your scale target and can &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#horizontalpodautoscalerconfig">tweak the general HPA knobs&lt;/a> for Gardener-managed clusters like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeControllerManager:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> horizontalPodAutoscaler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> syncPeriod: 15s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tolerance: 0.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> downscaleStabilization: 5m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> initialReadinessDelay: 30s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpuInitializationPeriod: 5m0s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="resources-vertical-scaling">Resources (Vertical Scaling)&lt;/h2>
&lt;p>While it is important to set a sufficient number of replicas, it is also important to give the pods sufficient resources (CPU and memory). This is especially true when you think about HA. When a zone goes down, you might need to get up replacement pods, if you don&amp;rsquo;t have them running already to take over the load from the impacted zone. Likewise, e.g. with active-active software components, you can expect the remaining pods to receive more load. If you cannot scale them out horizontally to serve the load, you will probably need to scale them out (or rather up) vertically. This is done by the &lt;a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler">vertical pod autoscaler&lt;/a> or VPA for short (not built-in; part of the &lt;a href="https://github.com/kubernetes/autoscaler">kubernetes/autoscaler&lt;/a> repository).&lt;/p>
&lt;p>A few caveats though:&lt;/p>
&lt;ul>
&lt;li>You cannot use HPA and VPA on the same metrics as they would influence each other, which would lead to pod trashing (more replicas require fewer resources; fewer resources require more replicas)&lt;/li>
&lt;li>Scaling horizontally doesn&amp;rsquo;t cause downtimes (at least not when out-scaling and only one replica is affected when in-scaling), but scaling vertically does (if the pod runs OOM anyway, but also when new recommendations are applied, resource requests for existing pods may be changed, which causes the pods to be rescheduled). Although the discussion is going on for a very long time now, that is still not supported in-place yet (see &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/1287-in-place-update-pod-resources/README.md">KEP 1287&lt;/a>, &lt;a href="https://github.com/kubernetes/kubernetes/pull/102884">implementation in Kubernetes&lt;/a>, &lt;a href="https://github.com/kubernetes/autoscaler/issues/4016">implementation in VPA&lt;/a>).&lt;/li>
&lt;/ul>
&lt;p>VPA is a useful tool and Gardener-managed clusters deploy a VPA by default for you (HPA is supported anyway as it&amp;rsquo;s built into the kube-controller-manager). You will have to create specific &lt;code>VerticalPodAutoscaler&lt;/code> resources for your scale target and can &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#verticalpodautoscaler">tweak the general VPA knobs&lt;/a> for Gardener-managed clusters like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> verticalPodAutoscaler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictAfterOOMThreshold: 10m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionRateBurst: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionRateLimit: -1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionTolerance: 0.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> recommendationMarginFraction: 0.15
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> updaterInterval: 1m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> recommenderInterval: 1m0s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>While horizontal pod autoscaling is relatively straight-forward, it takes a long time to master vertical pod autoscaling. We saw &lt;a href="https://github.com/kubernetes/autoscaler/issues/4498">performance issues&lt;/a>, hard-coded behavior (on OOM, memory is bumped by +20% and it may take a few iterations to reach a good level), unintended pod disruptions by applying new resource requests (after 12h all targeted pods will receive new requests even though individually they would be fine without, which also drives active-passive resource consumption up), difficulties to deal with spiky workload in general (due to the algorithmic approach it takes), recommended requests may exceed node capacity, limit scaling is proportional and therefore often questionable, and more. VPA is a double-edged sword: useful and necessary, but not easy to handle.&lt;/p>
&lt;p>For the Gardener-managed components, we mostly removed limits. Why?&lt;/p>
&lt;ul>
&lt;li>CPU limits have almost always only downsides. They cause needless CPU throttling, which is not even easily visible. CPU requests turn into &lt;code>cpu shares&lt;/code>, so if the node has capacity, the pod may consume the freely available CPU, but not if you have set limits, which curtail the pod by means of &lt;code>cpu quota&lt;/code>. There are only certain scenarios in which they may make sense, e.g. if you set requests=limits and thereby define a pod with &lt;code>guaranteed&lt;/code> &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod">QoS&lt;/a>, which influences your &lt;code>cgroup&lt;/code> placement. However, that is difficult to do for the components you implement yourself and practically impossible for the components you just consume, because what&amp;rsquo;s the correct value for requests/limits and will it hold true also if the load increases and what happens if a zone goes down or with the next update/version of this component? If anything, CPU limits caused outages, not helped prevent them.&lt;/li>
&lt;li>As for memory limits, they are slightly more useful, because CPU is compressible and memory is not, so if one pod runs berserk, it may take others down (with CPU, &lt;code>cpu shares&lt;/code> make it as fair as possible), depending on which OOM killer strikes (a complicated topic by itself). You don&amp;rsquo;t want the operating system OOM killer to strike as the result is unpredictable. Better, it&amp;rsquo;s the cgroup OOM killer or even the &lt;code>kubelet&lt;/code>&amp;rsquo;s eviction, if the consumption is slow enough as &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#interactions-of-pod-priority-and-qos">it takes priorities into consideration&lt;/a> even. If your component is critical and a singleton (e.g. node daemon set pods), you are better off also without memory limits, because letting the pod go OOM because of artificial/wrong memory limits can mean that the node becomes unusable. Hence, such components also better run only with no or a very high memory limit, so that you can catch the occasional memory leak (bug) eventually, but under normal operation, if you cannot decide about a true upper limit, rather not have limits and cause endless outages through them or when you need the pods the most (during a zone outage) where all your assumptions went out of the window.&lt;/li>
&lt;/ul>
&lt;p>The downside of having poor or no limits and poor and no requests is that nodes may &amp;ldquo;die&amp;rdquo; more often. Contrary to the expectation, even for managed services, the managed service is not responsible or cannot guarantee the health of a node under all circumstances, since the end user defines what is run on the nodes (shared responsibility). If the workload exhausts any resource, it will be the end of the node, e.g. by compressing the CPU too much (so that the &lt;code>kubelet&lt;/code> fails to do its work), exhausting the main memory too fast, disk space, file handles, or any other resource.&lt;/p>
&lt;p>The &lt;code>kubelet&lt;/code> allows for &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources">explicit reservation of resources&lt;/a> for operating system daemons (&lt;code>system-reserved&lt;/code>) and Kubernetes daemons (&lt;code>kube-reserved&lt;/code>) that are subtracted from the actual node resources and become the allocatable node resources for your workload/pods. All managed services configure these settings &amp;ldquo;by rule of thumb&amp;rdquo; (a balancing act), but cannot guarantee that the values won&amp;rsquo;t waste resources or always will be sufficient. You will have to fine-tune them eventually and adapt them to your needs. In addition, you can configure &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction">soft and hard eviction thresholds&lt;/a> to give the &lt;code>kubelet&lt;/code> some headroom to evict &amp;ldquo;greedy&amp;rdquo; pods in a controlled way. These settings can be configured for Gardener-managed clusters like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubelet:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeReserved: &lt;span style="color:#008000"># explicit resource reservation for Kubernetes daemons&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpu: 100m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memory: 1Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ephemeralStorage: 1Gi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> pid: 1000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionSoft: &lt;span style="color:#008000"># soft, i.e. graceful eviction (used if the node is about to run out of resources, avoiding hard evictions)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memoryAvailable: 200Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSAvailable: 10%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSInodesFree: 10%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSAvailable: 10%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSInodesFree: 10%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionSoftGracePeriod: &lt;span style="color:#008000"># caps pod&amp;#39;s `terminationGracePeriodSeconds` value during soft evictions (specific grace periods)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memoryAvailable: 1m30s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSAvailable: 1m30s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSInodesFree: 1m30s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSAvailable: 1m30s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSInodesFree: 1m30s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionHard: &lt;span style="color:#008000"># hard, i.e. immediate eviction (used if the node is out of resources, avoiding the OS generally run out of resources fail processes indiscriminately)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memoryAvailable: 100Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSAvailable: 5%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSInodesFree: 5%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSAvailable: 5%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSInodesFree: 5%
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionMinimumReclaim: &lt;span style="color:#008000"># additional resources to reclaim after hitting the hard eviction thresholds to not hit the same thresholds soon after again&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> memoryAvailable: 0Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSAvailable: 0Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> imageFSInodesFree: 0Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSAvailable: 0Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeFSInodesFree: 0Mi
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionMaxPodGracePeriod: 90 &lt;span style="color:#008000"># caps pod&amp;#39;s `terminationGracePeriodSeconds` value during soft evictions (general grace periods)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionPressureTransitionPeriod: 5m0s &lt;span style="color:#008000"># stabilization time window to avoid flapping of node eviction state&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can tweak these settings also individually per worker pool (&lt;code>spec.provider.workers.kubernetes.kubelet...&lt;/code>), which makes sense especially with different machine types (and also workload that you may want to schedule there).&lt;/p>
&lt;p>Physical memory is not compressible, but you can overcome this issue to some degree (alpha since Kubernetes &lt;code>v1.22&lt;/code> in combination with the feature gate &lt;code>NodeSwap&lt;/code> on the &lt;code>kubelet&lt;/code>) with swap memory. You can read more in this &lt;a href="https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap-alpha">introductory blog&lt;/a> and the &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory">docs&lt;/a>. If you chose to use it (still only alpha at the time of this writing) you may want to consider also the risks associated with swap memory:&lt;/p>
&lt;ul>
&lt;li>Reduced performance predictability&lt;/li>
&lt;li>Reduced performance up to page trashing&lt;/li>
&lt;li>Reduced security as secrets, normally held only in memory, could be swapped out to disk&lt;/li>
&lt;/ul>
&lt;p>That said, the various options mentioned above are only remotely related to HA and will not be further explored throughout this document, but just to remind you: if a zone goes down, load patterns will shift, existing pods will probably receive more load and will require more resources (especially because it is often practically impossible to set &amp;ldquo;proper&amp;rdquo; resource requests, which drive node allocation - limits are always ignored by the scheduler) or more pods will/must be placed on the existing and/or new nodes and then these settings, which are generally critical (especially if you switch on &lt;a href="https://gardener.cloud/docs/gardener/shoot_scheduling_profiles/">bin-packing for Gardener-managed clusters&lt;/a> as a cost saving measure), will become even more critical during a zone outage.&lt;/p>
&lt;h2 id="probes">Probes&lt;/h2>
&lt;p>Before we go down the rabbit hole even further and talk about how to spread your replicas, we need to talk about probes first, as they will become relevant later. Kubernetes supports three kinds of probes: &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes">startup, liveness, and readiness probes&lt;/a>. If you are a &lt;a href="https://twitter.com/thockin/status/1615468485987143682">visual thinker&lt;/a>, also check out this &lt;a href="https://speakerdeck.com/thockin/kubernetes-pod-probes">slide deck&lt;/a> by &lt;a href="https://www.linkedin.com/in/tim-hockin-6501072">Tim Hockin&lt;/a> (Kubernetes networking SIG chair).&lt;/p>
&lt;p>Basically, the &lt;code>startupProbe&lt;/code> and the &lt;code>livenessProbe&lt;/code> help you restart the container, if it&amp;rsquo;s unhealthy for whatever reason, by letting the &lt;code>kubelet&lt;/code> that orchestrates your containers on a node know, that it&amp;rsquo;s unhealthy. The former is a special case of the latter and only applied at the startup of your container, if you need to handle the startup phase differently (e.g. with very slow starting containers) from the rest of the lifetime of the container.&lt;/p>
&lt;p>Now, the &lt;code>readinessProbe&lt;/code> helps you manage the ready status of your container and thereby pod (any container that is not ready turns the pod not ready). This again has impact on endpoints and pod disruption budgets:&lt;/p>
&lt;ul>
&lt;li>If the pod is not ready, the endpoint will be removed and the pod will not receive traffic anymore&lt;/li>
&lt;li>If the pod is not ready, the pod counts into the pod disruption budget and if the budget is exceeded, no further voluntary pod disruptions will be permitted for the remaining ready pods (e.g. no eviction, no voluntary horizontal or vertical scaling, if the pod runs on a node that is about to be drained or in draining, draining will be paused until the max drain timeout passes)&lt;/li>
&lt;/ul>
&lt;p>As you can see, all of these probes are (also) related to HA (mostly the &lt;code>readinessProbe&lt;/code>, but depending on your workload, you can also leverage &lt;code>livenessProbe&lt;/code> and &lt;code>startupProbe&lt;/code> into your HA strategy). If Kubernetes doesn&amp;rsquo;t know about the individual status of your container/pod, it won&amp;rsquo;t do anything for you (right away). That said, later/indirectly something might/will happen via the node status that can also be ready or not ready, which influences the pods and load balancer listener registration (a not ready node will not receive cluster traffic anymore), but this process is worker pool global and reacts delayed and also doesn&amp;rsquo;t discriminate between the containers/pods on a node.&lt;/p>
&lt;p>In addition, Kubernetes also offers &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate">pod readiness gates&lt;/a> to amend your pod readiness with additional custom conditions (normally, only the sum of the container readiness matters, but pod readiness gates additionally count into the overall pod readiness). This may be useful if you want to block (by means of pod disruption budgets that we will talk about next) the roll-out of your workload/nodes in case some (possibly external) condition fails.&lt;/p>
&lt;h2 id="pod-disruption-budgets">Pod Disruption Budgets&lt;/h2>
&lt;p>One of the most important resources that help you on your way to HA are &lt;a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb">pod disruption budgets&lt;/a> or PDB for short. They tell Kubernetes how to deal with voluntary pod disruptions, e.g. during the deployment of your workload, when the nodes are rolled, or just in general when a pod shall be evicted/terminated. Basically, if the budget is reached, they block all voluntary pod disruptions (at least for a while until possibly other timeouts act or things happen that leave Kubernetes no choice anymore, e.g. the node is forcefully terminated). You should always define them for your workload.&lt;/p>
&lt;p>Very important to note is that they are based on the &lt;code>readinessProbe&lt;/code>, i.e. even if all of your replicas are &lt;code>lively&lt;/code>, but not enough of them are &lt;code>ready&lt;/code>, this blocks voluntary pod disruptions, so they are very critical and useful. Here an example (you can specify either &lt;code>minAvailable&lt;/code> or &lt;code>maxUnavailable&lt;/code> in absolute numbers or as percentage):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: policy/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: PodDisruptionBudget
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maxUnavailable: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> selector:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> matchLabels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And please do not specify a PDB of &lt;code>maxUnavailable&lt;/code> being 0 or similar. That&amp;rsquo;s pointless, even detrimental, as it blocks then even useful operations, forces always the hard timeouts that are less graceful and it doesn&amp;rsquo;t make sense in the context of HA. You cannot &amp;ldquo;force&amp;rdquo; HA by preventing voluntary pod disruptions, you must work with the pod disruptions in a resilient way. Besides, PDBs are really only about voluntary pod disruptions - something bad can happen to a node/pod at any time and PDBs won&amp;rsquo;t make this reality go away for you.&lt;/p>
&lt;p>PDBs will not always work as expected and can also get in your way, e.g. if the PDB is violated or would be violated, it may possibly block whatever you are trying to do to salvage the situation, e.g. drain a node or deploy a patch version (if the PDB is or would be violated, not even unhealthy pods would be evicted as they could theoretically become healthy again, which Kubernetes doesn&amp;rsquo;t know). In order to overcome this issue, it is now possible (alpha since Kubernetes &lt;code>v1.26&lt;/code> in combination with the feature gate &lt;code>PDBUnhealthyPodEvictionPolicy&lt;/code> on the API server, beta and enabled by default since Kubernetes &lt;code>v1.27&lt;/code>) to configure the so-called &lt;a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy">unhealthy pod eviction policy&lt;/a>. The default is still &lt;code>IfHealthyBudget&lt;/code> as a change in default would have changed the behavior (as described above), but you can now also set &lt;code>AlwaysAllow&lt;/code> at the PDB (&lt;code>spec.unhealthyPodEvictionPolicy&lt;/code>). For more information, please check out &lt;a href="https://github.com/kubernetes/kubernetes/issues/72320">this discussion&lt;/a>, &lt;a href="https://github.com/kubernetes/kubernetes/pull/105296">the PR&lt;/a> and &lt;a href="https://groups.google.com/g/kubernetes-sig-apps/c/_joO4swogKY?pli=1">this document&lt;/a> and balance the pros and cons for yourself. In short, the new &lt;code>AlwaysAllow&lt;/code> option is probably the better choice in most of the cases while &lt;code>IfHealthyBudget&lt;/code> is useful only if you have frequent temporary transitions or for special cases where you have already implemented controllers that depend on the old behavior.&lt;/p>
&lt;h2 id="pod-topology-spread-constraints">Pod Topology Spread Constraints&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints">Pod topology spread constraints&lt;/a> or PTSC for short (no official abbreviation exists, but we will use this in the following) are enormously helpful to distribute your replicas across multiple zones, nodes, or any other user-defined topology domain. They complement and improve on &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">pod (anti-)affinities&lt;/a> that still exist and can be used in combination.&lt;/p>
&lt;p>PTSCs are an improvement, because they allow for &lt;code>maxSkew&lt;/code> and &lt;code>minDomains&lt;/code>. You can steer the &amp;ldquo;level of tolerated imbalance&amp;rdquo; with &lt;code>maxSkew&lt;/code>, e.g. you probably want that to be at least 1, so that you can perform a rolling update, but this all depends on your &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment">deployment&lt;/a> (&lt;code>maxUnavailable&lt;/code> and &lt;code>maxSurge&lt;/code>), etc. &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#rolling-updates">Stateful sets&lt;/a> are a bit different (&lt;code>maxUnavailable&lt;/code>) as they are bound to volumes and depend on them, so there usually cannot be 2 pods requiring the same volume. &lt;code>minDomains&lt;/code> is a hint to tell the scheduler how far to spread, e.g. if all nodes in one zone disappeared because of a zone outage, it may &amp;ldquo;appear&amp;rdquo; as if there are only 2 zones in a 3 zones cluster and the scheduling decisions may end up wrong, so a &lt;code>minDomains&lt;/code> of 3 will tell the scheduler to spread to 3 zones before adding another replica in one zone. Be careful with this setting as it also means, if one zone is down the &amp;ldquo;spread&amp;rdquo; is already at least 1, if pods run in the other zones. This is useful where you have exactly as many replicas as you have zones and you do not want any imbalance. Imbalance is critical as if you end up with one, nobody is going to do the (active) re-balancing for you (unless you deploy and configure additional non-standard components such as the &lt;a href="https://github.com/kubernetes-sigs/descheduler">descheduler&lt;/a>). So, for instance, if you have something like a DBMS that you want to spread across 2 zones (active-passive) or 3 zones (consensus-based), you better specify &lt;code>minDomains&lt;/code> of 2 respectively 3 to force your replicas into at least that many zones before adding more replicas to another zone (if supported).&lt;/p>
&lt;p>Anyway, PTSCs are critical to have, but not perfect, so we saw (unsurprisingly, because that&amp;rsquo;s how the scheduler works), that the scheduler may block the deployment of new pods because it takes the decision pod-by-pod (see for instance &lt;a href="https://github.com/kubernetes/kubernetes/issues/109364">#109364&lt;/a>).&lt;/p>
&lt;h2 id="pod-affinities-and-anti-affinities">Pod Affinities and Anti-Affinities&lt;/h2>
&lt;p>As said, you can combine PTSCs with &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity">pod affinities and/or anti-affinities&lt;/a>. Especially &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">inter-pod (anti-)affinities&lt;/a> may be helpful to place pods &lt;em>apart&lt;/em>, e.g. because they are fall-backs for each other or you do not want multiple potentially resource-hungry &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#besteffort">&amp;ldquo;best-effort&amp;rdquo;&lt;/a> or &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#burstable">&amp;ldquo;burstable&amp;rdquo;&lt;/a> pods side-by-side (noisy neighbor problem), or &lt;em>together&lt;/em>, e.g. because they form a unit and you want to reduce the failure domain, reduce the network latency, and reduce the costs.&lt;/p>
&lt;h2 id="topology-aware-hints">Topology Aware Hints&lt;/h2>
&lt;p>While &lt;a href="https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints">topology aware hints&lt;/a> are not directly related to HA, they are very relevant in the HA context. Spreading your workload across multiple zones may increase network latency and cost significantly, if the traffic is not shaped. Topology aware hints (beta since Kubernetes &lt;code>v1.23&lt;/code>, replacing the now deprecated topology aware traffic routing with topology keys) help to route the traffic within the originating zone, if possible. Basically, they tell &lt;code>kube-proxy&lt;/code> how to setup your routing information, so that clients can talk to endpoints that are located within the same zone.&lt;/p>
&lt;p>Be aware however, that there are some limitations. Those are called &lt;a href="https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/#safeguards">safeguards&lt;/a> and if they strike, the hints are off and traffic is routed again randomly. Especially controversial is the balancing limitation as there is the assumption, that the load that hits an endpoint is determined by the allocatable CPUs in that topology zone, but that&amp;rsquo;s not always, if even often, the case (see for instance &lt;a href="https://github.com/kubernetes/kubernetes/issues/113731">#113731&lt;/a> and &lt;a href="https://github.com/kubernetes/kubernetes/issues/110714">#110714&lt;/a>). So, this limitation hits far too often and your hints are off, but then again, it&amp;rsquo;s about network latency and cost optimization first, so it&amp;rsquo;s better than nothing.&lt;/p>
&lt;h2 id="networking">Networking&lt;/h2>
&lt;p>We have talked about networking only to some small degree so far (&lt;code>readiness&lt;/code> probes, pod disruption budgets, topology aware hints). The most important component is probably your ingress load balancer - everything else is managed by Kubernetes. AWS, Azure, GCP, and also OpenStack offer multi-zonal load balancers, so make use of them. In Azure and GCP, LBs are regional whereas in AWS and OpenStack, they need to be bound to a zone, which the cloud-controller-manager does by observing the zone labels at the nodes (please note that this behavior is not always working as expected, see &lt;a href="https://github.com/kubernetes/cloud-provider-aws/issues/569">#570&lt;/a> where the AWS cloud-controller-manager is not readjusting to newly observed zones).&lt;/p>
&lt;p>Please be reminded that even if you use a service mesh like &lt;a href="https://istio.io">Istio&lt;/a>, the off-the-shelf installation/configuration usually never comes with productive settings (to simplify first-time installation and improve first-time user experience) and you will have to fine-tune your installation/configuration, much like the rest of your workload.&lt;/p>
&lt;h2 id="relevant-cluster-settings">Relevant Cluster Settings&lt;/h2>
&lt;p>Following now a summary/list of the more relevant settings you may like to tune for Gardener-managed clusters:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> controlPlane:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> highAvailability:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> failureTolerance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: zone &lt;span style="color:#008000"># valid values are `node` and `zone` (only available if your control plane resides in a region with 3+ zones)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeAPIServer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> defaultNotReadyTolerationSeconds: 300
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> defaultUnreachableTolerationSeconds: 300
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubelet:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeScheduler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> featureGates:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> MinDomainsInPodTopologySpread: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeControllerManager:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> nodeMonitorGracePeriod: 40s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> horizontalPodAutoscaler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> syncPeriod: 15s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tolerance: 0.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> downscaleStabilization: 5m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> initialReadinessDelay: 30s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cpuInitializationPeriod: 5m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> verticalPodAutoscaler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictAfterOOMThreshold: 10m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionRateBurst: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionRateLimit: -1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> evictionTolerance: 0.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> recommendationMarginFraction: 0.15
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> updaterInterval: 1m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> recommenderInterval: 1m0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clusterAutoscaler:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> expander: &lt;span style="color:#a31515">&amp;#34;least-waste&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scanInterval: 10s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownDelayAfterAdd: 60m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownDelayAfterDelete: 0s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownDelayAfterFailure: 3m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownUnneededTime: 30m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> scaleDownUtilizationThreshold: 0.5
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> provider:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> workers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> minimum: 6
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maximum: 60
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maxSurge: 3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> maxUnavailable: 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> zones:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - ... &lt;span style="color:#008000"># list of zones you want your worker pool nodes to be spread across, see above&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubelet:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ... &lt;span style="color:#008000"># similar to `kubelet` above (cluster-wide settings), but here per worker pool (pool-specific settings), see above&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineControllerManager: &lt;span style="color:#008000"># optional, it allows to configure the machine-controller settings.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineCreationTimeout: 20m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineHealthTimeout: 10m
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> machineDrainTimeout: 60h
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> systemComponents:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> coreDNS:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> autoscaling:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mode: horizontal &lt;span style="color:#008000"># valid values are `horizontal` (driven by CPU load) and `cluster-proportional` (driven by number of nodes/cores)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h4 id="on-speccontrolplanehighavailabilityfailuretolerancetype">On &lt;code>spec.controlPlane.highAvailability.failureTolerance.type&lt;/code>&lt;/h4>
&lt;p>If set, determines the degree of failure tolerance for your control plane. &lt;code>zone&lt;/code> is preferred, but only available if your control plane resides in a region with 3+ zones. See &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/#control-plane">above&lt;/a> and the &lt;a href="https://gardener.cloud/docs/guides/high-availability/control-plane/">docs&lt;/a>.&lt;/p>
&lt;h4 id="on-speckuberneteskubeapiserverdefaultunreachabletolerationseconds-and-defaultnotreadytolerationseconds">On &lt;code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds&lt;/code> and &lt;code>defaultNotReadyTolerationSeconds&lt;/code>&lt;/h4>
&lt;p>This is a very interesting &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver">API server setting&lt;/a> that lets Kubernetes decide how fast to evict pods from nodes whose status condition of type &lt;code>Ready&lt;/code> is either &lt;code>Unknown&lt;/code> (node status unknown, a.k.a unreachable) or &lt;code>False&lt;/code> (&lt;code>kubelet&lt;/code> not ready) (see &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#condition">node status conditions&lt;/a>; please note that &lt;code>kubectl&lt;/code> shows both values as &lt;code>NotReady&lt;/code> which is a somewhat &amp;ldquo;simplified&amp;rdquo; visualization).&lt;/p>
&lt;p>You can also override the cluster-wide API server settings &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions">individually per pod&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tolerations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: &lt;span style="color:#a31515">&amp;#34;node.kubernetes.io/unreachable&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operator: &lt;span style="color:#a31515">&amp;#34;Exists&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> effect: &lt;span style="color:#a31515">&amp;#34;NoExecute&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tolerationSeconds: 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - key: &lt;span style="color:#a31515">&amp;#34;node.kubernetes.io/not-ready&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> operator: &lt;span style="color:#a31515">&amp;#34;Exists&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> effect: &lt;span style="color:#a31515">&amp;#34;NoExecute&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tolerationSeconds: 0
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This will evict pods on unreachable or not-ready nodes immediately, but be cautious: &lt;code>0&lt;/code> is very aggressive and may lead to unnecessary disruptions. Again, you must decide for your own workload and balance out the pros and cons (e.g. long startup time).&lt;/p>
&lt;p>Please note, these settings replace &lt;code>spec.kubernetes.kubeControllerManager.podEvictionTimeout&lt;/code> that was deprecated with Kubernetes &lt;code>v1.26&lt;/code> (and acted as an upper bound).&lt;/p>
&lt;h4 id="on-speckuberneteskubeschedulerfeaturegatesmindomainsinpodtopologyspread">On &lt;code>spec.kubernetes.kubeScheduler.featureGates.MinDomainsInPodTopologySpread&lt;/code>&lt;/h4>
&lt;p>Required to be enabled for &lt;code>minDomains&lt;/code> to work with PTSCs (beta since Kubernetes &lt;code>v1.25&lt;/code>, but off by default). See &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/#pod-topology-spread-constraints">above&lt;/a> and the &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#topologyspreadconstraints-field">docs&lt;/a>. This tells the scheduler, how many topology domains to expect (=zones in the context of this document).&lt;/p>
&lt;h4 id="on-speckuberneteskubecontrollermanagernodemonitorgraceperiod">On &lt;code>spec.kubernetes.kubeControllerManager.nodeMonitorGracePeriod&lt;/code>&lt;/h4>
&lt;p>This is another very interesting &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager">kube-controller-manager setting&lt;/a> that can help you speed up or slow down how fast a node shall be considered &lt;code>Unknown&lt;/code> (node status unknown, a.k.a unreachable) when the &lt;code>kubelet&lt;/code> is not updating its status anymore (see &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#condition">node status conditions&lt;/a>), which effects eviction (see &lt;code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds&lt;/code> and &lt;code>defaultNotReadyTolerationSeconds&lt;/code> above). The shorter the time window, the faster Kubernetes will act, but the higher the chance of flapping behavior and pod trashing, so you may want to balance that out according to your needs, otherwise stick to the default which is a reasonable compromise.&lt;/p>
&lt;h4 id="on-speckuberneteskubecontrollermanagerhorizontalpodautoscaler">On &lt;code>spec.kubernetes.kubeControllerManager.horizontalPodAutoscaler...&lt;/code>&lt;/h4>
&lt;p>This configures horizontal pod autoscaling in Gardener-managed clusters. See &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/#replicas-horizontal-scaling">above&lt;/a> and the &lt;a href="https://kubernetes.io/de/docs/tasks/run-application/horizontal-pod-autoscale">docs&lt;/a> for the detailed fields.&lt;/p>
&lt;h4 id="on-speckubernetesverticalpodautoscaler">On &lt;code>spec.kubernetes.verticalPodAutoscaler...&lt;/code>&lt;/h4>
&lt;p>This configures vertical pod autoscaling in Gardener-managed clusters. See &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/#resources-vertical-scaling">above&lt;/a> and the &lt;a href="https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md">docs&lt;/a> for the detailed fields.&lt;/p>
&lt;h4 id="on-speckubernetesclusterautoscaler">On &lt;code>spec.kubernetes.clusterAutoscaler...&lt;/code>&lt;/h4>
&lt;p>This configures node auto-scaling in Gardener-managed clusters. See &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/#worker-pools">above&lt;/a> and the &lt;a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md">docs&lt;/a> for the detailed fields, especially about &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders">expanders&lt;/a>, which may become life-saving in case of a zone outage when a resource crunch is setting in and everybody rushes to get machines in the healthy zones.&lt;/p>
&lt;p>In case of a zone outage, it is critical to understand how the cluster autoscaler will put a worker pool in one zone into &amp;ldquo;back-off&amp;rdquo; and what the consequences for your workload will be. Unfortunately, the official cluster autoscaler documentation does not explain these details, but you can find hints in the &lt;a href="https://github.com/kubernetes/autoscaler/blob/b94f340af58eb063df9ebfcd65835f9a499a69a2/cluster-autoscaler/config/autoscaling_options.go#L214-L219">source code&lt;/a>:&lt;/p>
&lt;p>If a node fails to come up, the node group (worker pool in that zone) will go into &amp;ldquo;back-off&amp;rdquo;, at first 5m, then &lt;a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/utils/backoff/exponential_backoff.go#L77-L82">exponentially longer&lt;/a> until the maximum of 30m is reached. The &amp;ldquo;back-off&amp;rdquo; is reset after 3 hours. This in turn means, that nodes must be first considered &lt;code>Unknown&lt;/code>, which happens when &lt;code>spec.kubernetes.kubeControllerManager.nodeMonitorGracePeriod&lt;/code> lapses (e.g. at the beginning of a zone outage). Then they must either remain in this state until &lt;code>spec.provider.workers.machineControllerManager.machineHealthTimeout&lt;/code> lapses for them to be recreated, which will fail in the unhealthy zone, or &lt;code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds&lt;/code> lapses for the pods to be evicted (usually faster than node replacements, depending on your configuration), which will trigger the cluster autoscaler to create more capacity, but very likely in the same zone as it tries to balance its node groups at first, which will fail in the unhealthy zone. It will be considered failed only when &lt;code>maxNodeProvisionTime&lt;/code> lapses (usually close to &lt;code>spec.provider.workers.machineControllerManager.machineCreationTimeout&lt;/code>) and only then put the node group into &amp;ldquo;back-off&amp;rdquo; and not retry for 5m (at first and then exponentially longer). Only then you can expect new node capacity to be brought up somewhere else.&lt;/p>
&lt;p>During the time of ongoing node provisioning (before a node group goes into &amp;ldquo;back-off&amp;rdquo;), the cluster autoscaler may have &amp;ldquo;virtually scheduled&amp;rdquo; pending pods onto those new upcoming nodes and will not reevaluate these pods anymore unless the node provisioning fails (which will fail during a zone outage, but the cluster autoscaler cannot know that and will therefore reevaluate its decision only after it has given up on the new nodes).&lt;/p>
&lt;p>It&amp;rsquo;s critical to keep that in mind and accommodate for it. If you have already capacity up and running, the reaction time is usually much faster with leases (whatever you set) or endpoints (&lt;code>spec.kubernetes.kubeControllerManager.nodeMonitorGracePeriod&lt;/code>), but if you depend on new/fresh capacity, the above should inform you how long you will have to wait for it and for how long pods might be pending (because capacity is generally missing and pending pods may have been &amp;ldquo;virtually scheduled&amp;rdquo; to new nodes that won&amp;rsquo;t come up until the node group goes eventually into &amp;ldquo;back-off&amp;rdquo; and nodes in the healthy zones come up).&lt;/p>
&lt;h4 id="on-specproviderworkersminimum-maximum-maxsurge-maxunavailable-zones-and-machinecontrollermanager">On &lt;code>spec.provider.workers.minimum&lt;/code>, &lt;code>maximum&lt;/code>, &lt;code>maxSurge&lt;/code>, &lt;code>maxUnavailable&lt;/code>, &lt;code>zones&lt;/code>, and &lt;code>machineControllerManager&lt;/code>&lt;/h4>
&lt;p>Each worker pool in Gardener may be configured differently. Among many other settings like machine type, root disk, Kubernetes version, &lt;code>kubelet&lt;/code> settings, and many more you can also specify the lower and upper bound for the number of machines (&lt;code>minimum&lt;/code> and &lt;code>maximum&lt;/code>), how many machines may be added additionally during a rolling update (&lt;code>maxSurge&lt;/code>) and how many machines may be in termination/recreation during a rolling update (&lt;code>maxUnavailable&lt;/code>), and of course across how many zones the nodes shall be spread (&lt;code>zones&lt;/code>).&lt;/p>
&lt;p>Gardener divides &lt;code>minimum&lt;/code>, &lt;code>maximum&lt;/code>, &lt;code>maxSurge&lt;/code>, &lt;code>maxUnavailable&lt;/code> values by the number of zones specified for this worker pool. This fact must be considered when you plan the sizing of your worker pools.&lt;/p>
&lt;p>&lt;em>Example:&lt;/em>&lt;/p>
&lt;pre tabindex="0">&lt;code> provider:
workers:
- name: ...
minimum: 6
maximum: 60
maxSurge: 3
maxUnavailable: 0
zones: [&amp;#34;a&amp;#34;, &amp;#34;b&amp;#34;, &amp;#34;c&amp;#34;]
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>The resulting &lt;code>MachineDeployment&lt;/code>s &lt;strong>per zone&lt;/strong> will get &lt;code>minimum: 2&lt;/code>, &lt;code>maximum: 20&lt;/code>, &lt;code>maxSurge: 1&lt;/code>, &lt;code>maxUnavailable: 0&lt;/code>.&lt;/li>
&lt;li>If another zone is added all values will be divided by &lt;code>4&lt;/code>, resulting in:
&lt;ul>
&lt;li>Less workers per zone.&lt;/li>
&lt;li>⚠️ One &lt;code>MachineDeployment&lt;/code> with &lt;code>maxSurge: 0&lt;/code>, i.e. there will be a replacement of nodes without rolling updates.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Interesting is also the configuration for Gardener&amp;rsquo;s machine-controller-manager or MCM for short that provisions, monitors, terminates, replaces, or updates machines that back your nodes:&lt;/p>
&lt;ul>
&lt;li>The shorter &lt;code>machineCreationTimeout&lt;/code> is, the faster MCM will retry to create a machine/node, if the process is stuck on cloud provider side. It is set to useful/practical timeouts for the different cloud providers and you probably don&amp;rsquo;t want to change those (in the context of HA at least). Please align with the cluster autoscaler&amp;rsquo;s &lt;code>maxNodeProvisionTime&lt;/code>.&lt;/li>
&lt;li>The shorter &lt;code>machineHealthTimeout&lt;/code> is, the faster MCM will replace machines/nodes in case the kubelet isn&amp;rsquo;t reporting back, which translates to &lt;code>Unknown&lt;/code>, or reports back with &lt;code>NotReady&lt;/code>, or the &lt;a href="https://github.com/kubernetes/node-problem-detector">node-problem-detector&lt;/a> that Gardener deploys for you reports a non-recoverable issue/condition (e.g. read-only file system). If it is too short however, you risk node and pod trashing, so be careful.&lt;/li>
&lt;li>The shorter &lt;code>machineDrainTimeout&lt;/code> is, the faster you can get rid of machines/nodes that MCM decided to remove, but this puts a cap on the grace periods and PDBs. They are respected up until the drain timeout lapses - then the machine/node will be forcefully terminated, whether or not the pods are still in termination or not even terminated because of PDBs. Those PDBs will then be violated, so be careful here as well. Please align with the cluster autoscaler&amp;rsquo;s &lt;code>maxGracefulTerminationSeconds&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>Especially the last two settings may help you recover faster from cloud provider issues.&lt;/p>
&lt;h4 id="on-specsystemcomponentscorednsautoscaling">On &lt;code>spec.systemComponents.coreDNS.autoscaling&lt;/code>&lt;/h4>
&lt;p>DNS is critical, in general and also within a Kubernetes cluster. Gardener-managed clusters deploy &lt;a href="https://coredns.io">CoreDNS&lt;/a>, a graduated CNCF project. Gardener supports 2 auto-scaling modes for it, &lt;code>horizontal&lt;/code> (using HPA based on CPU) and &lt;code>cluster-proportional&lt;/code> (using &lt;a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">cluster proportional autoscaler&lt;/a> that scales the number of pods based on the number of nodes/cores, not to be confused with the cluster autoscaler that scales nodes based on their utilization). Check out the &lt;a href="https://gardener.cloud/docs/gardener/dns-autoscaling/">docs&lt;/a>, especially the &lt;a href="https://gardener.cloud/docs/gardener/dns-autoscaling/#trade-offs-of-horizontal-and-cluster-proportional-dns-autoscaling">trade-offs&lt;/a> why you would chose one over the other (&lt;code>cluster-proportional&lt;/code> gives you more configuration options, if CPU-based horizontal scaling is insufficient to your needs). Consider also Gardener&amp;rsquo;s feature &lt;a href="https://gardener.cloud/docs/gardener/node-local-dns/">node-local DNS&lt;/a> to decouple you further from the DNS pods and stabilize DNS. Again, that&amp;rsquo;s not strictly related to HA, but may become important during a zone outage, when load patterns shift and pods start to initialize/resolve DNS records more frequently in bulk.&lt;/p>
&lt;h2 id="more-caveats">More Caveats&lt;/h2>
&lt;p>Unfortunately, there are a few more things of note when it comes to HA in a Kubernetes cluster that may be &amp;ldquo;surprising&amp;rdquo; and hard to mitigate:&lt;/p>
&lt;ul>
&lt;li>If the &lt;code>kubelet&lt;/code> restarts, it will report all pods as &lt;code>NotReady&lt;/code> on startup until it reruns its probes (&lt;a href="https://github.com/kubernetes/kubernetes/issues/100277">#100277&lt;/a>), which leads to temporary endpoint and load balancer target removal (&lt;a href="https://github.com/kubernetes/kubernetes/issues/102367">#102367&lt;/a>). This topic is somewhat controversial. Gardener uses rolling updates and a jitter to spread necessary &lt;code>kubelet&lt;/code> restarts as good as possible.&lt;/li>
&lt;li>If a &lt;code>kube-proxy&lt;/code> pod on a node turns &lt;code>NotReady&lt;/code>, all load balancer traffic to all pods (on this node) under services with &lt;code>externalTrafficPolicy&lt;/code> &lt;code>local&lt;/code> will cease as the load balancer will then take this node out of serving. This topic is somewhat controversial as well. So, please remember that &lt;code>externalTrafficPolicy&lt;/code> &lt;code>local&lt;/code> not only has the disadvantage of imbalanced traffic spreading, but also a dependency to the kube-proxy pod that may and will be unavailable during updates. Gardener uses rolling updates to spread necessary &lt;code>kube-proxy&lt;/code> updates as good as possible.&lt;/li>
&lt;/ul>
&lt;p>These are just a few additional considerations. They may or may not affect you, but other intricacies may. It&amp;rsquo;s a reminder to be watchful as Kubernetes may have one or two relevant quirks that you need to consider (and will probably only find out over time and with extensive testing).&lt;/p>
&lt;h2 id="meaningful-availability">Meaningful Availability&lt;/h2>
&lt;p>Finally, let&amp;rsquo;s go back to where we started. We recommended to measure &lt;a href="https://research.google/pubs/pub50828">meaningful availability&lt;/a>. For instance, in Gardener, we do not trust only internal signals, but track also whether Gardener or the control planes that it manages are externally available through the external DNS records and load balancers, SNI-routing Istio gateways, etc. (the same path all users must take). It&amp;rsquo;s a huge difference whether the API server&amp;rsquo;s internal readiness probe passes or the user can actually reach the API server and it does what it&amp;rsquo;s supposed to do. Most likely, you will be in a similar spot and can do the same.&lt;/p>
&lt;p>What you do with these signals is another matter. Maybe there are some actionable metrics and you can trigger some active fail-over, maybe you can only use it to improve your HA setup altogether. In our case, we also use it to deploy mitigations, e.g. via our &lt;a href="https://github.com/gardener/dependency-watchdog">dependency-watchdog&lt;/a> that watches, for instance, Gardener-managed API servers and shuts down components like the controller managers to avert cascading knock-off effects (e.g. melt-down if the &lt;code>kubelets&lt;/code> cannot reach the API server, but the controller managers can and start taking down nodes and pods).&lt;/p>
&lt;p>Either way, understanding how users perceive your service is key to the improvement process as a whole. Even if you are not struck by a zone outage, the measures above and tracking the meaningful availability will help you improve your service.&lt;/p>
&lt;p>Thank you for your interest.&lt;/p></description></item><item><title>Docs: Chaos Engineering</title><link>https://gardener.cloud/docs/guides/high-availability/chaos-engineering/</link><pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/high-availability/chaos-engineering/</guid><description>
&lt;!-- BEGIN of section that must be kept in sync with sibling tutorial -->
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Gardener provides &lt;a href="https://chaostoolkit.org">&lt;code>chaostoolkit&lt;/code>&lt;/a> modules to simulate &lt;em>compute&lt;/em> and &lt;em>network&lt;/em> outages for various cloud providers such as &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/aws">AWS&lt;/a>, &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/azure">Azure&lt;/a>, &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/gcp">GCP&lt;/a>, &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/openstack">OpenStack/Converged Cloud&lt;/a>, and &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/vsphere">VMware vSphere&lt;/a>, as well as &lt;em>pod disruptions&lt;/em> for &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/k8s">any Kubernetes cluster&lt;/a>.&lt;/p>
&lt;p>The API, parameterization, and implementation is as homogeneous as possible across the different cloud providers, so that you have only minimal effort. As a Gardener user, you benefit from an &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/garden">additional &lt;code>garden&lt;/code> module&lt;/a> that leverages the generic modules, but exposes their functionality in the most simple, homogeneous, and secure way (no need to specify cloud provider credentials, cluster credentials, or filters explicitly; retrieves credentials and stores them in memory only).&lt;/p>
&lt;h2 id="installation">Installation&lt;/h2>
&lt;p>The name of the package is &lt;code>chaosgarden&lt;/code> and it was developed and tested with Python 3.9+. It&amp;rsquo;s being published to &lt;a href="https://pypi.org/project/chaosgarden">PyPI&lt;/a>, so that you can comfortably install it via Python&amp;rsquo;s package installer &lt;a href="https://pip.pypa.io/en/stable">pip&lt;/a> (you may want to &lt;a href="https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/#creating-a-virtual-environment">create a virtual environment&lt;/a> before installing it):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>pip install chaosgarden
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ℹ️ If you want to use the &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/vsphere">VMware vSphere module&lt;/a>, please note the remarks in &lt;a href="https://github.com/gardener/chaos-engineering/blob/main/requirements.txt">&lt;code>requirements.txt&lt;/code>&lt;/a> for &lt;code>vSphere&lt;/code>. Those are not contained in the published PyPI package.&lt;/p>
&lt;p>The package can be used directly from Python scripts and supports this usage scenario with additional convenience that helps launch actions and probes in background (more on actions and probes later), so that you can compose also complex scenarios with ease.&lt;/p>
&lt;!-- END of section that must be kept in sync with sibling tutorial -->
&lt;p>If this technology is new to you, you will probably prefer the &lt;a href="https://chaostoolkit.org">&lt;code>chaostoolkit&lt;/code>&lt;/a> &lt;a href="https://chaostoolkit.org/reference/usage/cli">CLI&lt;/a> in combination with &lt;a href="https://chaostoolkit.org/reference/api/experiment">experiment files&lt;/a>, so we need to &lt;a href="https://chaostoolkit.org/reference/usage/install/#install-the-cli">install the CLI&lt;/a> next:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>pip install chaostoolkit
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please verify that it was installed properly by running:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>chaos --help
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!-- BEGIN of section that must be kept in sync with sibling tutorial -->
&lt;h2 id="usage">Usage&lt;/h2>
&lt;p>ℹ️ We assume you are using Gardener and run Gardener-managed shoot clusters. You can also use the generic cloud provider and Kubernetes &lt;code>chaosgarden&lt;/code> modules, but configuration and secrets will then differ. Please see the &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs">module docs&lt;/a> for details.&lt;/p>
&lt;!-- END of section that must be kept in sync with sibling tutorial -->
&lt;h3 id="a-simple-experiment">A Simple Experiment&lt;/h3>
&lt;p>The most important command is the &lt;a href="https://chaostoolkit.org/reference/usage/run">&lt;code>run&lt;/code>&lt;/a> command, but before we can use it, we need to compile an experiment file first. Let&amp;rsquo;s start with a simple one, invoking only a read-only 📖 action from &lt;code>chaosgarden&lt;/code> that lists cloud provider machines and networks (depends on cloud provider) for the &amp;ldquo;first&amp;rdquo; zone of one of your shoot clusters.&lt;/p>
&lt;p>Let&amp;rsquo;s assume, your project is called &lt;code>my-project&lt;/code> and your shoot is called &lt;code>my-shoot&lt;/code>, then we need to create the following experiment:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;title&amp;#34;: &lt;span style="color:#a31515">&amp;#34;assess-filters-impact&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;description&amp;#34;: &lt;span style="color:#a31515">&amp;#34;assess-filters-impact&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;method&amp;#34;: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;action&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;name&amp;#34;: &lt;span style="color:#a31515">&amp;#34;assess-filters-impact&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;provider&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;python&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;module&amp;#34;: &lt;span style="color:#a31515">&amp;#34;chaosgarden.garden.actions&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;func&amp;#34;: &lt;span style="color:#a31515">&amp;#34;assess_cloud_provider_filters_impact&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;arguments&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;zone&amp;#34;: 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;configuration&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;garden_project&amp;#34;: &lt;span style="color:#a31515">&amp;#34;my-project&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;garden_shoot&amp;#34;: &lt;span style="color:#a31515">&amp;#34;my-shoot&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!-- BEGIN of section that must be kept in sync with sibling tutorial -->
&lt;p>We are not yet there and need one more thing to do before we can run it: We need to &amp;ldquo;target&amp;rdquo; the Gardener landscape resp. Gardener API server where you have created your shoot cluster (not to be confused with your shoot cluster API server). If you do not know what this is or how to download the Gardener API server &lt;code>kubeconfig&lt;/code>, please follow &lt;a href="https://gardener.cloud/docs/dashboard/project-operations/#prerequisites">these instructions&lt;/a>. You can either download your &lt;em>personal&lt;/em> credentials or &lt;em>project&lt;/em> credentials (see &lt;a href="https://gardener.cloud/docs/dashboard/automated-resource-management/#prerequisites">creation of a &lt;code>serviceaccount&lt;/code>&lt;/a>) to interact with Gardener. For now (fastest and most convenient way, but generally not recommended), let&amp;rsquo;s use your &lt;em>personal&lt;/em> credentials, but if you later plan to automate your experiments, please use proper &lt;em>project&lt;/em> credentials (a &lt;code>serviceaccount&lt;/code> is not bound to your person, but to the project, and can be restricted using &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac">RBAC roles and role bindings&lt;/a>, which is why we recommend this for production).&lt;/p>
&lt;p>To download your &lt;em>personal&lt;/em> credentials, open the Gardener Dashboard and click on your avatar in the upper right corner of the page. Click &amp;ldquo;My Account&amp;rdquo;, then look for the &amp;ldquo;Access&amp;rdquo; pane, then &amp;ldquo;Kubeconfig&amp;rdquo;, then press the &amp;ldquo;Download&amp;rdquo; button and save the &lt;code>kubeconfig&lt;/code> to disk. Run the following command next:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>export KUBECONFIG=path/to/kubeconfig
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!-- END of section that must be kept in sync with sibling tutorial -->
&lt;p>We are now set and you can run your first experiment:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>chaos run path/to/experiment
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You should see output like this (depends on cloud provider):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-txt" data-lang="txt">&lt;span style="display:flex;">&lt;span>[INFO] Validating the experiment&amp;#39;s syntax
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Installing signal handlers to terminate all active background threads on involuntary signals (note that SIGKILL cannot be handled).
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Experiment looks valid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Running experiment: assess-filters-impact
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Steady-state strategy: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Rollbacks strategy: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] No steady state hypothesis defined. That&amp;#39;s ok, just exploring.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Playing your experiment&amp;#39;s method now...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Action: assess-filters-impact
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Validating client credentials and listing probably impacted instances and/or networks with the given arguments zone=&amp;#39;world-1a&amp;#39; and filters={&amp;#39;instances&amp;#39;: [{&amp;#39;Name&amp;#39;: &amp;#39;tag-key&amp;#39;, &amp;#39;Values&amp;#39;: [&amp;#39;kubernetes.io/cluster/shoot--my-project--my-shoot&amp;#39;]}], &amp;#39;vpcs&amp;#39;: [{&amp;#39;Name&amp;#39;: &amp;#39;tag-key&amp;#39;, &amp;#39;Values&amp;#39;: [&amp;#39;kubernetes.io/cluster/shoot--my-project--my-shoot&amp;#39;]}]}:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] 1 instance(s) would be impacted:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] - i-aabbccddeeff0000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] 1 VPC(s) would be impacted:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] - vpc-aabbccddeeff0000
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Let&amp;#39;s rollback...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] No declared rollbacks, let&amp;#39;s move on.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[INFO] Experiment ended with status: completed
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>🎉 Congratulations! You successfully ran your first &lt;code>chaosgarden&lt;/code> experiment.&lt;/p>
&lt;h3 id="a-destructive-experiment">A Destructive Experiment&lt;/h3>
&lt;p>Now let&amp;rsquo;s break 🪓 your cluster. Be advised that this experiment will be destructive in the sense that we will temporarily network-partition all nodes in one availability zone (machine termination or restart is available with &lt;code>chaosgarden&lt;/code> as well). That means, these nodes and their pods won&amp;rsquo;t be able to &amp;ldquo;talk&amp;rdquo; to other nodes, pods, and services. Also, the API server will become unreachable for them and the API server will report them as unreachable (confusingly shown as &lt;code>NotReady&lt;/code> when you run &lt;code>kubectl get nodes&lt;/code> and &lt;code>Unknown&lt;/code> in the status &lt;code>Ready&lt;/code> condition when you run &lt;code>kubectl get nodes --output yaml&lt;/code>).&lt;/p>
&lt;p>Being unreachable will trigger service endpoint and load balancer de-registration (when the node&amp;rsquo;s grace period lapses) as well as eventually pod eviction and machine replacement (which will continue to fail under test). We won&amp;rsquo;t run the experiment long enough for all of these effects to materialize, but the longer you run it, the more will happen, up to temporarily giving up/going into &amp;ldquo;back-off&amp;rdquo; for the affected worker pool in that zone. You will also see that the Kubernetes cluster autoscaler will try to create a new machine almost immediately, if pods are pending for the affected zone (which will initially fail under test, but may succeed later, which again depends on the runtime of the experiment and whether or not the cluster autoscaler goes into &amp;ldquo;back-off&amp;rdquo; or not).&lt;/p>
&lt;p>But for now, all of this doesn&amp;rsquo;t matter as we want to start &amp;ldquo;small&amp;rdquo;. You can later read up more on the various settings and effects in our &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/">best practices guide on high availability&lt;/a>.&lt;/p>
&lt;p>Please create a new experiment file, this time with this content:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;title&amp;#34;: &lt;span style="color:#a31515">&amp;#34;run-network-failure-simulation&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;description&amp;#34;: &lt;span style="color:#a31515">&amp;#34;run-network-failure-simulation&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;method&amp;#34;: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;action&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;name&amp;#34;: &lt;span style="color:#a31515">&amp;#34;run-network-failure-simulation&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;provider&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;python&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;module&amp;#34;: &lt;span style="color:#a31515">&amp;#34;chaosgarden.garden.actions&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;func&amp;#34;: &lt;span style="color:#a31515">&amp;#34;run_cloud_provider_network_failure_simulation&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;arguments&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;mode&amp;#34;: &lt;span style="color:#a31515">&amp;#34;total&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;zone&amp;#34;: 0,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;duration&amp;#34;: 60
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;rollbacks&amp;#34;: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;action&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;name&amp;#34;: &lt;span style="color:#a31515">&amp;#34;rollback-network-failure-simulation&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;provider&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;python&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;module&amp;#34;: &lt;span style="color:#a31515">&amp;#34;chaosgarden.garden.actions&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;func&amp;#34;: &lt;span style="color:#a31515">&amp;#34;rollback_cloud_provider_network_failure_simulation&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;arguments&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;mode&amp;#34;: &lt;span style="color:#a31515">&amp;#34;total&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;zone&amp;#34;: 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;configuration&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;garden_project&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;env&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;key&amp;#34;: &lt;span style="color:#a31515">&amp;#34;GARDEN_PROJECT&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> },
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;garden_shoot&amp;#34;: {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;type&amp;#34;: &lt;span style="color:#a31515">&amp;#34;env&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &amp;#34;key&amp;#34;: &lt;span style="color:#a31515">&amp;#34;GARDEN_SHOOT&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>ℹ️ There is an even more destructive action that terminates or alternatively restarts machines in a given zone 🔥 (immediately or delayed with some randomness/chaos for maximum inconvenience for the nodes and pods). You can find links to all these examples at the end of this tutorial.&lt;/p>
&lt;p>This experiment is very similar, but this time we will break 🪓 your cluster - for &lt;code>60s&lt;/code>. If that&amp;rsquo;s too short to even see a node or pod transition from &lt;code>Ready&lt;/code> to &lt;code>NotReady&lt;/code> (actually &lt;code>Unknown&lt;/code>), then increase the &lt;code>duration&lt;/code>. Depending on the workload that your cluster runs, you may already see effects of the network partitioning, because it is effective immediately. It&amp;rsquo;s just that Kubernetes cannot know immediately and rather assumes that something is failing only &lt;strong>after&lt;/strong> the node&amp;rsquo;s grace period lapses, but the actual workload is impacted immediately.&lt;/p>
&lt;p>Most notably, this experiment also has a &lt;a href="https://chaostoolkit.org/reference/concepts/#rollbacks">&lt;code>rollbacks&lt;/code>&lt;/a> section, which is invoked even if you abort the experiment or it fails unexpectedly, but only if you run the CLI with the option &lt;code>--rollback-strategy always&lt;/code> which we will do soon. Any &lt;code>chaosgarden&lt;/code> action that can undo its activity, will do that implicitly when the &lt;code>duration&lt;/code> lapses, but it is a best practice to always configure a &lt;code>rollbacks&lt;/code> section in case something unexpected happens. Should you be in panic and just want to run the &lt;code>rollbacks&lt;/code> section, you can remove all other actions and the CLI will execute the &lt;code>rollbacks&lt;/code> section immediately.&lt;/p>
&lt;p>One other thing is different in the second experiment as well. We now read the name of the project and the shoot from the environment, i.e. a &lt;a href="https://chaostoolkit.org/reference/api/experiment/#configuration">&lt;code>configuration&lt;/code>&lt;/a> section can automatically expand &lt;a href="https://chaostoolkit.org/reference/api/experiment/#environment-configurations">environment variables&lt;/a>. Also useful to know (not shown here), &lt;code>chaostoolkit&lt;/code> supports &lt;a href="https://chaostoolkit.org/reference/api/experiment/#variable-substitution">variable substitution&lt;/a> too, so that you have to define variables only once. Please note that you can also add a &lt;a href="https://chaostoolkit.org/reference/api/experiment/#secrets">&lt;code>secrets&lt;/code>&lt;/a> section that can also automatically expand &lt;a href="https://chaostoolkit.org/reference/api/experiment/#environment-secrets">environment variables&lt;/a>. For instance, instead of targeting the Gardener API server via &lt;code>$KUBECONFIG&lt;/code>, which is supported by our &lt;code>chaosgarden&lt;/code> package natively, you can also explicitly refer to it in a &lt;code>secrets&lt;/code> section (for brevity reasons not shown here either).&lt;/p>
&lt;p>Let&amp;rsquo;s now run your second experiment (please watch your nodes and pods in parallel, e.g. by running &lt;code>watch kubectl get nodes,pods --output wide&lt;/code> in another terminal):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>export GARDEN_PROJECT=my-project
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export GARDEN_SHOOT=my-shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>chaos run --rollback-strategy always path/to/experiment
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output of the &lt;code>run&lt;/code> command will be similar to the one above, but longer. It will mention either machines or networks that were network-partitioned (depends on cloud provider), but should revert everything back to normal.&lt;/p>
&lt;p>Normally, you would not only run &lt;a href="https://chaostoolkit.org/reference/concepts/#actions">actions&lt;/a> in the &lt;code>method&lt;/code> section, but also &lt;a href="https://chaostoolkit.org/reference/concepts/#probes">probes&lt;/a> as part of a &lt;a href="https://chaostoolkit.org/reference/concepts/#steady-state-hypothesis">steady state hypothesis&lt;/a>. Such steady state hypothesis probes are run before and after the actions to validate that the &amp;ldquo;system&amp;rdquo; was in a healthy state before and gets back to a healthy state after the actions ran, hence show that the &amp;ldquo;system&amp;rdquo; is in a steady state when not under test. Eventually, you will write your own probes that don&amp;rsquo;t even have to be part of a steady state hypothesis. We at Gardener run multi-zone (multiple zones at once) and rolling-zone (strike each zone once) outages with continuous custom probes all within the &lt;code>method&lt;/code> section to validate our KPIs continuously under test (e.g. how long do the individual fail-overs take/how long is the actual outage). The most complex scenarios are even run via Python scripts as all actions and probes can also be invoked directly (which is what the CLI does).&lt;/p>
&lt;!-- BEGIN of section that must be kept in sync with sibling tutorial -->
&lt;h2 id="high-availability">High Availability&lt;/h2>
&lt;p>Developing highly available workload that can tolerate a zone outage is no trivial task. You can find more information on how to achieve this goal in our &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/">best practices guide on high availability&lt;/a>.&lt;/p>
&lt;p>Thank you for your interest in Gardener chaos engineering and making your workload more resilient.&lt;/p>
&lt;h2 id="further-reading">Further Reading&lt;/h2>
&lt;p>Here some links for further reading:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Examples&lt;/strong>: &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/tutorials/experiments">Experiments&lt;/a>, &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/tutorials/scripts">Scripts&lt;/a>&lt;/li>
&lt;li>&lt;strong>Gardener Chaos Engineering&lt;/strong>: &lt;a href="https://github.com/gardener/chaos-engineering">GitHub&lt;/a>, &lt;a href="https://pypi.org/project/chaosgarden">PyPI&lt;/a>, &lt;a href="https://github.com/gardener/chaos-engineering/tree/main/docs/garden">Module Docs for Gardener Users&lt;/a>&lt;/li>
&lt;li>&lt;strong>Chaos Toolkit Core&lt;/strong>: &lt;a href="https://chaostoolkit.org">Home Page&lt;/a>, &lt;a href="https://chaostoolkit.org/reference/usage/install">Installation&lt;/a>, &lt;a href="https://chaostoolkit.org/reference/concepts">Concepts&lt;/a>, &lt;a href="https://github.com/chaostoolkit/chaostoolkit">GitHub&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- END of section that must be kept in sync with sibling tutorial --></description></item><item><title>Docs: Control Plane</title><link>https://gardener.cloud/docs/guides/high-availability/control-plane/</link><pubDate>Fri, 17 Mar 2023 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/high-availability/control-plane/</guid><description>
&lt;h1 id="highly-available-shoot-control-plane">Highly Available Shoot Control Plane&lt;/h1>
&lt;p>Shoot resource offers a way to request for a highly available control plane.&lt;/p>
&lt;h2 id="failure-tolerance-types">Failure Tolerance Types&lt;/h2>
&lt;p>A highly available shoot control plane can be setup with either a failure tolerance of &lt;code>zone&lt;/code> or &lt;code>node&lt;/code>.&lt;/p>
&lt;h3 id="node-failure-tolerance">&lt;code>Node&lt;/code> Failure Tolerance&lt;/h3>
&lt;p>The failure tolerance of a &lt;code>node&lt;/code> will have the following characteristics:&lt;/p>
&lt;ul>
&lt;li>Control plane components will be spread across different nodes within a single availability zone. There will not be
more than one replica per node for each control plane component which has more than one replica.&lt;/li>
&lt;li>&lt;code>Worker pool&lt;/code> should have a minimum of 3 nodes.&lt;/li>
&lt;li>A multi-node etcd (quorum size of 3) will be provisioned, offering zero-downtime capabilities with each member in a
different node within a single availability zone.&lt;/li>
&lt;/ul>
&lt;h3 id="zone-failure-tolerance">&lt;code>Zone&lt;/code> Failure Tolerance&lt;/h3>
&lt;p>The failure tolerance of a &lt;code>zone&lt;/code> will have the following characteristics:&lt;/p>
&lt;ul>
&lt;li>Control plane components will be spread across different availability zones. There will be at least
one replica per zone for each control plane component which has more than one replica.&lt;/li>
&lt;li>Gardener scheduler will automatically select a &lt;code>seed&lt;/code> which has a minimum of 3 zones to host the shoot control plane.&lt;/li>
&lt;li>A multi-node etcd (quorum size of 3) will be provisioned, offering zero-downtime capabilities with each member in a
different zone.&lt;/li>
&lt;/ul>
&lt;h2 id="shoot-spec">Shoot Spec&lt;/h2>
&lt;p>To request for a highly available shoot control plane Gardener provides the following configuration in the shoot spec:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: core.gardener.cloud/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> controlPlane:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> highAvailability:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> failureTolerance:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: &amp;lt;node | zone&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Allowed Transitions&lt;/strong>&lt;/p>
&lt;p>If you already have a shoot cluster with non-HA control plane, then the following upgrades are possible:&lt;/p>
&lt;ul>
&lt;li>Upgrade of non-HA shoot control plane to HA shoot control plane with &lt;code>node&lt;/code> failure tolerance.&lt;/li>
&lt;li>Upgrade of non-HA shoot control plane to HA shoot control plane with &lt;code>zone&lt;/code> failure tolerance. However, it is essential that the &lt;code>seed&lt;/code> which is currently hosting the shoot control plane should be &lt;code>multi-zonal&lt;/code>. If it is not, then the request to upgrade will be rejected.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> There will be a small downtime during the upgrade, especially for etcd, which will transition from a single node etcd cluster to a multi-node etcd cluster.&lt;/p>
&lt;/blockquote>
&lt;p>&lt;strong>Disallowed Transitions&lt;/strong>&lt;/p>
&lt;p>If you already have a shoot cluster with HA control plane, then the following transitions are not possible:&lt;/p>
&lt;ul>
&lt;li>Upgrade of HA shoot control plane from &lt;code>node&lt;/code> failure tolerance to &lt;code>zone&lt;/code> failure tolerance is currently not supported, mainly because already existing volumes are bound to the zone they were created in originally.&lt;/li>
&lt;li>Downgrade of HA shoot control plane with &lt;code>zone&lt;/code> failure tolerance to &lt;code>node&lt;/code> failure tolerance is currently not supported, mainly because of the same reason as above, that already existing volumes are bound to the respective zones they were created in originally.&lt;/li>
&lt;li>Downgrade of HA shoot control plane with either &lt;code>node&lt;/code> or &lt;code>zone&lt;/code> failure tolerance, to a non-HA shoot control plane is currently not supported, mainly because &lt;a href="https://github.com/gardener/etcd-druid">etcd-druid&lt;/a> does not currently support scaling down of a multi-node etcd cluster to a single-node etcd cluster.&lt;/li>
&lt;/ul>
&lt;h2 id="zone-outage-situation">Zone Outage Situation&lt;/h2>
&lt;p>Implementing highly available software that can tolerate even a zone outage unscathed is no trivial task. You may find our &lt;a href="https://gardener.cloud/docs/guides/high-availability/best-practices/">HA Best Practices&lt;/a> helpful to get closer to that goal. In this document, we collected many options and settings for you that also Gardener internally uses to provide a highly available service.&lt;/p>
&lt;p>During a zone outage, you may be forced to change your cluster setup on short notice in order to compensate for failures and shortages resulting from the outage.
For instance, if the shoot cluster has worker nodes across three zones where one zone goes down, the computing power from these nodes is also gone during that time.
Changing the worker pool (&lt;code>shoot.spec.provider.workers[]&lt;/code>) and infrastructure (&lt;code>shoot.spec.provider.infrastructureConfig&lt;/code>) configuration can eliminate this disbalance, having enough machines in healthy availability zones that can cope with the requests of your applications.&lt;/p>
&lt;p>Gardener relies on a sophisticated reconciliation flow with several dependencies for which various flow steps wait for the &lt;em>readiness&lt;/em> of prior ones.
During a zone outage, this can block the entire flow, e.g., because all three &lt;code>etcd&lt;/code> replicas can never be ready when a zone is down, and required changes mentioned above can never be accomplished.
For this, a special one-off annotation &lt;code>shoot.gardener.cloud/skip-readiness&lt;/code> helps to skip any readiness checks in the flow.&lt;/p>
&lt;blockquote>
&lt;p>The &lt;code>shoot.gardener.cloud/skip-readiness&lt;/code> annotation serves as a last resort if reconciliation is stuck because of important changes during an AZ outage. Use it with caution, only in exceptional cases and after a case-by-case evaluation with your Gardener landscape administrator. If used together with other operations like Kubernetes version upgrades or credential rotation, the annotation may lead to a severe outage of your shoot control plane.&lt;/p>
&lt;/blockquote></description></item><item><title>Docs: Manage certificates with Gardener for default domain</title><link>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_default_domain_cert/</link><pubDate>Wed, 20 Jul 2022 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_default_domain_cert/</guid><description>
&lt;h1 id="manage-certificates-with-gardener-for-default-domain">Manage certificates with Gardener for default domain&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Dealing with applications on Kubernetes which offer a secure service endpoints (e.g. HTTPS) also require you to enable a
secured communication via SSL/TLS. With the &lt;a href="https://github.com/gardener/gardener-extension-shoot-cert-service">certificate extension&lt;/a> enabled, Gardener can manage commonly trusted X.509 certificate for your application
endpoint. From initially requesting certificate, it also handeles their renewal in time using the free Let&amp;rsquo;s Encrypt API.&lt;/p>
&lt;p>&lt;strong>There are two senarios with which you can use the certificate extension&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>You want to use a certificate for a subdomain the shoot&amp;rsquo;s default DNS (see &lt;code>.spec.dns.domain&lt;/code> of your shoot resource, e.g. &lt;code>short.ingress.shoot.project.default-domain.gardener.cloud&lt;/code>). If this is your case, please keep reading this article.&lt;/li>
&lt;li>You want to use a certificate for a custom domain. If this is your case, please see &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/">Manage certificates with Gardener for public domain&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>Before you start this guide there are a few requirements you need to fulfill:&lt;/p>
&lt;ul>
&lt;li>You have an existing shoot cluster&lt;/li>
&lt;/ul>
&lt;p>Since you are using the default DNS name, all DNS configuration should already be done and ready.&lt;/p>
&lt;h2 id="issue-a-certificate">Issue a certificate&lt;/h2>
&lt;p>Every X.509 certificate is represented by a Kubernetes custom resource &lt;code>certificate.cert.gardener.cloud&lt;/code> in your cluster. A &lt;code>Certificate&lt;/code> resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener&amp;rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.&lt;/p>
&lt;blockquote>
&lt;p>Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.&lt;/p>
&lt;/blockquote>
&lt;p>Certificates can be requested via 3 resources type&lt;/p>
&lt;ul>
&lt;li>Ingress&lt;/li>
&lt;li>Service (type LoadBalancer)&lt;/li>
&lt;li>certificate (Gardener CRD)&lt;/li>
&lt;/ul>
&lt;p>If either of the first 2 are used, a corresponding &lt;code>Certificate&lt;/code> resource will automatically be created.&lt;/p>
&lt;h3 id="using-an-ingress-resource">Using an ingress Resource&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: networking.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/purpose: managed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/issuer: custom-issuer # optional to specify custom issuer (use namespace/name for shoot issuers)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/follow-cname: &amp;#34;true&amp;#34; # optional, same as spec.followCNAME in certificates&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/secret-labels: &amp;#34;key1=value1,key2=value2&amp;#34; # optional labels for the certificate secret&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/preferred-chain: &amp;#34;chain name&amp;#34; # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-algorithm: ECDSA # optional to specify algorithm for private key, allowed values are &amp;#39;RSA&amp;#39; or &amp;#39;ECDSA&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-size: &amp;#34;384&amp;#34; # optional to specify size of private key, allowed values for RSA are &amp;#34;2048&amp;#34;, &amp;#34;3072&amp;#34;, &amp;#34;4096&amp;#34; and for ECDSA &amp;#34;256&amp;#34; and &amp;#34;384&amp;#34;spec:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tls:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - hosts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Must not exceed 64 characters.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - short.ingress.shoot.project.default-domain.gardener.cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Certificate and private key reside in this secret.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretName: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - host: short.ingress.shoot.project.default-domain.gardener.cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> http:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> paths:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - pathType: Prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: &lt;span style="color:#a31515">&amp;#34;/&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backend:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> number: 8080
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="using-a-service-type-loadbalancer">Using a service type LoadBalancer&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/purpose: managed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Certificate and private key reside in this secret.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/secretname: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># You may add more domains separated by commas (e.g. &amp;#34;service.shoot.project.default-domain.gardener.cloud, amazing.shoot.project.default-domain.gardener.cloud&amp;#34;)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns.gardener.cloud/dnsnames: &lt;span style="color:#a31515">&amp;#34;service.shoot.project.default-domain.gardener.cloud&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns.gardener.cloud/ttl: &lt;span style="color:#a31515">&amp;#34;600&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/issuer: custom-issuer # optional to specify custom issuer (use namespace/name for shoot issuers)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/follow-cname: &amp;#34;true&amp;#34; # optional, same as spec.followCNAME in certificates&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/secret-labels: &amp;#34;key1=value1,key2=value2&amp;#34; # optional labels for the certificate secret&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/preferred-chain: &amp;#34;chain name&amp;#34; # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-algorithm: ECDSA # optional to specify algorithm for private key, allowed values are &amp;#39;RSA&amp;#39; or &amp;#39;ECDSA&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-size: &amp;#34;384&amp;#34; # optional to specify size of private key, allowed values for RSA are &amp;#34;2048&amp;#34;, &amp;#34;3072&amp;#34;, &amp;#34;4096&amp;#34; and for ECDSA &amp;#34;256&amp;#34; and &amp;#34;384&amp;#34; name: test-service&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ports:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: http
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port: 80
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> protocol: TCP
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> targetPort: 8080
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: LoadBalancer
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="using-the-custom-certificate-resource">Using the custom Certificate resource&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: cert.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: cert-example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> commonName: short.ingress.shoot.project.default-domain.gardener.cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Optionnal if using the default issuer&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: garden
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you&amp;rsquo;re interested in the current progress of your request, you&amp;rsquo;re advised to consult the description, more specifically the &lt;code>status&lt;/code> attribute in case the issuance failed.&lt;/p>
&lt;h2 id="request-a-wildcard-certificate">Request a wildcard certificate&lt;/h2>
&lt;p>In order to avoid the creation of multiples certificates for every single endpoints, you may want to create a wildcard certificate for your shoot&amp;rsquo;s default cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: networking.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/purpose: managed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/commonName: &lt;span style="color:#a31515">&amp;#34;*.ingress.shoot.project.default-domain.gardener.cloud&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tls:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - hosts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - amazing.ingress.shoot.project.default-domain.gardener.cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretName: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - host: amazing.ingress.shoot.project.default-domain.gardener.cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> http:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> paths:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - pathType: Prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: &lt;span style="color:#a31515">&amp;#34;/&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backend:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> number: 8080
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please note that this can also be achived by directly adding an annotation to a Service type LoadBalancer. You could also create a Certificate object with a wildcard domain.&lt;/p>
&lt;h2 id="more-information">More information&lt;/h2>
&lt;p>For more information and more examples about using the certificate extension, please see &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/">Manage certificates with Gardener for public domain&lt;/a>&lt;/p></description></item><item><title>Docs: Manage certificates with Gardener for public domain</title><link>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/</link><pubDate>Wed, 20 Jul 2022 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/</guid><description>
&lt;h1 id="manage-certificates-with-gardener-for-public-domain">Manage certificates with Gardener for public domain&lt;/h1>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Dealing with applications on Kubernetes which offer a secure service endpoints (e.g. HTTPS) also require you to enable a
secured communication via SSL/TLS. With the &lt;a href="https://github.com/gardener/gardener-extension-shoot-cert-service">certificate extension&lt;/a> enabled, Gardener can manage commonly trusted X.509 certificate for your application
endpoint. From initially requesting certificate, it also handeles their renewal in time using the free Let&amp;rsquo;s Encrypt API.&lt;/p>
&lt;p>&lt;strong>There are two senarios with which you can use the certificate extension&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>You want to use a certificate for a subdomain the shoot&amp;rsquo;s default DNS (see &lt;code>.spec.dns.domain&lt;/code> of your shoot resource, e.g. &lt;code>short.ingress.shoot.project.default-domain.gardener.cloud&lt;/code>). If this is your case, please see &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_default_domain_cert/">Manage certificates with Gardener for default domain&lt;/a>&lt;/li>
&lt;li>You want to use a certificate for a custom domain. If this is your case, please keep reading this article.&lt;/li>
&lt;/ul>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>Before you start this guide there are a few requirements you need to fulfill:&lt;/p>
&lt;ul>
&lt;li>You have an existing shoot cluster&lt;/li>
&lt;li>Your custom domain is under a &lt;a href="https://www.iana.org/domains/root/db">public top level domain&lt;/a> (e.g. &lt;code>.com&lt;/code>)&lt;/li>
&lt;li>Your custom zone is resolvable with a public resolver via the internet (e.g. &lt;code>8.8.8.8&lt;/code>)&lt;/li>
&lt;li>You have a custom DNS provider configured and working (see &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-dns-service/dns_providers/">&amp;ldquo;DNS Providers&amp;rdquo;&lt;/a>)&lt;/li>
&lt;/ul>
&lt;p>As part of the &lt;a href="https://letsencrypt.org/">Let&amp;rsquo;s Encrypt&lt;/a> &lt;a href="https://tools.ietf.org/html/rfc8555">ACME&lt;/a> challenge validation process, Gardener sets a DNS TXT entry and Let&amp;rsquo;s Encrypt checks if it can both resolve and authenticate it. Therefore, it&amp;rsquo;s important that your DNS-entries are publicly resolvable. You can check this by querying e.g. Googles public DNS server and if it returns an entry your DNS is publicly visible:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># returns the A record for cert-example.example.com using Googles DNS server (8.8.8.8)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>dig cert-example.example.com @8.8.8.8 A
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="dns-provider">DNS provider&lt;/h3>
&lt;p>In order to issue certificates for a custom domain you need to specify a DNS provider which is permitted to create DNS records for subdomains of your requested domain in the certificate. For example, if you request a certificate for &lt;code>host.example.com&lt;/code> your DNS provider must be capable of managing subdomains of &lt;code>host.example.com&lt;/code>.&lt;/p>
&lt;p>DNS providers are normally specified in the shoot manifest. To learn more on how to configure one, please see the &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-dns-service/dns_providers/">DNS provider&lt;/a> documentation.&lt;/p>
&lt;h2 id="issue-a-certificate">Issue a certificate&lt;/h2>
&lt;p>Every X.509 certificate is represented by a Kubernetes custom resource &lt;code>certificate.cert.gardener.cloud&lt;/code> in your cluster. A &lt;code>Certificate&lt;/code> resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener&amp;rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.&lt;/p>
&lt;blockquote>
&lt;p>Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.&lt;/p>
&lt;/blockquote>
&lt;p>Certificates can be requested via 3 resources type&lt;/p>
&lt;ul>
&lt;li>Ingress&lt;/li>
&lt;li>Service (type LoadBalancer)&lt;/li>
&lt;li>Gateways (both Istio gateways and from the Gateway API)&lt;/li>
&lt;li>Certificate (Gardener CRD)&lt;/li>
&lt;/ul>
&lt;p>If either of the first 2 are used, a corresponding &lt;code>Certificate&lt;/code> resource will be created automatically.&lt;/p>
&lt;h3 id="using-an-ingress-resource">Using an Ingress Resource&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: networking.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/purpose: managed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Optional but recommended, this is going to create the DNS entry at the same time&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns.gardener.cloud/class: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns.gardener.cloud/ttl: &lt;span style="color:#a31515">&amp;#34;600&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/commonname: &amp;#34;*.example.com&amp;#34; # optional, if not specified the first name from spec.tls[].hosts is used as common name&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/dnsnames: &amp;#34;&amp;#34; # optional, if not specified the names from spec.tls[].hosts are used&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/follow-cname: &amp;#34;true&amp;#34; # optional, same as spec.followCNAME in certificates&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/secret-labels: &amp;#34;key1=value1,key2=value2&amp;#34; # optional labels for the certificate secret&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/issuer: custom-issuer # optional to specify custom issuer (use namespace/name for shoot issuers)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/preferred-chain: &amp;#34;chain name&amp;#34; # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-algorithm: ECDSA # optional to specify algorithm for private key, allowed values are &amp;#39;RSA&amp;#39; or &amp;#39;ECDSA&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-size: &amp;#34;384&amp;#34; # optional to specify size of private key, allowed values for RSA are &amp;#34;2048&amp;#34;, &amp;#34;3072&amp;#34;, &amp;#34;4096&amp;#34; and for ECDSA &amp;#34;256&amp;#34; and &amp;#34;384&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tls:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - hosts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Must not exceed 64 characters.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - amazing.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Certificate and private key reside in this secret.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretName: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - host: amazing.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> http:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> paths:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - pathType: Prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: &lt;span style="color:#a31515">&amp;#34;/&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backend:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> number: 8080
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Replace the &lt;code>hosts&lt;/code> and &lt;code>rules[].host&lt;/code> value again with your own domain and adjust the remaining Ingress attributes in accordance with your deployment (e.g. the above is for an &lt;code>istio&lt;/code> Ingress controller and forwards traffic to a &lt;code>service1&lt;/code> on port 80).&lt;/p>
&lt;h3 id="using-a-service-of-type-loadbalancer">Using a Service of type LoadBalancer&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/secretname: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns.gardener.cloud/dnsnames: example.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns.gardener.cloud/class: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Optional&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dns.gardener.cloud/ttl: &lt;span style="color:#a31515">&amp;#34;600&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/commonname: &lt;span style="color:#a31515">&amp;#34;*.example.example.com&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/dnsnames: &lt;span style="color:#a31515">&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/follow-cname: &amp;#34;true&amp;#34; # optional, same as spec.followCNAME in certificates&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/secret-labels: &amp;#34;key1=value1,key2=value2&amp;#34; # optional labels for the certificate secret&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/issuer: custom-issuer # optional to specify custom issuer (use namespace/name for shoot issuers)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/preferred-chain: &amp;#34;chain name&amp;#34; # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-algorithm: ECDSA # optional to specify algorithm for private key, allowed values are &amp;#39;RSA&amp;#39; or &amp;#39;ECDSA&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#cert.gardener.cloud/private-key-size: &amp;#34;384&amp;#34; # optional to specify size of private key, allowed values for RSA are &amp;#34;2048&amp;#34;, &amp;#34;3072&amp;#34;, &amp;#34;4096&amp;#34; and for ECDSA &amp;#34;256&amp;#34; and &amp;#34;384&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: test-service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ports:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: http
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port: 80
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> protocol: TCP
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> targetPort: 8080
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> type: LoadBalancer
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="using-a-gateway-resource">Using a Gateway resource&lt;/h3>
&lt;p>Please see &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/tutorials/istio-gateways/">Istio Gateways&lt;/a> or &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/tutorials/gateway-api-gateways/">Gateway API&lt;/a> for details.&lt;/p>
&lt;h3 id="using-the-custom-certificate-resource">Using the custom Certificate resource&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: cert.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: cert-example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> commonName: amazing.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Optionnal if using the default issuer&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: garden
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># If delegated domain for DNS01 challenge should be used. This has only an effect if a CNAME record is set for&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># &amp;#39;_acme-challenge.amazing.example.com&amp;#39;.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># For example: If a CNAME record exists &amp;#39;_acme-challenge.amazing.example.com&amp;#39; =&amp;gt; &amp;#39;_acme-challenge.writable.domain.com&amp;#39;,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># the DNS challenge will be written to &amp;#39;_acme-challenge.writable.domain.com&amp;#39;.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#followCNAME: true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># optionally set labels for the secret&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#secretLabels:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># key1: value1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># key2: value2&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Optionally specify the preferred certificate chain: if the CA offers multiple certificate chains, prefer the chain with an issuer matching this Subject Common Name. If no match, the default offered chain will be used.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#preferredChain: &amp;#34;ISRG Root X1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># Optionally specify algorithm and key size for private key. Allowed algorithms: &amp;#34;RSA&amp;#34; (allowed sizes: 2048, 3072, 4096) and &amp;#34;ECDSA&amp;#34; (allowed sizes: 256, 384)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># If not specified, RSA with 2048 is used.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#privateKey:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># algorithm: ECDSA&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># size: 384&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="supported-attributes">Supported attributes&lt;/h2>
&lt;p>Here is a list of all supported annotations regarding the certificate extension:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Path&lt;/th>
&lt;th>Annotation&lt;/th>
&lt;th>Value&lt;/th>
&lt;th>Required&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>N/A&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/purpose:&lt;/code>&lt;/td>
&lt;td>&lt;code>managed&lt;/code>&lt;/td>
&lt;td>Yes when using annotations&lt;/td>
&lt;td>Flag for Gardener that this specific Ingress or Service requires a certificate&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.commonName&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/commonname:&lt;/code>&lt;/td>
&lt;td>E.g. &amp;ldquo;*.demo.example.com&amp;rdquo; or &lt;br> &amp;ldquo;special.example.com&amp;rdquo;&lt;/td>
&lt;td>Certificate and Ingress : No &lt;br/> Service: Yes, if DNS names unset&lt;/td>
&lt;td>Specifies for which domain the certificate request will be created. If not specified, the names from spec.tls[].hosts are used. This entry must comply with the &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/#Character-Restrictions">64 character&lt;/a> limit.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.dnsNames&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/dnsnames:&lt;/code>&lt;/td>
&lt;td>E.g. &amp;ldquo;special.example.com&amp;rdquo;&lt;/td>
&lt;td>Certificate and Ingress : No &lt;br/> Service: Yes, if common name unset&lt;/td>
&lt;td>Additional domains the certificate should be valid for (Subject Alternative Name). If not specified, the names from spec.tls[].hosts are used. Entries in this list can be longer than 64 characters.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.secretRef.name&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/secretname:&lt;/code>&lt;/td>
&lt;td>&lt;code>any-name&lt;/code>&lt;/td>
&lt;td>Yes for certificate and Service&lt;/td>
&lt;td>Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it&amp;rsquo;ll be created automatically as soon as the certificate has been issued.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.issuerRef.name&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/issuer:&lt;/code>&lt;/td>
&lt;td>E.g. &lt;code>gardener&lt;/code>&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Specifies the issuer you want to use. Only necessary if you request certificates for &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/#Custom-Domains">custom domains&lt;/a>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>N/A&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/revoked:&lt;/code>&lt;/td>
&lt;td>&lt;code>true&lt;/code> otherwise always false&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Use only to revoke a certificate, see &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/#references">reference&lt;/a> for more details&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.followCNAME&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/follow-cname&lt;/code>&lt;/td>
&lt;td>E.g. &lt;code>true&lt;/code>&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Specifies that the usage of a delegated domain for DNS challenges is allowed. Details see &lt;a href="https://github.com/gardener/cert-management#follow-cname">Follow CNAME&lt;/a>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.preferredChain&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/preferred-chain&lt;/code>&lt;/td>
&lt;td>E.g. &lt;code>ISRG Root X1&lt;/code>&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Specifies the Common Name of the issuer for selecting the certificate chain. Details see &lt;a href="https://github.com/gardener/cert-management#preferred-chain">Preferred Chain&lt;/a>.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.secretLabels&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/secret-labels&lt;/code>&lt;/td>
&lt;td>for annotation use e.g. &lt;code>key1=value1,key2=value2&lt;/code>&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Specifies labels for the certificate secret.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.privateKey.algorithm&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/private-key-algorithm&lt;/code>&lt;/td>
&lt;td>&lt;code>RSA&lt;/code>, &lt;code>ECDSA&lt;/code>&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Specifies algorithm for private key generation. The default value is depending on configuration of the extension (default of the default is &lt;code>RSA&lt;/code>). You may request a new certificate without privateKey settings to find out the concrete defaults in your Gardener.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>spec.privateKey.size&lt;/code>&lt;/td>
&lt;td>&lt;code>cert.gardener.cloud/private-key-size&lt;/code>&lt;/td>
&lt;td>&lt;code>&amp;quot;256&amp;quot;&lt;/code>, &lt;code>&amp;quot;384&amp;quot;&lt;/code>, &lt;code>&amp;quot;2048&amp;quot;&lt;/code>, &lt;code>&amp;quot;3072&amp;quot;&lt;/code>, &lt;code>&amp;quot;4096&amp;quot;&lt;/code>&lt;/td>
&lt;td>No&lt;/td>
&lt;td>Specifies size for private key generation. Allowed values for &lt;code>RSA&lt;/code> are &lt;code>2048&lt;/code>, &lt;code>3072&lt;/code>, and &lt;code>4096&lt;/code>. For &lt;code>ECDSA&lt;/code> allowed values are &lt;code>256&lt;/code> and &lt;code>384&lt;/code>. The default values are depending on the configuration of the extension (defaults of the default values are &lt;code>3072&lt;/code> for &lt;code>RSA&lt;/code> and &lt;code>384&lt;/code> for &lt;code>ECDSA&lt;/code> respectively).&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="request-a-wildcard-certificate">Request a wildcard certificate&lt;/h2>
&lt;p>In order to avoid the creation of multiples certificates for every single endpoints, you may want to create a wildcard certificate for your shoot&amp;rsquo;s default cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: networking.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/purpose: managed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/commonName: &lt;span style="color:#a31515">&amp;#34;*.example.com&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> tls:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - hosts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - amazing.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretName: tls-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> rules:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - host: amazing.example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> http:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> paths:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - pathType: Prefix
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> path: &lt;span style="color:#a31515">&amp;#34;/&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> backend:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> service:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-svc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> port:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> number: 8080
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please note that this can also be achived by directly adding an annotation to a Service type LoadBalancer. You could also create a Certificate object with a wildcard domain.&lt;/p>
&lt;h2 id="using-a-custom-issuer">Using a custom Issuer&lt;/h2>
&lt;p>Most Gardener deployment with the certification extension enabled have a preconfigured &lt;code>garden&lt;/code> issuer. It is also usually configured to use Let&amp;rsquo;s Encrypt as the certificate provider.&lt;/p>
&lt;p>If you need a custom issuer for a specific cluster, please see &lt;a href="https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/custom_shoot_issuer/">Using a custom Issuer&lt;/a>&lt;/p>
&lt;h2 id="quotas">Quotas&lt;/h2>
&lt;p>For security reasons there may be a default quota on the certificate requests per day set globally in the controller
registration of the shoot-cert-service.&lt;/p>
&lt;p>The default quota only applies if there is no explicit quota defined for the issuer itself with the field
&lt;code>requestsPerDayQuota&lt;/code>, e.g.:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extensions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - type: shoot-cert-service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: CertConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - email: your-email@example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: custom-issuer &lt;span style="color:#008000"># issuer name must be specified in every custom issuer request, must not be &amp;#34;garden&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: &lt;span style="color:#a31515">&amp;#39;https://acme-v02.api.letsencrypt.org/directory&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requestsPerDayQuota: 10
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="dns-propagation">DNS Propagation&lt;/h2>
&lt;p>As stated before, cert-manager uses the ACME challenge protocol to authenticate that you are the DNS owner for the domain&amp;rsquo;s certificate you are requesting.
This works by creating a DNS TXT record in your DNS provider under &lt;code>_acme-challenge.example.example.com&lt;/code> containing a token to compare with. The TXT record is only applied during the domain validation.
Typically, the record is propagated within a few minutes. But if the record is not visible to the ACME server for any reasons, the certificate request is retried again after several minutes.
This means you may have to wait up to one hour after the propagation problem has been resolved before the certificate request is retried. Take a look in the events with &lt;code>kubectl describe ingress example&lt;/code> for troubleshooting.&lt;/p>
&lt;h2 id="character-restrictions">Character Restrictions&lt;/h2>
&lt;p>Due to restriction of the common name to 64 characters, you may to leave the common name unset in such cases.&lt;/p>
&lt;p>For example, the following request is invalid:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: cert.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: cert-invalid
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> commonName: morethan64characters.ingress.shoot.project.default-domain.gardener.cloud
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But it is valid to request a certificate for this domain if you have left the common name unset:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: cert.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: cert-example
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: default
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> dnsNames:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - morethan64characters.ingress.shoot.project.default-domain.gardener.cloud
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="references">References&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/gardener/cert-management">Gardener cert-management&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener-extension-shoot-dns-service">Managing DNS with Gardener&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Using a custom Issuer</title><link>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/custom_shoot_issuer/</link><pubDate>Wed, 20 Jul 2022 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/extensions/others/gardener-extension-shoot-cert-service/custom_shoot_issuer/</guid><description>
&lt;h1 id="using-a-custom-issuer">Using a custom Issuer&lt;/h1>
&lt;p>Another possibility to request certificates for custom domains is a dedicated issuer.&lt;/p>
&lt;blockquote>
&lt;p>Note: This is only needed if the default issuer provided by Gardener is restricted to shoot related domains or you are using domain names not visible to public DNS servers. &lt;strong>Which means that your senario most likely doesn&amp;rsquo;t require your to add an issuer&lt;/strong>.&lt;/p>
&lt;/blockquote>
&lt;p>The custom issuers are specified normally in the shoot manifest. If the &lt;code>shootIssuers&lt;/code> feature is enabled, it can alternatively be defined in the shoot cluster.&lt;/p>
&lt;h2 id="custom-issuer-in-the-shoot-manifest">Custom issuer in the shoot manifest&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extensions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - type: shoot-cert-service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: CertConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - email: your-email@example.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: custom-issuer &lt;span style="color:#008000"># issuer name must be specified in every custom issuer request, must not be &amp;#34;garden&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: &lt;span style="color:#a31515">&amp;#39;https://acme-v02.api.letsencrypt.org/directory&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> privateKeySecretName: my-privatekey &lt;span style="color:#008000"># referenced resource, the private key must be stored in the secret at `data.privateKey` (optionally, only needed as alternative to auto registration) &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#precheckNameservers: # to provide special set of nameservers to be used for prechecking DNSChallenges for an issuer&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#- dns1.private.company-net:53&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#- dns2.private.company-net:53&amp;#34; &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#shootIssuers:&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000"># if true, allows to specify issuers in the shoot cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000">#enabled: true &lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: my-privatekey
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> resourceRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: custom-issuer-privatekey &lt;span style="color:#008000"># name of secret in Gardener project&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you are using an ACME provider for private domains, you may need to change the nameservers used for
checking the availability of the DNS challenge&amp;rsquo;s TXT record before the certificate is requested from the ACME provider.
By default, only public DNS servers may be used for this purpose.
At least one of the &lt;code>precheckNameservers&lt;/code> must be able to resolve the private domain names.&lt;/p>
&lt;h3 id="using-the-custom-issuer">Using the custom issuer&lt;/h3>
&lt;p>To use the custom issuer in a certificate, just specify its name in the spec.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: cert.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: custom-issuer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For source resources like &lt;code>Ingress&lt;/code> or &lt;code>Service&lt;/code> use the &lt;code>cert.gardener.cloud/issuer&lt;/code> annotation.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: networking.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/purpose: managed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/issuer: custom-issuer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="custom-issuer-in-the-shoot-cluster">Custom issuer in the shoot cluster&lt;/h2>
&lt;p>&lt;em>Prerequiste&lt;/em>: The &lt;code>shootIssuers&lt;/code> feature has to be enabled.
It is either enabled globally in the &lt;code>ControllerDeployment&lt;/code> or in the shoot manifest
with:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> extensions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - type: shoot-cert-service
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> providerConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: CertConfig
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> shootIssuers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> enabled: &lt;span style="color:#00f">true&lt;/span> &lt;span style="color:#008000"># if true, allows to specify issuers in the shoot cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Example for specifying an &lt;code>Issuer&lt;/code> resource and its &lt;code>Secret&lt;/code> directly in any
namespace of the shoot cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: cert.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Issuer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-own-issuer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: my-namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> acme:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> domains:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> include:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - my.own.domain.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> email: some.user@my.own.domain.com
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> privateKeySecretRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-own-issuer-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: my-namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> server: https://acme-v02.api.letsencrypt.org/directory
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-own-issuer-secret
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: my-namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>type: Opaque
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>data:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> privateKey: ... &lt;span style="color:#008000"># replace &amp;#39;...&amp;#39; with valus encoded as base64&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="using-the-custom-shoot-issuer">Using the custom shoot issuer&lt;/h3>
&lt;p>To use the custom issuer in a certificate, just specify its name and namespace in the spec.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: cert.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Certificate
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: my-own-issuer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: my-namespace
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For source resources like &lt;code>Ingress&lt;/code> or &lt;code>Service&lt;/code> use the &lt;code>cert.gardener.cloud/issuer&lt;/code> annotation.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: networking.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: amazing-ingress
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/purpose: managed
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cert.gardener.cloud/issuer: my-namespace/my-own-issuer
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Docs: Specifying a Disruption Budget for Kubernetes Controllers</title><link>https://gardener.cloud/docs/guides/applications/pod-disruption-budget/</link><pubDate>Tue, 21 Jun 2022 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/applications/pod-disruption-budget/</guid><description>
&lt;h2 id="introduction-of-disruptions">Introduction of Disruptions&lt;/h2>
&lt;p>We need to understand that some kind of voluntary disruptions can happen to pods.
For example, they can be caused by cluster administrators who want to perform automated cluster actions, like upgrading and autoscaling clusters.
Typical application owner actions include:&lt;/p>
&lt;ul>
&lt;li>deleting the deployment or other controller that manages the pod&lt;/li>
&lt;li>updating a deployment&amp;rsquo;s pod template causing a restart&lt;/li>
&lt;li>directly deleting a pod (e.g., by accident)&lt;/li>
&lt;/ul>
&lt;h2 id="setup-pod-disruption-budgets">Setup Pod Disruption Budgets&lt;/h2>
&lt;p>Kubernetes offers a feature called PodDisruptionBudget (PDB) for each application.
A PDB limits the number of pods of a replicated application that are down simultaneously from voluntary disruptions.&lt;/p>
&lt;p>The most common use case is when you want to protect an application specified by one of the built-in Kubernetes controllers:&lt;/p>
&lt;ul>
&lt;li>Deployment&lt;/li>
&lt;li>ReplicationController&lt;/li>
&lt;li>ReplicaSet&lt;/li>
&lt;li>StatefulSet&lt;/li>
&lt;/ul>
&lt;p>A PodDisruptionBudget has three fields:&lt;/p>
&lt;ul>
&lt;li>A label selector &lt;code>.spec.selector&lt;/code> to specify the set of pods to which it applies.&lt;/li>
&lt;li>&lt;code>.spec.minAvailable&lt;/code> which is a description of the number of pods from that set that must still be available after the eviction, even in the absence of the evicted pod. minAvailable can be either an absolute number or a percentage.&lt;/li>
&lt;li>&lt;code>.spec.maxUnavailable&lt;/code> which is a description of the number of pods from that set that can be unavailable after the eviction. It can be either an absolute number or a percentage.&lt;/li>
&lt;/ul>
&lt;h2 id="cluster-upgrade-or-node-deletion-failed-due-to-pdb-violation">Cluster Upgrade or Node Deletion Failed due to PDB Violation&lt;/h2>
&lt;p>Misconfiguration of the PDB could block the cluster upgrade or node deletion processes. There are two main cases that can cause a misconfiguration.&lt;/p>
&lt;h3 id="case-1-the-replica-of-kubernetes-controllers-is-1">Case 1: The replica of Kubernetes controllers is 1&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>Only 1 replica is running: there is no &lt;code>replicaCount&lt;/code> setup or &lt;code>replicaCount&lt;/code> for the Kubernetes controllers is set to 1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>PDB configuration&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> minAvailable: 1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>To fix this PDB misconfiguration, you need to change the value of &lt;code>replicaCount&lt;/code> for the Kubernetes controllers to a number greater than 1&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="case-2-hpa-configuration-violates-pdb">Case 2: HPA configuration violates PDB&lt;/h3>
&lt;p>In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of automatically scaling the workload to match demand. The HorizontalPodAutoscaler manages the replicas field of the Kubernetes controllers.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>There is no &lt;code>replicaCount&lt;/code> setup or &lt;code>replicaCount&lt;/code> for the Kubernetes controllers is set to 1&lt;/p>
&lt;/li>
&lt;li>
&lt;p>PDB configuration&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> minAvailable: 1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>HPA configuration&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> minReplicas: 1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>To fix this PDB misconfiguration, you need to change the value of HPA &lt;code>minReplicas&lt;/code> to be greater than 1&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="related-links">Related Links&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">Specifying a Disruption Budget for Your Application&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaling&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Authenticating with an Identity Provider</title><link>https://gardener.cloud/docs/guides/administer-shoots/oidc-login/</link><pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/administer-shoots/oidc-login/</guid><description>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;p>Please read the following background material on &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens">Authenticating&lt;/a>.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Kubernetes on its own doesn’t provide any user management. In other words, users aren’t managed through Kubernetes resources. Whenever you refer to a human user it’s sufficient to use a unique ID, for example, an email address. Nevertheless, Gardener project owners can use an identity provider to authenticate user access for shoot clusters in the following way:&lt;/p>
&lt;ol>
&lt;li>&lt;a href="https://gardener.cloud/docs/guides/administer-shoots/oidc-login/#configure-an-identity-provider">Configure an Identity Provider&lt;/a> using &lt;strong>OpenID Connect&lt;/strong> (OIDC).&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/guides/administer-shoots/oidc-login/#configure-a-local-kubectl-oidc-login">Configure a local kubectl oidc-login&lt;/a> to enable &lt;code>oidc-login&lt;/code>.&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/guides/administer-shoots/oidc-login/#configure-the-shoot-cluster">Configure the shoot cluster&lt;/a> to share details of the OIDC-compliant identity provider with the Kubernetes API Server.&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/guides/administer-shoots/oidc-login/#authorize-an-authenticated-user">Authorize an authenticated user&lt;/a> using role-based access control (RBAC).&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/guides/administer-shoots/oidc-login/#verify-the-result">Verify the result&lt;/a>&lt;/li>
&lt;/ol>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Gardener allows administrators to modify aspects of the control plane setup. It gives administrators full control of how the control plane is parameterized. While this offers much flexibility, administrators need to ensure that they don’t configure a control plane that goes beyond the service level agreements of the responsible operators team.
&lt;/div>
&lt;h2 id="configure-an-identity-provider">Configure an Identity Provider&lt;/h2>
&lt;p>Create a tenant in an OIDC compatible Identity Provider. For simplicity, we use &lt;em>Auth0&lt;/em>, which has a free plan.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>In your tenant, create a client application to use authentication with &lt;code>kubectl&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Create-client-application_9d175d.png" alt="Create client application">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Provide a &lt;em>Name&lt;/em>, choose &lt;em>Native&lt;/em> as application type, and choose &lt;em>CREATE&lt;/em>.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Choose-application-type_bd5efb.png" alt="Choose application type">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>In the tab &lt;em>Settings&lt;/em>, copy the following parameters to a local text file:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;em>Domain&lt;/em>&lt;/p>
&lt;p>Corresponds to the &lt;strong>issuer&lt;/strong> in OIDC. It must be an &lt;code>https&lt;/code>-secured endpoint (Auth0 requires a trailing &lt;code>/&lt;/code> at the end). For more information, see &lt;a href="https://openid.net/specs/openid-connect-core-1_0.html#Terminology">Issuer Identifier&lt;/a>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Client ID&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>Client Secret&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Basic-information_ff08dc.png" alt="Basic information">&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Configure the client to have a callback url of &lt;code>http://localhost:8000&lt;/code>. This callback connects to your local &lt;code>kubectl oidc-login&lt;/code> plugin:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Configure-callback_fac02d.png" alt="Configure callback">&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Save your changes.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Verify that &lt;code>https://&amp;lt;Auth0 Domain&amp;gt;/.well-known/openid-configuration&lt;/code> is reachable.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Choose &lt;em>Users &amp;amp; Roles&lt;/em> &amp;gt; &lt;em>Users&lt;/em> &amp;gt; &lt;em>CREATE USERS&lt;/em> to create a user with a user and password:&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Create-user_6a8379.png" alt="Create user">&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Users must have a &lt;em>verified&lt;/em> email address.
&lt;/div>
&lt;h2 id="configure-a-local-kubectl-oidc-login">Configure a Local &lt;code>kubectl&lt;/code> &lt;code>oidc-login&lt;/code>&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Install the &lt;code>kubectl&lt;/code> plugin &lt;a href="https://github.com/int128/kubelogin">oidc-login&lt;/a>. We highly recommend the &lt;a href="https://github.com/kubernetes-sigs/krew">krew&lt;/a> installation tool, which also makes other plugins easily available.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl krew install oidc-login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The response looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>Updated the local copy of plugin index.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Installing plugin: oidc-login
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>CAVEATS:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>\
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>| See https://github.com/int128/kubelogin for more.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Installed plugin: oidc-login
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Prepare a &lt;code>kubeconfig&lt;/code> for later use:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>cp ~/.kube/config ~/.kube/config-oidc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Modify the configuration of &lt;code>~/.kube/config-oidc&lt;/code> as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Config
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>contexts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- context:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> cluster: shoot--project--mycluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user: my-oidc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: shoot--project--mycluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>users:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- name: my-oidc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> user:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> exec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiVersion: client.authentication.k8s.io/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> command: kubectl
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> args:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - oidc-login
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - get-token
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --oidc-issuer-url=https://&amp;lt;Issuer&amp;gt;/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --oidc-client-id=&amp;lt;Client ID&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --oidc-client-secret=&amp;lt;Client Secret&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - --oidc-extra-scope=email,offline_access,profile
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;p>To test our OIDC-based authentication, the context &lt;code>shoot--project--mycluster&lt;/code> of &lt;code>~/.kube/config-oidc&lt;/code> is used in a later step. For now, continue to use the configuration &lt;code>~/.kube/config&lt;/code> with administration rights for your cluster.&lt;/p>
&lt;h2 id="configure-the-shoot-cluster">Configure the Shoot Cluster&lt;/h2>
&lt;p>Modify the shoot cluster YAML as follows, using the client ID and the domain (as issuer) from the settings of the client application you created in Auth0:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>kind: Shoot
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: garden.sapcloud.io/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: mycluster
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: garden-project
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubernetes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kubeAPIServer:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> oidcConfig:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clientID: &amp;lt;Client ID&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> issuerURL: &lt;span style="color:#a31515">&amp;#34;https://&amp;lt;Issuer&amp;gt;/&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> usernameClaim: email
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This change of the &lt;code>Shoot&lt;/code> manifest triggers a reconciliation. Once the reconciliation is finished, your OIDC configuration is applied. It &lt;strong>doesn&amp;rsquo;t&lt;/strong> invalidate other certificate-based authentication methods. Wait for Gardener to reconcile the change. It can take up to 5 minutes.&lt;/p>
&lt;h2 id="authorize-an-authenticated-user">Authorize an Authenticated User&lt;/h2>
&lt;p>In Auth0, you created a user with a verified email address, &lt;code>test@test.com&lt;/code> in our example. For simplicity, we authorize a single user identified by this email address with the cluster role &lt;code>view&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: rbac.authorization.k8s.io/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: ClusterRoleBinding
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: viewer-test
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>roleRef:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: ClusterRole
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: view
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>subjects:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>- apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> kind: User
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: test@test.com
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As administrator, apply the cluster role binding in your shoot cluster.&lt;/p>
&lt;h2 id="verify-the-result">Verify the Result&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>To step into the shoes of your user, use the prepared &lt;code>kubeconfig&lt;/code> file &lt;code>~/.kube/config-oidc&lt;/code>, and switch to the context that uses &lt;code>oidc-login&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>cd ~/.kube
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>export KUBECONFIG=$(pwd)/config-oidc
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl config use-context `shoot--project--mycluster`
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>&lt;code>kubectl&lt;/code> delegates the authentication to plugin &lt;code>oidc-login&lt;/code> the first time the user uses &lt;code>kubectl&lt;/code> to contact the API server, for example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>kubectl get all
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The plugin opens a browser for an interactive authentication session with Auth0, and in parallel serves a local webserver for the configured callback.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Enter your login credentials.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Login-through-identity-provider_4ab5de.png" alt="Login through identity provider">&lt;/p>
&lt;p>You should get a successful response from the API server:&lt;/p>
&lt;pre tabindex="0">&lt;code>Opening in existing browser session.
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/kubernetes ClusterIP 100.64.0.1 &amp;lt;none&amp;gt; 443/TCP 86m
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
&lt;p>After a successful login, &lt;code>kubectl&lt;/code> uses a token for authentication so that you don’t have to provide user and password for every new &lt;code>kubectl&lt;/code> command. How long the token is valid can be configured. If you want to log in again earlier, reset plugin &lt;code>oidc-login&lt;/code>:&lt;/p>
&lt;ol>
&lt;li>Delete directory &lt;code>~/.kube/cache/oidc-login&lt;/code>.&lt;/li>
&lt;li>Delete the browser cache.&lt;/li>
&lt;/ol>
&lt;/div>
&lt;ol>
&lt;li>
&lt;p>To see if your user uses the cluster role &lt;code>view&lt;/code>, do some checks with &lt;code>kubectl auth can-i&lt;/code>.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The response for the following commands should be &lt;code>no&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>kubectl auth can-i create clusterrolebindings
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>kubectl auth can-i get secrets
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>kubectl auth can-i describe secrets
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>The response for the following commands should be &lt;code>yes&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>kubectl auth can-i list pods
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>kubectl auth can-i get pods
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>If the last step is successful, you’ve configured your cluster to authenticate against an identity provider using OIDC.&lt;/p>
&lt;h2 id="related-links">Related Links&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://auth0.com/pricing/">Auth0 Pricing&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Backup and Restore of Kubernetes Objects</title><link>https://gardener.cloud/docs/guides/administer-shoots/backup-restore/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/administer-shoots/backup-restore/</guid><description>
&lt;p>&lt;img src="https://gardener.cloud/__resources/teaser_6f9405.png" alt="Don&amp;amp;rsquo;t worry &amp;amp;hellip; have a backup">&lt;/p>
&lt;h2 id="tldr">TL;DR&lt;/h2>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Details of the description might change in the near future since Heptio was taken over by VMWare which might result in different GitHub repositories or other changes. Please don&amp;rsquo;t hesitate to inform us in case you encounter any issues.
&lt;/div>
&lt;p>In general, Backup and Restore (BR) covers activities enabling an organization to bring a system back in a consistent state, e.g., after a disaster or to setup a new system. These activities vary in a very broad way depending on the applications and its persistency.&lt;/p>
&lt;p>Kubernetes objects like Pods, Deployments, NetworkPolicies, etc. configure Kubernetes internal components and might as well include external components like load balancer and persistent volumes of the cloud provider. The BR of external components and their configurations might be difficult to handle in case manual configurations were needed to prepare these components.&lt;/p>
&lt;p>To set the expectations right from the beginning, this tutorial covers the BR of Kubernetes deployments which might use persistent volumes. The BR of any manual configuration of external components, e.g., via the cloud providers console, is not covered here, as well as the BR of a whole Kubernetes system.&lt;/p>
&lt;p>This tutorial puts the focus on the open source tool &lt;a href="https://velero.io/">Velero&lt;/a> (formerly Heptio Ark) and its functionality to explain the BR process.&lt;/p>
&lt;style>
#body-inner blockquote {
border: 0;
padding: 10px;
margin-top: 40px;
margin-bottom: 40px;
border-radius: 4px;
background-color: rgba(0,0,0,0.05);
box-shadow: 0 3px 6px rgba(0,0,0,0.16), 0 3px 6px rgba(0,0,0,0.23);
position:relative;
padding-left:60px;
}
#body-inner blockquote:before {
content: "i";
font-weight: bold;
position: absolute;
top: 0;
bottom: 0;
left: 0;
background-color: #00a273;
color: white;
vertical-align: middle;
margin: auto;
width: 36px;
font-size: 30px;
text-align: center;
}
&lt;/style>
&lt;p>Basically, Velero allows you to:&lt;/p>
&lt;ul>
&lt;li>backup and restore your Kubernetes cluster resources and persistent volumes (on-demand or scheduled)&lt;/li>
&lt;li>backup or restore all objects in your cluster, or filter resources by type, namespace, and/or label&lt;/li>
&lt;li>by default, all persistent volumes are backed up (configurable)&lt;/li>
&lt;li>replicate your production environment for development and testing environments&lt;/li>
&lt;li>define an expiration date per backup&lt;/li>
&lt;li>execute pre- and post-activities in a container of a pod when a backup is created (see &lt;a href="https://velero.io/docs/main/backup-hooks/#docs">Hooks&lt;/a>)&lt;/li>
&lt;li>extend Velero by Plugins, e.g., for Object and Block store (see &lt;a href="https://velero.io/docs/main/custom-plugins/#docs">Plugins&lt;/a>)&lt;/li>
&lt;/ul>
&lt;p>Velero consists of a server side component and a client tool. The server components consists of Custom Resource Definitions (CRD) and controllers to perform the activities. The client tool communicates with the K8s API server to, e.g., create objects like a Backup object.&lt;/p>
&lt;p>The diagram below explains the backup process. When creating a backup, Velero client makes a call to the Kubernetes API server to create a Backup object (1). The BackupController notices the new Backup object, validates the object (2) and begins the backup process (3). Based on the filter settings provided by the Velero client it collects the resources in question (3). The BackupController creates a tar ball with the Kubernetes objects and stores it in the backup location, e.g., AWS S3 (4) as well as snapshots of persistent volumes (5).&lt;/p>
&lt;p>The size of the backup tar ball corresponds to the number of objects in etcd. The gzipped archive contains the &lt;code>Json&lt;/code> representations of the objects.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/backup-process_0af76a.png" alt="Backup process">&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
As of the writing of this tutorial, Velero or any other BR tool for Shoot clusters is not provided by Gardener.
&lt;/div>
&lt;h2 id="getting-started">Getting Started&lt;/h2>
&lt;p>At first, clone the &lt;a href="https://github.com/vmware-tanzu/velero-plugin-for-aws">Velero GitHub repository&lt;/a> and get the Velero client from the &lt;a href="https://github.com/vmware-tanzu/velero-plugin-for-aws/releases">releases&lt;/a> or build it from source via &lt;code>make all&lt;/code> in the main directory of the cloned GitHub repository.&lt;/p>
&lt;p>To use an AWS S3 bucket as storage for the backup files and the persistent volumes, you need to:&lt;/p>
&lt;ul>
&lt;li>create a S3 bucket as the backup target&lt;/li>
&lt;li>create an AWS IAM user for Velero&lt;/li>
&lt;li>configure the Velero server&lt;/li>
&lt;li>create a secret for your AWS credentials&lt;/li>
&lt;/ul>
&lt;p>For details about this setup, check the &lt;a href="https://github.com/vmware-tanzu/velero-plugin-for-aws#set-permissions-for-velero">Set Permissions for Velero&lt;/a> documentation. Moreover, it is possible to use other &lt;a href="https://velero.io/docs/main/supported-providers/">supported storage providers&lt;/a>.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Per default, Velero is installed in the namespace &lt;code>velero&lt;/code>. To change the namespace, check the &lt;a href="https://velero.io/docs/main/namespace/#customize-the-namespace-during-install">documentation&lt;/a>.
&lt;/div>
&lt;p>Velero offers a wide range of filter possibilities for Kubernetes resources, e.g filter by namespaces, labels or resource types. The filter settings can be combined and used as &lt;em>include&lt;/em> or &lt;em>exclude&lt;/em>, which gives a great flexibility for selecting resources.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Carefully set labels and/or use namespaces for your deployments to make the selection of the resources to be backed up easier. The best practice would be to check in advance which resources are selected with the defined filter.
&lt;/div>
&lt;h2 id="exemplary-use-cases">Exemplary Use Cases&lt;/h2>
&lt;p>Below are some use cases which could give you an idea on how to use Velero. You can also check &lt;a href="https://velero.io/docs/main/">Velero&amp;rsquo;s documentation&lt;/a> for other introductory examples.&lt;/p>
&lt;h3 id="helm-based-deployments">Helm Based Deployments&lt;/h3>
&lt;p>To be able to use Helm charts in your Kubernetes cluster, you need to install the Helm client &lt;code>helm&lt;/code> and the server component &lt;code>tiller&lt;/code>. Per default the server component is installed in the namespace &lt;code>kube-system&lt;/code>. Even if it is possible to select single deployments via the filter settings of Velero, you should consider to install &lt;code>tiller&lt;/code> in a separate namespace via &lt;code>helm init --tiller-namespace &amp;lt;your namespace&amp;gt;&lt;/code>. This approach applies as well for all Helm charts to be deployed - consider separate namespaces for your deployments as well by using the parameter &lt;code>--namespace&lt;/code>.&lt;/p>
&lt;p>To backup a Helm based deployment, you need to backup both Tiller &lt;em>and&lt;/em> the deployment. Only then the deployments could be managed via Helm. As mentioned above, the selection of resources would be easier in case they are separated in namespaces.&lt;/p>
&lt;h3 id="separate-backup-locations">Separate Backup Locations&lt;/h3>
&lt;p>In case you run all your Kubernetes clusters on a single cloud provider, there is probably no need to store the backups in a bucket of a different cloud provider. However, if you run Kubernetes clusters on different cloud provider, you might consider to use a bucket on just one cloud provider as the target for the backups, e.g., to benefit from a lower price tag for the storage.&lt;/p>
&lt;p>Per default, Velero assumes that both the persistent volumes and the backup location are on the same cloud provider. During the setup of Velero, a secret is created using the credentials for a cloud provider user who has access to both objects (see the policies, e.g., for the &lt;a href="https://github.com/vmware-tanzu/velero-plugin-for-aws#set-permissions-for-velero">AWS configuration&lt;/a>).&lt;/p>
&lt;p>Now, since the backup location is different from the volume location, you need to follow these steps (described here for AWS):&lt;/p>
&lt;ul>
&lt;li>
&lt;p>configure as documented the volume storage location in &lt;code>examples/aws/06-volumesnapshotlocation.yaml&lt;/code> and provide the user credentials. In this case, the S3 related settings like the policies can be omitted&lt;/p>
&lt;/li>
&lt;li>
&lt;p>create the bucket for the backup in the cloud provider in question and a user with the appropriate credentials and store them in a separate file similar to &lt;code>credentials-ark&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>create a secret which contains two credentials, one for the volumes and one for the backup target, e.g., by using the command &lt;code>kubectl create secret generic cloud-credentials --namespace heptio-ark --from-file cloud=credentials-ark --from-file backup-target=backup-ark&lt;/code>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>configure in the deployment manifest &lt;code>examples/aws/10-deployment.yaml&lt;/code> the entries in &lt;code>volumeMounts&lt;/code>, &lt;code>env&lt;/code> and &lt;code>volumes&lt;/code> accordingly, e.g., for a cluster running on AWS and the backup target bucket on GCP a configuration could look similar to:&lt;/p>
&lt;details>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Some links might get broken in the near future since Heptio was taken over by VMWare which might result in different GitHub repositories or other changes. Please don&amp;rsquo;t hesitate to inform us in case you encounter any issues.
&lt;/div>
&lt;summary>Example Velero deployment&lt;/summary>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Copyright 2017 the Heptio Ark contributors.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Licensed under the Apache License, Version 2.0 (the &amp;#34;License&amp;#34;);&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># you may not use this file except in compliance with the License.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># You may obtain a copy of the License at&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># http://www.apache.org/licenses/LICENSE-2.0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Unless required by applicable law or agreed to in writing, software&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># distributed under the License is distributed on an &amp;#34;AS IS&amp;#34; BASIS,&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># See the License for the specific language governing permissions and&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># limitations under the License.&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>---
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1beta1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Deployment
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> namespace: velero
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: velero
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> replicas: 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> labels:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> component: velero
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> annotations:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> prometheus.io/scrape: &lt;span style="color:#a31515">&amp;#34;true&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> prometheus.io/port: &lt;span style="color:#a31515">&amp;#34;8085&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> prometheus.io/path: &lt;span style="color:#a31515">&amp;#34;/metrics&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> restartPolicy: Always
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> serviceAccountName: velero
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> containers:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: velero
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> image: gcr.io/heptio-images/velero:latest
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> command:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - /velero
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> args:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - server
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumeMounts:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: cloud-credentials
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /credentials
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: plugins
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /plugins
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: scratch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> mountPath: /scratch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> env:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: AWS_SHARED_CREDENTIALS_FILE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: /credentials/cloud
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: GOOGLE_APPLICATION_CREDENTIALS
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: /credentials/backup-target
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: VELERO_SCRATCH_DIR
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> value: /scratch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> volumes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: cloud-credentials
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secret:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> secretName: cloud-credentials
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: plugins
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> emptyDir: {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: scratch
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> emptyDir: {}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/details>
&lt;/li>
&lt;li>
&lt;p>finally, configure the backup storage location in &lt;code>examples/aws/05-backupstoragelocation.yaml&lt;/code> to use, in this case, a GCP bucket&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="limitations">Limitations&lt;/h2>
&lt;p>Below is a potentially incomplete list of limitations. You can also consult &lt;a href="https://velero.io/docs/main/">Velero&amp;rsquo;s documentation&lt;/a> to get up to date information.&lt;/p>
&lt;ul>
&lt;li>Only full backups of selected resources are supported. Incremental backups are not (yet) supported. However, by using filters it is possible to restrict the backup to specific resources&lt;/li>
&lt;li>Inconsistencies might occur in case of changes during the creation of the backup&lt;/li>
&lt;li>Application specific actions are not considered by default. However, they might be handled by using Velero&amp;rsquo;s &lt;a href="https://velero.io/docs/main/backup-hooks/#docs">Hooks&lt;/a> or &lt;a href="https://velero.io/docs/main/custom-plugins/#docs">Plugins&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Docs: Fun with kubectl Aliases</title><link>https://gardener.cloud/docs/guides/client-tools/bash-tips/</link><pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/guides/client-tools/bash-tips/</guid><description>
&lt;h2 id="speed-up-your-terminal-workflow">Speed up Your Terminal Workflow&lt;/h2>
&lt;p>Use the Kubernetes command-line tool, &lt;code>kubectl&lt;/code>, to deploy and manage applications on Kubernetes. Using kubectl, you can inspect cluster resources, as well as create, delete, and update components.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/teaser_419ed3.svg" alt="port-forward">&lt;/p>
&lt;p>You will probably run more than a hundred kubectl commands on some days and you should speed up your terminal workflow with with some shortcuts. Of course, there are good shortcuts and bad shortcuts (lazy coding, lack of security review, etc.), but let&amp;rsquo;s stick with the positives and talk about a good shortcut: &lt;strong>bash aliases&lt;/strong> in your &lt;code>.profile&lt;/code>.&lt;/p>
&lt;p>What are those mysterious &lt;code>.profile&lt;/code> and &lt;code>.bash_profile&lt;/code> files you&amp;rsquo;ve heard about?&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
The contents of a .profile file are executed on every log-in of the owner of the file
&lt;/div>
&lt;p>What&amp;rsquo;s the &lt;code>.bash_profile&lt;/code> then? It&amp;rsquo;s exactly the same, but under a different name. The unix shell you are logging into, in this case OS X, looks for &lt;code>etc/profile&lt;/code> and loads it if it exists. Then it looks for &lt;code>~/.bash_profile&lt;/code>, &lt;code>~/.bash_login&lt;/code> and finally &lt;code>~/.profile&lt;/code>, and loads the first one of these it finds.&lt;/p>
&lt;h2 id="populating-the-profile-file">Populating the &lt;code>.profile&lt;/code> File&lt;/h2>
&lt;p>Here is the fantastic time saver that needs to be in your shell profile:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># time save number one. shortcut for kubectl&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>alias k=&lt;span style="color:#a31515">&amp;#34;kubectl&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Start a shell in a pod AND kill them after leaving&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>alias ksh=&lt;span style="color:#a31515">&amp;#34;kubectl run busybox -i --tty --image=busybox --restart=Never --rm -- sh&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># opens a bash&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>alias kbash=&lt;span style="color:#a31515">&amp;#34;kubectl run busybox -i --tty --image=busybox --restart=Never --rm -- ash&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># activate/exports the kuberconfig.yaml in the current working directory&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>alias kexport=&lt;span style="color:#a31515">&amp;#34;export KUBECONFIG=`pwd`/kubeconfig.yaml&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># usage: kurl http://your-svc.namespace.cluster.local&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># we need for this our very own image...never trust an unknown image..&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>alias kurl=&lt;span style="color:#a31515">&amp;#34;docker run --rm byrnedo/alpine-curl&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>All the &lt;code>kubectl&lt;/code> &lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/#enabling-shell-autocompletion">tab completions&lt;/a> still work fine with these aliases, so you’re not losing that speed.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
&lt;p>If the approach above does not work for you add the following lines in your ~/.bashrc instead:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-sh" data-lang="sh">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># time save number one. shortcut for kubectl&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000">#&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>alias k=&lt;span style="color:#a31515">&amp;#34;kubectl&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Enable kubectl completion&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>source &amp;lt;(k completion bash | sed s/kubectl/k/g)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>
&lt;/div></description></item><item><title>Docs: 01 Multi Node Etcd Clusters</title><link>https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/</guid><description>
&lt;h1 id="multi-node-etcd-cluster-instances-via-etcd-druid">Multi-node etcd cluster instances via etcd-druid&lt;/h1>
&lt;p>This document proposes an approach (along with some alternatives) to support provisioning and management of multi-node etcd cluster instances via &lt;a href="https://github.com/gardener/etcd-druid">etcd-druid&lt;/a> and &lt;a href="https://github.com/gardener/etcd-backup-restore">etcd-backup-restore&lt;/a>.&lt;/p>
&lt;h2 id="content">Content&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#multi-node-etcd-cluster-instances-via-etcd-druid">Multi-node etcd cluster instances via etcd-druid&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#content">Content&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#goal">Goal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#background-and-motivation">Background and Motivation&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#single-node-etcd-cluster">Single-node etcd cluster&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#multi-node-etcd-cluster">Multi-node etcd-cluster&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#dynamic-multi-node-etcd-cluster">Dynamic multi-node etcd cluster&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#prior-art">Prior Art&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-operator-from-coreos">ETCD Operator from CoreOS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcdadm-from-kubernetes-sigs">etcdadm from kubernetes-sigs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-cluster-operator-from-improbable-engineering">Etcd Cluster Operator from Improbable-Engineering&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#general-approach-to-etcd-cluster-management">General Approach to ETCD Cluster Management&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping">Bootstrapping&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#assumptions">Assumptions&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">Adding a new member to an etcd cluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note">Note&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative">Alternative&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#managing-failures">Managing Failures&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">Removing an existing member from an etcd cluster&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">Restarting an existing member of an etcd cluster&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">Recovering an etcd cluster from failure of majority of members&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#kubernetes-context">Kubernetes Context&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-1">Alternative&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-configuration">ETCD Configuration&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-2">Alternative&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#data-persistence">Data Persistence&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent">Persistent&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">Ephemeral&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#disk">Disk&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#in-memory">In-memory&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#how-to-detect-if-valid-metadata-exists-in-an-etcd-member">How to detect if valid metadata exists in an etcd member&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommendation">Recommendation&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#how-to-detect-if-valid-data-exists-in-an-etcd-member">How to detect if valid data exists in an etcd member&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommendation-1">Recommendation&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#separating-peer-and-client-traffic">Separating peer and client traffic&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#cutting-off-client-requests">Cutting off client requests&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector">Manipulating Client Service podSelector&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#health-check">Health Check&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure">Backup Failure&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-3">Alternative&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">Status&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#members">Members&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note-1">Note&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-name-as-the-key">Member name as the key&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">Member Leases&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">Conditions&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#clustersize">ClusterSize&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-4">Alternative&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#decision-table-for-etcd-druid-based-on-the-status">Decision table for etcd-druid based on the status&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#1-pink-of-health">1. Pink of health&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#2-member-status-is-out-of-sync-with-their-leases">2. Member status is out of sync with their leases&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-1">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-1">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#3-all-members-are-ready-but-allmembersready-condition-is-stale">3. All members are &lt;code>Ready&lt;/code> but &lt;code>AllMembersReady&lt;/code> condition is stale&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-2">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-2">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#4-not-all-members-are-ready-but-allmembersready-condition-is-stale">4. Not all members are &lt;code>Ready&lt;/code> but &lt;code>AllMembersReady&lt;/code> condition is stale&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-3">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-3">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#5-majority-members-are-ready-but-ready-condition-is-stale">5. Majority members are &lt;code>Ready&lt;/code> but &lt;code>Ready&lt;/code> condition is stale&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-4">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-4">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#6-majority-members-are-notready-but-ready-condition-is-stale">6. Majority members are &lt;code>NotReady&lt;/code> but &lt;code>Ready&lt;/code> condition is stale&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-5">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-5">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#7-some-members-have-been-in-unknown-status-for-a-while">7. Some members have been in &lt;code>Unknown&lt;/code> status for a while&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-6">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-6">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#8-some-member-pods-are-not-ready-but-have-not-had-the-chance-to-update-their-status">8. Some member pods are not &lt;code>Ready&lt;/code> but have not had the chance to update their status&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-7">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-7">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#9-quorate-cluster-with-a-minority-of-members-notready">9. Quorate cluster with a minority of members &lt;code>NotReady&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-8">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-8">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#10-quorum-lost-with-a-majority-of-members-notready">10. Quorum lost with a majority of members &lt;code>NotReady&lt;/code>&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-9">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-9">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#11-scale-up-of-a-healthy-cluster">11. Scale up of a healthy cluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-10">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-10">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#12-scale-down-of-a-healthy-cluster">12. Scale down of a healthy cluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-11">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-11">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status">13. Superfluous member entries in &lt;code>Etcd&lt;/code> status&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-12">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#decision-table-for-etcd-backup-restore-during-initialization">Decision table for etcd-backup-restore during initialization&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#1-first-member-during-bootstrap-of-a-fresh-etcd-cluster">1. First member during bootstrap of a fresh etcd cluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-13">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-13">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#2-addition-of-a-new-following-member-during-bootstrap-of-a-fresh-etcd-cluster">2. Addition of a new following member during bootstrap of a fresh etcd cluster&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-14">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-14">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#3-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-and-data">3. Restart of an existing member of a quorate cluster with valid metadata and data&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-15">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-15">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#4-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-but-without-valid-data">4. Restart of an existing member of a quorate cluster with valid metadata but without valid data&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-16">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-16">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#5-restart-of-an-existing-member-of-a-quorate-cluster-without-valid-metadata">5. Restart of an existing member of a quorate cluster without valid metadata&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-17">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-17">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#6-restart-of-an-existing-member-of-a-non-quorate-cluster-with-valid-metadata-and-data">6. Restart of an existing member of a non-quorate cluster with valid metadata and data&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-18">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-18">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#7-restart-of-the-first-member-of-a-non-quorate-cluster-without-valid-data">7. Restart of the first member of a non-quorate cluster without valid data&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-19">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-19">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#8-restart-of-a-following-member-of-a-non-quorate-cluster-without-valid-data">8. Restart of a following member of a non-quorate cluster without valid data&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#observed-state-20">Observed state&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-20">Recommended Action&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup">Backup&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#leading-etcd-main-containers-sidecar-is-the-backup-leader">Leading ETCD main container’s sidecar is the backup leader&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#independent-leader-election-between-backup-restore-sidecars">Independent leader election between backup-restore sidecars&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#history-compaction">History Compaction&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#defragmentation">Defragmentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-in-etcd-backup-restore">Work-flows in etcd-backup-restore&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-independent-of-leader-election-in-all-members">Work-flows independent of leader election in all members&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member">Work-flows only on the leading member&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#high-availability">High Availability&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#zonal-cluster---single-availability-zone">Zonal Cluster - Single Availability Zone&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-5">Alternative&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#regional-cluster---multiple-availability-zones">Regional Cluster - Multiple Availability Zones&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-6">Alternative&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#poddisruptionbudget">PodDisruptionBudget&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#rolling-updates-to-etcd-members">Rolling updates to etcd members&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#follow-up">Follow Up&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral-volumes">Ephemeral Volumes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#shoot-control-plane-migration">Shoot Control-Plane Migration&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#performance-impact-of-multi-node-etcd-clusters">Performance impact of multi-node etcd clusters&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#metrics-dashboards-and-alerts">Metrics, Dashboards and Alerts&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#costs">Costs&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#future-work">Future Work&lt;/a>
&lt;ul>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#gardener-ring">Gardener Ring&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#autonomous-shoot-clusters">Autonomous Shoot Clusters&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#optimization-of-recovery-from-non-quorate-cluster-with-some-member-containing-valid-data">Optimization of recovery from non-quorate cluster with some member containing valid data&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#optimization-of-rolling-updates-to-unhealthy-etcd-clusters">Optimization of rolling updates to unhealthy etcd clusters&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="goal">Goal&lt;/h2>
&lt;ul>
&lt;li>Enhance etcd-druid and etcd-backup-restore to support provisioning and management of multi-node etcd cluster instances within a single Kubernetes cluster.&lt;/li>
&lt;li>The etcd CRD interface should be simple to use. It should preferably work with just setting the &lt;code>spec.replicas&lt;/code> field to the desired value and should not require any more configuration in the CRD than currently required for the single-node etcd instances. The &lt;code>spec.replicas&lt;/code> field is part of the &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource">&lt;code>scale&lt;/code> sub-resource&lt;/a> &lt;a href="https://github.com/gardener/etcd-druid/blob/eaf04a2d0e6c7a4f2c8c220182b7a141aabfc70b/api/v1alpha1/etcd_types.go#L299">implementation&lt;/a> in &lt;code>Etcd&lt;/code> CRD.&lt;/li>
&lt;li>The single-node and multi-node scenarios must be automatically identified and managed by &lt;code>etcd-druid&lt;/code> and &lt;code>etcd-backup-restore&lt;/code>.&lt;/li>
&lt;li>The etcd clusters (single-node or multi-node) managed by &lt;code>etcd-druid&lt;/code> and &lt;code>etcd-backup-restore&lt;/code> must automatically recover from failures (even quorum loss) and disaster (e.g. etcd member persistence/data loss) as much as possible.&lt;/li>
&lt;li>It must be possible to dynamically scale an etcd cluster horizontally (even between single-node and multi-node scenarios) by simply scaling the &lt;code>Etcd&lt;/code> scale sub-resource.&lt;/li>
&lt;li>It must be possible to (optionally) schedule the individual members of an etcd clusters on different nodes or even infrastructure availability zones (within the hosting Kubernetes cluster).&lt;/li>
&lt;/ul>
&lt;p>Though this proposal tries to cover most aspects related to single-node and multi-node etcd clusters, there are some more points that are not goals for this document but are still in the scope of either etcd-druid/etcd-backup-restore and/or gardener.
In such cases, a high-level description of how they can be &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#future-work">addressed in the future&lt;/a> are mentioned at the end of the document.&lt;/p>
&lt;h2 id="background-and-motivation">Background and Motivation&lt;/h2>
&lt;h3 id="single-node-etcd-cluster">Single-node etcd cluster&lt;/h3>
&lt;p>At present, &lt;code>etcd-druid&lt;/code> supports only single-node etcd cluster instances.
The advantages of this approach are given below.&lt;/p>
&lt;ul>
&lt;li>The problem domain is smaller.
There are no leader election and quorum related issues to be handled.
It is simpler to setup and manage a single-node etcd cluster.&lt;/li>
&lt;li>Single-node etcd clusters instances have &lt;a href="https://etcd.io/docs/v2/admin_guide/#optimal-cluster-size">less request latency&lt;/a> than multi-node etcd clusters because there is no requirement to replicate the changes to the other members before committing the changes.&lt;/li>
&lt;li>&lt;code>etcd-druid&lt;/code> provisions etcd cluster instances as pods (actually as &lt;code>statefulsets&lt;/code>) in a Kubernetes cluster and Kubernetes is quick (&amp;lt;&lt;code>20s&lt;/code>) to restart container/pods if they go down.&lt;/li>
&lt;li>Also, &lt;code>etcd-druid&lt;/code> is currently only used by gardener to provision etcd clusters to act as back-ends for Kubernetes control-planes and Kubernetes control-plane components (&lt;code>kube-apiserver&lt;/code>, &lt;code>kubelet&lt;/code>, &lt;code>kube-controller-manager&lt;/code>, &lt;code>kube-scheduler&lt;/code> etc.) can tolerate etcd going down and recover when it comes back up.&lt;/li>
&lt;li>Single-node etcd clusters incur less cost (CPU, memory and storage)&lt;/li>
&lt;li>It is easy to cut-off client requests if backups fail by using &lt;a href="https://github.com/gardener/etcd-druid/blob/eaf04a2d0e6c7a4f2c8c220182b7a141aabfc70b/charts/etcd/templates/etcd-statefulset.yaml#L54-L62">&lt;code>readinessProbe&lt;/code> on the &lt;code>etcd-backup-restore&lt;/code> healthz endpoint&lt;/a> to minimize the gap between the latest revision and the backup revision.&lt;/li>
&lt;/ul>
&lt;p>The disadvantages of using single-node etcd clusters are given below.&lt;/p>
&lt;ul>
&lt;li>The &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/master/docs/proposals/design.md#workflow">database verification&lt;/a> step by &lt;code>etcd-backup-restore&lt;/code> can introduce additional delays whenever etcd container/pod restarts (in total ~&lt;code>20-25s&lt;/code>).
This can be much longer if a database restoration is required.
Especially, if there are incremental snapshots that need to be replayed (this can be mitigated by &lt;a href="https://github.com/gardener/etcd-druid/issues/88">compacting the incremental snapshots in the background&lt;/a>).&lt;/li>
&lt;li>Kubernetes control-plane components can go into &lt;code>CrashloopBackoff&lt;/code> if etcd is down for some time. This is mitigated by the &lt;a href="https://github.com/gardener/gardener/blob/9e4a809008fb122a6d02045adc08b9c98b5cd564/charts/seed-bootstrap/charts/dependency-watchdog/templates/endpoint-configmap.yaml#L29-L41">dependency-watchdog&lt;/a>.
But Kubernetes control-plane components require a lot of resources and create a lot of load on the etcd cluster and the apiserver when they come out of &lt;code>CrashloopBackoff&lt;/code>.
Especially, in medium or large sized clusters (&amp;gt; &lt;code>20&lt;/code> nodes).&lt;/li>
&lt;li>Maintenance operations such as updates to etcd (and updates to &lt;code>etcd-druid&lt;/code> of &lt;code>etcd-backup-restore&lt;/code>), rolling updates to the nodes of the underlying Kubernetes cluster and vertical scaling of etcd pods are disruptive because they cause etcd pods to be restarted.
The vertical scaling of etcd pods is somewhat mitigated during scale down by doing it only during the target clusters&amp;rsquo; &lt;a href="https://github.com/gardener/gardener/blob/86aa30dfd095f7960ae50a81d2cee27c0d18408b/charts/seed-controlplane/charts/etcd/templates/etcd-hvpa.yaml#L53">maintenance window&lt;/a>.
But scale up is still disruptive.&lt;/li>
&lt;li>We currently use some form of elastic storage (via &lt;code>persistentvolumeclaims&lt;/code>) for storing which have some upper-bounds on the I/O latency and throughput. This can be potentially be a problem for large clusters (&amp;gt; &lt;code>220&lt;/code> nodes).
Also, some cloud providers (e.g. Azure) take a long time to attach/detach volumes to and from machines which increases the down time to the Kubernetes components that depend on etcd.
It is difficult to use ephemeral/local storage (to achieve better latency/throughput as well as to circumvent volume attachment/detachment) for single-node etcd cluster instances.&lt;/li>
&lt;/ul>
&lt;h3 id="multi-node-etcd-cluster">Multi-node etcd-cluster&lt;/h3>
&lt;p>The advantages of introducing support for multi-node etcd clusters via &lt;code>etcd-druid&lt;/code> are below.&lt;/p>
&lt;ul>
&lt;li>Multi-node etcd cluster is highly-available. It can tolerate disruption to individual etcd pods as long as the quorum is not lost (i.e. more than half the etcd member pods are healthy and ready).&lt;/li>
&lt;li>Maintenance operations such as updates to etcd (and updates to &lt;code>etcd-druid&lt;/code> of &lt;code>etcd-backup-restore&lt;/code>), rolling updates to the nodes of the underlying Kubernetes cluster and vertical scaling of etcd pods can be done non-disruptively by &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/">respecting &lt;code>poddisruptionbudgets&lt;/code>&lt;/a> for the various multi-node etcd cluster instances hosted on that cluster.&lt;/li>
&lt;li>Kubernetes control-plane components do not see any etcd cluster downtime unless quorum is lost (which is expected to be lot less frequent than current frequency of etcd container/pod restarts).&lt;/li>
&lt;li>We can consider using ephemeral/local storage for multi-node etcd cluster instances because individual member restarts can afford to take time to restore from backup before (re)joining the etcd cluster because the remaining members serve the requests in the meantime.&lt;/li>
&lt;li>High-availability across availability zones is also possible by specifying (anti)affinity for the etcd pods (possibly via &lt;a href="https://github.com/gardener/kupid">&lt;code>kupid&lt;/code>&lt;/a>).&lt;/li>
&lt;/ul>
&lt;p>Some disadvantages of using multi-node etcd clusters due to which it might still be desirable, in some cases, to continue to use single-node etcd cluster instances in the gardener context are given below.&lt;/p>
&lt;ul>
&lt;li>Multi-node etcd cluster instances are more complex to manage.
The problem domain is larger including the following.
&lt;ul>
&lt;li>Leader election&lt;/li>
&lt;li>Quorum loss&lt;/li>
&lt;li>Managing rolling changes&lt;/li>
&lt;li>Backups to be taken from only the leading member.&lt;/li>
&lt;li>More complex to cut-off client requests if backups fail to minimize the gap between the latest revision and the backup revision is under control.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Multi-node etcd cluster instances incur more cost (CPU, memory and storage).&lt;/li>
&lt;/ul>
&lt;h3 id="dynamic-multi-node-etcd-cluster">Dynamic multi-node etcd cluster&lt;/h3>
&lt;p>Though it is &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#non-goal">not part of this proposal&lt;/a>, it is conceivable to convert a single-node etcd cluster into a multi-node etcd cluster temporarily to perform some disruptive operation (etcd, &lt;code>etcd-backup-restore&lt;/code> or &lt;code>etcd-druid&lt;/code> updates, etcd cluster vertical scaling and perhaps even node rollout) and convert it back to a single-node etcd cluster once the disruptive operation has been completed. This will necessarily still involve a down-time because scaling from a single-node etcd cluster to a three-node etcd cluster will involve etcd pod restarts, it is still probable that it can be managed with a shorter down time than we see at present for single-node etcd clusters (on the other hand, converting a three-node etcd cluster to five node etcd cluster can be non-disruptive).&lt;/p>
&lt;p>This is &lt;em>definitely not&lt;/em> to argue in favour of such a dynamic approach in all cases (eventually, if/when dynamic multi-node etcd clusters are supported). On the contrary, it makes sense to make use of &lt;em>static&lt;/em> (fixed in size) multi-node etcd clusters for production scenarios because of the high-availability.&lt;/p>
&lt;h2 id="prior-art">Prior Art&lt;/h2>
&lt;h3 id="etcd-operator-from-coreos">ETCD Operator from CoreOS&lt;/h3>
&lt;blockquote>
&lt;p>&lt;a href="https://github.com/coreos/etcd-operator#etcd-operator">etcd operator&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://github.com/coreos/etcd-operator#project-status-archived">Project status: archived&lt;/a>&lt;/p>
&lt;p>This project is no longer actively developed or maintained. The project exists here for historical reference. If you are interested in the future of the project and taking over stewardship, please contact &lt;a href="mailto:etcd-dev@googlegroups.com">etcd-dev@googlegroups.com&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;h3 id="etcdadm-from-kubernetes-sigs">etcdadm from kubernetes-sigs&lt;/h3>
&lt;blockquote>
&lt;p>&lt;a href="https://github.com/kubernetes-sigs/etcdadm#etcdadm">etcdadm&lt;/a> is a command-line tool for operating an etcd cluster. It makes it easy to create a new cluster, add a member to, or remove a member from an existing cluster. Its user experience is inspired by kubeadm.&lt;/p>
&lt;/blockquote>
&lt;p>It is a tool more tailored for manual command-line based management of etcd clusters with no API&amp;rsquo;s.
It also makes no assumptions about the underlying platform on which the etcd clusters are provisioned and hence, doesn&amp;rsquo;t leverage any capabilities of Kubernetes.&lt;/p>
&lt;h3 id="etcd-cluster-operator-from-improbable-engineering">Etcd Cluster Operator from Improbable-Engineering&lt;/h3>
&lt;blockquote>
&lt;p>&lt;a href="https://github.com/improbable-eng/etcd-cluster-operator">Etcd Cluster Operator&lt;/a>&lt;/p>
&lt;p>Etcd Cluster Operator is an Operator for automating the creation and management of etcd inside of Kubernetes. It provides a custom resource definition (CRD) based API to define etcd clusters with Kubernetes resources, and enable management with native Kubernetes tooling._&lt;/p>
&lt;/blockquote>
&lt;p>Out of all the alternatives listed here, this one seems to be the only possible viable alternative.
Parts of its design/implementations are similar to some of the approaches mentioned in this proposal. However, we still don&amp;rsquo;t propose to use it as -&lt;/p>
&lt;ol>
&lt;li>The project is still in early phase and is not mature enough to be consumed as is in productive scenarios of ours.&lt;/li>
&lt;li>The resotration part is completely different which makes it difficult to adopt as-is and requries lot of re-work with the current restoration semantics with etcd-backup-restore making the usage counter-productive.&lt;/li>
&lt;/ol>
&lt;h2 id="general-approach-to-etcd-cluster-management">General Approach to ETCD Cluster Management&lt;/h2>
&lt;h3 id="bootstrapping">Bootstrapping&lt;/h3>
&lt;p>There are three ways to bootstrap an etcd cluster which are &lt;a href="https://etcd.io/docs/v3.4.0/op-guide/clustering/#static">static&lt;/a>, &lt;a href="https://etcd.io/docs/v3.4.0/op-guide/clustering/#etcd-discovery">etcd discovery&lt;/a> and &lt;a href="https://etcd.io/docs/v3.4.0/op-guide/clustering/#dns-discovery">DNS discovery&lt;/a>.
Out of these, the static way is the simplest (and probably faster to bootstrap the cluster) and has the least external dependencies.
Hence, it is preferred in this proposal.
But it requires that the initial (during bootstrapping) etcd cluster size (number of members) is already known before bootstrapping and that all of the members are already addressable (DNS,IP,TLS etc.).
Such information needs to be passed to the individual members during startup using the following static configuration.&lt;/p>
&lt;ul>
&lt;li>ETCD_INITIAL_CLUSTER
&lt;ul>
&lt;li>The list of peer URLs including all the members. This must be the same as the advertised peer URLs configuration. This can also be passed as &lt;code>initial-cluster&lt;/code> flag to etcd.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ETCD_INITIAL_CLUSTER_STATE
&lt;ul>
&lt;li>This should be set to &lt;code>new&lt;/code> while bootstrapping an etcd cluster.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>ETCD_INITIAL_CLUSTER_TOKEN
&lt;ul>
&lt;li>This is a token to distinguish the etcd cluster from any other etcd cluster in the same network.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="assumptions">Assumptions&lt;/h4>
&lt;ul>
&lt;li>ETCD_INITIAL_CLUSTER can use DNS instead of IP addresses. We need to verify this by deleting a pod (as against scaling down the statefulset) to ensure that the pod IP changes and see if the recreated pod (by the statefulset controller) re-joins the cluster automatically.&lt;/li>
&lt;li>DNS for the individual members is known or computable. This is true in the case of etcd-druid setting up an etcd cluster using a single statefulset. But it may not necessarily be true in other cases (multiple statefulset per etcd cluster or deployments instead of statefulsets or in the case of etcd cluster with members distributed across more than one Kubernetes cluster.&lt;/li>
&lt;/ul>
&lt;h3 id="adding-a-new-member-to-an-etcd-cluster">Adding a new member to an etcd cluster&lt;/h3>
&lt;p>A &lt;a href="https://etcd.io/docs/v3.4.0/op-guide/runtime-configuration/#add-a-new-member">new member can be added&lt;/a> to an existing etcd cluster instance using the following steps.&lt;/p>
&lt;ol>
&lt;li>If the latest backup snapshot exists, restore the member&amp;rsquo;s etcd data to the latest backup snapshot. This can reduce the load on the leader to bring the new member up to date when it joins the cluster.
&lt;ol>
&lt;li>If the latest backup snapshot doesn&amp;rsquo;t exist or if the latest backup snapshot is not accessible (please see &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure">backup failure&lt;/a>) and if the cluster itself is quorate, then the new member can be started with an empty data. But this will will be suboptimal because the new member will fetch all the data from the leading member to get up-to-date.&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>The cluster is informed that a new member is being added using the &lt;a href="https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L40">&lt;code>MemberAdd&lt;/code> API&lt;/a> including information like the member name and its advertised peer URLs.&lt;/li>
&lt;li>The new etcd member is then started with &lt;code>ETCD_INITIAL_CLUSTER_STATE=existing&lt;/code> apart from other required configuration.&lt;/li>
&lt;/ol>
&lt;p>This proposal recommends this approach.&lt;/p>
&lt;h4 id="note">Note&lt;/h4>
&lt;ul>
&lt;li>If there are incremental snapshots (taken by &lt;code>etcd-backup-restore&lt;/code>), they cannot be applied because that requires the member to be started in isolation without joining the cluster which is not possible.
This is acceptable if the amount of incremental snapshots are managed to be relatively small.
This adds one more reason to increase the priority of the issue of &lt;a href="https://github.com/gardener/etcd-druid/issues/88">incremental snapshot compaction&lt;/a>.&lt;/li>
&lt;li>There is a time window, between the &lt;code>MemberAdd&lt;/code> call and the new member joining the cluster and getting up to date, where the cluster is &lt;a href="https://etcd.io/docs/v3.3.12/learning/learner/#background">vulnerable to leader elections which could be disruptive&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h4 id="alternative">Alternative&lt;/h4>
&lt;p>With &lt;code>v3.4&lt;/code>, the new &lt;a href="https://etcd.io/docs/v3.3.12/learning/learner/#raft-learner">raft learner approach&lt;/a> can be used to mitigate some of the possible disruptions mentioned &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note">above&lt;/a>.
Then the steps will be as follows.&lt;/p>
&lt;ol>
&lt;li>If the latest backup snapshot exists, restore the member&amp;rsquo;s etcd data to the latest backup snapshot. This can reduce the load on the leader to bring the new member up to date when it joins the cluster.&lt;/li>
&lt;li>The cluster is informed that a new member is being added using the &lt;a href="https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L43">&lt;code>MemberAddAsLearner&lt;/code> API&lt;/a> including information like the member name and its advertised peer URLs.&lt;/li>
&lt;li>The new etcd member is then started with &lt;code>ETCD_INITIAL_CLUSTER_STATE=existing&lt;/code> apart from other required configuration.&lt;/li>
&lt;li>Once the new member (learner) is up to date, it can be promoted to a full voting member by using the &lt;a href="https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L52">&lt;code>MemberPromote&lt;/code> API&lt;/a>&lt;/li>
&lt;/ol>
&lt;p>This approach is new and involves more steps and is not recommended in this proposal.
It can be considered in future enhancements.&lt;/p>
&lt;h3 id="managing-failures">Managing Failures&lt;/h3>
&lt;p>A multi-node etcd cluster may face failures of &lt;a href="https://etcd.io/docs/v3.1.12/op-guide/failures/">diffent kinds&lt;/a> during its life-cycle.
The actions that need to be taken to manage these failures depend on the failure mode.&lt;/p>
&lt;h4 id="removing-an-existing-member-from-an-etcd-cluster">Removing an existing member from an etcd cluster&lt;/h4>
&lt;p>If a member of an etcd cluster becomes unhealthy, it must be explicitly removed from the etcd cluster, as soon as possible.
This can be done by using the &lt;a href="https://github.com/etcd-io/etcd/blob/6e800b9b0161ef874784fc6c679325acd67e2452/client/v3/cluster.go#L46">&lt;code>MemberRemove&lt;/code> API&lt;/a>.
This ensures that only healthy members participate as voting members.&lt;/p>
&lt;p>A member of an etcd cluster may be removed not just for managing failures but also for other reasons such as -&lt;/p>
&lt;ul>
&lt;li>The etcd cluster is being scaled down. I.e. the cluster size is being reduced&lt;/li>
&lt;li>An existing member is being replaced by a new one for some reason (e.g. upgrades)&lt;/li>
&lt;/ul>
&lt;p>If the majority of the members of the etcd cluster are healthy and the member that is unhealthy/being removed happens to be the &lt;a href="https://etcd.io/docs/v3.1.12/op-guide/failures/#leader-failure">leader&lt;/a> at that moment then the etcd cluster will automatically elect a new leader.
But if only a minority of etcd clusters are healthy after removing the member then the the cluster will no longer be &lt;a href="https://etcd.io/docs/v3.1.12/op-guide/failures/#majority-failure">quorate&lt;/a> and will stop accepting write requests.
Such an etcd cluster needs to be recovered via some kind of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">disaster-recovery&lt;/a>.&lt;/p>
&lt;h4 id="restarting-an-existing-member-of-an-etcd-cluster">Restarting an existing member of an etcd cluster&lt;/h4>
&lt;p>If the existing member of an etcd cluster restarts and retains an uncorrupted data directory after the restart, then it can simply re-join the cluster as an existing member without any API calls or configuration changes.
This is because the relevant metadata (including member ID and cluster ID) are &lt;a href="https://etcd.io/docs/v2/admin_guide/#lifecycle">maintained in the write ahead logs&lt;/a>.
However, if it doesn&amp;rsquo;t retain an uncorrupted data directory after the restart, then it must first be &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">removed&lt;/a> and &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">added&lt;/a> as a new member.&lt;/p>
&lt;h4 id="recovering-an-etcd-cluster-from-failure-of-majority-of-members">Recovering an etcd cluster from failure of majority of members&lt;/h4>
&lt;p>If a majority of members of an etcd cluster fail but if they retain their uncorrupted data directory then they can be simply restarted and they will re-form the existing etcd cluster when they come up.
However, if they do not retain their uncorrupted data directory, then the etcd cluster must be &lt;a href="https://etcd.io/docs/v3.4.0/op-guide/recovery/#restoring-a-cluster">recovered from latest snapshot in the backup&lt;/a>.
This is very similar to &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping">bootstrapping&lt;/a> with the additional initial step of restoring the latest snapshot in each of the members.
However, the same &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#note">limitation&lt;/a> about incremental snapshots, as in the case of adding a new member, applies here.
But unlike in the case of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">adding a new member&lt;/a>, not applying incremental snapshots is not acceptable in the case of etcd cluster recovery.
Hence, if incremental snapshots are required to be applied, the etcd cluster must be &lt;a href="https://etcd.io/docs/v3.4.0/op-guide/runtime-configuration/#restart-cluster-from-majority-failure">recovered&lt;/a> in the following steps.&lt;/p>
&lt;ol>
&lt;li>Restore a new single-member cluster using the latest snapshot.&lt;/li>
&lt;li>Apply incremental snapshots on the single-member cluster.&lt;/li>
&lt;li>Take a full snapshot which can now be used while adding the remaining members.&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">Add&lt;/a> new members using the latest snapshot created in the step above.&lt;/li>
&lt;/ol>
&lt;h2 id="kubernetes-context">Kubernetes Context&lt;/h2>
&lt;ul>
&lt;li>Users will provision an etcd cluster in a Kubernetes cluster by creating an etcd CRD resource instance.&lt;/li>
&lt;li>A multi-node etcd cluster is indicated if the &lt;code>spec.replicas&lt;/code> field is set to any value greater than 1. The etcd-druid will add validation to ensure that the &lt;code>spec.replicas&lt;/code> value is an odd number according to the requirements of etcd.&lt;/li>
&lt;li>The etcd-druid controller will provision a statefulset with the etcd main container and the etcd-backup-restore sidecar container. It will pass on the &lt;code>spec.replicas&lt;/code> field from the etcd resource to the statefulset. It will also supply the right pre-computed configuration to both the containers.&lt;/li>
&lt;li>The statefulset controller will create the pods based on the pod template in the statefulset spec and these individual pods will be the members that form the etcd cluster.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://gardener.cloud/__resources/01-multi-node-etcd_1afcbd.png" alt="Component diagram">&lt;/p>
&lt;p>This approach makes it possible to satisfy the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#assumption">assumption&lt;/a> that the DNS for the individual members of the etcd cluster must be known/computable.
This can be achieved by using a &lt;code>headless&lt;/code> service (along with the statefulset) for each etcd cluster instance.
Then we can address individual pods/etcd members via the predictable DNS name of &lt;code>&amp;lt;statefulset_name&amp;gt;-{0|1|2|3|…|n}.&amp;lt;headless_service_name&amp;gt;&lt;/code> from within the Kubernetes namespace (or from outside the Kubernetes namespace by appending &lt;code>.&amp;lt;namespace&amp;gt;.svc.&amp;lt;cluster_domain&amp;gt; suffix)&lt;/code>.
The etcd-druid controller can compute the above configurations automatically based on the &lt;code>spec.replicas&lt;/code> in the etcd resource.&lt;/p>
&lt;p>This proposal recommends this approach.&lt;/p>
&lt;h4 id="alternative-1">Alternative&lt;/h4>
&lt;p>One statefulset is used for each member (instead of one statefulset for all members).
While this approach gives a flexibility to have different pod specifications for the individual members, it makes managing the individual members (e.g. rolling updates) more complicated.
Hence, this approach is not recommended.&lt;/p>
&lt;h2 id="etcd-configuration">ETCD Configuration&lt;/h2>
&lt;p>As mentioned in the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#general-approach-to-etcd-cluster-management">general approach section&lt;/a>, there are differences in the configuration that needs to be passed to individual members of an etcd cluster in different scenarios such as &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping">bootstrapping&lt;/a>, &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">adding&lt;/a> a new member, &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">removing&lt;/a> a member, &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">restarting&lt;/a> an existing member etc.
Managing such differences in configuration for individual pods of a statefulset is tricky in the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#kubernetes-context">recommended approach&lt;/a> of using a single statefulset to manage all the member pods of an etcd cluster.
This is because statefulset uses the same pod template for all its pods.&lt;/p>
&lt;p>The recommendation is for &lt;code>etcd-druid&lt;/code> to provision the base configuration template in a &lt;code>ConfigMap&lt;/code> which is passed to all the pods via the pod template in the &lt;code>StatefulSet&lt;/code>.
The &lt;code>initialization&lt;/code> flow of &lt;code>etcd-backup-restore&lt;/code> (which is invoked every time the etcd container is (re)started) is then enhanced to generate the customized etcd configuration for the corresponding member pod (in a shared &lt;em>volume&lt;/em> between etcd and the backup-restore containers) based on the supplied template configuration.
This will require that &lt;code>etcd-backup-restore&lt;/code> will have to have a mechanism to detect which scenario listed &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-configuration">above&lt;/a> applies during any given member container/pod restart.&lt;/p>
&lt;h3 id="alternative-2">Alternative&lt;/h3>
&lt;p>As mentioned &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#alternative-1">above&lt;/a>, one statefulset is used for each member of the etcd cluster.
Then different configuration (generated directly by &lt;code>etcd-druid&lt;/code>) can be passed in the pod templates of the different statefulsets.
Though this approach is advantageous in the context of managing the different configuration, it is not recommended in this proposal because it makes the rest of the management (e.g. rolling updates) more complicated.&lt;/p>
&lt;h2 id="data-persistence">Data Persistence&lt;/h2>
&lt;p>The type of persistence used to store etcd data (including the member ID and cluster ID) has an impact on the steps that are needed to be taken when the member pods or containers (&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">minority&lt;/a> of them or &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">majority&lt;/a>) need to be recovered.&lt;/p>
&lt;h3 id="persistent">Persistent&lt;/h3>
&lt;p>Like the single-node case, &lt;code>persistentvolumes&lt;/code> can be used to persist ETCD data for all the member pods. The individual member pods then get their own &lt;code>persistentvolumes&lt;/code>.
The advantage is that individual members retain their member ID across pod restarts and even pod deletion/recreation across Kubernetes nodes.
This means that member pods that crash (or are unhealthy) can be &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">restarted&lt;/a> automatically (by configuring &lt;code>livenessProbe&lt;/code>) and they will re-join the etcd cluster using their existing member ID without any need for explicit etcd cluster management).&lt;/p>
&lt;p>The disadvantages of this approach are as follows.&lt;/p>
&lt;ul>
&lt;li>The number of persistentvolumes increases linearly with the cluster size which is a cost-related concern.&lt;/li>
&lt;li>Network-mounted persistentvolumes might eventually become a performance bottleneck under heavy load for a latency-sensitive component like ETCD.&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#single-node-etcd-cluster">Volume attach/detach issues&lt;/a> when associated with etcd cluster instances cause downtimes to the target shoot clusters that are backed by those etcd cluster instances.&lt;/li>
&lt;/ul>
&lt;h3 id="ephemeral">Ephemeral&lt;/h3>
&lt;p>The ephemeral volumes use-case is considered as an optimization and may be planned as a follow-up action.&lt;/p>
&lt;h4 id="disk">Disk&lt;/h4>
&lt;p>Ephemeral persistence can be achieved in Kubernetes by using either &lt;a href="https://kubernetes.io/docs/concepts/storage/volumes/#emptydir">&lt;code>emptyDir&lt;/code>&lt;/a> volumes or &lt;a href="https://kubernetes.io/docs/concepts/storage/volumes/#local">&lt;code>local&lt;/code> persistentvolumes&lt;/a> to persist ETCD data.
The advantages of this approach are as follows.&lt;/p>
&lt;ul>
&lt;li>Potentially faster disk I/O.&lt;/li>
&lt;li>The number of persistent volumes does not increase linearly with the cluster size (at least not technically).&lt;/li>
&lt;li>Issues related volume attachment/detachment can be avoided.&lt;/li>
&lt;/ul>
&lt;p>The main disadvantage of using ephemeral persistence is that the individual members may retain their identity and data across container restarts but not across pod deletion/recreation across Kubernetes nodes. If the data is lost then on restart of the member pod, the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">older member (represented by the container) has to be removed and a new member has to be added&lt;/a>.&lt;/p>
&lt;p>Using &lt;code>emptyDir&lt;/code> ephemeral persistence has the disadvantage that the volume doesn&amp;rsquo;t have its own identity.
So, if the member pod is recreated but scheduled on the same node as before then it will not retain the identity as the persistence is lost.
But it has the advantage that scheduling of pods is unencumbered especially during pod recreation as they are free to be scheduled anywhere.&lt;/p>
&lt;p>Using &lt;code>local&lt;/code> persistentvolumes has the advantage that the volume has its own indentity and hence, a recreated member pod will retain its identity if scheduled on the same node.
But it has the disadvantage of tying down the member pod to a node which is a problem if the node becomes unhealthy requiring etcd druid to take additional actions (such as deleting the local persistent volume).&lt;/p>
&lt;p>Based on these constraints, if ephemeral persistence is opted for, it is recommended to use &lt;code>emptyDir&lt;/code> ephemeral persistence.&lt;/p>
&lt;h4 id="in-memory">In-memory&lt;/h4>
&lt;p>In-memory ephemeral persistence can be achieved in Kubernetes by using &lt;code>emptyDir&lt;/code> with &lt;a href="https://kubernetes.io/docs/concepts/storage/volumes/#emptydir">&lt;code>medium: Memory&lt;/code>&lt;/a>.
In this case, a &lt;code>tmpfs&lt;/code> (RAM-backed file-system) volume will be used.
In addition to the advantages of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral persistence&lt;/a>, this approach can achieve the fastest possible &lt;em>disk I/O&lt;/em>.
Similarly, in addition to the disadvantages of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral persistence&lt;/a>, in-memory persistence has the following additional disadvantages.&lt;/p>
&lt;ul>
&lt;li>More memory required for the individual member pods.&lt;/li>
&lt;li>Individual members may not at all retain their data and identity across container restarts let alone across pod restarts/deletion/recreation across Kubernetes nodes.
I.e. every time an etcd container restarts, &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">the old member (represented by the container) will have to be removed and a new member has to be added&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="how-to-detect-if-valid-metadata-exists-in-an-etcd-member">How to detect if valid metadata exists in an etcd member&lt;/h3>
&lt;p>Since the likelyhood of a member not having valid metadata in the WAL files is much more likely in the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral&lt;/a> persistence scenario, one option is to pass the information that ephemeral persistence is being used to the &lt;code>etcd-backup-restore&lt;/code> sidecar (say, via command-line flags or environment variables).&lt;/p>
&lt;p>But in principle, it might be better to determine this from the WAL files directly so that the possibility of corrupted WAL files also gets handled correctly.
To do this, the &lt;a href="https://github.com/etcd-io/etcd/tree/main/server/storage/wal">wal&lt;/a> package has &lt;a href="https://github.com/etcd-io/etcd/blob/57a092b45d0eae6c9e600e62513ffcd2f1f25a92/server/wal/wal.go#L324-L326">some&lt;/a> &lt;a href="https://github.com/etcd-io/etcd/blob/57a092b45d0eae6c9e600e62513ffcd2f1f25a92/server/wal/wal.go#L429-L548">functions&lt;/a> that might be useful.&lt;/p>
&lt;h4 id="recommendation">Recommendation&lt;/h4>
&lt;p>It might be possible that using the &lt;a href="https://github.com/etcd-io/etcd/tree/main/server/storage/wal">wal&lt;/a> package for verifying if valid metadata exists might be performance intensive.
So, the performance impact needs to be measured.
If the performance impact is acceptable (both in terms of resource usage and time), it is recommended to use this way to verify if the member contains valid metadata.
Otherwise, alternatives such as a simple check that WAL folder exists coupled with the static information about use of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent">persistent&lt;/a> or &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral&lt;/a> storage might be considered.&lt;/p>
&lt;h3 id="how-to-detect-if-valid-data-exists-in-an-etcd-member">How to detect if valid data exists in an etcd member&lt;/h3>
&lt;p>The &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#decision-table-for-etcd-backup-restore-during-initialization">initialization sequence&lt;/a> in &lt;code>etcd-backup-restore&lt;/code> already includes &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/c98f76c7c55f7d1039687cc293536d7caf893ba5/pkg/initializer/validator/datavalidator.go#L78-L94">database verification&lt;/a>.
This would suffice to determine if the member has valid data.&lt;/p>
&lt;h3 id="recommendation-1">Recommendation&lt;/h3>
&lt;p>Though &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral&lt;/a> persistence has performance and logistics advantages,
it is recommended to start with &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent">persistent&lt;/a> data for the member pods.
In addition to the reasons and concerns listed above, there is also the additional concern that in case of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure">backup failure&lt;/a>, the risk of additional data loss is a bit higher if ephemeral persistence is used (simultaneous quoram loss is sufficient) when compared to persistent storage (simultaenous quorum loss with majority persistence loss is needed).
The risk might still be acceptable but the idea is to gain experience about how frequently member containers/pods get restarted/recreated, how frequently leader election happens among members of an etcd cluster and how frequently etcd clusters lose quorum.
Based on this experience, we can move towards using &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral&lt;/a> (perhaps even &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#in-memory">in-memory&lt;/a>) persistence for the member pods.&lt;/p>
&lt;h2 id="separating-peer-and-client-traffic">Separating peer and client traffic&lt;/h2>
&lt;p>The current single-node ETCD cluster implementation in &lt;code>etcd-druid&lt;/code> and &lt;code>etcd-backup-restore&lt;/code> uses a single &lt;code>service&lt;/code> object to act as the entry point for the client traffic.
There is no separation or distinction between the client and peer traffic because there is not much benefit to be had by making that distinction.&lt;/p>
&lt;p>In the multi-node ETCD cluster scenario, it makes sense to distinguish between and separate the peer and client traffic.
This can be done by using two &lt;code>services&lt;/code>.&lt;/p>
&lt;ul>
&lt;li>peer
&lt;ul>
&lt;li>To be used for peer communication. This could be a &lt;code>headless&lt;/code> service.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>client
&lt;ul>
&lt;li>To be used for client communication. This could be a normal &lt;code>ClusterIP&lt;/code> service like it is in the single-node case.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>The main advantage of this approach is that it makes it possible (if needed) to allow only peer to peer communication while blocking client communication. Such a thing might be required during some phases of some maintenance tasks (manual or automated).&lt;/p>
&lt;h3 id="cutting-off-client-requests">Cutting off client requests&lt;/h3>
&lt;p>At present, in the single-node ETCD instances, etcd-druid configures the readinessProbe of the etcd main container to probe the healthz endpoint of the etcd-backup-restore sidecar which considers the status of the latest backup upload in addition to the regular checks about etcd and the side car being up and healthy. This has the effect of setting the etcd main container (and hence the etcd pod) as not ready if the latest backup upload failed. This results in the endpoints controller removing the pod IP address from the endpoints list for the service which eventually cuts off ingress traffic coming into the etcd pod via the etcd client service. The rationale for this is to fail early when the backup upload fails rather than continuing to serve requests while the gap between the last backup and the current data increases which might lead to unacceptably large amount of data loss if disaster strikes.&lt;/p>
&lt;p>This approach will not work in the multi-node scenario because we need the individual member pods to be able to talk to each other to maintain the cluster quorum when backup upload fails but need to cut off only client ingress traffic.&lt;/p>
&lt;p>It is recommended to separate the backup health condition tracking taking appropriate remedial actions.
With that, the backup health condition tracking is now separated to the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">&lt;code>BackupReady&lt;/code> condition&lt;/a> in the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">&lt;code>Etcd&lt;/code> resource &lt;code>status&lt;/code>&lt;/a> and the cutting off of client traffic (which could now be done for more reasons than failed backups) can be achieved in a different way described &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector">below&lt;/a>.&lt;/p>
&lt;h4 id="manipulating-client-service-podselector">Manipulating Client Service podSelector&lt;/h4>
&lt;p>The client traffic can be cut off by updating (manually or automatically by some component) the &lt;code>podSelector&lt;/code> of the client service to add an additional label (say, unhealthy or disabled) such that the &lt;code>podSelector&lt;/code> no longer matches the member pods created by the statefulset.
This will result in the client ingress traffic being cut off.
The peer service is left unmodified so that peer communication is always possible.&lt;/p>
&lt;h2 id="health-check">Health Check&lt;/h2>
&lt;p>The etcd main container and the etcd-backup-restore sidecar containers will be configured with livenessProbe and readinessProbe which will indicate the health of the containers and effectively the corresponding ETCD cluster member pod.&lt;/p>
&lt;h3 id="backup-failure">Backup Failure&lt;/h3>
&lt;p>As described &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#cutting-off-client-requests">above&lt;/a> using &lt;code>readinessProbe&lt;/code> failures based on latest backup failure is not viable in the multi-node ETCD scenario.&lt;/p>
&lt;p>Though cutting off traffic by &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector">manipulating client &lt;code>service&lt;/code> &lt;code>podSelector&lt;/code>&lt;/a> is workable, it may not be desirable.&lt;/p>
&lt;p>It is recommended that on backup failure, the leading &lt;code>etcd-backup-restore&lt;/code> sidecar (the one that is responsible for taking backups at that point in time, as explained in the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup">backup section below&lt;/a>, updates the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">&lt;code>BackupReady&lt;/code> condition&lt;/a> in the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">&lt;code>Etcd&lt;/code> status&lt;/a> and raises a high priority alert to the landscape operators but &lt;em>&lt;em>does not&lt;/em>&lt;/em> cut off the client traffic.&lt;/p>
&lt;p>The reasoning behind this decision to not cut off the client traffic on backup failure is to allow the Kubernetes cluster&amp;rsquo;s control plane (which relies on the ETCD cluster) to keep functioning as long as possible and to avoid bringing down the control-plane due to a missed backup.&lt;/p>
&lt;p>The risk of this approach is that with a cascaded sequence of failures (on top of the backup failure), there is a chance of more data loss than the frequency of backup would otherwise indicate.&lt;/p>
&lt;p>To be precise, the risk of such an additional data loss manifests only when backup failure as well as a special case of quorum loss (majority of the members are not ready) happen in such a way that the ETCD cluster needs to be re-bootstrapped from the backup.
As described &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here&lt;/a>, re-bootstrapping the ETCD cluster requires restoration from the latest backup only when a majority of members no longer have uncorrupted data persistence.&lt;/p>
&lt;p>If &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#persistent">persistent storage&lt;/a> is used, this will happen only when backup failure as well as a majority of the disks/volumes backing the ETCD cluster members fail simultaneously.
This would indeed be rare and might be an acceptable risk.&lt;/p>
&lt;p>If &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral storage&lt;/a> is used (especially, &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#in-memory">in-memory&lt;/a>), the data loss will happen if a majority of the ETCD cluster members become &lt;code>NotReady&lt;/code> (requiring a pod restart) at the same time as the backup failure.
This may not be as rare as majority members&amp;rsquo; disk/volume failure.
The risk can be somewhat mitigated at least for planned maintenance operations by postponing potentially disruptive maintenance operations when &lt;code>BackupReady&lt;/code> condition is &lt;code>false&lt;/code> (vertical scaling, rolling updates, evictions due to node roll-outs).&lt;/p>
&lt;p>But in practice (when &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#ephemeral">ephemeral storage&lt;/a> is used), the current proposal suggests restoring from the latest full backup even when a minority of ETCD members (even a single pod) &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">restart&lt;/a> both to speed up the process of the new member catching up to the latest revision but also to avoid load on the leading member which needs to supply the data to bring the new member up-to-date.
But as described &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">here&lt;/a>, in case of a minority member failure while using ephemeral storage, it is possible to restart the new member with empty data and let it fetch all the data from the leading member (only if backup is not accessible).
Though this is suboptimal, it is workable given the constraints and conditions.
With this, the risk of additional data loss in the case of ephemeral storage is only if backup failure as well as quorum loss happens.
While this is still less rare than the risk of additional data loss in case of persistent storage, the risk might be tolerable. Provided the risk of quorum loss is not too high. This needs to be monitored/evaluated before opting for ephemeral storage.&lt;/p>
&lt;p>Given these constraints, it is better to dynamically avoid/postpone some potentially disruptive operations when &lt;code>BackupReady&lt;/code> condition is &lt;code>false&lt;/code>.
This has the effect of allowing &lt;code>n/2&lt;/code> members to be evicted when the backups are healthy and completely disabling evictions when backups are not healthy.&lt;/p>
&lt;ol>
&lt;li>Skip/postpone potentially disruptive maintenance operations (listed below) when the &lt;code>BackupReady&lt;/code> condition is &lt;code>false&lt;/code>.&lt;/li>
&lt;li>Vertical scaling.&lt;/li>
&lt;li>Rolling updates, Basically, any updates to the &lt;code>StatefulSet&lt;/code> spec which includes vertical scaling.&lt;/li>
&lt;li>Dynamically toggle the &lt;code>minAvailable&lt;/code> field of the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#poddisruptionbudget">&lt;code>PodDisruptionBudget&lt;/code>&lt;/a> between &lt;code>n/2 + 1&lt;/code> and &lt;code>n&lt;/code> (where &lt;code>n&lt;/code> is the ETCD desired cluster size) whenever the &lt;code>BackupReady&lt;/code> condition toggles between &lt;code>true&lt;/code> and &lt;code>false&lt;/code>.&lt;/li>
&lt;/ol>
&lt;p>This will mean that &lt;code>etcd-backup-restore&lt;/code> becomes Kubernetes-aware. But there might be reasons for making &lt;code>etcd-backup-restore&lt;/code> Kubernetes-aware anyway (e.g. to update the &lt;code>etcd&lt;/code> resource &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">status&lt;/a> with latest full snapshot details).
This enhancement should keep &lt;code>etcd-backup-restore&lt;/code> backward compatible.
I.e. it should be possible to use &lt;code>etcd-backup-restore&lt;/code> Kubernetes-unaware as before this proposal.
This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as &lt;code>--enable-client-service-updates&lt;/code> which can be defaulted to &lt;code>false&lt;/code> for backward compatibility).&lt;/p>
&lt;h5 id="alternative-3">Alternative&lt;/h5>
&lt;p>The alternative is for &lt;code>etcd-druid&lt;/code> to implement the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#manipulating-client-service-podselector">above functionality&lt;/a>.&lt;/p>
&lt;p>But &lt;code>etcd-druid&lt;/code> is centrally deployed in the host Kubernetes cluster and cannot scale well horizontally.
So, it can potentially be a bottleneck if it is involved in regular health check mechanism for all the etcd clusters it manages.
Also, the recommended approach above is more robust because it can work even if &lt;code>etcd-druid&lt;/code> is down when the backup upload of a particular etcd cluster fails.&lt;/p>
&lt;h2 id="status">Status&lt;/h2>
&lt;p>It is desirable (for the &lt;code>etcd-druid&lt;/code> and landscape administrators/operators) to maintain/expose status of the etcd cluster instances in the &lt;code>status&lt;/code> sub-resource of the &lt;code>Etcd&lt;/code> CRD.
The proposed structure for maintaining the status is as shown in the example below.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: druid.gardener.cloud/v1alpha1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: Etcd
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>metadata:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> name: etcd-main
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> replicas: 3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>status:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> conditions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - type: Ready &lt;span style="color:#008000"># Condition type for the readiness of the ETCD cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> status: &lt;span style="color:#a31515">&amp;#34;True&amp;#34;&lt;/span> &lt;span style="color:#008000"># Indicates of the ETCD Cluster is ready or not&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastHeartbeatTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastTransitionTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reason: Quorate &lt;span style="color:#008000"># Quorate|QuorumLost&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - type: AllMembersReady &lt;span style="color:#008000"># Condition type for the readiness of all the member of the ETCD cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> status: &lt;span style="color:#a31515">&amp;#34;True&amp;#34;&lt;/span> &lt;span style="color:#008000"># Indicates if all the members of the ETCD Cluster are ready&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastHeartbeatTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastTransitionTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reason: AllMembersReady &lt;span style="color:#008000"># AllMembersReady|NotAllMembersReady&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - type: BackupReady &lt;span style="color:#008000"># Condition type for the readiness of the backup of the ETCD cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> status: &lt;span style="color:#a31515">&amp;#34;True&amp;#34;&lt;/span> &lt;span style="color:#008000"># Indicates if the backup of the ETCD cluster is ready&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastHeartbeatTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastTransitionTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reason: FullBackupSucceeded &lt;span style="color:#008000"># FullBackupSucceeded|IncrementalBackupSucceeded|FullBackupFailed|IncrementalBackupFailed&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> clusterSize: 3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> replicas: 3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> members:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: etcd-main-0 &lt;span style="color:#008000"># member pod name&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> id: 272e204152 &lt;span style="color:#008000"># member Id&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> role: Leader &lt;span style="color:#008000"># Member|Leader&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> status: Ready &lt;span style="color:#008000"># Ready|NotReady|Unknown&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastTransitionTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reason: LeaseSucceeded &lt;span style="color:#008000"># LeaseSucceeded|LeaseExpired|UnknownGracePeriodExceeded|PodNotRead&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - name: etcd-main-1 &lt;span style="color:#008000"># member pod name&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> id: 272e204153 &lt;span style="color:#008000"># member Id&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> role: Member &lt;span style="color:#008000"># Member|Leader&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> status: Ready &lt;span style="color:#008000"># Ready|NotReady|Unknown&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> lastTransitionTime: &lt;span style="color:#a31515">&amp;#34;2020-11-10T12:48:01Z&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> reason: LeaseSucceeded &lt;span style="color:#008000"># LeaseSucceeded|LeaseExpired|UnknownGracePeriodExceeded|PodNotRead&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This proposal recommends that &lt;code>etcd-druid&lt;/code> (preferrably, the &lt;code>custodian&lt;/code> controller in &lt;code>etcd-druid&lt;/code>) maintains most of the information in the &lt;code>status&lt;/code> of the &lt;code>Etcd&lt;/code> resources described above.&lt;/p>
&lt;p>One exception to this is the &lt;code>BackupReady&lt;/code> condition which is recommended to be maintained by the &lt;em>leading&lt;/em> &lt;code>etcd-backup-restore&lt;/code> sidecar container.
This will mean that &lt;code>etcd-backup-restore&lt;/code> becomes Kubernetes-aware. But there are other reasons for making &lt;code>etcd-backup-restore&lt;/code> Kubernetes-aware anyway (e.g. to &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#health-check">maintain health conditions&lt;/a>).
This enhancement should keep &lt;code>etcd-backup-restore&lt;/code> backward compatible.
But it should be possible to use &lt;code>etcd-backup-restore&lt;/code> Kubernetes-unaware as before this proposal. This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as &lt;code>--enable-etcd-status-updates&lt;/code> which can be defaulted to &lt;code>false&lt;/code> for backward compatibility).&lt;/p>
&lt;h3 id="members">Members&lt;/h3>
&lt;p>The &lt;code>members&lt;/code> section of the status is intended to be maintained by &lt;code>etcd-druid&lt;/code> (preferraby, the &lt;code>custodian&lt;/code> controller of &lt;code>etcd-druid&lt;/code>) based on the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">&lt;code>leases&lt;/code> of the individual members&lt;/a>.&lt;/p>
&lt;h4 id="note-1">Note&lt;/h4>
&lt;p>An earlier design in this proposal was for the individual &lt;code>etcd-backup-restore&lt;/code> sidecars to update the corresponding &lt;code>status.members&lt;/code> entries themselves. But this was redesigned to use &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member &lt;code>leases&lt;/code>&lt;/a> to avoid conflicts rising from frequent updates and the limitations in the support for &lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">Server-Side Apply&lt;/a> in some versions of Kubernetes.&lt;/p>
&lt;p>The &lt;code>spec.holderIdentity&lt;/code> field in the &lt;code>leases&lt;/code> is used to communicate the ETCD member &lt;code>id&lt;/code> and &lt;code>role&lt;/code> between the &lt;code>etcd-backup-restore&lt;/code> sidecars and &lt;code>etcd-druid&lt;/code>.&lt;/p>
&lt;h4 id="member-name-as-the-key">Member name as the key&lt;/h4>
&lt;p>In an ETCD cluster, the member &lt;code>id&lt;/code> is the &lt;a href="https://etcd.io/docs/v3.4/dev-guide/api_reference_v3/#message-member-etcdserveretcdserverpbrpcproto">unique identifier for a member&lt;/a>.
However, this proposal recommends using a &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#kubernetes-context">single &lt;code>StatefulSet&lt;/code>&lt;/a> whose pods form the members of the ETCD cluster and &lt;code>Pods&lt;/code> of a &lt;code>StatefulSet&lt;/code> have &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#ordinal-index">uniquely indexed names&lt;/a> as well as &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-network-id">uniquely addressible DNS&lt;/a>.&lt;/p>
&lt;p>This proposal recommends that the &lt;code>name&lt;/code> of the member (which is the same as the name of the member &lt;code>Pod&lt;/code>) be used as the unique key to identify a member in the &lt;code>members&lt;/code> array.
This can minimise the need to cleanup superfluous entries in the &lt;code>members&lt;/code> array after the member pods are gone to some extent because the replacement pods for any member will share the same &lt;code>name&lt;/code> and will overwrite the entry with a &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">possibly new&lt;/a> member &lt;code>id&lt;/code>.&lt;/p>
&lt;p>There is still the possibility of not only &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status">superfluous entries in the &lt;code>members&lt;/code> array&lt;/a> but also &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member">superfluous &lt;code>members&lt;/code> in the ETCD cluster&lt;/a> for which there is no corresponding pod in the &lt;code>StatefulSet&lt;/code> anymore.&lt;/p>
&lt;p>For example, if an ETCD cluster is scaled up from &lt;code>3&lt;/code> to &lt;code>5&lt;/code> and the new members were failing constantly due to insufficient resources and then if the ETCD client is scaled back down to &lt;code>3&lt;/code> and failing member pods may not have the chance to clean up their &lt;code>member&lt;/code> entries (from the &lt;code>members&lt;/code> array as well as from the ETCD cluster) leading to superfluous members in the cluster that may have adverse effect on quorum of the cluster.&lt;/p>
&lt;p>Hence, the superfluous entries in both &lt;code>members&lt;/code> array as well as the ETCD cluster need to be &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12">cleaned up&lt;/a> &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member">as appropriate&lt;/a>.&lt;/p>
&lt;h4 id="member-leases">Member Leases&lt;/h4>
&lt;p>One &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/cluster-resources/lease-v1/">Kubernetes &lt;code>lease&lt;/code> object&lt;/a> per desired ETCD member is maintained by &lt;code>etcd-druid&lt;/code> (preferrably, the &lt;code>custodian&lt;/code> controller in &lt;code>etcd-druid&lt;/code>).
The &lt;code>lease&lt;/code> objects will be created in the same &lt;code>namespace&lt;/code> as their owning &lt;code>Etcd&lt;/code> object and will have the same &lt;code>name&lt;/code> as the member to which they correspond (which, in turn would be the same as &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-name-as-the-key">the &lt;code>pod&lt;/code> name in which the member ETCD process runs&lt;/a>).&lt;/p>
&lt;p>The &lt;code>lease&lt;/code> objects are created and deleted only by &lt;code>etcd-druid&lt;/code> but are continually renewed within the &lt;code>leaseDurationSeconds&lt;/code> by the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-independent-of-leader-election-in-all-members">individual &lt;code>etcd-backup-restore&lt;/code> sidecars&lt;/a> (corresponding to their members) if the the corresponding ETCD member is ready and is part of the ETCD cluster.&lt;/p>
&lt;p>This will mean that &lt;code>etcd-backup-restore&lt;/code> becomes Kubernetes-aware. But there are other reasons for making &lt;code>etcd-backup-restore&lt;/code> Kubernetes-aware anyway (e.g. to &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#health-check">maintain health conditions&lt;/a>).
This enhancement should keep &lt;code>etcd-backup-restore&lt;/code> backward compatible.
But it should be possible to use &lt;code>etcd-backup-restore&lt;/code> Kubernetes-unaware as before this proposal. This is possible either by auto-detecting the existence of kubeconfig or by an explicit command-line flag (such as &lt;code>--enable-etcd-lease-renewal&lt;/code> which can be defaulted to &lt;code>false&lt;/code> for backward compatibility).&lt;/p>
&lt;p>A &lt;code>member&lt;/code> entry in the &lt;code>Etcd&lt;/code> resource &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">&lt;code>status&lt;/code>&lt;/a> would be marked as &lt;code>Ready&lt;/code> (with &lt;code>reason: LeaseSucceeded&lt;/code>) if the corresponding &lt;code>pod&lt;/code> is ready and the corresponding &lt;code>lease&lt;/code> has not yet expired.
The &lt;code>member&lt;/code> entry would be marked as &lt;code>NotReady&lt;/code> if the corresponding &lt;code>pod&lt;/code> is not ready (with reason &lt;code>PodNotReady&lt;/code>) or as &lt;code>Unknown&lt;/code> if the corresponding &lt;code>lease&lt;/code> has expired (with &lt;code>reason: LeaseExpired&lt;/code>).&lt;/p>
&lt;p>While renewing the lease, the &lt;code>etcd-backup-restore&lt;/code> sidecars also maintain the ETCD member &lt;code>id&lt;/code> and their &lt;code>role&lt;/code> (&lt;code>Leader&lt;/code> or &lt;code>Member&lt;/code>) separated by &lt;code>:&lt;/code> in the &lt;code>spec.holderIdentity&lt;/code> field of the corresponding &lt;code>lease&lt;/code> object since this information is only available to the &lt;code>ETCD&lt;/code> member processes and the &lt;code>etcd-backup-restore&lt;/code> sidecars (e.g. &lt;code>272e204152:Leader&lt;/code> or &lt;code>272e204153:Member&lt;/code>).
When the &lt;code>lease&lt;/code> objects are created by &lt;code>etcd-druid&lt;/code>, the &lt;code>spec.holderIdentity&lt;/code> field would be empty.&lt;/p>
&lt;p>The value in &lt;code>spec.holderIdentity&lt;/code> in the &lt;code>leases&lt;/code> is parsed and copied onto the &lt;code>id&lt;/code> and &lt;code>role&lt;/code> fields of the corresponding &lt;code>status.members&lt;/code> by &lt;code>etcd-druid&lt;/code>.&lt;/p>
&lt;h3 id="conditions">Conditions&lt;/h3>
&lt;p>The &lt;code>conditions&lt;/code> section in the status describe the overall condition of the ETCD cluster.
The condition type &lt;code>Ready&lt;/code> indicates if the ETCD cluster as a whole is ready to serve requests (i.e. the cluster is quorate) even though some minority of the members are not ready.
The condition type &lt;code>AllMembersReady&lt;/code> indicates of all the members of the ETCD cluster are ready.
The distinction between these conditions could be significant for both external consumers of the status as well as &lt;code>etcd-druid&lt;/code> itself.
Some maintenance operations might be safe to do (e.g. rolling updates) only when all members of the cluster are ready.
The condition type &lt;code>BackupReady&lt;/code> indicates of the most recent backup upload (full or incremental) succeeded.
This information also might be significant because some maintenance operations might be safe to do (e.g. anything that involves re-bootstrapping the ETCD cluster) only when backup is ready.&lt;/p>
&lt;p>The &lt;code>Ready&lt;/code> and &lt;code>AllMembersReady&lt;/code> conditions can be maintained by &lt;code>etcd-druid&lt;/code> based on the status in the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#members">&lt;code>members&lt;/code> section&lt;/a>.
The &lt;code>BackupReady&lt;/code> condition will be maintained by the leading &lt;code>etcd-backup-restore&lt;/code> sidecar that is in charge of taking backups.&lt;/p>
&lt;p>More condition types could be introduced in the future if specific purposes arise.&lt;/p>
&lt;h3 id="clustersize">ClusterSize&lt;/h3>
&lt;p>The &lt;code>clusterSize&lt;/code> field contains the current size of the ETCD cluster. It will be actively kept up-to-date by &lt;code>etcd-druid&lt;/code> in all scenarios.&lt;/p>
&lt;ul>
&lt;li>Before &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#bootstrapping">bootstrapping&lt;/a> the ETCD cluster (during cluster creation or later bootstrapping because of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-9">quorum failure&lt;/a>), &lt;code>etcd-druid&lt;/code> will clear the &lt;code>status.members&lt;/code> array and set &lt;code>status.clusterSize&lt;/code> to be equal to &lt;code>spec.replicas&lt;/code>.&lt;/li>
&lt;li>While the ETCD cluster is quorate, &lt;code>etcd-druid&lt;/code> will actively set &lt;code>status.clusterSize&lt;/code> to be equal to length of the &lt;code>status.members&lt;/code> whenever the length of the array changes (say, due to scaling of the ETCD cluster).&lt;/li>
&lt;/ul>
&lt;p>Given that &lt;code>clusterSize&lt;/code> reliably represents the size of the ETCD cluster, it can be used to calculate the &lt;code>Ready&lt;/code> &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">condition&lt;/a>.&lt;/p>
&lt;h3 id="alternative-4">Alternative&lt;/h3>
&lt;p>The alternative is for &lt;code>etcd-druid&lt;/code> to maintain the status in the &lt;code>Etcd&lt;/code> status sub-resource.
But &lt;code>etcd-druid&lt;/code> is centrally deployed in the host Kubernetes cluster and cannot scale well horizontally.
So, it can potentially be a bottleneck if it is involved in regular health check mechanism for all the etcd clusters it manages.
Also, the recommended approach above is more robust because it can work even if &lt;code>etcd-druid&lt;/code> is down when the backup upload of a particular etcd cluster fails.&lt;/p>
&lt;h2 id="decision-table-for-etcd-druid-based-on-the-status">Decision table for etcd-druid based on the status&lt;/h2>
&lt;p>The following decision table describes the various criteria &lt;code>etcd-druid&lt;/code> takes into consideration to determine the different etcd cluster management scenarios and the corresponding reconciliation actions it must take.
The general principle is to detect the scenario and take the minimum action to move the cluster along the path to good health.
The path from any one scenario to a state of good health will typically involve going through multiple reconciliation actions which probably take the cluster through many other cluster management scenarios.
Especially, it is proposed that individual members auto-heal where possible, even in the case of the failure of a majority of members of the etcd cluster and that &lt;code>etcd-druid&lt;/code> takes action only if the auto-healing doesn&amp;rsquo;t happen for a configured period of time.&lt;/p>
&lt;h3 id="1-pink-of-health">1. Pink of health&lt;/h3>
&lt;h4 id="observed-state">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: &lt;code>true&lt;/code>&lt;/li>
&lt;li>AllMembersReady: &lt;code>true&lt;/code>&lt;/li>
&lt;li>BackupReady: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action">Recommended Action&lt;/h4>
&lt;p>Nothing to do&lt;/p>
&lt;h3 id="2-member-status-is-out-of-sync-with-their-leases">2. Member status is out of sync with their leases&lt;/h3>
&lt;h4 id="observed-state-1">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: &lt;code>l&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: &lt;code>true&lt;/code>&lt;/li>
&lt;li>AllMembersReady: &lt;code>true&lt;/code>&lt;/li>
&lt;li>BackupReady: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-1">Recommended Action&lt;/h4>
&lt;p>Mark the &lt;code>l&lt;/code> members corresponding to the expired &lt;code>leases&lt;/code> as &lt;code>Unknown&lt;/code> with reason &lt;code>LeaseExpired&lt;/code> and with &lt;code>id&lt;/code> populated from &lt;code>spec.holderIdentity&lt;/code> of the &lt;code>lease&lt;/code> if they are not already updated so.&lt;/p>
&lt;p>Mark the &lt;code>n - l&lt;/code> members corresponding to the active &lt;code>leases&lt;/code> as &lt;code>Ready&lt;/code> with reason &lt;code>LeaseSucceeded&lt;/code> and with &lt;code>id&lt;/code> populated from &lt;code>spec.holderIdentity&lt;/code> of the &lt;code>lease&lt;/code> if they are not already updated so.&lt;/p>
&lt;p>Please refer &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">here&lt;/a> for more details.&lt;/p>
&lt;h3 id="3-all-members-are-ready-but-allmembersready-condition-is-stale">3. All members are &lt;code>Ready&lt;/code> but &lt;code>AllMembersReady&lt;/code> condition is stale&lt;/h3>
&lt;h4 id="observed-state-2">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: N/A&lt;/li>
&lt;li>AllMembersReady: false&lt;/li>
&lt;li>BackupReady: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-2">Recommended Action&lt;/h4>
&lt;p>Mark the status condition type &lt;code>AllMembersReady&lt;/code> to &lt;code>true&lt;/code>.&lt;/p>
&lt;h3 id="4-not-all-members-are-ready-but-allmembersready-condition-is-stale">4. Not all members are &lt;code>Ready&lt;/code> but &lt;code>AllMembersReady&lt;/code> condition is stale&lt;/h3>
&lt;h4 id="observed-state-3">Observed state&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Cluster Size&lt;/p>
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>StatefulSet&lt;/code> replicas&lt;/p>
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Etcd&lt;/code> status&lt;/p>
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: N/A&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>0 &amp;lt;= r &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>nr&lt;/code> where &lt;code>0 &amp;lt; nr &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>u&lt;/code> where &lt;code>0 &amp;lt; u &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: &lt;code>h&lt;/code> where &lt;code>0 &amp;lt; h &amp;lt; n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: N/A&lt;/li>
&lt;li>AllMembersReady: true&lt;/li>
&lt;li>BackupReady: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>where &lt;code>(nr + u + h) &amp;gt; 0&lt;/code> or &lt;code>r &amp;lt; n&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-3">Recommended Action&lt;/h4>
&lt;p>Mark the status condition type &lt;code>AllMembersReady&lt;/code> to &lt;code>false&lt;/code>.&lt;/p>
&lt;h3 id="5-majority-members-are-ready-but-ready-condition-is-stale">5. Majority members are &lt;code>Ready&lt;/code> but &lt;code>Ready&lt;/code> condition is stale&lt;/h3>
&lt;h4 id="observed-state-4">Observed state&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Cluster Size&lt;/p>
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>StatefulSet&lt;/code> replicas&lt;/p>
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Etcd&lt;/code> status&lt;/p>
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>r &amp;gt; n/2&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>nr&lt;/code> where &lt;code>0 &amp;lt; nr &amp;lt; n/2&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>u&lt;/code> where &lt;code>0 &amp;lt; u &amp;lt; n/2&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: &lt;code>false&lt;/code>&lt;/li>
&lt;li>AllMembersReady: N/A&lt;/li>
&lt;li>BackupReady: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>where &lt;code>0 &amp;lt; (nr + u + h) &amp;lt; n/2&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-4">Recommended Action&lt;/h4>
&lt;p>Mark the status condition type &lt;code>Ready&lt;/code> to &lt;code>true&lt;/code>.&lt;/p>
&lt;h3 id="6-majority-members-are-notready-but-ready-condition-is-stale">6. Majority members are &lt;code>NotReady&lt;/code> but &lt;code>Ready&lt;/code> condition is stale&lt;/h3>
&lt;h4 id="observed-state-5">Observed state&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Cluster Size&lt;/p>
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>StatefulSet&lt;/code> replicas&lt;/p>
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>Etcd&lt;/code> status&lt;/p>
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>0 &amp;lt; r &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>nr&lt;/code> where &lt;code>0 &amp;lt; nr &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>u&lt;/code> where &lt;code>0 &amp;lt; u &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: &lt;code>true&lt;/code>&lt;/li>
&lt;li>AllMembersReady: N/A&lt;/li>
&lt;li>BackupReady: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>where &lt;code>(nr + u + h) &amp;gt; n/2&lt;/code> or &lt;code>r &amp;lt; n/2&lt;/code>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-5">Recommended Action&lt;/h4>
&lt;p>Mark the status condition type &lt;code>Ready&lt;/code> to &lt;code>false&lt;/code>.&lt;/p>
&lt;h3 id="7-some-members-have-been-in-unknown-status-for-a-while">7. Some members have been in &lt;code>Unknown&lt;/code> status for a while&lt;/h3>
&lt;h4 id="observed-state-6">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: N/A&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: N/A&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>u&lt;/code> where &lt;code>u &amp;lt;= n&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: N/A&lt;/li>
&lt;li>AllMembersReady: N/A&lt;/li>
&lt;li>BackupReady: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-6">Recommended Action&lt;/h4>
&lt;p>Mark the &lt;code>u&lt;/code> members as &lt;code>NotReady&lt;/code> in &lt;code>Etcd&lt;/code> status with &lt;code>reason: UnknownGracePeriodExceeded&lt;/code>.&lt;/p>
&lt;h3 id="8-some-member-pods-are-not-ready-but-have-not-had-the-chance-to-update-their-status">8. Some member pods are not &lt;code>Ready&lt;/code> but have not had the chance to update their status&lt;/h3>
&lt;h4 id="observed-state-7">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>s&lt;/code> where &lt;code>s &amp;lt; n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: N/A&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: N/A&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: N/A&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: N/A&lt;/li>
&lt;li>AllMembersReady: N/A&lt;/li>
&lt;li>BackupReady: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-7">Recommended Action&lt;/h4>
&lt;p>Mark the &lt;code>n - s&lt;/code> members (corresponding to the pods that are not &lt;code>Ready&lt;/code>) as &lt;code>NotReady&lt;/code> in &lt;code>Etcd&lt;/code> status with &lt;code>reason: PodNotReady&lt;/code>&lt;/p>
&lt;h3 id="9-quorate-cluster-with-a-minority-of-members-notready">9. Quorate cluster with a minority of members &lt;code>NotReady&lt;/code>&lt;/h3>
&lt;h4 id="observed-state-8">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n - f&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>f&lt;/code> where &lt;code>f &amp;lt; n/2&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: true&lt;/li>
&lt;li>AllMembersReady: false&lt;/li>
&lt;li>BackupReady: true&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-8">Recommended Action&lt;/h4>
&lt;p>Delete the &lt;code>f&lt;/code> &lt;code>NotReady&lt;/code> member pods to force restart of the pods if they do not automatically restart via failed &lt;code>livenessProbe&lt;/code>. The expectation is that they will either re-join the cluster as an existing member or remove themselves and join as new members on restart of the container or pod and &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">renew their &lt;code>leases&lt;/code>&lt;/a>.&lt;/p>
&lt;h3 id="10-quorum-lost-with-a-majority-of-members-notready">10. Quorum lost with a majority of members &lt;code>NotReady&lt;/code>&lt;/h3>
&lt;h4 id="observed-state-9">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n - f&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: &lt;code>f&lt;/code> where &lt;code>f &amp;gt;= n/2&lt;/code>&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: N/A&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: false&lt;/li>
&lt;li>AllMembersReady: false&lt;/li>
&lt;li>BackupReady: true&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-9">Recommended Action&lt;/h4>
&lt;p>Scale down the &lt;code>StatefulSet&lt;/code> to &lt;code>replicas: 0&lt;/code>. Ensure that all member pods are deleted. Ensure that all the members are removed from &lt;code>Etcd&lt;/code> status. Delete and recreate all the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member &lt;code>leases&lt;/code>&lt;/a>. Recover the cluster from loss of quorum as discussed &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here&lt;/a>.&lt;/p>
&lt;h3 id="11-scale-up-of-a-healthy-cluster">11. Scale up of a healthy cluster&lt;/h3>
&lt;h4 id="observed-state-10">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: &lt;code>d&lt;/code>&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code> where &lt;code>d &amp;gt; n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: 0&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: 0&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: 0&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: true&lt;/li>
&lt;li>AllMembersReady: true&lt;/li>
&lt;li>BackupReady: true&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-10">Recommended Action&lt;/h4>
&lt;p>Add &lt;code>d - n&lt;/code> new members by scaling the &lt;code>StatefulSet&lt;/code> to &lt;code>replicas: d&lt;/code>. The rest of the &lt;code>StatefulSet&lt;/code> spec need not be updated until the next cluster bootstrapping (alternatively, the rest of the &lt;code>StatefulSet&lt;/code> spec can be updated pro-actively once the new members join the cluster. This will trigger a rolling update).&lt;/p>
&lt;p>Also, create the additional &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member &lt;code>leases&lt;/code>&lt;/a> for the &lt;code>d - n&lt;/code> new members.&lt;/p>
&lt;h3 id="12-scale-down-of-a-healthy-cluster">12. Scale down of a healthy cluster&lt;/h3>
&lt;h4 id="observed-state-11">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: &lt;code>d&lt;/code>&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code> where &lt;code>d &amp;lt; n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>n&lt;/code>&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: 0&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: 0&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: 0&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: true&lt;/li>
&lt;li>AllMembersReady: true&lt;/li>
&lt;li>BackupReady: true&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-11">Recommended Action&lt;/h4>
&lt;p>Remove &lt;code>d - n&lt;/code> existing members (numbered &lt;code>d&lt;/code>, &lt;code>d + 1&lt;/code> &amp;hellip; &lt;code>n&lt;/code>) by scaling the &lt;code>StatefulSet&lt;/code> to &lt;code>replicas: d&lt;/code>. The &lt;code>StatefulSet&lt;/code> spec need not be updated until the next cluster bootstrapping (alternatively, the &lt;code>StatefulSet&lt;/code> spec can be updated pro-actively once the superfluous members exit the cluster. This will trigger a rolling update).&lt;/p>
&lt;p>Also, delete the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member &lt;code>leases&lt;/code>&lt;/a> for the &lt;code>d - n&lt;/code> members being removed.&lt;/p>
&lt;p>The &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status">superfluous entries in the &lt;code>members&lt;/code> array&lt;/a> will be cleaned up as explained &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12">here&lt;/a>.
The superfluous members in the ETCD cluster will be cleaned up by the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member">leading &lt;code>etcd-backup-restore&lt;/code> sidecar&lt;/a>.&lt;/p>
&lt;h3 id="13-superfluous-member-entries-in-etcd-status">13. Superfluous member entries in &lt;code>Etcd&lt;/code> status&lt;/h3>
&lt;h4 id="observed-state-12">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size
&lt;ul>
&lt;li>Desired: N/A&lt;/li>
&lt;li>Current: &lt;code>n&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>StatefulSet&lt;/code> replicas
&lt;ul>
&lt;li>Desired: n&lt;/li>
&lt;li>Ready: n&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status
&lt;ul>
&lt;li>members
&lt;ul>
&lt;li>Total: &lt;code>m&lt;/code> where &lt;code>m &amp;gt; n&lt;/code>&lt;/li>
&lt;li>Ready: N/A&lt;/li>
&lt;li>Members &lt;code>NotReady&lt;/code> for long enough to be evicted, i.e. &lt;code>lastTransitionTime &amp;gt; notReadyGracePeriod&lt;/code>: N/A&lt;/li>
&lt;li>Members with readiness status &lt;code>Unknown&lt;/code> long enough to be considered &lt;code>NotReady&lt;/code>, i.e. &lt;code>lastTransitionTime &amp;gt; unknownGracePeriod&lt;/code>: N/A&lt;/li>
&lt;li>Members with expired &lt;code>lease&lt;/code>: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>conditions:
&lt;ul>
&lt;li>Ready: N/A&lt;/li>
&lt;li>AllMembersReady: N/A&lt;/li>
&lt;li>BackupReady: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-12">Recommended Action&lt;/h4>
&lt;p>Remove the superfluous &lt;code>m - n&lt;/code> member entries from &lt;code>Etcd&lt;/code> status (numbered &lt;code>n&lt;/code>, &lt;code>n+1&lt;/code> &amp;hellip; &lt;code>m&lt;/code>).
Remove the superfluous &lt;code>m - n&lt;/code> &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member &lt;code>leases&lt;/code>&lt;/a> if they exist.
The superfluous members in the ETCD cluster will be cleaned up by the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#work-flows-only-on-the-leading-member">leading &lt;code>etcd-backup-restore&lt;/code> sidecar&lt;/a>.&lt;/p>
&lt;h2 id="decision-table-for-etcd-backup-restore-during-initialization">Decision table for etcd-backup-restore during initialization&lt;/h2>
&lt;p>As discussed above, the initialization sequence of &lt;code>etcd-backup-restore&lt;/code> in a member pod needs to &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#etcd-configuration">generate suitable etcd configuration&lt;/a> for its etcd container.
It also might have to handle the etcd database verification and restoration functionality differently in &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#restarting-an-existing-member-of-an-etcd-cluster">different&lt;/a> &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">scenarios&lt;/a>.&lt;/p>
&lt;p>The initialization sequence itself is proposed to be as follows.
It is an enhancement of the &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/master/docs/proposals/design.md#workflow">existing&lt;/a> initialization sequence.
&lt;img src="https://gardener.cloud/__resources/01-etcd-member-initialization-sequence_364f5e.png" alt="etcd member initialization sequence">&lt;/p>
&lt;p>The details of the decisions to be taken during the initialization are given below.&lt;/p>
&lt;h3 id="1-first-member-during-bootstrap-of-a-fresh-etcd-cluster">1. First member during bootstrap of a fresh etcd cluster&lt;/h3>
&lt;h4 id="observed-state-13">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: &lt;code>false&lt;/code>&lt;/li>
&lt;li>Data directory is valid and up-to-date: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: &lt;code>false&lt;/code>&lt;/li>
&lt;li>Backup has incremental snapshots: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-13">Recommended Action&lt;/h4>
&lt;p>Generate etcd configuration with &lt;code>n&lt;/code> initial cluster peer URLs and initial cluster state new and return success.&lt;/p>
&lt;h3 id="2-addition-of-a-new-following-member-during-bootstrap-of-a-fresh-etcd-cluster">2. Addition of a new following member during bootstrap of a fresh etcd cluster&lt;/h3>
&lt;h4 id="observed-state-14">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>m&lt;/code> where &lt;code>0 &amp;lt; m &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>m&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: &lt;code>false&lt;/code>&lt;/li>
&lt;li>Data directory is valid and up-to-date: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: &lt;code>false&lt;/code>&lt;/li>
&lt;li>Backup has incremental snapshots: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-14">Recommended Action&lt;/h4>
&lt;p>Generate etcd configuration with &lt;code>n&lt;/code> initial cluster peer URLs and initial cluster state new and return success.&lt;/p>
&lt;h3 id="3-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-and-data">3. Restart of an existing member of a quorate cluster with valid metadata and data&lt;/h3>
&lt;h4 id="observed-state-15">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>m&lt;/code> where &lt;code>m &amp;gt; n/2&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>r &amp;gt; n/2&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: &lt;code>true&lt;/code>&lt;/li>
&lt;li>Data directory is valid and up-to-date: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: N/A&lt;/li>
&lt;li>Backup has incremental snapshots: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-15">Recommended Action&lt;/h4>
&lt;p>Re-use previously generated etcd configuration and return success.&lt;/p>
&lt;h3 id="4-restart-of-an-existing-member-of-a-quorate-cluster-with-valid-metadata-but-without-valid-data">4. Restart of an existing member of a quorate cluster with valid metadata but without valid data&lt;/h3>
&lt;h4 id="observed-state-16">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>m&lt;/code> where &lt;code>m &amp;gt; n/2&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>r &amp;gt; n/2&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: &lt;code>true&lt;/code>&lt;/li>
&lt;li>Data directory is valid and up-to-date: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: N/A&lt;/li>
&lt;li>Backup has incremental snapshots: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-16">Recommended Action&lt;/h4>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">Remove&lt;/a> self as a member (old member ID) from the etcd cluster as well as &lt;code>Etcd&lt;/code> status. &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">Add&lt;/a> self as a new member of the etcd cluster as well as in the &lt;code>Etcd&lt;/code> status. If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here&lt;/a> for the reason for not restoring incremental snapshots). Generate etcd configuration with &lt;code>n&lt;/code> initial cluster peer URLs and initial cluster state &lt;code>existing&lt;/code> and return success.&lt;/p>
&lt;h3 id="5-restart-of-an-existing-member-of-a-quorate-cluster-without-valid-metadata">5. Restart of an existing member of a quorate cluster without valid metadata&lt;/h3>
&lt;h4 id="observed-state-17">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>m&lt;/code> where &lt;code>m &amp;gt; n/2&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>r &amp;gt; n/2&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: &lt;code>false&lt;/code>&lt;/li>
&lt;li>Data directory is valid and up-to-date: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: N/A&lt;/li>
&lt;li>Backup has incremental snapshots: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-17">Recommended Action&lt;/h4>
&lt;p>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#removing-an-existing-member-from-an-etcd-cluster">Remove&lt;/a> self as a member (old member ID) from the etcd cluster as well as &lt;code>Etcd&lt;/code> status. &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#adding-a-new-member-to-an-etcd-cluster">Add&lt;/a> self as a new member of the etcd cluster as well as in the &lt;code>Etcd&lt;/code> status. If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here&lt;/a> for the reason for not restoring incremental snapshots). Generate etcd configuration with &lt;code>n&lt;/code> initial cluster peer URLs and initial cluster state &lt;code>existing&lt;/code> and return success.&lt;/p>
&lt;h3 id="6-restart-of-an-existing-member-of-a-non-quorate-cluster-with-valid-metadata-and-data">6. Restart of an existing member of a non-quorate cluster with valid metadata and data&lt;/h3>
&lt;h4 id="observed-state-18">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>m&lt;/code> where &lt;code>m &amp;lt; n/2&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>r &amp;lt; n/2&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: &lt;code>true&lt;/code>&lt;/li>
&lt;li>Data directory is valid and up-to-date: &lt;code>true&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: N/A&lt;/li>
&lt;li>Backup has incremental snapshots: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-18">Recommended Action&lt;/h4>
&lt;p>Re-use previously generated etcd configuration and return success.&lt;/p>
&lt;h3 id="7-restart-of-the-first-member-of-a-non-quorate-cluster-without-valid-data">7. Restart of the first member of a non-quorate cluster without valid data&lt;/h3>
&lt;h4 id="observed-state-19">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>0&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: N/A&lt;/li>
&lt;li>Data directory is valid and up-to-date: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: N/A&lt;/li>
&lt;li>Backup has incremental snapshots: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-19">Recommended Action&lt;/h4>
&lt;p>If backups do not exist, create an empty data and WAL directory. If backups exist, restore the latest full snapshot. Start a single-node embedded etcd with initial cluster peer URLs containing only own peer URL and initial cluster state &lt;code>new&lt;/code>. If incremental snapshots exist, apply them serially (honouring source transactions). Take and upload a full snapshot after incremental snapshots are applied successfully (please see &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here&lt;/a> for more reasons why). Generate etcd configuration with &lt;code>n&lt;/code> initial cluster peer URLs and initial cluster state &lt;code>new&lt;/code> and return success.&lt;/p>
&lt;h3 id="8-restart-of-a-following-member-of-a-non-quorate-cluster-without-valid-data">8. Restart of a following member of a non-quorate cluster without valid data&lt;/h3>
&lt;h4 id="observed-state-20">Observed state&lt;/h4>
&lt;ul>
&lt;li>Cluster Size: &lt;code>n&lt;/code>&lt;/li>
&lt;li>&lt;code>Etcd&lt;/code> status members:
&lt;ul>
&lt;li>Total: &lt;code>m&lt;/code> where &lt;code>1 &amp;lt; m &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Ready: &lt;code>r&lt;/code> where &lt;code>1 &amp;lt; r &amp;lt; n&lt;/code>&lt;/li>
&lt;li>Status contains own member: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Data persistence
&lt;ul>
&lt;li>WAL directory has cluster/ member metadata: N/A&lt;/li>
&lt;li>Data directory is valid and up-to-date: &lt;code>false&lt;/code>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Backup
&lt;ul>
&lt;li>Backup exists: N/A&lt;/li>
&lt;li>Backup has incremental snapshots: N/A&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h4 id="recommended-action-20">Recommended Action&lt;/h4>
&lt;p>If backups do not exist, create an empty data and WAL directory. If backups exist, restore only the latest full snapshot (please see &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">here&lt;/a> for the reason for not restoring incremental snapshots). Generate etcd configuration with &lt;code>n&lt;/code> initial cluster peer URLs and initial cluster state &lt;code>existing&lt;/code> and return success.&lt;/p>
&lt;h2 id="backup">Backup&lt;/h2>
&lt;p>Only one of the etcd-backup-restore sidecars among the members are required to take the backup for a given ETCD cluster. This can be called a &lt;code>backup leader&lt;/code>. There are two possibilities to ensure this.&lt;/p>
&lt;h3 id="leading-etcd-main-containers-sidecar-is-the-backup-leader">Leading ETCD main container’s sidecar is the backup leader&lt;/h3>
&lt;p>The backup-restore sidecar could poll the etcd cluster and/or its own etcd main container to see if it is the leading member in the etcd cluster.
This information can be used by the backup-restore sidecars to decide that sidecar of the leading etcd main container is the backup leader (i.e. responsible to for taking/uploading backups regularly).&lt;/p>
&lt;p>The advantages of this approach are as follows.&lt;/p>
&lt;ul>
&lt;li>The approach is operationally and conceptually simple. The leading etcd container and backup-restore sidecar are always located in the same pod.&lt;/li>
&lt;li>Network traffic between the backup container and the etcd cluster will always be local.&lt;/li>
&lt;/ul>
&lt;p>The disadvantage is that this approach may not age well in the future if we think about moving the backup-restore container as a separate pod rather than a sidecar container.&lt;/p>
&lt;h3 id="independent-leader-election-between-backup-restore-sidecars">Independent leader election between backup-restore sidecars&lt;/h3>
&lt;p>We could use the etcd &lt;code>lease&lt;/code> mechanism to perform leader election among the backup-restore sidecars. For example, using something like &lt;a href="https://pkg.go.dev/go.etcd.io/etcd/clientv3/concurrency#Election.Campaign">&lt;code>go.etcd.io/etcd/clientv3/concurrency&lt;/code>&lt;/a>.&lt;/p>
&lt;p>The advantage and disadvantages are pretty much the opposite of the approach &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#leading-etcd-main-containers-sidecar-is-the-backup-leader">above&lt;/a>.
The advantage being that this approach may age well in the future if we think about moving the backup-restore container as a separate pod rather than a sidecar container.&lt;/p>
&lt;p>The disadvantages are as follows.&lt;/p>
&lt;ul>
&lt;li>The approach is operationally and conceptually a bit complex. The leading etcd container and backup-restore sidecar might potentially belong to different pods.&lt;/li>
&lt;li>Network traffic between the backup container and the etcd cluster might potentially be across nodes.&lt;/li>
&lt;/ul>
&lt;h2 id="history-compaction">History Compaction&lt;/h2>
&lt;p>This proposal recommends to configure &lt;a href="https://etcd.io/docs/v3.2.17/op-guide/maintenance/#history-compaction">automatic history compaction&lt;/a> on the individual members.&lt;/p>
&lt;h2 id="defragmentation">Defragmentation&lt;/h2>
&lt;p>Defragmentation is already &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/0dfdd50fbfc5ebc88238be3bc79c3ac3fc242c08/cmd/options.go#L209">triggered periodically&lt;/a> by &lt;code>etcd-backup-restore&lt;/code>.
This proposal recommends to enhance this functionality to be performed only by the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup">leading&lt;/a> backup-restore container.
The defragmentation must be performed only when etcd cluster is in full health and must be done in a rolling manner for each members to &lt;a href="https://etcd.io/docs/v3.2.17/op-guide/maintenance/#defragmentation">avoid disruption&lt;/a>.
The leading member should be defragmented last after all the rest of the members have been defragmented to minimise potential leadership changes caused by defragmentation.
If the etcd cluster is unhealthy when it is time to trigger scheduled defragmentation, the defragmentation must be postponed until the cluster becomes healthy. This check must be done before triggering defragmentation for each member.&lt;/p>
&lt;h2 id="work-flows-in-etcd-backup-restore">Work-flows in etcd-backup-restore&lt;/h2>
&lt;p>There are different work-flows in etcd-backup-restore.
Some existing flows like initialization, scheduled backups and defragmentation have been enhanced or modified.
Some new work-flows like status updates have been introduced.
Some of these work-flows are sensitive to which &lt;code>etcd-backup-restore&lt;/code> container is &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup">leading&lt;/a> and some are not.&lt;/p>
&lt;p>The life-cycle of these work-flows is shown below.
&lt;img src="https://gardener.cloud/__resources/01-etcd-backup-restore-work-flows-life-cycle_eec586.png" alt="etcd-backup-restore work-flows life-cycle">&lt;/p>
&lt;h3 id="work-flows-independent-of-leader-election-in-all-members">Work-flows independent of leader election in all members&lt;/h3>
&lt;ul>
&lt;li>Serve the &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/master/pkg/server/httpAPI.go#L101-L107">HTTP API&lt;/a> that all members are expected to support currently but some HTTP API call which are used to take &lt;a href="https://github.com/gardener/etcd-backup-restore/blob/5dfcc1f848a9f325d41a24eae4defb70d997c215/pkg/server/httpAPI.go#L103-L105">out-of-sync delta or full snapshot&lt;/a> should delegate the incoming HTTP requests to the &lt;code>leading-sidecar&lt;/code> and one of the possible approach to achieve this is via an &lt;a href="https://pkg.go.dev/net/http/httputil#ReverseProxy.ServeHTTP">HTTP reverse proxy&lt;/a>.&lt;/li>
&lt;li>Check the health of the respective etcd member and renew the corresponding &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#member-leases">member &lt;code>lease&lt;/code>&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="work-flows-only-on-the-leading-member">Work-flows only on the leading member&lt;/h3>
&lt;ul>
&lt;li>Take &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup">backups&lt;/a> (full and incremental) at configured regular intervals&lt;/li>
&lt;li>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#defragmentation">Defragment&lt;/a> all the members sequentially at configured regular intervals&lt;/li>
&lt;li>Cleanup superflous members from the ETCD cluster for which there is no corresponding pod (the &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#ordinal-index">ordinal&lt;/a> in the pod name is greater than the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#clustersize">cluster size&lt;/a>) at regular intervals (or whenever the &lt;code>Etcd&lt;/code> resource &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">status&lt;/a> changes by watching it)
&lt;ul>
&lt;li>The cleanup of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#13-superfluous-member-entries-in-etcd-status">superfluous entries in &lt;code>status.members&lt;/code> array&lt;/a> is already covered &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recommended-action-12">here&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="high-availability">High Availability&lt;/h2>
&lt;p>Considering that high-availability is the primary reason for using a multi-node etcd cluster, it makes sense to distribute the individual member pods of the etcd cluster across different physical nodes.
If the underlying Kubernetes cluster has nodes from multiple availability zones, it makes sense to also distribute the member pods across nodes from different availability zones.&lt;/p>
&lt;p>One possibility to do this is via &lt;a href="https://kubernetes.io/docs/reference/scheduling/policies/#priorities">&lt;code>SelectorSpreadPriority&lt;/code>&lt;/a> of &lt;code>kube-scheduler&lt;/code> but this is only &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone">best-effort&lt;/a> and may not always be enforced strictly.&lt;/p>
&lt;p>It is better to use &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">pod anti-affinity&lt;/a> to enforce such distribution of member pods.&lt;/p>
&lt;h3 id="zonal-cluster---single-availability-zone">Zonal Cluster - Single Availability Zone&lt;/h3>
&lt;p>A zonal cluster is configured to consist of nodes belonging to only a single availability zone in a region of the cloud provider.
In such a case, we can at best distribute the member pods of a multi-node etcd cluster instance only across different nodes in the configured availability zone.&lt;/p>
&lt;p>This can be done by specifying &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">pod anti-affinity&lt;/a> in the specification of the member pods using &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#kubernetes-io-hostname">&lt;code>kubernetes.io/hostname&lt;/code>&lt;/a> as the topology key.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: StatefulSet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> affinity:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> podAntiAffinity:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requiredDuringSchedulingIgnoredDuringExecution:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - labelSelector: {} &lt;span style="color:#008000"># podSelector that matches the member pods of the given etcd cluster instance&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologyKey: &lt;span style="color:#a31515">&amp;#34;kubernetes.io/hostname&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The recommendation is to keep &lt;code>etcd-druid&lt;/code> agnostic of such topics related scheduling and cluster-topology and to use &lt;a href="https://github.com/gardener/kupid">kupid&lt;/a> to &lt;a href="https://github.com/gardener/kupid#mutating-higher-order-controllers">orthogonally inject&lt;/a> the desired &lt;a href="https://github.com/gardener/kupid/blob/master/config/samples/cpsp-pod-affinity-anti-affinity.yaml">pod anti-affinity&lt;/a>.&lt;/p>
&lt;h4 id="alternative-5">Alternative&lt;/h4>
&lt;p>Another option is to build the functionality into &lt;code>etcd-druid&lt;/code> to include the required pod anti-affinity when it provisions the &lt;code>StatefulSet&lt;/code> that manages the member pods.
While this has the advantage of avoiding a dependency on an external component like &lt;a href="https://github.com/gardener/kupid">kupid&lt;/a>, the disadvantage is that we might need to address development or testing use-cases where it might be desirable to avoid distributing member pods and schedule them on as less number of nodes as possible.
Also, as mentioned &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#regional-cluster---multiple-availability-zones">below&lt;/a>, &lt;a href="https://github.com/gardener/kupid">kupid&lt;/a> can be used to distribute member pods of an etcd cluster instance across nodes in a single availability zone as well as across nodes in multiple availability zones with very minor variation.
This keeps the solution uniform regardless of the topology of the underlying Kubernetes cluster.&lt;/p>
&lt;h3 id="regional-cluster---multiple-availability-zones">Regional Cluster - Multiple Availability Zones&lt;/h3>
&lt;p>A regional cluster is configured to consist of nodes belonging to multiple availability zones (typically, three) in a region of the cloud provider.
In such a case, we can distribute the member pods of a multi-node etcd cluster instance across nodes belonging to different availability zones.&lt;/p>
&lt;p>This can be done by specifying &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity">pod anti-affinity&lt;/a> in the specification of the member pods using &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone">&lt;code>topology.kubernetes.io/zone&lt;/code>&lt;/a> as the topology key.
In Kubernetes clusters using Kubernetes release older than &lt;code>1.17&lt;/code>, the older (and now deprecated) &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#failure-domainbetakubernetesiozone">&lt;code>failure-domain.beta.kubernetes.io/zone&lt;/code>&lt;/a> might have to be used as the topology key.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>apiVersion: apps/v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kind: StatefulSet
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> template:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> spec:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> affinity:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> podAntiAffinity:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> requiredDuringSchedulingIgnoredDuringExecution:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> - labelSelector: {} &lt;span style="color:#008000"># podSelector that matches the member pods of the given etcd cluster instance&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> topologyKey: &amp;#34;topology.kubernetes.io/zone
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The recommendation is to keep &lt;code>etcd-druid&lt;/code> agnostic of such topics related scheduling and cluster-topology and to use &lt;a href="https://github.com/gardener/kupid">kupid&lt;/a> to &lt;a href="https://github.com/gardener/kupid#mutating-higher-order-controllers">orthogonally inject&lt;/a> the desired &lt;a href="https://github.com/gardener/kupid/blob/master/config/samples/cpsp-pod-affinity-anti-affinity.yaml">pod anti-affinity&lt;/a>.&lt;/p>
&lt;h4 id="alternative-6">Alternative&lt;/h4>
&lt;p>Another option is to build the functionality into &lt;code>etcd-druid&lt;/code> to include the required pod anti-affinity when it provisions the &lt;code>StatefulSet&lt;/code> that manages the member pods.
While this has the advantage of avoiding a dependency on an external component like &lt;a href="https://github.com/gardener/kupid">kupid&lt;/a>, the disadvantage is that such built-in support necessarily limits what kind of topologies of the underlying cluster will be supported.
Hence, it is better to keep &lt;code>etcd-druid&lt;/code> altogether agnostic of issues related to scheduling and cluster-topology.&lt;/p>
&lt;h3 id="poddisruptionbudget">PodDisruptionBudget&lt;/h3>
&lt;p>This proposal recommends that &lt;code>etcd-druid&lt;/code> should deploy &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets">&lt;code>PodDisruptionBudget&lt;/code>&lt;/a> (&lt;code>minAvailable&lt;/code> set to &lt;code>floor(&amp;lt;cluster size&amp;gt;/2) + 1&lt;/code>) for multi-node etcd clusters (if &lt;code>AllMembersReady&lt;/code> &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">condition&lt;/a> is &lt;code>true&lt;/code>) to ensure that any planned disruptive operation can try and honour the disruption budget to ensure high availability of the etcd cluster while making potentially disrupting maintenance operations.&lt;/p>
&lt;p>Also, it is recommended to toggle the &lt;code>minAvailable&lt;/code> field between &lt;code>floor(&amp;lt;cluster size&amp;gt;/2)&lt;/code> and &lt;code>&amp;lt;number of members with status Ready true&amp;gt;&lt;/code> whenever the &lt;code>AllMembersReady&lt;/code> condition toggles between &lt;code>true&lt;/code> and &lt;code>false&lt;/code>.
This is to disable eviction of any member pods when not all members are &lt;code>Ready&lt;/code>.&lt;/p>
&lt;p>In case of a conflict, the recommendation is to use the highest of the applicable values for &lt;code>minAvailable&lt;/code>.&lt;/p>
&lt;h2 id="rolling-updates-to-etcd-members">Rolling updates to etcd members&lt;/h2>
&lt;p>Any changes to the &lt;code>Etcd&lt;/code> resource spec that might result in a change to &lt;code>StatefulSet&lt;/code> spec or otherwise result in a rolling update of member pods should be applied/propagated by &lt;code>etcd-druid&lt;/code> only when the etcd cluster is fully healthy to reduce the risk of quorum loss during the updates.
This would include vertical autoscaling changes (via, &lt;a href="https://github.com/gardener/hvpa-controller">HVPA&lt;/a>).
If the cluster &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#status">status&lt;/a> unhealthy (i.e. if either &lt;code>AllMembersReady&lt;/code> or &lt;code>BackupReady&lt;/code> &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#conditions">conditions&lt;/a> are &lt;code>false&lt;/code>), &lt;code>etcd-druid&lt;/code> must restore it to full health &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure">before proceeding&lt;/a> with such operations that lead to rolling updates.
This can be further optimized in the future to handle the cases where rolling updates can still be performed on an etcd cluster that is not fully healthy.&lt;/p>
&lt;h2 id="follow-up">Follow Up&lt;/h2>
&lt;h3 id="ephemeral-volumes">Ephemeral Volumes&lt;/h3>
&lt;p>See section &lt;em>&lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#Ephemeral_Volumes">Ephemeral Volumes&lt;/a>&lt;/em>.&lt;/p>
&lt;h3 id="shoot-control-plane-migration">Shoot Control-Plane Migration&lt;/h3>
&lt;p>This proposal adds support for multi-node etcd clusters but it should not have significant impact on &lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md">shoot control-plane migration&lt;/a> any more than what already present in the single-node etcd cluster scenario.
But to be sure, this needs to be discussed further.&lt;/p>
&lt;h3 id="performance-impact-of-multi-node-etcd-clusters">Performance impact of multi-node etcd clusters&lt;/h3>
&lt;p>Multi-node etcd clusters incur a cost on &lt;a href="https://etcd.io/docs/v2/admin_guide/#optimal-cluster-size">write performance&lt;/a> as compared to single-node etcd clusters.
This performance impact needs to be measured and documented.
Here, we should compare different persistence option for the multi-nodeetcd clusters so that we have all the information necessary to take the decision balancing the high-availability, performance and costs.&lt;/p>
&lt;h3 id="metrics-dashboards-and-alerts">Metrics, Dashboards and Alerts&lt;/h3>
&lt;p>There are already metrics exported by etcd and &lt;code>etcd-backup-restore&lt;/code> which are visualized in monitoring dashboards and also used in triggering alerts.
These might have hidden assumptions about single-node etcd clusters.
These might need to be enhanced and potentially new metrics, dashboards and alerts configured to cover the multi-node etcd cluster scenario.&lt;/p>
&lt;p>Especially, a high priority alert must be raised if &lt;code>BackupReady&lt;/code> &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#condition">condition&lt;/a> becomes &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#backup-failure">&lt;code>false&lt;/code>&lt;/a>.&lt;/p>
&lt;h3 id="costs">Costs&lt;/h3>
&lt;p>Multi-node etcd clusters will clearly involve higher cost (when compared with single-node etcd clusters) just going by the CPU and memory usage for the additional members.
Also, the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#data-persistence">different options&lt;/a> for persistence for etcd data for the members will have different cost implications.
Such cost impact needs to be assessed and documented to help navigate the trade offs between high availability, performance and costs.&lt;/p>
&lt;h2 id="future-work">Future Work&lt;/h2>
&lt;h3 id="gardener-ring">Gardener Ring&lt;/h3>
&lt;p>&lt;a href="https://github.com/gardener/gardener/issues/233">Gardener Ring&lt;/a>, requires provisioning and management of an etcd cluster with the members distributed across more than one Kubernetes cluster.
This cannot be achieved by etcd-druid alone which has only the view of a single Kubernetes cluster.
An additional component that has the view of all the Kubernetes clusters involved in setting up the gardener ring will be required to achieve this.
However, etcd-druid can be used by such a higher-level component/controller (for example, by supplying the initial cluster configuration) such that individual etcd-druid instances in the individual Kubernetes clusters can manage the corresponding etcd cluster members.&lt;/p>
&lt;h3 id="autonomous-shoot-clusters">Autonomous Shoot Clusters&lt;/h3>
&lt;p>&lt;a href="https://github.com/gardener/gardener/issues/2906">Autonomous Shoot Clusters&lt;/a> also will require a highly availble etcd cluster to back its control-plane and the multi-node support proposed here can be leveraged in that context.
However, the current proposal will not meet all the needs of a autonomous shoot cluster.
Some additional components will be required that have the overall view of the autonomous shoot cluster and they can use etcd-druid to manage the multi-node etcd cluster. But this scenario may be different from that of &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#gardener-ring">Gardener Ring&lt;/a> in that the individual etcd members of the cluster may not be hosted on different Kubernetes clusters.&lt;/p>
&lt;h3 id="optimization-of-recovery-from-non-quorate-cluster-with-some-member-containing-valid-data">Optimization of recovery from non-quorate cluster with some member containing valid data&lt;/h3>
&lt;p>It might be possible to optimize the actions during the recovery of a non-quorate cluster where some of the members contain valid data and some other don&amp;rsquo;t.
The optimization involves verifying the data of the valid members to determine the data of which member is the most recent (even considering the latest backup) so that the &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#recovering-an-etcd-cluster-from-failure-of-majority-of-members">full snapshot&lt;/a> can be taken from it before recovering the etcd cluster.
Such an optimization can be attempted in the future.&lt;/p>
&lt;h3 id="optimization-of-rolling-updates-to-unhealthy-etcd-clusters">Optimization of rolling updates to unhealthy etcd clusters&lt;/h3>
&lt;p>As mentioned &lt;a href="https://gardener.cloud/docs/other-components/etcd-druid/proposals/01-multi-node-etcd-clusters/#rolling-updates-to-etcd-members">above&lt;/a>, optimizations to proceed with rolling updates to unhealthy etcd clusters (without first restoring the cluster to full health) can be pursued in future work.&lt;/p></description></item></channel></rss>