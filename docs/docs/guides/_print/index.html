<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/guides/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/guides/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Guides | Gardener</title><meta name=description content="Walkthroughs of common activities"><meta property="og:title" content="Guides"><meta property="og:description" content="Walkthroughs of common activities"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/guides/"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Guides"><meta itemprop=description content="Walkthroughs of common activities"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Guides"><meta name=twitter:description content="Walkthroughs of common activities"><link rel=preload href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css as=style><link href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.eca5a60ed08d3da0a7d7b68bdc9340c6.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/guides/>Return to the regular view of this page</a>.</p></div><h1 class=title>Guides</h1><div class=lead>Walkthroughs of common activities</div><div class=content></div></div><div class=td-content><h1 id=pg-2f3c2b33632a5b32f28090351784eb13>1 - Set Up Client Tools</h1></div><div class=td-content><h1 id=pg-d5b88accd33e41e23fc01854ca4067ef>1.1 - Fun with kubectl Aliases</h1><div class=lead>Some bash tips that save you some time</div><h2 id=speed-up-your-terminal-workflow>Speed up Your Terminal Workflow</h2><p>Use the Kubernetes command-line tool, <code>kubectl</code>, to deploy and manage applications on Kubernetes. Using kubectl, you can inspect cluster resources, as well as create, delete, and update components.</p><p><img src=/__resources/teaser_6c2e98.svg alt=port-forward></p><p>You will probably run more than a hundred kubectl commands on some days and you should speed up your terminal workflow with with some shortcuts. Of course, there are good shortcuts and bad shortcuts (lazy coding, lack of security review, etc.), but let&rsquo;s stick with the positives and talk about a good shortcut: <strong>bash aliases</strong> in your <code>.profile</code>.</p><p>What are those mysterious <code>.profile</code> and <code>.bash_profile</code> files you&rsquo;ve heard about?</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>The contents of a .profile file are executed on every log-in of the owner of the file</div><p>What&rsquo;s the <code>.bash_profile</code> then? It&rsquo;s exactly the same, but under a different name. The unix shell you are logging into, in this case OS X, looks for <code>etc/profile</code> and loads it if it exists. Then it looks for <code>~/.bash_profile</code>, <code>~/.bash_login</code> and finally <code>~/.profile</code>, and loads the first one of these it finds.</p><h2 id=populating-the-profile-file>Populating the <code>.profile</code> File</h2><p>Here is the fantastic time saver that needs to be in your shell profile:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># time save number one. shortcut for kubectl</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>alias k=<span style=color:#a31515>&#34;kubectl&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Start a shell in a pod AND kill them after leaving</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>alias ksh=<span style=color:#a31515>&#34;kubectl run busybox -i --tty --image=busybox --restart=Never --rm -- sh&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># opens a bash</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>alias kbash=<span style=color:#a31515>&#34;kubectl run busybox -i --tty --image=busybox --restart=Never --rm -- ash&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># activate/exports the kuberconfig.yaml in the current working directory</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>alias kexport=<span style=color:#a31515>&#34;export KUBECONFIG=`pwd`/kubeconfig.yaml&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># usage: kurl http://your-svc.namespace.cluster.local</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span><span style=color:green># we need for this our very own image...never trust an unknown image..</span>
</span></span><span style=display:flex><span>alias kurl=<span style=color:#a31515>&#34;docker run --rm byrnedo/alpine-curl&#34;</span>
</span></span></code></pre></div><p>All the <code>kubectl</code> <a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/#enabling-shell-autocompletion>tab completions</a> still work fine with these aliases, so you’re not losing that speed.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4><p>If the approach above does not work for you add the following lines in your ~/.bashrc instead:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># time save number one. shortcut for kubectl</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>alias k=<span style=color:#a31515>&#34;kubectl&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Enable kubectl completion</span>
</span></span><span style=display:flex><span>source &lt;(k completion bash | sed s/kubectl/k/g)
</span></span></code></pre></div></div></div><div class=td-content style=page-break-before:always><h1 id=pg-1c95a7bfc839f3ac9a674f5034b53305>1.2 - Kubeconfig Context as bash Prompt</h1><div class=lead>Expose the active kubeconfig into bash</div><h2 id=overview>Overview</h2><p>Use the Kubernetes command-line tool, <strong>kubectl</strong>, to deploy and manage applications on Kubernetes.
Using kubectl, you can inspect cluster resources, as well as create, delete, and update components.</p><p><img src=/__resources/howto-kubeconfig-bash_f6391b.gif alt=port-forward></p><p>By default, the kubectl configuration is located at <code>~/.kube/config</code>.</p><p>Let us suppose that you have two clusters, one for development work and one for scratch work.</p><p>How to handle this easily without copying the used configuration always to the right place?</p><h2 id=export-the-kubeconfig-enviroment-variable>Export the KUBECONFIG Enviroment Variable</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>bash$ export KUBECONFIG=&lt;PATH-TO-M&gt;-CONFIG&gt;/kubeconfig-dev.yaml
</span></span></code></pre></div><p>How to determine which cluster is used by the kubectl command?</p><h2 id=determine-active-cluster>Determine Active Cluster</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>bash$ kubectl cluster-info
</span></span><span style=display:flex><span>Kubernetes master is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com
</span></span><span style=display:flex><span>KubeDNS is running at https://api.dev.garden.shoot.canary.k8s-hana.ondemand.com/api/v1/proxy/namespaces/kube-system/services/kube-dns
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>To further debug and diagnose cluster problems, use <span style=color:#a31515>&#39;kubectl cluster-info dump&#39;</span>.
</span></span><span style=display:flex><span>bash$ 
</span></span></code></pre></div><h2 id=display-cluster-in-the-bash---linux-and-alike>Display Cluster in the bash - Linux and Alike</h2><p>I found this tip on Stackoverflow and find it worth to be added here.
Edit your <code>~/.bash_profile</code> and add the following code snippet to show the current K8s
context in the shell&rsquo;s prompt:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>prompt_k8s(){
</span></span><span style=display:flex><span>  k8s_current_context=<span style=color:#00f>$(</span>kubectl config current-context 2&gt; /dev/null<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>  <span style=color:#00f>if</span> [[ $? -eq 0 ]] ; <span style=color:#00f>then</span> echo -e <span style=color:#a31515>&#34;(</span><span style=color:#a31515>${</span>k8s_current_context<span style=color:#a31515>}</span><span style=color:#a31515>) &#34;</span>; <span style=color:#00f>fi</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span> 
</span></span><span style=display:flex><span>PS1+=<span style=color:#a31515>&#39;$(prompt_k8s)&#39;</span>
</span></span></code></pre></div><p>After this, your bash command prompt contains the active KUBECONFIG context and you always know
which cluster is active - <em>develop</em> or <em>production</em>.</p><p>e.g.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>bash$ export KUBECONFIG=/Users/d023280/Documents/workspace/gardener-ui/kubeconfig_gardendev.yaml 
</span></span><span style=display:flex><span>bash (garden_dev)$ 
</span></span></code></pre></div><p>Note the <strong>(garden_dev)</strong> prefix in the bash command prompt.</p><p><strong>This helps immensely to avoid thoughtless mistakes.</strong></p><h2 id=display-cluster-in-the-powershell---windows>Display Cluster in the PowerShell - Windows</h2><p>Display current K8s cluster in the title of PowerShell window.</p><p>Create a <a href=https://superuser.com/a/1045659>profile</a> file for your shell under <code>%UserProfile%\Documents\Windows­PowerShell\Microsoft.PowerShell_profile.ps1</code></p><p>Copy following code to <code>Microsoft.PowerShell_profile.ps1</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span> <span style=color:#00f>function</span> prompt_k8s {
</span></span><span style=display:flex><span>     $k8s_current_context = (kubectl config current-context) | Out-String
</span></span><span style=display:flex><span>     <span style=color:#00f>if</span>($?) {
</span></span><span style=display:flex><span>         <span style=color:#00f>return</span> $k8s_current_context
</span></span><span style=display:flex><span>     }<span style=color:#00f>else</span> {
</span></span><span style=display:flex><span>         <span style=color:#00f>return</span> <span style=color:#a31515>&#34;No K8S contenxt found&#34;</span>
</span></span><span style=display:flex><span>     }
</span></span><span style=display:flex><span> }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span> $host.ui.rawui.WindowTitle = prompt_k8s
</span></span></code></pre></div><p><img src=/__resources/howto-bash_kubeconfig_powershell_3d19a6.png alt=port-forward></p><p>If you want to switch to different cluster, you can set <code>KUBECONFIG</code> to new value, and re-run the file <code>Microsoft.PowerShell_profile.ps1</code></p></div><div class=td-content style=page-break-before:always><h1 id=pg-268a3007674e22e02204b60f4c31e2b8>1.3 - Organizing Access Using kubeconfig Files</h1><h2 id=overview>Overview</h2><p>The kubectl command-line tool uses <code>kubeconfig</code> files to find the information it needs to choose a cluster and
communicate with the API server of a cluster.</p><h2 id=problem>Problem</h2><p>If you&rsquo;ve become aware of a security breach that affects you, you may want to revoke or cycle credentials
in case anything was leaked. However, this is not possible with the initial or master <code>kubeconfig</code> from your
cluster.</p><p><img src=/__resources/teaser_e3e966.svg alt=teaser></p><h2 id=pitfall>Pitfall</h2><p>Never distribute the <code>kubeconfig</code>, which you can download directly within the Gardener dashboard, for a productive cluster.</p><p><img src=/__resources/kubeconfig-initial_42c43d.png alt=kubeconfig-dont></p><h2 id=create-a-custom-kubeconfig-file-for-each-user>Create a Custom kubeconfig File for Each User</h2><p>Create a separate <code>kubeconfig</code> for each user. One of the big advantages of this approach is that you can revoke them and control
the permissions better. A limitation to single namespaces is also possible here.</p><p>The script creates a new <code>ServiceAccount</code> with read privileges in the whole cluster (Secrets are excluded).
To run the script, <a href=https://deno.land/>Deno</a>, a secure TypeScript runtime, must be installed.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-TypeScript data-lang=TypeScript><span style=display:flex><span><span>#</span>!<span>/usr/bin/env -S deno run --allow-run</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>/*
</span></span></span><span style=display:flex><span><span style=color:green>* This script create Kubernetes ServiceAccount and other required resource and print KUBECONFIG to console.
</span></span></span><span style=display:flex><span><span style=color:green>* Depending on your requirements you might want change clusterRoleBindingTemplate() function
</span></span></span><span style=display:flex><span><span style=color:green>*
</span></span></span><span style=display:flex><span><span style=color:green>* In order to execute this script it&#39;s required to install Deno.js https://deno.land/ (TypeScript &amp; JavaScript runtime).
</span></span></span><span style=display:flex><span><span style=color:green>* It&#39;s single executable binary for the major OSs from the original author of the Node.js
</span></span></span><span style=display:flex><span><span style=color:green>* example: deno run --allow-run kubeconfig-for-custom-user.ts d00001
</span></span></span><span style=display:flex><span><span style=color:green>* example: deno run --allow-run kubeconfig-for-custom-user.ts d00001 --delete
</span></span></span><span style=display:flex><span><span style=color:green>*
</span></span></span><span style=display:flex><span><span style=color:green>* known issue: shebang does works under the Linux but not for Windows Linux Subsystem
</span></span></span><span style=display:flex><span><span style=color:green>*/</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>const</span> KUBECTL = <span style=color:#a31515>&#34;/usr/local/bin/kubectl&#34;</span> <span style=color:green>//or
</span></span></span><span style=display:flex><span><span style=color:green>// const KUBECTL = &#34;C:\\Program Files\\Docker\\Docker\\resources\\bin\\kubectl.exe&#34;
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span><span style=color:#00f>const</span> serviceAccName = Deno.args[0]
</span></span><span style=display:flex><span><span style=color:#00f>const</span> deleteIt = Deno.args[1]
</span></span><span style=display:flex><span><span style=color:#00f>if</span> (serviceAccName == <span style=color:#00f>undefined</span> || serviceAccName == <span style=color:#a31515>&#34;--delete&#34;</span> ) {
</span></span><span style=display:flex><span>    console.log(<span style=color:#a31515>&#34;please provide username as an argument, for example: deno run --allow-run kubeconfig-for-custom-user.ts USER_NAME [--delete]&#34;</span>)
</span></span><span style=display:flex><span>    Deno.exit(1)
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>if</span> (deleteIt == <span style=color:#a31515>&#34;--delete&#34;</span>) {
</span></span><span style=display:flex><span>    exec([KUBECTL, <span style=color:#a31515>&#34;delete&#34;</span>, <span style=color:#a31515>&#34;serviceaccount&#34;</span>, serviceAccName])
</span></span><span style=display:flex><span>    exec([KUBECTL, <span style=color:#a31515>&#34;delete&#34;</span>, <span style=color:#a31515>&#34;secret&#34;</span>, <span style=color:#a31515>`</span><span style=color:#a31515>${</span>serviceAccName<span style=color:#a31515>}</span><span style=color:#a31515>-secret`</span>])
</span></span><span style=display:flex><span>    exec([KUBECTL, <span style=color:#a31515>&#34;delete&#34;</span>, <span style=color:#a31515>&#34;clusterrolebinding&#34;</span>, <span style=color:#a31515>`view-</span><span style=color:#a31515>${</span>serviceAccName<span style=color:#a31515>}</span><span style=color:#a31515>-global`</span>])
</span></span><span style=display:flex><span>    Deno.exit(0)
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>await</span> exec([KUBECTL, <span style=color:#a31515>&#34;create&#34;</span>, <span style=color:#a31515>&#34;serviceaccount&#34;</span>, serviceAccName, <span style=color:#a31515>&#34;-o&#34;</span>, <span style=color:#a31515>&#34;json&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>await</span> exec([KUBECTL, <span style=color:#a31515>&#34;create&#34;</span>, <span style=color:#a31515>&#34;-o&#34;</span>, <span style=color:#a31515>&#34;json&#34;</span>, <span style=color:#a31515>&#34;-f&#34;</span>, <span style=color:#a31515>&#34;-&#34;</span>], secretYamlTemplate())
</span></span><span style=display:flex><span><span style=color:#00f>let</span> secret = <span style=color:#00f>await</span> exec([KUBECTL, <span style=color:#a31515>&#34;get&#34;</span>, <span style=color:#a31515>&#34;secret&#34;</span>, <span style=color:#a31515>`</span><span style=color:#a31515>${</span>serviceAccName<span style=color:#a31515>}</span><span style=color:#a31515>-secret`</span>, <span style=color:#a31515>&#34;-o&#34;</span>, <span style=color:#a31515>&#34;json&#34;</span>])
</span></span><span style=display:flex><span><span style=color:#00f>let</span> caCRT = secret.data[<span style=color:#a31515>&#34;ca.crt&#34;</span>];
</span></span><span style=display:flex><span><span style=color:#00f>let</span> userToken = atob(secret.data[<span style=color:#a31515>&#34;token&#34;</span>]); <span style=color:green>//decode base64
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span><span style=color:#00f>let</span> kubeConfig = <span style=color:#00f>await</span> exec([KUBECTL, <span style=color:#a31515>&#34;config&#34;</span>, <span style=color:#a31515>&#34;view&#34;</span>, <span style=color:#a31515>&#34;--minify&#34;</span>, <span style=color:#a31515>&#34;-o&#34;</span>, <span style=color:#a31515>&#34;json&#34;</span>]);
</span></span><span style=display:flex><span><span style=color:#00f>let</span> clusterApi = kubeConfig.clusters[0].cluster.server
</span></span><span style=display:flex><span><span style=color:#00f>let</span> clusterName = kubeConfig.clusters[0].name
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>await</span> exec([KUBECTL, <span style=color:#a31515>&#34;create&#34;</span>, <span style=color:#a31515>&#34;-o&#34;</span>, <span style=color:#a31515>&#34;json&#34;</span>, <span style=color:#a31515>&#34;-f&#34;</span>, <span style=color:#a31515>&#34;-&#34;</span>], clusterRoleBindingTemplate())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>console.log(kubeConfigTemplate(caCRT, userToken, clusterApi, clusterName, serviceAccName + <span style=color:#a31515>&#34;-&#34;</span> + clusterName))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>async</span> <span style=color:#00f>function</span> exec(args: <span style=color:#2b91af>string</span>[], stdInput?: <span style=color:#2b91af>string</span>): Promise&lt;Object&gt; {
</span></span><span style=display:flex><span>    console.log(<span style=color:#a31515>&#34;# &#34;</span>+args.join(<span style=color:#a31515>&#34; &#34;</span>))
</span></span><span style=display:flex><span>    <span style=color:#00f>let</span> opt: <span style=color:#2b91af>Deno.RunOptions</span> = {
</span></span><span style=display:flex><span>        cmd: <span style=color:#2b91af>args</span>,
</span></span><span style=display:flex><span>        stdout: <span style=color:#a31515>&#34;piped&#34;</span>,
</span></span><span style=display:flex><span>        stderr: <span style=color:#a31515>&#34;piped&#34;</span>,
</span></span><span style=display:flex><span>        stdin: <span style=color:#a31515>&#34;piped&#34;</span>,
</span></span><span style=display:flex><span>    };
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00f>const</span> p = Deno.run(opt);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00f>if</span> (stdInput != <span style=color:#00f>undefined</span>) {
</span></span><span style=display:flex><span>        <span style=color:#00f>await</span> p.stdin.write(<span style=color:#00f>new</span> TextEncoder().encode(stdInput));
</span></span><span style=display:flex><span>        <span style=color:#00f>await</span> p.stdin.close();
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#00f>const</span> status = <span style=color:#00f>await</span> p.status()
</span></span><span style=display:flex><span>    <span style=color:#00f>const</span> output = <span style=color:#00f>await</span> p.output()
</span></span><span style=display:flex><span>    <span style=color:#00f>const</span> stderrOutput = <span style=color:#00f>await</span> p.stderrOutput()
</span></span><span style=display:flex><span>    <span style=color:#00f>if</span> (status.code === 0) {
</span></span><span style=display:flex><span>        <span style=color:#00f>return</span> JSON.parse(<span style=color:#00f>new</span> TextDecoder().decode(output))
</span></span><span style=display:flex><span>    } <span style=color:#00f>else</span> {
</span></span><span style=display:flex><span>        <span style=color:#00f>let</span> error = <span style=color:#00f>new</span> TextDecoder().decode(stderrOutput);
</span></span><span style=display:flex><span>        <span style=color:#00f>return</span> <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>function</span> clusterRoleBindingTemplate() {
</span></span><span style=display:flex><span>    <span style=color:#00f>return</span> <span style=color:#a31515>`
</span></span></span><span style=display:flex><span><span style=color:#a31515>apiVersion: rbac.authorization.k8s.io/v1
</span></span></span><span style=display:flex><span><span style=color:#a31515>kind: ClusterRoleBinding
</span></span></span><span style=display:flex><span><span style=color:#a31515>metadata:
</span></span></span><span style=display:flex><span><span style=color:#a31515>  name: view-</span><span style=color:#a31515>${</span>serviceAccName<span style=color:#a31515>}</span><span style=color:#a31515>-global
</span></span></span><span style=display:flex><span><span style=color:#a31515>subjects:
</span></span></span><span style=display:flex><span><span style=color:#a31515>- kind: ServiceAccount
</span></span></span><span style=display:flex><span><span style=color:#a31515>  name: </span><span style=color:#a31515>${</span>serviceAccName<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>  namespace: default
</span></span></span><span style=display:flex><span><span style=color:#a31515>roleRef:
</span></span></span><span style=display:flex><span><span style=color:#a31515>  kind: ClusterRole
</span></span></span><span style=display:flex><span><span style=color:#a31515>  name: view
</span></span></span><span style=display:flex><span><span style=color:#a31515>  apiGroup: rbac.authorization.k8s.io    
</span></span></span><span style=display:flex><span><span style=color:#a31515>`</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>function</span> secretYamlTemplate() {
</span></span><span style=display:flex><span>    <span style=color:#00f>return</span> <span style=color:#a31515>`
</span></span></span><span style=display:flex><span><span style=color:#a31515>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#a31515>kind: Secret
</span></span></span><span style=display:flex><span><span style=color:#a31515>metadata:
</span></span></span><span style=display:flex><span><span style=color:#a31515>  name: </span><span style=color:#a31515>${</span>serviceAccName<span style=color:#a31515>}</span><span style=color:#a31515>-secret
</span></span></span><span style=display:flex><span><span style=color:#a31515>  annotations:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    kubernetes.io/service-account.name: </span><span style=color:#a31515>${</span>serviceAccName<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>type: kubernetes.io/service-account-token`</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>function</span> kubeConfigTemplate(certificateAuthority: <span style=color:#2b91af>string</span>, token: <span style=color:#2b91af>string</span>, clusterApi: <span style=color:#2b91af>string</span>, clusterName: <span style=color:#2b91af>string</span>, username: <span style=color:#2b91af>string</span>) {
</span></span><span style=display:flex><span>    <span style=color:#00f>return</span> <span style=color:#a31515>`
</span></span></span><span style=display:flex><span><span style=color:#a31515>## KUBECONFIG generated on </span><span style=color:#a31515>${</span><span style=color:#00f>new</span> Date()<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#a31515>clusters:
</span></span></span><span style=display:flex><span><span style=color:#a31515>- cluster:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    certificate-authority-data: </span><span style=color:#a31515>${</span>certificateAuthority<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    server: </span><span style=color:#a31515>${</span>clusterApi<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>  name: </span><span style=color:#a31515>${</span>clusterName<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>contexts:
</span></span></span><span style=display:flex><span><span style=color:#a31515>- context:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    cluster: </span><span style=color:#a31515>${</span>clusterName<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    user: </span><span style=color:#a31515>${</span>username<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>  name: </span><span style=color:#a31515>${</span>clusterName<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>current-context: </span><span style=color:#a31515>${</span>clusterName<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>kind: Config
</span></span></span><span style=display:flex><span><span style=color:#a31515>preferences: {}
</span></span></span><span style=display:flex><span><span style=color:#a31515>users:
</span></span></span><span style=display:flex><span><span style=color:#a31515>- name: </span><span style=color:#a31515>${</span>username<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>  user:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    token: </span><span style=color:#a31515>${</span>token<span style=color:#a31515>}</span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>`</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>If <strong>edit</strong> or <strong>admin</strong> rights are to be assigned, the <code>ClusterRoleBinding</code> must be adapted in the <code>roleRef</code> section
with the roles listed below.</p><p>Furthermore, you can restrict this to a single namespace by not creating a <code>ClusterRoleBinding</code> but only a <code>RoleBinding</code>
within the desired namespace.</p><table><thead><tr><th>Default ClusterRole</th><th>Default ClusterRoleBinding</th><th>Description</th></tr></thead><tbody><tr><td>cluster-admin</td><td>system:masters group</td><td>Allows super-user access to perform any action on any resource. When used in a ClusterRoleBinding, it gives full control over every resource in the cluster and in all namespaces. When used in a RoleBinding, it gives full control over every resource in the rolebinding&rsquo;s namespace, including the namespace itself.</td></tr><tr><td>admin</td><td>None</td><td>Allows admin access, intended to be granted within a namespace using a RoleBinding. If used in a RoleBinding, allows read/write access to most resources in a namespace, including the ability to create roles and rolebindings within the namespace. It does not allow write access to resource quota or to the namespace itself.</td></tr><tr><td>edit</td><td>None</td><td>Allows read/write access to most objects in a namespace. It does not allow viewing or modifying roles or rolebindings.</td></tr><tr><td>view</td><td>None</td><td>Allows read-only access to see most objects in a namespace. It does not allow viewing roles or rolebindings. It does not allow viewing secrets, since those are escalating.</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-5c18612002901d96ad951fadfaba3172>2 - Security and Compliance</h1></div><div class=td-content><h1 id=pg-c56592130abf04a930c26d6a08520204>2.1 - Regional Restrictions</h1><div class=lead>How Gardener supports regional restrictions</div><h2 id=shared-responsibility-model>Shared Responsibility Model</h2><p>Gardener, like most cloud providers&rsquo; Kubernetes offerings, is dedicated for a global setup. And just like how most cloud providers offer means to fulfil regional restrictions, Gardener also has some means built in for this purpose. Similarly, Gardener also follows a shared responsibility model where users are obliged to use the provided Gardener means in a way which results in compliance with regional restrictions.</p><h3 id=regions>Regions</h3><p>Gardener users need to understand that Gardener is a generic tool and has no built-in knowledge about regions as geographical or political conglomerates. For Gardener, regions are only strings. To create regional restrictions is an obligation of all Gardener users who orchestrate existing Gardener functionality to reach evidence which can be audited later on.</p><h3 id=support-for-regional-restrictions>Support for Regional Restrictions</h3><p>Gardener offers functionality to support the most important kind of regional restrictions in its global setup:</p><ul><li><strong>No Restriction:</strong> All seeds in all regions can be allowed to host the control plane of all shoots.</li><li><strong>Restriction by Dedication:</strong> Shoots running in a region can be configured so that only dedicated seeds in dedicated regions are allowed to host the shoot’s control plane. This can be achieved by adding labels to a seed and subsequently restricting shoot control plane placement to appropriately labeled seeds by using the field <code>spec.seedSelector</code> (<a href=https://github.com/gardener/gardener/blob/v1.84.1/example/90-shoot.yaml#L365-L368>example</a>).</li><li><strong>Restriction by Tainting:</strong> Some seeds running in some dedicated regions are not allowed to host the control plane of any shoots unless explicitly allowed. This can be achieved by tainting seeds appropriately (<a href=https://github.com/gardener/gardener/blob/v1.84.1/example/50-seed.yaml#L86-L88>example</a>) which in turn requires explicit tolerations if a shoot&rsquo;s control plane should be placed on such tainted seeds (<a href=https://github.com/gardener/gardener/blob/v1.84.1/example/90-shoot.yaml#L360-L361>example</a>).</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cffb3dcf36355a1a8fe956b75f9915c5>2.2 - Kubernetes Cluster Hardening Procedure</h1><div class=lead>Compliant user management of your Gardener projects</div><h2 id=overview>Overview</h2><p>The Gardener team takes security seriously, which is why we mandate the Security Technical Implementation Guide (STIG) for Kubernetes as published by the Defense Information Systems Agency (DISA) <a href=https://public.cyber.mil/stigs/downloads/>here</a>. We offer Gardener adopters the opportunity to show compliance with DISA Kubernetes STIG via the compliance checker tool <a href=https://github.com/gardener/diki>diki</a>. The latest release in machine readable format can be found in the <a href="https://public.cyber.mil/stigs/downloads/?_dl_facet_stigs=container-platform">STIGs Document Library</a> by searching for Kubernetes.</p><h2 id=kubernetes-clusters-security-requirements>Kubernetes Clusters Security Requirements</h2><p><a href=https://cyber.trackr.live/stig/Kubernetes/1/11>DISA Kubernetes STIG version 1 release 11</a> contains 91 rules overall. <strong>Only the following rules, however, apply to you</strong>. Some of them are secure-by-default, so your responsibility is to make sure that they are not changed. For your convenience, the requirements are grouped logically and per role:</p><h2 id=rules-relevant-for-cluster-admins>Rules Relevant for Cluster Admins</h2><h3 id=control-plane-configuration>Control Plane Configuration</h3><table><thead><tr><th>ID</th><th>Description</th><th>Secure By Default</th><th>Comments</th></tr></thead><tbody><tr><td>242390</td><td>Kubernetes API server must have anonymous authentication disabled</td><td>✅</td><td>Disabled unless you enable it via <a href=https://gardener.cloud/docs/gardener/api-reference/core/#kubeapiserverconfig>enableAnnonymousAuthentication</a></td></tr><tr><td>245543</td><td>Kubernetes API Server must disable token authentication to protect information in transit</td><td>✅</td><td>Disabled unless you enable it via <a href=https://gardener.cloud/docs/gardener/api-reference/core/#kubernetes>enableStaticTokenKubeconfig</a></td></tr><tr><td>242400</td><td>Kubernetes API server must have Alpha APIs disabled</td><td>✅</td><td>Disabled unless you enable it via <a href=https://gardener.cloud/docs/gardener/api-reference/core/#kubernetesconfig>featureGates</a></td></tr><tr><td>242436</td><td>Kubernetes API server must have the ValidatingAdmissionWebhook enabled</td><td>✅</td><td>Enabled unless you disable it explicitly via <a href=https://gardener.cloud/docs/gardener/api-reference/core/#kubeapiserverconfig>admissionPlugins</a></td></tr><tr><td>242398</td><td>Kubernetes DynamicAuditing must not be enabled</td><td>✅</td><td>Disabled unless you enable it via <a href=https://gardener.cloud/docs/gardener/api-reference/core/#kubernetesconfig>featureGates</a></td></tr><tr><td>242399</td><td>Kubernetes DynamicKubeletConfig must not be enabled</td><td>✅</td><td>Disabled unless you enable it via <a href=https://gardener.cloud/docs/gardener/api-reference/core/#kubernetesconfig>featureGates</a></td></tr><tr><td>242393</td><td>Kubernetes Worker Nodes must not have sshd service running</td><td>❌</td><td>Active to allow debugging of network issues, but it is possible to deactivate via the <a href=https://gardener.cloud/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.SSHAccess>sshAccess</a> setting</td></tr><tr><td>242394</td><td>Kubernetes Worker Nodes must not have the sshd service enabled</td><td>❌</td><td>Enabled to allow debugging of network issues, but it is possible to deactivate via the <a href=https://gardener.cloud/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.SSHAccess>sshAccess</a> setting</td></tr><tr><td>242434</td><td>Kubernetes Kubelet must enable kernel protection</td><td>✅</td><td>Enabled for Kubernetes v1.26 or later unless disabled explicitly via <a href=https://gardener.cloud/docs/gardener/api-reference/core/#kubeletconfig>protectKernalDefaults</a></td></tr><tr><td>245541</td><td>Kubernetes Kubelet must not disable timeouts</td><td>✅</td><td>Enabled for Kubernetes v1.26 or later unless disabled explicitly via <a href=https://gardener.cloud/docs/gardener/api-reference/core/#kubeletconfig>streamingConnectionIdleTimeout</a></td></tr></tbody></table><h3 id=audit-configuration>Audit Configuration</h3><table><thead><tr><th>ID</th><th>Description</th><th>Secure By Default</th><th>Comments</th></tr></thead><tbody><tr><td>242402</td><td>The Kubernetes API Server must have an audit log path set</td><td>❌</td><td>It is the user&rsquo;s responsibility to configure an audit extension that meets the requirements of their organization. Depending on the audit extension implementation the audit logs do not always need to be written on the filesystem, i.e. when <code>--audit-webhook-config-file</code> is set and logs are sent to an audit backend.</td></tr><tr><td>242403</td><td>Kubernetes API Server must generate audit records that identify what type of event has occurred, identify the source of the event, contain the event results, identify any users, and identify any containers associated with the event</td><td>❌</td><td>Users should set an audit policy that meets the requirements of their organization. Please consult the <a href=https://gardener.cloud/docs/gardener/shoot_auditpolicy/>Shoot Audit Policy documentation</a>.</td></tr><tr><td>242461</td><td>Kubernetes API Server audit logs must be enabled</td><td>❌</td><td>Users should set an audit policy that meets the requirements of their organization. Please consult the <a href=https://gardener.cloud/docs/gardener/shoot_auditpolicy/>Shoot Audit Policy documentation</a>.</td></tr><tr><td>242462</td><td>The Kubernetes API Server must be set to audit log max size</td><td>❌</td><td>It is the user&rsquo;s responsibility to configure an audit extension that meets the requirements of their organization. Depending on the audit extension implementation the audit logs do not always need to be written on the filesystem, i.e. when <code>--audit-webhook-config-file</code> is set and logs are sent to an audit backend.</td></tr><tr><td>242463</td><td>The Kubernetes API Server must be set to audit log maximum backup</td><td>❌</td><td>It is the user&rsquo;s responsibility to configure an audit extension that meets the requirements of their organization. Depending on the audit extension implementation the audit logs do not always need to be written on the filesystem, i.e. when <code>--audit-webhook-config-file</code> is set and logs are sent to an audit backend.</td></tr><tr><td>242464</td><td>The Kubernetes API Server audit log retention must be set</td><td>❌</td><td>It is the user&rsquo;s responsibility to configure an audit extension that meets the requirements of their organization. Depending on the audit extension implementation the audit logs do not always need to be written on the filesystem, i.e. when <code>--audit-webhook-config-file</code> is set and logs are sent to an audit backend.</td></tr><tr><td>242465</td><td>The Kubernetes API Server audit log path must be set</td><td>❌</td><td>It is the user&rsquo;s responsibility to configure an audit extension that meets the requirements of their organization. Depending on the audit extension implementation the audit logs do not always need to be written on the filesystem, i.e. when <code>--audit-webhook-config-file</code> is set and logs are sent to an audit backend.</td></tr></tbody></table><h3 id=end-user-workload>End User Workload</h3><table><thead><tr><th>ID</th><th>Description</th><th>Secure By Default</th><th>Comments</th></tr></thead><tbody><tr><td>242395</td><td>Kubernetes dashboard must not be enabled</td><td>✅</td><td>Not installed unless you install it via <a href=https://gardener.cloud/docs/gardener/api-reference/core/#addon>kubernetesDashboard</a>.</td></tr><tr><td>242414</td><td>Kubernetes cluster must use non-privileged host ports for user pods</td><td>❌</td><td>Do not use any ports below 1024 for your own workload.</td></tr><tr><td>242415</td><td>Secrets in Kubernetes must not be stored as environment variables</td><td>❌</td><td>Always mount secrets as volumes and never as environment variables.</td></tr><tr><td>242383</td><td>User-managed resources must be created in dedicated namespaces</td><td>❌</td><td>Create and use your own/dedicated namespaces and never place anything into the default, kube-system, kube-public, or kube-node-lease namespace. The default namespace is never to be used while the other above listed namespaces are only to be used by the Kubernetes provider (here Gardener).</td></tr><tr><td>242417</td><td>Kubernetes must separate user functionality</td><td>❌</td><td>While 242383 is about all resources, this rule is specifically about pods. Create and use your own/dedicated namespaces and never place pods into the default, kube-system, kube-public, or kube-node-lease namespace. The default namespace is never to be used while the other above listed namespaces are only to be used by the Kubernetes provider (here Gardener).</td></tr><tr><td>242437</td><td>Kubernetes must have a pod security policy set</td><td>✅</td><td>Set, but Gardener can only set default pod security policies (PSP) and does so only until v1.24 as with v1.25 PSPs were removed (deprecated since v1.21) and replaced with <a href=https://kubernetes.io/docs/concepts/security/pod-security-standards>Pod Security Standards</a> (see <a href=https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future>this blog</a> for more information). Whatever the technology, you are responsible to configure custom-tailured appropriate PSPs respectively use them or PSSs, depending on your own workload and security needs (only you know what a pod should be allowed to do).</td></tr><tr><td>242442</td><td>Kubernetes must remove old components after updated versions have been installed</td><td>❌</td><td>While Gardener manages all its components in its system namespaces (automated), you are naturally responsible for your own workload.</td></tr><tr><td>254800</td><td>Kubernetes must have a Pod Security Admission control file configured</td><td>❌</td><td>Gardener ensures that the pod security configuration allows system components to be deployed in the kube-system namespace but does not set configurations that can affect user namespaces. It is recommended that users enforce a minimum of <a href=https://kubernetes.io/docs/concepts/security/pod-security-admission/#pod-security-levels>baseline pod security level for their workload</a> via <a href=https://gardener.cloud/docs/gardener/pod-security/#admission-configuration-for-the-podsecurity-admission-plugin>PodSecurity admission plugin</a>.</td></tr></tbody></table><h2 id=rules-relevant-for-service-providers>Rules Relevant for Service Providers</h2><table><thead><tr><th>ID</th><th>Description</th></tr></thead><tbody><tr><td>242376</td><td>The Kubernetes Controller Manager must use TLS 1.2, at a minimum, to protect the confidentiality of sensitive data during electronic dissemination.</td></tr><tr><td>242377</td><td>The Kubernetes Scheduler must use TLS 1.2, at a minimum, to protect the confidentiality of sensitive data during electronic dissemination.</td></tr><tr><td>242378</td><td>The Kubernetes API Server must use TLS 1.2, at a minimum, to protect the confidentiality of sensitive data during electronic dissemination.</td></tr><tr><td>242379</td><td>The Kubernetes etcd must use TLS to protect the confidentiality of sensitive data during electronic dissemination.</td></tr><tr><td>242380</td><td>The Kubernetes etcd must use TLS to protect the confidentiality of sensitive data during electronic dissemination.</td></tr><tr><td>242381</td><td>The Kubernetes Controller Manager must create unique service accounts for each work payload.</td></tr><tr><td>242382</td><td>The Kubernetes API Server must enable Node,RBAC as the authorization mode.</td></tr><tr><td>242384</td><td>The Kubernetes Scheduler must have secure binding.</td></tr><tr><td>242385</td><td>The Kubernetes Controller Manager must have secure binding.</td></tr><tr><td>242386</td><td>The Kubernetes API server must have the insecure port flag disabled.</td></tr><tr><td>242387</td><td>The Kubernetes Kubelet must have the &ldquo;readOnlyPort&rdquo; flag disabled.</td></tr><tr><td>242388</td><td>The Kubernetes API server must have the insecure bind address not set.</td></tr><tr><td>242389</td><td>The Kubernetes API server must have the secure port set.</td></tr><tr><td>242391</td><td>The Kubernetes Kubelet must have anonymous authentication disabled.</td></tr><tr><td>242392</td><td>The Kubernetes kubelet must enable explicit authorization.</td></tr><tr><td>242396</td><td>Kubernetes Kubectl cp command must give expected access and results.</td></tr><tr><td>242397</td><td>The Kubernetes kubelet staticPodPath must not enable static pods.</td></tr><tr><td>242404</td><td>Kubernetes Kubelet must deny hostname override.</td></tr><tr><td>242405</td><td>The Kubernetes manifests must be owned by root.</td></tr><tr><td>242406</td><td>The Kubernetes KubeletConfiguration file must be owned by root.</td></tr><tr><td>242407</td><td>The Kubernetes KubeletConfiguration files must have file permissions set to 644 or more restrictive.</td></tr><tr><td>242408</td><td>The Kubernetes manifest files must have least privileges.</td></tr><tr><td>242409</td><td>Kubernetes Controller Manager must disable profiling.</td></tr><tr><td>242410</td><td>The Kubernetes API Server must enforce ports, protocols, and services (PPS) that adhere to the Ports, Protocols, and Services Management Category Assurance List (PPSM CAL).</td></tr><tr><td>242411</td><td>The Kubernetes Scheduler must enforce ports, protocols, and services (PPS) that adhere to the Ports, Protocols, and Services Management Category Assurance List (PPSM CAL).</td></tr><tr><td>242412</td><td>The Kubernetes Controllers must enforce ports, protocols, and services (PPS) that adhere to the Ports, Protocols, and Services Management Category Assurance List (PPSM CAL).</td></tr><tr><td>242413</td><td>The Kubernetes etcd must enforce ports, protocols, and services (PPS) that adhere to the Ports, Protocols, and Services Management Category Assurance List (PPSM CAL).</td></tr><tr><td>242418</td><td>The Kubernetes API server must use approved cipher suites.</td></tr><tr><td>242419</td><td>Kubernetes API Server must have the SSL Certificate Authority set.</td></tr><tr><td>242420</td><td>Kubernetes Kubelet must have the SSL Certificate Authority set.</td></tr><tr><td>242421</td><td>Kubernetes Controller Manager must have the SSL Certificate Authority set.</td></tr><tr><td>242422</td><td>Kubernetes API Server must have a certificate for communication.</td></tr><tr><td>242423</td><td>Kubernetes etcd must enable client authentication to secure service.</td></tr><tr><td>242424</td><td>Kubernetes Kubelet must enable tlsPrivateKeyFile for client authentication to secure service.</td></tr><tr><td>242425</td><td>Kubernetes Kubelet must enable tlsCertFile for client authentication to secure service.</td></tr><tr><td>242426</td><td>Kubernetes etcd must enable client authentication to secure service.</td></tr><tr><td>242427</td><td>Kubernetes etcd must have a key file for secure communication.</td></tr><tr><td>242428</td><td>Kubernetes etcd must have a certificate for communication.</td></tr><tr><td>242429</td><td>Kubernetes etcd must have the SSL Certificate Authority set.</td></tr><tr><td>242430</td><td>Kubernetes etcd must have a certificate for communication.</td></tr><tr><td>242431</td><td>Kubernetes etcd must have a key file for secure communication.</td></tr><tr><td>242432</td><td>Kubernetes etcd must have peer-cert-file set for secure communication.</td></tr><tr><td>242433</td><td>Kubernetes etcd must have a peer-key-file set for secure communication.</td></tr><tr><td>242438</td><td>Kubernetes API Server must configure timeouts to limit attack surface.</td></tr><tr><td>242443</td><td>Kubernetes must contain the latest updates as authorized by IAVMs, CTOs, DTMs, and STIGs.</td></tr><tr><td>242444</td><td>The Kubernetes component manifests must be owned by root.</td></tr><tr><td>242445</td><td>The Kubernetes component etcd must be owned by etcd.</td></tr><tr><td>242446</td><td>The Kubernetes conf files must be owned by root.</td></tr><tr><td>242447</td><td>The Kubernetes Kube Proxy must have file permissions set to 644 or more restrictive.</td></tr><tr><td>242448</td><td>The Kubernetes Kube Proxy must be owned by root.</td></tr><tr><td>242449</td><td>The Kubernetes Kubelet certificate authority file must have file permissions set to 644 or more restrictive.</td></tr><tr><td>242450</td><td>The Kubernetes Kubelet certificate authority must be owned by root.</td></tr><tr><td>242451</td><td>The Kubernetes component PKI must be owned by root.</td></tr><tr><td>242452</td><td>The Kubernetes kubelet KubeConfig must have file permissions set to 644 or more restrictive.</td></tr><tr><td>242453</td><td>The Kubernetes kubelet KubeConfig file must be owned by root.</td></tr><tr><td>242454</td><td>The Kubernetes kubeadm.conf must be owned by root.</td></tr><tr><td>242455</td><td>The Kubernetes kubeadm.conf must have file permissions set to 644 or more restrictive.</td></tr><tr><td>242456</td><td>The Kubernetes kubelet config must have file permissions set to 644 or more restrictive.</td></tr><tr><td>242457</td><td>The Kubernetes kubelet config must be owned by root.</td></tr><tr><td>242459</td><td>The Kubernetes etcd must have file permissions set to 644 or more restrictive.</td></tr><tr><td>242460</td><td>The Kubernetes admin.conf must have file permissions set to 644 or more restrictive.</td></tr><tr><td>242466</td><td>The Kubernetes PKI CRT must have file permissions set to 644 or more restrictive.</td></tr><tr><td>242467</td><td>The Kubernetes PKI keys must have file permissions set to 600 or more restrictive.</td></tr><tr><td>245542</td><td>Kubernetes API Server must disable basic authentication to protect information in transit.</td></tr><tr><td>245544</td><td>Kubernetes endpoints must use approved organizational certificate and key pair to protect information in transit.</td></tr><tr><td>254801</td><td>Kubernetes must enable PodSecurity admission controller on static pods and Kubelets.</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-0da6797c13c65c971a228abbeae11faf>3 - High Availability</h1></div><div class=td-content><h1 id=pg-6647773f8a1c094335ba88f765cd9f11>3.1 - Best Practices</h1><h1 id=implementing-high-availability-and-tolerating-zone-outages>Implementing High Availability and Tolerating Zone Outages</h1><p>Developing highly available workload that can tolerate a zone outage is no trivial task. You will find here various recommendations to get closer to that goal. While many recommendations are general enough, the examples are specific in how to achieve this in a Gardener-managed cluster and where/how to tweak the different control plane components. If you do not use Gardener, it may be still a worthwhile read.</p><p>First however, what is a zone outage? It sounds like a clear-cut &ldquo;thing&rdquo;, but it isn&rsquo;t. There are many things that can go haywire. Here are some examples:</p><ul><li>Elevated cloud provider API error rates for individual or multiple services</li><li>Network bandwidth reduced or latency increased, usually also effecting storage sub systems as they are network attached</li><li>No networking at all, no DNS, machines shutting down or restarting, &mldr;</li><li>Functional issues, of either the entire service (e.g. all block device operations) or only parts of it (e.g. LB listener registration)</li><li>All services down, temporarily or permanently (the proverbial burning down data center 🔥)</li></ul><p>This and everything in between make it hard to prepare for such events, but you can still do a lot. The most important recommendation is to not target specific issues exclusively - tomorrow another service will fail in an unanticipated way. Also, focus more on <a href=https://research.google/pubs/pub50828>meaningful availability</a> than on internal signals (useful, but not as relevant as the former). Always prefer automation over manual intervention (e.g. leader election is a pretty robust mechanism, auto-scaling may be required as well, etc.).</p><p>Also remember that HA is costly - you need to balance it against the cost of an outage as silly as this may sound, e.g. running all this excess capacity &ldquo;just in case&rdquo; vs. &ldquo;going down&rdquo; vs. a risk-based approach in between where you have means that will kick in, but they are not guaranteed to work (e.g. if the cloud provider is out of resource capacity). Maybe some of your components must run at the highest possible availability level, but others not - that&rsquo;s a decision only you can make.</p><h2 id=control-plane>Control Plane</h2><p>The Kubernetes cluster control plane is managed by Gardener (as pods in separate infrastructure clusters to which you have no direct access) and can be set up with no failure tolerance (control plane pods will be recreated best-effort when resources are available) or one of the <a href=/docs/guides/high-availability/control-plane/>failure tolerance types <code>node</code> or <code>zone</code></a>.</p><p>Strictly speaking, static workload does not depend on the (high) availability of the control plane, but static workload doesn&rsquo;t rhyme with Cloud and Kubernetes and also means, that when you possibly need it the most, e.g. during a zone outage, critical self-healing or auto-scaling functionality won&rsquo;t be available to you and your workload, if your control plane is down as well. That&rsquo;s why, even though the resource consumption is significantly higher, we generally recommend to use the failure tolerance type <code>zone</code> for the control planes of productive clusters, at least in all regions that have 3+ zones. Regions that have only 1 or 2 zones don&rsquo;t support the failure tolerance type <code>zone</code> and then your second best option is the failure tolerance type <code>node</code>, which means a zone outage can still take down your control plane, but individual node outages won&rsquo;t.</p><p>In the <code>shoot</code> resource it&rsquo;s merely only this what you need to add:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  controlPlane:
</span></span><span style=display:flex><span>    highAvailability:
</span></span><span style=display:flex><span>      failureTolerance:
</span></span><span style=display:flex><span>        type: zone <span style=color:green># valid values are `node` and `zone` (only available if your control plane resides in a region with 3+ zones)</span>
</span></span></code></pre></div><p>This setting will scale out all control plane components for a Gardener cluster as necessary, so that no single zone outage can take down the control plane for longer than just a few seconds for the fail-over to take place (e.g. lease expiration and new leader election or readiness probe failure and endpoint removal). Components run highly available in either active-active (servers) or active-passive (controllers) mode at all times, the persistence (ETCD), which is consensus-based, will tolerate the loss of one zone and still maintain quorum and therefore remain operational. These are all patterns that we will revisit down below also for your own workload.</p><h2 id=worker-pools>Worker Pools</h2><p>Now that you have configured your Kubernetes cluster control plane in HA, i.e. spread it across multiple zones, you need to do the same for your own workload, but in order to do so, you need to spread your nodes across multiple zones first.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: ...
</span></span><span style=display:flex><span>      minimum: 6
</span></span><span style=display:flex><span>      maximum: 60
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - ...
</span></span></code></pre></div><p>Prefer regions with at least 2, better 3+ zones and list the zones in the <code>zones</code> section for each of your worker pools. Whether you need 2 or 3 zones at a minimum depends on your fail-over concept:</p><ul><li>Consensus-based software components (like ETCD) depend on maintaining a quorum of <code>(n/2)+1</code>, so you need at least 3 zones to tolerate the outage of 1 zone.</li><li>Primary/Secondary-based software components need just 2 zones to tolerate the outage of 1 zone.</li><li>Then there are software components that can scale out horizontally. They are probably fine with 2 zones, but you also need to think about the load-shift and that the remaining zone must then pick up the work of the unhealthy zone. With 2 zones, the remaining zone must cope with an increase of 100% load. With 3 zones, the remaining zones must only cope with an increase of 50% load (per zone).</li></ul><p>In general, the question is also whether you have the fail-over capacity already up and running or not. If not, i.e. you depend on re-scheduling to a healthy zone or auto-scaling, be aware that during a zone outage, you will see a resource crunch in the healthy zones. If you have no automation, i.e. only human operators (a.k.a. &ldquo;red button approach&rdquo;), you probably will not get the machines you need and even with automation, it may be tricky. But holding the capacity available at all times is costly. In the end, that&rsquo;s a decision only you can make. If you made that decision, please adapt the <code>minimum</code>, <code>maximum</code>, <code>maxSurge</code> and <code>maxUnavailable</code> settings for your worker pools accordingly (visit <a href=/docs/guides/high-availability/best-practices/#on-specproviderworkersminimum-maximum-maxsurge-maxunavailable-zones-and-machinecontrollermanager>this section</a> for more information).</p><p>Also, consider fall-back worker pools (with different/alternative machine types) and <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders>cluster autoscaler expanders</a> using a <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/expander/priority/readme.md>priority-based strategy</a>.</p><p>Gardener-managed clusters deploy the <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>cluster autoscaler</a> or CA for short and you can <a href=https://gardener.cloud/docs/gardener/api-reference/core/#clusterautoscaler>tweak the general CA knobs</a> for Gardener-managed clusters like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    clusterAutoscaler:
</span></span><span style=display:flex><span>      expander: <span style=color:#a31515>&#34;least-waste&#34;</span>
</span></span><span style=display:flex><span>      scanInterval: 10s
</span></span><span style=display:flex><span>      scaleDownDelayAfterAdd: 60m
</span></span><span style=display:flex><span>      scaleDownDelayAfterDelete: 0s
</span></span><span style=display:flex><span>      scaleDownDelayAfterFailure: 3m
</span></span><span style=display:flex><span>      scaleDownUnneededTime: 30m
</span></span><span style=display:flex><span>      scaleDownUtilizationThreshold: 0.5
</span></span></code></pre></div><p>If you want to be ready for a sudden spike or have some buffer in general, <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-can-i-configure-overprovisioning-with-cluster-autoscaler>over-provision nodes by means of &ldquo;placeholder&rdquo; pods</a> with <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption>low priority</a> and appropriate resource requests. This way, they will demand nodes to be provisioned for them, but if any pod comes up with a regular/higher priority, the low priority pods will be evicted to make space for the more important ones. Strictly speaking, this is not related to HA, but it may be important to keep this in mind as you generally want critical components to be rescheduled as fast as possible and if there is no node available, it may take 3 minutes or longer to do so (depending on the cloud provider). Besides, not only zones can fail, but also individual nodes.</p><h2 id=replicas-horizontal-scaling>Replicas (Horizontal Scaling)</h2><p>Now let&rsquo;s talk about your workload. In most cases, this will mean to run multiple replicas. If you cannot do that (a.k.a. you have a singleton), that&rsquo;s a bad situation to be in. Maybe you can run a spare (secondary) as backup? If you cannot, you depend on quick detection and rescheduling of your singleton (more on that below).</p><p>Obviously, things get messier with persistence. If you have persistence, you should ideally replicate your data, i.e. let your spare (secondary) &ldquo;follow&rdquo; your main (primary). If your software doesn&rsquo;t support that, you have to deploy other means, e.g. <a href=https://kubernetes.io/docs/concepts/storage/volume-snapshots>volume snapshotting</a> or side-backups (specific to the software you deploy; keep the backups regional, so that you can switch to another zone at all times). If you have to do those, your HA scenario becomes more a DR scenario and terms like RPO and RTO become relevant to you:</p><ul><li><strong>Recovery Point Objective (RPO)</strong>: Potential data loss, i.e. how much data will you lose at most (time between backups)</li><li><strong>Recovery Time Objective (RTO)</strong>: Time until recovery, i.e. how long does it take you to be operational again (time to restore)</li></ul><p>Also, keep in mind that your persistent volumes are usually zonal, i.e. once you have a volume in one zone, it&rsquo;s bound to that zone and you cannot get up your pod in another zone w/o first recreating the volume yourself (Kubernetes won&rsquo;t help you here directly).</p><p>Anyway, best avoid that, if you can (from technical and cost perspective). The best solution (and also the most costly one) is to run multiple replicas in multiple zones and keep your data replicated at all times, so that your RPO is always 0 (best). That&rsquo;s what we do for Gardener-managed cluster HA control planes (ETCD) as any data loss may be disastrous and lead to orphaned resources (in addition, we deploy side cars that do side-backups for disaster recovery, with full and incremental snapshots with an RPO of 5m).</p><p>So, how to run with multiple replicas? That&rsquo;s the easiest part in Kubernetes and the two most important resources, <code>Deployments</code> and <code>StatefulSet</code>, support that out of the box:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment | StatefulSet
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: ...
</span></span></code></pre></div><p>The problem comes with the number of replicas. It&rsquo;s easy only if the number is static, e.g. 2 for active-active/passive or 3 for consensus-based software components, but what with software components that can scale out horizontally? Here you usually do not set the number of replicas statically, but make use of the <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale>horizontal pod autoscaler</a> or HPA for short (built-in; part of the kube-controller-manager). There are also other options like the <a href=https://github.com/kubernetes-sigs/cluster-proportional-autoscaler>cluster proportional autoscaler</a>, but while the former works based on metrics, the latter is more a guestimate approach that derives the number of replicas from the number of nodes/cores in a cluster. Sometimes useful, but often blind to the actual demand.</p><p>So, HPA it is then for most of the cases. However, what is the resource (e.g. CPU or memory) that drives the number of desired replicas? Again, this is up to you, but not always are CPU or memory the best choices. In some cases, <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#scaling-on-custom-metrics>custom metrics</a> may be more appropriate, e.g. requests per second (it was also for us).</p><p>You will have to create specific <code>HorizontalPodAutoscaler</code> resources for your scale target and can <a href=https://gardener.cloud/docs/gardener/api-reference/core/#horizontalpodautoscalerconfig>tweak the general HPA knobs</a> for Gardener-managed clusters like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeControllerManager:
</span></span><span style=display:flex><span>      horizontalPodAutoscaler:
</span></span><span style=display:flex><span>        syncPeriod: 15s
</span></span><span style=display:flex><span>        tolerance: 0.1
</span></span><span style=display:flex><span>        downscaleStabilization: 5m0s
</span></span><span style=display:flex><span>        initialReadinessDelay: 30s
</span></span><span style=display:flex><span>        cpuInitializationPeriod: 5m0s
</span></span></code></pre></div><h2 id=resources-vertical-scaling>Resources (Vertical Scaling)</h2><p>While it is important to set a sufficient number of replicas, it is also important to give the pods sufficient resources (CPU and memory). This is especially true when you think about HA. When a zone goes down, you might need to get up replacement pods, if you don&rsquo;t have them running already to take over the load from the impacted zone. Likewise, e.g. with active-active software components, you can expect the remaining pods to receive more load. If you cannot scale them out horizontally to serve the load, you will probably need to scale them out (or rather up) vertically. This is done by the <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>vertical pod autoscaler</a> or VPA for short (not built-in; part of the <a href=https://github.com/kubernetes/autoscaler>kubernetes/autoscaler</a> repository).</p><p>A few caveats though:</p><ul><li>You cannot use HPA and VPA on the same metrics as they would influence each other, which would lead to pod trashing (more replicas require fewer resources; fewer resources require more replicas)</li><li>Scaling horizontally doesn&rsquo;t cause downtimes (at least not when out-scaling and only one replica is affected when in-scaling), but scaling vertically does (if the pod runs OOM anyway, but also when new recommendations are applied, resource requests for existing pods may be changed, which causes the pods to be rescheduled). Although the discussion is going on for a very long time now, that is still not supported in-place yet (see <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/1287-in-place-update-pod-resources/README.md>KEP 1287</a>, <a href=https://github.com/kubernetes/kubernetes/pull/102884>implementation in Kubernetes</a>, <a href=https://github.com/kubernetes/autoscaler/issues/4016>implementation in VPA</a>).</li></ul><p>VPA is a useful tool and Gardener-managed clusters deploy a VPA by default for you (HPA is supported anyway as it&rsquo;s built into the kube-controller-manager). You will have to create specific <code>VerticalPodAutoscaler</code> resources for your scale target and can <a href=https://gardener.cloud/docs/gardener/api-reference/core/#verticalpodautoscaler>tweak the general VPA knobs</a> for Gardener-managed clusters like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    verticalPodAutoscaler:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      evictAfterOOMThreshold: 10m0s
</span></span><span style=display:flex><span>      evictionRateBurst: 1
</span></span><span style=display:flex><span>      evictionRateLimit: -1
</span></span><span style=display:flex><span>      evictionTolerance: 0.5
</span></span><span style=display:flex><span>      recommendationMarginFraction: 0.15
</span></span><span style=display:flex><span>      updaterInterval: 1m0s
</span></span><span style=display:flex><span>      recommenderInterval: 1m0s
</span></span></code></pre></div><p>While horizontal pod autoscaling is relatively straight-forward, it takes a long time to master vertical pod autoscaling. We saw <a href=https://github.com/kubernetes/autoscaler/issues/4498>performance issues</a>, hard-coded behavior (on OOM, memory is bumped by +20% and it may take a few iterations to reach a good level), unintended pod disruptions by applying new resource requests (after 12h all targeted pods will receive new requests even though individually they would be fine without, which also drives active-passive resource consumption up), difficulties to deal with spiky workload in general (due to the algorithmic approach it takes), recommended requests may exceed node capacity, limit scaling is proportional and therefore often questionable, and more. VPA is a double-edged sword: useful and necessary, but not easy to handle.</p><p>For the Gardener-managed components, we mostly removed limits. Why?</p><ul><li>CPU limits have almost always only downsides. They cause needless CPU throttling, which is not even easily visible. CPU requests turn into <code>cpu shares</code>, so if the node has capacity, the pod may consume the freely available CPU, but not if you have set limits, which curtail the pod by means of <code>cpu quota</code>. There are only certain scenarios in which they may make sense, e.g. if you set requests=limits and thereby define a pod with <code>guaranteed</code> <a href=https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod>QoS</a>, which influences your <code>cgroup</code> placement. However, that is difficult to do for the components you implement yourself and practically impossible for the components you just consume, because what&rsquo;s the correct value for requests/limits and will it hold true also if the load increases and what happens if a zone goes down or with the next update/version of this component? If anything, CPU limits caused outages, not helped prevent them.</li><li>As for memory limits, they are slightly more useful, because CPU is compressible and memory is not, so if one pod runs berserk, it may take others down (with CPU, <code>cpu shares</code> make it as fair as possible), depending on which OOM killer strikes (a complicated topic by itself). You don&rsquo;t want the operating system OOM killer to strike as the result is unpredictable. Better, it&rsquo;s the cgroup OOM killer or even the <code>kubelet</code>&rsquo;s eviction, if the consumption is slow enough as <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#interactions-of-pod-priority-and-qos>it takes priorities into consideration</a> even. If your component is critical and a singleton (e.g. node daemon set pods), you are better off also without memory limits, because letting the pod go OOM because of artificial/wrong memory limits can mean that the node becomes unusable. Hence, such components also better run only with no or a very high memory limit, so that you can catch the occasional memory leak (bug) eventually, but under normal operation, if you cannot decide about a true upper limit, rather not have limits and cause endless outages through them or when you need the pods the most (during a zone outage) where all your assumptions went out of the window.</li></ul><p>The downside of having poor or no limits and poor and no requests is that nodes may &ldquo;die&rdquo; more often. Contrary to the expectation, even for managed services, the managed service is not responsible or cannot guarantee the health of a node under all circumstances, since the end user defines what is run on the nodes (shared responsibility). If the workload exhausts any resource, it will be the end of the node, e.g. by compressing the CPU too much (so that the <code>kubelet</code> fails to do its work), exhausting the main memory too fast, disk space, file handles, or any other resource.</p><p>The <code>kubelet</code> allows for <a href=https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources>explicit reservation of resources</a> for operating system daemons (<code>system-reserved</code>) and Kubernetes daemons (<code>kube-reserved</code>) that are subtracted from the actual node resources and become the allocatable node resources for your workload/pods. All managed services configure these settings &ldquo;by rule of thumb&rdquo; (a balancing act), but cannot guarantee that the values won&rsquo;t waste resources or always will be sufficient. You will have to fine-tune them eventually and adapt them to your needs. In addition, you can configure <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction>soft and hard eviction thresholds</a> to give the <code>kubelet</code> some headroom to evict &ldquo;greedy&rdquo; pods in a controlled way. These settings can be configured for Gardener-managed clusters like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubelet:
</span></span><span style=display:flex><span>      systemReserved:                          <span style=color:green># explicit resource reservation for operating system daemons</span>
</span></span><span style=display:flex><span>        cpu: 100m
</span></span><span style=display:flex><span>        memory: 1Gi
</span></span><span style=display:flex><span>        ephemeralStorage: 1Gi
</span></span><span style=display:flex><span>        pid: 1000
</span></span><span style=display:flex><span>      kubeReserved:                            <span style=color:green># explicit resource reservation for Kubernetes daemons</span>
</span></span><span style=display:flex><span>        cpu: 100m
</span></span><span style=display:flex><span>        memory: 1Gi
</span></span><span style=display:flex><span>        ephemeralStorage: 1Gi
</span></span><span style=display:flex><span>        pid: 1000
</span></span><span style=display:flex><span>      evictionSoft:                            <span style=color:green># soft, i.e. graceful eviction (used if the node is about to run out of resources, avoiding hard evictions)</span>
</span></span><span style=display:flex><span>        memoryAvailable: 200Mi
</span></span><span style=display:flex><span>        imageFSAvailable: 10%
</span></span><span style=display:flex><span>        imageFSInodesFree: 10%
</span></span><span style=display:flex><span>        nodeFSAvailable: 10%
</span></span><span style=display:flex><span>        nodeFSInodesFree: 10%
</span></span><span style=display:flex><span>      evictionSoftGracePeriod:                 <span style=color:green># caps pod&#39;s `terminationGracePeriodSeconds` value during soft evictions (specific grace periods)</span>
</span></span><span style=display:flex><span>        memoryAvailable: 1m30s
</span></span><span style=display:flex><span>        imageFSAvailable: 1m30s
</span></span><span style=display:flex><span>        imageFSInodesFree: 1m30s
</span></span><span style=display:flex><span>        nodeFSAvailable: 1m30s
</span></span><span style=display:flex><span>        nodeFSInodesFree: 1m30s
</span></span><span style=display:flex><span>      evictionHard:                            <span style=color:green># hard, i.e. immediate eviction (used if the node is out of resources, avoiding the OS generally run out of resources fail processes indiscriminately)</span>
</span></span><span style=display:flex><span>        memoryAvailable: 100Mi
</span></span><span style=display:flex><span>        imageFSAvailable: 5%
</span></span><span style=display:flex><span>        imageFSInodesFree: 5%
</span></span><span style=display:flex><span>        nodeFSAvailable: 5%
</span></span><span style=display:flex><span>        nodeFSInodesFree: 5%
</span></span><span style=display:flex><span>      evictionMinimumReclaim:                  <span style=color:green># additional resources to reclaim after hitting the hard eviction thresholds to not hit the same thresholds soon after again</span>
</span></span><span style=display:flex><span>        memoryAvailable: 0Mi
</span></span><span style=display:flex><span>        imageFSAvailable: 0Mi
</span></span><span style=display:flex><span>        imageFSInodesFree: 0Mi
</span></span><span style=display:flex><span>        nodeFSAvailable: 0Mi
</span></span><span style=display:flex><span>        nodeFSInodesFree: 0Mi
</span></span><span style=display:flex><span>      evictionMaxPodGracePeriod: 90            <span style=color:green># caps pod&#39;s `terminationGracePeriodSeconds` value during soft evictions (general grace periods)</span>
</span></span><span style=display:flex><span>      evictionPressureTransitionPeriod: 5m0s   <span style=color:green># stabilization time window to avoid flapping of node eviction state</span>
</span></span></code></pre></div><p>You can tweak these settings also individually per worker pool (<code>spec.provider.workers.kubernetes.kubelet...</code>), which makes sense especially with different machine types (and also workload that you may want to schedule there).</p><p>Physical memory is not compressible, but you can overcome this issue to some degree (alpha since Kubernetes <code>v1.22</code> in combination with the feature gate <code>NodeSwap</code> on the <code>kubelet</code>) with swap memory. You can read more in this <a href=https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap-alpha>introductory blog</a> and the <a href=https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory>docs</a>. If you chose to use it (still only alpha at the time of this writing) you may want to consider also the risks associated with swap memory:</p><ul><li>Reduced performance predictability</li><li>Reduced performance up to page trashing</li><li>Reduced security as secrets, normally held only in memory, could be swapped out to disk</li></ul><p>That said, the various options mentioned above are only remotely related to HA and will not be further explored throughout this document, but just to remind you: if a zone goes down, load patterns will shift, existing pods will probably receive more load and will require more resources (especially because it is often practically impossible to set &ldquo;proper&rdquo; resource requests, which drive node allocation - limits are always ignored by the scheduler) or more pods will/must be placed on the existing and/or new nodes and then these settings, which are generally critical (especially if you switch on <a href=/docs/gardener/shoot_scheduling_profiles/>bin-packing for Gardener-managed clusters</a> as a cost saving measure), will become even more critical during a zone outage.</p><h2 id=probes>Probes</h2><p>Before we go down the rabbit hole even further and talk about how to spread your replicas, we need to talk about probes first, as they will become relevant later. Kubernetes supports three kinds of probes: <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes>startup, liveness, and readiness probes</a>. If you are a <a href=https://twitter.com/thockin/status/1615468485987143682>visual thinker</a>, also check out this <a href=https://speakerdeck.com/thockin/kubernetes-pod-probes>slide deck</a> by <a href=https://www.linkedin.com/in/tim-hockin-6501072>Tim Hockin</a> (Kubernetes networking SIG chair).</p><p>Basically, the <code>startupProbe</code> and the <code>livenessProbe</code> help you restart the container, if it&rsquo;s unhealthy for whatever reason, by letting the <code>kubelet</code> that orchestrates your containers on a node know, that it&rsquo;s unhealthy. The former is a special case of the latter and only applied at the startup of your container, if you need to handle the startup phase differently (e.g. with very slow starting containers) from the rest of the lifetime of the container.</p><p>Now, the <code>readinessProbe</code> helps you manage the ready status of your container and thereby pod (any container that is not ready turns the pod not ready). This again has impact on endpoints and pod disruption budgets:</p><ul><li>If the pod is not ready, the endpoint will be removed and the pod will not receive traffic anymore</li><li>If the pod is not ready, the pod counts into the pod disruption budget and if the budget is exceeded, no further voluntary pod disruptions will be permitted for the remaining ready pods (e.g. no eviction, no voluntary horizontal or vertical scaling, if the pod runs on a node that is about to be drained or in draining, draining will be paused until the max drain timeout passes)</li></ul><p>As you can see, all of these probes are (also) related to HA (mostly the <code>readinessProbe</code>, but depending on your workload, you can also leverage <code>livenessProbe</code> and <code>startupProbe</code> into your HA strategy). If Kubernetes doesn&rsquo;t know about the individual status of your container/pod, it won&rsquo;t do anything for you (right away). That said, later/indirectly something might/will happen via the node status that can also be ready or not ready, which influences the pods and load balancer listener registration (a not ready node will not receive cluster traffic anymore), but this process is worker pool global and reacts delayed and also doesn&rsquo;t discriminate between the containers/pods on a node.</p><p>In addition, Kubernetes also offers <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate>pod readiness gates</a> to amend your pod readiness with additional custom conditions (normally, only the sum of the container readiness matters, but pod readiness gates additionally count into the overall pod readiness). This may be useful if you want to block (by means of pod disruption budgets that we will talk about next) the roll-out of your workload/nodes in case some (possibly external) condition fails.</p><h2 id=pod-disruption-budgets>Pod Disruption Budgets</h2><p>One of the most important resources that help you on your way to HA are <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb>pod disruption budgets</a> or PDB for short. They tell Kubernetes how to deal with voluntary pod disruptions, e.g. during the deployment of your workload, when the nodes are rolled, or just in general when a pod shall be evicted/terminated. Basically, if the budget is reached, they block all voluntary pod disruptions (at least for a while until possibly other timeouts act or things happen that leave Kubernetes no choice anymore, e.g. the node is forcefully terminated). You should always define them for your workload.</p><p>Very important to note is that they are based on the <code>readinessProbe</code>, i.e. even if all of your replicas are <code>lively</code>, but not enough of them are <code>ready</code>, this blocks voluntary pod disruptions, so they are very critical and useful. Here an example (you can specify either <code>minAvailable</code> or <code>maxUnavailable</code> in absolute numbers or as percentage):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: policy/v1
</span></span><span style=display:flex><span>kind: PodDisruptionBudget
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  maxUnavailable: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      ...
</span></span></code></pre></div><p>And please do not specify a PDB of <code>maxUnavailable</code> being 0 or similar. That&rsquo;s pointless, even detrimental, as it blocks then even useful operations, forces always the hard timeouts that are less graceful and it doesn&rsquo;t make sense in the context of HA. You cannot &ldquo;force&rdquo; HA by preventing voluntary pod disruptions, you must work with the pod disruptions in a resilient way. Besides, PDBs are really only about voluntary pod disruptions - something bad can happen to a node/pod at any time and PDBs won&rsquo;t make this reality go away for you.</p><p>PDBs will not always work as expected and can also get in your way, e.g. if the PDB is violated or would be violated, it may possibly block whatever you are trying to do to salvage the situation, e.g. drain a node or deploy a patch version (if the PDB is or would be violated, not even unhealthy pods would be evicted as they could theoretically become healthy again, which Kubernetes doesn&rsquo;t know). In order to overcome this issue, it is now possible (alpha since Kubernetes <code>v1.26</code> in combination with the feature gate <code>PDBUnhealthyPodEvictionPolicy</code> on the API server) to configure the so-called <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy>unhealthy pod eviction policy</a>. The default is still <code>IfHealthyBudget</code> as a change in default would have changed the behavior (as described above), but you can now also set <code>AlwaysAllow</code> at the PDB (<code>spec.unhealthyPodEvictionPolicy</code>). For more information, please check out <a href=https://github.com/kubernetes/kubernetes/issues/72320>this discussion</a>, <a href=https://github.com/kubernetes/kubernetes/pull/105296>the PR</a> and <a href="https://groups.google.com/g/kubernetes-sig-apps/c/_joO4swogKY?pli=1">this document</a> and balance the pros and cons for yourself. In short,
the new <code>AlwaysAllow</code> option is probably the better choice in most of the cases while <code>IfHealthyBudget</code> is useful only if you have frequent temporary transitions or for special cases where you have already implemented controllers that depend on the old behavior.</p><h2 id=pod-topology-spread-constraints>Pod Topology Spread Constraints</h2><p><a href=https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints>Pod topology spread constraints</a> or PTSC for short (no official abbreviation exists, but we will use this in the following) are enormously helpful to distribute your replicas across multiple zones, nodes, or any other user-defined topology domain. They complement and improve on <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>pod (anti-)affinities</a> that still exist and can be used in combination.</p><p>PTSCs are an improvement, because they allow for <code>maxSkew</code> and <code>minDomains</code>. You can steer the &ldquo;level of tolerated imbalance&rdquo; with <code>maxSkew</code>, e.g. you probably want that to be at least 1, so that you can perform a rolling update, but this all depends on your <a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment>deployment</a> (<code>maxUnavailable</code> and <code>maxSurge</code>), etc. <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#rolling-updates>Stateful sets</a> are a bit different (<code>maxUnavailable</code>) as they are bound to volumes and depend on them, so there usually cannot be 2 pods requiring the same volume. <code>minDomains</code> is a hint to tell the scheduler how far to spread, e.g. if all nodes in one zone disappeared because of a zone outage, it may &ldquo;appear&rdquo; as if there are only 2 zones in a 3 zones cluster and the scheduling decisions may end up wrong, so a <code>minDomains</code> of 3 will tell the scheduler to spread to 3 zones before adding another replica in one zone. Be careful with this setting as it also means, if one zone is down the &ldquo;spread&rdquo; is already at least 1, if pods run in the other zones. This is useful where you have exactly as many replicas as you have zones and you do not want any imbalance. Imbalance is critical as if you end up with one, nobody is going to do the (active) re-balancing for you (unless you deploy and configure additional non-standard components such as the <a href=https://github.com/kubernetes-sigs/descheduler>descheduler</a>). So, for instance, if you have something like a DBMS that you want to spread across 2 zones (active-passive) or 3 zones (consensus-based), you better specify <code>minDomains</code> of 2 respectively 3 to force your replicas into at least that many zones before adding more replicas to another zone (if supported).</p><p>Anyway, PTSCs are critical to have, but not perfect, so we saw (unsurprisingly, because that&rsquo;s how the scheduler works), that the scheduler may block the deployment of new pods because it takes the decision pod-by-pod (see for instance <a href=https://github.com/kubernetes/kubernetes/issues/109364>#109364</a>).</p><h2 id=pod-affinities-and-anti-affinities>Pod Affinities and Anti-Affinities</h2><p>As said, you can combine PTSCs with <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>pod affinities and/or anti-affinities</a>. Especially <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>inter-pod (anti-)affinities</a> may be helpful to place pods <em>apart</em>, e.g. because they are fall-backs for each other or you do not want multiple potentially resource-hungry <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#besteffort>&ldquo;best-effort&rdquo;</a> or <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#burstable>&ldquo;burstable&rdquo;</a> pods side-by-side (noisy neighbor problem), or <em>together</em>, e.g. because they form a unit and you want to reduce the failure domain, reduce the network latency, and reduce the costs.</p><h2 id=topology-aware-hints>Topology Aware Hints</h2><p>While <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints>topology aware hints</a> are not directly related to HA, they are very relevant in the HA context. Spreading your workload across multiple zones may increase network latency and cost significantly, if the traffic is not shaped. Topology aware hints (beta since Kubernetes <code>v1.23</code>, replacing the now deprecated topology aware traffic routing with topology keys) help to route the traffic within the originating zone, if possible. Basically, they tell <code>kube-proxy</code> how to setup your routing information, so that clients can talk to endpoints that are located within the same zone.</p><p>Be aware however, that there are some limitations. Those are called <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/#safeguards>safeguards</a> and if they strike, the hints are off and traffic is routed again randomly. Especially controversial is the balancing limitation as there is the assumption, that the load that hits an endpoint is determined by the allocatable CPUs in that topology zone, but that&rsquo;s not always, if even often, the case (see for instance <a href=https://github.com/kubernetes/kubernetes/issues/113731>#113731</a> and <a href=https://github.com/kubernetes/kubernetes/issues/110714>#110714</a>). So, this limitation hits far too often and your hints are off, but then again, it&rsquo;s about network latency and cost optimization first, so it&rsquo;s better than nothing.</p><h2 id=networking>Networking</h2><p>We have talked about networking only to some small degree so far (<code>readiness</code> probes, pod disruption budgets, topology aware hints). The most important component is probably your ingress load balancer - everything else is managed by Kubernetes. AWS, Azure, GCP, and also OpenStack offer multi-zonal load balancers, so make use of them. In Azure and GCP, LBs are regional whereas in AWS and OpenStack, they need to be bound to a zone, which the cloud-controller-manager does by observing the zone labels at the nodes (please note that this behavior is not always working as expected, see <a href=https://github.com/kubernetes/cloud-provider-aws/issues/569>#570</a> where the AWS cloud-controller-manager is not readjusting to newly observed zones).</p><p>Please be reminded that even if you use a service mesh like <a href=https://istio.io>Istio</a>, the off-the-shelf installation/configuration usually never comes with productive settings (to simplify first-time installation and improve first-time user experience) and you will have to fine-tune your installation/configuration, much like the rest of your workload.</p><h2 id=relevant-cluster-settings>Relevant Cluster Settings</h2><p>Following now a summary/list of the more relevant settings you may like to tune for Gardener-managed clusters:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  controlPlane:
</span></span><span style=display:flex><span>    highAvailability:
</span></span><span style=display:flex><span>      failureTolerance:
</span></span><span style=display:flex><span>        type: zone <span style=color:green># valid values are `node` and `zone` (only available if your control plane resides in a region with 3+ zones)</span>
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      defaultNotReadyTolerationSeconds: 300
</span></span><span style=display:flex><span>      defaultUnreachableTolerationSeconds: 300
</span></span><span style=display:flex><span>    kubelet:
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>    kubeScheduler:
</span></span><span style=display:flex><span>      featureGates:
</span></span><span style=display:flex><span>        MinDomainsInPodTopologySpread: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    kubeControllerManager:
</span></span><span style=display:flex><span>      nodeMonitorGracePeriod: 40s
</span></span><span style=display:flex><span>      horizontalPodAutoscaler:
</span></span><span style=display:flex><span>        syncPeriod: 15s
</span></span><span style=display:flex><span>        tolerance: 0.1
</span></span><span style=display:flex><span>        downscaleStabilization: 5m0s
</span></span><span style=display:flex><span>        initialReadinessDelay: 30s
</span></span><span style=display:flex><span>        cpuInitializationPeriod: 5m0s
</span></span><span style=display:flex><span>    verticalPodAutoscaler:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      evictAfterOOMThreshold: 10m0s
</span></span><span style=display:flex><span>      evictionRateBurst: 1
</span></span><span style=display:flex><span>      evictionRateLimit: -1
</span></span><span style=display:flex><span>      evictionTolerance: 0.5
</span></span><span style=display:flex><span>      recommendationMarginFraction: 0.15
</span></span><span style=display:flex><span>      updaterInterval: 1m0s
</span></span><span style=display:flex><span>      recommenderInterval: 1m0s
</span></span><span style=display:flex><span>    clusterAutoscaler:
</span></span><span style=display:flex><span>      expander: <span style=color:#a31515>&#34;least-waste&#34;</span>
</span></span><span style=display:flex><span>      scanInterval: 10s
</span></span><span style=display:flex><span>      scaleDownDelayAfterAdd: 60m
</span></span><span style=display:flex><span>      scaleDownDelayAfterDelete: 0s
</span></span><span style=display:flex><span>      scaleDownDelayAfterFailure: 3m
</span></span><span style=display:flex><span>      scaleDownUnneededTime: 30m
</span></span><span style=display:flex><span>      scaleDownUtilizationThreshold: 0.5
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: ...
</span></span><span style=display:flex><span>      minimum: 6
</span></span><span style=display:flex><span>      maximum: 60
</span></span><span style=display:flex><span>      maxSurge: 3
</span></span><span style=display:flex><span>      maxUnavailable: 0
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - ... <span style=color:green># list of zones you want your worker pool nodes to be spread across, see above</span>
</span></span><span style=display:flex><span>      kubernetes:
</span></span><span style=display:flex><span>        kubelet:
</span></span><span style=display:flex><span>          ... <span style=color:green># similar to `kubelet` above (cluster-wide settings), but here per worker pool (pool-specific settings), see above</span>
</span></span><span style=display:flex><span>      machineControllerManager: <span style=color:green># optional, it allows to configure the machine-controller settings.</span>
</span></span><span style=display:flex><span>        machineCreationTimeout: 20m
</span></span><span style=display:flex><span>        machineHealthTimeout: 10m
</span></span><span style=display:flex><span>        machineDrainTimeout: 60h
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    coreDNS:
</span></span><span style=display:flex><span>      autoscaling:
</span></span><span style=display:flex><span>        mode: horizontal <span style=color:green># valid values are `horizontal` (driven by CPU load) and `cluster-proportional` (driven by number of nodes/cores)</span>
</span></span></code></pre></div><h4 id=on-speccontrolplanehighavailabilityfailuretolerancetype>On <code>spec.controlPlane.highAvailability.failureTolerance.type</code></h4><p>If set, determines the degree of failure tolerance for your control plane. <code>zone</code> is preferred, but only available if your control plane resides in a region with 3+ zones. See <a href=/docs/guides/high-availability/best-practices/#control-plane>above</a> and the <a href=/docs/guides/high-availability/control-plane/>docs</a>.</p><h4 id=on-speckuberneteskubeapiserverdefaultunreachabletolerationseconds-and-defaultnotreadytolerationseconds>On <code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds</code> and <code>defaultNotReadyTolerationSeconds</code></h4><p>This is a very interesting <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver>API server setting</a> that lets Kubernetes decide how fast to evict pods from nodes whose status condition of type <code>Ready</code> is either <code>Unknown</code> (node status unknown, a.k.a unreachable) or <code>False</code> (<code>kubelet</code> not ready) (see <a href=https://kubernetes.io/docs/concepts/architecture/nodes/#condition>node status conditions</a>; please note that <code>kubectl</code> shows both values as <code>NotReady</code> which is a somewhat &ldquo;simplified&rdquo; visualization).</p><p>You can also override the cluster-wide API server settings <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions>individually per pod</a>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tolerations:
</span></span><span style=display:flex><span>  - key: <span style=color:#a31515>&#34;node.kubernetes.io/unreachable&#34;</span>
</span></span><span style=display:flex><span>    operator: <span style=color:#a31515>&#34;Exists&#34;</span>
</span></span><span style=display:flex><span>    effect: <span style=color:#a31515>&#34;NoExecute&#34;</span>
</span></span><span style=display:flex><span>    tolerationSeconds: 0
</span></span><span style=display:flex><span>  - key: <span style=color:#a31515>&#34;node.kubernetes.io/not-ready&#34;</span>
</span></span><span style=display:flex><span>    operator: <span style=color:#a31515>&#34;Exists&#34;</span>
</span></span><span style=display:flex><span>    effect: <span style=color:#a31515>&#34;NoExecute&#34;</span>
</span></span><span style=display:flex><span>    tolerationSeconds: 0
</span></span></code></pre></div><p>This will evict pods on unreachable or not-ready nodes immediately, but be cautious: <code>0</code> is very aggressive and may lead to unnecessary disruptions. Again, you must decide for your own workload and balance out the pros and cons (e.g. long startup time).</p><p>Please note, these settings replace <code>spec.kubernetes.kubeControllerManager.podEvictionTimeout</code> that was deprecated with Kubernetes <code>v1.26</code> (and acted as an upper bound).</p><h4 id=on-speckuberneteskubeschedulerfeaturegatesmindomainsinpodtopologyspread>On <code>spec.kubernetes.kubeScheduler.featureGates.MinDomainsInPodTopologySpread</code></h4><p>Required to be enabled for <code>minDomains</code> to work with PTSCs (beta since Kubernetes <code>v1.25</code>, but off by default). See <a href=/docs/guides/high-availability/best-practices/#pod-topology-spread-constraints>above</a> and the <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#topologyspreadconstraints-field>docs</a>. This tells the scheduler, how many topology domains to expect (=zones in the context of this document).</p><h4 id=on-speckuberneteskubecontrollermanagernodemonitorgraceperiod>On <code>spec.kubernetes.kubeControllerManager.nodeMonitorGracePeriod</code></h4><p>This is another very interesting <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager>kube-controller-manager setting</a> that can help you speed up or slow down how fast a node shall be considered <code>Unknown</code> (node status unknown, a.k.a unreachable) when the <code>kubelet</code> is not updating its status anymore (see <a href=https://kubernetes.io/docs/concepts/architecture/nodes/#condition>node status conditions</a>), which effects eviction (see <code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds</code> and <code>defaultNotReadyTolerationSeconds</code> above). The shorter the time window, the faster Kubernetes will act, but the higher the chance of flapping behavior and pod trashing, so you may want to balance that out according to your needs, otherwise stick to the default which is a reasonable compromise.</p><h4 id=on-speckuberneteskubecontrollermanagerhorizontalpodautoscaler>On <code>spec.kubernetes.kubeControllerManager.horizontalPodAutoscaler...</code></h4><p>This configures horizontal pod autoscaling in Gardener-managed clusters. See <a href=/docs/guides/high-availability/best-practices/#replicas-horizontal-scaling>above</a> and the <a href=https://kubernetes.io/de/docs/tasks/run-application/horizontal-pod-autoscale>docs</a> for the detailed fields.</p><h4 id=on-speckubernetesverticalpodautoscaler>On <code>spec.kubernetes.verticalPodAutoscaler...</code></h4><p>This configures vertical pod autoscaling in Gardener-managed clusters. See <a href=/docs/guides/high-availability/best-practices/#resources-vertical-scaling>above</a> and the <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/FAQ.md>docs</a> for the detailed fields.</p><h4 id=on-speckubernetesclusterautoscaler>On <code>spec.kubernetes.clusterAutoscaler...</code></h4><p>This configures node auto-scaling in Gardener-managed clusters. See <a href=/docs/guides/high-availability/best-practices/#worker-pools>above</a> and the <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md>docs</a> for the detailed fields, especially about <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders>expanders</a>, which may become life-saving in case of a zone outage when a resource crunch is setting in and everybody rushes to get machines in the healthy zones.</p><p>In case of a zone outage, it is critical to understand how the cluster autoscaler will put a worker pool in one zone into &ldquo;back-off&rdquo; and what the consequences for your workload will be. Unfortunately, the official cluster autoscaler documentation does not explain these details, but you can find hints in the <a href=https://github.com/kubernetes/autoscaler/blob/b94f340af58eb063df9ebfcd65835f9a499a69a2/cluster-autoscaler/config/autoscaling_options.go#L214-L219>source code</a>:</p><p>If a node fails to come up, the node group (worker pool in that zone) will go into &ldquo;back-off&rdquo;, at first 5m, then <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/utils/backoff/exponential_backoff.go#L77-L82>exponentially longer</a> until the maximum of 30m is reached. The &ldquo;back-off&rdquo; is reset after 3 hours. This in turn means, that nodes must be first considered <code>Unknown</code>, which happens when <code>spec.kubernetes.kubeControllerManager.nodeMonitorGracePeriod</code> lapses (e.g. at the beginning of a zone outage). Then they must either remain in this state until <code>spec.provider.workers.machineControllerManager.machineHealthTimeout</code> lapses for them to be recreated, which will fail in the unhealthy zone, or <code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds</code> lapses for the pods to be evicted (usually faster than node replacements, depending on your configuration), which will trigger the cluster autoscaler to create more capacity, but very likely in the same zone as it tries to balance its node groups at first, which will fail in the unhealthy zone. It will be considered failed only when <code>maxNodeProvisionTime</code> lapses (usually close to <code>spec.provider.workers.machineControllerManager.machineCreationTimeout</code>) and only then put the node group into &ldquo;back-off&rdquo; and not retry for 5m (at first and then exponentially longer). Only then you can expect new node capacity to be brought up somewhere else.</p><p>During the time of ongoing node provisioning (before a node group goes into &ldquo;back-off&rdquo;), the cluster autoscaler may have &ldquo;virtually scheduled&rdquo; pending pods onto those new upcoming nodes and will not reevaluate these pods anymore unless the node provisioning fails (which will fail during a zone outage, but the cluster autoscaler cannot know that and will therefore reevaluate its decision only after it has given up on the new nodes).</p><p>It&rsquo;s critical to keep that in mind and accommodate for it. If you have already capacity up and running, the reaction time is usually much faster with leases (whatever you set) or endpoints (<code>spec.kubernetes.kubeControllerManager.nodeMonitorGracePeriod</code>), but if you depend on new/fresh capacity, the above should inform you how long you will have to wait for it and for how long pods might be pending (because capacity is generally missing and pending pods may have been &ldquo;virtually scheduled&rdquo; to new nodes that won&rsquo;t come up until the node group goes eventually into &ldquo;back-off&rdquo; and nodes in the healthy zones come up).</p><h4 id=on-specproviderworkersminimum-maximum-maxsurge-maxunavailable-zones-and-machinecontrollermanager>On <code>spec.provider.workers.minimum</code>, <code>maximum</code>, <code>maxSurge</code>, <code>maxUnavailable</code>, <code>zones</code>, and <code>machineControllerManager</code></h4><p>Each worker pool in Gardener may be configured differently. Among many other settings like machine type, root disk, Kubernetes version, <code>kubelet</code> settings, and many more you can also specify the lower and upper bound for the number of machines (<code>minimum</code> and <code>maximum</code>), how many machines may be added additionally during a rolling update (<code>maxSurge</code>) and how many machines may be in termination/recreation during a rolling update (<code>maxUnavailable</code>), and of course across how many zones the nodes shall be spread (<code>zones</code>).</p><p>Gardener divides <code>minimum</code>, <code>maximum</code>, <code>maxSurge</code>, <code>maxUnavailable</code> values by the number of zones specified for this worker pool. This fact must be considered when you plan the sizing of your worker pools.</p><p><em>Example:</em></p><pre tabindex=0><code>  provider:
    workers:
    - name: ...
      minimum: 6
      maximum: 60
      maxSurge: 3
      maxUnavailable: 0
      zones: [&#34;a&#34;, &#34;b&#34;, &#34;c&#34;]
</code></pre><ul><li>The resulting <code>MachineDeployment</code>s <strong>per zone</strong> will get <code>minimum: 2</code>, <code>maximum: 20</code>, <code>maxSurge: 1</code>, <code>maxUnavailable: 0</code>.</li><li>If another zone is added all values will be divided by <code>4</code>, resulting in:<ul><li>Less workers per zone.</li><li>⚠️ One <code>MachineDeployment</code> with <code>maxSurge: 0</code>, i.e. there will be a replacement of nodes without rolling updates.</li></ul></li></ul><p>Interesting is also the configuration for Gardener&rsquo;s machine-controller-manager or MCM for short that provisions, monitors, terminates, replaces, or updates machines that back your nodes:</p><ul><li>The shorter <code>machineCreationTimeout</code> is, the faster MCM will retry to create a machine/node, if the process is stuck on cloud provider side. It is set to useful/practical timeouts for the different cloud providers and you probably don&rsquo;t want to change those (in the context of HA at least). Please align with the cluster autoscaler&rsquo;s <code>maxNodeProvisionTime</code>.</li><li>The shorter <code>machineHealthTimeout</code> is, the faster MCM will replace machines/nodes in case the kubelet isn&rsquo;t reporting back, which translates to <code>Unknown</code>, or reports back with <code>NotReady</code>, or the <a href=https://github.com/kubernetes/node-problem-detector>node-problem-detector</a> that Gardener deploys for you reports a non-recoverable issue/condition (e.g. read-only file system). If it is too short however, you risk node and pod trashing, so be careful.</li><li>The shorter <code>machineDrainTimeout</code> is, the faster you can get rid of machines/nodes that MCM decided to remove, but this puts a cap on the grace periods and PDBs. They are respected up until the drain timeout lapses - then the machine/node will be forcefully terminated, whether or not the pods are still in termination or not even terminated because of PDBs. Those PDBs will then be violated, so be careful here as well. Please align with the cluster autoscaler&rsquo;s <code>maxGracefulTerminationSeconds</code>.</li></ul><p>Especially the last two settings may help you recover faster from cloud provider issues.</p><h4 id=on-specsystemcomponentscorednsautoscaling>On <code>spec.systemComponents.coreDNS.autoscaling</code></h4><p>DNS is critical, in general and also within a Kubernetes cluster. Gardener-managed clusters deploy <a href=https://coredns.io>CoreDNS</a>, a graduated CNCF project. Gardener supports 2 auto-scaling modes for it, <code>horizontal</code> (using HPA based on CPU) and <code>cluster-proportional</code> (using <a href=https://github.com/kubernetes-sigs/cluster-proportional-autoscaler>cluster proportional autoscaler</a> that scales the number of pods based on the number of nodes/cores, not to be confused with the cluster autoscaler that scales nodes based on their utilization). Check out the <a href=/docs/gardener/dns-autoscaling/>docs</a>, especially the <a href=/docs/gardener/dns-autoscaling/#trade-offs-of-horizontal-and-cluster-proportional-dns-autoscaling>trade-offs</a> why you would chose one over the other (<code>cluster-proportional</code> gives you more configuration options, if CPU-based horizontal scaling is insufficient to your needs). Consider also Gardener&rsquo;s feature <a href=/docs/gardener/node-local-dns/>node-local DNS</a> to decouple you further from the DNS pods and stabilize DNS. Again, that&rsquo;s not strictly related to HA, but may become important during a zone outage, when load patterns shift and pods start to initialize/resolve DNS records more frequently in bulk.</p><h2 id=more-caveats>More Caveats</h2><p>Unfortunately, there are a few more things of note when it comes to HA in a Kubernetes cluster that may be &ldquo;surprising&rdquo; and hard to mitigate:</p><ul><li>If the <code>kubelet</code> restarts, it will report all pods as <code>NotReady</code> on startup until it reruns its probes (<a href=https://github.com/kubernetes/kubernetes/issues/100277>#100277</a>), which leads to temporary endpoint and load balancer target removal (<a href=https://github.com/kubernetes/kubernetes/issues/102367>#102367</a>). This topic is somewhat controversial. Gardener uses rolling updates and a jitter to spread necessary <code>kubelet</code> restarts as good as possible.</li><li>If a <code>kube-proxy</code> pod on a node turns <code>NotReady</code>, all load balancer traffic to all pods (on this node) under services with <code>externalTrafficPolicy</code> <code>local</code> will cease as the load balancer will then take this node out of serving. This topic is somewhat controversial as well. So, please remember that <code>externalTrafficPolicy</code> <code>local</code> not only has the disadvantage of imbalanced traffic spreading, but also a dependency to the kube-proxy pod that may and will be unavailable during updates. Gardener uses rolling updates to spread necessary <code>kube-proxy</code> updates as good as possible.</li></ul><p>These are just a few additional considerations. They may or may not affect you, but other intricacies may. It&rsquo;s a reminder to be watchful as Kubernetes may have one or two relevant quirks that you need to consider (and will probably only find out over time and with extensive testing).</p><h2 id=meaningful-availability>Meaningful Availability</h2><p>Finally, let&rsquo;s go back to where we started. We recommended to measure <a href=https://research.google/pubs/pub50828>meaningful availability</a>. For instance, in Gardener, we do not trust only internal signals, but track also whether Gardener or the control planes that it manages are externally available through the external DNS records and load balancers, SNI-routing Istio gateways, etc. (the same path all users must take). It&rsquo;s a huge difference whether the API server&rsquo;s internal readiness probe passes or the user can actually reach the API server and it does what it&rsquo;s supposed to do. Most likely, you will be in a similar spot and can do the same.</p><p>What you do with these signals is another matter. Maybe there are some actionable metrics and you can trigger some active fail-over, maybe you can only use it to improve your HA setup altogether. In our case, we also use it to deploy mitigations, e.g. via our <a href=https://github.com/gardener/dependency-watchdog>dependency-watchdog</a> that watches, for instance, Gardener-managed API servers and shuts down components like the controller managers to avert cascading knock-off effects (e.g. melt-down if the <code>kubelets</code> cannot reach the API server, but the controller managers can and start taking down nodes and pods).</p><p>Either way, understanding how users perceive your service is key to the improvement process as a whole. Even if you are not struck by a zone outage, the measures above and tracking the meaningful availability will help you improve your service.</p><p>Thank you for your interest.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b5a6d1d059abdd75c072cd004ca974fb>3.2 - Chaos Engineering</h1><h2 id=overview>Overview</h2><p>Gardener provides <a href=https://chaostoolkit.org><code>chaostoolkit</code></a> modules to simulate <em>compute</em> and <em>network</em> outages for various cloud providers such as <a href=https://github.com/gardener/chaos-engineering/tree/main/docs/aws>AWS</a>, <a href=https://github.com/gardener/chaos-engineering/tree/main/docs/azure>Azure</a>, <a href=https://github.com/gardener/chaos-engineering/tree/main/docs/gcp>GCP</a>, <a href=https://github.com/gardener/chaos-engineering/tree/main/docs/openstack>OpenStack/Converged Cloud</a>, and <a href=https://github.com/gardener/chaos-engineering/tree/main/docs/vsphere>VMware vSphere</a>, as well as <em>pod disruptions</em> for <a href=https://github.com/gardener/chaos-engineering/tree/main/docs/k8s>any Kubernetes cluster</a>.</p><p>The API, parameterization, and implementation is as homogeneous as possible across the different cloud providers, so that you have only minimal effort. As a Gardener user, you benefit from an <a href=https://github.com/gardener/chaos-engineering/tree/main/docs/garden>additional <code>garden</code> module</a> that leverages the generic modules, but exposes their functionality in the most simple, homogeneous, and secure way (no need to specify cloud provider credentials, cluster credentials, or filters explicitly; retrieves credentials and stores them in memory only).</p><h2 id=installation>Installation</h2><p>The name of the package is <code>chaosgarden</code> and it was developed and tested with Python 3.9+. It&rsquo;s being published to <a href=https://pypi.org/project/chaosgarden>PyPI</a>, so that you can comfortably install it via Python&rsquo;s package installer <a href=https://pip.pypa.io/en/stable>pip</a> (you may want to <a href=https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/#creating-a-virtual-environment>create a virtual environment</a> before installing it):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>pip install chaosgarden
</span></span></code></pre></div><p>ℹ️ If you want to use the <a href=https://github.com/gardener/chaos-engineering/tree/main/docs/vsphere>VMware vSphere module</a>, please note the remarks in <a href=https://github.com/gardener/chaos-engineering/blob/main/requirements.txt><code>requirements.txt</code></a> for <code>vSphere</code>. Those are not contained in the published PyPI package.</p><p>The package can be used directly from Python scripts and supports this usage scenario with additional convenience that helps launch actions and probes in background (more on actions and probes later), so that you can compose also complex scenarios with ease.</p><p>If this technology is new to you, you will probably prefer the <a href=https://chaostoolkit.org><code>chaostoolkit</code></a> <a href=https://chaostoolkit.org/reference/usage/cli>CLI</a> in combination with <a href=https://chaostoolkit.org/reference/api/experiment>experiment files</a>, so we need to <a href=https://chaostoolkit.org/reference/usage/install/#install-the-cli>install the CLI</a> next:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>pip install chaostoolkit
</span></span></code></pre></div><p>Please verify that it was installed properly by running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>chaos --help
</span></span></code></pre></div><h2 id=usage>Usage</h2><p>ℹ️ We assume you are using Gardener and run Gardener-managed shoot clusters. You can also use the generic cloud provider and Kubernetes <code>chaosgarden</code> modules, but configuration and secrets will then differ. Please see the <a href=https://github.com/gardener/chaos-engineering/tree/main/docs>module docs</a> for details.</p><h3 id=a-simple-experiment>A Simple Experiment</h3><p>The most important command is the <a href=https://chaostoolkit.org/reference/usage/run><code>run</code></a> command, but before we can use it, we need to compile an experiment file first. Let&rsquo;s start with a simple one, invoking only a read-only 📖 action from <code>chaosgarden</code> that lists cloud provider machines and networks (depends on cloud provider) for the &ldquo;first&rdquo; zone of one of your shoot clusters.</p><p>Let&rsquo;s assume, your project is called <code>my-project</code> and your shoot is called <code>my-shoot</code>, then we need to create the following experiment:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    &#34;title&#34;: <span style=color:#a31515>&#34;assess-filters-impact&#34;</span>,
</span></span><span style=display:flex><span>    &#34;description&#34;: <span style=color:#a31515>&#34;assess-filters-impact&#34;</span>,
</span></span><span style=display:flex><span>    &#34;method&#34;: [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            &#34;type&#34;: <span style=color:#a31515>&#34;action&#34;</span>,
</span></span><span style=display:flex><span>            &#34;name&#34;: <span style=color:#a31515>&#34;assess-filters-impact&#34;</span>,
</span></span><span style=display:flex><span>            &#34;provider&#34;: {
</span></span><span style=display:flex><span>                &#34;type&#34;: <span style=color:#a31515>&#34;python&#34;</span>,
</span></span><span style=display:flex><span>                &#34;module&#34;: <span style=color:#a31515>&#34;chaosgarden.garden.actions&#34;</span>,
</span></span><span style=display:flex><span>                &#34;func&#34;: <span style=color:#a31515>&#34;assess_cloud_provider_filters_impact&#34;</span>,
</span></span><span style=display:flex><span>                &#34;arguments&#34;: {
</span></span><span style=display:flex><span>                    &#34;zone&#34;: 0
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    &#34;configuration&#34;: {
</span></span><span style=display:flex><span>        &#34;garden_project&#34;: <span style=color:#a31515>&#34;my-project&#34;</span>,
</span></span><span style=display:flex><span>        &#34;garden_shoot&#34;: <span style=color:#a31515>&#34;my-shoot&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>We are not yet there and need one more thing to do before we can run it: We need to &ldquo;target&rdquo; the Gardener landscape resp. Gardener API server where you have created your shoot cluster (not to be confused with your shoot cluster API server). If you do not know what this is or how to download the Gardener API server <code>kubeconfig</code>, please follow <a href=/docs/dashboard/project-operations/#prerequisites>these instructions</a>. You can either download your <em>personal</em> credentials or <em>project</em> credentials (see <a href=/docs/dashboard/automated-resource-management/#prerequisites>creation of a <code>serviceaccount</code></a>) to interact with Gardener. For now (fastest and most convenient way, but generally not recommended), let&rsquo;s use your <em>personal</em> credentials, but if you later plan to automate your experiments, please use proper <em>project</em> credentials (a <code>serviceaccount</code> is not bound to your person, but to the project, and can be restricted using <a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac>RBAC roles and role bindings</a>, which is why we recommend this for production).</p><p>To download your <em>personal</em> credentials, open the Gardener Dashboard and click on your avatar in the upper right corner of the page. Click &ldquo;My Account&rdquo;, then look for the &ldquo;Access&rdquo; pane, then &ldquo;Kubeconfig&rdquo;, then press the &ldquo;Download&rdquo; button and save the <code>kubeconfig</code> to disk. Run the following command next:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>export KUBECONFIG=path/to/kubeconfig
</span></span></code></pre></div><p>We are now set and you can run your first experiment:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>chaos run path/to/experiment
</span></span></code></pre></div><p>You should see output like this (depends on cloud provider):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-txt data-lang=txt><span style=display:flex><span>[INFO] Validating the experiment&#39;s syntax
</span></span><span style=display:flex><span>[INFO] Installing signal handlers to terminate all active background threads on involuntary signals (note that SIGKILL cannot be handled).
</span></span><span style=display:flex><span>[INFO] Experiment looks valid
</span></span><span style=display:flex><span>[INFO] Running experiment: assess-filters-impact
</span></span><span style=display:flex><span>[INFO] Steady-state strategy: default
</span></span><span style=display:flex><span>[INFO] Rollbacks strategy: default
</span></span><span style=display:flex><span>[INFO] No steady state hypothesis defined. That&#39;s ok, just exploring.
</span></span><span style=display:flex><span>[INFO] Playing your experiment&#39;s method now...
</span></span><span style=display:flex><span>[INFO] Action: assess-filters-impact
</span></span><span style=display:flex><span>[INFO] Validating client credentials and listing probably impacted instances and/or networks with the given arguments zone=&#39;world-1a&#39; and filters={&#39;instances&#39;: [{&#39;Name&#39;: &#39;tag-key&#39;, &#39;Values&#39;: [&#39;kubernetes.io/cluster/shoot--my-project--my-shoot&#39;]}], &#39;vpcs&#39;: [{&#39;Name&#39;: &#39;tag-key&#39;, &#39;Values&#39;: [&#39;kubernetes.io/cluster/shoot--my-project--my-shoot&#39;]}]}:
</span></span><span style=display:flex><span>[INFO] 1 instance(s) would be impacted:
</span></span><span style=display:flex><span>[INFO] - i-aabbccddeeff0000
</span></span><span style=display:flex><span>[INFO] 1 VPC(s) would be impacted:
</span></span><span style=display:flex><span>[INFO] - vpc-aabbccddeeff0000
</span></span><span style=display:flex><span>[INFO] Let&#39;s rollback...
</span></span><span style=display:flex><span>[INFO] No declared rollbacks, let&#39;s move on.
</span></span><span style=display:flex><span>[INFO] Experiment ended with status: completed
</span></span></code></pre></div><p>🎉 Congratulations! You successfully ran your first <code>chaosgarden</code> experiment.</p><h3 id=a-destructive-experiment>A Destructive Experiment</h3><p>Now let&rsquo;s break 🪓 your cluster. Be advised that this experiment will be destructive in the sense that we will temporarily network-partition all nodes in one availability zone (machine termination or restart is available with <code>chaosgarden</code> as well). That means, these nodes and their pods won&rsquo;t be able to &ldquo;talk&rdquo; to other nodes, pods, and services. Also, the API server will become unreachable for them and the API server will report them as unreachable (confusingly shown as <code>NotReady</code> when you run <code>kubectl get nodes</code> and <code>Unknown</code> in the status <code>Ready</code> condition when you run <code>kubectl get nodes --output yaml</code>).</p><p>Being unreachable will trigger service endpoint and load balancer de-registration (when the node&rsquo;s grace period lapses) as well as eventually pod eviction and machine replacement (which will continue to fail under test). We won&rsquo;t run the experiment long enough for all of these effects to materialize, but the longer you run it, the more will happen, up to temporarily giving up/going into &ldquo;back-off&rdquo; for the affected worker pool in that zone. You will also see that the Kubernetes cluster autoscaler will try to create a new machine almost immediately, if pods are pending for the affected zone (which will initially fail under test, but may succeed later, which again depends on the runtime of the experiment and whether or not the cluster autoscaler goes into &ldquo;back-off&rdquo; or not).</p><p>But for now, all of this doesn&rsquo;t matter as we want to start &ldquo;small&rdquo;. You can later read up more on the various settings and effects in our <a href=/docs/guides/high-availability/best-practices/>best practices guide on high availability</a>.</p><p>Please create a new experiment file, this time with this content:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    &#34;title&#34;: <span style=color:#a31515>&#34;run-network-failure-simulation&#34;</span>,
</span></span><span style=display:flex><span>    &#34;description&#34;: <span style=color:#a31515>&#34;run-network-failure-simulation&#34;</span>,
</span></span><span style=display:flex><span>    &#34;method&#34;: [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            &#34;type&#34;: <span style=color:#a31515>&#34;action&#34;</span>,
</span></span><span style=display:flex><span>            &#34;name&#34;: <span style=color:#a31515>&#34;run-network-failure-simulation&#34;</span>,
</span></span><span style=display:flex><span>            &#34;provider&#34;: {
</span></span><span style=display:flex><span>                &#34;type&#34;: <span style=color:#a31515>&#34;python&#34;</span>,
</span></span><span style=display:flex><span>                &#34;module&#34;: <span style=color:#a31515>&#34;chaosgarden.garden.actions&#34;</span>,
</span></span><span style=display:flex><span>                &#34;func&#34;: <span style=color:#a31515>&#34;run_cloud_provider_network_failure_simulation&#34;</span>,
</span></span><span style=display:flex><span>                &#34;arguments&#34;: {
</span></span><span style=display:flex><span>                    &#34;mode&#34;: <span style=color:#a31515>&#34;total&#34;</span>,
</span></span><span style=display:flex><span>                    &#34;zone&#34;: 0,
</span></span><span style=display:flex><span>                    &#34;duration&#34;: 60
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    &#34;rollbacks&#34;: [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            &#34;type&#34;: <span style=color:#a31515>&#34;action&#34;</span>,
</span></span><span style=display:flex><span>            &#34;name&#34;: <span style=color:#a31515>&#34;rollback-network-failure-simulation&#34;</span>,
</span></span><span style=display:flex><span>            &#34;provider&#34;: {
</span></span><span style=display:flex><span>                &#34;type&#34;: <span style=color:#a31515>&#34;python&#34;</span>,
</span></span><span style=display:flex><span>                &#34;module&#34;: <span style=color:#a31515>&#34;chaosgarden.garden.actions&#34;</span>,
</span></span><span style=display:flex><span>                &#34;func&#34;: <span style=color:#a31515>&#34;rollback_cloud_provider_network_failure_simulation&#34;</span>,
</span></span><span style=display:flex><span>                &#34;arguments&#34;: {
</span></span><span style=display:flex><span>                    &#34;mode&#34;: <span style=color:#a31515>&#34;total&#34;</span>,
</span></span><span style=display:flex><span>                    &#34;zone&#34;: 0
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>            }
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    &#34;configuration&#34;: {
</span></span><span style=display:flex><span>        &#34;garden_project&#34;: {
</span></span><span style=display:flex><span>            &#34;type&#34;: <span style=color:#a31515>&#34;env&#34;</span>,
</span></span><span style=display:flex><span>            &#34;key&#34;: <span style=color:#a31515>&#34;GARDEN_PROJECT&#34;</span>
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        &#34;garden_shoot&#34;: {
</span></span><span style=display:flex><span>            &#34;type&#34;: <span style=color:#a31515>&#34;env&#34;</span>,
</span></span><span style=display:flex><span>            &#34;key&#34;: <span style=color:#a31515>&#34;GARDEN_SHOOT&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>ℹ️ There is an even more destructive action that terminates or alternatively restarts machines in a given zone 🔥 (immediately or delayed with some randomness/chaos for maximum inconvenience for the nodes and pods). You can find links to all these examples at the end of this tutorial.</p><p>This experiment is very similar, but this time we will break 🪓 your cluster - for <code>60s</code>. If that&rsquo;s too short to even see a node or pod transition from <code>Ready</code> to <code>NotReady</code> (actually <code>Unknown</code>), then increase the <code>duration</code>. Depending on the workload that your cluster runs, you may already see effects of the network partitioning, because it is effective immediately. It&rsquo;s just that Kubernetes cannot know immediately and rather assumes that something is failing only <strong>after</strong> the node&rsquo;s grace period lapses, but the actual workload is impacted immediately.</p><p>Most notably, this experiment also has a <a href=https://chaostoolkit.org/reference/concepts/#rollbacks><code>rollbacks</code></a> section, which is invoked even if you abort the experiment or it fails unexpectedly, but only if you run the CLI with the option <code>--rollback-strategy always</code> which we will do soon. Any <code>chaosgarden</code> action that can undo its activity, will do that implicitly when the <code>duration</code> lapses, but it is a best practice to always configure a <code>rollbacks</code> section in case something unexpected happens. Should you be in panic and just want to run the <code>rollbacks</code> section, you can remove all other actions and the CLI will execute the <code>rollbacks</code> section immediately.</p><p>One other thing is different in the second experiment as well. We now read the name of the project and the shoot from the environment, i.e. a <a href=https://chaostoolkit.org/reference/api/experiment/#configuration><code>configuration</code></a> section can automatically expand <a href=https://chaostoolkit.org/reference/api/experiment/#environment-configurations>environment variables</a>. Also useful to know (not shown here), <code>chaostoolkit</code> supports <a href=https://chaostoolkit.org/reference/api/experiment/#variable-substitution>variable substitution</a> too, so that you have to define variables only once. Please note that you can also add a <a href=https://chaostoolkit.org/reference/api/experiment/#secrets><code>secrets</code></a> section that can also automatically expand <a href=https://chaostoolkit.org/reference/api/experiment/#environment-secrets>environment variables</a>. For instance, instead of targeting the Gardener API server via <code>$KUBECONFIG</code>, which is supported by our <code>chaosgarden</code> package natively, you can also explicitly refer to it in a <code>secrets</code> section (for brevity reasons not shown here either).</p><p>Let&rsquo;s now run your second experiment (please watch your nodes and pods in parallel, e.g. by running <code>watch kubectl get nodes,pods --output wide</code> in another terminal):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>export GARDEN_PROJECT=my-project
</span></span><span style=display:flex><span>export GARDEN_SHOOT=my-shoot
</span></span><span style=display:flex><span>chaos run --rollback-strategy always path/to/experiment
</span></span></code></pre></div><p>The output of the <code>run</code> command will be similar to the one above, but longer. It will mention either machines or networks that were network-partitioned (depends on cloud provider), but should revert everything back to normal.</p><p>Normally, you would not only run <a href=https://chaostoolkit.org/reference/concepts/#actions>actions</a> in the <code>method</code> section, but also <a href=https://chaostoolkit.org/reference/concepts/#probes>probes</a> as part of a <a href=https://chaostoolkit.org/reference/concepts/#steady-state-hypothesis>steady state hypothesis</a>. Such steady state hypothesis probes are run before and after the actions to validate that the &ldquo;system&rdquo; was in a healthy state before and gets back to a healthy state after the actions ran, hence show that the &ldquo;system&rdquo; is in a steady state when not under test. Eventually, you will write your own probes that don&rsquo;t even have to be part of a steady state hypothesis. We at Gardener run multi-zone (multiple zones at once) and rolling-zone (strike each zone once) outages with continuous custom probes all within the <code>method</code> section to validate our KPIs continuously under test (e.g. how long do the individual fail-overs take/how long is the actual outage). The most complex scenarios are even run via Python scripts as all actions and probes can also be invoked directly (which is what the CLI does).</p><h2 id=high-availability>High Availability</h2><p>Developing highly available workload that can tolerate a zone outage is no trivial task. You can find more information on how to achieve this goal in our <a href=/docs/guides/high-availability/best-practices/>best practices guide on high availability</a>.</p><p>Thank you for your interest in Gardener chaos engineering and making your workload more resilient.</p><h2 id=further-reading>Further Reading</h2><p>Here some links for further reading:</p><ul><li><strong>Examples</strong>: <a href=https://github.com/gardener/chaos-engineering/tree/main/docs/tutorials/experiments>Experiments</a>, <a href=https://github.com/gardener/chaos-engineering/tree/main/docs/tutorials/scripts>Scripts</a></li><li><strong>Gardener Chaos Engineering</strong>: <a href=https://github.com/gardener/chaos-engineering>GitHub</a>, <a href=https://pypi.org/project/chaosgarden>PyPI</a>, <a href=https://github.com/gardener/chaos-engineering/tree/main/docs/garden>Module Docs for Gardener Users</a></li><li><strong>Chaos Toolkit Core</strong>: <a href=https://chaostoolkit.org>Home Page</a>, <a href=https://chaostoolkit.org/reference/usage/install>Installation</a>, <a href=https://chaostoolkit.org/reference/concepts>Concepts</a>, <a href=https://github.com/chaostoolkit/chaostoolkit>GitHub</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-747cf5936d365b93b5b0facc71cbec83>3.3 - Control Plane</h1><h1 id=highly-available-shoot-control-plane>Highly Available Shoot Control Plane</h1><p>Shoot resource offers a way to request for a highly available control plane.</p><h2 id=failure-tolerance-types>Failure Tolerance Types</h2><p>A highly available shoot control plane can be setup with either a failure tolerance of <code>zone</code> or <code>node</code>.</p><h3 id=node-failure-tolerance><code>Node</code> Failure Tolerance</h3><p>The failure tolerance of a <code>node</code> will have the following characteristics:</p><ul><li>Control plane components will be spread across different nodes within a single availability zone. There will not be
more than one replica per node for each control plane component which has more than one replica.</li><li><code>Worker pool</code> should have a minimum of 3 nodes.</li><li>A multi-node etcd (quorum size of 3) will be provisioned, offering zero-downtime capabilities with each member in a
different node within a single availability zone.</li></ul><h3 id=zone-failure-tolerance><code>Zone</code> Failure Tolerance</h3><p>The failure tolerance of a <code>zone</code> will have the following characteristics:</p><ul><li>Control plane components will be spread across different availability zones. There will be at least
one replica per zone for each control plane component which has more than one replica.</li><li>Gardener scheduler will automatically select a <code>seed</code> which has a minimum of 3 zones to host the shoot control plane.</li><li>A multi-node etcd (quorum size of 3) will be provisioned, offering zero-downtime capabilities with each member in a
different zone.</li></ul><h2 id=shoot-spec>Shoot Spec</h2><p>To request for a highly available shoot control plane Gardener provides the following configuration in the shoot spec:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  controlPlane:
</span></span><span style=display:flex><span>    highAvailability:
</span></span><span style=display:flex><span>      failureTolerance:
</span></span><span style=display:flex><span>        type: &lt;node | zone&gt;
</span></span></code></pre></div><p><strong>Allowed Transitions</strong></p><p>If you already have a shoot cluster with non-HA control plane, then the following upgrades are possible:</p><ul><li>Upgrade of non-HA shoot control plane to HA shoot control plane with <code>node</code> failure tolerance.</li><li>Upgrade of non-HA shoot control plane to HA shoot control plane with <code>zone</code> failure tolerance. However, it is essential that the <code>seed</code> which is currently hosting the shoot control plane should be <code>multi-zonal</code>. If it is not, then the request to upgrade will be rejected.</li></ul><blockquote><p><strong>Note:</strong> There will be a small downtime during the upgrade, especially for etcd, which will transition from a single node etcd cluster to a multi-node etcd cluster.</p></blockquote><p><strong>Disallowed Transitions</strong></p><p>If you already have a shoot cluster with HA control plane, then the following transitions are not possible:</p><ul><li>Upgrade of HA shoot control plane from <code>node</code> failure tolerance to <code>zone</code> failure tolerance is currently not supported, mainly because already existing volumes are bound to the zone they were created in originally.</li><li>Downgrade of HA shoot control plane with <code>zone</code> failure tolerance to <code>node</code> failure tolerance is currently not supported, mainly because of the same reason as above, that already existing volumes are bound to the respective zones they were created in originally.</li><li>Downgrade of HA shoot control plane with either <code>node</code> or <code>zone</code> failure tolerance, to a non-HA shoot control plane is currently not supported, mainly because <a href=https://github.com/gardener/etcd-druid>etcd-druid</a> does not currently support scaling down of a multi-node etcd cluster to a single-node etcd cluster.</li></ul><h2 id=zone-outage-situation>Zone Outage Situation</h2><p>Implementing highly available software that can tolerate even a zone outage unscathed is no trivial task. You may find our <a href=/docs/guides/high-availability/best-practices/>HA Best Practices</a> helpful to get closer to that goal. In this document, we collected many options and settings for you that also Gardener internally uses to provide a highly available service.</p><p>During a zone outage, you may be forced to change your cluster setup on short notice in order to compensate for failures and shortages resulting from the outage.
For instance, if the shoot cluster has worker nodes across three zones where one zone goes down, the computing power from these nodes is also gone during that time.
Changing the worker pool (<code>shoot.spec.provider.workers[]</code>) and infrastructure (<code>shoot.spec.provider.infrastructureConfig</code>) configuration can eliminate this disbalance, having enough machines in healthy availability zones that can cope with the requests of your applications.</p><p>Gardener relies on a sophisticated reconciliation flow with several dependencies for which various flow steps wait for the <em>readiness</em> of prior ones.
During a zone outage, this can block the entire flow, e.g., because all three <code>etcd</code> replicas can never be ready when a zone is down, and required changes mentioned above can never be accomplished.
For this, a special one-off annotation <code>shoot.gardener.cloud/skip-readiness</code> helps to skip any readiness checks in the flow.</p><blockquote><p>The <code>shoot.gardener.cloud/skip-readiness</code> annotation serves as a last resort if reconciliation is stuck because of important changes during an AZ outage. Use it with caution, only in exceptional cases and after a case-by-case evaluation with your Gardener landscape administrator. If used together with other operations like Kubernetes version upgrades or credential rotation, the annotation may lead to a severe outage of your shoot control plane.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-4e337bb85d7a6b78c910960ab1f92f56>4 - Administer Client (Shoot) Clusters</h1></div><div class=td-content><h1 id=pg-ab8330cee4aa2d08d796a6b45fdcabc3>4.1 - Scalability of Gardener Managed Kubernetes Clusters</h1><div class=lead>Know the boundary conditions when scaling your workloads</div><p>Have you ever wondered how much more your Kubernetes cluster can scale before it breaks down?</p><p>Of course, the answer is heavily dependent on your workloads. But be assured, any cluster will break eventually. Therefore, the best mitigation is to plan for sharding early and run multiple clusters instead of trying to optimize everything hoping to survive with a single cluster.
Still, it is helpful to know when the time has come to scale out. This document aims at giving you the basic knowledge to keep a Gardener-managed Kubernetes cluster up and running while it scales according to your needs.</p><h2 id=welcome-to-planet-scale-please-mind-the-gap>Welcome to Planet Scale, Please Mind the Gap!</h2><p>For a complex, distributed system like Kubernetes it is impossible to give absolute thresholds for its scalability. Instead, the limit of a cluster&rsquo;s scalability is a combination of various, interconnected dimensions.</p><p>Let&rsquo;s take a rather simple example of two dimensions - the number of <code>Pods</code> per <code>Node</code> and number of <code>Nodes</code> in a cluster. According to the <a href=https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md>scalability thresholds documentation</a>, Kubernetes can scale up to 5000 <code>Nodes</code> and with default settings accommodate a maximum of 110 <code>Pods</code> on a single <code>Node</code>. Pushing only a single dimension towards its limit will likely harm the cluster. But if both are pushed simultaneously, any cluster will break way before reaching one dimension&rsquo;s limit.</p><p><img src=/__resources/pod-nodes_94e68d.png alt="Pods and Nodes"></p><p>What sounds rather straightforward in theory can be a bit trickier in reality. While 110 <code>Pods</code> is the default limit, we successfully pushed beyond that and in certain cases run up to 200 <code>Pods</code> per <code>Node</code> without breaking the cluster. This is possible in an environment where one knows and controls all workloads and cluster configurations. It still requires careful testing, though, and comes at the cost of limiting the scalability of other dimensions, like the number of <code>Nodes</code>.</p><p>Of course, a Kubernetes cluster has a plethora of dimensions. Thus, when looking at a simple questions like <em>&ldquo;How many resources can I store in ETCD?&rdquo;</em>, the only meaningful answer must be: <em>&ldquo;it depends&rdquo;</em></p><p>The following sections will help you to identify relevant dimensions and how they affect a Gardener-managed Kubernetes cluster&rsquo;s scalability.</p><h2 id=official-kubernetes-thresholds-and-scalability-considerations>&ldquo;Official&rdquo; Kubernetes Thresholds and Scalability Considerations</h2><p>To get started with the topic, please check the basic guidance provided by the Kubernetes community (specifically SIG Scalability):</p><ul><li><a href=https://github.com/kubernetes/community/blob/master/sig-scalability/slos/slos.md#how-we-define-scalability>How we define scalability?</a></li><li><a href=https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md>Kubernetes Scalability Thresholds</a></li></ul><p>Furthermore, the problem space has been discussed in a <a href="https://www.youtube.com/watch?v=t_Ww6ELKl4Q">KubeCon talk</a>, the slides for which can be found <a href=https://docs.google.com/presentation/d/1aWjxpY4YJ4KJQUTqaVHdR4sbhwqDiW30EF4_hGCc-gI>here</a>. You should at least read the slides before continuing.</p><p>Essentially, it comes down to this:</p><blockquote><p>If you promise to:</p><ul><li>correctly configure your cluster</li><li>use extensibility features &ldquo;reasonably&rdquo;</li><li>keep the load in the cluster within recommended limits</li></ul><p>Then we promise that your cluster will function properly.</p></blockquote><p>With that knowledge in mind, let&rsquo;s look at Gardener and eventually pick up the question about the number of objects in ETCD raised above.</p><h2 id=gardener-specific-considerations>Gardener-Specific Considerations</h2><p>The following considerations are based on experience with various large clusters that scaled in different dimensions. Just as explained above, pushing beyond even one of the limits is likely to cause issues at some point in time (but not guaranteed). Depending on the setup of your workloads however, it might work unexpectedly well. Nevertheless, we urge you take conscious decisions and rather think about sharding your workloads. Please keep in mind - your workload affects the overall stability and scalability of a cluster significantly.</p><h3 id=etcd>ETCD</h3><p><strong>The following section is based on a setup where ETCD <code>Pods</code> run on a dedicated <code>Node</code> pool and each <code>Node</code> has 8 vCPU and 32GB memory at least.</strong></p><p>ETCD has a practical space limit of 8 GB. It caps the number of objects one can technically have in a Kubernetes cluster.</p><p>Of course, the number is heavily influenced by each object&rsquo;s size, especially when considering that secrets and configmaps may store up to 1MB of data. Another dimension is a cluster&rsquo;s churn rate. Since ETCD stores a history of the keyspace, a higher churn rate reduces the number of objects. Gardener runs <a href=https://etcd.io/docs/v3.4/op-guide/maintenance/#history-compaction>compaction</a> every 30min and <a href=https://etcd.io/docs/v3.4/op-guide/maintenance/#defragmentation>defragmentation</a> once per day during a cluster&rsquo;s maintenance window to ensure proper ETCD operations. However, it is still possible to overload ETCD. If the space limit is reached, ETCD will only accept <code>READ</code> or <code>DELETE</code> requests and manual interaction by a Gardener operator is needed to disarm the alarm, once you got below the threshold.</p><p>To avoid such a situation, you can monitor the current ETCD usage via the &ldquo;ETCD&rdquo; dashboard of the monitoring stack. It gives you the current DB size, as well as historical data for the past 2 weeks. While there are improvements planned to trigger compaction and defragmentation based on DB size, an ETCD should not grow up to this threshold. A typical, healthy DB size is less than 3 GB.</p><p>Furthermore, the dashboard has a panel called &ldquo;Memory&rdquo;, which indicates the memory usage of the etcd pod(s). Using more than 16GB memory is a clear red flag, and you should reduce the load on ETCD.</p><p>Another dimension you should be aware of is the object count in ETCD. You can check it via the &ldquo;API Server&rdquo; dashboard, which features a &ldquo;ETCD Object Counts By Resource&rdquo; panel. The overall number of objects (excluding <code>events</code>, as they are stored in a different etcd instance) should not exceed 100k for most use cases.</p><h3 id=kube-api-server>Kube API Server</h3><p><strong>The following section is based on a setup where <code>kube-apiserver</code> run as <code>Pods</code> and are scheduled to <code>Nodes</code> with at least 8 vCPU and 32GB memory.</strong></p><p>Gardener can scale the <code>Deployment</code> of a <code>kube-apiserver</code> horizontally and vertically. Horizontal scaling is limited to a certain number of replicas and should not concern a stakeholder much. However, the CPU / memory consumption of an individual <code>kube-apiserver</code> pod poses a potential threat to the overall availability of your cluster. The vertical scaling of any <code>kube-apiserver</code> is limited by the amount of resources available on a single <code>Node</code>. Outgrowing the resources of a <code>Node</code> will cause a downtime and render the cluster unavailable.</p><p>In general, continuous CPU usage of up to 3 cores and 16 GB memory per <code>kube-apiserver</code> pod is considered to be safe. This gives some room to absorb spikes, for example when the caches are initialized. You can check the resource consumption by selecting <code>kube-apiserver</code> <code>Pods</code> in the &ldquo;Kubernetes <code>Pods</code>&rdquo; dashboard. If these boundaries are exceeded constantly, you need to investigate and derive measures to lower the load.</p><p>Further information is also recorded and made available through the monitoring stack. The dashboard &ldquo;API Server Request Duration and Response Size&rdquo; provides insights into the request processing time of <code>kube-apiserver</code> <code>Pods</code>. Related information like request rates, dropped requests or termination codes (e.g., <code>429</code> for too many requests) can be obtained from the dashboards &ldquo;API Server&rdquo; and &ldquo;Kubernetes API Server Details&rdquo;. They provide a good indicator for how well the system is dealing with its current load.</p><p>Reducing the load on the API servers can become a challenge. To get started, you may try to:</p><ul><li>Use immutable <a href=https://kubernetes.io/docs/concepts/configuration/secret/#secret-immutable>secrets</a> and <a href=https://kubernetes.io/docs/concepts/configuration/configmap/#configmap-immutable>configmaps</a> where possible to save watches. This pays off, especially when you have a high number of <code>Nodes</code> or just lots of secrets in general.</li><li>Applications interacting with the K8s API: If you know an object by its name, use it. Using label selector queries is expensive, as the filtering happens only within the <code>kube-apiserver</code> and not <code>etcd</code>, hence all resources must first pass completely from <code>etcd</code> to <code>kube-apiserver</code>.</li><li>Use (single object) caches within your controllers. Check the <a href=https://github.com/gardener/gardener/issues/7593>&ldquo;Use cache for ShootStates in Gardenlet&rdquo; issue</a> for an example.</li></ul><h3 id=nodes><code>Nodes</code></h3><p>When talking about the scalability of a Kubernetes cluster, <code>Nodes</code> are probably mentioned in the first place&mldr; well, obviously not in this guide. While vanilla Kubernetes lists 5000 <code>Nodes</code> as its upper limit, pushing that dimension is not feasible. Most clusters should run with fewer than 300 <code>Nodes</code>. But of course, the actual limit depends on the workloads deployed and can be lower or higher. As you scale your cluster, be extra careful and closely monitor ETCD and <code>kube-apiserver</code>.</p><p>The scalability of <code>Nodes</code> is subject to a range of limiting factors. Some of them can only be defined upon cluster creation and remain immutable during a cluster lifetime. So let&rsquo;s discuss the most important dimensions.</p><p><strong>CIDR</strong>:</p><p>Upon cluster creation, you have to specify or use the default values for several network segments. There are dedicated CIDRs for services, <code>Pods</code>, and <code>Nodes</code>. Each defines a range of IP addresses available for the individual resource type. Obviously, the maximum of possible <code>Nodes</code> is capped by the CIDR for <code>Nodes</code>.
However, there is a second limiting factor, which is the pod CIDR combined with the <code>nodeCIDRMaskSize</code>. This mask is used to divide the pod CIDR into smaller subnets, where each blocks gets assigned to a node. With a <code>/16</code> pod network and a <code>/24</code> nodeCIDRMaskSize, a cluster can scale up to 256 <code>Nodes</code>. Please check <a href=/docs/gardener/shoot_networking/>Shoot Networking</a> for details.</p><p>Even though a <code>/24</code> nodeCIDRMaskSize translates to a theoretical 256 pod IP addresses per <code>Node</code>, the <code>maxPods</code> setting should be less than 1/2 of this value. This gives the system some breathing room for churn and minimizes the risk for strange effects like mis-routed packages caused by immediate re-use of IPs.</p><p><strong>Cloud provider capacity</strong>:</p><p>Most of the time, <code>Nodes</code> in Kubernetes translate to virtual machines on a hyperscaler. An attempt to add more <code>Nodes</code> to a cluster might fail due to capacity issues resulting in an error message like this:</p><pre tabindex=0><code>Cloud provider message - machine codes error: code = [Internal] message = [InsufficientInstanceCapacity: We currently do not have sufficient &lt;instance type&gt; capacity in the Availability Zone you requested. Our system will be working on provisioning additional capacity. 
</code></pre><p>In heavily utilized regions, individual clusters are competing for scarce resources. So before choosing a region / zone, try to ensure that the hyperscaler supports your anticipated growth. This might be done through quota requests or by contacting the respective support teams.
To mitigate such a situation, you may configure a worker pool with a different <code>Node</code> type and a corresponding <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/expander/priority/readme.md>priority expander</a> as part of a <a href=/docs/gardener/api-reference/core/#clusterautoscaler>shoot&rsquo;s autoscaler section</a>. Please consult the <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders>Autoscaler FAQ</a> for more details.</p><p><strong>Rolling of <code>Node</code> pools</strong>:</p><p>The overall number of <code>Nodes</code> is affecting the duration of a cluster&rsquo;s maintenance. When upgrading a <code>Node</code> pool to a new OS image or Kubernetes version, all machines will be drained and deleted, and replaced with new ones. The more <code>Nodes</code> a cluster has, the longer this process will take, given that workloads are typically protected by <code>PodDisruptionBudgets</code>. Check <a href=/docs/gardener/shoot_updates/>Shoot Updates and Upgrades</a> for details. Be sure to take this into consideration when planning maintenance.</p><p><strong>Root disk</strong>:</p><p>You should be aware that the <code>Node</code> configuration impacts your workload&rsquo;s performance too. Take the root disk of a <code>Node</code>, for example. While most hyperscalers offer the usage of HDD and SSD disks, it is strongly recommended to use SSD volumes as root disks. When there are lots of <code>Pods</code> on a <code>Node</code> or workloads making extensive use of <code>emptyDir</code> volumes, disk throttling becomes an issue. When a disk hits its IOPS limits, processes are stuck in IO-wait and slow down significantly. This can lead to a slow-down in the kubelet&rsquo;s heartbeat mechanism and result in <code>Nodes</code> being replaced automatically, as they appear to be unhealthy. To analyze such a situation, you might have to run tools like <code>iostat</code>, <code>sar</code> or <code>top</code> directly on a <code>Node</code>.</p><p>Switching to an I/O optimized instance type (if offered for your infrastructure) can help to resolve issue. Please keep in mind that disks used via <code>PersistentVolumeClaims</code> have I/O limits as well. Sometimes these limits are related to the size and/or can be increased for individual disks.</p><h3 id=cloud-provider-infrastructure-limits>Cloud Provider (Infrastructure) Limits</h3><p>In addition to the already mentioned capacity restrictions, a cloud provider may impose other limitations to a Kubernetes cluster&rsquo;s scalability. One category is the account quota defining the number of resources allowed globally or per region. Make sure to request appropriate values that suit your needs and contain a buffer, for example for having more <code>Nodes</code> during a rolling update.</p><p>Another dimension is the network throughput per VM or network interface. While you may be able to choose a network-optimized <code>Node</code> type for your workload to mitigate issues, you cannot influence the available bandwidth for control plane components. Therefore, please ensure that the traffic on the ETCD does not exceed 100MB/s. The ETCD dashboard provides data for monitoring this metric.</p><p>In some environments the upstream DNS might become an issue too and make your workloads subject to rate limiting. Given the heterogeneity of cloud providers incl. private data centers, it is not possible to give any thresholds. Still, the &ldquo;CoreDNS&rdquo; and &ldquo;NodeLocalDNS&rdquo; dashboards can help to derive a workload&rsquo;s usage pattern. Check the <a href=/docs/gardener/dns-autoscaling/>DNS autoscaling</a> and <a href=/docs/gardener/node-local-dns/>NodeLocalDNS</a> documentations for available configuration options.</p><h3 id=webhooks>Webhooks</h3><p>While webhooks provide powerful means to manage a cluster, they are equally powerful in breaking a cluster upon a malfunction or unavailability. Imagine using a policy enforcing system like <a href=https://kyverno.io/docs/>Kyverno</a> or <a href=https://open-policy-agent.github.io/gatekeeper/website/docs/>Open Policy Agent Gatekeeper</a>. As part of the stack, both will deploy webhooks which are invoked for almost everything that happens in a cluster. Now, if this webhook gets either overloaded or is simply not available, the cluster will stop functioning properly.</p><p>Hence, you have to ensure proper sizing, quick processing time, and availability of the webhook serving <code>Pods</code> when deploying webhooks. Please consult Dynamic Admission Control (<a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#availability>Availability</a> and <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#timeouts>Timeouts</a> sections) for details. You should also be aware of the time added to any request that has to go through a webhook, as the <code>kube-apiserver</code> sends the request for mutation / validation to another pod and waits for the response. The more resources being subject to an external webhook, the more likely this will become a bottleneck when having a high churn rate on resources. Within the Gardener monitoring stack, you can check the extra time per webhook via the &ldquo;API Server (Admission Details)&rdquo; dashboard, which has a panel for &ldquo;Duration per Webhook&rdquo;.</p><p>In Gardener, any webhook timeout should be less than 15 seconds. Due to the separation of Kubernetes data-plane (shoot) and control-plane (seed) in Gardener, the extra hop from <code>kube-apiserver</code> (control-plane) to webhook (data-plane) is more expensive. Please check <a href=/docs/gardener/shoot_status/>Shoot Status</a> for more details.</p><h3 id=custom-resource-definitions>Custom Resource Definitions</h3><p>Using Custom Resource Definitions (CRD) to extend a cluster&rsquo;s API is a common Kubernetes pattern and so is writing an operator to act upon custom resources. Writing an efficient controller reduces the load on the <code>kube-apiserver</code> and allows for better scaling. As a starting point, you might want to read Gardener&rsquo;s <a href=/docs/gardener/kubernetes-clients/>Kubernetes Clients Guide</a>.</p><p>Another problematic dimension is the usage of <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#webhook-conversion>conversion webhooks</a> when having resources stored in different versions. Not only do they add latency (see <a href=/docs/guides/administer-shoots/scalability/#webhooks>Webhooks</a>) but can also block the kube-controllermanager&rsquo;s garbage collection. If a conversion webhook is unavailable, the garbage collector fails to list all resources and does not perform any cleanup. In order to avoid such a situation, it is highly recommended to use conversion webhooks only when necessary and complete the migration to a new version as soon as possible.</p><h2 id=conclusion>Conclusion</h2><p>As outlined by SIG Scalability, it is quite impossible to give limits or even recommendations fitting every individual use case. Instead, this guide outlines relevant dimensions and gives rather conservative recommendations based on usage patterns observed. By combining this information, it is possible to operate and scale a cluster in stable manner.</p><p>While going beyond is certainly possible for some dimensions, it significantly increases the risk of instability. Typically, limits on the control-plane are introduced by the availability of resources like CPU or memory on a single machine and can hardly be influenced by any user. Therefore, utilizing the existing resources efficiently is key. Other parameters are controlled by a user. In these cases, careful testing may reveal actual limits for a specific use case.</p><p>Please keep in mind that all aspects of a workload greatly influence the stability and scalability of a Kubernetes cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b9afe5cef9e415c0b02c6447660eee6c>4.2 - Authenticating with an Identity Provider</h1><div class=lead>Use OpenID Connect to authenticate users to access shoot clusters</div><h2 id=prerequisites>Prerequisites</h2><p>Please read the following background material on <a href=https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens>Authenticating</a>.</p><h2 id=overview>Overview</h2><p>Kubernetes on its own doesn’t provide any user management. In other words, users aren’t managed through Kubernetes resources. Whenever you refer to a human user it’s sufficient to use a unique ID, for example, an email address. Nevertheless, Gardener project owners can use an identity provider to authenticate user access for shoot clusters in the following way:</p><ol><li><a href=/docs/guides/administer-shoots/oidc-login/#configure-an-identity-provider>Configure an Identity Provider</a> using <strong>OpenID Connect</strong> (OIDC).</li><li><a href=/docs/guides/administer-shoots/oidc-login/#configure-a-local-kubectl-oidc-login>Configure a local kubectl oidc-login</a> to enable <code>oidc-login</code>.</li><li><a href=/docs/guides/administer-shoots/oidc-login/#configure-the-shoot-cluster>Configure the shoot cluster</a> to share details of the OIDC-compliant identity provider with the Kubernetes API Server.</li><li><a href=/docs/guides/administer-shoots/oidc-login/#authorize-an-authenticated-user>Authorize an authenticated user</a> using role-based access control (RBAC).</li><li><a href=/docs/guides/administer-shoots/oidc-login/#verify-the-result>Verify the result</a></li></ol><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Gardener allows administrators to modify aspects of the control plane setup. It gives administrators full control of how the control plane is parameterized. While this offers much flexibility, administrators need to ensure that they don’t configure a control plane that goes beyond the service level agreements of the responsible operators team.</div><h2 id=configure-an-identity-provider>Configure an Identity Provider</h2><p>Create a tenant in an OIDC compatible Identity Provider. For simplicity, we use <em>Auth0</em>, which has a free plan.</p><ol><li><p>In your tenant, create a client application to use authentication with <code>kubectl</code>:</p><p><img src=/__resources/Create-client-application_283a5e.png alt="Create client application"></p></li><li><p>Provide a <em>Name</em>, choose <em>Native</em> as application type, and choose <em>CREATE</em>.</p><p><img src=/__resources/Choose-application-type_1cec2c.png alt="Choose application type"></p></li><li><p>In the tab <em>Settings</em>, copy the following parameters to a local text file:</p><ul><li><p><em>Domain</em></p><p>Corresponds to the <strong>issuer</strong> in OIDC. It must be an <code>https</code>-secured endpoint (Auth0 requires a trailing <code>/</code> at the end). For more information, see <a href=https://openid.net/specs/openid-connect-core-1_0.html#Terminology>Issuer Identifier</a>.</p></li><li><p><em>Client ID</em></p></li><li><p><em>Client Secret</em></p><p><img src=/__resources/Basic-information_dd3b74.png alt="Basic information"></p></li></ul></li><li><p>Configure the client to have a callback url of <code>http://localhost:8000</code>. This callback connects to your local <code>kubectl oidc-login</code> plugin:</p><p><img src=/__resources/Configure-callback_aa8d88.png alt="Configure callback"></p></li><li><p>Save your changes.</p></li><li><p>Verify that <code>https://&lt;Auth0 Domain>/.well-known/openid-configuration</code> is reachable.</p></li><li><p>Choose <em>Users & Roles</em> > <em>Users</em> > <em>CREATE USERS</em> to create a user with a user and password:</p><p><img src=/__resources/Create-user_ef9d0e.png alt="Create user"></p></li></ol><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Users must have a <em>verified</em> email address.</div><h2 id=configure-a-local-kubectl-oidc-login>Configure a Local <code>kubectl</code> <code>oidc-login</code></h2><ol><li><p>Install the <code>kubectl</code> plugin <a href=https://github.com/int128/kubelogin>oidc-login</a>. We highly recommend the <a href=https://github.com/kubernetes-sigs/krew>krew</a> installation tool, which also makes other plugins easily available.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl krew install oidc-login
</span></span></code></pre></div><p>The response looks like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>Updated the local copy of plugin index.
</span></span><span style=display:flex><span>Installing plugin: oidc-login
</span></span><span style=display:flex><span>CAVEATS:
</span></span><span style=display:flex><span>\
</span></span><span style=display:flex><span>|  You need to setup the OIDC provider, Kubernetes API server, role binding and kubeconfig.
</span></span><span style=display:flex><span>|  See https://github.com/int128/kubelogin for more.
</span></span><span style=display:flex><span>/
</span></span><span style=display:flex><span>Installed plugin: oidc-login
</span></span></code></pre></div></li><li><p>Prepare a <code>kubeconfig</code> for later use:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>cp ~/.kube/config ~/.kube/config-oidc
</span></span></code></pre></div></li><li><p>Modify the configuration of <code>~/.kube/config-oidc</code> as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: shoot--project--mycluster
</span></span><span style=display:flex><span>    user: my-oidc
</span></span><span style=display:flex><span>  name: shoot--project--mycluster
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: my-oidc
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    exec:
</span></span><span style=display:flex><span>      apiVersion: client.authentication.k8s.io/v1beta1
</span></span><span style=display:flex><span>      command: kubectl
</span></span><span style=display:flex><span>      args:
</span></span><span style=display:flex><span>      - oidc-login
</span></span><span style=display:flex><span>      - get-token
</span></span><span style=display:flex><span>      - --oidc-issuer-url=https://&lt;Issuer&gt;/ 
</span></span><span style=display:flex><span>      - --oidc-client-id=&lt;Client ID&gt;
</span></span><span style=display:flex><span>      - --oidc-client-secret=&lt;Client Secret&gt;
</span></span><span style=display:flex><span>      - --oidc-extra-scope=email,offline_access,profile
</span></span></code></pre></div></li></ol><p>To test our OIDC-based authentication, the context <code>shoot--project--mycluster</code> of <code>~/.kube/config-oidc</code> is used in a later step. For now, continue to use the configuration <code>~/.kube/config</code> with administration rights for your cluster.</p><h2 id=configure-the-shoot-cluster>Configure the Shoot Cluster</h2><p>Modify the shoot cluster YAML as follows, using the client ID and the domain (as issuer) from the settings of the client application you created in Auth0:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>apiVersion: garden.sapcloud.io/v1beta1
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: mycluster
</span></span><span style=display:flex><span>  namespace: garden-project
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      oidcConfig:
</span></span><span style=display:flex><span>        clientID: &lt;Client ID&gt;
</span></span><span style=display:flex><span>        issuerURL: <span style=color:#a31515>&#34;https://&lt;Issuer&gt;/&#34;</span>
</span></span><span style=display:flex><span>        usernameClaim: email
</span></span></code></pre></div><p>This change of the <code>Shoot</code> manifest triggers a reconciliation. Once the reconciliation is finished, your OIDC configuration is applied. It <strong>doesn&rsquo;t</strong> invalidate other certificate-based authentication methods. Wait for Gardener to reconcile the change. It can take up to 5 minutes.</p><h2 id=authorize-an-authenticated-user>Authorize an Authenticated User</h2><p>In Auth0, you created a user with a verified email address, <code>test@test.com</code> in our example. For simplicity, we authorize a single user identified by this email address with the cluster role <code>view</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style=display:flex><span>kind: ClusterRoleBinding
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: viewer-test
</span></span><span style=display:flex><span>roleRef:
</span></span><span style=display:flex><span>  apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>  kind: ClusterRole
</span></span><span style=display:flex><span>  name: view
</span></span><span style=display:flex><span>subjects:
</span></span><span style=display:flex><span>- apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>  kind: User
</span></span><span style=display:flex><span>  name: test@test.com
</span></span></code></pre></div><p>As administrator, apply the cluster role binding in your shoot cluster.</p><h2 id=verify-the-result>Verify the Result</h2><ol><li><p>To step into the shoes of your user, use the prepared <code>kubeconfig</code> file <code>~/.kube/config-oidc</code>, and switch to the context that uses <code>oidc-login</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>cd ~/.kube
</span></span><span style=display:flex><span>export KUBECONFIG=$(pwd)/config-oidc
</span></span><span style=display:flex><span>kubectl config use-context `shoot--project--mycluster`
</span></span></code></pre></div></li><li><p><code>kubectl</code> delegates the authentication to plugin <code>oidc-login</code> the first time the user uses <code>kubectl</code> to contact the API server, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl get all
</span></span></code></pre></div><p>The plugin opens a browser for an interactive authentication session with Auth0, and in parallel serves a local webserver for the configured callback.</p></li><li><p>Enter your login credentials.</p><p><img src=/__resources/Login-through-identity-provider_03681e.png alt="Login through identity provider"></p><p>You should get a successful response from the API server:</p><pre tabindex=0><code>Opening in existing browser session.
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   100.64.0.1   &lt;none&gt;        443/TCP   86m
</code></pre></li></ol><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4><p>After a successful login, <code>kubectl</code> uses a token for authentication so that you don’t have to provide user and password for every new <code>kubectl</code> command. How long the token is valid can be configured. If you want to log in again earlier, reset plugin <code>oidc-login</code>:</p><ol><li>Delete directory <code>~/.kube/cache/oidc-login</code>.</li><li>Delete the browser cache.</li></ol></div><ol><li><p>To see if your user uses the cluster role <code>view</code>, do some checks with <code>kubectl auth can-i</code>.</p><ul><li><p>The response for the following commands should be <code>no</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl auth can-i create clusterrolebindings
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl auth can-i get secrets
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl auth can-i describe secrets
</span></span></code></pre></div></li><li><p>The response for the following commands should be <code>yes</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl auth can-i list pods
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span>kubectl auth can-i get pods
</span></span></code></pre></div></li></ul></li></ol><p>If the last step is successful, you’ve configured your cluster to authenticate against an identity provider using OIDC.</p><h2 id=related-links>Related Links</h2><ul><li><a href=https://auth0.com/pricing/>Auth0 Pricing</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-966f6ba4cfb48e81de16f924f7aeb01d>4.3 - Backup and Restore of Kubernetes Objects</h1><div class=lead>Details about backup and recovery of Kubernetes objects based on the open source tool <a href=https://velero.io/>Velero</a>.</div><p><img src=/__resources/teaser_fb4130.png alt="Don&amp;rsquo;t worry &amp;hellip; have a backup"></p><h2 id=tldr>TL;DR</h2><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Details of the description might change in the near future since Heptio was taken over by VMWare which might result in different GitHub repositories or other changes. Please don&rsquo;t hesitate to inform us in case you encounter any issues.</div><p>In general, Backup and Restore (BR) covers activities enabling an organization to bring a system back in a consistent state, e.g., after a disaster or to setup a new system. These activities vary in a very broad way depending on the applications and its persistency.</p><p>Kubernetes objects like Pods, Deployments, NetworkPolicies, etc. configure Kubernetes internal components and might as well include external components like load balancer and persistent volumes of the cloud provider. The BR of external components and their configurations might be difficult to handle in case manual configurations were needed to prepare these components.</p><p>To set the expectations right from the beginning, this tutorial covers the BR of Kubernetes deployments which might use persistent volumes. The BR of any manual configuration of external components, e.g., via the cloud providers console, is not covered here, as well as the BR of a whole Kubernetes system.</p><p>This tutorial puts the focus on the open source tool <a href=https://velero.io/>Velero</a> (formerly Heptio Ark) and its functionality to explain the BR process.</p><style>#body-inner blockquote{border:0;padding:10px;margin-top:40px;margin-bottom:40px;border-radius:4px;background-color:rgba(0,0,0,5%);box-shadow:0 3px 6px rgba(0,0,0,.16),0 3px 6px rgba(0,0,0,.23);position:relative;padding-left:60px}#body-inner blockquote:before{content:"i";font-weight:700;position:absolute;top:0;bottom:0;left:0;background-color:#00a273;color:#fff;vertical-align:middle;margin:auto;width:36px;font-size:30px;text-align:center}</style><p>Basically, Velero allows you to:</p><ul><li>backup and restore your Kubernetes cluster resources and persistent volumes (on-demand or scheduled)</li><li>backup or restore all objects in your cluster, or filter resources by type, namespace, and/or label</li><li>by default, all persistent volumes are backed up (configurable)</li><li>replicate your production environment for development and testing environments</li><li>define an expiration date per backup</li><li>execute pre- and post-activities in a container of a pod when a backup is created (see <a href=https://velero.io/docs/main/backup-hooks/#docs>Hooks</a>)</li><li>extend Velero by Plugins, e.g., for Object and Block store (see <a href=https://velero.io/docs/main/custom-plugins/#docs>Plugins</a>)</li></ul><p>Velero consists of a server side component and a client tool. The server components consists of Custom Resource Definitions (CRD) and controllers to perform the activities. The client tool communicates with the K8s API server to, e.g., create objects like a Backup object.</p><p>The diagram below explains the backup process. When creating a backup, Velero client makes a call to the Kubernetes API server to create a Backup object (1). The BackupController notices the new Backup object, validates the object (2) and begins the backup process (3). Based on the filter settings provided by the Velero client it collects the resources in question (3). The BackupController creates a tar ball with the Kubernetes objects and stores it in the backup location, e.g., AWS S3 (4) as well as snapshots of persistent volumes (5).</p><p>The size of the backup tar ball corresponds to the number of objects in etcd. The gzipped archive contains the <code>Json</code> representations of the objects.</p><p><img src=/__resources/backup-process_08beba.png alt="Backup process"></p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>As of the writing of this tutorial, Velero or any other BR tool for Shoot clusters is not provided by Gardener.</div><h2 id=getting-started>Getting Started</h2><p>At first, clone the <a href=https://github.com/vmware-tanzu/velero-plugin-for-aws>Velero GitHub repository</a> and get the Velero client from the <a href=https://github.com/vmware-tanzu/velero-plugin-for-aws/releases>releases</a> or build it from source via <code>make all</code> in the main directory of the cloned GitHub repository.</p><p>To use an AWS S3 bucket as storage for the backup files and the persistent volumes, you need to:</p><ul><li>create a S3 bucket as the backup target</li><li>create an AWS IAM user for Velero</li><li>configure the Velero server</li><li>create a secret for your AWS credentials</li></ul><p>For details about this setup, check the <a href=https://github.com/vmware-tanzu/velero-plugin-for-aws#set-permissions-for-velero>Set Permissions for Velero</a> documentation. Moreover, it is possible to use other <a href=https://velero.io/docs/main/supported-providers/>supported storage providers</a>.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Per default, Velero is installed in the namespace <code>velero</code>. To change the namespace, check the <a href=https://velero.io/docs/main/namespace/#customize-the-namespace-during-install>documentation</a>.</div><p>Velero offers a wide range of filter possibilities for Kubernetes resources, e.g filter by namespaces, labels or resource types. The filter settings can be combined and used as <em>include</em> or <em>exclude</em>, which gives a great flexibility for selecting resources.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Carefully set labels and/or use namespaces for your deployments to make the selection of the resources to be backed up easier. The best practice would be to check in advance which resources are selected with the defined filter.</div><h2 id=exemplary-use-cases>Exemplary Use Cases</h2><p>Below are some use cases which could give you an idea on how to use Velero. You can also check <a href=https://velero.io/docs/main/>Velero&rsquo;s documentation</a> for other introductory examples.</p><h3 id=helm-based-deployments>Helm Based Deployments</h3><p>To be able to use Helm charts in your Kubernetes cluster, you need to install the Helm client <code>helm</code> and the server component <code>tiller</code>. Per default the server component is installed in the namespace <code>kube-system</code>. Even if it is possible to select single deployments via the filter settings of Velero, you should consider to install <code>tiller</code> in a separate namespace via <code>helm init --tiller-namespace &lt;your namespace></code>. This approach applies as well for all Helm charts to be deployed - consider separate namespaces for your deployments as well by using the parameter <code>--namespace</code>.</p><p>To backup a Helm based deployment, you need to backup both Tiller <em>and</em> the deployment. Only then the deployments could be managed via Helm. As mentioned above, the selection of resources would be easier in case they are separated in namespaces.</p><h3 id=separate-backup-locations>Separate Backup Locations</h3><p>In case you run all your Kubernetes clusters on a single cloud provider, there is probably no need to store the backups in a bucket of a different cloud provider. However, if you run Kubernetes clusters on different cloud provider, you might consider to use a bucket on just one cloud provider as the target for the backups, e.g., to benefit from a lower price tag for the storage.</p><p>Per default, Velero assumes that both the persistent volumes and the backup location are on the same cloud provider. During the setup of Velero, a secret is created using the credentials for a cloud provider user who has access to both objects (see the policies, e.g., for the <a href=https://github.com/vmware-tanzu/velero-plugin-for-aws#set-permissions-for-velero>AWS configuration</a>).</p><p>Now, since the backup location is different from the volume location, you need to follow these steps (described here for AWS):</p><ul><li><p>configure as documented the volume storage location in <code>examples/aws/06-volumesnapshotlocation.yaml</code> and provide the user credentials. In this case, the S3 related settings like the policies can be omitted</p></li><li><p>create the bucket for the backup in the cloud provider in question and a user with the appropriate credentials and store them in a separate file similar to <code>credentials-ark</code></p></li><li><p>create a secret which contains two credentials, one for the volumes and one for the backup target, e.g., by using the command <code>kubectl create secret generic cloud-credentials --namespace heptio-ark --from-file cloud=credentials-ark --from-file backup-target=backup-ark</code></p></li><li><p>configure in the deployment manifest <code>examples/aws/10-deployment.yaml</code> the entries in <code>volumeMounts</code>, <code>env</code> and <code>volumes</code> accordingly, e.g., for a cluster running on AWS and the backup target bucket on GCP a configuration could look similar to:</p><details><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Some links might get broken in the near future since Heptio was taken over by VMWare which might result in different GitHub repositories or other changes. Please don&rsquo;t hesitate to inform us in case you encounter any issues.</div><summary>Example Velero deployment</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green># Copyright 2017 the Heptio Ark contributors.</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span><span style=color:green># Licensed under the Apache License, Version 2.0 (the &#34;License&#34;);</span>
</span></span><span style=display:flex><span><span style=color:green># you may not use this file except in compliance with the License.</span>
</span></span><span style=display:flex><span><span style=color:green># You may obtain a copy of the License at</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span><span style=color:green>#     http://www.apache.org/licenses/LICENSE-2.0</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span><span style=color:green># Unless required by applicable law or agreed to in writing, software</span>
</span></span><span style=display:flex><span><span style=color:green># distributed under the License is distributed on an &#34;AS IS&#34; BASIS,</span>
</span></span><span style=display:flex><span><span style=color:green># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
</span></span><span style=display:flex><span><span style=color:green># See the License for the specific language governing permissions and</span>
</span></span><span style=display:flex><span><span style=color:green># limitations under the License.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: apps/v1beta1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  namespace: velero
</span></span><span style=display:flex><span>  name: velero
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        component: velero
</span></span><span style=display:flex><span>      annotations:
</span></span><span style=display:flex><span>        prometheus.io/scrape: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>        prometheus.io/port: <span style=color:#a31515>&#34;8085&#34;</span>
</span></span><span style=display:flex><span>        prometheus.io/path: <span style=color:#a31515>&#34;/metrics&#34;</span>
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      restartPolicy: Always
</span></span><span style=display:flex><span>      serviceAccountName: velero
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>        - name: velero
</span></span><span style=display:flex><span>          image: gcr.io/heptio-images/velero:latest
</span></span><span style=display:flex><span>          command:
</span></span><span style=display:flex><span>            - /velero
</span></span><span style=display:flex><span>          args:
</span></span><span style=display:flex><span>            - server
</span></span><span style=display:flex><span>          volumeMounts:
</span></span><span style=display:flex><span>            - name: cloud-credentials
</span></span><span style=display:flex><span>              mountPath: /credentials
</span></span><span style=display:flex><span>            - name: plugins
</span></span><span style=display:flex><span>              mountPath: /plugins
</span></span><span style=display:flex><span>            - name: scratch
</span></span><span style=display:flex><span>              mountPath: /scratch
</span></span><span style=display:flex><span>          env:
</span></span><span style=display:flex><span>            - name: AWS_SHARED_CREDENTIALS_FILE
</span></span><span style=display:flex><span>              value: /credentials/cloud
</span></span><span style=display:flex><span>            - name: GOOGLE_APPLICATION_CREDENTIALS
</span></span><span style=display:flex><span>              value: /credentials/backup-target
</span></span><span style=display:flex><span>            - name: VELERO_SCRATCH_DIR
</span></span><span style=display:flex><span>              value: /scratch
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>        - name: cloud-credentials
</span></span><span style=display:flex><span>          secret:
</span></span><span style=display:flex><span>            secretName: cloud-credentials
</span></span><span style=display:flex><span>        - name: plugins
</span></span><span style=display:flex><span>          emptyDir: {}
</span></span><span style=display:flex><span>        - name: scratch
</span></span><span style=display:flex><span>          emptyDir: {}
</span></span></code></pre></div></details></li><li><p>finally, configure the backup storage location in <code>examples/aws/05-backupstoragelocation.yaml</code> to use, in this case, a GCP bucket</p></li></ul><h2 id=limitations>Limitations</h2><p>Below is a potentially incomplete list of limitations. You can also consult <a href=https://velero.io/docs/main/>Velero&rsquo;s documentation</a> to get up to date information.</p><ul><li>Only full backups of selected resources are supported. Incremental backups are not (yet) supported. However, by using filters it is possible to restrict the backup to specific resources</li><li>Inconsistencies might occur in case of changes during the creation of the backup</li><li>Application specific actions are not considered by default. However, they might be handled by using Velero&rsquo;s <a href=https://velero.io/docs/main/backup-hooks/#docs>Hooks</a> or <a href=https://velero.io/docs/main/custom-plugins/#docs>Plugins</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1e813ebc27f8226905b87ae776154c90>4.4 - Create / Delete a Shoot Cluster</h1><h2 id=create-a-shoot-cluster>Create a Shoot Cluster</h2><p>As you have already prepared an <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>example Shoot manifest</a> in the steps described in the development documentation, please open another Terminal pane/window with the <code>KUBECONFIG</code> environment variable pointing to the Garden development cluster and send the manifest to the Kubernetes API server:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f your-shoot-aws.yaml
</span></span></code></pre></div><p>You should see that Gardener has immediately picked up your manifest and has started to deploy the Shoot cluster.</p><p>In order to investigate what is happening in the Seed cluster, please download its proper Kubeconfig yourself (see next paragraph). The namespace of the Shoot cluster in the Seed cluster will look like that: <code>shoot-johndoe-johndoe-1</code>, whereas the first <code>johndoe</code> is your namespace in the Garden cluster (also called &ldquo;project&rdquo;) and the <code>johndoe-1</code> suffix is the actual name of the Shoot cluster.</p><p>To connect to the newly created Shoot cluster, you must download its Kubeconfig as well. Please connect to the proper Seed cluster, navigate to the Shoot namespace, and download the Kubeconfig from the <code>kubecfg</code> secret in that namespace.</p><h2 id=delete-a-shoot-cluster>Delete a Shoot Cluster</h2><p>In order to delete your cluster, you have to set an annotation confirming the deletion first, and trigger the deletion after that. You can use the prepared <code>delete shoot</code> script which takes the Shoot name as first parameter. The namespace can be specified by the second parameter, but it is optional. If you don&rsquo;t state it, it defaults to your namespace (the username you are logged in with to your machine).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ ./hack/usage/delete shoot johndoe-1 johndoe
</span></span></code></pre></div><p>(the <code>hack</code> bash script can be found at <a href=https://github.com/gardener/gardener/blob/master/hack/usage/delete>GitHub</a>)</p><h1 id=configure-a-shoot-cluster-alert-receiver>Configure a Shoot cluster alert receiver</h1><p>The receiver of the Shoot alerts can be configured from the <code>.spec.monitoring.alerting.emailReceivers</code> section in the Shoot specification. The value of the field has to be a list of valid mail addresses.</p><p>The alerting for the Shoot clusters is handled by the Prometheus Alertmanager. The Alertmanager will be deployed next to the control plane when the <code>Shoot</code> resource specifies <code>.spec.monitoring.alerting.emailReceivers</code> and if a <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-alerting.yaml>SMTP secret</a> exists.</p><p>If the field gets removed then the Alertmanager will be also removed during the next reconcilation of the cluster. The opposite is also valid if the field is added to an existing cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3e14557e3c5a7bfd7defc224b1e6eefc>4.5 - Create a Shoot Cluster Into an Existing AWS VPC</h1><h2 id=overview>Overview</h2><p>Gardener can create a new VPC, or use an existing one for your shoot cluster. Depending on your needs, you may want to create shoot(s) into an already created VPC.
The tutorial describes how to create a shoot cluster into an existing AWS VPC. The steps are identical for Alicloud, Azure, and GCP. Please note that the existing VPC must be in the same region like the shoot cluster that you want to deploy into the VPC.</p><h2 id=tldr>TL;DR</h2><p>If <code>.spec.provider.infrastructureConfig.networks.vpc.cidr</code> is specified, Gardener will create a new VPC with the given CIDR block and respectively will delete it on shoot deletion.<br>If <code>.spec.provider.infrastructureConfig.networks.vpc.id</code> is specified, Gardener will use the existing VPC and respectively won&rsquo;t delete it on shoot deletion.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4><p>It&rsquo;s not recommended to create a shoot cluster into a VPC that is managed by Gardener (that is created for another shoot cluster). In this case the deletion of the initial shoot cluster will fail to delete the VPC because there will be resources attached to it.</p><p>Gardener won&rsquo;t delete any manually created (unmanaged) resources in your cloud provider account.</p></div><h2 id=1-configure-the-aws-cli>1. Configure the AWS CLI</h2><p>The <code>aws configure</code> command is a convenient way to setup your AWS CLI. It will prompt you for your credentials and settings which will be used in the following AWS CLI invocations:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ aws configure
</span></span><span style=display:flex><span>AWS Access Key ID [None]: &lt;ACCESS_KEY_ID&gt;
</span></span><span style=display:flex><span>AWS Secret Access Key [None]: &lt;SECRET_ACCESS_KEY&gt;
</span></span><span style=display:flex><span>Default region name [None]: &lt;DEFAULT_REGION&gt;
</span></span><span style=display:flex><span>Default output format [None]: &lt;DEFAULT_OUTPUT_FORMAT&gt;
</span></span></code></pre></div><h2 id=2-create-a-vpc>2. Create a VPC</h2><p>Create the VPC by running the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ aws ec2 create-vpc --cidr-block &lt;cidr-block&gt;
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#a31515>&#34;Vpc&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;VpcId&#34;</span>: <span style=color:#a31515>&#34;vpc-ff7bbf86&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;InstanceTenancy&#34;</span>: <span style=color:#a31515>&#34;default&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;Tags&#34;</span>: [],
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;CidrBlockAssociations&#34;</span>: [
</span></span><span style=display:flex><span>          {
</span></span><span style=display:flex><span>              <span style=color:#a31515>&#34;AssociationId&#34;</span>: <span style=color:#a31515>&#34;vpc-cidr-assoc-6e42b505&#34;</span>,
</span></span><span style=display:flex><span>              <span style=color:#a31515>&#34;CidrBlock&#34;</span>: <span style=color:#a31515>&#34;10.0.0.0/16&#34;</span>,
</span></span><span style=display:flex><span>              <span style=color:#a31515>&#34;CidrBlockState&#34;</span>: {
</span></span><span style=display:flex><span>                  <span style=color:#a31515>&#34;State&#34;</span>: <span style=color:#a31515>&#34;associated&#34;</span>
</span></span><span style=display:flex><span>              }
</span></span><span style=display:flex><span>          }
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;Ipv6CidrBlockAssociationSet&#34;</span>: [],
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;State&#34;</span>: <span style=color:#a31515>&#34;pending&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;DhcpOptionsId&#34;</span>: <span style=color:#a31515>&#34;dopt-38f7a057&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;CidrBlock&#34;</span>: <span style=color:#a31515>&#34;10.0.0.0/16&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#a31515>&#34;IsDefault&#34;</span>: false
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Gardener requires the VPC to have enabled <a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html>DNS support</a>, i.e the attributes <code>enableDnsSupport</code> and <code>enableDnsHostnames</code> must be set to <em>true</em>. <code>enableDnsSupport</code> attribute is enabled by default, <code>enableDnsHostnames</code> - not. Set the <code>enableDnsHostnames</code> attribute to <em>true</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ aws ec2 modify-vpc-attribute --vpc-id vpc-ff7bbf86 --enable-dns-hostnames
</span></span></code></pre></div><h2 id=3-create-an-internet-gateway>3. Create an Internet Gateway</h2><p>Gardener also requires that an internet gateway is attached to the VPC. You can create one by using:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ aws ec2 create-internet-gateway
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;InternetGateway&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;Tags&#34;</span>: [],
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;InternetGatewayId&#34;</span>: <span style=color:#a31515>&#34;igw-c0a643a9&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;Attachments&#34;</span>: []
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>and attach it to the VPC using:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ aws ec2 attach-internet-gateway --internet-gateway-id igw-c0a643a9 --vpc-id vpc-ff7bbf86
</span></span></code></pre></div><h2 id=4-create-the-shoot>4. Create the Shoot</h2><p>Prepare your shoot manifest (you could check the <a href=https://github.com/gardener/gardener/tree/master/example>example manifests</a>). Please make sure that you choose the region in which you had created the VPC earlier (step 2). Also, put your VPC ID in the <code>.spec.provider.infrastructureConfig.networks.vpc.id</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  region: &lt;aws-region-of-vpc&gt;
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          id: vpc-ff7bbf86
</span></span><span style=display:flex><span>    <span style=color:green># ...</span>
</span></span></code></pre></div><p>Apply your shoot manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f your-shoot-aws.yaml
</span></span></code></pre></div><p>Ensure that the shoot cluster is properly created:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get shoot $SHOOT_NAME -n $SHOOT_NAMESPACE
</span></span><span style=display:flex><span>NAME           CLOUDPROFILE   VERSION   SEED   DOMAIN           OPERATION   PROGRESS   APISERVER   CONTROL   NODES   SYSTEM   AGE
</span></span><span style=display:flex><span>&lt;SHOOT_NAME&gt;   aws            1.15.0    aws    &lt;SHOOT_DOMAIN&gt;   Succeeded   100        True        True      True    True     20m
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-1d36f4dd0527e378c014bd87c14c0c59>4.6 - Fix Problematic Conversion Webhooks</h1><h2 id=reasoning>Reasoning</h2><p><strong>Custom Resource Definition (CRD)</strong> is what you use to define a <code>Custom Resource</code>. This is a powerful way to extend Kubernetes capabilities beyond the default installation, adding any kind of API objects useful for your application.</p><p>The CustomResourceDefinition API provides a workflow for introducing and upgrading to new versions of a CustomResourceDefinition. In a scenario where a CRD adds support for a new version and switches its <code>spec.versions.storage</code> field to it (i.e., from <code>v1beta1</code> to <code>v1)</code>, existing objects are not migrated in etcd. For more information, see <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#previous-storage-versions>Versions in CustomResourceDefinitions</a>.</p><p>This creates a mismatch between the requested and stored version for all clients (kubectl, KCM, etc.). When the CRD also declares the usage of a conversion webhook, it gets called whenever a client requests information about a resource that still exists in the old version. If the CRD is created by the end-user, the webhook runs on the shoot side, whereas controllers / kapi-servers run separated, as part of the control-plane. For the webhook to be reachable, a working VPN connection <code>seed -> shoot</code> is essential. In scenarios where the VPN connection is broken, the <strong>kube-controller-manager</strong> eventually stops its garbage collection, as that requires it to list <code>v1.PartialObjectMetadata</code> for everything to build a dependency graph. Without the kube-controller-manager&rsquo;s garbage collector, managed resources get stuck during update/rollout.</p><h2 id=breaking-situations>Breaking Situations</h2><p>When a user upgrades to <code>failureTolerance: node|zone</code>, that will cause the VPN <strong>deployments</strong> to be <em>replaced</em> by <strong>statefulsets</strong>. However, as the VPN connection is broken upon teardown of the deployment, garbage collection will fail, leading to a situation that is stuck until an operator manually tackles it.</p><p>Such a situation can be avoided if the end-user has correctly configured CRDs containing conversion webhooks.</p><h2 id=checking-problematic-crds>Checking Problematic CRDs</h2><p>In order to make sure there are no version problematic CRDs, please run the script below in your shoot. It will return the name of the CRDs in case they have one of the 2 problems:</p><ul><li>the returned version of the CR is different than what is maintained in the <code>status.storedVersions</code> field of the CRD.</li><li>the <code>status.storedVersions</code> field of the CRD has more than 1 version defined.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#00f>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#00f></span>
</span></span><span style=display:flex><span>set -e -o pipefail
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#a31515>&#34;Checking all CRDs in the cluster...&#34;</span>
</span></span><span style=display:flex><span><span style=color:#00f>for</span> p in <span style=color:#00f>$(</span>kubectl get crd | awk <span style=color:#a31515>&#39;NR&gt;1&#39;</span> | awk <span style=color:#a31515>&#39;{print $1}&#39;</span><span style=color:#00f>)</span>; <span style=color:#00f>do</span>
</span></span><span style=display:flex><span>  strategy=<span style=color:#00f>$(</span>kubectl get crd <span style=color:#a31515>&#34;</span>$p<span style=color:#a31515>&#34;</span> -o json | jq -r .spec.conversion.strategy<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#00f>if</span> [ <span style=color:#a31515>&#34;</span>$strategy<span style=color:#a31515>&#34;</span> == <span style=color:#a31515>&#34;Webhook&#34;</span> ]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>     crd_name=<span style=color:#00f>$(</span>kubectl get crd <span style=color:#a31515>&#34;</span>$p<span style=color:#a31515>&#34;</span> -o json | jq -r .metadata.name<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>     number_of_stored_versions=<span style=color:#00f>$(</span>kubectl get crd <span style=color:#a31515>&#34;</span>$crd_name<span style=color:#a31515>&#34;</span> -o json  | jq <span style=color:#a31515>&#39;.status.storedVersions | length&#39;</span><span style=color:#00f>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#00f>if</span> [[ <span style=color:#a31515>&#34;</span>$number_of_stored_versions<span style=color:#a31515>&#34;</span> == 1 ]]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>         returned_cr_version=<span style=color:#00f>$(</span>kubectl get <span style=color:#a31515>&#34;</span>$crd_name<span style=color:#a31515>&#34;</span> -A -o json |  jq -r <span style=color:#a31515>&#39;.items[] | .apiVersion&#39;</span>  | sed <span style=color:#a31515>&#39;s:.*/::&#39;</span><span style=color:#00f>)</span>
</span></span><span style=display:flex><span>         <span style=color:#00f>if</span> [ -z <span style=color:#a31515>&#34;</span>$returned_cr_version<span style=color:#a31515>&#34;</span> ]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>           <span style=color:#00f>continue</span>
</span></span><span style=display:flex><span>         <span style=color:#00f>else</span>
</span></span><span style=display:flex><span>           variable=<span style=color:#00f>$(</span>echo <span style=color:#a31515>&#34;</span>$returned_cr_version<span style=color:#a31515>&#34;</span> | xargs -n1 | sort -u | xargs<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>           present_version=<span style=color:#00f>$(</span>kubectl get crd <span style=color:#a31515>&#34;</span>$crd_name<span style=color:#a31515>&#34;</span> -o json  |  jq -cr <span style=color:#a31515>&#39;.status.storedVersions |.[]&#39;</span><span style=color:#00f>)</span>
</span></span><span style=display:flex><span>           <span style=color:#00f>if</span> [[ $variable != <span style=color:#a31515>&#34;</span>$present_version<span style=color:#a31515>&#34;</span> ]]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>             echo <span style=color:#a31515>&#34;ERROR: Stored version differs from the version that CRs are being returned. </span>$crd_name<span style=color:#a31515> with conversion webhook needs to be fixed&#34;</span>
</span></span><span style=display:flex><span>           <span style=color:#00f>fi</span>
</span></span><span style=display:flex><span>         <span style=color:#00f>fi</span>
</span></span><span style=display:flex><span>      <span style=color:#00f>fi</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#00f>if</span> [[ <span style=color:#a31515>&#34;</span>$number_of_stored_versions<span style=color:#a31515>&#34;</span> -gt 1 ]]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>         returned_cr_version=<span style=color:#00f>$(</span>kubectl get <span style=color:#a31515>&#34;</span>$crd_name<span style=color:#a31515>&#34;</span> -A -o json |  jq -r <span style=color:#a31515>&#39;.items[] | .apiVersion&#39;</span>  | sed <span style=color:#a31515>&#39;s:.*/::&#39;</span><span style=color:#00f>)</span>
</span></span><span style=display:flex><span>         <span style=color:#00f>if</span> [ -z <span style=color:#a31515>&#34;</span>$returned_cr_version<span style=color:#a31515>&#34;</span> ]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>           <span style=color:#00f>continue</span>
</span></span><span style=display:flex><span>         <span style=color:#00f>else</span>
</span></span><span style=display:flex><span>           echo <span style=color:#a31515>&#34;ERROR: Too many stored versions defined. </span>$crd_name<span style=color:#a31515> with conversion webhook needs to be fixed&#34;</span>
</span></span><span style=display:flex><span>         <span style=color:#00f>fi</span>
</span></span><span style=display:flex><span>      <span style=color:#00f>fi</span>
</span></span><span style=display:flex><span>  <span style=color:#00f>fi</span>
</span></span><span style=display:flex><span><span style=color:#00f>done</span>
</span></span><span style=display:flex><span>echo <span style=color:#a31515>&#34;Problematic CRDs are reported above.&#34;</span>
</span></span></code></pre></div><h1 id=resolve-crds>Resolve CRDs</h1><p>Below we give the steps needed to be taken in order to fix the CRDs reported by the script above.</p><p>Inspect all your CRDs that have conversion webhooks in place. If you have more than 1 version defined in its <code>spec.status.storedVersions</code> field, then initiate migration as described in <strong>Option 2</strong> in the <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#upgrade-existing-objects-to-a-new-stored-version>Upgrade existing objects to a new stored version</a> guide.</p><p>For convenience, we have provided the necessary steps below.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Please test the following steps on a non-productive landscape to make sure that the new CR version doesn’t break any of your existing workloads.</div><ol><li><p>Please check/set the old CR version to <code>storage:false</code> and set the new CR version to <code>storage:true</code>.</p><p>For the sake of an example, let’s consider the two versions <code>v1beta1</code> (old) and <code>v1</code> (new).</p><p>Before:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>versions:
</span></span><span style=display:flex><span>- name: v1beta1
</span></span><span style=display:flex><span>......
</span></span><span style=display:flex><span>storage: true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>- name: v1
</span></span><span style=display:flex><span>......
</span></span><span style=display:flex><span>storage: false
</span></span></code></pre></div><p>After:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>versions:
</span></span><span style=display:flex><span>- name: v1beta1
</span></span><span style=display:flex><span>......
</span></span><span style=display:flex><span>storage: false
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>- name: v1
</span></span><span style=display:flex><span>......
</span></span><span style=display:flex><span>storage: true
</span></span></code></pre></div></li><li><p>Convert <code>custom-resources</code> to the newest version.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get &lt;custom-resource-name&gt; -A -ojson | k apply -f -
</span></span></code></pre></div></li><li><p>Patch the CRD to keep only the latest version under storedVersions.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl patch customresourcedefinitions &lt;crd-name&gt; --subresource=<span style=color:#a31515>&#39;status&#39;</span> --type=<span style=color:#a31515>&#39;merge&#39;</span> -p <span style=color:#a31515>&#39;{&#34;status&#34;:{&#34;storedVersions&#34;:[&#34;your-latest-cr-version&#34;]}}&#39;</span>
</span></span></code></pre></div></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-8b7023e0cf4c0cede29db9e4d2493f97>4.7 - GPU Enabled Cluster</h1><div class=lead>Setting up a GPU Enabled Cluster for Deep Learning</div><h2 id=disclaimer>Disclaimer</h2><p>Be aware, that the following sections might be opinionated. Kubernetes, and the GPU support in particular,
are rapidly evolving, which means that this guide is likely to be outdated sometime soon. For this reason,
<strong>contributions are highly appreciated</strong> to update this guide.</p><h2 id=create-a-cluster>Create a Cluster</h2><p>First thing first, let’s create a Kubernetes (K8s) cluster with GPU accelerated nodes. In this example we will use an AWS
<strong>p2.xlarge</strong> EC2 instance because it&rsquo;s the cheapest available option at the moment. Use such cheap instances
for learning to limit your resource costs. <strong>This costs around 1€/hour per GPU</strong></p><p><img src=/__resources/howto-gpu_a719bf.png alt=gpu-selection></p><h2 id=install-nvidia-driver-as-daemonset>Install NVidia Driver as Daemonset</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: DaemonSet
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nvidia-driver-installer
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    k8s-app: nvidia-driver-installer
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      name: nvidia-driver-installer
</span></span><span style=display:flex><span>      k8s-app: nvidia-driver-installer
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        name: nvidia-driver-installer
</span></span><span style=display:flex><span>        k8s-app: nvidia-driver-installer
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      hostPID: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      initContainers:
</span></span><span style=display:flex><span>      - image: squat/modulus:4a1799e7aa0143bcbb70d354bab3e419b1f54972
</span></span><span style=display:flex><span>        name: modulus
</span></span><span style=display:flex><span>        args:
</span></span><span style=display:flex><span>        - compile
</span></span><span style=display:flex><span>        - nvidia
</span></span><span style=display:flex><span>        - <span style=color:#a31515>&#34;410.104&#34;</span>
</span></span><span style=display:flex><span>        securityContext:
</span></span><span style=display:flex><span>          privileged: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        env:
</span></span><span style=display:flex><span>        - name: MODULUS_CHROOT
</span></span><span style=display:flex><span>          value: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>        - name: MODULUS_INSTALL
</span></span><span style=display:flex><span>          value: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>        - name: MODULUS_INSTALL_DIR
</span></span><span style=display:flex><span>          value: /opt/drivers
</span></span><span style=display:flex><span>        - name: MODULUS_CACHE_DIR
</span></span><span style=display:flex><span>          value: /opt/modulus/cache
</span></span><span style=display:flex><span>        - name: MODULUS_LD_ROOT
</span></span><span style=display:flex><span>          value: /root
</span></span><span style=display:flex><span>        - name: IGNORE_MISSING_MODULE_SYMVERS
</span></span><span style=display:flex><span>          value: <span style=color:#a31515>&#34;1&#34;</span>          
</span></span><span style=display:flex><span>        volumeMounts:
</span></span><span style=display:flex><span>        - name: etc-coreos
</span></span><span style=display:flex><span>          mountPath: /etc/coreos
</span></span><span style=display:flex><span>          readOnly: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - name: usr-share-coreos
</span></span><span style=display:flex><span>          mountPath: /usr/share/coreos
</span></span><span style=display:flex><span>          readOnly: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - name: ld-root
</span></span><span style=display:flex><span>          mountPath: /root
</span></span><span style=display:flex><span>        - name: module-cache
</span></span><span style=display:flex><span>          mountPath: /opt/modulus/cache
</span></span><span style=display:flex><span>        - name: module-install-dir-base
</span></span><span style=display:flex><span>          mountPath: /opt/drivers
</span></span><span style=display:flex><span>        - name: dev
</span></span><span style=display:flex><span>          mountPath: /dev
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - image: <span style=color:#a31515>&#34;gcr.io/google-containers/pause:3.1&#34;</span>
</span></span><span style=display:flex><span>        name: pause
</span></span><span style=display:flex><span>      tolerations:
</span></span><span style=display:flex><span>      - key: <span style=color:#a31515>&#34;nvidia.com/gpu&#34;</span>
</span></span><span style=display:flex><span>        effect: <span style=color:#a31515>&#34;NoSchedule&#34;</span>
</span></span><span style=display:flex><span>        operator: <span style=color:#a31515>&#34;Exists&#34;</span>
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>      - name: etc-coreos
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /etc/coreos
</span></span><span style=display:flex><span>      - name: usr-share-coreos
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /usr/share/coreos
</span></span><span style=display:flex><span>      - name: ld-root
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /
</span></span><span style=display:flex><span>      - name: module-cache
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /opt/modulus/cache
</span></span><span style=display:flex><span>      - name: dev
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /dev
</span></span><span style=display:flex><span>      - name: module-install-dir-base
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /opt/drivers
</span></span></code></pre></div><h2 id=install-device-plugin>Install Device Plugin</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: DaemonSet
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    k8s-app: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>    <span style=color:green>#addonmanager.kubernetes.io/mode: Reconcile</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      k8s-app: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        k8s-app: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>      annotations:
</span></span><span style=display:flex><span>        scheduler.alpha.kubernetes.io/critical-pod: <span style=color:#a31515>&#39;&#39;</span>
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      priorityClassName: system-node-critical
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>      - name: device-plugin
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /var/lib/kubelet/device-plugins
</span></span><span style=display:flex><span>      - name: dev
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /dev
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - image: <span style=color:#a31515>&#34;k8s.gcr.io/nvidia-gpu-device-plugin@sha256:08509a36233c5096bb273a492251a9a5ca28558ab36d74007ca2a9d3f0b61e1d&#34;</span>
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/usr/bin/nvidia-gpu-device-plugin&#34;</span>, <span style=color:#a31515>&#34;-logtostderr&#34;</span>, <span style=color:#a31515>&#34;-host-path=/opt/drivers/nvidia&#34;</span>]
</span></span><span style=display:flex><span>        name: nvidia-gpu-device-plugin
</span></span><span style=display:flex><span>        resources:
</span></span><span style=display:flex><span>          requests:
</span></span><span style=display:flex><span>            cpu: 50m
</span></span><span style=display:flex><span>            memory: 10Mi
</span></span><span style=display:flex><span>          limits:
</span></span><span style=display:flex><span>            cpu: 50m
</span></span><span style=display:flex><span>            memory: 10Mi
</span></span><span style=display:flex><span>        securityContext:
</span></span><span style=display:flex><span>          privileged: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        volumeMounts:
</span></span><span style=display:flex><span>        - name: device-plugin
</span></span><span style=display:flex><span>          mountPath: /device-plugin
</span></span><span style=display:flex><span>        - name: dev
</span></span><span style=display:flex><span>          mountPath: /dev
</span></span><span style=display:flex><span>  updateStrategy:
</span></span><span style=display:flex><span>    type: RollingUpdate
</span></span></code></pre></div><h2 id=test>Test</h2><p>To run an example training on a GPU node, first start a base image with Tensorflow with GPU support & Keras:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: deeplearning-workbench
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: deeplearning-workbench
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: deeplearning-workbench
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: deeplearning-workbench
</span></span><span style=display:flex><span>        image: afritzler/deeplearning-workbench
</span></span><span style=display:flex><span>        resources:
</span></span><span style=display:flex><span>          limits:
</span></span><span style=display:flex><span>            nvidia.com/gpu: 1
</span></span><span style=display:flex><span>      tolerations:
</span></span><span style=display:flex><span>      - key: <span style=color:#a31515>&#34;nvidia.com/gpu&#34;</span>
</span></span><span style=display:flex><span>        effect: <span style=color:#a31515>&#34;NoSchedule&#34;</span>
</span></span><span style=display:flex><span>        operator: <span style=color:#a31515>&#34;Exists&#34;</span>
</span></span></code></pre></div><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4><p>the <code>tolerations</code> section above is not required if you deploy the <code>ExtendedResourceToleration</code>
admission controller to your cluster. You can do this in the <code>kubernetes</code> section of your Gardener
cluster <code>shoot.yaml</code> as follows:</p><pre tabindex=0><code>  kubernetes:
    kubeAPIServer:
      admissionPlugins:
      - name: ExtendedResourceToleration
</code></pre></div><p>Now exec into the container and start an example Keras training:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl exec -it deeplearning-workbench-8676458f5d-p4d2v -- /bin/bash
</span></span><span style=display:flex><span>cd /keras/example
</span></span><span style=display:flex><span>python imdb_cnn.py
</span></span></code></pre></div><h2 id=related-links>Related Links</h2><ul><li><a href=https://github.com/afritzler/kubernetes-gpu>Andreas Fritzler</a> from the Gardener Core team for the R&D, who has provided this setup.</li><li><a href=https://github.com/squat/modulus>Build and install NVIDIA driver on CoreOS</a></li><li><a href=https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/device-plugins/nvidia-gpu/daemonset.yaml>Nvidia Device Plugin</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e61f5c2e9c4734848ebe1281078994f0>4.8 - Shoot Cluster Maintenance</h1><div class=lead>Understanding and configuring Gardener&rsquo;s Day-2 operations for Shoot clusters.</div><h2 id=overview>Overview</h2><p>Day two operations for shoot clusters are related to:</p><ul><li>The Kubernetes version of the control plane and the worker nodes</li><li>The operating system version of the worker nodes</li></ul><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>When referring to an update of the &ldquo;operating system version&rdquo; in this document, the update of the machine image of the shoot cluster&rsquo;s worker nodes is meant. For example, Amazon Machine Images (AMI) for AWS.</div><p>The following table summarizes what options Gardener offers to maintain these versions:</p><table><thead><tr><th style=text-align:left></th><th style=text-align:left>Auto-Update</th><th style=text-align:left>Forceful Updates</th><th style=text-align:left>Manual Updates</th></tr></thead><tbody><tr><td style=text-align:left>Kubernetes version</td><td style=text-align:left>Patches only</td><td style=text-align:left>Patches and consecutive minor updates only</td><td style=text-align:left>yes</td></tr><tr><td style=text-align:left>Operating system version</td><td style=text-align:left>yes</td><td style=text-align:left>yes</td><td style=text-align:left>yes</td></tr></tbody></table><h2 id=allowed-target-versions-in-the-cloudprofile>Allowed Target Versions in the <code>CloudProfile</code></h2><p>Administrators maintain the allowed target versions that you can update to in the <code>CloudProfile</code> for each IaaS-Provider. Users with access to a Gardener project can check supported target versions with:</p><pre tabindex=0><code>kubectl get cloudprofile [IAAS-SPECIFIC-PROFILE] -o yaml
</code></pre><table><thead><tr><th style=text-align:left>Path</th><th style=text-align:left>Description</th><th style=text-align:left>More Information</th></tr></thead><tbody><tr><td style=text-align:left><code>spec.kubernetes.versions</code></td><td style=text-align:left>The supported Kubernetes version <code>major.minor.patch</code>.</td><td style=text-align:left><a href=https://github.com/kubernetes/design-proposals-archive/blob/main/release/versioning.md#patch-releases>Patch releases</a></td></tr><tr><td style=text-align:left><code>spec.machineImages</code></td><td style=text-align:left>The supported operating system versions for worker nodes</td><td style=text-align:left></td></tr></tbody></table><p>Both the Kubernetes version and the operating system version follow semantic versioning that allows Gardener to handle updates automatically.</p><p>For more information, see <a href=http://semver.org/>Semantic Versioning</a>.</p><h3 id=impact-of-version-classifications-on-updates>Impact of Version Classifications on Updates</h3><p>Gardener allows to classify versions in the <code>CloudProfile</code> as <code>preview</code>, <code>supported</code>, <code>deprecated</code>, or <code>expired</code>. During maintenance operations, <code>preview</code> versions are excluded from updates, because they’re often recently released versions that haven’t yet undergone thorough testing and may contain bugs or security issues.</p><p>For more information, see <a href=/docs/gardener/shoot_versions/#version-classifications>Version Classifications</a>.</p><h2 id=let-gardener-manage-your-updates>Let Gardener Manage Your Updates</h2><h3 id=the-maintenance-window>The Maintenance Window</h3><p>Gardener can manage updates for you automatically. It offers users to specify a <em>maintenance window</em> during which updates are scheduled:</p><ul><li>The time interval of the maintenance window can’t be less than 30 minutes or more than 6 hours.</li><li>If there’s no maintenance window specified during the creation of a shoot cluster, Gardener chooses a maintenance window randomly to spread the load.</li></ul><p>You can either specify the maintenance window in the shoot cluster specification (<code>.spec.maintenance.timeWindow</code>) or the start time of the maintenance window using the Gardener dashboard (<strong>CLUSTERS</strong> > <strong>[YOUR-CLUSTER]</strong> > <strong>OVERVIEW</strong> > <strong>Lifecycle</strong> > <strong>Maintenance</strong>).</p><h3 id=auto-update-and-forceful-updates>Auto-Update and Forceful Updates</h3><p>To trigger updates during the maintenance window automatically, Gardener offers the following methods:</p><ul><li><p><em>Auto-update</em>:<br>Gardener starts an update during the next maintenance window whenever there’s a version available in the <code>CloudProfile</code> that is higher than the one of your shoot cluster specification, and that isn’t classified as <code>preview</code> version. For Kubernetes versions, auto-update only updates to higher patch levels.</p><p>You can either activate auto-update on the Gardener dashboard (<strong>CLUSTERS</strong> > <strong>[YOUR-CLUSTER]</strong> > <strong>OVERVIEW</strong> > <strong>Lifecycle</strong> > <strong>Maintenance</strong>) or in the shoot cluster specification:</p><ul><li><code>.spec.maintenance.autoUpdate.kubernetesVersion: true</code></li><li><code>.spec.maintenance.autoUpdate.machineImageVersion: true</code></li></ul></li><li><p><em>Forceful updates</em>:<br>In the maintenance window, Gardener compares the current version given in the shoot cluster specification with the version list in the <code>CloudProfile</code>. If the version has an expiration date and if the date is before the start of the maintenance window, Gardener starts an update to the highest version available in the <code>CloudProfile</code> that isn’t classified as <code>preview</code> version. The highest version in <code>CloudProfile</code> can’t have an expiration date. For Kubernetes versions, Gardener only updates to higher patch levels or consecutive minor versions.</p></li></ul><p>If you don’t want to wait for the next maintenance window, you can annotate the shoot cluster specification with <code>shoot.gardener.cloud/operation: maintain</code>. Gardener then checks immediately if there’s an auto-update or a forceful update needed.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Forceful version updates are executed even if the auto-update for the Kubernetes version(or the auto-update for the machine image version) is deactivated (set to <code>false</code>).</div><p>With expiration dates, administrators can give shoot cluster owners more time for testing before the actual version update happens, which allows for smoother transitions to new versions.</p><h3 id=kubernetes-update-paths>Kubernetes Update Paths</h3><p>The bigger the delta of the Kubernetes source version and the Kubernetes target version, the better it must be planned and executed by operators. Gardener only provides automatic support for updates that can be applied safely to the cluster workload:</p><table><thead><tr><th style=text-align:left>Update Type</th><th style=text-align:left>Example</th><th style=text-align:left>Update Method</th></tr></thead><tbody><tr><td style=text-align:left>Patches</td><td style=text-align:left><code>1.10.12</code> to <code>1.10.13</code></td><td style=text-align:left>auto-update or Forceful update</td></tr><tr><td style=text-align:left>Update to consecutive minor version</td><td style=text-align:left><code>1.10.12</code> to <code>1.11.10</code></td><td style=text-align:left>Forceful update</td></tr><tr><td style=text-align:left>Other</td><td style=text-align:left><code>1.10.12</code> to <code>1.12.0</code></td><td style=text-align:left>Manual update</td></tr></tbody></table><p>Gardener doesn’t support automatic updates of nonconsecutive minor versions, because Kubernetes doesn’t guarantee updateability in this case. However, multiple minor version updates are possible if not only the minor source version is expired, but also the minor target version is expired. Gardener then updates the Kubernetes version first to the expired target version, and waits for the next maintenance window to update this version to the next minor target version.</p><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>The administrator who maintains the <code>CloudProfile</code> has to ensure that the list of Kubernetes versions consists of consecutive minor versions, for example, from <code>1.10.x</code> to <code>1.11.y</code>. If the minor version increases in bigger steps, for example, from <code>1.10.x</code> to <code>1.12.y</code>, then the shoot cluster updates will fail during the maintenance window.</div><h2 id=manual-updates>Manual Updates</h2><p>To update the Kubernetes version or the node operating system manually, change the <code>.spec.kubernetes.version</code> field or the <code>.spec.provider.workers.machine.image.version</code> field correspondingly.</p><p>Manual updates are required if you would like to do a minor update of the Kubernetes version. Gardener doesn’t do such updates automatically, as they can have breaking changes that could impact the cluster workload.</p><p>Manual updates are either executed immediately (default) or can be confined to the maintenance time window.<br>Choosing the latter option causes changes to the cluster (for example, node pool rolling-updates) and the subsequent reconciliation to only predictably happen during a defined time window (available since <a href=https://github.com/gardener/gardener/releases/tag/v1.4.0>Gardener version 1.4</a>).</p><p>For more information, see <a href=/docs/gardener/shoot_maintenance/#confine-specification-changesupdates-roll-out>Confine Specification Changes/Update Roll Out</a>.</p><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>Before applying such an update on minor or major releases, operators should check for all the breaking changes introduced in the target Kubernetes release changelog.</div><h2 id=examples>Examples</h2><p>In the examples for the <code>CloudProfile</code> and the shoot cluster specification, only the fields relevant for the example are shown.</p><h3 id=auto-update-of-kubernetes-version>Auto-Update of Kubernetes Version</h3><p>Let&rsquo;s assume that the Kubernetes versions <code>1.10.5</code> and <code>1.11.0</code> were added in the following <code>CloudProfile</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.11.0
</span></span><span style=display:flex><span>    - version: 1.10.5
</span></span><span style=display:flex><span>    - version: 1.10.0
</span></span></code></pre></div><p>Before this change, the shoot cluster specification looked like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.10.0
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0000
</span></span><span style=display:flex><span>      end: 230000+0000
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>As a consequence, the shoot cluster is updated to Kubernetes version <code>1.10.5</code> between 22:00-23:00 UTC. Your shoot cluster isn&rsquo;t updated automatically to <code>1.11.0</code>, even though it&rsquo;s the highest Kubernetes version in the <code>CloudProfile</code>, because Gardener only does automatic updates of the Kubernetes patch level.</p><h3 id=forceful-update-due-to-expired-kubernetes-version>Forceful Update Due to Expired Kubernetes Version</h3><p>Let&rsquo;s assume the following <code>CloudProfile</code> exists on the cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.12.8
</span></span><span style=display:flex><span>    - version: 1.11.10
</span></span><span style=display:flex><span>    - version: 1.10.13
</span></span><span style=display:flex><span>    - version: 1.10.12
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2019-04-13T08:00:00Z&#34;</span>
</span></span></code></pre></div><p>Let&rsquo;s assume the shoot cluster has the following specification:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.10.12
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0100
</span></span><span style=display:flex><span>      end: 230000+0100
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The shoot cluster specification refers to a Kubernetes version that has an <code>expirationDate</code>. In the maintenance window on <code>2019-04-12</code>, the Kubernetes version stays the same as it’s still not expired. But in the maintenance window on <code>2019-04-14</code>, the Kubernetes version of the shoot cluster is updated to <code>1.10.13</code> (independently of the value of <code>.spec.maintenance.autoUpdate.kubernetesVersion</code>).</p><h3 id=forceful-update-to-new-minor-kubernetes-version>Forceful Update to New Minor Kubernetes Version</h3><p>Let&rsquo;s assume the following <code>CloudProfile</code> exists on the cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.12.8
</span></span><span style=display:flex><span>    - version: 1.11.10
</span></span><span style=display:flex><span>    - version: 1.11.09
</span></span><span style=display:flex><span>    - version: 1.10.12
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2019-04-13T08:00:00Z&#34;</span>
</span></span></code></pre></div><p>Let&rsquo;s assume the shoot cluster has the following specification:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.10.12
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0100
</span></span><span style=display:flex><span>      end: 230000+0100
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The shoot cluster specification refers a Kubernetes version that has an <code>expirationDate</code>. In the maintenance window on <code>2019-04-14</code>, the Kubernetes version of the shoot cluster is updated to <code>1.11.10</code>, which is the highest patch version of minor target version <code>1.11</code> that follows the source version <code>1.10</code>.</p><h3 id=automatic-update-from-expired-machine-image-version>Automatic Update from Expired Machine Image Version</h3><p>Let&rsquo;s assume the following <code>CloudProfile</code> exists on the cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2191.5.0
</span></span><span style=display:flex><span>    - version: 2191.4.1
</span></span><span style=display:flex><span>    - version: 2135.6.0
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2019-04-13T08:00:00Z&#34;</span>
</span></span></code></pre></div><p>Let&rsquo;s assume the shoot cluster has the following specification:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: name
</span></span><span style=display:flex><span>      maximum: 1
</span></span><span style=display:flex><span>      minimum: 1
</span></span><span style=display:flex><span>      maxSurge: 1
</span></span><span style=display:flex><span>      maxUnavailable: 0
</span></span><span style=display:flex><span>      image:
</span></span><span style=display:flex><span>        name: coreos
</span></span><span style=display:flex><span>        version: 2135.6.0
</span></span><span style=display:flex><span>        type: m5.large
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>        size: 20Gi
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    timeWindow:
</span></span><span style=display:flex><span>      begin: 220000+0100
</span></span><span style=display:flex><span>      end: 230000+0100
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The shoot cluster specification refers a machine image version that has an <code>expirationDate</code>. In the maintenance window on <code>2019-04-12</code>, the machine image version stays the same as it’s still not expired. But in the maintenance window on <code>2019-04-14</code>, the machine image version of the shoot cluster is updated to <code>2191.5.0</code> (independently of the value of <code>.spec.maintenance.autoUpdate.machineImageVersion</code>) as version <code>2135.6.0</code> is expired.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cb46ed036ac0e6a51660487edd21ab5a>5 - Networking</h1></div><div class=td-content><h1 id=pg-b9a49b1591fee6a21f4409b3adfec594>5.1 - Enable IPv4/IPv6 (dual-stack) Ingress on AWS</h1><div class=lead>Use IPv4/IPv6 (dual-stack) Ingress in an IPv4 single-stack cluster on AWS</div><h1 id=using-ipv4ipv6-dual-stack-ingress-in-an-ipv4-single-stack-cluster>Using IPv4/IPv6 (dual-stack) Ingress in an IPv4 single-stack cluster</h1><h2 id=motivation>Motivation</h2><p>IPv6 adoption is continuously growing, already overtaking IPv4 in certain regions, e.g. India, or scenarios, e.g. mobile.
Even though most IPv6 installations deploy means to reach IPv4, it might still be beneficial to expose services
natively via IPv4 and IPv6 instead of just relying on IPv4.</p><h2 id=disadvantages-of-full-ipv4ipv6-dual-stack-deployments>Disadvantages of full IPv4/IPv6 (dual-stack) Deployments</h2><p>Enabling full IPv4/IPv6 (dual-stack) support in a kubernetes cluster is a major endeavor. It requires a lot of changes
and restarts of all pods so that all pods get addresses for both IP families. A side-effect of dual-stack networking
is that failures may be hidden as network traffic may take the other protocol to reach the target. For this reason and
also due to reduced operational complexity, service teams might lean towards staying in a single-stack environment as
much as possible. Luckily, this is possible with Gardener and IPv4/IPv6 (dual-stack) ingress on AWS.</p><h2 id=simplifying-ipv4ipv6-dual-stack-ingress-with-protocol-translation-on-aws>Simplifying IPv4/IPv6 (dual-stack) Ingress with Protocol Translation on AWS</h2><p>Fortunately, the network load balancer on AWS supports automatic protocol translation, i.e. it can expose both IPv4 and
IPv6 endpoints while communicating with just one protocol to the backends. Under the hood, automatic protocol translation
takes place. Client IP address preservation can be achieved by using proxy protocol.</p><p>This approach enables users to expose IPv4 workload to IPv6-only clients without having to change the workload/service.
Without requiring invasive changes, it allows a fairly simple first step into the IPv6 world for services just requiring
ingress (incoming) communication.</p><h2 id=necessary-shoot-cluster-configuration-changes-for-ipv4ipv6-dual-stack-ingress>Necessary Shoot Cluster Configuration Changes for IPv4/IPv6 (dual-stack) Ingress</h2><p>To be able to utilize IPv4/IPv6 (dual-stack) Ingress in an IPv4 shoot cluster, the cluster needs to meet two preconditions:</p><ol><li><code>dualStack.enabled</code> needs to be set to <code>true</code> to configure VPC/subnet for IPv6 and add a routing rule for IPv6.
(This does not add IPv6 addresses to kubernetes nodes.)</li><li><code>loadBalancerController.enabled</code> needs to be set to <code>true</code> as well to use the load balancer controller, which supports
dual-stack ingress.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      dualStack:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>      loadBalancerController:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>When <code>infrastructureConfig.networks.vpc.id</code> is set to the ID of an existing VPC, please make sure that your VPC has an <a href=https://docs.aws.amazon.com/vpc/latest/userguide/modify-vpcs.html#vpc-associate-ipv6-cidr>Amazon-provided IPv6 CIDR block added</a>.</p><p>After adapting the shoot specification and reconciling the cluster, dual-stack load balancers can be created using
kubernetes services objects.</p><h2 id=creating-an-ipv4ipv6-dual-stack-ingress>Creating an IPv4/IPv6 (dual-stack) Ingress</h2><p>With the preconditions set, creating an IPv4/IPv6 load balancer is as easy as annotating a service with the correct
annotations:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-type: external
</span></span><span style=display:flex><span>  name: ...
</span></span><span style=display:flex><span>  namespace: ...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  type: LoadBalancer
</span></span></code></pre></div><p>In case the client IP address should be preserved, the following annotation can be used to enable proxy protocol.
(The pod receiving the traffic needs to be configured for proxy protocol as well.)</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: <span style=color:#a31515>&#34;*&#34;</span>
</span></span></code></pre></div><p>Please note that changing an existing <code>Service</code> to dual-stack may cause the creation of a new load balancer without
deletion of the old AWS load balancer resource. While this helps in a seamless migration by not cutting existing
connections it may lead to wasted/forgotten resources. Therefore, the (manual) cleanup needs to be taken into account
when migrating an existing <code>Service</code> instance.</p><p>For more details see <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/service/nlb/>AWS Load Balancer Documentation - Network Load Balancer</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-8bf7073b401ae0906e3f7b333d442043>5.2 - Manage Certificates with Gardener</h1><div class=lead>Use the Gardener cert-management to get fully managed, publicly trusted TLS certificates</div><h1 id=manage-certificates-with-gardener-for-public-domain>Manage certificates with Gardener for public domain</h1><h2 id=introduction>Introduction</h2><p>Dealing with applications on Kubernetes which offer a secure service endpoints (e.g. HTTPS) also require you to enable a
secured communication via SSL/TLS. With the <a href=https://github.com/gardener/gardener-extension-shoot-cert-service>certificate extension</a> enabled, Gardener can manage commonly trusted X.509 certificate for your application
endpoint. From initially requesting certificate, it also handeles their renewal in time using the free Let&rsquo;s Encrypt API.</p><p><strong>There are two senarios with which you can use the certificate extension</strong></p><ul><li>You want to use a certificate for a subdomain the shoot&rsquo;s default DNS (see <code>.spec.dns.domain</code> of your shoot resource, e.g. <code>short.ingress.shoot.project.default-domain.gardener.cloud</code>). If this is your case, please see <a href=/docs/guides/networking/certificate-extension-default-domain/>Manage certificates with Gardener for default domain</a></li><li>You want to use a certificate for a custom domain. If this is your case, please keep reading this article.</li></ul><h2 id=prerequisites>Prerequisites</h2><p>Before you start this guide there are a few requirements you need to fulfill:</p><ul><li>You have an existing shoot cluster</li><li>Your custom domain is under a <a href=https://www.iana.org/domains/root/db>public top level domain</a> (e.g. <code>.com</code>)</li><li>Your custom zone is resolvable with a public resolver via the internet (e.g. <code>8.8.8.8</code>)</li><li>You have a custom DNS provider configured and working (see <a href=/docs/extensions/others/gardener-extension-shoot-dns-service/dns_providers/>&ldquo;DNS Providers&rdquo;</a>)</li></ul><p>As part of the <a href=https://letsencrypt.org/>Let&rsquo;s Encrypt</a> <a href=https://tools.ietf.org/html/rfc8555>ACME</a> challenge validation process, Gardener sets a DNS TXT entry and Let&rsquo;s Encrypt checks if it can both resolve and authenticate it. Therefore, it&rsquo;s important that your DNS-entries are publicly resolvable. You can check this by querying e.g. Googles public DNS server and if it returns an entry your DNS is publicly visible:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># returns the A record for cert-example.example.com using Googles DNS server (8.8.8.8)</span>
</span></span><span style=display:flex><span>dig cert-example.example.com @8.8.8.8 A
</span></span></code></pre></div><h3 id=dns-provider>DNS provider</h3><p>In order to issue certificates for a custom domain you need to specify a DNS provider which is permitted to create DNS records for subdomains of your requested domain in the certificate. For example, if you request a certificate for <code>host.example.com</code> your DNS provider must be capable of managing subdomains of <code>host.example.com</code>.</p><p>DNS providers are normally specified in the shoot manifest. To learn more on how to configure one, please see the <a href=/docs/extensions/others/gardener-extension-shoot-dns-service/dns_providers/>DNS provider</a> documentation.</p><h2 id=issue-a-certificate>Issue a certificate</h2><p>Every X.509 certificate is represented by a Kubernetes custom resource <code>certificate.cert.gardener.cloud</code> in your cluster. A <code>Certificate</code> resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener&rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.</p><blockquote><p>Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.</p></blockquote><p>Certificates can be requested via 3 resources type</p><ul><li>Ingress</li><li>Service (type LoadBalancer)</li><li>Certificate (Gardener CRD)</li></ul><p>If either of the first 2 are used, a corresponding <code>Certificate</code> resource will be created automatically.</p><h3 id=using-an-ingress-resource>Using an ingress Resource</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    <span style=color:green># Optional but recommended, this is going to create the DNS entry at the same time</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/commonname: &#34;*.example.com&#34;              # optional, if not specified the first name from spec.tls[].hosts is used as common name</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/dnsnames: &#34;&#34;                             # optional, if not specified the names from spec.tls[].hosts are used</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/follow-cname: &#34;true&#34;                     # optional, same as spec.followCNAME in certificates</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/secret-labels: &#34;key1=value1,key2=value2&#34; # optional labels for the certificate secret</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/issuer: custom-issuer                    # optional to specify custom issuer (use namespace/name for shoot issuers)</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/preferred-chain: &#34;chain name&#34;            # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tls:
</span></span><span style=display:flex><span>  - hosts:
</span></span><span style=display:flex><span>    <span style=color:green># Must not exceed 64 characters.</span>
</span></span><span style=display:flex><span>    - amazing.example.com
</span></span><span style=display:flex><span>    <span style=color:green># Certificate and private key reside in this secret.</span>
</span></span><span style=display:flex><span>    secretName: tls-secret
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: amazing.example.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span></code></pre></div><p>Replace the <code>hosts</code> and <code>rules[].host</code> value again with your own domain and adjust the remaining Ingress attributes in accordance with your deployment (e.g. the above is for an <code>istio</code> Ingress controller and forwards traffic to a <code>service1</code> on port 80).</p><h3 id=using-a-service-type-loadbalancer>Using a service type LoadBalancer</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/secretname: tls-secret
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: example.example.com
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>    <span style=color:green># Optional</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    cert.gardener.cloud/commonname: <span style=color:#a31515>&#34;*.example.example.com&#34;</span>
</span></span><span style=display:flex><span>    cert.gardener.cloud/dnsnames: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/follow-cname: &#34;true&#34;                     # optional, same as spec.followCNAME in certificates</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/secret-labels: &#34;key1=value1,key2=value2&#34; # optional labels for the certificate secret</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/issuer: custom-issuer                    # optional to specify custom issuer (use namespace/name for shoot issuers)</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/preferred-chain: &#34;chain name&#34;            # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>  name: test-service
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>    - name: http
</span></span><span style=display:flex><span>      port: 80
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>      targetPort: 8080
</span></span><span style=display:flex><span>  type: LoadBalancer
</span></span></code></pre></div><h3 id=using-the-custom-certificate-resource>Using the custom Certificate resource</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Certificate
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cert-example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  commonName: amazing.example.com
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: tls-secret
</span></span><span style=display:flex><span>    namespace: default
</span></span><span style=display:flex><span>  <span style=color:green># Optionnal if using the default issuer</span>
</span></span><span style=display:flex><span>  issuerRef:
</span></span><span style=display:flex><span>    name: garden
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># If delegated domain for DNS01 challenge should be used. This has only an effect if a CNAME record is set for</span>
</span></span><span style=display:flex><span>  <span style=color:green># &#39;_acme-challenge.amazing.example.com&#39;.</span>
</span></span><span style=display:flex><span>  <span style=color:green># For example: If a CNAME record exists &#39;_acme-challenge.amazing.example.com&#39; =&gt; &#39;_acme-challenge.writable.domain.com&#39;,</span>
</span></span><span style=display:flex><span>  <span style=color:green># the DNS challenge will be written to &#39;_acme-challenge.writable.domain.com&#39;.</span>
</span></span><span style=display:flex><span>  <span style=color:green>#followCNAME: true</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># optionally set labels for the secret</span>
</span></span><span style=display:flex><span>  <span style=color:green>#secretLabels:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#  key1: value1</span>
</span></span><span style=display:flex><span>  <span style=color:green>#  key2: value2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># Optionally specify the preferred certificate chain: if the CA offers multiple certificate chains, prefer the chain with an issuer matching this Subject Common Name. If no match, the default offered chain will be used.</span>
</span></span><span style=display:flex><span>  <span style=color:green>#preferredChain: &#34;ISRG Root X1&#34;</span>
</span></span></code></pre></div><h2 id=supported-attributes>Supported attributes</h2><p>Here is a list of all supported annotations regarding the certificate extension:</p><table><thead><tr><th>Path</th><th>Annotation</th><th>Value</th><th>Required</th><th>Description</th></tr></thead><tbody><tr><td>N/A</td><td><code>cert.gardener.cloud/purpose:</code></td><td><code>managed</code></td><td>Yes when using annotations</td><td>Flag for Gardener that this specific Ingress or Service requires a certificate</td></tr><tr><td><code>spec.commonName</code></td><td><code>cert.gardener.cloud/commonname:</code></td><td>E.g. &ldquo;*.demo.example.com&rdquo; or<br>&ldquo;special.example.com&rdquo;</td><td>Certificate and Ingress : No<br>Service: yes</td><td>Specifies for which domain the certificate request will be created. If not specified, the names from spec.tls[].hosts are used. This entry must comply with the <a href=/docs/guides/networking/certificate-extension/#Character-Restrictions>64 character</a> limit.</td></tr><tr><td><code>spec.dnsName</code></td><td><code>cert.gardener.cloud/dnsnames:</code></td><td>E.g. &ldquo;special.example.com&rdquo;</td><td>Certificate and Ingress : No<br>Service: yes</td><td>Additional domains the certificate should be valid for (Subject Alternative Name). If not specified, the names from spec.tls[].hosts are used. Entries in this list can be longer than 64 characters.</td></tr><tr><td><code>spec.secretRef.name</code></td><td><code>cert.gardener.cloud/secretname:</code></td><td><code>any-name</code></td><td>Yes for certificate and Service</td><td>Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it&rsquo;ll be created automatically as soon as the certificate has been issued.</td></tr><tr><td><code>spec.issuerRef.name</code></td><td><code>cert.gardener.cloud/issuer:</code></td><td>E.g. <code>gardener</code></td><td>No</td><td>Specifies the issuer you want to use. Only necessary if you request certificates for <a href=/docs/guides/networking/certificate-extension/#Custom-Domains>custom domains</a>.</td></tr><tr><td>N/A</td><td><code>cert.gardener.cloud/revoked:</code></td><td><code>true</code> otherwise always false</td><td>No</td><td>Use only to revoke a certificate, see <a href=/docs/guides/networking/certificate-extension/#references>reference</a> for more details</td></tr><tr><td><code>spec.followCNAME</code></td><td><code>cert.gardener.cloud/follow-cname</code></td><td>E.g. <code>true</code></td><td>No</td><td>Specifies that the usage of a delegated domain for DNS challenges is allowed. Details see <a href=https://github.com/gardener/cert-management#follow-cname>Follow CNAME</a>.</td></tr><tr><td><code>spec.preferredChain</code></td><td><code>cert.gardener.cloud/preferred-chain</code></td><td>E.g. <code>ISRG Root X1</code></td><td>No</td><td>Specifies the Common Name of the issuer for selecting the certificate chain. Details see <a href=https://github.com/gardener/cert-management#preferred-chain>Preferred Chain</a>.</td></tr><tr><td><code>spec.secretLabels</code></td><td><code>cert.gardener.cloud/secret-labels</code></td><td>for annotation use e.g. <code>key1=value1,key2=value2</code></td><td>No</td><td>Specifies labels for the certificate secret.</td></tr></tbody></table><h2 id=request-a-wildcard-certificate>Request a wildcard certificate</h2><p>In order to avoid the creation of multiples certificates for every single endpoints, you may want to create a wildcard certificate for your shoot&rsquo;s default cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    cert.gardener.cloud/commonName: <span style=color:#a31515>&#34;*.example.com&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tls:
</span></span><span style=display:flex><span>  - hosts:
</span></span><span style=display:flex><span>    - amazing.example.com
</span></span><span style=display:flex><span>    secretName: tls-secret
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: amazing.example.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span></code></pre></div><p>Please note that this can also be achived by directly adding an annotation to a Service type LoadBalancer. You could also create a Certificate object with a wildcard domain.</p><h2 id=using-a-custom-issuer>Using a custom Issuer</h2><p>Most Gardener deployment with the certification extension enabled have a preconfigured <code>garden</code> issuer. It is also usually configured to use Let&rsquo;s Encrypt as the certificate provider.</p><p>If you need a custom issuer for a specific cluster, please see <a href=/docs/extensions/others/gardener-extension-shoot-cert-service/custom_shoot_issuer/>Using a custom Issuer</a></p><h2 id=quotas>Quotas</h2><p>For security reasons there may be a default quota on the certificate requests per day set globally in the controller
registration of the shoot-cert-service.</p><p>The default quota only applies if there is no explicit quota defined for the issuer itself with the field
<code>requestsPerDayQuota</code>, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: shoot-cert-service
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: CertConfig
</span></span><span style=display:flex><span>      issuers:
</span></span><span style=display:flex><span>        - email: your-email@example.com
</span></span><span style=display:flex><span>          name: custom-issuer <span style=color:green># issuer name must be specified in every custom issuer request, must not be &#34;garden&#34;</span>
</span></span><span style=display:flex><span>          server: <span style=color:#a31515>&#39;https://acme-v02.api.letsencrypt.org/directory&#39;</span>
</span></span><span style=display:flex><span>          requestsPerDayQuota: 10
</span></span></code></pre></div><h2 id=dns-propagation>DNS Propagation</h2><p>As stated before, cert-manager uses the ACME challenge protocol to authenticate that you are the DNS owner for the domain&rsquo;s certificate you are requesting. This works by creating a DNS TXT record in your DNS provider under <code>_acme-challenge.example.example.com</code> containing a token to compare with. The TXT record is only visible during the domain validation. Typically, the record is propagated within a few minutes. But if the record is not visible to the ACME server for any reasons, the certificate request is retried again after several minutes. This means you may have to wait up to one hour after the propagation problem has been resolved before the certificate request is retried. Take a look in the events with <code>kubectl describe ingress example</code> for troubleshooting.</p><h2 id=character-restrictions>Character Restrictions</h2><p>Due to the ACME protocol specification, at least one domain of the domains you request a certificate for must not exceed a character limit of 64 (CN - Common Name).</p><p>For example, the following request is invalid:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Certificate
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cert-invalid
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  commonName: morethan64characters.ingress.shoot.project.default-domain.gardener.cloud
</span></span></code></pre></div><p>But it is valid to request a certificate for this domain if you have at least one domain which does not exceed the mentioned limit:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Certificate
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cert-example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  commonName: short.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>  dnsNames:
</span></span><span style=display:flex><span>  - morethan64characters.ingress.shoot.project.default-domain.gardener.cloud
</span></span></code></pre></div><h2 id=references>References</h2><ul><li><a href=https://github.com/gardener/cert-management>Gardener cert-management</a></li><li><a href=https://github.com/gardener/gardener-extension-shoot-dns-service>Managing DNS with Gardener</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c09241178b22185f6fdcdec2b31ccffa>5.3 - Manage Certificates with Gardener for Default Domain</h1><div class=lead>Use the Gardener cert-management to get fully managed, publicly trusted TLS certificates</div><h1 id=manage-certificates-with-gardener-for-default-domain>Manage certificates with Gardener for default domain</h1><h2 id=introduction>Introduction</h2><p>Dealing with applications on Kubernetes which offer a secure service endpoints (e.g. HTTPS) also require you to enable a
secured communication via SSL/TLS. With the <a href=https://github.com/gardener/gardener-extension-shoot-cert-service>certificate extension</a> enabled, Gardener can manage commonly trusted X.509 certificate for your application
endpoint. From initially requesting certificate, it also handeles their renewal in time using the free Let&rsquo;s Encrypt API.</p><p><strong>There are two senarios with which you can use the certificate extension</strong></p><ul><li>You want to use a certificate for a subdomain the shoot&rsquo;s default DNS (see <code>.spec.dns.domain</code> of your shoot resource, e.g. <code>short.ingress.shoot.project.default-domain.gardener.cloud</code>). If this is your case, please keep reading this article.</li><li>You want to use a certificate for a custom domain. If this is your case, please see <a href=/docs/guides/networking/certificate-extension/>Manage certificates with Gardener for public domain</a></li></ul><h2 id=prerequisites>Prerequisites</h2><p>Before you start this guide there are a few requirements you need to fulfill:</p><ul><li>You have an existing shoot cluster</li></ul><p>Since you are using the default DNS name, all DNS configuration should already be done and ready.</p><h2 id=issue-a-certificate>Issue a certificate</h2><p>Every X.509 certificate is represented by a Kubernetes custom resource <code>certificate.cert.gardener.cloud</code> in your cluster. A <code>Certificate</code> resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener&rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.</p><blockquote><p>Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.</p></blockquote><p>Certificates can be requested via 3 resources type</p><ul><li>Ingress</li><li>Service (type LoadBalancer)</li><li>certificate (Gardener CRD)</li></ul><p>If either of the first 2 are used, a corresponding <code>Certificate</code> resource will automatically be created.</p><h3 id=using-an-ingress-resource>Using an ingress Resource</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/issuer: custom-issuer                    # optional to specify custom issuer (use namespace/name for shoot issuers)</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/follow-cname: &#34;true&#34;                     # optional, same as spec.followCNAME in certificates</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/secret-labels: &#34;key1=value1,key2=value2&#34; # optional labels for the certificate secret</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/preferred-chain: &#34;chain name&#34;            # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tls:
</span></span><span style=display:flex><span>  - hosts:
</span></span><span style=display:flex><span>    <span style=color:green># Must not exceed 64 characters.</span>
</span></span><span style=display:flex><span>    - short.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>    <span style=color:green># Certificate and private key reside in this secret.</span>
</span></span><span style=display:flex><span>    secretName: tls-secret
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: short.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span></code></pre></div><h3 id=using-a-service-type-loadbalancer>Using a service type LoadBalancer</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    <span style=color:green># Certificate and private key reside in this secret.</span>
</span></span><span style=display:flex><span>    cert.gardener.cloud/secretname: tls-secret
</span></span><span style=display:flex><span>    <span style=color:green># You may add more domains separated by commas (e.g. &#34;service.shoot.project.default-domain.gardener.cloud, amazing.shoot.project.default-domain.gardener.cloud&#34;)</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: <span style=color:#a31515>&#34;service.shoot.project.default-domain.gardener.cloud&#34;</span> 
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/issuer: custom-issuer                    # optional to specify custom issuer (use namespace/name for shoot issuers)</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/follow-cname: &#34;true&#34;                     # optional, same as spec.followCNAME in certificates</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/secret-labels: &#34;key1=value1,key2=value2&#34; # optional labels for the certificate secret</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/preferred-chain: &#34;chain name&#34;            # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)</span>
</span></span><span style=display:flex><span>  name: test-service
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>    - name: http
</span></span><span style=display:flex><span>      port: 80
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>      targetPort: 8080
</span></span><span style=display:flex><span>  type: LoadBalancer
</span></span></code></pre></div><h3 id=using-the-custom-certificate-resource>Using the custom Certificate resource</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Certificate
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cert-example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  commonName: short.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: tls-secret
</span></span><span style=display:flex><span>    namespace: default
</span></span><span style=display:flex><span>  <span style=color:green># Optionnal if using the default issuer</span>
</span></span><span style=display:flex><span>  issuerRef:
</span></span><span style=display:flex><span>    name: garden
</span></span></code></pre></div><p>If you&rsquo;re interested in the current progress of your request, you&rsquo;re advised to consult the description, more specifically the <code>status</code> attribute in case the issuance failed.</p><h2 id=request-a-wildcard-certificate>Request a wildcard certificate</h2><p>In order to avoid the creation of multiples certificates for every single endpoints, you may want to create a wildcard certificate for your shoot&rsquo;s default cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    cert.gardener.cloud/commonName: <span style=color:#a31515>&#34;*.ingress.shoot.project.default-domain.gardener.cloud&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tls:
</span></span><span style=display:flex><span>  - hosts:
</span></span><span style=display:flex><span>    - amazing.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>    secretName: tls-secret
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: amazing.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span></code></pre></div><p>Please note that this can also be achived by directly adding an annotation to a Service type LoadBalancer. You could also create a Certificate object with a wildcard domain.</p><h2 id=more-information>More information</h2><p>For more information and more examples about using the certificate extension, please see <a href=/docs/guides/networking/certificate-extension/>Manage certificates with Gardener for public domain</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-e57621adc8993774cdc941af01f77fa1>5.4 - Managing DNS with Gardener</h1><div class=lead>Setup Gardener-managed DNS records in cluster.</div><h1 id=request-dns-names-in-shoot-clusters>Request DNS Names in Shoot Clusters</h1><h2 id=introduction>Introduction</h2><p>Within a shoot cluster, it is possible to request DNS records via the following resource types:</p><ul><li><a href=https://kubernetes.io/docs/concepts/services-networking/ingress/>Ingress</a></li><li><a href=https://kubernetes.io/docs/concepts/services-networking/service/>Service</a></li><li><a href=https://github.com/gardener/external-dns-management/blob/master/README.md#the-model>DNSEntry</a></li></ul><p>It is necessary that the Gardener installation your shoot cluster runs in is equipped with a <code>shoot-dns-service</code> extension. This extension uses the seed&rsquo;s dns management infrastructure to maintain DNS names for shoot clusters. Please ask your Gardener operator if the extension is available in your environment.</p><h2 id=shoot-feature-gate>Shoot Feature Gate</h2><p>In some Gardener setups the <code>shoot-dns-service</code> extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-dns-service
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=before-you-start>Before you start</h2><p>You should :</p><ul><li>Have created a shoot cluster</li><li>Have created and correctly configured a DNS Provider (Please consult <a href=/docs/extensions/others/gardener-extension-shoot-dns-service/dns_providers/>this page</a> for more information)</li><li>Have a basic understanding of DNS (see link under <a href=/docs/guides/networking/dns-extension/#references>References</a>)</li></ul><p>There are 2 types of DNS that you can use within Kubernetes :</p><ul><li>internal (usually managed by coreDNS)</li><li>external (managed by a public DNS provider).</li></ul><p>This page, and the extension, exclusively works for external DNS handling.</p><p>Gardener allows 2 way of managing your external DNS:</p><ul><li>Manually, which means you are in charge of creating / maintaining your Kubernetes related DNS entries</li><li>Via the Gardener DNS extension</li></ul><h2 id=gardener-dns-extension>Gardener DNS extension</h2><p>The managed external DNS records feature of the Gardener clusters makes all this easier. You do not need DNS service provider specific knowledge, and in fact you do not need to leave your cluster at all to achieve that. You simply annotate the Ingress / Service that needs its DNS records managed and it will be automatically created / managed by Gardener.</p><p>Managed external DNS records are supported with the following DNS provider types:</p><ul><li>aws-route53</li><li>azure-dns</li><li>azure-private-dns</li><li>google-clouddns</li><li>openstack-designate</li><li>alicloud-dns</li><li>cloudflare-dns</li></ul><h3 id=request-dns-records-for-ingress-resources>Request DNS records for Ingress resources</h3><p>To request a DNS name for <code>Ingress</code>, <code>Service</code> or <code>Gateway</code> (Istio or Gateway API) objects in the shoot cluster it must be annotated with the DNS class <code>garden</code> and an annotation denoting the desired DNS names.</p><p>Example for an annotated Ingress resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    <span style=color:green># Let Gardener manage external DNS records for this Ingress.</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: special.example.com <span style=color:green># Use &#34;*&#34; to collects domains names from .spec.rules[].host</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>    <span style=color:green># If you are delegating the certificate management to Gardener, uncomment the following line</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/purpose: managed</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: special.example.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span><span style=display:flex><span>  <span style=color:green># Uncomment the following part if you are delegating the certificate management to Gardener</span>
</span></span><span style=display:flex><span>  <span style=color:green>#tls:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#  - hosts:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#      - special.example.com</span>
</span></span><span style=display:flex><span>  <span style=color:green>#    secretName: my-cert-secret-name</span>
</span></span></code></pre></div><p>For an Ingress, the DNS names are already declared in the specification. Nevertheless the <em>dnsnames</em> annotation must be present. Here a subset of the DNS names of the ingress can be specified. If DNS names for all names are desired, the value <code>all</code> can be used.</p><p>Keep in mind that ingress resources are ignored unless an ingress controller is set up. Gardener does not provide an ingress controller by default. For more details, see <a href=https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/>Ingress Controllers</a> and <a href=https://kubernetes.io/docs/concepts/services-networking/service/>Service</a> in the Kubernetes documentation.</p><h3 id=request-dns-records-for-service-type-loadbalancer>Request DNS records for service type LoadBalancer</h3><p>Example for an annotated Service (it must have the type <code>LoadBalancer</code>) resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-svc
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    <span style=color:green># Let Gardener manage external DNS records for this Service.</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: special.example.com
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    app: amazing-app
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>    - protocol: TCP
</span></span><span style=display:flex><span>      port: 80
</span></span><span style=display:flex><span>      targetPort: 8080
</span></span><span style=display:flex><span>  type: LoadBalancer
</span></span></code></pre></div><h3 id=request-dns-records-for-gateway-resources>Request DNS records for Gateway resources</h3><p>Please see <a href=/docs/extensions/others/gardener-extension-shoot-dns-service/tutorials/istio-gateways/>Istio Gateways</a> or <a href=/docs/extensions/others/gardener-extension-shoot-dns-service/tutorials/gateway-api-gateways/>Gateway API</a> for details.</p><h3 id=creating-a-dnsentry-resource-explicitly>Creating a DNSEntry resource explicitly</h3><p>It is also possible to create a DNS entry via the Kubernetes resource called <code>DNSEntry</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: dns.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: DNSEntry
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    <span style=color:green># Let Gardener manage this DNS entry.</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>  name: special-dnsentry
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  dnsName: special.example.com
</span></span><span style=display:flex><span>  ttl: 600
</span></span><span style=display:flex><span>  targets:
</span></span><span style=display:flex><span>  - 1.2.3.4
</span></span></code></pre></div><p>If one of the accepted DNS names is a direct subname of the shoot&rsquo;s ingress domain, this is already handled by the standard wildcard entry for the ingress domain. Therefore this name should be excluded from the <em>dnsnames</em> list in the annotation. If only this DNS name is configured in the ingress, no explicit DNS entry is required, and the DNS annotations should be omitted at all.</p><p>You can check the status of the <code>DNSEntry</code> with</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get dnsentry
</span></span><span style=display:flex><span>NAME          DNS                                                            TYPE          PROVIDER      STATUS    AGE
</span></span><span style=display:flex><span>mydnsentry    special.example.com     aws-route53   default/aws   Ready     24s
</span></span></code></pre></div><p>As soon as the status of the entry is <code>Ready</code>, the provider has accepted the new DNS record. Depending on the provider and your DNS settings and cache, <strong>it may take up to 24 hours for the new entry to be propagated over all internet</strong>.</p><p>More examples can be found <a href=https://github.com/gardener/external-dns-management/blob/master/examples/>here</a></p><h3 id=request-dns-records-for-serviceingress-resources-using-a-dnsannotation-resource>Request DNS records for Service/Ingress resources using a DNSAnnotation resource</h3><p>In rare cases it may not be possible to add annotations to a <code>Service</code> or <code>Ingress</code> resource object.</p><p>E.g.: the helm chart used to deploy the resource may not be adaptable for some reasons or some automation is used, which always restores the original content of the resource object by dropping any additional annotations.</p><p>In these cases, it is recommended to use an additional <code>DNSAnnotation</code> resource in order to have more flexibility that <code>DNSentry resources</code>. The <code>DNSAnnotation</code> resource makes the DNS shoot service behave as if annotations have been added to the referenced resource.</p><p>For the Ingress example shown above, you can create a <code>DNSAnnotation</code> resource alternatively to provide the annotations.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: dns.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: DNSAnnotation
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>  name: test-ingress-annotation
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  resourceRef:
</span></span><span style=display:flex><span>    kind: Ingress
</span></span><span style=display:flex><span>    apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>    name: test-ingress
</span></span><span style=display:flex><span>    namespace: default
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: <span style=color:#a31515>&#39;*&#39;</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden    
</span></span></code></pre></div><p>Note that the DNSAnnotation resource itself needs the <code>dns.gardener.cloud/class=garden</code> annotation. This also only works for annotations known to the DNS shoot service (see <a href=/docs/guides/networking/dns-extension/#accepted-external-dns-records-annotations>Accepted External DNS Records Annotations</a>).</p><p>For more details, see also <a href=https://github.com/gardener/external-dns-management#dnsannotation-objects>DNSAnnotation objects</a></p><h3 id=accepted-external-dns-records-annotations>Accepted External DNS Records Annotations</h3><p>Here are all of the accepted annotation related to the DNS extension:</p><table><thead><tr><th>Annotation</th><th>Description</th></tr></thead><tbody><tr><td>dns.gardener.cloud/dnsnames</td><td>Mandatory for service and ingress resources, accepts a comma-separated list of DNS names if multiple names are required. For ingress you can use the special value <code>'*'</code>. In this case, the DNS names are collected from <code>.spec.rules[].host</code>.</td></tr><tr><td>dns.gardener.cloud/class</td><td>Mandatory, in the context of the shoot-dns-service it must always be set to <code>garden</code>.</td></tr><tr><td>dns.gardener.cloud/ttl</td><td>Recommended, overrides the default Time-To-Live of the DNS record.</td></tr><tr><td>dns.gardener.cloud/cname-lookup-interval</td><td>Only relevant if multiple domain name targets are specified. It specifies the lookup interval for CNAMEs to map them to IP addresses (in seconds)</td></tr><tr><td>dns.gardener.cloud/realms</td><td>Internal, for restricting provider access for shoot DNS entries. Typcially not set by users of the shoot-dns-service.</td></tr><tr><td>dns.gardener.cloud/ip-stack</td><td>Only relevant for provider type <code>aws-route53</code> if target is an AWS load balancer domain name. Can be set for service, ingress and DNSEntry resources. It specify which DNS records with alias targets are created instead of the usual <code>CNAME</code> records. If the annotation is not set (or has the value <code>ipv4</code>), only an <code>A</code> record is created. With value <code>dual-stack</code>, both <code>A</code> and <code>AAAA</code> records are created. With value <code>ipv6</code> only an <code>AAAA</code> record is created.</td></tr><tr><td>service.beta.kubernetes.io/aws-load-balancer-ip-address-type=dualstack</td><td>For services, behaves similar to <code>dns.gardener.cloud/ip-stack=dual-stack</code>.</td></tr><tr><td>loadbalancer.openstack.org/load-balancer-address</td><td>Internal, for services only: support for PROXY protocol on Openstack (which needs a hostname as ingress). Typcially not set by users of the shoot-dns-service.</td></tr></tbody></table><p>If one of the accepted DNS names is a direct subdomain of the shoot&rsquo;s ingress domain, this is already handled by the standard wildcard entry for the ingress domain. Therefore, this name should be excluded from the <em>dnsnames</em> list in the annotation. If only this DNS name is configured in the ingress, no explicit DNS entry is required, and the DNS annotations should be omitted at all.</p><h2 id=troubleshooting>Troubleshooting</h2><h3 id=general-dns-tools>General DNS tools</h3><p>To check the DNS resolution, use the <code>nslookup</code> or <code>dig</code> command.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ nslookup special.your-domain.com
</span></span></code></pre></div><p>or with dig</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ dig +short special.example.com
</span></span><span style=display:flex><span>Depending on your network settings, you may get a successful response faster using a public DNS server (e.g. 8.8.8.8, 8.8.4.4, or 1.1.1.1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dig @8.8.8.8 +short special.example.com
</span></span></code></pre></div><h3 id=dns-record-events>DNS record events</h3><p>The DNS controller publishes Kubernetes events for the resource which requested the DNS record (Ingress, Service, DNSEntry). These events reveal more information about the DNS requests being processed and are especially useful to check any kind of misconfiguration, e.g. requests for a domain you don&rsquo;t own.</p><p>Events for a successfully created DNS record:</p><pre tabindex=0><code>$ kubectl describe service my-service

Events:
  Type    Reason          Age                From                    Message
  ----    ------          ----               ----                    -------
  Normal  dns-annotation  19s                dns-controller-manager  special.example.com: dns entry is pending
  Normal  dns-annotation  19s (x3 over 19s)  dns-controller-manager  special.example.com: dns entry pending: waiting for dns reconciliation
  Normal  dns-annotation  9s (x3 over 10s)   dns-controller-manager  special.example.com: dns entry active
</code></pre><p>Please note, events vanish after their retention period (usually <code>1h</code>).</p><h3 id=dnsentry-status>DNSEntry status</h3><p><code>DNSEntry</code> resources offer a <code>.status</code> sub-resource which can be used to check the current state of the object.</p><p>Status of a erroneous <code>DNSEntry</code>.</p><pre tabindex=0><code>  status:
    message: No responsible provider found
    observedGeneration: 3
    provider: remote
    state: Error
</code></pre><h2 id=references>References</h2><ul><li><a href=https://www.cloudflare.com/en-ca/learning/dns/what-is-dns>Understanding DNS</a></li><li><a href=https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/>Kubernetes Internal DNS</a></li><li><a href=https://github.com/gardener/external-dns-management/blob/master/pkg/apis/dns/v1alpha1/dnsentry.go>DNSEntry API (Golang)</a></li><li><a href=/docs/guides/networking/certificate-extension/>Managing Certificates with Gardener</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-78c4187b83c79f343c8ecf75cbf6052d>6 - Monitor and Troubleshoot</h1></div><div class=td-content><h1 id=pg-6754a4f00b400f52e32db31333cfd45e>6.1 - Analyzing Node Removal and Failures</h1><div class=lead>Utilize Gardener&rsquo;s Monitoring and Logging to analyze removal and failures of nodes</div><h2 id=overview>Overview</h2><p>Sometimes operators want to find out why a certain node got removed. This guide helps to identify possible causes.
There are a few potential reasons why nodes can be removed:</p><ul><li><a href=/docs/guides/monitoring-and-troubleshooting/analysing-node-failures/#find-out-whether-the-node-was-unhealthy>broken node</a>: a node becomes unhealthy and machine-controller-manager terminates it in an attempt to replace the unhealthy node with a new one</li><li><a href=/docs/guides/monitoring-and-troubleshooting/analysing-node-failures/#scale-down>scale-down</a>: cluster-autoscaler sees that a node is under-utilized and therefore scales down a worker pool</li><li><a href=/docs/guides/monitoring-and-troubleshooting/analysing-node-failures/#node-rolling>node rolling</a>: configuration changes to a worker pool (or cluster) require all nodes of one or all worker pools to be rolled and thus all nodes to be replaced. Some possible changes are:<ul><li>the K8s/OS version</li><li>changing machine types</li></ul></li></ul><p>Helpful information can be obtained by using the logging stack. See <a href=/docs/gardener/logging-usage/>Logging Stack</a> for how to utilize the logging information in Gardener.</p><h2 id=find-out-whether-the-node-was-unhealthy>Find Out Whether the Node Was <code>unhealthy</code></h2><h3 id=check-the-node-events>Check the Node Events</h3><p>A good first indication on what happened to a node can be obtained from the node&rsquo;s events. Events are scraped and ingested into the logging system, so they can be found in the explore tab of Grafana (make sure to select <code>loki</code> as datasource) with a query like <code>{job="event-logging"} | unpack | object="Node/&lt;node-name>"</code> or find any event mentioning the node in question via a broader query like <code>{job="event-logging"}|="&lt;node-name>"</code>.</p><p>A potential result might reveal</p><pre tabindex=0><code>{&#34;_entry&#34;:&#34;Node ip-10-55-138-185.eu-central-1.compute.internal status is now: NodeNotReady&#34;,&#34;count&#34;:1,&#34;firstTimestamp&#34;:&#34;2023-04-05T12:02:08Z&#34;,&#34;lastTimestamp&#34;:&#34;2023-04-05T12:02:08Z&#34;,&#34;namespace&#34;:&#34;default&#34;,&#34;object&#34;:&#34;Node/ip-10-55-138-185.eu-central-1.compute.internal&#34;,&#34;origin&#34;:&#34;shoot&#34;,&#34;reason&#34;:&#34;NodeNotReady&#34;,&#34;source&#34;:&#34;node-controller&#34;,&#34;type&#34;:&#34;Normal&#34;}
</code></pre><h3 id=check-machine-controller-manager-logs>Check machine-controller-manager Logs</h3><p>If a node was getting unhealthy, the last conditions can be found in the logs of the <code>machine-controller-manager</code> by using a query like <code>{pod_name=~"machine-controller-manager.*"}|="&lt;node-name>"</code>.</p><p><strong>Caveat</strong>: every <code>node</code> resource is backed by a corresponding <code>machine</code> resource managed by machine-controller-manager. Usually two corresponding <code>node</code> and <code>machine</code> resources have the same name with the exception of AWS. Here you first need to find with the above query the corresponding <code>machine</code> name, typically via a log like this</p><pre tabindex=0><code>2023-04-05 12:02:08	{&#34;log&#34;:&#34;Conditions of Machine \&#34;shoot--demo--cluster-pool-z1-6dffc-jh4z4\&#34; with providerID \&#34;aws:///eu-central-1/i-0a6ad1ca4c2e615dc\&#34; and backing node \&#34;ip-10-55-138-185.eu-central-1.compute.internal\&#34; are changing&#34;,&#34;pid&#34;:&#34;1&#34;,&#34;severity&#34;:&#34;INFO&#34;,&#34;source&#34;:&#34;machine_util.go:629&#34;}
</code></pre><p>This reveals that <code>node</code> <code>ip-10-55-138-185.eu-central-1.compute.internal</code> is backed by <code>machine</code> <code>shoot--demo--cluster-pool-z1-6dffc-jh4z4</code>. On infrastructures other than AWS you can omit this step.</p><p>With the machine name at hand, now search for log entries with <code>{pod_name=~"machine-controller-manager.*"}|="&lt;machine-name>"</code>.
In case the node had failing conditions, you&rsquo;d find logs like this:</p><pre tabindex=0><code>2023-04-05 12:02:08	{&#34;log&#34;:&#34;Machine shoot--demo--cluster-pool-z1-6dffc-jh4z4 is unhealthy - changing MachineState to Unknown. Node conditions: [{Type:ClusterNetworkProblem Status:False LastHeartbeatTime:2023-04-05 11:58:39 +0000 UTC LastTransitionTime:2023-03-23 11:59:29 +0000 UTC Reason:NoNetworkProblems Message:no cluster network problems} ... {Type:Ready Status:Unknown LastHeartbeatTime:2023-04-05 11:55:27 +0000 UTC LastTransitionTime:2023-04-05 12:02:07 +0000 UTC Reason:NodeStatusUnknown Message:Kubelet stopped posting node status.}]&#34;,&#34;pid&#34;:&#34;1&#34;,&#34;severity&#34;:&#34;WARN&#34;,&#34;source&#34;:&#34;machine_util.go:637&#34;}
</code></pre><p>In the example above, the reason for an unhealthy node was that <code>kubelet</code> failed to renew its heartbeat. Typical reasons would be either a broken VM (that couldn&rsquo;t execute <code>kubelet</code> anymore) or a broken network. Note that some VM terminations performed by the infrastructure provider are actually expected (e.g., <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instances-status-check_sched.html>scheduled events on AWS</a>)</p><p>In both cases, the infrastructure provider might be able to provide more information on particular VM or network failures.</p><p>Whatever the failure condition might have been, if a node gets unhealthy, it will be terminated by <code>machine-controller-manager</code> after the <code>machineHealthTimeout</code> has elapsed (this parameter can be configured in your <a href=https://github.com/gardener/gardener/blob/v1.68.0/example/90-shoot.yaml#L132>shoot spec</a>).</p><h3 id=check-the-node-logs>Check the Node Logs</h3><p>For each <code>node</code> the kernel and <code>kubelet</code> logs, as well as a few others, are scraped and can be queried with this query <code>{nodename="&lt;node-name>"}</code>
This might reveal OS specific issues or, in the absence of any logs (e.g., after the node went unhealthy), might indicate a network disruption or sudden VM termination. Note that some VM terminations performed by the infrastructure provider are actually expected (e.g., <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-instances-status-check_sched.html>scheduled events on AWS</a>).</p><p>Infrastructure providers might be able to provide more information on particular VM failures in such cases.</p><h3 id=check-the-network-problem-detector-dashboard>Check the Network Problem Detector Dashboard</h3><p>If your Gardener installation utilizes <a href=https://github.com/gardener/gardener-extension-shoot-networking-problemdetector>gardener-extension-shoot-networking-problemdetector</a>, you can check the dashboard named &ldquo;Network Problem Detector&rdquo; in Grafana for hints on network issues on the node of interest.</p><h2 id=scale-down>Scale-Down</h2><p>In general, scale-downs are managed by the <a href=https://github.com/gardener/autoscaler>cluster-autoscaler</a>, its logs can be found with the query <code>{container_name="cluster-autoscaler"}</code>.
Attempts to remove a node can be found with the query <code>{container_name="cluster-autoscaler"}|="Scale-down: removing empty node"</code></p><p>If a scale-down has caused disruptions in your workload, consider protecting your workload by adding <code>PodDisruptionBudgets</code> (see the <a href=https://github.com/gardener/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node>autoscaler FAQ</a> for more options).</p><h2 id=node-rolling>Node Rolling</h2><p>Node rolling can be caused by ,e.g.:</p><ul><li>change of the K8s minor version of the cluster or a worker pool</li><li>change of the OS version of the cluster or a worker pool</li><li>change of the disk size/type or machine size/type of a worker pool</li><li>change of node labels</li></ul><p>Changes like the above are done by altering the shoot specification and thus are recorded in the external auditlog system that is configured for the garden cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9ef04269409623a8c7d8d6ccafcfaf80>6.2 - Get a Shell to a Gardener Shoot Worker Node</h1><div class=lead>Describes the methods for getting shell access to worker nodes</div><h2 id=overview>Overview</h2><p>To troubleshoot certain problems in a Kubernetes cluster, operators need access to the host of the Kubernetes node. This can be required if a node misbehaves or fails to join the cluster in the first place.</p><p>With access to the host, it is for instance possible to check the <code>kubelet</code> logs and interact with common tools such as <code>systemctl</code>and <code>journalctl</code>.</p><p>The first section of this guide explores options to get a shell to the node of a Gardener Kubernetes cluster.
The options described in the second section do not rely on Kubernetes capabilities to get shell access to a node and thus can also be used if an instance failed to join the cluster.</p><p>This guide only covers how to get access to the host, but does not cover troubleshooting methods.</p><ul><li><a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#overview>Overview</a></li><li><a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#get-a-shell-to-an-operational-cluster-node>Get a Shell to an Operational Cluster Node</a><ul><li><a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#gardener-dashboard>Gardener Dashboard</a><ul><li><a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#result>Result</a></li></ul></li><li><a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#gardener-ops-toolbelt>Gardener Ops Toolbelt</a></li><li><a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#custom-root-pod>Custom Root Pod</a></li></ul></li><li><a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#ssh-access-to-a-node-that-failed-to-join-the-cluster>SSH Access to a Node That Failed to Join the Cluster</a><ul><li><a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#identifying-the-problematic-instance>Identifying the Problematic Instance</a></li><li><a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#gardenctl-ssh>gardenctl ssh</a></li><li><a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#ssh-with-a-manually-created-bastion-on-aws>SSH with a Manually Created Bastion on AWS</a><ul><li><a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#create-the-bastion-security-group>Create the Bastion Security Group</a></li><li><a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#create-the-bastion-instance>Create the Bastion Instance</a></li></ul></li><li><a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#connecting-to-the-target-instance>Connecting to the Target Instance</a></li></ul></li><li><a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#cleanup>Cleanup</a></li></ul><h2 id=get-a-shell-to-an-operational-cluster-node>Get a Shell to an Operational Cluster Node</h2><p>The following describes four different approaches to get a shell to an operational Shoot worker node.
As a prerequisite to troubleshooting a Kubernetes node, the node must have joined the cluster successfully and be able to run a pod.
All of the described approaches involve scheduling a pod with root permissions and mounting the root filesystem.</p><h3 id=gardener-dashboard>Gardener Dashboard</h3><p><strong>Prerequisite</strong>: the terminal feature is configured for the Gardener dashboard.</p><ol><li>Navigate to the cluster overview page and find the <code>Terminal</code> in the <code>Access</code> tile.</li></ol><img style=margin-left:0;width:80%;height:auto alt="Access Tile" src=/__resources/9fb6ca4ff9b7480f93debba833f48590_864d29.png><br><p>Select the target Cluster (Garden, Seed / Control Plane, Shoot cluster) depending on the requirements and
access rights (only certain users have access to the Seed Control Plane).</p><ol start=2><li>To open the terminal configuration, interact with the top right-hand corner of the screen.</li></ol><img style=margin-left:0 alt="Terminal configuration" src=/__resources/db573582bfc544d294cbde8906a74e07_3f5ce9.png><br><ol start=3><li>Set the Terminal Runtime to &ldquo;Privileged&rdquo;. Also, specify the target node from the drop-down menu.</li></ol><img style=margin-left:0;width:50%;height:auto alt="Dashboard terminal pod configuration" src=/__resources/f7b10d48edf44c17ba838ff5c429e39d_1230dc.png><br><h4 id=result>Result</h4><p>The Dashboard then schedules a pod and opens a shell session to the node.</p><p>To get access to the common binaries installed on the host, prefix the command with <code>chroot /hostroot</code>.
Note that the path depends on where the root path is mounted in the container.
In the default image used by the Dashboard, it is under <code>/hostroot</code>.</p><img style=margin-left:0 alt="Dashboard terminal pod configuration" src=/__resources/3da659e9cc4744a2ad3e1c6a50d39c04_68252d.png><br><h3 id=gardener-ops-toolbelt>Gardener Ops Toolbelt</h3><p><strong>Prerequisite</strong>: <code>kubectl</code> is available.</p><p>The <a href=https://github.com/gardener/ops-toolbelt>Gardener ops-toolbelt</a> can be used as a convenient way to deploy a root pod to a node.
The pod uses an image that is bundled with a bunch of useful <a href=https://github.com/gardener/ops-toolbelt/tree/master/dockerfile-configs>troubleshooting tools</a>.
This is also the same image that is used by default when using the Gardener Dashboard terminal feature as described in the <a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#gardener-dashboard>previous section</a>.</p><p>The easiest way to use the <a href=https://github.com/gardener/ops-toolbelt>Gardener ops-toolbelt</a> is to execute
the <a href=https://github.com/gardener/ops-toolbelt/blob/master/hacks/ops-pod><code>ops-pod</code> script</a> in the <code>hacks</code> folder.
To get root shell access to a node, execute the aforementioned script by supplying the target node name as an argument:</p><pre tabindex=0><code>$ &lt;path-to-ops-toolbelt-repo&gt;/hacks/ops-pod &lt;target-node&gt;
</code></pre><h3 id=custom-root-pod>Custom Root Pod</h3><p>Alternatively, a pod can be <a href=https://kubernetes.io/docs/concepts/configuration/assign-pod-node/>assigned</a> to a target node and a shell can
be opened via <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/>standard Kubernetes means</a>.
To enable root access to the node, the pod specification requires proper <code>securityContext</code> and <code>volume</code> properties.</p><p>For instance, you can use the following pod manifest, after changing <target-node-name>with the name of the node you want this pod attached to:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: privileged-pod
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  nodeSelector:
</span></span><span style=display:flex><span>    kubernetes.io/hostname: &lt;target-node-name&gt;
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: busybox
</span></span><span style=display:flex><span>    image: busybox
</span></span><span style=display:flex><span>    stdin: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    securityContext:
</span></span><span style=display:flex><span>      privileged: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    volumeMounts:
</span></span><span style=display:flex><span>    - name: host-root-volume
</span></span><span style=display:flex><span>      mountPath: /host
</span></span><span style=display:flex><span>      readOnly: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  volumes:
</span></span><span style=display:flex><span>  - name: host-root-volume
</span></span><span style=display:flex><span>    hostPath:
</span></span><span style=display:flex><span>      path: /
</span></span><span style=display:flex><span>  hostNetwork: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  hostPID: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  restartPolicy: Never
</span></span></code></pre></div><h2 id=ssh-access-to-a-node-that-failed-to-join-the-cluster>SSH Access to a Node That Failed to Join the Cluster</h2><p>This section explores two options that can be used to get SSH access to a node that failed to join the cluster.
As it is not possible to schedule a pod on the node, the Kubernetes-based methods explored so far cannot be used in this scenario.</p><p>Additionally, Gardener typically provisions worker instances in a private subnet of the VPC, hence - there is no public IP address that could be used for direct SSH access.</p><p>For this scenario, cloud providers typically have extensive documentation (e.g <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstances.html>AWS</a> & <a href=https://cloud.google.com/compute/docs/instances/connecting-to-instance>GCP</a>
and in <a href=https://cloud.google.com/compute/docs/instances/connecting-advanced#vpn>some cases tooling support</a>).
However, these approaches are mostly cloud provider specific, require interaction via their CLI and API or sometimes
the installation of a <a href=https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-install-ssm-agent.html>cloud provider specific agent</a> on the node.</p><p>Alternatively, <code>gardenctl</code> can be used providing a cloud provider agnostic and out-of-the-box support to get ssh access to an instance in a private subnet.
Currently <code>gardenctl</code> supports AWS, GCP, Openstack, Azure and Alibaba Cloud.</p><h3 id=identifying-the-problematic-instance>Identifying the Problematic Instance</h3><p>First, the problematic instance has to be identified.
In Gardener, worker pools can be created in different cloud provider regions, zones, and accounts.</p><p>The instance would typically show up as successfully started / running in the cloud provider dashboard or API and it is not immediately obvious which one has a problem.
Instead, we can use the Gardener API / CRDs to obtain the faulty instance identifier in a cloud-agnostic way.</p><p>Gardener uses the <a href=https://github.com/gardener/machine-controller-manager>Machine Controller Manager</a> to create the Shoot worker nodes.
For each worker node, the Machine Controller Manager creates a <code>Machine</code> CRD in the Shoot namespace in the respective <code>Seed</code> cluster.
Usually the problematic instance can be identified, as the respective <code>Machine</code> CRD has status <code>pending</code>.</p><p>The instance / node name can be obtained from the <code>Machine</code> <code>.status</code> field:</p><pre tabindex=0><code>$ kubectl get machine &lt;machine-name&gt; -o json | jq -r .status.node
</code></pre><p>This is all the information needed to go ahead and use <code>gardenctl ssh</code> to get a shell to the node.
In addition, the used cloud provider, the specific identifier of the instance, and the instance region can be identified from the <code>Machine</code> CRD.</p><p>Get the identifier of the instance via:</p><pre tabindex=0><code>$ kubectl get machine &lt;machine-name&gt; -o json | jq -r .spec.providerID // e.g aws:///eu-north-1/i-069733c435bdb4640
</code></pre><p>The identifier shows that the instance belongs to the cloud provider <code>aws</code> with the ec2 instance-id <code>i-069733c435bdb4640</code> in region <code>eu-north-1</code>.</p><p>To get more information about the instance, check out the <code>MachineClass</code> (e.g <code>AWSMachineClass</code>) that is associated with each <code>Machine</code> CRD in the <code>Shoot</code> namespace of the <code>Seed</code> cluster.
The <code>AWSMachineClass</code> contains the machine image (<a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html>ami</a>), machine-type, iam information, network-interfaces, subnets, security groups and attached volumes.</p><p>Of course, the information can also be used to get the instance with the cloud provider CLI / API.</p><h3 id=gardenctl-ssh>gardenctl ssh</h3><p>Using the node name of the problematic instance, we can use the <code>gardenctl ssh</code> command to get SSH access to the cloud provider
instance via an automatically set up <a href=https://en.wikipedia.org/wiki/Bastion_host>bastion host</a>.
<code>gardenctl</code> takes care of spinning up the <code>bastion</code> instance, setting up the SSH keys, ports and security groups and opens a root shell on the target instance.
After the SSH session has ended, <code>gardenctl</code> deletes the created cloud provider resources.</p><p>Use the following commands:</p><ol><li>First, target a Garden cluster containing all the Shoot definitions.</li></ol><pre tabindex=0><code>$ gardenctl target garden &lt;target-garden&gt;
</code></pre><ol start=2><li>Target an available Shoot by name.
This sets up the context, configures the <code>kubeconfig</code> file of the Shoot cluster and downloads the cloud provider credentials.
Subsequent commands will execute in this context.</li></ol><pre tabindex=0><code>$ gardenctl target shoot &lt;target-shoot&gt;
</code></pre><ol start=3><li>This uses the cloud provider credentials to spin up the bastion and to open a shell on the target instance.</li></ol><pre tabindex=0><code>$ gardenctl ssh &lt;target-node&gt;
</code></pre><h3 id=ssh-with-a-manually-created-bastion-on-aws>SSH with a Manually Created Bastion on AWS</h3><p>In case you are not using <code>gardenctl</code> or want to control the bastion instance yourself, you can also manually set it up.
The steps described here are generally the same as <a href=https://github.com/gardener/gardenctl/blob/10a537942b94234914758c0f6d053dc1cf218ecd/pkg/cmd/ssh_aws.go#L53-L52>those used by <code>gardenctl</code> internally</a>.
Despite some cloud provider specifics, they can be generalized to the following list:</p><ul><li>Open port 22 on the target instance.</li><li>Create an instance / VM in a public subnet (the bastion instance needs to have a public IP address).</li><li>Set-up security groups and roles, and open port 22 for the bastion instance.</li></ul><p>The following diagram shows an overview of how the SSH access to the target instance works:</p><img style=margin-left:0 alt="SSH Bastion diagram" src=/__resources/913441003e5641bc90249bdc07d55656_90d384.png><br><p>This guide demonstrates the setup of a bastion on AWS.</p><p><strong>Prerequisites:</strong></p><ul><li>The <code>AWS CLI</code> is set up.</li><li>Obtain target <code>instance-id</code> (see <a href=/docs/guides/monitoring-and-troubleshooting/shell-to-node/#identifying-the-problematic-instance>Identifying the Problematic Instance</a>).</li><li>Obtain the VPC ID the Shoot resources are created in. This can be found in the <code>Infrastructure</code> CRD in the <code>Shoot</code> namespace in the <code>Seed</code>.</li><li>Make sure that port 22 on the target instance is open (default for Gardener deployed instances).<ul><li>Extract security group via:</li></ul><pre tabindex=0><code>$ aws ec2 describe-instances --instance-ids &lt;instance-id&gt;
</code></pre><ul><li>Check for rule that allows inbound connections on port 22:</li></ul><pre tabindex=0><code>$ aws ec2 describe-security-groups --group-ids=&lt;security-group-id&gt;
</code></pre><ul><li>If not available, create the rule with the following comamnd:</li></ul><pre tabindex=0><code>$ aws ec2 authorize-security-group-ingress --group-id &lt;security-group-id&gt;  --protocol tcp --port 22 --cidr 0.0.0.0/0
</code></pre></li></ul><h4 id=create-the-bastion-security-group>Create the Bastion Security Group</h4><ol><li>The common name of the security group is <code>&lt;shoot-name>-bsg</code>. Create the security group:</li></ol><pre tabindex=0><code>$ aws ec2 create-security-group --group-name &lt;bastion-security-group-name&gt;  --description ssh-access --vpc-id &lt;VPC-ID&gt;
</code></pre><ol start=2><li>Optionally, create identifying tags for the security group:</li></ol><pre tabindex=0><code>$ aws ec2 create-tags --resources &lt;bastion-security-group-id&gt; --tags Key=component,Value=&lt;tag&gt;
</code></pre><ol start=3><li>Create a permission in the bastion security group that allows ssh access on port 22:</li></ol><pre tabindex=0><code>$ aws ec2 authorize-security-group-ingress --group-id &lt;bastion-security-group-id&gt;  --protocol tcp --port 22 --cidr 0.0.0.0/0
</code></pre><ol start=4><li>Create an IAM role for the bastion instance with the name <code>&lt;shoot-name>-bastions</code>:</li></ol><pre tabindex=0><code>$ aws iam create-role --role-name &lt;shoot-name&gt;-bastions
</code></pre><p>The content should be:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>&#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>&#34;Statement&#34;: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>        &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>        &#34;Action&#34;: [
</span></span><span style=display:flex><span>            <span style=color:#a31515>&#34;ec2:DescribeRegions&#34;</span>
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>        &#34;Resource&#34;: [
</span></span><span style=display:flex><span>            <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>        ]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><ol start=5><li>Create the instance profile and name it <code>&lt;shoot-name>-bastions</code>:</li></ol><pre tabindex=0><code>$ aws iam create-instance-profile --instance-profile-name &lt;name&gt;
</code></pre><ol start=6><li>Add the created role to the instance profile:</li></ol><pre tabindex=0><code>$ aws iam add-role-to-instance-profile --instance-profile-name &lt;instance-profile-name&gt; --role-name &lt;role-name&gt;
</code></pre><h4 id=create-the-bastion-instance>Create the Bastion Instance</h4><p>Next, in order to be able to <code>ssh</code> into the bastion instance, the instance has to be set up with a user with a public ssh key.
Create a user <code>gardener</code> that has the same Gardener-generated public ssh key as the target instance.</p><ol><li>First, we need to get the public part of the <code>Shoot</code> ssh-key.
The ssh-key is stored in a secret in the the project namespace in the Garden cluster.
The name is: <code>&lt;shoot-name>-ssh-publickey</code>.
Get the key via:</li></ol><pre tabindex=0><code>$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\&#34;id_rsa.pub\&#34;
</code></pre><ol start=2><li>A script handed over as <code>user-data</code> to the bastion <code>ec2</code> instance, can be used to create the <code>gardener</code> user and add the ssh-key.
For your convenience, you can use the following script to generate the <code>user-data</code>.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#00f>#!/bin/bash -eu
</span></span></span><span style=display:flex><span><span style=color:#00f></span>saveUserDataFile () {
</span></span><span style=display:flex><span>  ssh_key=$1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cat &gt; gardener-bastion-userdata.sh <span style=color:#a31515>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#a31515>#!/bin/bash -eu
</span></span></span><span style=display:flex><span><span style=color:#a31515>id gardener || useradd gardener -mU
</span></span></span><span style=display:flex><span><span style=color:#a31515>mkdir -p /home/gardener/.ssh
</span></span></span><span style=display:flex><span><span style=color:#a31515>echo &#34;$ssh_key&#34; &gt; /home/gardener/.ssh/authorized_keys
</span></span></span><span style=display:flex><span><span style=color:#a31515>chown gardener:gardener /home/gardener/.ssh/authorized_keys
</span></span></span><span style=display:flex><span><span style=color:#a31515>echo &#34;gardener ALL=(ALL) NOPASSWD:ALL&#34; &gt;/etc/sudoers.d/99-gardener-user
</span></span></span><span style=display:flex><span><span style=color:#a31515>EOF</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>if</span> [ -p /dev/stdin ]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>    read -r input
</span></span><span style=display:flex><span>    cat | saveUserDataFile <span style=color:#a31515>&#34;</span>$input<span style=color:#a31515>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#00f>else</span>
</span></span><span style=display:flex><span>    pbpaste | saveUserDataFile <span style=color:#a31515>&#34;</span>$input<span style=color:#a31515>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#00f>fi</span>
</span></span></code></pre></div><ol start=3><li>Use the script by handing-over the public ssh-key of the <code>Shoot</code> cluster:</li></ol><pre tabindex=0><code>$ kubectl get secret aws-gvisor.ssh-keypair -o json | jq -r .data.\&#34;id_rsa.pub\&#34; | ./generate-userdata.sh
</code></pre><p>This generates a file called <code>gardener-bastion-userdata.sh</code> in the same directory containing the <code>user-data</code>.</p><ol start=4><li>The following information is needed to create the bastion instance:</li></ol><p><code>bastion-IAM-instance-profile-name</code>
- Use the created instance profile with the name <code>&lt;shoot-name>-bastions</code></p><p><code>image-id</code>
- It is possible to use the same image-id as the one used for the target instance (or any other image). Has cloud provider specific format (AWS: <code>ami</code>).</p><p><code>ssh-public-key-name</code></p><pre tabindex=0><code>- This is the ssh key pair already created in the Shoot&#39;s cloud provider account by Gardener during the `Infrastructure` CRD reconciliation.
- The name is usually: `&lt;shoot-name&gt;-ssh-publickey`
</code></pre><p><code>subnet-id</code>
- Choose a subnet that is attached to an <code>Internet Gateway</code> and <code>NAT Gateway</code> (bastion instance must have a public IP).
- The Gardener created public subnet with the name <code>&lt;shoot-name>-public-utility-&lt;xy></code> can be used.
Please check the created subnets with the cloud provider.</p><p><code>bastion-security-group-id</code>
- Use the id of the created bastion security group.</p><p><code>file-path-to-userdata</code>
- Use the filepath to the <code>user-data</code> file generated in the previous step.</p><ul><li><code>bastion-instance-name</code><ul><li>Optionaly, you can tag the instance.</li><li>Usually <code>&lt;shoot-name>-bastions</code></li></ul></li></ul><ol start=5><li>Create the bastion instance via:</li></ol><pre tabindex=0><code>$ ec2 run-instances --iam-instance-profile Name=&lt;bastion-IAM-instance-profile-name&gt; --image-id &lt;image-id&gt;  --count 1 --instance-type t3.nano --key-name &lt;ssh-public-key-name&gt;  --security-group-ids &lt;bastion-security-group-id&gt; --subnet-id &lt;subnet-id&gt; --associate-public-ip-address --user-data &lt;file-path-to-userdata&gt; --tag-specifications ResourceType=instance,Tags=[{Key=Name,Value=&lt;bastion-instance-name&gt;},{Key=component,Value=&lt;mytag&gt;}] ResourceType=volume,Tags=[{Key=component,Value=&lt;mytag&gt;}]&#34;
</code></pre><p>Capture the <code>instance-id</code> from the response and wait until the <code>ec2</code> instance is running and has a public IP address.</p><h3 id=connecting-to-the-target-instance>Connecting to the Target Instance</h3><ol><li>Save the private key of the ssh-key-pair in a temporary local file for later use:</li></ol><pre tabindex=0><code>$ umask 077

$ kubectl get secret &lt;shoot-name&gt;.ssh-keypair -o json | jq -r .data.\&#34;id_rsa\&#34; | base64 -d &gt; id_rsa.key
</code></pre><ol start=2><li>Use the private ssh key to ssh into the bastion instance:</li></ol><pre tabindex=0><code>$ ssh -i &lt;path-to-private-key&gt; gardener@&lt;public-bastion-instance-ip&gt; 
</code></pre><ol start=3><li>If that works, connect from your local terminal to the target instance via the bastion:</li></ol><pre tabindex=0><code>$ ssh  -i &lt;path-to-private-key&gt; -o ProxyCommand=&#34;ssh -W %h:%p -i &lt;private-key&gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no gardener@&lt;public-ip-bastion&gt;&#34; gardener@&lt;private-ip-target-instance&gt; -o IdentitiesOnly=yes -o StrictHostKeyChecking=no
</code></pre><h2 id=cleanup>Cleanup</h2><p>Do not forget to cleanup the created resources. Otherwise Gardener will eventually fail to delete the Shoot.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f8aac904b306d99ba557bd2e49bfe345>6.3 - How to Debug a Pod</h1><div class=lead>Your pod doesn&rsquo;t run as expected. Are there any log files? Where? How could I debug a pod?</div><h2 id=introduction>Introduction</h2><p>Kubernetes offers powerful options to get more details about startup or runtime failures of pods as e.g. described in
<a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/>Application Introspection and Debugging</a>
or <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/>Debug Pods and Replication Controllers</a>.</p><p>In order to identify pods with potential issues, you could e.g. run <code>kubectl get pods --all-namespaces | grep -iv Running</code> to filter
out the pods which are not in the state <code>Running</code>. One of frequent error state is <code>CrashLoopBackOff</code>, which tells that
a pod crashes right after the start. Kubernetes then tries to restart the pod again, but often the pod startup fails again.</p><p>Here is a short list of possible reasons which might lead to a pod crash:</p><ol><li>Error during image pull caused by e.g. wrong/missing secrets or wrong/missing image</li><li>The app runs in an error state caused e.g. by missing environmental variables (ConfigMaps) or secrets</li><li>Liveness probe failed</li><li>Too high resource consumption (memory and/or CPU) or too strict quota settings</li><li>Persistent volumes can&rsquo;t be created/mounted</li><li>The container image is not updated</li></ol><p>Basically, the commands <code>kubectl logs ...</code> and <code>kubectl describe ...</code> with different parameters are used to get more
detailed information. By calling e.g. <code>kubectl logs --help</code> you can get more detailed information about the command and its
parameters.</p><p>In the next sections you&rsquo;ll find some basic approaches to get some ideas what went wrong.</p><p>Remarks:</p><ul><li>Even if the pods seem to be running, as the status <code>Running</code> indicates, a high counter of the <code>Restarts</code> shows potential problems</li><li>You can get a good overview of the troubleshooting process with the interactive tutorial <a href=https://kubernetes.io/docs/tutorials/kubernetes-basics/explore-intro/>Troubleshooting with Kubectl</a> available which explains basic debugging activities</li><li>The examples below are deployed into the namespace <code>default</code>. In case you want to change it, use the optional
parameter <code>--namespace &lt;your-namespace></code> to select the target namespace. The examples require a Kubernetes release ≥ <em>1.8</em>.</li></ul><h2 id=prerequisites>Prerequisites</h2><p>Your deployment was successful (no logical/syntactical errors in the manifest files), but the pod(s) aren&rsquo;t running.</p><h2 id=error-caused-by-wrong-image-name>Error Caused by Wrong Image Name</h2><p>Start by running <code>kubectl describe pod &lt;your-pod> &lt;your-namespace></code> to get detailed information about the pod startup.</p><p>In the <code>Events</code> section, you should get an error message like <code>Failed to pull image ...</code> and <code>Reason: Failed</code>. The pod is
in state <code>ImagePullBackOff</code>.</p><p>The example below is based on a <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/>demo in the Kubernetes documentation</a>. In all examples, the <code>default</code> namespace is used.</p><p>First, perform a cleanup with:</p><p><code>kubectl delete pod termination-demo</code></p><p>Next, create a resource based on the yaml content below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod 
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: termination-demo-container
</span></span><span style=display:flex><span>    image: debiann
</span></span><span style=display:flex><span>    command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>    args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log&#34;</span>]
</span></span></code></pre></div><p><code>kubectl describe pod termination-demo</code> lists in the <code>Event</code> section the content</p><pre tabindex=0><code>Events:
  FirstSeen	LastSeen	Count	From							SubObjectPath					Type		Reason			Message
  ---------	--------	-----	----							-------------					--------	------			-------
  2m		2m		1	default-scheduler											Normal		Scheduled		Successfully assigned termination-demo to ip-10-250-17-112.eu-west-1.compute.internal
  2m		2m		1	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded for volume &#34;default-token-sgccm&#34; 
  2m		1m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Pulling			pulling image &#34;debiann&#34;
  2m		1m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Warning		Failed			Failed to pull image &#34;debiann&#34;: rpc error: code = Unknown desc = Error: image library/debiann:latest not found
  2m		54s		10	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Warning		FailedSync		Error syncing pod
  2m		54s		6	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		BackOff			Back-off pulling image &#34;debiann&#34;
</code></pre><p>The error message with <code>Reason: Failed</code> tells you that there is an error during pulling the image. A closer look at the
image name indicates a misspelling.</p><h2 id=the-app-runs-in-an-error-state-caused-eg-by-missing-environmental-variables-configmaps-or-secrets>The App Runs in an Error State Caused e.g. by Missing Environmental Variables (ConfigMaps) or Secrets</h2><p>This example illustrates the behavior in the case when the app expects environment variables but the corresponding Kubernetes artifacts are missing.</p><p>First, perform a cleanup with:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kubectl delete deployment termination-demo
</span></span><span style=display:flex><span>kubectl delete configmaps app-env
</span></span></code></pre></div><p>Next, deploy the following manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1beta2 
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>     app: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: termination-demo
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: termination-demo
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: termination-demo-container
</span></span><span style=display:flex><span>        image: debian
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>        args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;sed \&#34;s/foo/bar/\&#34; &lt; $MYFILE&#34;</span>]
</span></span></code></pre></div><p>Now, the command <code>kubectl get pods</code> lists the pod <code>termination-demo-xxx</code> in the state <code>Error</code> or <code>CrashLoopBackOff</code>.
The command <code>kubectl describe pod termination-demo-xxx</code> tells you that there is no error during startup but gives no clue about what caused the crash.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  FirstSeen	LastSeen	Count	From							SubObjectPath					Type		Reason		Message
</span></span><span style=display:flex><span>  ---------	--------	-----	----							-------------					--------	------		-------
</span></span><span style=display:flex><span>  19m		19m		1	default-scheduler											Normal		Scheduled	Successfully assigned termination-demo-5fb484867d-xz2x9 to ip-10-250-17-112.eu-west-1.compute.internal
</span></span><span style=display:flex><span>  19m		19m		1	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded <span style=color:#00f>for</span> volume <span style=color:#a31515>&#34;default-token-sgccm&#34;</span> 
</span></span><span style=display:flex><span>  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Pulling		pulling image <span style=color:#a31515>&#34;debian&#34;</span>
</span></span><span style=display:flex><span>  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Pulled		Successfully pulled image <span style=color:#a31515>&#34;debian&#34;</span>
</span></span><span style=display:flex><span>  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Created		Created container
</span></span><span style=display:flex><span>  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Started		Started container
</span></span><span style=display:flex><span>  19m		14m		24	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Warning		BackOff		Back-off restarting failed container
</span></span><span style=display:flex><span>  19m		4m		69	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Warning		FailedSync	Error syncing pod
</span></span></code></pre></div><p>The command <code>kubectl get logs termination-demo-xxx</code> gives access to the output, the application writes on <code>stderr</code> and
<code>stdout</code>. In this case, you should get an output similar to:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>/bin/sh: 1: cannot open : No such file
</span></span></code></pre></div><p>So you need to have a closer look at the application. In this case, the environmental variable <code>MYFILE</code> is missing. To fix this
issue, you could e.g. add a ConfigMap to your deployment as is shown in the manifest listed below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: app-env
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  MYFILE: <span style=color:#a31515>&#34;/etc/profile&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: apps/v1beta2 
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>     app: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: termination-demo
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: termination-demo
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: termination-demo-container
</span></span><span style=display:flex><span>        image: debian
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>        args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;sed \&#34;s/foo/bar/\&#34; &lt; $MYFILE&#34;</span>]
</span></span><span style=display:flex><span>        envFrom:
</span></span><span style=display:flex><span>        - configMapRef:
</span></span><span style=display:flex><span>            name: app-env 
</span></span></code></pre></div><p>Note that once you fix the error and re-run the scenario, you might still see the pod in a <code>CrashLoopBackOff</code> status.
It is because the container finishes the command <code>sed ...</code> and runs to completion. In order to keep the container in a <code>Running</code> status,
a long running task is required, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: app-env
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  MYFILE: <span style=color:#a31515>&#34;/etc/profile&#34;</span>
</span></span><span style=display:flex><span>  SLEEP: <span style=color:#a31515>&#34;5&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: apps/v1beta2
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>     app: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: termination-demo
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: termination-demo
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: termination-demo-container
</span></span><span style=display:flex><span>        image: debian
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:green># args: [&#34;-c&#34;, &#34;sed \&#34;s/foo/bar/\&#34; &lt; $MYFILE&#34;]</span>
</span></span><span style=display:flex><span>        args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;while true; do sleep $SLEEP; echo sleeping; done;&#34;</span>]
</span></span><span style=display:flex><span>        envFrom:
</span></span><span style=display:flex><span>        - configMapRef:
</span></span><span style=display:flex><span>            name: app-env
</span></span></code></pre></div><h2 id=too-high-resource-consumption-memory-andor-cpu-or-too-strict-quota-settings>Too High Resource Consumption (Memory and/or CPU) or Too Strict Quota Settings</h2><p>You can optionally specify the amount of memory and/or CPU your container gets during runtime. In case these settings are missing,
the default requests settings are taken: CPU: 0m (in Milli CPU) and RAM: 0Gi, which indicate no other limits other than the
ones of the node(s) itself. For more details, e.g. about how to configure limits, see <a href=https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/>Configure Default Memory Requests and Limits for a Namespace</a>.</p><p>In case your application needs more resources, Kubernetes distinguishes between <code>requests</code> and <code>limit</code> settings: <code>requests</code>
specify the guaranteed amount of resource, whereas <code>limit</code> tells Kubernetes the maximum amount of resource the container might
need. Mathematically, both settings could be described by the relation <code>0 &lt;= requests &lt;= limit</code>. For both settings you need to
consider the total amount of resources your nodes provide. For a detailed description of the concept, see <a href=https://github.com/kubernetes/design-proposals-archive/blob/main/node/resource-qos.md>Resource Quality of Service in Kubernetes</a>.</p><p>Use <code>kubectl describe nodes</code> to get a first overview of the resource consumption in your cluster. Of special interest are the
figures indicating the amount of CPU and Memory Requests at the bottom of the output.</p><p>The next example demonstrates what happens in case the CPU request is too high in order to be managed by your cluster.</p><p>First, perform a cleanup with:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kubectl delete deployment termination-demo
</span></span><span style=display:flex><span>kubectl delete configmaps app-env
</span></span></code></pre></div><p>Next, adapt the <code>cpu</code> below in the yaml below to be slightly higher than the remaining CPU resources in your cluster and deploy
this manifest. In this example, <code>600m</code> (milli CPUs) are requested in a Kubernetes system with a single 2 core worker
node which results in an error message.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1beta2 
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>     app: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: termination-demo
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: termination-demo
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: termination-demo-container
</span></span><span style=display:flex><span>        image: debian
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>        args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log&#34;</span>]
</span></span><span style=display:flex><span>        resources:
</span></span><span style=display:flex><span>          requests:
</span></span><span style=display:flex><span>            cpu: <span style=color:#a31515>&#34;600m&#34;</span> 
</span></span></code></pre></div><p>The command <code>kubectl get pods</code> lists the pod <code>termination-demo-xxx</code> in the state <code>Pending</code>. More details on why this happens
could be found by using the command <code>kubectl describe pod termination-demo-xxx</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl describe po termination-demo-fdb7bb7d9-mzvfw
</span></span><span style=display:flex><span>Name:           termination-demo-fdb7bb7d9-mzvfw
</span></span><span style=display:flex><span>Namespace:      default
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Containers:
</span></span><span style=display:flex><span>  termination-demo-container:
</span></span><span style=display:flex><span>    Image:      debian
</span></span><span style=display:flex><span>    Port:       &lt;none&gt;
</span></span><span style=display:flex><span>    Host Port:  &lt;none&gt;
</span></span><span style=display:flex><span>    Command:
</span></span><span style=display:flex><span>      /bin/sh
</span></span><span style=display:flex><span>    Args:
</span></span><span style=display:flex><span>      -c
</span></span><span style=display:flex><span>      sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log
</span></span><span style=display:flex><span>    Requests:
</span></span><span style=display:flex><span>      cpu:        6
</span></span><span style=display:flex><span>    Environment:  &lt;none&gt;
</span></span><span style=display:flex><span>    Mounts:
</span></span><span style=display:flex><span>      /var/run/secrets/kubernetes.io/serviceaccount from default-token-t549m (ro)
</span></span><span style=display:flex><span>Conditions:
</span></span><span style=display:flex><span>  Type           Status
</span></span><span style=display:flex><span>  PodScheduled   False
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  Type     Reason            Age               From               Message
</span></span><span style=display:flex><span>  ----     ------            ----              ----               -------
</span></span><span style=display:flex><span>  Warning  FailedScheduling  9s (x7 over 40s)  default-scheduler  0/2 nodes are available: 2 Insufficient cpu.
</span></span></code></pre></div><p>You can find more details in:</p><ul><li><a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>Managing Compute Resources for Containters</a></li><li><a href=https://github.com/kubernetes/design-proposals-archive/blob/main/node/resource-qos.md>Resource Quality of Service in Kubernetes</a></li></ul><p>Remarks:</p><ul><li>This example works similarly when specifying a too high request for memory</li><li>In case you configured an autoscaler range when creating your Kubernetes cluster, another worker node will be spinned up automatically if you didn&rsquo;t reach the maximum number of worker nodes</li><li>In case your app is running out of memory (the memory settings are too small), you will typically find an <code>OOMKilled</code> (Out Of Memory) message in the <code>Events</code> section of the <code>kubectl describe pod ...</code> output</li></ul><h2 id=the-container-image-is-not-updated>The Container Image Is Not Updated</h2><p>You applied a fix in your app, created a new container image and pushed it into your container repository. After redeploying your Kubernetes manifests, you expected to get the updated app, but the same bug is still in the new deployment present.</p><p>This behavior is related to how Kubernetes decides whether to pull a new docker image or to use the cached one.</p><p>In case you didn&rsquo;t change the image tag, the default image policy <em>IfNotPresent</em> tells Kubernetes to use the cached image (see <a href=https://kubernetes.io/docs/concepts/containers/images/>Images</a>).</p><p>As a best practice, you should not use the tag <code>latest</code> and change the image tag in case you changed anything in your image (see <a href=https://kubernetes.io/docs/concepts/configuration/overview/#container-images>Configuration Best Practices</a>).</p><p>Please have a look at this FAQ <a href=https://github.tools.sap/kubernetes/documentation/blob/master/website/documentation/015-tutorials/image-pull-policy/_index.md>Container Image Not Updating</a> for further details.</p><h2 id=related-links>Related Links</h2><ul><li><a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/>Application Introspection and Debugging</a></li><li><a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/>Debug Pods and Replication Controllers</a></li><li><a href=https://kubernetes.io/docs/concepts/cluster-administration/logging/>Logging Architecture</a></li><li><a href=https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/>Configure Default Memory Requests and Limits for a Namespace</a></li><li><a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>Managing Compute Resources for Containters</a></li><li><a href=https://github.com/kubernetes/design-proposals-archive/blob/main/node/resource-qos.md>Resource Quality of Service in Kubernetes</a></li><li><a href=https://kubernetes.io/docs/tutorials/kubernetes-basics/explore-intro/>Interactive Tutorial Troubleshooting with Kubectl</a></li><li><a href=https://kubernetes.io/docs/concepts/containers/images/>Images</a></li><li><a href=https://kubernetes.io/docs/concepts/configuration/overview/#container-images>Kubernetes Best Practises</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cce198c05b67d5037cc341751389bfc6>6.4 - tail -f /var/log/my-application.log</h1><div class=lead>Aggregate log files from different pods</div><h2 id=problem>Problem</h2><p>One thing that always bothered me was that I couldn&rsquo;t get logs of several pods at once with <code>kubectl</code>. A simple
<code>tail -f &lt;path-to-logfile></code> isn&rsquo;t possible at all. Certainly, you can use <code>kubectl logs -f &lt;pod-id></code>, but it doesn&rsquo;t
help if you want to monitor more than one pod at a time.</p><p>This is something you really need a lot, at least if you run several instances of a pod behind a <code>deployment</code>.
This is even more so if you don&rsquo;t have a Kibana or a similar setup.</p><img src=/__resources/howto-kubetail_85f441.png width=100%><h2 id=solution>Solution</h2><p>Luckily, there are smart developers out there who always come up with solutions. The <strong>finding of the week</strong> is
a small bash script that allows you to aggregate log files of several pods at the same time in
a simple way. The script is called <code>kubetail</code> and is available at
<a href=https://github.com/johanhaleby/kubetail>GitHub</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-54886e62cc8767ab592a5259b6e86b39>7 - Applications</h1></div><div class=td-content><h1 id=pg-62135cfd7078016510b68233833b7d69>7.1 - Specifying a Disruption Budget for Kubernetes Controllers</h1><h2 id=introduction-of-disruptions>Introduction of Disruptions</h2><p>We need to understand that some kind of voluntary disruptions can happen to pods.
For example, they can be caused by cluster administrators who want to perform automated cluster actions, like upgrading and autoscaling clusters.
Typical application owner actions include:</p><ul><li>deleting the deployment or other controller that manages the pod</li><li>updating a deployment&rsquo;s pod template causing a restart</li><li>directly deleting a pod (e.g., by accident)</li></ul><h2 id=setup-pod-disruption-budgets>Setup Pod Disruption Budgets</h2><p>Kubernetes offers a feature called PodDisruptionBudget (PDB) for each application.
A PDB limits the number of pods of a replicated application that are down simultaneously from voluntary disruptions.</p><p>The most common use case is when you want to protect an application specified by one of the built-in Kubernetes controllers:</p><ul><li>Deployment</li><li>ReplicationController</li><li>ReplicaSet</li><li>StatefulSet</li></ul><p>A PodDisruptionBudget has three fields:</p><ul><li>A label selector <code>.spec.selector</code> to specify the set of pods to which it applies.</li><li><code>.spec.minAvailable</code> which is a description of the number of pods from that set that must still be available after the eviction, even in the absence of the evicted pod. minAvailable can be either an absolute number or a percentage.</li><li><code>.spec.maxUnavailable</code> which is a description of the number of pods from that set that can be unavailable after the eviction. It can be either an absolute number or a percentage.</li></ul><h2 id=cluster-upgrade-or-node-deletion-failed-due-to-pdb-violation>Cluster Upgrade or Node Deletion Failed due to PDB Violation:</h2><p>Misconfiguration of the PDB could block the cluster upgrade or node deletion processes. There are two main cases that can cause a misconfiguration.</p><h3 id=case-1-the-replica-of-kubernetes-controllers-is-1>Case 1: The replica of Kubernetes controllers is 1</h3><ul><li>Only 1 replica is running: there is no <code>replicaCount</code> setup or <code>replicaCount</code> for the Kubernetes controllers is set to 1</li><li>PDB configuration<pre tabindex=0><code>  spec:
    minAvailable: 1
</code></pre></li><li>To fix this PDB misconfiguration, you need to change the value of <code>replicaCount</code> for the Kubernetes controllers to a number greater than 1</li></ul><h3 id=case-2-hpa-configuration-violates-pdb>Case 2: HPA configuration violates PDB</h3><p>In Kubernetes, a HorizontalPodAutoscaler automatically updates a workload resource (such as a Deployment or StatefulSet), with the aim of automatically scaling the workload to match demand.
The HorizontalPodAutoscaler manages the replicas field of the Kubernetes controllers.</p><ul><li>There is no <code>replicaCount</code> setup or <code>replicaCount</code> for the Kubernetes controllers is set to 1</li><li>PDB configuration<pre tabindex=0><code>  spec:
    minAvailable: 1
</code></pre></li><li>HPA configuration<pre tabindex=0><code>  spec:
    minReplicas: 1
</code></pre></li><li>To fix this PDB misconfiguration, you need to change the value of HPA <code>minReplicas</code> to be greater than 1</li></ul><h2 id=related-links>Related Links</h2><ul><li><a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb/>Specifying a Disruption Budget for Your Application</a></li><li><a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscaling</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7c572a9ae88d7a5421e956eafa49a39e>7.2 - Access a Port of a Pod Locally</h1><h2 id=question>Question</h2><p>You have deployed an application with a web UI or an internal endpoint in your Kubernetes (K8s) cluster. How to access this endpoint <strong>without an external load balancer</strong> (e.g. Ingress)?</p><p>This tutorial presents two options:</p><ul><li>Using Kubernetes port forward</li><li>Using Kubernetes apiserver proxy</li></ul><p>Please note that the options described here are mostly for quick testing or troubleshooting your application. For enabling access to your application for productive environment, please refer to the <a href=https://kubernetes.io/docs/concepts/services-networking/service/>official Kubernetes documentation</a>.</p><h2 id=solution-1-using-kubernetes-port-forward>Solution 1: Using Kubernetes port forward</h2><p>You could use the port forwarding functionality of <code>kubectl</code> to access the pods from your local host <strong>without involving a service</strong>.</p><p>To access any pod follow these steps:</p><ol><li>Run <code>kubectl get pods</code></li><li>Note down the name of the pod in question as <code>&lt;your-pod-name></code></li><li>Run <code>kubectl port-forward &lt;your-pod-name> &lt;local-port>:&lt;your-app-port></code></li><li>Run a web browser or curl locally and enter the URL: <code>http(s)://localhost:&lt;local-port></code></li></ol><p>In addition, <code>kubectl port-forward</code> allows using a resource name, such as a deployment name or service name, to select a matching pod to port forward.
More details can be found in the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/>Kubernetes documentation</a>.</p><p>The main drawback of this approach is that the pod&rsquo;s name changes as soon as it is restarted. Moreover, you need to have a web browser on your client and you need to make sure that the local port is not already used by an application running on your system. Finally, sometimes the port forwarding is canceled due to nonobvious reasons. This leads to a kind of shaky approach. A more stable possibility is based on accessing the app via the kube-proxy, which accesses the corresponding service.</p><p><img src=/__resources/howto-port-forward_f07bcc.svg alt=port-forward></p><h2 id=solution-2-using-the-apiserver-proxy-of-your-kubernetes-cluster>Solution 2: Using the apiserver proxy of Your Kubernetes Cluster</h2><p>There are <a href=https://kubernetes.io/docs/concepts/cluster-administration/proxies/>several different proxies</a> in Kubernetes. In this tutorial we will be using <em>apiserver proxy</em> to enable the access to the services in your cluster without Ingress. <strong>Unlike the first solution, here a service is required.</strong></p><p>Use the following format to compose a URL for accessing your service through an existing proxy on the Kubernetes cluster:</p><p><code>https://&lt;your-cluster-master>/api/v1/namespace/&lt;your-namespace>/services/&lt;your-service>:&lt;your-service-port>/proxy/&lt;service-endpoint></code></p><p><strong>Example:</strong></p><table><thead><tr><th>your-main-cluster</th><th style=text-align:center>your-namespace</th><th style=text-align:right>your-service</th><th style=text-align:right>your-service-port</th><th style=text-align:right>your-service-endpoint</th><th style=text-align:right>url to access service</th></tr></thead><tbody><tr><td>api.testclstr.cpet.k8s.sapcloud.io</td><td style=text-align:center>default</td><td style=text-align:right>nginx-svc</td><td style=text-align:right>80</td><td style=text-align:right>/</td><td style=text-align:right><code>http://api.testclstr.cpet.k8s.sapcloud.io/api/v1/namespaces/default/services/nginx-svc:80/proxy/</code></td></tr><tr><td>api.testclstr.cpet.k8s.sapcloud.io</td><td style=text-align:center>default</td><td style=text-align:right>docker-nodejs-svc</td><td style=text-align:right>4500</td><td style=text-align:right>/cpu?baseNumber=4</td><td style=text-align:right><code>https://api.testclstr.cpet.k8s.sapcloud.io/api/v1/namespaces/default/services/docker-nodejs-svc:4500/proxy/cpu?baseNumber=4</code></td></tr></tbody></table><p>For more details on the format, please refer to the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services>official Kubernetes documentation</a>.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>There are applications which do not support relative URLs yet, e.g. <a href=https://github.com/prometheus/prometheus/issues/1583>Prometheus</a> (as of November, 2022).
This typically leads to missing JavaScript objects, which could be investigated with your browser&rsquo;s development tools. If such an issue occurs, please use the <code>port-forward</code> approach <a href=/docs/guides/applications/access-pod-from-local/#solution-1-using-kubernetes-port-forward>described above</a>.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-a9c93d6cbc648f7853fd4d0aae5fd5f3>7.3 - Auditing Kubernetes for Secure Setup</h1><div class=lead>A few insecure configurations in Kubernetes</div><p><img src=/__resources/teaser_62a0a1.svg alt=teaser></p><h2 id=increasing-the-security-of-all-gardener-stakeholders>Increasing the Security of All Gardener Stakeholders</h2><p>In summer 2018, the <a href=https://github.com/gardener/gardener>Gardener project team</a> asked <a href=https://kinvolk.io/>Kinvolk</a> to execute several penetration tests in its role as third-party contractor. The goal of this ongoing work was to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a <a href=https://kubernetes.io/blog/2018/05/17/gardener/#kubernetes-control-plane>Control-Plane-as-a-Service</a> with a <a href=https://kubernetes.io/blog/2018/05/17/gardener/#network-air-gap>network air gap</a>.</p><p>Along the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.</p><h2 id=major-findings>Major Findings</h2><p>From this experience, we’d like to share a few examples of security issues that could happen on a Kubernetes installation and how to fix them.</p><p>Alban Crequy (<a href=https://kinvolk.io/>Kinvolk</a>) and Dirk Marwinski (<a href=https://www.sap.com>SAP SE</a>) gave a presentation entitled <a href=https://kccncchina2018english.sched.com/event/H2Hd/hardening-multi-cloud-kubernetes-clusters-as-a-service-dirk-marwinski-sap-se-alban-crequy-kinvolk-gmbh>Hardening Multi-Cloud Kubernetes Clusters as a Service</a> at KubeCon 2018 in Shanghai presenting some of the findings.</p><p>Here is a summary of the findings:</p><ul><li><p>Privilege escalation due to insecure configuration of the Kubernetes
API server</p><ul><li>Root cause: Same certificate authority (CA) is used for both the
API server and the proxy that allows accessing the API server.</li><li>Risk: Users can get access to the API server.</li><li>Recommendation: Always use different CAs.</li></ul></li><li><p>Exploration of the control plane network with malicious
HTTP-redirects</p><ul><li><p>Root cause: See detailed description below.</p></li><li><p>Risk: Provoked error message contains full HTTP payload from an
existing endpoint which can be exploited. The contents of the
payload depends on your setup, but can potentially be user data,
configuration data, and credentials.</p></li><li><p>Recommendation:</p><ul><li>Use the latest version of Gardener</li><li>Ensure the seed cluster&rsquo;s container network supports
network policies. Clusters that have been created with
<a href=https://github.com/gardener/kubify>Kubify</a> are not
protected as Flannel is used there which doesn&rsquo;t support
network policies.</li></ul></li></ul></li><li><p>Reading private AWS metadata via Grafana</p><ul><li>Root cause: It is possible to configuring a new custom data
source in Grafana, we could send HTTP requests to target the
control</li><li>Risk: Users can get the &ldquo;user-data&rdquo; for the seed cluster from
the metadata service and retrieve a kubeconfig for that
Kubernetes cluster</li><li>Recommendation: Lockdown Grafana features to only what&rsquo;s
necessary in this setup, block all unnecessary outgoing traffic,
move Grafana to a different network, lockdown unauthenticated
endpoints</li></ul></li></ul><h2 id=scenario-1-privilege-escalation-with-insecure-api-server>Scenario 1: Privilege Escalation with Insecure API Server</h2><p>In most configurations, different components connect directly to the Kubernetes API server, often using a <code>kubeconfig</code> with a client
certificate. The API server is started with the flag:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>/hyperkube apiserver --client-ca-file=/srv/kubernetes/ca/ca.crt ...
</span></span></code></pre></div><p>The API server will check whether the client certificate presented by kubectl, kubelet, scheduler or another component is really signed by the configured certificate authority for clients.</p><p><img src=/__resources/image3_240022.png alt><em>The API server can have many clients of various kinds</em><br><br><br></p><p>However, it is possible to configure the API server differently for use with an intermediate authenticating proxy. The proxy will authenticate the client with its own custom method and then issue HTTP requests to the API server with additional HTTP headers specifying the user name and group name. The API server should only accept HTTP requests with HTTP headers from a legitimate proxy. To allow the API server to check incoming requests, you need pass on a list of certificate authorities (CAs) to it. Requests coming from a proxy are only accepted if they use a client certificate that is signed by one of the CAs of that list.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>--requestheader-client-ca-file=/srv/kubernetes/ca/ca-proxy.crt
</span></span><span style=display:flex><span>--requestheader-username-headers=X-Remote-User
</span></span><span style=display:flex><span>--requestheader-group-headers=X-Remote-Group
</span></span></code></pre></div><p><img src=/__resources/image2_4208a9.png alt><em>API server clients can reach the API server through an authenticating proxy</em><br><br><br></p><p>So far, so good. But what happens if the malicious user “Mallory” tries to connect directly to the API server and reuses
the HTTP headers to pretend to be someone else?</p><p><img src=/__resources/image8_6f21b9.png alt><em>What happens when a client bypasses the proxy, connecting directly to the API server?</em><br><br><br></p><p>With a correct configuration, Mallory’s kubeconfig will have a certificate signed by the API server certificate authority
but not signed by the proxy certificate authority. So the API server will not accept the extra HTTP header “X-Remote-Group: system:masters”.</p><p>You only run into an issue when the same certificate authority is used for both the API server and the proxy. Then, any Kubernetes
client certificate can be used to take the role of different user or group as the API server will accept the user header and
group header.</p><p>The <code>kubectl</code> tool does not normally add those HTTP headers but it’s pretty easy to generate the corresponding HTTP
requests manually.</p><p>We worked on <a href=https://github.com/kubernetes/website/pull/10093>improving the Kubernetes documentation</a> to make clearer
that this configuration should be avoided.</p><h2 id=scenario-2-exploration-of-the-control-plane-network-with-malicious-http-redirects>Scenario 2: Exploration of the Control Plane Network with Malicious HTTP-Redirects</h2><p>The API server is a central component of Kubernetes and many components initiate connections to it, including the kubelet
running on worker nodes. Most of the requests from those clients will end up updating Kubernetes objects (pods, services,
deployments, and so on) in the etcd database but the API server usually does not need to initiate TCP connections itself.</p><p><img src=/__resources/image7_3b6115.png alt><em>The API server is mostly a component that receives requests</em><br><br><br></p><p>However, there are exceptions. Some <code>kubectl</code> commands will trigger the API server to open a new
connection to the kubelet. <code>kubectl exec</code> is one of those commands. In order to get the standard I/Os from the pod,
the API server will start an HTTP connection to the kubelet on the worker node where the pod is running. Depending on
the container runtime used, it can be done in different ways, but one way to do it is for the kubelet to reply with a
HTTP-302 redirection to the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md>Container Runtime Interface (CRI)</a>.
Basically, the kubelet is telling the API server to get the streams from CRI itself directly instead of forwarding. The
redirection from the kubelet will only change the port and path from the URL; the IP address will not be changed because
the kubelet and the CRI component run on the same worker node.</p><p><img src=/__resources/image1_946466.png alt><em>But the API server also initiates some connections, for example, to worker nodes</em><br><br><br></p><p>It’s often quite easy for users of a Kubernetes cluster to get access to worker nodes and tamper with the kubelet. They
could be given explicit SSH access or they could be given a kubeconfig with enough privileges to create privileged pods
or even just pods with “host” volumes.</p><p>In contrast, users (even those with “system:masters” permissions or “root” rights) are often not given access to the control plane.
On setups like, for example, GKE or Gardener, the control plane is running on separate nodes, with a different administrative
access. It could be hosted on a different cloud provider account. So users are not free to explore the internal network
in the control plane.</p><p>What would happen if a user was tampering with the kubelet to make it maliciously redirect <code>kubectl exec</code> requests to
a different random endpoint? Most likely the given endpoint would not speak to the streaming server protocol, so there would
be an error. However, the full HTTP payload from the endpoint is included in the error message printed by kubectl exec.</p><p><img src=/__resources/image6_8cfda7.png alt><em>The API server is tricked to connect to other components</em><br><br><br></p><p>The impact of this issue depends on the specific setup. But in many configurations, we could find a metadata service
(such as the <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-metadata.html>AWS metadata service</a>)
containing user data, configurations and credentials. The setup we explored had a different AWS account and a different
<a href=https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html>EC2 instance profile</a>
for the worker nodes and the control plane. This issue allowed users to get access to the AWS metadata service in the
context of the control plane, which they should not have access to.</p><p>We have reported this issue to the <a href=https://kubernetes.io/docs/reference/issues-security/security/>Kubernetes Security mailing list</a>
and the public pull request that addresses the issue has been merged <a href=https://github.com/kubernetes/kubernetes/pull/66516>PR#66516</a>.
It provides a way to enforce HTTP redirect validation (disabled by default).</p><p>But there are several other ways that users could trigger the API server to generate HTTP requests and get the reply
payload back, so it is advised to isolate the API server and other components from the network as additional precautious measures.
Depending on where the API server runs, it could be with <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>Kubernetes Network Policies</a>, <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html>EC2 Security Groups</a> or just iptables directly. Following the <a href=https://en.wikipedia.org/wiki/Defense_in_depth_(computing)>defense in depth principle</a>,
it is a good idea to apply the API server HTTP redirect validation when it is available as well as firewall rules.</p><p>In Gardener, this has been fixed with Kubernetes network policies along with changes to ensure the API server does
not need to contact the metadata service. You can see more details in the <a href=https://groups.google.com/forum/#!forum/gardener>announcements on the Gardener mailing list</a>.
This is tracked in <a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-2475">CVE-2018-2475</a>.</p><p><em>To be protected from this issue, stakeholders should:</em></p><ul><li><em>Use the latest version of Gardener</em></li><li><em>Ensure the seed cluster’s container network supports network policies. Clusters that have been created with
<a href=https://github.com/gardener/kubify>Kubify</a> are not protected as Flannel is used there which doesn’t support network
policies.</em></li></ul><h2 id=scenario-3-reading-private-aws-metadata-via-grafana>Scenario 3: Reading Private AWS Metadata via Grafana</h2><p>For our tests, we had access to a Kubernetes setup where users are not only given access to the API server in the control
plane, but also to a Grafana instance that is used to gather data from their Kubernetes clusters via Prometheus. The control
plane is managed and users don’t have access to the nodes that it runs. They can only access the API server and Grafana
via a load balancer. The internal network of the control plane is therefore hidden to users.</p><p><img src=/__resources/image5_8e3d9a.png alt><em>Prometheus and Grafana can be used to monitor worker nodes</em><br><br><br></p><p>Unfortunately, that setup was not protecting the control plane network from nosy users. By configuring a new custom
data source in Grafana, we could send HTTP requests to target the control plane network, for example the AWS metadata
service. The reply payload is not displayed on the Grafana Web UI but it is possible to access it from the debugging
console of the Chrome browser.</p><p><img src=/__resources/image9_0831d6.png alt><em>Credentials can be retrieved from the debugging console of Chrome</em><br><br><br></p><p><img src=/__resources/image4_e70065.png alt><em>Adding a Grafana data source is a way to issue HTTP requests to arbitrary targets</em><br><br><br></p><p>In that installation, users could get the “user-data” for the seed cluster from the metadata service and retrieve a
kubeconfig for that Kubernetes cluster.</p><p>There are many possible measures to avoid this situation: lockdown Grafana features to only what’s necessary in this setup, block all
unnecessary outgoing traffic, move Grafana to a different network, or lockdown unauthenticated endpoints, among others.</p><h2 id=conclusion>Conclusion</h2><p>The three scenarios above show pitfalls with a Kubernetes setup. A lot of them were specific to the Kubernetes
installation: different cloud providers or different configurations will show different weaknesses. Users should no longer be given access to Grafana.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2e8efeb071c3d7a7cfbd4334db1fb8b8>7.4 - Container Image Not Pulled</h1><div class=lead>Wrong Container Image or Invalid Registry Permissions</div><h2 id=problem>Problem</h2><p>Two of the most common causes of this problems are specifying the wrong container image or trying to use private images without providing registry credentials.</p><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>There is no observable difference in pod status between a missing image and incorrect registry permissions.
In either case, Kubernetes will report an <code>ErrImagePull</code> status for the pods. For this reason, this article deals with
both scenarios.</div><h2 id=example>Example</h2><p>Let&rsquo;s see an example. We&rsquo;ll create a pod named <em>fail</em>, referencing a non-existent Docker image:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl run -i --tty fail --image=tutum/curl:1.123456
</span></span></code></pre></div><p>The command doesn&rsquo;t return and you can terminate the process with <code>Ctrl+C</code>.</p><h2 id=error-analysis>Error Analysis</h2><p>We can then inspect our pods and see that we have one pod with a status of <strong>ErrImagePull</strong> or <strong>ImagePullBackOff</strong>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ (minikube) kubectl get pods
</span></span><span style=display:flex><span>NAME                      READY     STATUS         RESTARTS   AGE
</span></span><span style=display:flex><span>client-5b65b6c866-cs4ch   1/1       Running        1          1m
</span></span><span style=display:flex><span>fail-6667d7685d-7v6w8     0/1       ErrImagePull   0          &lt;invalid&gt;
</span></span><span style=display:flex><span>vuejs-578574b75f-5x98z    1/1       Running        0          1d
</span></span><span style=display:flex><span>$ (minikube) 
</span></span></code></pre></div><p>For some additional information, we can <code>describe</code> the failing pod.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl describe pod fail-6667d7685d-7v6w8
</span></span></code></pre></div><p>As you can see in the events section, your image can&rsquo;t be pulled:</p><pre tabindex=0><code>Name:		fail-6667d7685d-7v6w8
Namespace:	default
Node:		minikube/192.168.64.10
Start Time:	Wed, 22 Nov 2017 10:01:59 +0100
Labels:		pod-template-hash=2223832418
		run=fail
Annotations:	kubernetes.io/created-by={&#34;kind&#34;:&#34;SerializedReference&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;reference&#34;:{&#34;kind&#34;:&#34;ReplicaSet&#34;,&#34;namespace&#34;:&#34;default&#34;,&#34;name&#34;:&#34;fail-6667d7685d&#34;,&#34;uid&#34;:&#34;cc4ccb3f-cf63-11e7-afca-4a7a1fa05b3f&#34;,&#34;a...
.
.
.
.
Events:
  FirstSeen	LastSeen	Count	From			SubObjectPath		Type		Reason			Message
  ---------	--------	-----	----			-------------		--------	------			-------
  1m		1m		1	default-scheduler				Normal		Scheduled		Successfully assigned fail-6667d7685d-7v6w8 to minikube
  1m		1m		1	kubelet, minikube				Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded for volume &#34;default-token-9fr6r&#34; 
  1m		6s		4	kubelet, minikube	spec.containers{fail}	Normal		Pulling			pulling image &#34;tutum/curl:1.123456&#34;
  1m		5s		4	kubelet, minikube	spec.containers{fail}	Warning		Failed			Failed to pull image &#34;tutum/curl:1.123456&#34;: rpc error: code = Unknown desc = Error response from daemon: manifest for tutum/curl:1.123456 not found
  1m		&lt;invalid&gt;	10	kubelet, minikube				Warning		FailedSync		Error syncing pod
  1m		&lt;invalid&gt;	6	kubelet, minikube	spec.containers{fail}	Normal		BackOff			Back-off pulling image &#34;tutum/curl:1.123456&#34;
</code></pre><p><strong>Why couldn&rsquo;t Kubernetes pull the image?</strong>
There are three primary candidates besides network connectivity issues:</p><ul><li>The image tag is incorrect</li><li>The image doesn&rsquo;t exist</li><li>Kubernetes doesn&rsquo;t have permissions to pull that image</li></ul><p>If you don&rsquo;t notice a typo in your image tag, then it&rsquo;s time to test using your local machine. I usually start by
running <strong>docker pull on my local development machine</strong> with the exact same image tag. In this case, I would
run <code>docker pull tutum/curl:1.123456</code>.</p><p>If this succeeds, then it probably means that Kubernetes doesn&rsquo;t have the correct permissions to pull that image.</p><p>Add the docker registry user/pwd to your cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl create secret docker-registry dockersecret --docker-server=https://index.docker.io/v1/ --docker-username=&lt;username&gt; --docker-password=&lt;password&gt; --docker-email=&lt;email&gt;
</span></span></code></pre></div><p>If the exact image tag fails, then I will test without an explicit image tag:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>docker pull tutum/curl
</span></span></code></pre></div><p>This command will attempt to pull the latest tag. If this succeeds, then that means the originally specified tag doesn&rsquo;t exist. Go to the Docker registry and check which tags are available for this image.</p><p>If <code>docker pull tutum/curl</code> (without an exact tag) fails, then we have a bigger problem - that image does not exist at all in our image registry.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e7486173c78c00fc1dfc4e45f04c5c7f>7.5 - Container Image Not Updating</h1><div class=lead>Updating images in your cluster during development</div><h2 id=introduction>Introduction</h2><p>A container image should use a fixed tag or the SHA of the image. It should not use the tags <strong>latest</strong>, <strong>head</strong>, <strong>canary</strong>, or other tags that are designed to be <em>floating</em>.</p><h2 id=problem>Problem</h2><p>If you have encountered this issue, you have probably done something along the lines of:</p><ul><li>Deploy anything using an image tag (e.g. <code>cp-enablement/awesomeapp:1.0</code>)</li><li>Fix a bug in awesomeapp</li><li>Build a new image and push it with the <strong>same tag</strong> (<code>cp-enablement/awesomeapp:1.0</code>)</li><li>Update the deployment</li><li>Realize that the bug is still present</li><li>Repeat steps 3-5 without any improvement</li></ul><p>The problem relates to how Kubernetes decides whether to do a <em>docker pull</em> when starting a container.
Since we tagged our image as <em>:1.0</em>, the default pull policy is <strong>IfNotPresent</strong>. The Kubelet already has a local
copy of <code>cp-enablement/awesomeapp:1.0</code>, so it doesn&rsquo;t attempt to do a docker pull. When the new Pods come up,
they&rsquo;re still using the old broken Docker image.</p><p>There are a couple of ways to resolve this, with the recommended one being to <strong>use unique tags</strong>.</p><h2 id=solution>Solution</h2><p>In order to fix the problem, you can use the following bash script that runs anytime the deployment is updated to create a new tag
and push it to the registry.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#00f>#!/usr/bin/env bash
</span></span></span><span style=display:flex><span><span style=color:#00f></span>
</span></span><span style=display:flex><span><span style=color:green># Set the docker image name and the corresponding repository</span>
</span></span><span style=display:flex><span><span style=color:green># Ensure that you change them in the deployment.yml as well.</span>
</span></span><span style=display:flex><span><span style=color:green># You must be logged in with docker login.</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span><span style=color:green># CHANGE THIS TO YOUR Docker.io SETTINGS</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>PROJECT=awesomeapp
</span></span><span style=display:flex><span>REPOSITORY=cp-enablement
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># causes the shell to exit if any subcommand or pipeline returns a non-zero status.</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>set -e
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># set debug mode</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>set -x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># build my nodeJS app</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>npm run build
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># get the latest version ID from the Docker.io registry and increment them</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>VERSION=<span style=color:#00f>$(</span>curl https://registry.hub.docker.com/v1/repositories/$REPOSITORY/$PROJECT/tags  | sed -e <span style=color:#a31515>&#39;s/[][]//g&#39;</span> -e <span style=color:#a31515>&#39;s/&#34;//g&#39;</span> -e <span style=color:#a31515>&#39;s/ //g&#39;</span> | tr <span style=color:#a31515>&#39;}&#39;</span> <span style=color:#a31515>&#39;\n&#39;</span>  | awk -F: <span style=color:#a31515>&#39;{print $3}&#39;</span> | grep v| tail -n 1<span style=color:#00f>)</span>
</span></span><span style=display:flex><span>VERSION=<span style=color:#a31515>${</span>VERSION:1<span style=color:#a31515>}</span>
</span></span><span style=display:flex><span>((VERSION++))
</span></span><span style=display:flex><span>VERSION=<span style=color:#a31515>&#34;v</span>$VERSION<span style=color:#a31515>&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># build the new docker image</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span>echo <span style=color:#a31515>&#39;&gt;&gt;&gt; Building new image&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#a31515>&#39;&gt;&gt;&gt; Push new image&#39;</span>
</span></span><span style=display:flex><span>docker push $REPOSITORY/$PROJECT:$VERSION
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-a7727382924a6be0bac7859384e0cf01>7.6 - Custom Seccomp Profile</h1><h2 id=overview>Overview</h2><p><a href=https://en.wikipedia.org/wiki/Seccomp>Seccomp</a> (secure computing mode) is a security facility in the Linux kernel for restricting the set of system calls applications can make.</p><p>Starting from Kubernetes v1.3.0, the Seccomp feature is in <code>Alpha</code>. To configure it on a <code>Pod</code>, the following annotations can be used:</p><ul><li><code>seccomp.security.alpha.kubernetes.io/pod: &lt;seccomp-profile></code> where <code>&lt;seccomp-profile></code> is the seccomp profile to apply to all containers in a <code>Pod</code>.</li><li><code>container.seccomp.security.alpha.kubernetes.io/&lt;container-name>: &lt;seccomp-profile></code> where <code>&lt;seccomp-profile></code> is the seccomp profile to apply to <code>&lt;container-name></code> in a <code>Pod</code>.</li></ul><p>More details can be found in the <code>PodSecurityPolicy</code> <a href=https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp>documentation</a>.</p><h2 id=installation-of-a-custom-profile>Installation of a Custom Profile</h2><p>By default, kubelet loads custom Seccomp profiles from <code>/var/lib/kubelet/seccomp/</code>. There are two ways in which Seccomp profiles can be added to a <code>Node</code>:</p><ul><li>to be baked in the machine image</li><li>to be added at runtime</li></ul><p>This guide focuses on creating those profiles via a <code>DaemonSet</code>.</p><p>Create a file called <code>seccomp-profile.yaml</code> with the following content:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: seccomp-profile
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  my-profile.json: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    {
</span></span></span><span style=display:flex><span><span style=color:#a31515>      &#34;defaultAction&#34;: &#34;SCMP_ACT_ALLOW&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>      &#34;syscalls&#34;: [
</span></span></span><span style=display:flex><span><span style=color:#a31515>        {
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;name&#34;: &#34;chmod&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>          &#34;action&#34;: &#34;SCMP_ACT_ERRNO&#34;
</span></span></span><span style=display:flex><span><span style=color:#a31515>        }
</span></span></span><span style=display:flex><span><span style=color:#a31515>      ]
</span></span></span><span style=display:flex><span><span style=color:#a31515>    }</span>    
</span></span></code></pre></div><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>The policy above is a very simple one and not suitable for complex applications. The <a href=https://github.com/moby/moby/blob/v17.05.0-ce/profiles/seccomp/default.json>default docker profile</a> can be used a reference. Feel free to modify it to your needs.</div><p>Apply the <code>ConfigMap</code> in your cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> kubectl apply -f seccomp-profile.yaml
</span></span><span style=display:flex><span>configmap/seccomp-profile created
</span></span></code></pre></div><p>The next steps is to create the <code>DaemonSet</code> Seccomp installer. It&rsquo;s going to copy the policy from above in <code>/var/lib/kubelet/seccomp/my-profile.json</code>.</p><p>Create a file called <code>seccomp-installer.yaml</code> with the following content:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: DaemonSet
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: seccomp
</span></span><span style=display:flex><span>  namespace: kube-system
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    security: seccomp
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      security: seccomp
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        security: seccomp
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      initContainers:
</span></span><span style=display:flex><span>      - name: installer
</span></span><span style=display:flex><span>        image: alpine:3.10.0
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>, <span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;cp -r -L /seccomp/*.json /host/seccomp/&#34;</span>]
</span></span><span style=display:flex><span>        volumeMounts:
</span></span><span style=display:flex><span>        - name: profiles
</span></span><span style=display:flex><span>          mountPath: /seccomp
</span></span><span style=display:flex><span>        - name: hostseccomp
</span></span><span style=display:flex><span>          mountPath: /host/seccomp
</span></span><span style=display:flex><span>          readOnly: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: pause
</span></span><span style=display:flex><span>        image: k8s.gcr.io/pause:3.1
</span></span><span style=display:flex><span>      terminationGracePeriodSeconds: 5
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>      - name: hostseccomp
</span></span><span style=display:flex><span>        hostPath:
</span></span><span style=display:flex><span>          path: /var/lib/kubelet/seccomp
</span></span><span style=display:flex><span>      - name: profiles
</span></span><span style=display:flex><span>        configMap:
</span></span><span style=display:flex><span>          name: seccomp-profile
</span></span></code></pre></div><p>Create the installer and wait until it&rsquo;s ready on all <code>Nodes</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> kubectl apply -f seccomp-installer.yaml
</span></span><span style=display:flex><span>daemonset.apps/seccomp-installer created
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>$</span> kubectl -n kube-system get pods -l security=seccomp
</span></span><span style=display:flex><span>NAME                      READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>seccomp-installer-wjbxq   1/1     Running   0          21s
</span></span></code></pre></div><h2 id=create-a-pod-using-a-custom-seccomp-profile>Create a Pod Using a Custom Seccomp Profile</h2><p>Finally, we want to create a profile which uses our new Seccomp profile <code>my-profile.json</code>.</p><p>Create a file called <code>my-seccomp-pod.yaml</code> with the following content:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: seccomp-app
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    seccomp.security.alpha.kubernetes.io/pod: <span style=color:#a31515>&#34;localhost/my-profile.json&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green># you can specify seccomp profile per container. If you add another profile you can configure</span>
</span></span><span style=display:flex><span>    <span style=color:green># it for a specific container - &#39;pause&#39; in this case.</span>
</span></span><span style=display:flex><span>    <span style=color:green># container.seccomp.security.alpha.kubernetes.io/pause: &#34;localhost/some-other-profile.json&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: pause
</span></span><span style=display:flex><span>    image: k8s.gcr.io/pause:3.1
</span></span></code></pre></div><p>Create the <code>Pod</code> and see that it&rsquo;s running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> kubectl apply -f my-seccomp-pod.yaml
</span></span><span style=display:flex><span>pod/seccomp-app created
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>$</span> kubectl get pod seccomp-app
</span></span><span style=display:flex><span>NAME         READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>seccomp-app  1/1     Running   0          42s
</span></span></code></pre></div><h2 id=throubleshooting>Throubleshooting</h2><p>If an invalid or a non-existing profile is used, then the <code>Pod</code> will be stuck in <code>ContainerCreating</code> phase:</p><p><code>broken-seccomp-pod.yaml</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: broken-seccomp
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    seccomp.security.alpha.kubernetes.io/pod: <span style=color:#a31515>&#34;localhost/not-existing-profile.json&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: pause
</span></span><span style=display:flex><span>    image: k8s.gcr.io/pause:3.1
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=font-weight:700>$</span> kubectl apply -f broken-seccomp-pod.yaml
</span></span><span style=display:flex><span>pod/broken-seccomp created
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>$</span> kubectl get pod broken-seccomp
</span></span><span style=display:flex><span>NAME            READY   STATUS              RESTARTS   AGE
</span></span><span style=display:flex><span>broken-seccomp  1/1     ContainerCreating   0          2m
</span></span><span style=display:flex><span><span>
</span></span></span><span style=display:flex><span><span></span><span style=font-weight:700>$</span> kubectl describe pod broken-seccomp
</span></span><span style=display:flex><span>Name:               broken-seccomp
</span></span><span style=display:flex><span>Namespace:          default
</span></span><span style=display:flex><span>....
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  Type     Reason                  Age               From                     Message
</span></span><span style=display:flex><span>  ----     ------                  ----              ----                     -------
</span></span><span style=display:flex><span>  Normal   Scheduled               18s               default-scheduler        Successfully assigned kube-system/broken-seccomp to docker-desktop
</span></span><span style=display:flex><span>  Warning  FailedCreatePodSandBox  4s (x2 over 18s)  kubelet, docker-desktop  Failed create pod sandbox: rpc error: code = Unknown desc = failed to make sandbox docker config for pod &#34;broken-seccomp&#34;: failed to generate sandbox security options
</span></span><span style=display:flex><span>for sandbox &#34;broken-seccomp&#34;: failed to generate seccomp security options for container: cannot load seccomp profile &#34;/var/lib/kubelet/seccomp/not-existing-profile.json&#34;: open /var/lib/kubelet/seccomp/not-existing-profile.json: no such file or directory
</span></span></code></pre></div><h2 id=related-links>Related Links</h2><ul><li><a href=https://en.wikipedia.org/wiki/Seccomp>Seccomp</a></li><li><a href=https://lwn.net/Articles/656307/>A Seccomp Overview</a></li><li><a href=https://docs.docker.com/engine/security/seccomp>Seccomp Security Profiles for Docker</a></li><li><a href=https://man7.org/conf/lpc2015/limiting_kernel_attack_surface_with_seccomp-LPC_2015-Kerrisk.pdf>Using Seccomp to Limit the Kernel Attack Surface</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e924b7dd3bcae7fcc2cc1417b9c0c141>7.7 - Dockerfile Pitfalls</h1><div class=lead>Common Dockerfile pitfalls</div><h2 id=using-the-latest-tag-for-an-image>Using the <code>latest</code> Tag for an Image</h2><p>Many Dockerfiles use the <code>FROM package:latest</code> pattern at the top of their Dockerfiles to pull the latest image from a Docker registry.</p><h3 id=bad-dockerfile>Bad Dockerfile</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=display:flex><span><span style=color:#00f>FROM</span><span style=color:#a31515> alpine</span><span>
</span></span></span></code></pre></div><p>While simple, using the latest tag for an image means that your build can suddenly break if that image gets updated. This can lead to problems where everything builds fine locally (because your local cache thinks it is the latest), while a build server may fail, because some pipelines make a clean pull on every build. Additionally, troubleshooting can prove to be difficult, since the maintainer of the Dockerfile didn&rsquo;t actually make any changes.</p><h3 id=good-dockerfile>Good Dockerfile</h3><p>A digest takes the place of the tag when pulling an image. This will ensure that your Dockerfile remains immutable.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-Dockerfile data-lang=Dockerfile><span style=display:flex><span><span style=color:#00f>FROM</span><span style=color:#a31515> alpine@sha256:7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430</span><span>
</span></span></span></code></pre></div><h2 id=running-aptapkyum-update>Running apt/apk/yum update</h2><p>Running <code>apt-get install</code> is one of those things virtually every Debian-based Dockerfile will have to do in order to satiate some external package requirements your code needs to run. However, using <code>apt-get</code> as an example, this comes with its own problems.</p><p><strong>apt-get upgrade</strong></p><p>This will update all your packages to their latests versions, which can be bad because it prevents your Dockerfile from creating consistent, immutable builds.</p><p><strong>apt-get update (in a different line than the one running your apt-get install command)</strong></p><p>Running <code>apt-get update</code> as a single line entry will get cached by the build and won&rsquo;t actually run every time you need to run <code>apt-get install</code>. Instead, make sure you run <code>apt-get update</code> in the same line with all the packages to ensure that all are updated correctly.</p><h2 id=avoid-big-container-images>Avoid Big Container Images</h2><p>Building a small container image will reduce the time needed to start or restart pods. An image based on the popular <a href=http://alpinelinux.org/>Alpine Linux project</a> is much smaller than most distribution based images (~5MB). For most popular languages and products, there is usually an official Alpine Linux image, e.g. <a href=https://hub.docker.com/_/golang/>golang</a>, <a href=https://hub.docker.com/_/node/>nodejs</a>, and <a href=https://hub.docker.com/_/postgres/>postgres</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$  docker images
</span></span><span style=display:flex><span>REPOSITORY                                                      TAG                     IMAGE ID            CREATED             SIZE
</span></span><span style=display:flex><span>postgres                                                        9.6.9-alpine            6583932564f8        13 days ago         39.26 MB
</span></span><span style=display:flex><span>postgres                                                        9.6                     d92dad241eff        13 days ago         235.4 MB
</span></span><span style=display:flex><span>postgres                                                        10.4-alpine             93797b0f31f4        13 days ago         39.56 MB
</span></span></code></pre></div><p>In addition, for compiled languages such as Go or C++ that do not require build time tooling during runtime, it is recommended to avoid build time tooling in the final images. With Docker&rsquo;s support for <a href=https://docs.docker.com/engine/userguide/eng-image/multistage-build/>multi-stages builds</a>, this can be easily achieved with minimal effort. Such an example can be found at <a href=https://docs.docker.com/develop/develop-images/multistage-build/#name-your-build-stages>Multi-stage builds</a>.</p><p>Google&rsquo;s <a href=https://github.com/GoogleContainerTools/distroless>distroless</a> image is also a good base image.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-470a15019f300767d4cb47508b3acf52>7.8 - Dynamic Volume Provisioning</h1><div class=lead>Running a Postgres database on Kubernetes</div><h2 id=overview>Overview</h2><p>The example shows how to run a Postgres database on Kubernetes and how to dynamically provision and mount the storage
volumes needed by the database</p><h2 id=run-postgres-database>Run Postgres Database</h2><p>Define the following Kubernetes resources in a yaml file:</p><ul><li>PersistentVolumeClaim (PVC)</li><li>Deployment</li></ul><h3 id=persistentvolumeclaim>PersistentVolumeClaim</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: PersistentVolumeClaim
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: postgresdb-pvc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  accessModes:
</span></span><span style=display:flex><span>    - ReadWriteOnce
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>    requests:
</span></span><span style=display:flex><span>      storage: 9Gi
</span></span><span style=display:flex><span>  storageClassName: <span style=color:#a31515>&#39;default&#39;</span>
</span></span></code></pre></div><p>This defines a PVC using the storage class <code>default</code>. Storage classes abstract from the underlying storage provider as well
as other parameters, like disk-type (e.g.; solid-state vs standard disks).</p><p>The default storage class has the annotation <strong>{&ldquo;storageclass.kubernetes.io/is-default-class&rdquo;:&ldquo;true&rdquo;}</strong>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl describe sc default
</span></span><span style=display:flex><span>Name:            default
</span></span><span style=display:flex><span>IsDefaultClass:  Yes
</span></span><span style=display:flex><span>Annotations:     kubectl.kubernetes.io/last-applied-configuration={<span style=color:#a31515>&#34;apiVersion&#34;</span>:<span style=color:#a31515>&#34;storage.k8s.io/v1beta1&#34;</span>,<span style=color:#a31515>&#34;kind&#34;</span>:<span style=color:#a31515>&#34;StorageClass&#34;</span>,<span style=color:#a31515>&#34;metadata&#34;</span>:{<span style=color:#a31515>&#34;annotations&#34;</span>:{<span style=color:#a31515>&#34;storageclass.kubernetes.io/is-default-class&#34;</span>:<span style=color:#a31515>&#34;true&#34;</span>},<span style=color:#a31515>&#34;labels&#34;</span>:{<span style=color:#a31515>&#34;addonmanager.kubernetes.io/mode&#34;</span>:<span style=color:#a31515>&#34;Exists&#34;</span>},<span style=color:#a31515>&#34;name&#34;</span>:<span style=color:#a31515>&#34;default&#34;</span>,<span style=color:#a31515>&#34;namespace&#34;</span>:<span style=color:#a31515>&#34;&#34;</span>},<span style=color:#a31515>&#34;parameters&#34;</span>:{<span style=color:#a31515>&#34;type&#34;</span>:<span style=color:#a31515>&#34;gp2&#34;</span>},<span style=color:#a31515>&#34;provisioner&#34;</span>:<span style=color:#a31515>&#34;kubernetes.io/aws-ebs&#34;</span>}
</span></span><span style=display:flex><span>,storageclass.kubernetes.io/is-default-class=true
</span></span><span style=display:flex><span>Provisioner:           kubernetes.io/aws-ebs
</span></span><span style=display:flex><span>Parameters:            type=gp2
</span></span><span style=display:flex><span>AllowVolumeExpansion:  &lt;unset&gt;
</span></span><span style=display:flex><span>MountOptions:          &lt;none&gt;
</span></span><span style=display:flex><span>ReclaimPolicy:         Delete
</span></span><span style=display:flex><span>VolumeBindingMode:     Immediate
</span></span><span style=display:flex><span>Events:                &lt;none&gt;
</span></span></code></pre></div><p>A Persistent Volume is automatically created when it is dynamically provisioned. In the following example, the PVC is defined
as &ldquo;postgresdb-pvc&rdquo;, and a corresponding PV &ldquo;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb&rdquo; is created and associated with the PVC automatically.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl create -f .<span style=color:#a31515>\p</span>ostgres_deployment.yaml
</span></span><span style=display:flex><span>persistentvolumeclaim <span style=color:#a31515>&#34;postgresdb-pvc&#34;</span> created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Delete           Bound     default/postgresdb-pvc   default                  3s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl get pvc
</span></span><span style=display:flex><span>NAME             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span style=display:flex><span>postgresdb-pvc   Bound     pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            default        8s
</span></span></code></pre></div><p>Notice that the <strong>RECLAIM POLICY</strong> is <strong>Delete</strong> (default value), which is one of the two reclaim policies, the other
one is <strong>Retain</strong>. (A third policy <strong>Recycle</strong> has been deprecated). In the case of <strong>Delete</strong>, the PV is deleted automatically
when the PVC is removed, and the data on the PVC will also be lost.</p><p>On the other hand, a PV with <strong>Retain</strong> policy will not be deleted when the PVC is removed, and moved to <strong>Release</strong> status, so
that data can be recovered by Administrators later.</p><p>You can use the <code>kubectl patch</code> command to change the reclaim policy as described in <a href=https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/>Change the Reclaim Policy of a PersistentVolume</a>
or use <code>kubectl edit pv &lt;pv-name></code> to edit it online as shown below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Delete           Bound     default/postgresdb-pvc   default                  44m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># change the reclaim policy from &#34;Delete&#34; to &#34;Retain&#34;</span>
</span></span><span style=display:flex><span>$ kubectl edit pv pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb
</span></span><span style=display:flex><span>persistentvolume <span style=color:#a31515>&#34;pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb&#34;</span> edited
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># check the reclaim policy afterwards</span>
</span></span><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Retain           Bound     default/postgresdb-pvc   default                  45m
</span></span></code></pre></div><h3 id=deployment>Deployment</h3><p>Once a PVC is created, you can use it in your container via <code>volumes.persistentVolumeClaim.claimName</code>. In the below
example, the PVC <strong>postgresdb-pvc</strong> is mounted as readable and writable, and in <code>volumeMounts</code> two paths in the container are mounted to subfolders in the volume.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: postgres
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: postgres
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    deployment.kubernetes.io/revision: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  strategy:
</span></span><span style=display:flex><span>    type: RollingUpdate
</span></span><span style=display:flex><span>    rollingUpdate:
</span></span><span style=display:flex><span>      maxUnavailable: 1
</span></span><span style=display:flex><span>      maxSurge: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: postgres
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      name: postgres
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: postgres
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>        - name: postgres
</span></span><span style=display:flex><span>          image: <span style=color:#a31515>&#34;cpettech.docker.repositories.sap.ondemand.com/jtrack_postgres:howto&#34;</span>
</span></span><span style=display:flex><span>          env:
</span></span><span style=display:flex><span>            - name: POSTGRES_USER
</span></span><span style=display:flex><span>              value: postgres
</span></span><span style=display:flex><span>            - name: POSTGRES_PASSWORD
</span></span><span style=display:flex><span>              value: p5FVqfuJFrM42cVX9muQXxrC3r8S9yn0zqWnFR6xCoPqxqVQ
</span></span><span style=display:flex><span>            - name: POSTGRES_INITDB_XLOGDIR
</span></span><span style=display:flex><span>              value: <span style=color:#a31515>&#34;/var/log/postgresql/logs&#34;</span>
</span></span><span style=display:flex><span>          ports:
</span></span><span style=display:flex><span>            - containerPort: 5432
</span></span><span style=display:flex><span>          volumeMounts:
</span></span><span style=display:flex><span>            - mountPath: /var/lib/postgresql/data
</span></span><span style=display:flex><span>              name: postgre-db
</span></span><span style=display:flex><span>              subPath: data     <span style=color:green># https://github.com/kubernetes/website/pull/2292.  Solve the issue of crashing initdb due to non-empty directory (i.e. lost+found)</span>
</span></span><span style=display:flex><span>            - mountPath: /var/log/postgresql/logs
</span></span><span style=display:flex><span>              name: postgre-db
</span></span><span style=display:flex><span>              subPath: logs
</span></span><span style=display:flex><span>      volumes:
</span></span><span style=display:flex><span>        - name: postgre-db
</span></span><span style=display:flex><span>          persistentVolumeClaim:
</span></span><span style=display:flex><span>            claimName: postgresdb-pvc
</span></span><span style=display:flex><span>            readOnly: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      imagePullSecrets:
</span></span><span style=display:flex><span>      - name: cpettechregistry
</span></span></code></pre></div><p>To check the mount points in the container:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get po
</span></span><span style=display:flex><span>NAME                        READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>postgres-7f485fd768-c5jf9   1/1       Running   0          32m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl exec -it postgres-7f485fd768-c5jf9 bash
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>root@postgres-7f485fd768-c5jf9:/# ls /var/lib/postgresql/data/
</span></span><span style=display:flex><span>base    pg_clog       pg_dynshmem  pg_ident.conf  pg_multixact  pg_replslot  pg_snapshots  pg_stat_tmp  pg_tblspc    PG_VERSION  postgresql.auto.conf  postmaster.opts
</span></span><span style=display:flex><span>global  pg_commit_ts  pg_hba.conf  pg_logical     pg_notify     pg_serial    pg_stat       pg_subtrans  pg_twophase  pg_xlog     postgresql.conf       postmaster.pid
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>root@postgres-7f485fd768-c5jf9:/# ls /var/log/postgresql/logs/
</span></span><span style=display:flex><span>000000010000000000000001  archive_status
</span></span></code></pre></div><h2 id=deleting-a-persistentvolumeclaim>Deleting a PersistentVolumeClaim</h2><p>In case of a <strong>Delete</strong> policy, deleting a PVC will also delete its associated PV. If <strong>Retain</strong> is the reclaim policy, the
PV will change status from <strong>Bound</strong> to <strong>Released</strong> when the PVC is deleted.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Check pvc and pv before deletion</span>
</span></span><span style=display:flex><span>$ kubectl get pvc
</span></span><span style=display:flex><span>NAME             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span style=display:flex><span>postgresdb-pvc   Bound     pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            default        50m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Retain           Bound     default/postgresdb-pvc   default                  50m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># delete pvc</span>
</span></span><span style=display:flex><span>$ kubectl delete pvc postgresdb-pvc
</span></span><span style=display:flex><span>persistentvolumeclaim <span style=color:#a31515>&#34;postgresdb-pvc&#34;</span> deleted
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># pv changed to status &#34;Released&#34;</span>
</span></span><span style=display:flex><span>$ kubectl get pv
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                    STORAGECLASS   REASON    AGE
</span></span><span style=display:flex><span>pvc-06c81c30-72ea-11e8-ada2-aa3b2329c8bb   9Gi        RWO            Retain           Released   default/postgresdb-pvc   default                  51m
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-334d7849804a4e61fb62520a7206b022>7.9 - Install Knative in Gardener Clusters</h1><div class=lead>A walkthrough the steps for installing Knative in Gardener shoot clusters.</div><h2 id=overview>Overview</h2><p>This guide walks you through the installation of the latest version of Knative
using pre-built images on a <a href=https://gardener.cloud>Gardener</a> created cluster
environment. To set up your own Gardener, see the
<a href=/docs/gardener/>documentation</a>
or have a look at the
<a href=https://github.com/gardener/landscape-setup-template>landscape-setup-template</a>
project. To learn more about this open source project, read the
<a href=https://kubernetes.io/blog/2018/05/17/gardener/>blog on kubernetes.io</a>.</p><h2 id=prerequsites>Prerequsites</h2><p>Knative requires a Kubernetes cluster v1.15 or newer.</p><h2 id=steps>Steps</h2><h3 id=install-and-configure-kubectl>Install and Configure kubectl</h3><ol><li><p>If you already have <code>kubectl</code> CLI, run <code>kubectl version --short</code> to check
the version. You need v1.10 or newer. If your <code>kubectl</code> is older, follow the
next step to install a newer version.</p></li><li><p><a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl>Install the kubectl CLI</a>.</p></li></ol><h3 id=access-gardener>Access Gardener</h3><ol><li><p>Create a project in the Gardener dashboard. This will essentially create a
Kubernetes namespace with the name <code>garden-&lt;my-project></code>.</p></li><li><p><a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/#configure-kubectl>Configure access to your Gardener project</a>
using a kubeconfig.</p><p>If you are not the Gardener Administrator already, you
can create a technical user in the Gardener dashboard.
Go to the &ldquo;Members&rdquo; section and add a service account.
You can then download the kubeconfig for your project.
You can skip this step if you create your cluster using the
user interface; it is only needed for programmatic access, make sure you set
<code>export KUBECONFIG=garden-my-project.yaml</code> in your shell.
<img src=/__resources/gardener_service_account_d1ec7d.png alt="Download kubeconfig for Gardener"></p></li></ol><h3 id=creating-a-kubernetes-cluster>Creating a Kubernetes Cluster</h3><p>You can create your cluster using <code>kubectl</code> CLI by providing a cluster
specification yaml file. You can find an example for GCP in the
<a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>gardener/gardener repository</a>.
Make sure the namespace matches that of your project. Then just apply the
prepared so-called &ldquo;shoot&rdquo; cluster CRD with kubectl:</p><pre tabindex=0><code>kubectl apply --filename my-cluster.yaml
</code></pre><p>The easier alternative is to create the cluster following the cluster creation
wizard in the Gardener dashboard:
<img src=/__resources/gardener_shoot_creation_79ef94.png alt="shoot creation" title="shoot creation via the dashboard"></p><h3 id=configure-kubectl-for-your-cluster>Configure kubectl for Your Cluster</h3><p>You can now download the kubeconfig for your freshly created cluster in the
Gardener dashboard or via the CLI as follows:</p><pre tabindex=0><code>kubectl --namespace shoot--my-project--my-cluster get secret kubecfg --output jsonpath={.data.kubeconfig} | base64 --decode &gt; my-cluster.yaml
</code></pre><p>This kubeconfig file has full administrators access to you cluster. For the rest
of this guide, be sure you have <code>export KUBECONFIG=my-cluster.yaml</code> set.</p><h2 id=installing-istio>Installing Istio</h2><p>Knative depends on Istio. If your cloud platform offers a managed Istio
installation, we recommend installing Istio that way, unless you need the
ability to customize your installation.</p><p>Otherwise, see the <a href=https://knative.dev/docs/install/installing-istio/>Installing Istio for Knative guide</a>
to install Istio.</p><p>You must install Istio on your Kubernetes cluster before continuing with these
instructions to install Knative.</p><h2 id=installing-cluster-local-gateway-for-serving-cluster-internal-traffic>Installing <code>cluster-local-gateway</code> for Serving Cluster-Internal Traffic</h2><p>If you installed Istio, you can install a <code>cluster-local-gateway</code> within your Knative cluster so that you can serve cluster-internal traffic. If you want to configure your revisions to use routes that are visible only within your cluster, <a href=https://knative.dev/docs/admin/install/knative-offerings/>install and use the <code>cluster-local-gateway</code></a>.</p><h2 id=installing-knative>Installing Knative</h2><p>The following commands install all available Knative components as well as the
standard set of observability plugins. Knative&rsquo;s installation guide - <a href=https://knative.dev/docs/admin/install/>Installing Knative</a>.</p><ol><li><p>If you are upgrading from Knative 0.3.x: Update your domain and static IP
address to be associated with the LoadBalancer <code>istio-ingressgateway</code> instead
of <code>knative-ingressgateway</code>. Then run the following to clean up leftover
resources:</p><pre tabindex=0><code>kubectl delete svc knative-ingressgateway -n istio-system
kubectl delete deploy knative-ingressgateway -n istio-system
</code></pre><p>If you have the Knative Eventing Sources component installed, you will also
need to delete the following resource before upgrading:</p><pre tabindex=0><code>kubectl delete statefulset/controller-manager -n knative-sources
</code></pre><p>While the deletion of this resource during the upgrade process will not
prevent modifications to Eventing Source resources, those changes will not be
completed until the upgrade process finishes.</p></li><li><p>To install Knative, first install the CRDs by running the <code>kubectl apply</code>
command once with the <code>-l knative.dev/crd-install=true</code> flag. This prevents
race conditions during the install, which cause intermittent errors:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply --selector knative.dev/crd-install=true <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml
</span></span></code></pre></div></li><li><p>To complete the installation of Knative and its dependencies, run the
<code>kubectl apply</code> command again, this time without the <code>--selector</code> flag:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply --filename https://github.com/knative/serving/releases/download/v0.12.1/serving.yaml <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/eventing/releases/download/v0.12.1/eventing.yaml <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--filename https://github.com/knative/serving/releases/download/v0.12.1/monitoring.yaml
</span></span></code></pre></div></li><li><p>Monitor the Knative components until all of the components show a <code>STATUS</code> of
<code>Running</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods --namespace knative-serving
</span></span><span style=display:flex><span>kubectl get pods --namespace knative-eventing
</span></span><span style=display:flex><span>kubectl get pods --namespace knative-monitoring
</span></span></code></pre></div></li></ol><h2 id=set-your-custom-domain>Set Your Custom Domain</h2><ol><li>Fetch the external IP or CNAME of the knative-ingressgateway:</li></ol><pre tabindex=0><code>kubectl --namespace istio-system get service knative-ingressgateway
NAME                     TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                                      AGE
knative-ingressgateway   LoadBalancer   100.70.219.81   35.233.41.212   80:32380/TCP,443:32390/TCP,32400:32400/TCP   4d
</code></pre><ol start=2><li>Create a wildcard DNS entry in your custom domain to point to the above IP or
CNAME:</li></ol><pre tabindex=0><code>*.knative.&lt;my domain&gt; == A 35.233.41.212
# or CNAME if you are on AWS
*.knative.&lt;my domain&gt; == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com
</code></pre><ol start=3><li>Adapt your Knative config-domain (set your domain in the data field):</li></ol><pre tabindex=0><code>kubectl --namespace knative-serving get configmaps config-domain --output yaml
apiVersion: v1
data:
  knative.&lt;my domain&gt;: &#34;&#34;
kind: ConfigMap
  name: config-domain
  namespace: knative-serving
</code></pre><h2 id=whats-next>What&rsquo;s Next</h2><p>Now that your cluster has Knative installed, you can see what Knative has to
offer.</p><p>Deploy your first app with the
<a href=https://knative.dev/docs/serving/getting-started-knative-app/>Getting Started with Knative App Deployment</a>
guide.</p><p>Get started with Knative Eventing by walking through one of the
<a href=https://knative.dev/docs/eventing/samples/>Eventing Samples</a>.</p><p><a href=https://knative.dev/docs/serving/installing-cert-manager/>Install Cert-Manager</a> if you want to use the
<a href=https://knative.dev/docs/serving/encryption/enabling-automatic-tls-certificate-provisioning/>automatic TLS cert provisioning feature</a>.</p><h2 id=cleaning-up>Cleaning Up</h2><p>Use the Gardener dashboard to delete your cluster, or execute the following with
kubectl pointing to your <code>garden-my-project.yaml</code> kubeconfig:</p><pre tabindex=0><code>kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project annotate shoot my-cluster confirmation.gardener.cloud/deletion=true

kubectl --kubeconfig garden-my-project.yaml --namespace garden--my-project delete shoot my-cluster
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-f45e1e3bb19a23ceeebe6d769078b409>7.10 - Integrity and Immutability</h1><div class=lead>Ensure that you always get the right image</div><h2 id=introduction>Introduction</h2><p>When transferring data among networked systems, <strong>trust is a central concern</strong>. In particular, when communicating over an untrusted medium such as the internet, it is critical to ensure the <strong>integrity and immutability</strong> of all the data a system operates on. Especially if you use Docker Engine to push and pull images (data) to a <strong>public registry</strong>.</p><p>This immutability offers you a guarantee that any and all containers that you instantiate will be absolutely identical at inception. Surprise surprise, deterministic operations.</p><h2 id=a-lesson-in-deterministic-ops>A Lesson in Deterministic Ops</h2><p>Docker Tags are about as reliable and disposable as this guy down here.</p><p><img src=/__resources/howto-content-trust_8fd6f7.svg alt=docker-labels></p><p>Seems simple enough. You have probably already deployed hundreds of YAML&rsquo;s or started endless counts of Docker containers.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run --name mynginx1 -P -d nginx:1.13.9
</span></span></code></pre></div><p>or</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: rss-site
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: web
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: web
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>        - name: front-end
</span></span><span style=display:flex><span>          image: nginx:1.13.9
</span></span><span style=display:flex><span>          ports:
</span></span><span style=display:flex><span>            - containerPort: 80
</span></span></code></pre></div><p><strong>But Tags are mutable and humans are prone to error. Not a good combination.</strong> Here, we’ll dig into why the use of tags can
be dangerous and how to deploy your containers across a pipeline and across environments with <strong>determinism in mind</strong>.</p><p>Let&rsquo;s say that you want to ensure that whether it’s today or 5 years from now, that specific deployment uses the very same image that
you have defined. Any updates or newer versions of an image should be executed as a new deployment. <strong>The solution: digest</strong></p><p>A digest takes the place of the tag when pulling an image. For example, to pull the above image by digest, run the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>docker run --name mynginx1 -P -d nginx@sha256:4771d09578c7c6a65299e110b3ee1c0a2592f5ea2618d23e4ffe7a4cab1ce5de
</span></span></code></pre></div><p>You can now make sure that the same image is always loaded at every deployment. It doesn&rsquo;t matter if the TAG of the image has been changed or not. <strong>This solves the problem of repeatability.</strong></p><h2 id=content-trust>Content Trust</h2><p>However, there’s an additionally hidden danger. It is possible for an attacker to replace a server image with another
one infected with malware.</p><p><img src=/__resources/howto-content-trust-replace_7b55d1.svg alt=docker-content-trust></p><p><a href=https://docs.docker.com/engine/security/trust/content_trust/>Docker Content trust</a> gives you the ability to verify both the integrity and the publisher of all the data received from a registry over any channel.</p><p>Prior to version 1.8, Docker didn’t have a way to verify the authenticity of a server image. But in v1.8, a new feature
called <strong>Docker Content Trust</strong> was introduced to automatically sign and verify the signature of a publisher.</p><p>So, as soon as a server image is downloaded, it is cross-checked with the signature of the publisher to see if someone tampered with it in any way. <strong>This solves the problem of trust.</strong></p><p>In addition, you should scan all images for known vulnerabilities.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7d122ee5ae2c428bfe3d1293dd385721>7.11 - Kubernetes Antipatterns</h1><div class=lead>Common antipatterns for Kubernetes and Docker</div><p><img src=/__resources/howto-antipattern_027298.png alt=antipattern></p><p>This HowTo covers common Kubernetes antipatterns that we have seen over the past months.</p><h2 id=running-as-root-user>Running as Root User</h2><p>Whenever possible, do not run containers as root user. One could be tempted to say that Kubernetes pods and nodes are well separated. Host and containers running on it share the same kernel. If a container is compromised, the root user in the container has full control over the
underlying node.</p><p>Watch the very good presentation by Liz Rice at the KubeCon 2018</p><iframe width=560 height=315 src=https://www.youtube.com/embed/ltrV-Qmh3oY frameborder=0 allow="autoplay; encrypted-media" allowfullscreen></iframe><p>Use <code>RUN groupadd -r anygroup && useradd -r -g anygroup myuser</code> to create a group and add a user to it. Use the <code>USER</code> command to switch to this user. Note that you may also consider to provide <a href=https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user>an explicit UID/GID</a> if required.</p><p>For example:</p><pre tabindex=0><code>ARG GF_UID=&#34;500&#34;
ARG GF_GID=&#34;500&#34;

# add group &amp; user
RUN groupadd -r -g $GF_GID appgroup &amp;&amp; \
   useradd appuser -r -u $GF_UID -g appgroup

USER appuser
</code></pre><h2 id=store-data-or-logs-in-containers>Store Data or Logs in Containers</h2><p>Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the
container, as they are lost when the container is closed. Use persistence volumes instead to persist data outside
of containers. Using an <a href=https://www.elastic.co/de/what-is/elk-stack>ELK stack</a> is another good option for storing and processing logs.</p><h2 id=using-pod-ip-addresses>Using Pod IP Addresses</h2><p>Each pod is assigned an IP address. It is necessary for pods to communicate with each other to build an application, e.g. an application
must communicate with a database. Existing pods are terminated and new pods are constantly started. If you would rely on the IP address of a pod or container, you would need to update the application configuration constantly. This makes the application fragile.</p><p>Create services instead. They provide a logical name that can be assigned independently of the varying number and IP addresses of containers. Services are the basic concept for load balancing within Kubernetes.</p><h2 id=more-than-one-process-in-a-container>More Than One Process in a Container</h2><p>A docker file provides a <code>CMD</code> and <code>ENTRYPOINT</code> to start the image. <code>CMD</code> is often used around a script that makes a configuration and then
starts the container. Do not try to start multiple processes with this script. It is important to consider the separation of concerns when creating docker images. Running multiple processes in a single pod makes managing your containers, collecting logs and updating each process more difficult.</p><p>You can split the image into multiple containers and manage them independently - even in one pod. Bear in mind that Kubernetes only monitors the process with <code>PID=1</code>. If more than one process is started within a container, then these no longer fall under the control of Kubernetes.</p><h2 id=creating-images-in-a-running-container>Creating Images in a Running Container</h2><p>A new image can be created with the <code>docker commit</code> command. This is useful if changes have been made to the container and you want to persist them for later error analysis. However, images created like this are not reproducible and completely worthless for a CI/CD environment. Furthermore, another developer cannot recognize which components the image contains. Instead, always make changes to the docker file, close existing containers and start a new container with the updated image.</p><h2 id=saving-passwords-in-a-docker-image-->Saving Passwords in a docker Image 💀</h2><p><strong>Do not save passwords in a Docker file!</strong> They are in plain text and are checked into a repository. That makes them completely vulnerable even if you are using a private repository like the Artifactory.</p><p>Always use <a href=https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure>Secrets or ConfigMaps</a> to provision passwords or inject them by mounting a persistent volume.</p><h2 id=using-the-latest-tag>Using the &rsquo;latest&rsquo; Tag</h2><p>Starting an image with <em>tomcat</em> is tempting. If no tags are specified, a container is started with the <code>tomcat:latest</code> image. This image may no longer be up to date and refer to an older version instead. Running a production application requires complete control of the environment with exact versions of the image.</p><p>Make sure you always use a tag or even better the <strong>sha256 hash</strong> of the image e.g. <code>tomcat@sha256:c34ce3c1fcc0c7431e1392cc3abd0dfe2192ffea1898d5250f199d3ac8d8720f</code>.</p><h3 id=why-use-the-sha256-hash>Why Use the sha256 Hash?</h3><p>Tags are not immutable and can be overwritten by a developer at any time. In this case you don&rsquo;t have complete control over your image - which is bad.</p><h2 id=different-images-per-environment>Different Images per Environment</h2><p>Don&rsquo;t create different images for development, testing, staging and production environments. The image should be the <strong>source of truth</strong> and should only be created once and pushed to the repository. This <code>image:tag</code> should be used for different environments in the future.</p><h2 id=depend-on-start-order-of-pods>Depend on Start Order of Pods</h2><p>Applications often depend on containers being started in a certain order. For example, a database container must be up and running before an application can connect to it. The application should be resilient to such changes, as the db pod can be unreachable or restarted at any time. The application container should be able to handle such situations without terminating or crashing.</p><h2 id=additional-anti-patterns-and-patterns>Additional Anti-Patterns and Patterns</h2><p>In the community, vast experience has been collected to improve the stability and usability of Docker and Kubernetes.</p><p>Refer to <a href=https://github.com/gravitational/workshop/blob/master/k8sprod.md>Kubernetes Production Patterns</a> for more information.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ffdcb3c3c73c058191fdf52cbca5e94c>7.12 - Namespace Isolation</h1><div class=lead>Deny all traffic from other namespaces</div><h2 id=overview>Overview</h2><p>You can configure a <strong>NetworkPolicy</strong> to deny all the traffic from other namespaces while allowing all the traffic
coming from the same namespace the pod was deployed into.</p><img src=/__resources/howto-namespaceisolation_81566d.png width=100%><p><strong>There are many reasons why you may chose to employ Kubernetes network policies:</strong></p><ul><li>Isolate multi-tenant deployments</li><li>Regulatory compliance</li><li>Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each other</li></ul><p>Kubernetes <strong>network policies</strong> are application centric compared to infrastructure/network centric standard firewalls.
<strong>There are no explicit CIDRs or IP addresses used</strong> for matching source or destination IP’s.
<strong>Network policies build up on labels and selectors</strong> which are key concepts of Kubernetes that are used to organize
(for e.g all DB tier pods of an app) and select subsets of objects.</p><h2 id=example>Example</h2><p>We create two nginx HTTP-Servers in two namespaces and block all traffic between the two namespaces. E.g. you are
unable to get content from <em>namespace1</em> if you are sitting in <em>namespace2</em>.</p><h2 id=setup-the-namespaces>Setup the Namespaces</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># create two namespaces for test purpose</span>
</span></span><span style=display:flex><span>kubectl create ns customer1
</span></span><span style=display:flex><span>kubectl create ns customer2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># create a standard HTTP web server</span>
</span></span><span style=display:flex><span>kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer1
</span></span><span style=display:flex><span>kubectl run nginx --image=nginx --replicas=1 --port=80 -n=customer2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># expose the port 80 for external access</span>
</span></span><span style=display:flex><span>kubectl expose deployment nginx --port=80 --type=NodePort -n=customer1
</span></span><span style=display:flex><span>kubectl expose deployment nginx --port=80 --type=NodePort -n=customer2
</span></span></code></pre></div><hr><h2 id=test-without-np>Test Without NP</h2><img src=/__resources/howto-namespaceisolation-without_50687c.png width=80%><p>Create a pod with <em>curl</em> preinstalled inside the namespace <em>customer1</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># create a &#34;bash&#34; pod in one namespace</span>
</span></span><span style=display:flex><span>kubectl run -i --tty client --image=tutum/curl -n=customer1
</span></span></code></pre></div><p>Try to <em>curl</em> the exposed nginx server to get the default index.html page. <strong>Execute this in the bash prompt of the pod created above.</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># get the index.html from the nginx of the namespace &#34;customer1&#34; =&gt; success</span>
</span></span><span style=display:flex><span>curl http://nginx.customer1
</span></span><span style=display:flex><span><span style=color:green># get the index.html from the nginx of the namespace &#34;customer2&#34; =&gt; success</span>
</span></span><span style=display:flex><span>curl http://nginx.customer2
</span></span></code></pre></div><p>Both calls are done in a pod within the namespace <em>customer1</em> and both nginx servers are always reachable, no matter in what namespace.</p><hr><h2 id=test-with-np>Test with NP</h2><img src=/__resources/howto-namespaceisolation-with_df46d2.png width=80%><p>Install the <strong>NetworkPolicy</strong> from your shell:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: deny-from-other-namespaces
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>  - from:
</span></span><span style=display:flex><span>    - podSelector: {}
</span></span></code></pre></div><ul><li>it applies the policy to ALL pods in the named namespace as the <code>spec.podSelector.matchLabels</code> is empty and therefore selects all pods.</li><li>it allows traffic from ALL pods in the named namespace, as <code>spec.ingress.from.podSelector</code> is empty and therefore selects all pods.</li></ul><pre tabindex=0><code>kubectl apply -f ./network-policy.yaml -n=customer1
kubectl apply -f ./network-policy.yaml -n=customer2
</code></pre><p>After this, <code>curl http://nginx.customer2</code> shouldn&rsquo;t work anymore if you are a service inside the namespace <em>customer1</em> and
vice versa<div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>This policy, once applied, will also disable all external traffic to these pods. For example, you can create a service of type <code>LoadBalancer</code> in namespace <code>customer1</code> that match the nginx pod. When you request the service by its <code>&lt;EXTERNAL_IP>:&lt;PORT></code>, then the network policy that will deny the ingress traffic from the service and the request will time out.</div></p><h2 id=related-links>Related Links</h2><p>You can get more information on how to configure the <strong>NetworkPolicies</strong> at:</p><ul><li><a href=https://docs.projectcalico.org/v3.0/getting-started/kubernetes/tutorials/advanced-policy>Calico WebSite</a></li><li><a href=https://github.com/ahmetb/kubernetes-network-policy-recipes>Kubernetes NP Recipes</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-90f02665c7456e1a24afc104f91bea70>7.13 - Orchestration of Container Startup</h1><div class=lead>How to orchestrate a startup sequence of multiple containers</div><h2 id=disclaimer>Disclaimer</h2><p>If an application depends on other services deployed separately, do not rely on a certain start sequence of containers. Instead,
ensure that the application can cope with unavailability of the services it depends on.</p><h2 id=introduction>Introduction</h2><p>Kubernetes offers a feature called <a href=https://kubernetes.io/docs/concepts/workloads/pods/init-containers/>InitContainers</a>
to perform some tasks during a pod&rsquo;s initialization.
In this tutorial, we demonstrate how to use <code>InitContainers</code> in order to orchestrate a starting sequence of multiple containers.
The tutorial uses the example app <a href=https://medium.com/@xcoulon/deploying-your-first-web-app-on-minikube-6e98d2884b3a>url-shortener</a>,
which consists of two components:</p><ul><li>postgresql database</li><li>webapp which depends on the postgresql database and provides two endpoints: <em>create a short url from a given location</em> and <em>redirect from a given short URL to the corresponding target location</em></li></ul><p>This app represents the minimal example where an application relies on another service or database. In this example,
if the application starts before the database is ready, the application will fail as shown below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl logs webapp-958cf5567-h247n
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2018-06-12T11:02:42Z&#34;</span> level=info msg=<span style=color:#a31515>&#34;Connecting to Postgres database using: host=`postgres:5432` dbname=`url_shortener_db` username=`user`\n&#34;</span>
</span></span><span style=display:flex><span>time=<span style=color:#a31515>&#34;2018-06-12T11:02:42Z&#34;</span> level=fatal msg=<span style=color:#a31515>&#34;failed to start: failed to open connection to database: dial tcp: lookup postgres on 100.64.0.10:53: no such host\n&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl get po -w
</span></span><span style=display:flex><span>NAME                                READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Pending   0         0s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Pending   0         0s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       ContainerCreating   0         0s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       ContainerCreating   0         1s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Error     0         2s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Error     1         3s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       CrashLoopBackOff   1         4s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Error     2         18s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       CrashLoopBackOff   2         29s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       Error     3         43s
</span></span><span style=display:flex><span>webapp-958cf5567-h247n   0/1       CrashLoopBackOff   3         56s
</span></span></code></pre></div><p>If the <code>restartPolicy</code> is set to <code>Always</code> (default) in the yaml file, the application will continue to restart the pod with an exponential back-off delay in case of failure.</p><h2 id=using-initcontaniner>Using InitContaniner</h2><p>To avoid such a situation, <code>InitContainers</code> can be defined, which are executed prior to the application container. If one
of the <code>InitContainers</code> fails, the application container won&rsquo;t be triggered.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: webapp
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: webapp
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: webapp
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      initContainers:  <span style=color:green># check if DB is ready, and only continue when true</span>
</span></span><span style=display:flex><span>      - name: check-db-ready
</span></span><span style=display:flex><span>        image: postgres:9.6.5
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#39;sh&#39;</span>, <span style=color:#a31515>&#39;-c&#39;</span>,  <span style=color:#a31515>&#39;until pg_isready -h postgres -p 5432;  do echo waiting for database; sleep 2; done;&#39;</span>]
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - image: xcoulon/go-url-shortener:0.1.0
</span></span><span style=display:flex><span>        name: go-url-shortener
</span></span><span style=display:flex><span>        env:
</span></span><span style=display:flex><span>        - name: POSTGRES_HOST
</span></span><span style=display:flex><span>          value: postgres
</span></span><span style=display:flex><span>        - name: POSTGRES_PORT
</span></span><span style=display:flex><span>          value: <span style=color:#a31515>&#34;5432&#34;</span>
</span></span><span style=display:flex><span>        - name: POSTGRES_DATABASE
</span></span><span style=display:flex><span>          value: url_shortener_db
</span></span><span style=display:flex><span>        - name: POSTGRES_USER
</span></span><span style=display:flex><span>          value: user
</span></span><span style=display:flex><span>        - name: POSTGRES_PASSWORD
</span></span><span style=display:flex><span>          value: mysecretpassword
</span></span><span style=display:flex><span>        ports:
</span></span><span style=display:flex><span>        - containerPort: 8080
</span></span></code></pre></div><p>In the above example, the <code>InitContainers</code> use the docker image <code>postgres:9.6.5</code>, which is different from the application container.
This also brings the advantage of not having to include unnecessary tools (e.g. pg_isready) in the application container.</p><p>With introduction of <code>InitContainers</code>, in case the database is not available yet, the pod startup will look like similarly to:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get po -w
</span></span><span style=display:flex><span>NAME                                READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>nginx-deployment-5cc79d6bfd-t9n8h   1/1       Running   0          5d
</span></span><span style=display:flex><span>privileged-pod                      1/1       Running   0          4d
</span></span><span style=display:flex><span>webapp-fdcb49cbc-4gs4n   0/1       Pending   0         0s
</span></span><span style=display:flex><span>webapp-fdcb49cbc-4gs4n   0/1       Pending   0         0s
</span></span><span style=display:flex><span>webapp-fdcb49cbc-4gs4n   0/1       Init:0/1   0         0s
</span></span><span style=display:flex><span>webapp-fdcb49cbc-4gs4n   0/1       Init:0/1   0         1s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl  logs webapp-fdcb49cbc-4gs4n
</span></span><span style=display:flex><span>Error from server (BadRequest): container <span style=color:#a31515>&#34;go-url-shortener&#34;</span> in pod <span style=color:#a31515>&#34;webapp-fdcb49cbc-4gs4n&#34;</span> is waiting to start: PodInitializing
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-02cf553d7991e467aa6ce1be8575e57c>7.14 - Out-Dated HTML and JS Files Delivered</h1><div class=lead>Why is my application always outdated?</div><h2 id=problem>Problem</h2><p><strong>After updating your HTML and JavaScript sources in your web application, the Kubernetes cluster delivers outdated versions - why?</strong></p><h2 id=overview>Overview</h2><p>By default, Kubernetes service pods are not accessible from the external
network, but only from other pods within the same Kubernetes cluster.</p><p>The Gardener cluster has a built-in configuration for HTTP load balancing called <strong>Ingress</strong>,
defining rules for external connectivity to Kubernetes services. Users who want external access
to their Kubernetes services create an ingress resource that defines rules,
including the URI path, backing service name, and other information. The Ingress controller
can then automatically program a frontend load balancer to enable Ingress configuration.</p><p><img src=/__resources/howto-nginx_d563b7.svg alt=nginx></p><h2 id=example-ingress-configuration>Example Ingress Configuration</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: vuejs-ingress
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: test.ingress.&lt;GARDENER-CLUSTER&gt;.&lt;GARDENER-PROJECT&gt;.shoot.canary.k8s-hana.ondemand.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - backend:
</span></span><span style=display:flex><span>          serviceName: vuejs-svc
</span></span><span style=display:flex><span>          servicePort: 8080
</span></span></code></pre></div><p>where:</p><ul><li><strong>&lt;GARDENER-CLUSTER></strong>: The cluster name in the Gardener</li><li><strong>&lt;GARDENER-PROJECT></strong>: You project name in the Gardener</li></ul><h2 id=diagnosing-the-problem>Diagnosing the Problem</h2><p>The ingress controller we are using is <strong>NGINX</strong>. NGINX is a software load balancer, web server, and <strong>content cache</strong> built on top of open
source NGINX.</p><p><strong>NGINX caches the content as specified in the HTTP header.</strong> If the HTTP header is missing,
it is assumed that the cache is <strong>forever</strong> and NGINX never updates the content in the
stupidest case.</p><h2 id=solution>Solution</h2><p>In general, you can avoid this pitfall with one of the solutions below:</p><ul><li>Use a cache buster + HTTP-Cache-Control (prefered)</li><li>Use HTTP-Cache-Control with a lower retention period</li><li>Disable the caching in the ingress (just for dev purposes)</li></ul><p>Learning how to set the HTTP header or setup a cache buster is left to you, as an exercise
for your web framework (e.g. Express/NodeJS, SpringBoot, &mldr;)</p><p>Here is an example on how to disable the cache control for your ingress, done with an annotation in your
ingress YAML (during development).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: networking.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    ingress.kubernetes.io/cache-enable: <span style=color:#a31515>&#34;false&#34;</span>
</span></span><span style=display:flex><span>  name: vuejs-ingress
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: test.ingress.&lt;GARDENER-CLUSTER&gt;.&lt;GARDENER-PROJECT&gt;.shoot.canary.k8s-hana.ondemand.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - backend:
</span></span><span style=display:flex><span>          serviceName: vuejs-svc
</span></span><span style=display:flex><span>          servicePort: 8080
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-65dfe81309edaac7ac0f7a1b91b6b916>7.15 - Remove Committed Secrets in Github 💀</h1><div class=lead>Never ever commit a kubeconfig.yaml into github</div><h2 id=overview>Overview</h2><p>If you commit sensitive data, such as a <code>kubeconfig.yaml</code> or <code>SSH key</code> into a Git repository, you can remove it from
the history. To entirely remove unwanted files from a repository&rsquo;s history you can use the git <code>filter-branch</code> command.</p><p>The git <code>filter-branch</code> command rewrites your repository&rsquo;s history, which changes the SHAs for existing commits that you alter and any dependent commits. Changed commit SHAs may affect open pull requests in your repository. <strong>Merging or closing all open pull requests before removing files from your repository is recommended.</strong></p><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>If someone has already checked out the repository, then of course they have the secret on their computer. So ALWAYS revoke the OAuthToken/Password or whatever it was immediately.</div><h2 id=purging-a-file-from-your-repositorys-history>Purging a File from Your Repository&rsquo;s History</h2><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>If you run <code>git filter-branch</code> after stashing changes, you won&rsquo;t be able to retrieve your changes with other
stash commands. Before running <code>git filter-branch</code>, we recommend unstashing any changes you&rsquo;ve made. To unstash the
last set of changes you&rsquo;ve stashed, run <code>git stash show -p | git apply -R</code>. For more information, see <a href=https://git-scm.com/book/en/v2/Git-Tools-Stashing-and-Cleaning>Git Tools - Stashing and Cleaning</a>.</div><p>To illustrate how <code>git filter-branch</code> works, we&rsquo;ll show you how to remove your file with sensitive data from the
history of your repository and add it to .gitignore to ensure that it is not accidentally re-committed.</p><p><strong>1. Navigate into the repository&rsquo;s working directory:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd YOUR-REPOSITORY
</span></span></code></pre></div><p><strong>2. Run the following command, replacing <code>PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA</code> with the path to the file you want to remove,
not just its filename.</strong></p><p>These arguments will:</p><ul><li>Force Git to process, but not check out, the entire history of every branch and tag</li><li>Remove the specified file, as well as any empty commits generated as a result</li><li>Overwrite your existing tags</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git filter-branch --force --index-filter <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span><span style=color:#a31515>&#39;git rm --cached --ignore-unmatch PATH-TO-YOUR-FILE-WITH-SENSITIVE-DATA&#39;</span> <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>--prune-empty --tag-name-filter cat -- --all
</span></span></code></pre></div><p><strong>3. Add your file with sensitive data to <code>.gitignore</code> to ensure that you don&rsquo;t accidentally commit it again:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span> echo <span style=color:#a31515>&#34;YOUR-FILE-WITH-SENSITIVE-DATA&#34;</span> &gt;&gt; .gitignore
</span></span></code></pre></div><p>Double-check that you&rsquo;ve removed everything you wanted to from your repository&rsquo;s history, and that all of your
branches are checked out. Once you&rsquo;re happy with the state of your repository, continue to the next step.</p><p><strong>4. Force-push your local changes to overwrite your GitHub repository, as well as all the branches you&rsquo;ve pushed up:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git push origin --force --all
</span></span></code></pre></div><p><strong>4. In order to remove the sensitive file from your tagged releases, you&rsquo;ll also need to force-push against your Git tags:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git push origin --force --tags
</span></span></code></pre></div><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>Tell your collaborators to <strong>rebase, not merge</strong>, any branches they created off of your old (tainted) repository history.
One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging.</div><h2 id=related-links>Related Links</h2><ul><li><a href=https://help.github.com/articles/removing-sensitive-data-from-a-repository/>Removing Sensitive Data from a Repository</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-77bb872706e47f8307662d7339184bb5>7.16 - Using Prometheus and Grafana to Monitor K8s</h1><div class=lead>How to deploy and configure Prometheus and Grafana to collect and monitor kubelet container metrics</div><h2 id=disclaimer>Disclaimer</h2><p>This post is meant to give a basic end-to-end description for deploying and using Prometheus and Grafana. Both
applications offer a wide range of flexibility, which needs to be considered in case you have specific requirements.
Such advanced details are not in the scope of this topic.</p><h2 id=introduction>Introduction</h2><p><a href=https://prometheus.io/>Prometheus</a> is an open-source systems monitoring and alerting toolkit for recording numeric
time series. It fits both machine-centric monitoring as well as monitoring of highly dynamic service-oriented
architectures. In a world of microservices, its support for multi-dimensional data collection and querying is a
particular strength.</p><p>Prometheus is the second hosted project to <a href=https://prometheus.io/blog/2018/08/09/prometheus-graduates-within-cncf/>graduate within CNCF</a>.</p><p>The following characteristics make Prometheus a good match for monitoring Kubernetes clusters:</p><ul><li><p>Pull-based Monitoring<br>Prometheus is a <a href=https://prometheus.io/blog/2016/07/23/pull-does-not-scale-or-does-it/>pull-based</a> monitoring system,
which means that the Prometheus server dynamically discovers and pulls metrics from your services running in Kubernetes.</p></li><li><p>Labels
Prometheus and Kubernetes share the same label (key-value) concept that can be used to select objects in the system.<br>Labels are used to identify time series and sets of label matchers can be used in the query language
(<a href=https://prometheus.io/docs/prometheus/latest/querying/basics/>PromQL</a>) to select the time series to be aggregated.</p></li><li><p>Exporters<br>There are many <a href=https://prometheus.io/docs/instrumenting/exporters/>exporters</a> available, which enable integration of
databases or even other monitoring systems not already providing a way to export metrics to Prometheus.
One prominent exporter is the so called <a href=https://github.com/prometheus/node_exporter>node-exporter</a>, which allows to
monitor hardware and OS related metrics of Unix systems.</p></li><li><p>Powerful Query Language<br>The Prometheus query language <a href=https://prometheus.io/docs/prometheus/latest/querying/basics/>PromQL</a> lets the user
select and aggregate time series data in real time. Results can either be shown as a graph, viewed
as tabular data in the Prometheus expression browser, or consumed by external systems via the <a href=https://prometheus.io/docs/prometheus/latest/querying/api/>HTTP API</a>.</p></li></ul><p>Find query examples on <a href=https://github.com/infinityworks/prometheus-example-queries/blob/master/README.md>Prometheus Query Examples</a>.</p><p>One very popular open-source visualization tool not only for Prometheus is <a href=https://grafana.com>Grafana</a>. Grafana is a
metric analytics and visualization suite. It is popular for visualizing time series data for infrastructure
and application analytics but many use it in other domains including industrial sensors, home automation, weather, and
process control. For more information, see the <a href=http://docs.grafana.org/>Grafana Documentation</a>.</p><p>Grafana accesses data via <a href=https://grafana.com/docs/grafana/latest/basics/>Data Sources</a>. The continuously growing
list of supported backends includes Prometheus.</p><p>Dashboards are created by combining panels, e.g. <a href=http://docs.grafana.org/reference/graph/>Graph</a> and <a href=http://docs.grafana.org/reference/dashlist/>Dashlist</a>.</p><p>In this example, we describe an End-To-End scenario including the deployment of Prometheus and a basic monitoring
configuration as the one provided for Kubernetes clusters created by Gardener.</p><p>If you miss elements on the Prometheus web page when accessing it via its service URL <code>https://&lt;your K8s FQN>/api/v1/namespaces/&lt;your-prometheus-namespace>/services/prometheus-prometheus-server:80/proxy</code>,
this is probably caused by a Prometheus issue - <a href=https://github.com/prometheus/prometheus/issues/1583>#1583</a>
To workaround this issue, setup a port forward <code>kubectl port-forward -n &lt;your-prometheus-namespace> &lt;prometheus-pod> 9090:9090</code>
on your client and access the Prometheus UI from there with your locally installed web browser. This issue is not relevant
in case you use the service type <code>LoadBalancer</code>.</p><h2 id=preparation>Preparation</h2><p>The deployment of <a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus</a> and <a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana</a> is based on Helm charts.<br>Make sure to implement the Helm settings before deploying the Helm charts.</p><p>The Kubernetes clusters provided by <a href=https://github.com/gardener>Gardener</a> use role based
access control (<a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/>RBAC</a>). To authorize the Prometheus
node-exporter to access hardware and OS relevant metrics of your cluster&rsquo;s worker nodes, specific artifacts need to be
deployed.</p><p>Bind the Prometheus service account to the <code>garden.sapcloud.io:monitoring:prometheus</code> cluster role by running the command
<code>kubectl apply -f crbinding.yaml</code>.</p><p>Content of <code>crbinding.yaml</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: rbac.authorization.k8s.io/v1beta1
</span></span><span style=display:flex><span>kind: ClusterRoleBinding
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: &lt;your-prometheus-name&gt;-server
</span></span><span style=display:flex><span>roleRef:
</span></span><span style=display:flex><span>  apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span>  kind: ClusterRole
</span></span><span style=display:flex><span>  name: garden.sapcloud.io:monitoring:prometheus
</span></span><span style=display:flex><span>subjects:
</span></span><span style=display:flex><span>- kind: ServiceAccount
</span></span><span style=display:flex><span>  name: &lt;your-prometheus-name&gt;-server
</span></span><span style=display:flex><span>  namespace: &lt;your-prometheus-namespace&gt;
</span></span></code></pre></div><h2 id=deployment-of-prometheus-and-grafana>Deployment of Prometheus and Grafana</h2><p>Only minor changes are needed to deploy <a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus</a>
and <a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana</a> based on Helm charts.</p><p>Copy the following configuration into a file called <code>values.yaml</code> and deploy Prometheus:
<code>helm install &lt;your-prometheus-name> --namespace &lt;your-prometheus-namespace> stable/prometheus -f values.yaml</code></p><p>Typically, Prometheus and Grafana are deployed into the same namespace. There is no technical reason behind this, so feel
free to choose different namespaces.</p><p>Content of <code>values.yaml</code> for Prometheus:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>rbac:
</span></span><span style=display:flex><span>  create: <span style=color:#00f>false</span> <span style=color:green># Already created in Preparation step</span>
</span></span><span style=display:flex><span>nodeExporter:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>false</span> <span style=color:green># The node-exporter is already deployed by default</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>server:
</span></span><span style=display:flex><span>  global:
</span></span><span style=display:flex><span>    scrape_interval: 30s
</span></span><span style=display:flex><span>    scrape_timeout: 30s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>serverFiles:
</span></span><span style=display:flex><span>  prometheus.yml:
</span></span><span style=display:flex><span>    rule_files:
</span></span><span style=display:flex><span>      - /etc/config/rules
</span></span><span style=display:flex><span>      - /etc/config/alerts      
</span></span><span style=display:flex><span>    scrape_configs:
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kube-kubelet&#39;</span>
</span></span><span style=display:flex><span>      honor_labels: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      scheme: https
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      tls_config:
</span></span><span style=display:flex><span>      <span style=color:green># This is needed because the kubelets&#39; certificates are not generated</span>
</span></span><span style=display:flex><span>      <span style=color:green># for a specific pod IP</span>
</span></span><span style=display:flex><span>        insecure_skip_verify: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>      - role: node
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>      - target_label: __metrics_path__
</span></span><span style=display:flex><span>        replacement: /metrics
</span></span><span style=display:flex><span>      - source_labels: [__meta_kubernetes_node_address_InternalIP]
</span></span><span style=display:flex><span>        target_label: instance
</span></span><span style=display:flex><span>      - action: labelmap
</span></span><span style=display:flex><span>        regex: __meta_kubernetes_node_label_(.+)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kube-kubelet-cadvisor&#39;</span>
</span></span><span style=display:flex><span>      honor_labels: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      scheme: https
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      tls_config:
</span></span><span style=display:flex><span>      <span style=color:green># This is needed because the kubelets&#39; certificates are not generated</span>
</span></span><span style=display:flex><span>      <span style=color:green># for a specific pod IP</span>
</span></span><span style=display:flex><span>        insecure_skip_verify: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>      - role: node
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>      - target_label: __metrics_path__
</span></span><span style=display:flex><span>        replacement: /metrics/cadvisor
</span></span><span style=display:flex><span>      - source_labels: [__meta_kubernetes_node_address_InternalIP]
</span></span><span style=display:flex><span>        target_label: instance
</span></span><span style=display:flex><span>      - action: labelmap
</span></span><span style=display:flex><span>        regex: __meta_kubernetes_node_label_(.+)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:green># Example scrape config for probing services via the Blackbox Exporter.</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Relabelling allows to configure the actual service scrape endpoint using the following annotations:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/probe`: Only probe services that have a value of `true`</span>
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kubernetes-services&#39;</span>
</span></span><span style=display:flex><span>      metrics_path: /probe
</span></span><span style=display:flex><span>      params:
</span></span><span style=display:flex><span>        module: [http_2xx]
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>        - role: service
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
</span></span><span style=display:flex><span>          action: keep
</span></span><span style=display:flex><span>          regex: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - source_labels: [__address__]
</span></span><span style=display:flex><span>          target_label: __param_target
</span></span><span style=display:flex><span>        - target_label: __address__
</span></span><span style=display:flex><span>          replacement: blackbox
</span></span><span style=display:flex><span>        - source_labels: [__param_target]
</span></span><span style=display:flex><span>          target_label: instance
</span></span><span style=display:flex><span>        - action: labelmap
</span></span><span style=display:flex><span>          regex: __meta_kubernetes_service_label_(.+)
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_namespace]
</span></span><span style=display:flex><span>          target_label: kubernetes_namespace
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_name]
</span></span><span style=display:flex><span>          target_label: kubernetes_name
</span></span><span style=display:flex><span>    <span style=color:green># Example scrape config for pods</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># Relabelling allows to configure the actual service scrape endpoint using the following annotations:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/scrape`: Only scrape pods that have a value of `true`</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/path`: If the metrics path is not `/metrics` override this.</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.</span>
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kubernetes-pods&#39;</span>
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>        - role: pod
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
</span></span><span style=display:flex><span>          action: keep
</span></span><span style=display:flex><span>          regex: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: __metrics_path__
</span></span><span style=display:flex><span>          regex: (.+)
</span></span><span style=display:flex><span>        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          regex: (.+):(?:\d+);(\d+)
</span></span><span style=display:flex><span>          replacement: ${1}:${2}
</span></span><span style=display:flex><span>          target_label: __address__
</span></span><span style=display:flex><span>        - action: labelmap
</span></span><span style=display:flex><span>          regex: __meta_kubernetes_pod_label_(.+)
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_namespace]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: kubernetes_namespace
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_pod_name]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: kubernetes_pod_name
</span></span><span style=display:flex><span>    <span style=color:green># Scrape config for service endpoints.</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># The relabeling allows the actual service scrape endpoint to be configured</span>
</span></span><span style=display:flex><span>    <span style=color:green># via the following annotations:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/scrape`: Only scrape services that have a value of `true`</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need</span>
</span></span><span style=display:flex><span>    <span style=color:green># to set this to `https` &amp; most likely set the `tls_config` of the scrape config.</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/path`: If the metrics path is not `/metrics` override this.</span>
</span></span><span style=display:flex><span>    <span style=color:green># * `prometheus.io/port`: If the metrics are exposed on a different port to the</span>
</span></span><span style=display:flex><span>    <span style=color:green># service then set this appropriately.</span>
</span></span><span style=display:flex><span>    - job_name: <span style=color:#a31515>&#39;kubernetes-service-endpoints&#39;</span>
</span></span><span style=display:flex><span>      kubernetes_sd_configs:
</span></span><span style=display:flex><span>        - role: endpoints
</span></span><span style=display:flex><span>      relabel_configs:
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
</span></span><span style=display:flex><span>          action: keep
</span></span><span style=display:flex><span>          regex: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: __scheme__
</span></span><span style=display:flex><span>          regex: (https?)
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: __metrics_path__
</span></span><span style=display:flex><span>          regex: (.+)
</span></span><span style=display:flex><span>        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: __address__
</span></span><span style=display:flex><span>          regex: (.+)(?::\d+);(\d+)
</span></span><span style=display:flex><span>          replacement: $1:$2
</span></span><span style=display:flex><span>        - action: labelmap
</span></span><span style=display:flex><span>          regex: __meta_kubernetes_service_label_(.+)
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_namespace]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: kubernetes_namespace
</span></span><span style=display:flex><span>        - source_labels: [__meta_kubernetes_service_name]
</span></span><span style=display:flex><span>          action: replace
</span></span><span style=display:flex><span>          target_label: kubernetes_name <span style=color:green># Add your additional configuration here...</span>
</span></span></code></pre></div><p>Next, deploy Grafana. Since the deployment in this post is based on the Helm default values, the settings below are set
explicitly in case the default changed.
Deploy Grafana via <code>helm install grafana --namespace &lt;your-prometheus-namespace> stable/grafana -f values.yaml</code>. Here, the same namespace is chosen for Prometheus and for Grafana.</p><p>Content of <code>values.yaml</code> for Grafana:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>server:
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>    enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  service:
</span></span><span style=display:flex><span>    type: ClusterIP
</span></span></code></pre></div><p>Check the running state of the pods on the Kubernetes Dashboard or by running <code>kubectl get pods -n &lt;your-prometheus-namespace></code>.
In case of errors, check the log files of the pod(s) in question.</p><p>The text output of Helm after the deployment of Prometheus and Grafana contains very useful information, e.g. the user
and password of the Grafana Admin user. The credentials are stored as secrets in the namespace <code>&lt;your-prometheus-namespace></code>
and could be decoded via <code>kubectl get secret --namespace &lt;my-grafana-namespace> grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo</code>.</p><h2 id=basic-functional-tests>Basic Functional Tests</h2><p>To access the web UI of both applications, use port forwarding of port 9090.</p><p>Setup port forwarding for port 9090:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl port-forward -n &lt;your-prometheus-namespace&gt; &lt;your-prometheus-server-pod&gt; 9090:9090
</span></span></code></pre></div><p>Open <code>http://localhost:9090</code> in your web browser. Select Graph from the top tab and enter the following expressing to show the overall CPU usage for a server (see <a href=https://github.com/infinityworks/prometheus-example-queries/blob/master/README.md>Prometheus Query Examples</a>):</p><pre tabindex=0><code>100 * (1 - avg by(instance)(irate(node_cpu{mode=&#39;idle&#39;}[5m])))
</code></pre><p>This should show some data in a graph.</p><p>To show the same data in Grafana setup port forwarding for port 3000 for the
Grafana pod and open the Grafana Web UI by opening <code>http://localhost:3000</code> in a browser.
Enter the credentials of the admin user.</p><p>Next, you need to enter the server name of your Prometheus deployment. This name is shown directly after the
installation via helm.</p><p>Run</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>helm status &lt;your-prometheus-name&gt;
</span></span></code></pre></div><p>to find this name. Below, this server name is referenced by <code>&lt;your-prometheus-server-name></code>.</p><p>First, you need to add your Prometheus server as data source:</p><ol><li>Select <em>Dashboards → Data Sources</em></li><li>Select <em>Add data source</em></li><li>Enter
<em>Name</em>: <code>&lt;your-prometheus-datasource-name></code><br><em>Type</em>: Prometheus<br><em>URL</em>: <code>http://&lt;your-prometheus-server-name></code><br><em>Access</em>: <code>proxy</code></li><li>Select <em>Save & Test</em></li></ol><p>In case of failure, check the Prometheus URL in the Kubernetes Dashboard.</p><p>To add a Graph follow these steps:</p><ol><li>In the left corner, select <em>Dashboards → New</em> to create a new dashboard</li><li>Select <em>Graph</em> to create a new graph</li><li>Next, select the <em>Panel Title → Edit</em></li><li>Select your Prometheus Data Source in the drop down list</li><li>Enter the expression <code>100 * (1 - avg by(instance)(irate(node_cpu{mode='idle'}[5m])))</code> in the entry field A</li><li>Select the floppy disk symbol (Save) on top</li></ol><p>Now you should have a very basic Prometheus and Grafana setup for your Kubernetes cluster.</p><p>As a next step you can implement monitoring for your applications by implementing the <a href=https://prometheus.io/docs/instrumenting/clientlibs/>Prometheus client API</a>.</p><h2 id=related-links>Related Links</h2><ul><li><a href=https://prometheus.io/>Prometheus</a></li><li><a href=https://github.com/kubernetes/charts/tree/master/stable/prometheus>Prometheus Helm Chart</a></li><li><a href=https://www.weave.works/blog/prometheus-kubernetes-perfect-match/>Prometheus and Kubernetes: A Perfect Match</a></li><li><a href=https://grafana.com>Grafana</a></li><li><a href=https://github.com/kubernetes/charts/tree/master/stable/grafana>Grafana Helm Chart</a></li></ul></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2023 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.7b24c0fb082ffb2de6cb14d6c95e9f8053053709ffcf8c761ef8e9ad2f8021e4.js integrity="sha256-eyTA+wgv+y3myxTWyV6fgFMFNwn/z4x2HvjprS+AIeQ=" crossorigin=anonymous></script></body></html>