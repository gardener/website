<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://gardener.cloud/blog/2025/05/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/blog/2025/05/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>May | Gardener</title><meta name=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:url" content="https://gardener.cloud/blog/2025/05/"><meta property="og:site_name" content="Gardener"><meta property="og:title" content="May"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:locale" content="en_US"><meta property="og:type" content="website"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="May"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta itemprop=datePublished content="2025-05-21T00:00:00+00:00"><meta itemprop=dateModified content="2025-05-21T00:00:00+00:00"><meta itemprop=image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="May"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.64d56283aba037cc3a217d684edadfb4e3c57ca54122947d2f030f74bcd28a27.css as=style integrity="sha256-ZNVig6ugN8w6IX1oTtrftOPFfKVBIpR9LwMPdLzSiic=" crossorigin=anonymous><link href=/scss/main.min.64d56283aba037cc3a217d684edadfb4e3c57ca54122947d2f030f74bcd28a27.css rel=stylesheet integrity="sha256-ZNVig6ugN8w6IX1oTtrftOPFfKVBIpR9LwMPdLzSiic=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class="td-section td-blog"><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg width="90" height="90" viewBox="0 0 90 90" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009F76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=navbar-brand__name>Gardener</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://demo.gardener.cloud target=_blank rel=noopener><span>Demo</span></a></li><li class=nav-item><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class=nav-item><a class=nav-link href=/docs><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/blog><span>Blogs</span></a></li><li class=nav-item><a class=nav-link href=/community><span>Community</span></a></li><li class=nav-item><a class=nav-link href=https://join.slack.com/t/gardener-cloud/shared_invite/zt-33c9daems-3oOorhnqOSnldZPWqGmIBw target=_blank rel=noopener><span>Join us on</span></a></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.0364fcacf5988c666dfcede3b5edb6bd.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 ps-md-5 pe-md-4" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/blog/2025/05/>Return to the regular view of this page</a>.</p></div><h1 class=title>May</h1><div class=content></div></div><div class=td-content><h1 id=pg-c9f1fffb57fbf0b00ef79ab5f034da98>Fine-Tuning kube-proxy Readiness: Ensuring Accurate Health Checks During Node Scale-Down</h1><div class="td-byline mb-4"><time datetime=2025-05-21 class=text-body-secondary>Wednesday, May 21, 2025</time></div><p>Gardener has recently refined how it determines the readiness of <code>kube-proxy</code> components within managed Kubernetes clusters. This adjustment leads to more accurate system health reporting, especially during node scale-down operations orchestrated by <code>cluster-autoscaler</code>.</p><h3 id=the-challenge-kube-proxy-readiness-during-node-scale-down>The Challenge: kube-proxy Readiness During Node Scale-Down<a class=td-heading-self-link href=#the-challenge-kube-proxy-readiness-during-node-scale-down aria-label="Heading self-link"></a></h3><p>Previously, Gardener utilized <code>kube-proxy</code>&rsquo;s <code>/healthz</code> endpoint for its readiness probe. While generally effective, this endpoint&rsquo;s behavior changed in Kubernetes 1.28 (as part of <a href="https://github.com/alexanderConstantinescu/kubernetes-enhancements/blob/e3d8adae9cf79338add2149db0900e47a4c64338/keps/sig-network/3836-kube-proxy-improved-ingress-connectivity-reliability/README.md?plain=1#L105-L107">KEP-3836</a> and implemented in <a href=https://github.com/kubernetes/kubernetes/pull/116470>kubernetes/kubernetes#116470</a>). The <code>/healthz</code> endpoint now reports <code>kube-proxy</code> as unhealthy if its node is marked for deletion by <code>cluster-autoscaler</code> (e.g., via a specific taint) or has a deletion timestamp.</p><p>This behavior is intended to help external load balancers (particularly those using <code>externalTrafficPolicy: Cluster</code> on infrastructures like GCP) avoid sending <em>new</em> traffic to nodes that are about to be terminated. However, for Gardener&rsquo;s internal system component health checks, this meant that <code>kube-proxy</code> could appear unready for extended periods if node deletion was delayed due to <code>PodDisruptionBudgets</code> or long <code>terminationGracePeriodSeconds</code>. This could lead to misleading &ldquo;unhealthy&rdquo; states for the cluster&rsquo;s system components.</p><h3 id=the-solution-aligning-with-upstream-kube-proxy-enhancements>The Solution: Aligning with Upstream kube-proxy Enhancements<a class=td-heading-self-link href=#the-solution-aligning-with-upstream-kube-proxy-enhancements aria-label="Heading self-link"></a></h3><p>To address this, Gardener now leverages the <code>/livez</code> endpoint for <code>kube-proxy</code>&rsquo;s readiness probe in clusters running Kubernetes version 1.28 and newer. The <code>/livez</code> endpoint, also introduced as part of the aforementioned <code>kube-proxy</code> improvements, checks the actual liveness of the <code>kube-proxy</code> process itself, without considering the node&rsquo;s termination status.</p><p>For clusters running Kubernetes versions 1.27.x and older (where <code>/livez</code> is not available), Gardener will continue to use the <code>/healthz</code> endpoint for the readiness probe.</p><p>This change, detailed in <a href=https://github.com/gardener/gardener/pull/12015>gardener/gardener#12015</a>, ensures that Gardener&rsquo;s readiness check for <code>kube-proxy</code> accurately reflects <code>kube-proxy</code>&rsquo;s operational status rather than the node&rsquo;s lifecycle state. It&rsquo;s important to note that this adjustment does not interfere with the goals of KEP-3836; cloud controller managers can still utilize the <code>/healthz</code> endpoint for their load balancer health checks as intended.</p><h3 id=benefits-for-gardener-operators>Benefits for Gardener Operators<a class=td-heading-self-link href=#benefits-for-gardener-operators aria-label="Heading self-link"></a></h3><p>This enhancement brings a key benefit to Gardener operators:</p><ul><li><strong>More Accurate System Health:</strong> The system components health check will no longer report <code>kube-proxy</code> as unhealthy simply because its node is being gracefully terminated by <code>cluster-autoscaler</code>. This reduces false alarms and provides a clearer view of the cluster&rsquo;s actual health.</li><li><strong>Smoother Operations:</strong> Operations teams will experience fewer unnecessary alerts related to <code>kube-proxy</code> during routine scale-down events, allowing them to focus on genuine issues.</li></ul><p>By adapting its <code>kube-proxy</code> readiness checks, Gardener continues to refine its operational robustness, providing a more stable and predictable management experience.</p><h3 id=further-information>Further Information<a class=td-heading-self-link href=#further-information aria-label="Heading self-link"></a></h3><ul><li><strong>GitHub Pull Request:</strong> <a href=https://github.com/gardener/gardener/pull/12015>gardener/gardener#12015</a></li><li><strong>Recording of the presentation segment:</strong> <a href="https://youtu.be/ssvXpPliOY0?t=1151">Watch on YouTube (starts at the relevant section)</a></li><li><strong>Upstream KEP:</strong> <a href="https://github.com/alexanderConstantinescu/kubernetes-enhancements/blob/e3d8adae9cf79338add2149db0900e47a4c64338/keps/sig-network/3836-kube-proxy-improved-ingress-connectivity-reliability/README.md?plain=1#L105-L107">KEP-3836: Kube-proxy improved ingress connectivity reliability</a></li><li><strong>Upstream Kubernetes PR:</strong> <a href=https://github.com/kubernetes/kubernetes/pull/116470>kubernetes/kubernetes#116470</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-584d5f8b980f6c0ea31f0083ecf96c90>New in Gardener: Forceful Redeployment of gardenlets for Enhanced Operational Control</h1><div class="td-byline mb-4"><time datetime=2025-05-21 class=text-body-secondary>Wednesday, May 21, 2025</time></div><p>Gardener continues to enhance its operational capabilities, and a recent improvement introduces a much-requested feature for managing gardenlets: the ability to forcefully trigger their redeployment. This provides operators with greater control and a streamlined recovery path for specific scenarios.</p><h3 id=the-standard-gardenlet-lifecycle>The Standard gardenlet Lifecycle<a class=td-heading-self-link href=#the-standard-gardenlet-lifecycle aria-label="Heading self-link"></a></h3><p>gardenlets, crucial components in the Gardener architecture, are typically deployed into seed clusters. For setups utilizing the <code>seedmanagement.gardener.cloud/v1alpha1.Gardenlet</code> resource, particularly in unmanaged seeds (those not backed by a shoot cluster and <code>ManagedSeed</code> resource), the <code>gardener-operator</code> handles the initial deployment of the gardenlet.</p><p>Once this initial deployment is complete, the gardenlet takes over its own lifecycle, leveraging a self-upgrade strategy to keep itself up-to-date. Under normal circumstances, the <code>gardener-operator</code> does not intervene further after this initial phase.</p><h3 id=when-things-go-awry-the-need-for-intervention>When Things Go Awry: The Need for Intervention<a class=td-heading-self-link href=#when-things-go-awry-the-need-for-intervention aria-label="Heading self-link"></a></h3><p>While the self-upgrade mechanism is robust, certain situations can arise where a gardenlet might require a more direct intervention. For example:</p><ul><li>The gardenlet&rsquo;s client certificate to the virtual garden cluster might have expired or become invalid.</li><li>The gardenlet <code>Deployment</code> in the seed cluster might have been accidentally deleted or become corrupted.</li></ul><p>In such cases, because the <code>gardener-operator</code>&rsquo;s responsibility typically ends after the initial deployment, the gardenlet might not be able to recover on its own, potentially leading to operational issues.</p><h3 id=empowering-operators-the-force-redeploy-annotation>Empowering Operators: The Force-Redeploy Annotation<a class=td-heading-self-link href=#empowering-operators-the-force-redeploy-annotation aria-label="Heading self-link"></a></h3><p>To address these challenges, Gardener now allows operators to instruct the <code>gardener-operator</code> to forcefully redeploy a gardenlet. This is achieved by annotating the specific <code>Gardenlet</code> resource with:</p><pre tabindex=0><code>gardener.cloud/operation=force-redeploy
</code></pre><p>When this annotation is applied, it signals the <code>gardener-operator</code> to re-initiate the deployment process for the targeted gardenlet, effectively overriding the usual hands-off approach after initial setup.</p><h3 id=how-it-works>How It Works<a class=td-heading-self-link href=#how-it-works aria-label="Heading self-link"></a></h3><p>The process for a forceful redeployment is straightforward:</p><ol><li>An operator identifies a gardenlet that requires redeployment due to issues like an expired certificate or a missing deployment.</li><li>The operator applies the <code>gardener.cloud/operation=force-redeploy</code> annotation to the corresponding <code>seedmanagement.gardener.cloud/v1alpha1.Gardenlet</code> resource in the virtual garden cluster.</li><li><strong>Important:</strong> If the gardenlet is for a remote cluster and its kubeconfig <code>Secret</code> was previously removed (a standard cleanup step after initial deployment), this <code>Secret</code> must be recreated, and its reference (<code>.spec.kubeconfigSecretRef</code>) must be re-added to the <code>Gardenlet</code> specification.</li><li>The <code>gardener-operator</code> detects the annotation and proceeds to redeploy the gardenlet, applying its configurations and charts anew.</li><li>Once the redeployment is successfully completed, the <code>gardener-operator</code> automatically removes the <code>gardener.cloud/operation=force-redeploy</code> annotation from the <code>Gardenlet</code> resource. Similar to the initial deployment, it will also clean up the referenced kubeconfig <code>Secret</code> and set <code>.spec.kubeconfigSecretRef</code> to <code>nil</code> if it was provided.</li></ol><h3 id=benefits>Benefits<a class=td-heading-self-link href=#benefits aria-label="Heading self-link"></a></h3><p>This new feature offers significant advantages for Gardener operators:</p><ul><li><strong>Enhanced Recovery:</strong> Provides a clear and reliable mechanism to recover gardenlets from specific critical failure states.</li><li><strong>Improved Operational Flexibility:</strong> Offers more direct control over the gardenlet lifecycle when exceptional circumstances demand it.</li><li><strong>Reduced Manual Effort:</strong> Streamlines the process of restoring a misbehaving gardenlet, minimizing potential downtime or complex manual recovery procedures.</li></ul><p>This enhancement underscores Gardener&rsquo;s commitment to operational excellence and responsiveness to the needs of its user community.</p><h3 id=dive-deeper>Dive Deeper<a class=td-heading-self-link href=#dive-deeper aria-label="Heading self-link"></a></h3><p>To learn more about this feature, you can explore the following resources:</p><ul><li><strong>GitHub Pull Request:</strong> <a href=https://github.com/gardener/gardener/pull/11972>gardener/gardener#11972</a></li><li><strong>Official Documentation:</strong> <a href=https://github.com/gardener/gardener/tree/master/docs/deployment/deploy_gardenlet_via_operator.md#forceful-re-deployment>Forceful Re-Deployment of gardenlets</a></li><li><strong>Community Meeting Recording (starts at the relevant segment):</strong> <a href="https://youtu.be/ssvXpPliOY0?t=338">Gardener Review Meeting on YouTube</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7f61ecaa1cf2430641f2c9565b2c53d1>Streamlined Node Onboarding: Introducing `gardenadm token` and `gardenadm join`</h1><div class="td-byline mb-4"><time datetime=2025-05-21 class=text-body-secondary>Wednesday, May 21, 2025</time></div><p>Gardener continues to enhance its <code>gardenadm</code> tool, simplifying the management of autonomous Shoot clusters. Recently, new functionalities have been introduced to streamline the process of adding worker nodes to these clusters: the <code>gardenadm token</code> command suite and the corresponding <code>gardenadm join</code> command. These additions offer a more convenient and Kubernetes-native experience for cluster expansion.</p><h3 id=managing-bootstrap-tokens-with-gardenadm-token>Managing Bootstrap Tokens with <code>gardenadm token</code><a class=td-heading-self-link href=#managing-bootstrap-tokens-with-gardenadm-token aria-label="Heading self-link"></a></h3><p>A key aspect of securely joining nodes to a Kubernetes cluster is the use of bootstrap tokens. The new <code>gardenadm token</code> command provides a set of subcommands to manage these tokens effectively within your autonomous Shoot cluster&rsquo;s control plane node. This functionality is analogous to the familiar <code>kubeadm token</code> commands.</p><p>The available subcommands include:</p><ul><li><strong><code>gardenadm token list</code></strong>: Displays all current bootstrap tokens. You can also use the <code>--with-token-secrets</code> flag to include the token secrets in the output for easier inspection.</li><li><strong><code>gardenadm token generate</code></strong>: Generates a cryptographically random bootstrap token. This command only prints the token; it does not create it on the server.</li><li><strong><code>gardenadm token create [token]</code></strong>: Creates a new bootstrap token on the server. If you provide a token (in the format <code>[a-z0-9]{6}.[a-z0-9]{16}</code>), it will be used. If no token is supplied, <code>gardenadm</code> will automatically generate a random one and create it.<ul><li>A particularly helpful option for this command is <code>--print-join-command</code>. When used, instead of just outputting the token, it prints the complete <code>gardenadm join</code> command, ready to be copied and executed on the worker node you intend to join. You can also specify flags like <code>--description</code>, <code>--validity</code>, and <code>--worker-pool-name</code> to customize the token and the generated join command.</li></ul></li><li><strong><code>gardenadm token delete &lt;token-value...></code></strong>: Deletes one or more bootstrap tokens from the server. You can specify tokens by their ID, the full token string, or the name of the Kubernetes Secret storing the token (e.g., <code>bootstrap-token-&lt;id></code>).</li></ul><p>These commands provide comprehensive control over the lifecycle of bootstrap tokens, enhancing security and operational ease.</p><h3 id=joining-worker-nodes-with-gardenadm-join>Joining Worker Nodes with <code>gardenadm join</code><a class=td-heading-self-link href=#joining-worker-nodes-with-gardenadm-join aria-label="Heading self-link"></a></h3><p>Once a bootstrap token is created (ideally using <code>gardenadm token create --print-join-command</code> on a control plane node), the new <code>gardenadm join</code> command facilitates the process of adding a new worker node to the autonomous Shoot cluster.</p><p>The command is executed on the prospective worker machine and typically looks like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>gardenadm join --bootstrap-token &lt;token_id.token_secret&gt; --ca-certificate &lt;base64_encoded_ca_bundle&gt; --gardener-node-agent-secret-name &lt;os_config_secret_name&gt; &lt;control_plane_api_server_address&gt;
</span></span></code></pre></div><p>Key parameters include:</p><ul><li><code>--bootstrap-token</code>: The token obtained from the <code>gardenadm token create</code> command.</li><li><code>--ca-certificate</code>: The base64-encoded CA certificate bundle of the cluster&rsquo;s API server.</li><li><code>--gardener-node-agent-secret-name</code>: The name of the Secret in the <code>kube-system</code> namespace of the control plane that contains the OperatingSystemConfig (OSC) for the <code>gardener-node-agent</code>. This OSC dictates how the node should be configured.</li><li><code>&lt;control_plane_api_server_address></code>: The address of the Kubernetes API server of the autonomous cluster.</li></ul><p>Upon execution, <code>gardenadm join</code> performs several actions:</p><ol><li>It discovers the Kubernetes version of the control plane using the provided bootstrap token and CA certificate.</li><li>It checks if the <code>gardener-node-agent</code> has already been initialized on the machine.</li><li>If not already joined, it prepares the <code>gardener-node-init</code> configuration. This involves setting up a systemd service (<code>gardener-node-init.service</code>) which, in turn, downloads and runs the <code>gardener-node-agent</code>.</li><li>The <code>gardener-node-agent</code> then uses the bootstrap token to securely download its specific OperatingSystemConfig from the control plane.</li><li>Finally, it applies this configuration, setting up the kubelet and other necessary components, thereby officially joining the node to the cluster.</li></ol><p>After the node has successfully joined, the bootstrap token used for the process will be automatically deleted by the <code>kube-controller-manager</code> once it expires. However, it can also be manually deleted immediately using <code>gardenadm token delete</code> on the control plane node for enhanced security.</p><p>These new <code>gardenadm</code> commands significantly simplify the expansion of autonomous Shoot clusters, providing a robust and user-friendly mechanism for managing bootstrap tokens and joining worker nodes.</p><h3 id=further-information>Further Information<a class=td-heading-self-link href=#further-information aria-label="Heading self-link"></a></h3><ul><li><strong><code>gardenadm token</code> Pull Request:</strong> <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/28-autonomous-shoot-clusters.md>GEP-28</a> <code>gardenadm token</code> (<a href=https://github.com/gardener/gardener/pull/11934>#11934</a>)</li><li><strong><code>gardenadm join</code> Pull Request:</strong> <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/28-autonomous-shoot-clusters.md>GEP-28</a> <code>gardenadm join</code> (<a href=https://github.com/gardener/gardener/pull/11942>#11942</a>)</li><li><strong>Recording of the demo:</strong> Watch the demo starting at <a href="https://youtu.be/ssvXpPliOY0?t=768">12m48s</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3d3c662dbe65fcd8da97232e9214b1e0>Enhanced Network Flexibility: Gardener Now Supports CIDR Overlap for Non-HA Shoots</h1><div class="td-byline mb-4"><time datetime=2025-05-19 class=text-body-secondary>Monday, May 19, 2025</time></div><p>Gardener is continually evolving to offer greater flexibility and efficiency in managing Kubernetes clusters. A significant enhancement has been introduced that addresses a common networking challenge: the requirement for completely disjoint network CIDR blocks between a shoot cluster and its seed cluster. Now, Gardener allows for IPv4 network overlap in specific scenarios, providing users with more latitude in their network planning.</p><h3 id=addressing-ip-address-constraints>Addressing IP Address Constraints<a class=td-heading-self-link href=#addressing-ip-address-constraints aria-label="Heading self-link"></a></h3><p>Previously, all shoot cluster networks (pods, services, nodes) had to be distinct from the seed cluster&rsquo;s networks. This could be challenging in environments with limited IP address space or complex network topologies. With this new feature, IPv4 or dual-stack shoot clusters can now define pod, service, and node networks that overlap with the IPv4 networks of their seed cluster.</p><h3 id=how-it-works-nat-for-seamless-connectivity>How It Works: NAT for Seamless Connectivity<a class=td-heading-self-link href=#how-it-works-nat-for-seamless-connectivity aria-label="Heading self-link"></a></h3><p>This capability is enabled through a double Network Address Translation (NAT) mechanism within the VPN connection established between the shoot and seed clusters. When IPv4 network overlap is configured, Gardener intelligently maps the overlapping shoot and seed networks to a dedicated set of newly reserved IPv4 ranges. These ranges are used exclusively within the VPN pods to ensure seamless communication, effectively resolving any conflicts that would arise from the overlapping IPs.</p><p>The reserved mapping ranges are:</p><ul><li><code>241.0.0.0/8</code>: Seed Pod Mapping Range</li><li><code>242.0.0.0/8</code>: Shoot Node Mapping Range</li><li><code>243.0.0.0/8</code>: Shoot Service Mapping Range</li><li><code>244.0.0.0/8</code>: Shoot Pod Mapping Range</li></ul><h3 id=conditions-for-utilizing-overlapping-networks>Conditions for Utilizing Overlapping Networks<a class=td-heading-self-link href=#conditions-for-utilizing-overlapping-networks aria-label="Heading self-link"></a></h3><p>To leverage this new network flexibility, the following conditions must be met:</p><ol><li><strong>Non-Highly-Available VPN:</strong> The shoot cluster must utilize a non-highly-available (non-HA) VPN. This is typically the configuration for shoots with a non-HA control plane.</li><li><strong>IPv4 or Dual-Stack Shoots:</strong> The shoot cluster must be configured as either single-stack IPv4 or dual-stack (IPv4/IPv6). The overlap feature specifically pertains to IPv4 networks.</li><li><strong>Non-Use of Reserved Ranges:</strong> The shoot cluster&rsquo;s own defined networks (for pods, services, and nodes) must not utilize any of the Gardener-reserved IP ranges, including the newly introduced mapping ranges listed above, or the existing <code>240.0.0.0/8</code> range (Kube-ApiServer Mapping Range).</li></ol><p>It&rsquo;s important to note that Gardener will prevent the migration of a non-HA shoot to an HA setup if its network ranges currently overlap with the seed, as this feature is presently limited to non-HA VPN configurations. For single-stack IPv6 shoots, Gardener continues to enforce non-overlapping IPv6 networks to avoid any potential issues, although IPv6 address space exhaustion is less common.</p><h3 id=benefits-for-gardener-users>Benefits for Gardener Users<a class=td-heading-self-link href=#benefits-for-gardener-users aria-label="Heading self-link"></a></h3><p>This enhancement offers increased flexibility in IP address management, particularly beneficial for users operating numerous shoot clusters or those in environments with constrained IPv4 address availability. By relaxing the strict disjointedness requirement for non-HA shoots, Gardener simplifies network allocation and reduces the operational overhead associated with IP address planning.</p><h3 id=explore-further>Explore Further<a class=td-heading-self-link href=#explore-further aria-label="Heading self-link"></a></h3><p>To dive deeper into this feature, you can review the original pull request and the updated documentation:</p><ul><li><strong>GitHub PR:</strong> <a href=https://github.com/gardener/gardener/pull/11582>feat: Allow CIDR overlap for non-HA VPN shoots (#11582)</a></li><li><strong>Gardener Documentation:</strong> <a href=/docs/gardener/networking/shoot_networking/#overlapping-ipv4-networks-between-seed-and-shoot>Shoot Networking</a></li><li><strong>Developer Talk Recording:</strong> <a href="https://youtu.be/ZwurVm1IJ7o?t=0">Gardener Development - Sprint Review #131</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0ccde31ce45ecbf3c0a91d5eed0a85e4>Enhanced Node Management: Introducing In-Place Updates in Gardener</h1><div class="td-byline mb-4"><time datetime=2025-05-19 class=text-body-secondary>Monday, May 19, 2025</time></div><p>Gardener is committed to providing efficient and flexible Kubernetes cluster management. Traditionally, updates to worker pool configurations, such as machine image or Kubernetes minor version changes, trigger a rolling update. This process involves replacing existing nodes with new ones, which is a robust approach for many scenarios. However, for environments with physical or bare-metal nodes, or stateful workloads sensitive to node replacement, or if the virtual machine type is scarce, this can introduce challenges like extended update times and potential disruptions.</p><p>To address these needs, Gardener now introduces <strong>In-Place Node Updates</strong>. This new capability allows certain updates to be applied directly to existing worker nodes without requiring their replacement, significantly reducing disruption and speeding up update processes for compatible changes.</p><h3 id=new-update-strategies-for-worker-pools>New Update Strategies for Worker Pools<a class=td-heading-self-link href=#new-update-strategies-for-worker-pools aria-label="Heading self-link"></a></h3><p>Gardener now supports three distinct update strategies for your worker pools, configurable via the <code>updateStrategy</code> field in the <code>Shoot</code> specification&rsquo;s worker pool definition:</p><ul><li><strong><code>AutoRollingUpdate</code></strong>: This is the classic and default strategy. When updates occur, nodes are cordoned, drained, terminated, and replaced with new nodes incorporating the changes.</li><li><strong><code>AutoInPlaceUpdate</code></strong>: With this strategy, compatible updates are applied directly to the existing nodes. The MachineControllerManager (MCM) automatically selects nodes, cordons and drains them, and then signals the Gardener Node Agent (GNA) to perform the update. Once GNA confirms success, MCM uncordons the node.</li><li><strong><code>ManualInPlaceUpdate</code></strong>: This strategy also applies updates directly to existing nodes but gives operators fine-grained control. After an update is specified, MCM marks all nodes in the pool as candidates. Operators must then manually label individual nodes to select them for the in-place update process, which then proceeds similarly to the <code>AutoInPlaceUpdate</code> strategy.</li></ul><p>The <code>AutoInPlaceUpdate</code> and <code>ManualInPlaceUpdate</code> strategies are available when the <code>InPlaceNodeUpdates</code> feature gate is enabled in the <code>gardener-apiserver</code>.</p><h3 id=what-can-be-updated-in-place>What Can Be Updated In-Place?<a class=td-heading-self-link href=#what-can-be-updated-in-place aria-label="Heading self-link"></a></h3><p>In-place updates are designed to handle a variety of common operational tasks more efficiently:</p><ul><li><strong>Machine Image Updates</strong>: Newer versions of a machine image can be rolled out by executing an update command directly on the node, provided the image and cloud profile are configured to support this.</li><li><strong>Kubernetes Minor Version Updates</strong>: Updates to the Kubernetes minor version of worker nodes can be applied in-place.</li><li><strong>Kubelet Configuration Changes</strong>: Modifications to the Kubelet configuration can be applied directly.</li><li><strong>Credentials Rotation</strong>: Critical for security, rotation of Certificate Authorities (CAs) and ServiceAccount signing keys can now be performed on existing nodes without replacement.</li></ul><p>However, some changes still necessitate a rolling update (node replacement):</p><ul><li>Changing the machine image name (e.g., switching from Ubuntu to Garden Linux).</li><li>Modifying the machine type.</li><li>Altering volume types or sizes.</li><li>Changing the Container Runtime Interface (CRI) name (e.g., from Docker to containerd).</li><li>Enabling or disabling node-local DNS.</li></ul><h3 id=key-api-and-component-adaptations>Key API and Component Adaptations<a class=td-heading-self-link href=#key-api-and-component-adaptations aria-label="Heading self-link"></a></h3><p>Several Gardener components and APIs have been enhanced to support in-place updates:</p><ul><li><strong>CloudProfile</strong>: The <code>CloudProfile</code> API now allows specifying <code>inPlaceUpdates</code> configuration within <code>machineImage.versions</code>. This includes a boolean <code>supported</code> field to indicate if a version supports in-place updates and an optional <code>minVersionForUpdate</code> string to define the minimum OS version from which an in-place update to the current version is permissible.</li><li><strong>Shoot Specification</strong>: As mentioned, the <code>spec.provider.workers[].updateStrategy</code> field allows selection of the desired update strategy. Additionally, <code>spec.provider.workers[].machineControllerManagerSettings</code> now includes <code>machineInPlaceUpdateTimeout</code> and <code>disableHealthTimeout</code> (which defaults to <code>true</code> for in-place strategies to prevent premature machine deletion during lengthy updates). For <code>ManualInPlaceUpdate</code>, <code>maxSurge</code> defaults to <code>0</code> and <code>maxUnavailable</code> to <code>1</code>.</li><li><strong>OperatingSystemConfig (OSC)</strong>: The OSC resource, managed by OS extensions, now includes <code>status.inPlaceUpdates.osUpdate</code> where extensions can specify the <code>command</code> and <code>args</code> for the Gardener Node Agent to execute for machine image (Operating System) updates. The <code>spec.inPlaceUpdates</code> field in the OSC will carry information like the target Operating System version, Kubelet version, and credential rotation status to the node.</li><li><strong>Gardener Node Agent (GNA)</strong>: GNA is responsible for executing the in-place updates on the node. It watches for a specific node condition ( <code>InPlaceUpdate</code> with reason <code>ReadyForUpdate</code>) set by MCM, performs the OS update, Kubelet updates, or credentials rotation, restarts necessary pods (like DaemonSets), and then labels the node with the update outcome.</li><li><strong>MachineControllerManager (MCM)</strong>: MCM orchestrates the in-place update process. For in-place strategies, while new machine classes and machine sets are created to reflect the desired state, the actual machine objects are not deleted and recreated. Instead, their ownership is transferred to the new machine set. MCM handles cordoning, draining, and setting node conditions to coordinate with GNA.</li><li><strong>Shoot Status & Constraints</strong>: To provide visibility, the <code>status.inPlaceUpdates.pendingWorkerUpdates</code> field in the <code>Shoot</code> now lists worker pools pending <code>autoInPlaceUpdate</code> or <code>manualInPlaceUpdate</code>. A new <code>ShootManualInPlaceWorkersUpdated</code> constraint is added if any manual in-place updates are pending, ensuring users are aware.</li><li><strong>Worker Status</strong>: The <code>Worker</code> extension resource now includes <code>status.inPlaceUpdates.workerPoolToHashMap</code> to track the configuration hash of worker pools that have undergone in-place updates. This helps Gardener determine if a pool is up-to-date.</li><li><strong>Forcing Updates</strong>: If an in-place update is stuck, the <code>gardener.cloud/operation=force-in-place-update</code> annotation can be added to the Shoot to allow subsequent changes or retries.</li></ul><h3 id=benefits-of-in-place-updates>Benefits of In-Place Updates<a class=td-heading-self-link href=#benefits-of-in-place-updates aria-label="Heading self-link"></a></h3><ul><li><strong>Reduced Disruption</strong>: Minimizes workload interruptions by avoiding full node replacements for compatible updates.</li><li><strong>Faster Updates</strong>: Applying changes directly can be quicker than provisioning new nodes, especially for OS patches or configuration changes.</li><li><strong>Bare-Metal Efficiency</strong>: Particularly beneficial for bare-metal environments where node provisioning is more time-consuming and complex.</li><li><strong>Stateful Workload Friendly</strong>: Lessens the impact on stateful applications that might be sensitive to node churn.</li></ul><p>In-place node updates represent a significant step forward in Gardener&rsquo;s operational flexibility, offering a more nuanced and efficient approach to managing node lifecycles, especially in demanding or specialized environments.</p><h3 id=dive-deeper>Dive Deeper<a class=td-heading-self-link href=#dive-deeper aria-label="Heading self-link"></a></h3><p>To explore the technical details and contributions that made this feature possible, refer to the following resources:</p><ul><li><strong>Parent Issue for &ldquo;[GEP-31] Support for In-Place Node Updates&rdquo;</strong>: <a href=https://github.com/gardener/gardener/issues/10219>Issue #10219</a></li><li><strong>GEP-31: In-Place Node Updates of Shoot Clusters</strong>: <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/31-inplace-node-update.md>GEP-31: In-Place Node Updates of Shoot Clusters</a></li><li><strong>Developer Talk Recording (starting at 39m37s)</strong>: <a href="https://youtu.be/ZwurVm1IJ7o?t=2377">Youtube</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d120589c63a3e999d70ebea3e76f10bf>Gardener Dashboard 1.80: Streamlined Credentials, Enhanced Cluster Views, and Real-Time Updates</h1><div class="td-byline mb-4"><time datetime=2025-05-19 class=text-body-secondary>Monday, May 19, 2025</time></div><p>Gardener Dashboard version 1.80 introduces several significant enhancements aimed at improving user experience, credentials management, and overall operational efficiency. These updates bring more clarity to credential handling, a smoother experience for managing large numbers of clusters, and a move towards a more reactive interface.</p><h3 id=unified-and-enhanced-credentials-management>Unified and Enhanced Credentials Management<a class=td-heading-self-link href=#unified-and-enhanced-credentials-management aria-label="Heading self-link"></a></h3><p>The management of secrets and credentials has been significantly revamped for better clarity and functionality:</p><ul><li><strong>Introducing CredentialsBindings:</strong> The dashboard now fully supports <code>CredentialsBinding</code> resources alongside the existing <code>SecretBinding</code> resources. This allows for referencing both Secrets and, in the future, Workload Identities more explicitly. While <code>CredentialsBindings</code> referencing Workload Identity resources are visible for cluster creation, editing or deleting them via the dashboard is not yet supported.</li><li><strong>&ldquo;Credentials&rdquo; Page:</strong> The former &ldquo;Secrets&rdquo; page has been renamed to &ldquo;Credentials.&rdquo; It features a new &ldquo;Kind&rdquo; column and distinct icons to clearly differentiate between <code>SecretBinding</code> and <code>CredentialsBinding</code> types, especially useful when resources share names. The column showing the referenced credential resource name has been removed as this information is part of the binding&rsquo;s details.</li><li><strong>Contextual Information and Safeguards:</strong> When editing a secret, all its associated data is now displayed, providing better context. If an underlying secret is referenced by multiple bindings, a hint is shown to prevent unintended impacts. Deletion of a binding is prevented if the underlying secret is still in use by another binding.</li><li><strong>Simplified Creation and Editing:</strong> New secrets created via the dashboard will now automatically generate a <code>CredentialsBinding</code>. While existing <code>SecretBindings</code> remain updatable, the creation of new <code>SecretBindings</code> through the dashboard is no longer supported, encouraging the adoption of the more versatile <code>CredentialsBinding</code>. The edit dialog for secrets now pre-fills current data, allowing for easier modification of specific fields.</li><li><strong>Handling Missing Secrets:</strong> The UI now provides clear information and guidance if a <code>CredentialsBinding</code> or <code>SecretBinding</code> references a secret that no longer exists.</li></ul><h3 id=revamped-cluster-list-for-improved-scalability>Revamped Cluster List for Improved Scalability<a class=td-heading-self-link href=#revamped-cluster-list-for-improved-scalability aria-label="Heading self-link"></a></h3><p>Navigating and managing a large number of clusters is now more efficient:</p><ul><li><strong>Virtual Scrolling:</strong> The cluster list has adopted virtual scrolling. Rows are rendered dynamically as you scroll, replacing the previous pagination system. This significantly improves performance and provides a smoother browsing experience, especially for environments with hundreds or thousands of clusters.</li><li><strong>Optimized Row Display:</strong> The height of individual rows in the cluster list has been reduced, allowing more clusters to be visible on the screen at once. Additionally, expandable content within a row (like worker details or ticket labels) now has a maximum height with internal scrolling, ensuring consistent row sizes and smooth virtual scrolling performance.</li></ul><h3 id=real-time-updates-for-projects>Real-Time Updates for Projects<a class=td-heading-self-link href=#real-time-updates-for-projects aria-label="Heading self-link"></a></h3><p>The dashboard is becoming more dynamic with the introduction of real-time updates:</p><ul><li><strong>Instant Project Changes:</strong> Modifications to projects, such as creation or deletion, are now reflected instantly in the project list and interface without requiring a page reload. This is achieved through WebSocket communication.</li><li><strong>Foundation for Future Reactivity:</strong> This enhancement for projects lays the groundwork for bringing real-time updates to other resources within the dashboard, such as Seeds and the Garden resource, in future releases.</li></ul><h3 id=other-notable-enhancements>Other Notable Enhancements<a class=td-heading-self-link href=#other-notable-enhancements aria-label="Heading self-link"></a></h3><ul><li><strong>Kubeconfig Update:</strong> The kubeconfig generated for garden cluster access via the &ldquo;Account&rdquo; page now uses the <code>--oidc-pkce-method</code> flag, replacing the deprecated <code>--oidc-use-pkce</code> flag. Users encountering deprecation messages should redownload their kubeconfig.</li><li><strong>Notification Behavior:</strong> Kubernetes warning notifications are now automatically dismissed after 5 seconds. However, all notifications will remain visible as long as the mouse cursor is hovering over them, giving users more time to read important messages.</li><li><strong>API Server URL Path:</strong> Support has been added for kubeconfigs that include a path in the API server URL.</li></ul><p>These updates in Gardener Dashboard 1.80 collectively enhance usability, provide better control over credentials, and improve performance for large-scale operations.</p><p>For a comprehensive list of all features, bug fixes, and contributor acknowledgments, please refer to the <a href=https://github.com/gardener/dashboard/releases/tag/1.80.0>official release notes</a>.
You can also view the segment of the community call discussing these dashboard updates <a href="https://youtu.be/ZwurVm1IJ7o?t=1793">here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bf04bed69dc8e0a4cc4625a9d8e34906>Gardener: Powering Enterprise Kubernetes at Scale and Europe's Sovereign Cloud Future</h1><div class="td-byline mb-4"><time datetime=2025-05-12 class=text-body-secondary>Monday, May 12, 2025</time></div><p>The Kubernetes ecosystem is dynamic, offering a wealth of tools to manage the complexities of modern cloud-native applications. For enterprises seeking to provision and manage Kubernetes clusters efficiently, securely, and at scale, a robust and comprehensive solution is paramount. Gardener, born from years of managing tens of thousands of clusters efficiently across diverse platforms and in demanding environments, stands out as a fully open-source choice for delivering fully managed Kubernetes Clusters as a Service. It already empowers organizations like SAP, STACKIT, T-Systems, and others (see <a href=https://gardener.cloud/adopter>adopters</a>) and has become a core technology for <a href=https://neonephos.org/projects>NeoNephos</a>, a project aimed at advancing digital autonomy in Europe (see <a href="https://www.youtube.com/watch?v=85MDID9Ju04&amp;t=621s">KubeCon London 2025 Keynote</a> and <a href=https://neonephos.org/2025/03/31/the-linux-foundation-announces-the-launch-of-neonephos-to-advance-digital-autonomy-in-europe>press announcement</a>).</p><h3 id=the-gardener-approach-an-architecture-forged-by-experience>The Gardener Approach: An Architecture Forged by Experience<a class=td-heading-self-link href=#the-gardener-approach-an-architecture-forged-by-experience aria-label="Heading self-link"></a></h3><p>At the heart of Gardener&rsquo;s architecture is the concept of &ldquo;Kubeception&rdquo; (see <a href="https://github.com/gardener/gardener?tab=readme-ov-file#gardener">readme</a> and <a href=/docs/gardener/concepts/architecture/>architecture</a>). This approach involves using Kubernetes to manage Kubernetes. Gardener runs on a Kubernetes cluster (called a <strong>runtime cluster</strong>), facilitates access through a self-managed node-less Kubernetes cluster (the <strong>garden cluster</strong>), manages Kubernetes control planes as pods within other self-managed Kubernetes clusters that provide high scalability (called <strong>seed clusters</strong>), and ultimately provisions end-user Kubernetes clusters (called <strong>shoot clusters</strong>).</p><p>This multi-layered architecture isn&rsquo;t complexity for its own sake. Gardener&rsquo;s design and extensive feature set are the product of over eight years of continuous development and refinement, directly shaped by the high-scale, security-sensitive, and enterprise-grade requirements of its users. Experience has shown that such a sophisticated structure is key to addressing significant challenges in scalability, security, and operational manageability. For instance:</p><ul><li><strong>Scalability:</strong> Gardener achieves considerable scalability through its use of <strong>seed clusters</strong>, which it also manages. This allows for the distribution of control planes, preventing bottlenecks. The design even envisions leveraging Gardener to host its own management components (as an <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/28-autonomous-shoot-clusters.md><strong>autonomous cluster</strong></a>), showcasing its resilience without risking circular dependencies.</li><li><strong>Security:</strong> A fundamental principle in Gardener is the strict isolation of control planes from data planes. This extends to Gardener itself, which runs in a dedicated management cluster but exposes its API to end-users through a workerless virtual cluster. This workerless cluster acts as an isolated access point, presenting no compute surface for potentially malicious pods, thereby significantly enhancing security.</li><li><strong>API Power & User Experience:</strong> Gardener utilizes the full capabilities of the Kubernetes API server. This enables advanced functionalities and sophisticated API change management. Crucially, for the end-user, interaction remains 100% Kubernetes-native. Users employ standard custom resources to instruct Gardener, meaning any tool, library, or language binding that supports Kubernetes CRDs inherently supports Gardener.</li></ul><h3 id=delivering-fully-managed-kubernetes-clusters-as-a-service>Delivering Fully Managed Kubernetes Clusters as a Service<a class=td-heading-self-link href=#delivering-fully-managed-kubernetes-clusters-as-a-service aria-label="Heading self-link"></a></h3><p>Gardener provides a comprehensive &ldquo;fully managed Kubernetes Clusters as a Service&rdquo; offering. This means it handles much more than just spinning up a cluster; it manages the entire lifecycle and operational aspects. Here’s a glimpse into its capabilities:</p><ol><li><p><strong>Full Cluster Lifecycle Management:</strong></p><ul><li><strong>Infrastructure Provisioning:</strong> Gardener takes on the provisioning and management of underlying cloud infrastructure, including VPCs, subnets, NAT gateways, security groups, IAM roles, and virtual machines across a wide range of providers like AWS, Azure, GCP, OpenStack, and more.</li><li><strong>Worker Node Management:</strong> It meticulously manages worker pools, covering OS images, machine types, autoscaling configurations (min/max/surge), update strategies, volume management, CRI configuration, and provider-specific settings.</li></ul></li><li><p><strong>Enterprise Platform Governance:</strong></p><ul><li><strong>Cloud Profiles:</strong> Gardener is designed with the comprehensive needs of enterprise platform operators in mind. Managing a fleet of clusters for an organization requires more than just provisioning; it demands clear governance over available resources, versions, and their lifecycle. Gardener addresses this through its declarative API, allowing platform administrators to define and enforce policies such as which Kubernetes versions are &ldquo;supported,&rdquo; &ldquo;preview,&rdquo; or &ldquo;deprecated,&rdquo; along with their expiration dates. Similarly, it allows control over available machine images, their versions, and lifecycle status. This level of granular control and lifecycle management for the underlying components of a Kubernetes service is crucial for enterprise adoption and stable operations. This is a key consideration often left as an additional implementation burden for platform teams using other cluster provisioning tools, where such governance features must be built on top. Gardener, by contrast, integrates these concerns directly into its API and operational model, simplifying the task for platform operators.</li></ul></li><li><p><strong>Advanced Networking:</strong></p><ul><li><strong>CNI Plugin Management:</strong> Gardener manages the deployment and configuration of CNI plugins such as Calico or Cilium.</li><li><strong>Dual-Stack Networking:</strong> It offers comprehensive support for IPv4, IPv6, and dual-stack configurations for pods, services, and nodes.</li><li><strong>NodeLocal DNS Cache:</strong> To enhance DNS performance and reliability, Gardener can deploy and manage NodeLocal DNS.</li></ul></li><li><p><strong>Comprehensive Autoscaling:</strong></p><ul><li><strong>Cluster Autoscaler:</strong> Gardener manages the Cluster Autoscaler for worker nodes, enabling dynamic scaling based on pod scheduling demands.</li><li><strong>Horizontal and Vertical Pod Autoscaler (VPA):</strong> It manages HPA/VPA for workloads and applies it to control plane components, optimizing resource utilization (see <a href=https://gardener.cloud/blog/2025/04-17-leaner-clusters-lower-bills>blog</a>).</li></ul></li><li><p><strong>Operational Excellence & Maintenance:</strong></p><ul><li><strong>Automated Kubernetes Upgrades:</strong> Gardener handles automated Kubernetes version upgrades for both control plane and worker nodes, with configurable maintenance windows.</li><li><strong>Automated OS Image Updates:</strong> It manages automated machine image updates for worker nodes.</li><li><strong>Cluster Hibernation:</strong> To optimize costs, Gardener supports hibernating clusters, scaling down components during inactivity.</li><li><strong>Scheduled Maintenance:</strong> It allows defining specific maintenance windows for predictability.</li><li><strong>Robust Credentials Rotation:</strong> Gardener features automated mechanisms for rotating <strong>all</strong> credentials. It provisions fine-grained, dedicated, and individual CAs, certificates, credentials, and secrets for each component — whether Kubernetes-related (such as service account keys or etcd encryption keys) or Gardener-specific (such as opt-in SSH keys or observability credentials). The Gardener installation, the seeds, and all shoots have their own distinct sets of credentials — amounting to more than 150 per shoot cluster control plane and hundreds of thousands for larger Gardener installations overall. All these credentials are rotated automatically and without downtime — most continuously, while some (like the API server CA) require user initiation to ensure operational awareness. For a deeper dive into Gardener&rsquo;s credential rotation, see our <a href="https://www.youtube.com/watch?v=3V8oFQ16mTg&amp;t=29s">Cloud Native Rejekts talk</a>). This granular approach effectively prevents lateral movement, significantly strengthening the security posture.</li></ul></li><li><p><strong>Enhanced Security & Access Control:</strong></p><ul><li><strong>OIDC Integration:</strong> Gardener supports OIDC configuration for the <code>kube-apiserver</code> for secure user authentication.</li><li><strong>Customizable Audit Policies:</strong> It allows specifying custom audit policies for detailed logging.</li><li><strong>Managed Service Account Issuers:</strong> Gardener can manage service account issuers, enhancing workload identity security.</li><li><strong>SSH Access Control:</strong> It provides mechanisms to manage SSH access to worker nodes securely if opted in (Gardener itself doesn&rsquo;t require SSH access to worker nodes).</li><li><strong>Workload Identity:</strong> Gardener supports workload identity features, allowing pods to securely authenticate to cloud provider services.</li></ul></li><li><p><strong>Powerful Extensibility:</strong></p><ul><li><strong>Extension Framework and Ecosystem:</strong> Gardener features a robust extension mechanism for deep integration of cloud providers, operating systems, container runtimes, or services like DNS management, certificate management, registry caches, network filtering, image signature verification, and more.</li><li><strong>Catered to Platform Builders:</strong> This extensibility also allows platform builders to deploy custom extensions into the self-managed seed cluster infrastructure that hosts shoot cluster control planes. This offers robust isolation for these custom components from the user&rsquo;s shoot cluster worker nodes, enhancing both security and operational stability.</li></ul></li><li><p><strong>Integrated DNS and Certificate Management:</strong></p><ul><li><strong>External DNS Management:</strong> Gardener can manage DNS records for the cluster&rsquo;s API server and services via its <code>shoot-dns-service</code> extension.</li><li><strong>Automated Certificate Management:</strong> Through extensions like <code>shoot-cert-service</code>, it manages TLS certificates, including ACME integration. Gardener also provides its own robust DNS (<code>dns-management</code>) and certificate (<code>cert-management</code>) solutions designed for enterprise scale. These custom solutions were developed because, at the scale Gardener operates, many deep optimizations were necessary, e.g., to avoid being rate-limited by upstream providers.</li></ul></li></ol><h3 id=a-kubernetes-native-foundation-for-sovereign-cloud>A Kubernetes-Native Foundation for Sovereign Cloud<a class=td-heading-self-link href=#a-kubernetes-native-foundation-for-sovereign-cloud aria-label="Heading self-link"></a></h3><p>The modern IT landscape is rapidly evolving away from primitive virtual machines towards distributed systems. Kubernetes has emerged as the de facto standard for deploying and managing these modern, cloud-native applications and services at scale. Gardener is squarely positioned at the forefront of this shift, offering a Kubernetes-native approach to managing Kubernetes clusters themselves. It possesses a mature, declarative, Kubernetes-native API for full cluster lifecycle management. Unlike services that might expose proprietary APIs, Gardener’s approach is inherently Kubernetes-native and multi-cloud. This unified API is comprehensive, offering a consistent way to manage diverse cluster landscapes.</p><p>Its nature as a fully open-source project is particularly relevant for initiatives like NeoNephos, which aim to build sovereign cloud solutions. All core features, stable releases, and essential operational components are available to the community. This inherent cloud-native, Kubernetes-centric design, coupled with its open-source nature and ability to run on diverse infrastructures (including on-premise and local cloud providers), provides the transparency, control, and technological independence crucial for digital sovereignty. Gardener delivers full sovereign control <em>today</em>, enabling organizations to run all modern applications and services at scale with complete authority over their infrastructure and data. This is a significant reason why many cloud providers and enterprises that champion sovereignty are choosing Gardener as their foundation and actively contributing to its ecosystem.</p><h3 id=operational-depth-reflecting-real-world-scale>Operational Depth Reflecting Real-World Scale<a class=td-heading-self-link href=#operational-depth-reflecting-real-world-scale aria-label="Heading self-link"></a></h3><p>Gardener&rsquo;s operational maturity is a direct reflection of its long evolution, shaped by the demands of enterprise users and real-world, large-scale deployments. This maturity translates into statistical evidence and track records of uptime for end-users and their critical services. For instance, Gardener includes fully automated, incremental etcd backups with a recovery point objective (RPO) of five minutes and supports autonomous, hands-off restoration workflows via <code>etcd-druid</code>. Features like Vertical Pod Autoscalers (VPAs), PodDisruptionBudgets (PDBs), NetworkPolicies, PriorityClasses, and sophisticated pod placement strategies are integral to Gardener&rsquo;s offering, ensuring high availability and fault tolerance. Gardener&rsquo;s automation deals with many of the usual exceptions and does not require human DevOps intervention for most operational tasks. Gardener&rsquo;s commitment to robust security is evident in <a href=https://gardener.cloud/blog/2021/09.12-navigating-cloud-native-security/#gardeners-proactive-security-posture>Gardener&rsquo;s proactive security posture</a>, which has proven effective in real-world scenarios. This depth of experience and automation ultimately translates into first-class Service Level Agreements (SLAs) that businesses can trust and rely on. As a testament to this, SAP entrusts Gardener with its Systems of Record. This level of operational excellence enables Gardener to meet the expectations of today’s most demanding Kubernetes use cases.</p><h3 id=conclusion-a-solid-foundation-for-your-kubernetes-strategy>Conclusion: A Solid Foundation for Your Kubernetes Strategy<a class=td-heading-self-link href=#conclusion-a-solid-foundation-for-your-kubernetes-strategy aria-label="Heading self-link"></a></h3><p>For enterprises and organizations seeking a comprehensive, truly open-source solution for managing the full lifecycle of Kubernetes clusters at scale, Gardener offers a compelling proposition. Its mature architecture, rich feature set, operational robustness, built-in enterprise governance capabilities, and commitment to the open-source community provide a solid foundation for running demanding Kubernetes workloads with confidence. This makes it a suitable technical underpinning for ambitious projects like NeoNephos, contributing to a future of greater digital autonomy.</p><p>We invite you to explore <a href=https://gardener.cloud/>Gardener</a> and discover how it can empower your enterprise-grade and -scale Kubernetes journey.</p></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://demo.gardener.cloud>Demo</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://gardener-cloud.slack.com/><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://x.com/GardenerProject><img src=/images/branding/x-logo-white.svg class=media-icon><div class=media-text>X</div></a></li></ul><span class=copyright>Copyright 2019-2025 Gardener project authors.
<a href=https://www.sap.com/about/legal/terms-of-use.html>Terms of Use
<i class="fa fa-external-link" aria-hidden=true></i>
</a>|
<a href=https://www.sap.com/about/legal/terms-of-use.html>Privacy Statement
<i class="fa fa-external-link" aria-hidden=true></i>
</a>|
<a href=https://www.sap.com/about/legal/terms-of-use.html>Legal Disclosure
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=/js/main.min.403ff095218c662472dab60ed98ecbb19431682de5ac7c6159891241cd366af5.js integrity="sha256-QD/wlSGMZiRy2rYO2Y7LsZQxaC3lrHxhWYkSQc02avU=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script><script src=/js/navbar.js></script><script src=/js/filtering.js></script><script src=/js/page-content.js></script><script src=/js/community-index.js></script></body></html>