<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/gardener/concepts/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/gardener/concepts/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Concepts | Gardener</title><meta name=description content><meta property="og:title" content="Concepts"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/gardener/concepts/"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Concepts"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Concepts"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.52b703b92d167c14e82f904cd88f9dbe92798d607a8949235304e48c7cd0a116.css as=style><link href=/scss/main.min.52b703b92d167c14e82f904cd88f9dbe92798d607a8949235304e48c7cd0a116.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.2288972a9944b3e6f2a6760e38d1839e.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/gardener/concepts/>Return to the regular view of this page</a>.</p></div><h1 class=title>Concepts</h1><div class=content></div></div><div class=td-content><h1 id=pg-330a9a0a66325841218788d3193e6fdf>1 - APIServer Admission Plugins</h1><div class=lead>A list of all gardener managed admission plugins together with their responsibilities</div><h2 id=overview>Overview</h2><p>Similar to the kube-apiserver, the gardener-apiserver comes with a few in-tree managed admission plugins.
If you want to get an overview of the what and why of admission plugins then <a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/>this document</a> might be a good start.</p><p>This document lists all existing admission plugins with a short explanation of what it is responsible for.</p><h2 id=clusteropenidconnectpreset-openidconnectpreset><code>ClusterOpenIDConnectPreset</code>, <code>OpenIDConnectPreset</code></h2><p><em>(both enabled by default)</em></p><p>These admission controllers react on <code>CREATE</code> operations for <code>Shoot</code>s.
If the <code>Shoot</code> does not specify any OIDC configuration (<code>.spec.kubernetes.kubeAPIServer.oidcConfig=nil</code>), then it tries to find a matching <code>ClusterOpenIDConnectPreset</code> or <code>OpenIDConnectPreset</code>, respectively.
If there are multiple matches, then the one with the highest weight &ldquo;wins&rdquo;.
In this case, the admission controller will default the OIDC configuration in the <code>Shoot</code>.</p><h2 id=controllerregistrationresources><code>ControllerRegistrationResources</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>ControllerRegistration</code>s.
It validates that there exists only one <code>ControllerRegistration</code> in the system that is primarily responsible for a given kind/type resource combination.
This prevents misconfiguration by the Gardener administrator/operator.</p><h2 id=customverbauthorizer><code>CustomVerbAuthorizer</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>Project</code>s.
It validates whether the user is bound to a RBAC role with the <code>modify-spec-tolerations-whitelist</code> verb in case the user tries to change the <code>.spec.tolerations.whitelist</code> field of the respective <code>Project</code> resource.
Usually, regular project members are not bound to this custom verb, allowing the Gardener administrator to manage certain toleration whitelists on <code>Project</code> basis.</p><h2 id=deletionconfirmation><code>DeletionConfirmation</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>DELETE</code> operations for <code>Project</code>s and <code>Shoot</code>s and <code>ShootState</code>s.
It validates that the respective resource is annotated with a deletion confirmation annotation, namely <code>confirmation.gardener.cloud/deletion=true</code>.
Only if this annotation is present it allows the <code>DELETE</code> operation to pass.
This prevents users from accidental/undesired deletions.</p><h2 id=exposureclass><code>ExposureClass</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>Create</code> operations for <code>Shoot</code>s.
It mutates <code>Shoot</code> resources which have an <code>ExposureClass</code> referenced by merging both their <code>shootSelectors</code> and/or <code>tolerations</code> into the <code>Shoot</code> resource.</p><h2 id=extensionvalidator><code>ExtensionValidator</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>BackupEntry</code>s, <code>BackupBucket</code>s, <code>Seed</code>s, and <code>Shoot</code>s.
For all the various extension types in the specifications of these objects, it validates whether there exists a <code>ControllerRegistration</code> in the system that is primarily responsible for the stated extension type(s).
This prevents misconfigurations that would otherwise allow users to create such resources with extension types that don&rsquo;t exist in the cluster, effectively leading to failing reconciliation loops.</p><h2 id=extensionlabels><code>ExtensionLabels</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>BackupBucket</code>s, <code>BackupEntry</code>s, <code>CloudProfile</code>s, <code>Seed</code>s, <code>SecretBinding</code>s and <code>Shoot</code>s. For all the various extension types in the specifications of these objects, it adds a corresponding label in the resource. This would allow extension admission webhooks to filter out the resources they are responsible for and ignore all others. This label is of the form <code>&lt;extension-type>.extensions.gardener.cloud/&lt;extension-name> : "true"</code>. For example, an extension label for provider extension type <code>aws</code>, looks like <code>provider.extensions.gardener.cloud/aws : "true"</code>.</p><h2 id=projectvalidator><code>ProjectValidator</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> operations for <code>Project</code>s.
It prevents creating <code>Project</code>s with a non-empty <code>.spec.namespace</code> if the value in <code>.spec.namespace</code> does not start with <code>garden-</code>.</p><p>⚠️ This admission plugin will be removed in a future release and its business logic will be incorporated into the static validation of the <code>gardener-apiserver</code>.</p><h2 id=resourcequota><code>ResourceQuota</code></h2><p><em>(enabled by default)</em></p><p>This admission controller enables <a href=https://kubernetes.io/docs/concepts/policy/resource-quotas/#object-count-quota>object count ResourceQuotas</a> for Gardener resources, e.g. <code>Shoots</code>, <code>SecretBindings</code>, <code>Projects</code>, etc.</p><blockquote><p>⚠️ In addition to this admission plugin, the <a href=https://github.com/kubernetes/kubernetes/blob/release-1.2/docs/design/admission_control_resource_quota.md#resource-quota-controller>ResourceQuota controller</a> must be enabled for the Kube-Controller-Manager of your Garden cluster.</p></blockquote><h2 id=resourcereferencemanager><code>ResourceReferenceManager</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>CloudProfile</code>s, <code>Project</code>s, <code>SecretBinding</code>s, <code>Seed</code>s, and <code>Shoot</code>s.
Generally, it checks whether referred resources stated in the specifications of these objects exist in the system (e.g., if a referenced <code>Secret</code> exists).
However, it also has some special behaviours for certain resources:</p><ul><li><code>CloudProfile</code>s: It rejects removing Kubernetes or machine image versions if there is at least one <code>Shoot</code> that refers to them.</li><li><code>Project</code>s: It sets the <code>.spec.createdBy</code> field for newly created <code>Project</code> resources, and defaults the <code>.spec.owner</code> field in case it is empty (to the same value of <code>.spec.createdBy</code>).</li><li><code>Shoot</code>s: It sets the <code>gardener.cloud/created-by=&lt;username></code> annotation for newly created <code>Shoot</code> resources.</li></ul><h2 id=seedvalidator><code>SeedValidator</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>DELETE</code> operations for <code>Seed</code>s.
Rejects the deletion if <code>Shoot</code>(s) reference the seed cluster.</p><h2 id=shootdns><code>ShootDNS</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>Shoot</code>s.
It tries to assign a default domain to the <code>Shoot</code>.
It also validates the DNS configuration (<code>.spec.dns</code>) for shoots.</p><h2 id=shootnodelocaldnsenabledbydefault><code>ShootNodeLocalDNSEnabledByDefault</code></h2><p><em>(disabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> operations for <code>Shoot</code>s.
If enabled, it will enable node local dns within the shoot cluster (for more information, see <a href=/docs/gardener/node-local-dns/>NodeLocalDNS Configuration</a>) by setting <code>spec.systemComponents.nodeLocalDNS.enabled=true</code> for newly created Shoots.
Already existing Shoots and new Shoots that explicitly disable node local dns (<code>spec.systemComponents.nodeLocalDNS.enabled=false</code>)
will not be affected by this admission plugin.</p><h2 id=shootquotavalidator><code>ShootQuotaValidator</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>Shoot</code>s.
It validates the resource consumption declared in the specification against applicable <code>Quota</code> resources.
Only if the applicable <code>Quota</code> resources admit the configured resources in the <code>Shoot</code> then it allows the request.
Applicable <code>Quota</code>s are referred in the <code>SecretBinding</code> that is used by the <code>Shoot</code>.</p><h2 id=shootresourcereservation><code>ShootResourceReservation</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>Shoot</code>s.
It injects the <code>Kubernetes.Kubelet.KubeReserved</code> setting for kubelet either as global setting for a shoot or on a per worker pool basis.
If the admission configuration (see <a href=https://github.com/gardener/gardener/blob/master/example/20-admissionconfig.yaml>this example</a>) for the <code>ShootResourceReservation</code> plugin contains <code>useGKEFormula: false</code> (the default), then it sets a static default resource reservation for the shoot.</p><p>If <code>useGKEFormula: true</code> is set, then the plugin injects resource reservations based on the machine type similar to GKE&rsquo;s <a href=https://cloud.google.com/kubernetes-engine/docs/concepts/plan-node-sizes#resource_reservations>formula for resource reservation</a> into each worker pool.
Already existing resource reservations are not modified; this also means that resource reservations are not automatically updated if the machine type for a worker pool is changed.
If a shoot contains global resource reservations, then no per worker pool resource reservations are injected.</p><h2 id=shootvpaenabledbydefault><code>ShootVPAEnabledByDefault</code></h2><p><em>(disabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> operations for <code>Shoot</code>s.
If enabled, it will enable the managed <code>VerticalPodAutoscaler</code> components (for more information, see <a href=/docs/gardener/shoot_autoscaling/#vertical-pod-auto-scaling>Vertical Pod Auto-Scaling</a>)
by setting <code>spec.kubernetes.verticalPodAutoscaler.enabled=true</code> for newly created Shoots.
Already existing Shoots and new Shoots that explicitly disable VPA (<code>spec.kubernetes.verticalPodAutoscaler.enabled=false</code>)
will not be affected by this admission plugin.</p><h2 id=shoottolerationrestriction><code>ShootTolerationRestriction</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>Shoot</code>s.
It validates the <code>.spec.tolerations</code> used in <code>Shoot</code>s against the whitelist of its <code>Project</code>, or against the whitelist configured in the admission controller&rsquo;s configuration, respectively.
Additionally, it defaults the <code>.spec.tolerations</code> in <code>Shoot</code>s with those configured in its <code>Project</code>, and those configured in the admission controller&rsquo;s configuration, respectively.</p><h2 id=shootvalidator><code>ShootValidator</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code>, <code>UPDATE</code> and <code>DELETE</code> operations for <code>Shoot</code>s.
It validates certain configurations in the specification against the referred <code>CloudProfile</code> (e.g., machine images, machine types, used Kubernetes version, &mldr;).
Generally, it performs validations that cannot be handled by the static API validation due to their dynamic nature (e.g., when something needs to be checked against referred resources).
Additionally, it takes over certain defaulting tasks (e.g., default machine image for worker pools, default Kubernetes version).</p><h2 id=shootmanagedseed><code>ShootManagedSeed</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>UPDATE</code> and <code>DELETE</code> operations for <code>Shoot</code>s.
It validates certain configuration values in the specification that are specific to <code>ManagedSeed</code>s (e.g. the nginx-addon of the Shoot has to be disabled, the Shoot VPA has to be enabled).
It rejects the deletion if the <code>Shoot</code> is referred to by a <code>ManagedSeed</code>.</p><h2 id=managedseedvalidator><code>ManagedSeedValidator</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>ManagedSeeds</code>s.
It validates certain configuration values in the specification against the referred <code>Shoot</code>, for example Seed provider, network ranges, DNS domain, etc.
Similar to <code>ShootValidator</code>, it performs validations that cannot be handled by the static API validation due to their dynamic nature.
Additionally, it performs certain defaulting tasks, making sure that configuration values that are not specified are defaulted to the values of the referred <code>Shoot</code>, for example Seed provider, network ranges, DNS domain, etc.</p><h2 id=managedseedshoot><code>ManagedSeedShoot</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>DELETE</code> operations for <code>ManagedSeed</code>s.
It rejects the deletion if there are <code>Shoot</code>s that are scheduled onto the <code>Seed</code> that is registered by the <code>ManagedSeed</code>.</p><h2 id=shootdnsrewriting><code>ShootDNSRewriting</code></h2><p><em>(disabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> operations for <code>Shoot</code>s.
If enabled, it adds a set of common suffixes configured in its admission plugin configuration to the <code>Shoot</code> (<code>spec.systemComponents.coreDNS.rewriting.commonSuffixes</code>) (for more information, see <a href=/docs/gardener/dns-search-path-optimization/>DNS Search Path Optimization</a>).
Already existing <code>Shoot</code>s will not be affected by this admission plugin.</p><h2 id=namespacedcloudprofilevalidator><code>NamespacedCloudProfileValidator</code></h2><p><em>(enabled by default)</em></p><p>This admission controller reacts on <code>CREATE</code> and <code>UPDATE</code> operations for <code>NamespacedCloudProfile</code>s.
It primarily validates if the referenced parent <code>CloudProfile</code> exists in the system. In addition, the admission controller ensures that the <code>NamespacedCloudProfile</code> only configures new machine types, and does not overwrite those from the parent <code>CloudProfile</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c898670f6aa58e28fb8510640734a620>2 - Architecture</h1><div class=lead>The concepts behind the Gardener architecture</div><h2 id=official-definition---what-is-kubernetes>Official Definition - What is Kubernetes?</h2><blockquote><p>&ldquo;Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.&rdquo;</p></blockquote><h2 id=introduction---basic-principle>Introduction - Basic Principle</h2><p>The foundation of the Gardener (providing <strong>Kubernetes Clusters as a Service</strong>) is Kubernetes itself, because Kubernetes is the go-to solution to manage software in the Cloud, even when it&rsquo;s Kubernetes itself (see also OpenStack which is provisioned more and more on top of Kubernetes as well).</p><p>While self-hosting, meaning to run Kubernetes components inside Kubernetes, is a popular topic in the community, we apply a special pattern catering to the needs of our cloud platform to provision hundreds or even thousands of clusters. We take a so-called &ldquo;seed&rdquo; cluster and seed the control plane (such as the API server, scheduler, controllers, etcd persistence and others) of an end-user cluster, which we call &ldquo;shoot&rdquo; cluster, as pods into the &ldquo;seed&rdquo; cluster. That means that one &ldquo;seed&rdquo; cluster, of which we will have one per IaaS and region, hosts the control planes of multiple &ldquo;shoot&rdquo; clusters. That allows us to avoid dedicated hardware/virtual machines for the &ldquo;shoot&rdquo; cluster control planes. We simply put the control plane into pods/containers and since the &ldquo;seed&rdquo; cluster watches them, they can be deployed with a replica count of 1 and only need to be scaled out when the control plane gets under pressure, but no longer for HA reasons. At the same time, the deployments get simpler (standard Kubernetes deployment) and easier to update (standard Kubernetes rolling update). The actual &ldquo;shoot&rdquo; cluster consists only of the worker nodes (no control plane) and therefore the users may get full administrative access to their clusters.</p><h2 id=setting-the-scene---components-and-procedure>Setting The Scene - Components and Procedure</h2><p>We provide a central operator UI, which we call the &ldquo;Gardener Dashboard&rdquo;. It talks to a dedicated cluster, which we call the &ldquo;Garden&rdquo; cluster, and uses custom resources managed by an <a href=https://kubernetes.io/docs/concepts/api-extension/custom-resources/#api-server-aggregation>aggregated API server</a> (one of the general extension concepts of Kubernetes) to represent &ldquo;shoot&rdquo; clusters. In this &ldquo;Garden&rdquo; cluster runs the &ldquo;Gardener&rdquo;, which is basically a Kubernetes controller that watches the custom resources and acts upon them, i.e. creates, updates/modifies, or deletes &ldquo;shoot&rdquo; clusters. The creation follows basically these steps:</p><ul><li>Create a namespace in the &ldquo;seed&rdquo; cluster for the &ldquo;shoot&rdquo; cluster, which will host the &ldquo;shoot&rdquo; cluster control plane.</li><li>Generate secrets and credentials, which the worker nodes will need to talk to the control plane.</li><li>Create the infrastructure (using <a href=https://www.terraform.io/>Terraform</a>), which basically consists out of the network setup.</li><li>Deploy the &ldquo;shoot&rdquo; cluster control plane into the &ldquo;shoot&rdquo; namespace in the &ldquo;seed&rdquo; cluster, containing the &ldquo;machine-controller-manager&rdquo; pod.</li><li>Create machine CRDs in the &ldquo;seed&rdquo; cluster, describing the configuration and the number of worker machines for the &ldquo;shoot&rdquo; (the machine-controller-manager watches the CRDs and creates virtual machines out of it).</li><li>Wait for the &ldquo;shoot&rdquo; cluster API server to become responsive (pods will be scheduled, persistent volumes and load balancers are created by Kubernetes via the respective cloud provider).</li><li>Finally, we deploy <code>kube-system</code> daemons like <code>kube-proxy</code> and further add-ons like the <code>dashboard</code> into the &ldquo;shoot&rdquo; cluster and the cluster becomes active.</li></ul><h2 id=overview-architecture-diagram>Overview Architecture Diagram</h2><p><img src=/__resources/gardener-architecture-overview_e55e9a.png alt="Gardener Overview Architecture Diagram"></p><h2 id=detailed-architecture-diagram>Detailed Architecture Diagram</h2><p><img src=/__resources/gardener-architecture-detailed_25a94d.png alt="Gardener Detailed Architecture Diagram"></p><p>Note: The <code>kubelet</code>, as well as the pods inside the &ldquo;shoot&rdquo; cluster, talks through the front-door (load balancer IP; public Internet) to its &ldquo;shoot&rdquo; cluster API server running in the &ldquo;seed&rdquo; cluster. The reverse communication from the API server to the pod, service, and node networks happens through a VPN connection that we deploy into the &ldquo;seed&rdquo; and &ldquo;shoot&rdquo; clusters.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-350220592a34dc9c03bc21617e258ada>3 - Backup and Restore</h1><div class=lead>Understand the etcd backup and restore capabilities of Gardener</div><h2 id=overview>Overview</h2><p>Kubernetes uses <a href=https://etcd.io/>etcd</a> as the key-value store for its resource definitions. Gardener supports the backup and restore of etcd. It is the responsibility of the shoot owners to backup the workload data.</p><p>Gardener uses an <a href=https://github.com/gardener/etcd-backup-restore>etcd-backup-restore</a> component to backup the etcd backing the Shoot cluster regularly and restore it in case of disaster. It is deployed as sidecar via <a href=https://github.com/gardener/etcd-druid>etcd-druid</a>. This doc mainly focuses on the backup and restore configuration used by Gardener when deploying these components. For more details on the design and internal implementation details, please refer to <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/06-etcd-druid.md>GEP-06</a> and the documentation on individual repositories.</p><h2 id=bucket-provisioning>Bucket Provisioning</h2><p>Refer to the <a href=/docs/gardener/extensions/backupbucket/>backup bucket extension document</a> to find out details about configuring the backup bucket.</p><h2 id=backup-policy>Backup Policy</h2><p>etcd-backup-restore supports full snapshot and delta snapshots over full snapshot. In Gardener, this configuration is currently hard-coded to the following parameters:</p><ul><li>Full Snapshot schedule:<ul><li>Daily, <code>24hr</code> interval.</li><li>For each Shoot, the schedule time in a day is randomized based on the configured Shoot maintenance window.</li></ul></li><li>Delta Snapshot schedule:<ul><li>At <code>5min</code> interval.</li><li>If aggregated events size since last snapshot goes beyond <code>100Mib</code>.</li></ul></li><li>Backup History / Garbage backup deletion policy:<ul><li>Gardener configures backup restore to have <code>Exponential</code> garbage collection policy.</li><li>As per policy, the following backups are retained:<ul><li>All full backups and delta backups for the previous hour.</li><li>Latest full snapshot of each previous hour for the day.</li><li>Latest full snapshot of each previous day for 7 days.</li><li>Latest full snapshot of the previous 4 weeks.</li></ul></li><li>Garbage Collection is configured at <code>12hr</code> interval.</li></ul></li><li>Listing:<ul><li>Gardener doesn&rsquo;t have any API to list out the backups.</li><li>To find the backups list, an admin can checkout the <code>BackupEntry</code> resource associated with the Shoot which holds the bucket and prefix details on the object store.</li></ul></li></ul><h2 id=restoration>Restoration</h2><p>The restoration process of etcd is automated through the etcd-backup-restore component from the latest snapshot. Gardener doesn&rsquo;t support Point-In-Time-Recovery (PITR) of etcd. In case of an etcd disaster, the etcd is recovered from the latest backup automatically. For further details, please refer the <a href=https://github.com/gardener/etcd-backup-restore/blob/master/docs/proposals/restoration.md>Restoration</a> topic. Post restoration of etcd, the Shoot reconciliation loop brings the cluster back to its previous state.</p><p>Again, the Shoot owner is responsible for maintaining the backup/restore of his workload. Gardener only takes care of the cluster&rsquo;s etcd.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-deb4b0a712a04f62d9a5892607237729>4 - Cluster API</h1><div class=lead>Understand the evolution of the Gardener API and its relation to the Cluster API</div><h2 id=relation-between-gardener-api-and-cluster-api-sig-cluster-lifecycle>Relation Between Gardener API and Cluster API (SIG Cluster Lifecycle)</h2><p>In essence, the Cluster API harmonizes how to get to clusters, while Gardener goes one step further and also harmonizes the clusters themselves. The Cluster API delegates the specifics to so-called providers for infrastructures or control planes via specific CR(D)s, while Gardener only has one cluster CR(D). Different Cluster API providers, e.g. for AWS, Azure, GCP, etc., give you vastly different Kubernetes clusters. In contrast, Gardener gives you the exact same clusters with the exact same K8s version, operating system, control plane configuration like for API server or kubelet, add-ons like overlay network, HPA/VPA, DNS and certificate controllers, ingress and network policy controllers, control plane monitoring and logging stacks, down to the behavior of update procedures, auto-scaling, self-healing, etc., on all supported infrastructures. These homogeneous clusters are an essential goal for Gardener, as its main purpose is to simplify operations for teams that need to develop and ship software on Kubernetes clusters on a plethora of infrastructures (a.k.a. multi-cloud).</p><p>Incidentally, Gardener influenced the Machine API in the Cluster API with its <a href=https://github.com/gardener/machine-controller-manager>Machine Controller Manager</a> and was the <a href=https://github.com/kubernetes-sigs/cluster-api/commit/00b1ead264aea6f88585559056c180771cce3815>first to adopt it</a>. You can find more information on that in the <a href="https://www.youtube.com/watch?v=Mtg8jygK3Hs">joint SIG Cluster Lifecycle KubeCon talk</a> where @hardikdr from our Gardener team in India spoke.</p><p>That means that we follow the <a href=https://github.com/kubernetes-sigs/cluster-api#cluster-api>Cluster API</a> with great interest and are active members. It was completely overhauled from <code>v1alpha1</code> to <code>v1alpha2</code>. But because <code>v1alpha2</code> made too many assumptions about the bring-up of masters and was enforcing master machine operations (for more information, see <a href=https://cluster-api.sigs.k8s.io/user/concepts.html#control-plane>The Cluster API Book</a>: “As of <code>v1alpha2</code>, Machine-Based is the only control plane type that Cluster API supports”), services that managed their control planes differently like GKE or Gardener couldn&rsquo;t adopt it (e.g. <a href=https://cloud.google.com/anthos/gke/docs/on-prem/concepts/cluster-api>Google only supports <code>v1alpha1</code></a>). In 2020 <a href=https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience/><code>v1alpha3</code></a> was introduced and made it possible (again) to integrate managed services like GKE or Gardener. The mapping from the Gardener API to the Cluster API is mostly syntactic.</p><p>To wrap it up, while the Cluster API knows about clusters, it doesn&rsquo;t know about their make-up. With Gardener, we wanted to go beyond that and harmonize the make-up of the clusters themselves and make them homogeneous across all supported infrastructures. Gardener can therefore deliver homogeneous clusters with exactly the same configuration and behavior on all infrastructures (see also <a href=https://testgrid.k8s.io/conformance-gardener>Gardener&rsquo;s coverage in the official conformance test grid</a>).</p><p>With <a href=https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience>Cluster API <code>v1alpha3</code></a> and the support for declarative control plane management, it has became possible (again) to enable Kubernetes managed services like GKE or Gardener. We would be more than happy if the community would be interested to contribute a Gardener control plane provider.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a7a632a48267b040a5183cbf28440ddf>5 - etcd</h1><div class=lead>How Gardener uses the etcd key-value store</div><h2 id=etcd---key-value-store-for-kubernetes>etcd - Key-Value Store for Kubernetes</h2><p><a href=https://etcd.io/>etcd</a> is a strongly consistent key-value store and the most prevalent choice for the Kubernetes
persistence layer. All API cluster objects like <code>Pod</code>s, <code>Deployment</code>s, <code>Secret</code>s, etc., are stored in <code>etcd</code>, which
makes it an essential part of a <a href=https://kubernetes.io/docs/concepts/overview/components/#control-plane-components>Kubernetes control plane</a>.</p><h2 id=garden-or-shoot-cluster-persistence>Garden or Shoot Cluster Persistence</h2><p>Each garden or shoot cluster gets its very own persistence for the control plane.
It runs in the shoot namespace on the respective seed cluster (or in the <code>garden</code> namespace in the garden cluster, respectively).
Concretely, there are two etcd instances per shoot cluster, which the <code>kube-apiserver</code> is configured to use in the following way:</p><ul><li><code>etcd-main</code></li></ul><p>A store that contains all &ldquo;cluster critical&rdquo; or &ldquo;long-term&rdquo; objects.
These object kinds are typically considered for a backup to prevent any data loss.</p><ul><li><code>etcd-events</code></li></ul><p>A store that contains all <code>Event</code> objects (<code>events.k8s.io</code>) of a cluster.
<code>Events</code> usually have a short retention period and occur frequently, but are not essential for a disaster recovery.</p><p>The setup above prevents both, the critical <code>etcd-main</code> is not flooded by Kubernetes <code>Events</code>, as well as backup space is not occupied by non-critical data.
This separation saves time and resources.</p><h2 id=etcd-operator>etcd Operator</h2><p>Configuring, maintaining, and health-checking etcd is outsourced to a dedicated operator called <a href=https://github.com/gardener/etcd-druid/>etcd Druid</a>.
When a <a href=/docs/gardener/concepts/gardenlet/><code>gardenlet</code></a> reconciles a <code>Shoot</code> resource or a <a href=/docs/gardener/concepts/operator/><code>gardener-operator</code></a> reconciles a <code>Garden</code> resource, they manage an <a href=https://github.com/gardener/etcd-druid/blob/1d427e9167adac1476d1847c0e265c2c09d6bc62/config/samples/druid_v1alpha1_etcd.yaml><code>Etcd</code></a> resource in the seed or garden cluster, containing necessary information (backup information, defragmentation schedule, resources, etc.).
<code>etcd-druid</code> needs to manage the lifecycle of the desired etcd instance (today <code>main</code> or <code>events</code>).
Likewise, when the <code>Shoot</code> or <code>Garden</code> is deleted, <code>gardenlet</code> or <code>gardener-operator</code> deletes the <code>Etcd</code> resources and <a href=https://github.com/gardener/etcd-druid/>etcd Druid</a> takes care of cleaning up all related objects, e.g. the backing <code>StatefulSet</code>s.</p><h2 id=autoscaling>Autoscaling</h2><p>Gardenlet maintains <a href=https://github.com/gardener/hvpa-controller/blob/master/config/samples/autoscaling_v1alpha1_hvpa.yaml><code>HVPA</code></a> objects for etcd <code>StatefulSet</code>s if the corresponding <a href=/docs/gardener/deployment/feature_gates/>feature gate</a> is enabled.
This enables a vertical scaling for etcd.
Downscaling is handled more pessimistically to prevent many subsequent etcd restarts.
Thus, for <code>production</code> and <code>infrastructure</code> shoot clusters (or all garden clusters), downscaling is deactivated for the main etcd.
For all other shoot clusters, lower advertised requests/limits are only applied during a shoot&rsquo;s maintenance time window.</p><h2 id=backup>Backup</h2><p>If <code>Seed</code>s specify backups for etcd (<a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>example</a>), then Gardener and the respective <a href=/docs/gardener/extensions/>provider extensions</a> are responsible for creating a bucket on the cloud provider&rsquo;s side (modelled through a <a href=/docs/gardener/extensions/backupbucket/>BackupBucket resource</a>).
The bucket stores backups of <code>Shoot</code>s scheduled on that <code>Seed</code>.
Furthermore, Gardener creates a <a href=/docs/gardener/extensions/backupentry/>BackupEntry</a>, which subdivides the bucket and thus makes it possible to store backups of multiple shoot clusters.</p><p>How long backups are stored in the bucket after a shoot has been deleted depends on the configured <em>retention period</em> in the <code>Seed</code> resource.
Please see this <a href=https://github.com/gardener/gardener/blob/849cd857d0d20e5dde26b9740ca2814603a56dfd/example/20-componentconfig-gardenlet.yaml#L20>example configuration</a> for more information.</p><p>For <code>Garden</code>s specifying backups for etcd (<a href=https://github.com/gardener/gardener/blob/master/example/operator/20-garden.yaml>example</a>), the bucket must be pre-created externally and provided via the <code>Garden</code> specification.</p><p>Both etcd instances are configured to run with a special backup-restore <em>sidecar</em>.
It takes care about regularly backing up etcd data and restoring it in case of data loss (in the main etcd only).
The sidecar also performs defragmentation and other house-keeping tasks.
More information can be found in the <a href=https://github.com/gardener/etcd-backup-restore>component&rsquo;s GitHub repository</a>.</p><h2 id=housekeeping>Housekeeping</h2><p><a href=https://etcd.io/docs/v3.3/op-guide/maintenance/>etcd maintenance tasks</a> must be performed from time to time in order to re-gain database storage and to ensure the system&rsquo;s reliability.
The <a href=https://github.com/gardener/etcd-backup-restore>backup-restore</a> <em>sidecar</em> takes care about this job as well.</p><p>For both <code>Shoot</code>s and <code>Garden</code>s, a random time <strong>within the shoot&rsquo;s maintenance time</strong> is chosen for scheduling these tasks.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ccbcc1039e04b807c6daf64b9352f3d3>6 - Gardener Admission Controller</h1><div class=lead>Functions and list of handlers for the Gardener Admission Controller</div><h2 id=overview>Overview</h2><p>While the Gardener API server works with <a href=/docs/gardener/concepts/apiserver_admission_plugins/>admission plugins</a> to validate and mutate resources belonging to Gardener related API groups, e.g. <code>core.gardener.cloud</code>, the same is needed for resources belonging to non-Gardener API groups as well, e.g. secrets in the <code>core</code> API group.
Therefore, the Gardener Admission Controller runs a http(s) server with the following handlers which serve as validating/mutating endpoints for <a href=https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/>admission webhooks</a>.
It is also used to serve http(s) handlers for authorization webhooks.</p><h2 id=admission-webhook-handlers>Admission Webhook Handlers</h2><p>This section describes the admission webhook handlers that are currently served.</p><h3 id=admission-plugin-secret-validator>Admission Plugin Secret Validator</h3><p>In <code>Shoot</code>, <code>AdmissionPlugin</code> can have reference to other files. This validation handler validates the referred admission plugin secret and ensures that the secret always contains the required data <code>kubeconfig</code>.</p><h3 id=kubeconfig-secret-validator>Kubeconfig Secret Validator</h3><p><a href=https://github.com/kubernetes/kubectl/issues/697>Malicious Kubeconfigs</a> applied by end users may cause a leakage of sensitive data.
This handler checks if the incoming request contains a Kubernetes secret with a <code>.data.kubeconfig</code> field and denies the request if the Kubeconfig structure violates Gardener&rsquo;s security standards.</p><h3 id=namespace-validator>Namespace Validator</h3><p>Namespaces are the backing entities of Gardener projects in which shoot cluster objects reside.
This validation handler protects active namespaces against premature deletion requests.
Therefore, it denies deletion requests if a namespace still contains shoot clusters or if it belongs to a non-deleting Gardener project (without <code>.metadata.deletionTimestamp</code>).</p><h3 id=resource-size-validator>Resource Size Validator</h3><p>Since users directly apply Kubernetes native objects to the Garden cluster, it also involves the risk of being vulnerable to DoS attacks because these resources are continuously watched and read by controllers.
One example is the creation of shoot resources with large annotation values (up to 256 kB per value), which can cause severe out-of-memory issues for the gardenlet component.
<a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>Vertical autoscaling</a> can help to mitigate such situations, but we cannot expect to scale infinitely, and thus need means to block the attack itself.</p><p>The Resource Size Validator checks arbitrary incoming admission requests against a configured maximum size for the resource&rsquo;s group-version-kind combination. It denies the request if the object exceeds the quota.</p><p>Example for Gardener Admission Controller configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>server:
</span></span><span style=display:flex><span>  resourceAdmissionConfiguration:
</span></span><span style=display:flex><span>    limits:
</span></span><span style=display:flex><span>    - apiGroups: [<span style=color:#a31515>&#34;core.gardener.cloud&#34;</span>]
</span></span><span style=display:flex><span>      apiVersions: [<span style=color:#a31515>&#34;*&#34;</span>]
</span></span><span style=display:flex><span>      resources: [<span style=color:#a31515>&#34;shoots&#34;</span>]
</span></span><span style=display:flex><span>      size: 100k
</span></span><span style=display:flex><span>    - apiGroups: [<span style=color:#a31515>&#34;&#34;</span>]
</span></span><span style=display:flex><span>      apiVersions: [<span style=color:#a31515>&#34;v1&#34;</span>]
</span></span><span style=display:flex><span>      resources: [<span style=color:#a31515>&#34;secrets&#34;</span>]
</span></span><span style=display:flex><span>      size: 100k
</span></span><span style=display:flex><span>    unrestrictedSubjects:
</span></span><span style=display:flex><span>    - kind: Group
</span></span><span style=display:flex><span>      name: gardener.cloud:system:seeds
</span></span><span style=display:flex><span>      apiGroup: rbac.authorization.k8s.io
</span></span><span style=display:flex><span> <span style=color:green>#  - kind: User</span>
</span></span><span style=display:flex><span> <span style=color:green>#    name: admin</span>
</span></span><span style=display:flex><span> <span style=color:green>#    apiGroup: rbac.authorization.k8s.io</span>
</span></span><span style=display:flex><span> <span style=color:green>#  - kind: ServiceAccount</span>
</span></span><span style=display:flex><span> <span style=color:green>#    name: &#34;*&#34;</span>
</span></span><span style=display:flex><span> <span style=color:green>#    namespace: garden</span>
</span></span><span style=display:flex><span> <span style=color:green>#    apiGroup: &#34;&#34;</span>
</span></span><span style=display:flex><span>    operationMode: block <span style=color:green>#log</span>
</span></span></code></pre></div><p>With the configuration above, the Resource Size Validator denies requests for shoots with Gardener&rsquo;s core API group which exceed a size of 100 kB. The same is done for Kubernetes secrets.</p><p>As this feature is meant to protect the system from malicious requests sent by users, it is recommended to exclude trusted groups, users or service accounts from the size restriction via <code>resourceAdmissionConfiguration.unrestrictedSubjects</code>.
For example, the backing user for the gardenlet should always be capable of changing the shoot resource instead of being blocked due to size restrictions.
This is because the gardenlet itself occasionally changes the shoot specification, labels or annotations, and might violate the quota if the existing resource is already close to the quota boundary.
Also, operators are supposed to be trusted users and subjecting them to a size limitation can inhibit important operational tasks.
Wildcard ("*") in subject <code>name</code> is supported.</p><p>Size limitations depend on the individual Gardener setup and choosing the wrong values can affect the availability of your Gardener service.
<code>resourceAdmissionConfiguration.operationMode</code> allows to control if a violating request is actually denied (default) or only logged.
It&rsquo;s recommended to start with <code>log</code>, check the logs for exceeding requests, adjust the limits if necessary and finally switch to <code>block</code>.</p><h3 id=seedrestriction>SeedRestriction</h3><p>Please refer to <a href=/docs/gardener/deployment/gardenlet_api_access/>Scoped API Access for Gardenlets</a> for more information.</p><h2 id=authorization-webhook-handlers>Authorization Webhook Handlers</h2><p>This section describes the authorization webhook handlers that are currently served.</p><h3 id=seedauthorization>SeedAuthorization</h3><p>Please refer to <a href=/docs/gardener/deployment/gardenlet_api_access/>Scoped API Access for Gardenlets</a> for more information.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-467f70291b55149caa507ac09cae563f>7 - Gardener API Server</h1><div class=lead>Understand the Gardener API server extension and the resources it exposes</div><h2 id=overview>Overview</h2><p>The Gardener API server is a Kubernetes-native extension based on its <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/>aggregation layer</a>.
It is registered via an <code>APIService</code> object and designed to run inside a Kubernetes cluster whose API it wants to extend.</p><p>After registration, it exposes the following resources:</p><h2 id=cloudprofiles><code>CloudProfile</code>s</h2><p><code>CloudProfile</code>s are resources that describe a specific environment of an underlying infrastructure provider, e.g. AWS, Azure, etc.
Each shoot has to reference a <code>CloudProfile</code> to declare the environment it should be created in.
In a <code>CloudProfile</code>, the gardener operator specifies certain constraints like available machine types, regions, which Kubernetes versions they want to offer, etc.
End-users can read <code>CloudProfile</code>s to see these values, but only operators can change the content or create/delete them.
When a shoot is created or updated, then an admission plugin checks that only allowed values are used via the referenced <code>CloudProfile</code>.</p><p>Additionally, a <code>CloudProfile</code> may contain a <code>providerConfig</code>, which is a special configuration dedicated for the infrastructure provider.
Gardener does not evaluate or understand this config, but extension controllers might need it for declaration of provider-specific constraints, or global settings.</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml>this</a> example manifest and consult the documentation of your provider extension controller to get information about its <code>providerConfig</code>.</p><h3 id=namespacedcloudprofiles><code>NamespacedCloudProfile</code>s</h3><p>In addition to <code>CloudProfile</code>s, <code>NamespacedCloudProfile</code>s exist to enable project level <code>CloudProfile</code>s.
Please view <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/25-namespaced-cloud-profiles.md>GEP-25</a> for additional information.
This feature is currently under development and not ready for productive use.
At the moment, only the necessary APIs and validations exist to allow for extensions to adapt to the new <code>NamespacedCloudProfile</code> resource.</p><h2 id=internalsecrets><code>InternalSecret</code>s</h2><p>End-users can read and/or write <code>Secret</code>s in their project namespaces in the garden cluster. This prevents Gardener components from storing such &ldquo;Gardener-internal&rdquo; secrets in the respective project namespace.
<code>InternalSecret</code>s are resources that contain shoot or project-related secrets that are &ldquo;Gardener-internal&rdquo;, i.e., secrets used and managed by the system that end-users don&rsquo;t have access to.
<code>InternalSecret</code>s are defined like plain Kubernetes <code>Secret</code>s, behave exactly like them, and can be used in the same manners. The only difference is, that the <code>InternalSecret</code> resource is a dedicated API resource (exposed by gardener-apiserver).
This allows separating access to &ldquo;normal&rdquo; secrets and internal secrets by the usual RBAC means.</p><p>Gardener uses an <code>InternalSecret</code> per Shoot for syncing the client CA to the project namespace in the garden cluster (named <code>&lt;shoot-name>.ca-client</code>). The <a href=/docs/gardener/shoot_access/#shootsadminkubeconfig-subresource><code>shoots/adminkubeconfig</code> subresource</a> signs short-lived client certificates by retrieving the CA from the <code>InternalSecret</code>.</p><p>Operators should configure <code>gardener-apiserver</code> to encrypt the <code>internalsecrets.core.gardener.cloud</code> resource in etcd.</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/11-internal-secret.yaml>this</a> example manifest.</p><h2 id=seeds><code>Seed</code>s</h2><p><code>Seed</code>s are resources that represent seed clusters.
Gardener does not care about how a seed cluster got created - the only requirement is that it is of at least Kubernetes v1.25 and passes the Kubernetes conformance tests.
The Gardener operator has to either deploy the gardenlet into the cluster they want to use as seed (recommended, then the gardenlet will create the <code>Seed</code> object itself after bootstrapping) or provide the kubeconfig to the cluster inside a secret (that is referenced by the <code>Seed</code> resource) and create the <code>Seed</code> resource themselves.</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/45-secret-seed-backup.yaml>this</a>, <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml>this</a>, and optionally <a href=https://github.com/gardener/gardener/blob/master/example/40-secret-seed.yaml>this</a> example manifests.</p><h2 id=shoot-quotas>Shoot <code>Quota</code>s</h2><p>To allow end-users not having their dedicated infrastructure account to try out Gardener, the operator can register an account owned by them that they allow to be used for trial clusters.
Trial clusters can be put under quota so that they don&rsquo;t consume too many resources (resulting in costs) and that one user cannot consume all resources on their own.
These clusters are automatically terminated after a specified time, but end-users may extend the lifetime manually if needed.</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/60-quota.yaml>this</a> example manifest.</p><h2 id=projects><code>Project</code>s</h2><p>The first thing before creating a shoot cluster is to create a <code>Project</code>.
A project is used to group multiple shoot clusters together.
End-users can invite colleagues to the project to enable collaboration, and they can either make them <code>admin</code> or <code>viewer</code>.
After an end-user has created a project, they will get a dedicated namespace in the garden cluster for all their shoots.</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/05-project-dev.yaml>this</a> example manifest.</p><h2 id=secretbindings><code>SecretBinding</code>s</h2><p>Now that the end-user has a namespace the next step is registering their infrastructure provider account.</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/70-secret-provider.yaml>this</a> example manifest and consult the documentation of the extension controller for the respective infrastructure provider to get information about which keys are required in this secret.</p><p>After the secret has been created, the end-user has to create a special <code>SecretBinding</code> resource that binds this secret.
Later, when creating shoot clusters, they will reference such binding.</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/80-secretbinding.yaml>this</a> example manifest.</p><h2 id=shoots><code>Shoot</code>s</h2><p>Shoot cluster contain various settings that influence how end-user Kubernetes clusters will look like in the end.
As Gardener heavily relies on extension controllers for operating system configuration, networking, and infrastructure specifics, the end-user has the possibility (and responsibility) to provide these provider-specific configurations as well.
Such configurations are not evaluated by Gardener (because it doesn&rsquo;t know/understand them), but they are only transported to the respective extension controller.</p><p>⚠️ This means that any configuration issues/mistake on the end-user side that relates to a provider-specific flag or setting cannot be caught during the update request itself but only later during the reconciliation (unless a validator webhook has been registered in the garden cluster by an operator).</p><p>Please see <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>this</a> example manifest and consult the documentation of the provider extension controller to get information about its <code>spec.provider.controlPlaneConfig</code>, <code>.spec.provider.infrastructureConfig</code>, and <code>.spec.provider.workers[].providerConfig</code>.</p><h2 id=clusteropenidconnectpresets><code>(Cluster)OpenIDConnectPreset</code>s</h2><p>Please see <a href=/docs/gardener/openidconnect-presets/>this</a> separate documentation file.</p><h2 id=overview-data-model>Overview Data Model</h2><p><img src=/__resources/gardener-data-model-overview_64f7c1.png alt="Gardener Overview Data Model"></p></div><div class=td-content style=page-break-before:always><h1 id=pg-38b8e4d34b02317602d0c383c3c76cac>8 - Gardener Controller Manager</h1><div class=lead>Understand where the gardener-controller-manager runs and its functionalities</div><h2 id=overview>Overview</h2><p>The <code>gardener-controller-manager</code> (often refered to as &ldquo;GCM&rdquo;) is a component that runs next to the Gardener API server, similar to the Kubernetes Controller Manager.
It runs several controllers that do not require talking to any seed or shoot cluster.
Also, as of today, it exposes an HTTP server that is serving several health check endpoints and metrics.</p><p>This document explains the various functionalities of the <code>gardener-controller-manager</code> and their purpose.</p><h2 id=controllers>Controllers</h2><h3 id=bastion-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerbastion><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/bastion><code>Bastion</code> Controller</a></h3><p><code>Bastion</code> resources have a limited lifetime which can be extended up to a certain amount by performing a heartbeat on them.
The <code>Bastion</code> controller is responsible for deleting expired or rotten <code>Bastion</code>s.</p><ul><li>&ldquo;expired&rdquo; means a <code>Bastion</code> has exceeded its <code>status.expirationTimestamp</code>.</li><li>&ldquo;rotten&rdquo; means a <code>Bastion</code> is older than the configured <code>maxLifetime</code>.</li></ul><p>The <code>maxLifetime</code> defaults to 24 hours and is an option in the <code>BastionControllerConfiguration</code> which is part of <code>gardener-controller-manager</code>s <code>ControllerManagerControllerConfiguration</code>, see <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-controller-manager.yaml>the example config file</a> for details.</p><p>The controller also deletes <code>Bastion</code>s in case the referenced <code>Shoot</code>:</p><ul><li>no longer exists</li><li>is marked for deletion (i.e., have a non-<code>nil</code> <code>.metadata.deletionTimestamp</code>)</li><li>was migrated to another seed (i.e., <code>Shoot.spec.seedName</code> is different than <code>Bastion.spec.seedName</code>).</li></ul><p>The deletion of <code>Bastion</code>s triggers the <code>gardenlet</code> to perform the necessary cleanups in the Seed cluster, so some time can pass between deletion and the <code>Bastion</code> actually disappearing.
Clients like <code>gardenctl</code> are advised to not re-use <code>Bastion</code>s whose deletion timestamp has been set already.</p><p>Refer to <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/15-manage-bastions-and-ssh-key-pair-rotation.md>GEP-15</a> for more information on the lifecycle of
<code>Bastion</code> resources.</p><h3 id=certificatesigningrequest-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollercertificatesigningrequest><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/certificatesigningrequest><code>CertificateSigningRequest</code> Controller</a></h3><p>After the <a href=/docs/gardener/concepts/gardenlet/>gardenlet</a> gets deployed on the Seed cluster, it needs to establish itself as a trusted party to communicate with the Gardener API server. It runs through a bootstrap flow similar to the <a href=https://kubernetes.io/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/>kubelet bootstrap</a> process.</p><p>On startup, the gardenlet uses a <code>kubeconfig</code> with a <a href=https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/>bootstrap token</a> which authenticates it as being part of the <code>system:bootstrappers</code> group. This kubeconfig is used to create a <a href=https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/><code>CertificateSigningRequest</code></a> (CSR) against the Gardener API server.</p><p>The controller in <code>gardener-controller-manager</code> checks whether the <code>CertificateSigningRequest</code> has the expected organisation, common name and usages which the gardenlet would request.</p><p>It only auto-approves the CSR if the client making the request is allowed to &ldquo;create&rdquo; the
<code>certificatesigningrequests/seedclient</code> subresource. Clients with the <code>system:bootstrappers</code> group are bound to the <code>gardener.cloud:system:seed-bootstrapper</code> <code>ClusterRole</code>, hence, they have such privileges. As the bootstrap kubeconfig for the gardenlet contains a bootstrap token which is authenticated as being part of the <a href=https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/clusterrolebinding-seed-bootstrapper.yaml><code>systems:bootstrappers</code> group</a>, its created CSR gets auto-approved.</p><h3 id=cloudprofile-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollercloudprofile><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/cloudprofile><code>CloudProfile</code> Controller</a></h3><p><code>CloudProfile</code>s are essential when it comes to reconciling <code>Shoot</code>s since they contain constraints (like valid machine types, Kubernetes versions, or machine images) and sometimes also some global configuration for the respective environment (typically via provider-specific configuration in <code>.spec.providerConfig</code>).</p><p>Consequently, to ensure that <code>CloudProfile</code>s in-use are always present in the system until the last referring <code>Shoot</code> gets deleted, the controller adds a finalizer which is only released when there is no <code>Shoot</code> referencing the <code>CloudProfile</code> anymore.</p><h3 id=controllerdeployment-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollercontrollerdeployment><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/controllerdeployment><code>ControllerDeployment</code> Controller</a></h3><p>Extensions are registered in the garden cluster via <code>ControllerRegistration</code> and deployment of respective extensions are specified via <code>ControllerDeployment</code>. For more info refer to <a href=/docs/gardener/extensions/controllerregistration/>Registering Extension Controllers</a>.</p><p>This controller ensures that <code>ControllerDeployment</code> in-use always exists until the last <code>ControllerRegistration</code> referencing them gets deleted. The controller adds a finalizer which is only released when there is no <code>ControllerRegistration</code> referencing the <code>ControllerDeployment</code> anymore.</p><h3 id=controllerregistration-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollercontrollerregistration><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/controllerregistration><code>ControllerRegistration</code> Controller</a></h3><p>The <code>ControllerRegistration</code> controller makes sure that the required <a href=/docs/gardener/#extensions>Gardener Extensions</a> specified by the <a href=/docs/gardener/extensions/controllerregistration/><code>ControllerRegistration</code></a> resources are present in the seed clusters.
It also takes care of the creation and deletion of <code>ControllerInstallation</code> objects for a given seed cluster.
The controller has three reconciliation loops.</p><h4 id=main-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollercontrollerregistrationseed><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/controllerregistration/seed>&ldquo;Main&rdquo; Reconciler</a></h4><p>This reconciliation loop watches the <code>Seed</code> objects and determines which <code>ControllerRegistration</code>s are required for them and reconciles the corresponding <code>ControllerInstallation</code> resources to reach the determined state.
To begin with, it computes the kind/type combinations of extensions required for the seed.
For this, the controller examines a live list of <code>ControllerRegistration</code>s, <code>ControllerInstallation</code>s, <code>BackupBucket</code>s, <code>BackupEntry</code>s, <code>Shoot</code>s, and <code>Secret</code>s from the garden cluster.
For example, it examines the shoots running on the seed and deducts the kind/type, like <code>Infrastructure/gcp</code>.
The seed (<code>seed.spec.provider.type</code>) and DNS (<code>seed.spec.dns.provider.type</code>) provider types are considered when calculating the list of required <code>ControllerRegistration</code>s, as well.
It also decides whether they should always be deployed based on the <code>.spec.deployment.policy</code>.
For the configuration options, please see this <a href=/docs/gardener/extensions/controllerregistration/#deployment-configuration-options>section</a>.</p><p>Based on these required combinations, each of them are mapped to <code>ControllerRegistration</code> objects and then to their corresponding <code>ControllerInstallation</code> objects (if existing).
The controller then creates or updates the required <code>ControllerInstallation</code> objects for the given seed.
It also deletes every existing <code>ControllerInstallation</code> whose referenced <code>ControllerRegistration</code> is not part of the required list.
For example, if the shoots in the seed are no longer using the DNS provider <code>aws-route53</code>, then the controller proceeds to delete the respective <code>ControllerInstallation</code> object.</p><h4 id=controllerregistration-finalizer-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollercontrollerregistrationcontrollerregistrationfinalizer><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/controllerregistration/controllerregistrationfinalizer>"<code>ControllerRegistration</code> Finalizer" Reconciler</a></h4><p>This reconciliation loop watches the <code>ControllerRegistration</code> resource and adds finalizers to it when they are created.
In case a deletion request comes in for the resource, i.e., if a <code>.metadata.deletionTimestamp</code> is set, it actively scans for a <code>ControllerInstallation</code> resource using this <code>ControllerRegistration</code>, and decides whether the deletion can be allowed.
In case no related <code>ControllerInstallation</code> is present, it removes the finalizer and marks it for deletion.</p><h4 id=seed-finalizer-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollercontrollerregistrationseedfinalizer><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/controllerregistration/seedfinalizer>"<code>Seed</code> Finalizer" Reconciler</a></h4><p>This loop also watches the <code>Seed</code> object and adds finalizers to it at creation.
If a <code>.metadata.deletionTimestamp</code> is set for the seed, then the controller checks for existing <code>ControllerInstallation</code> objects which reference this seed.
If no such objects exist, then it removes the finalizer and allows the deletion.</p><h4 id=extension-clusterrole-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollercontrollerregistrationextensionclusterrole><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/controllerregistration/extensionclusterrole>&ldquo;Extension <code>ClusterRole</code>&rdquo; Reconciler</a></h4><p>This reconciler watches two resources in the garden cluster:</p><ul><li><code>ClusterRole</code>s labelled with <code>authorization.gardener.cloud/custom-extensions-permissions=true</code></li><li><code>ServiceAccount</code>s in seed namespaces matching the selector provided via the <code>authorization.gardener.cloud/extensions-serviceaccount-selector</code> annotation of such <code>ClusterRole</code>s.</li></ul><p>Its core task is to maintain a <code>ClusterRoleBinding</code> resource referencing the respective <code>ClusterRole</code>.
This gets bound to all <code>ServiceAccount</code>s in seed namespaces whose labels match the selector provided via the <code>authorization.gardener.cloud/extensions-serviceaccount-selector</code> annotation of such <code>ClusterRole</code>s.</p><p>You can read more about the purpose of this reconciler in <a href=/docs/gardener/extensions/garden-api-access/#additional-permissions>this document</a>.</p><h3 id=event-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerevent><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/event><code>Event</code> Controller</a></h3><p>With the Gardener Event Controller, you can prolong the lifespan of events related to Shoot clusters.
This is an optional controller which will become active once you provide the below mentioned configuration.</p><p>All events in K8s are deleted after a configurable time-to-live (controlled via a <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/>kube-apiserver argument</a> called <code>--event-ttl</code> (defaulting to 1 hour)).
The need to prolong the time-to-live for Shoot cluster events frequently arises when debugging customer issues on live systems.
This controller leaves events involving Shoots untouched, while deleting all other events after a configured time.
In order to activate it, provide the following configuration:</p><ul><li><code>concurrentSyncs</code>: The amount of goroutines scheduled for reconciling events.</li><li><code>ttlNonShootEvents</code>: When an event reaches this time-to-live it gets deleted unless it is a Shoot-related event (defaults to <code>1h</code>, equivalent to the <code>event-ttl</code> default).</li></ul><blockquote><p>⚠️ In addition, you should also configure the <code>--event-ttl</code> for the kube-apiserver to define an upper-limit of how long Shoot-related events should be stored. The <code>--event-ttl</code> should be larger than the <code>ttlNonShootEvents</code> or this controller will have no effect.</p></blockquote><h3 id=exposureclass-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerexposureclass><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/exposureclass><code>ExposureClass</code> Controller</a></h3><p><code>ExposureClass</code> abstracts the ability to expose a Shoot clusters control plane in certain network environments (e.g. corporate networks, DMZ, internet) on all Seeds or a subset of the Seeds. For more information, see <a href=/docs/gardener/exposureclasses/>ExposureClasses</a>.</p><p>Consequently, to ensure that <code>ExposureClass</code>es in-use are always present in the system until the last referring <code>Shoot</code> gets deleted, the controller adds a finalizer which is only released when there is no <code>Shoot</code> referencing the <code>ExposureClass</code> anymore.</p><h3 id=managedseedset-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollermanagedseedset><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/managedseedset><code>ManagedSeedSet</code> Controller</a></h3><p><code>ManagedSeedSet</code> objects maintain a stable set of replicas of <code>ManagedSeed</code>s, i.e. they guarantee the availability of a specified number of identical <code>ManagedSeed</code>s on an equal number of identical <code>Shoot</code>s.
The <code>ManagedSeedSet</code> controller creates and deletes <code>ManagedSeed</code>s and <code>Shoot</code>s in response to changes to the replicas and selector fields. For more information, refer to the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/13-automated-seed-management.md#managedseedsets><code>ManagedSeedSet</code> proposal document</a>.</p><ol><li>The reconciler first gets all the replicas of the given <code>ManagedSeedSet</code> in the <code>ManagedSeedSet</code>&rsquo;s namespace and with the matching selector. Each replica is a struct that contains a <code>ManagedSeed</code>, its corresponding <code>Seed</code> and <code>Shoot</code> objects.</li><li>Then the pending replica is retrieved, if it exists.</li><li>Next it determines the ready, postponed, and deletable replicas.<ul><li>A replica is considered <code>ready</code> when a <code>Seed</code> owned by a <code>ManagedSeed</code> has been registered either directly or by deploying <code>gardenlet</code> into a <code>Shoot</code>, the <code>Seed</code> is <code>Ready</code> and the <code>Shoot</code>&rsquo;s status is <code>Healthy</code>.</li><li>If a replica is not ready and it is not pending, i.e. it is not specified in the <code>ManagedSeed</code>&rsquo;s <code>status.pendingReplica</code> field, then it is added to the <code>postponed</code> replicas.</li><li>A replica is deletable if it has no scheduled <code>Shoot</code>s and the replica&rsquo;s <code>Shoot</code> and <code>ManagedSeed</code> do not have the <code>seedmanagement.gardener.cloud/protect-from-deletion</code> annotation.</li></ul></li><li>Finally, it checks the actual and target replica counts. If the actual count is less than the target count, the controller scales up the replicas by creating new replicas to match the desired target count. If the actual count is more than the target, the controller deletes replicas to match the desired count. Before scale-out or scale-in, the controller first reconciles the pending replica (there can always only be one) and makes sure the replica is ready before moving on to the next one.<ul><li><code>Scale-out</code>(actual count &lt; target count)<ul><li>During the scale-out phase, the controller first creates the <code>Shoot</code> object from the <code>ManagedSeedSet</code>&rsquo;s <code>spec.shootTemplate</code> field and adds the replica to the <code>status.pendingReplica</code> of the <code>ManagedSeedSet</code>.</li><li>For the subsequent reconciliation steps, the controller makes sure that the pending replica is ready before proceeding to the next replica. Once the <code>Shoot</code> is created successfully, the <code>ManagedSeed</code> object is created from the <code>ManagedSeedSet</code>&rsquo;s <code>spec.template</code>. The <code>ManagedSeed</code> object is reconciled by the <code>ManagedSeed</code> controller and a <code>Seed</code> object is created for the replica. Once the replica&rsquo;s <code>Seed</code> becomes ready and the <code>Shoot</code> becomes healthy, the replica also becomes ready.</li></ul></li><li><code>Scale-in</code>(actual count > target count)<ul><li>During the scale-in phase, the controller first determines the replica that can be deleted. From the deletable replicas, it chooses the one with the lowest priority and deletes it. Priority is determined in the following order:<ul><li>First, compare replica statuses. Replicas with &ldquo;less advanced&rdquo; status are considered lower priority. For example, a replica with <code>StatusShootReconciling</code> status has a lower value than a replica with <code>StatusShootReconciled</code> status. Hence, in this case, a replica with a <code>StatusShootReconciling</code> status will have lower priority and will be considered for deletion.</li><li>Then, the replicas are compared with the readiness of their <code>Seed</code>s. Replicas with non-ready <code>Seed</code>s are considered lower priority.</li><li>Then, the replicas are compared with the health statuses of their <code>Shoot</code>s. Replicas with &ldquo;worse&rdquo; statuses are considered lower priority.</li><li>Finally, the replica ordinals are compared. Replicas with lower ordinals are considered lower priority.</li></ul></li></ul></li></ul></li></ol><h3 id=quota-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerquota><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/quota><code>Quota</code> Controller</a></h3><p><code>Quota</code> object limits the resources consumed by shoot clusters either per provider secret or per project/namespace.</p><p>Consequently, to ensure that <code>Quota</code>s in-use are always present in the system until the last <code>SecretBinding</code> that references them gets deleted, the controller adds a finalizer which is only released when there is no <code>SecretBinding</code> referencing the <code>Quota</code> anymore.</p><h3 id=project-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerproject><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/project><code>Project</code> Controller</a></h3><p>There are multiple controllers responsible for different aspects of <code>Project</code> objects.
Please also refer to the <a href=/docs/gardener/projects/><code>Project</code> documentation</a>.</p><h4 id=main-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerprojectproject><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/project/project>&ldquo;Main&rdquo; Reconciler</a></h4><p>This reconciler manages a dedicated <code>Namespace</code> for each <code>Project</code>.
The namespace name can either be specified explicitly in <code>.spec.namespace</code> (must be prefixed with <code>garden-</code>) or it will be determined by the controller.
If <code>.spec.namespace</code> is set, it tries to create it. If it already exists, it tries to adopt it.
This will only succeed if the <code>Namespace</code> was previously labeled with <code>gardener.cloud/role=project</code> and <code>project.gardener.cloud/name=&lt;project-name></code>.
This is to prevent end-users from being able to adopt arbitrary namespaces and escalate their privileges, e.g. the <code>kube-system</code> namespace.</p><p>After the namespace was created/adopted, the controller creates several <code>ClusterRole</code>s and <code>ClusterRoleBinding</code>s that allow the project members to access related resources based on their roles.
These RBAC resources are prefixed with <code>gardener.cloud:system:project{-member,-viewer}:&lt;project-name></code>.
Gardener administrators and extension developers can define their own roles. For more information, see <a href=/docs/gardener/extensions/project-roles/>Extending Project Roles</a> for more information.</p><p>In addition, operators can configure the Project controller to maintain a default <a href=https://kubernetes.io/docs/concepts/policy/resource-quotas/>ResourceQuota</a> for project namespaces.
Quotas can especially limit the creation of user facing resources, e.g. <code>Shoots</code>, <code>SecretBindings</code>, <code>Secrets</code> and thus protect the garden cluster from massive resource exhaustion but also enable operators to align quotas with respective enterprise policies.</p><blockquote><p>⚠️ <strong>Gardener itself is not exempted from configured quotas</strong>. For example, Gardener creates <code>Secrets</code> for every shoot cluster in the project namespace and at the same time increases the available quota count. Please mind this additional resource consumption.</p></blockquote><p>The controller configuration provides a template section <code>controllers.project.quotas</code> where such a ResourceQuota (see the example below) can be deposited.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>controllers:
</span></span><span style=display:flex><span>  project:
</span></span><span style=display:flex><span>    quotas:
</span></span><span style=display:flex><span>    - config:
</span></span><span style=display:flex><span>        apiVersion: v1
</span></span><span style=display:flex><span>        kind: ResourceQuota
</span></span><span style=display:flex><span>        spec:
</span></span><span style=display:flex><span>          hard:
</span></span><span style=display:flex><span>            count/shoots.core.gardener.cloud: <span style=color:#a31515>&#34;100&#34;</span>
</span></span><span style=display:flex><span>            count/secretbindings.core.gardener.cloud: <span style=color:#a31515>&#34;10&#34;</span>
</span></span><span style=display:flex><span>            count/secrets: <span style=color:#a31515>&#34;800&#34;</span>
</span></span><span style=display:flex><span>      projectSelector: {}
</span></span></code></pre></div><p>The Project controller takes the specified <code>config</code> and creates a <code>ResourceQuota</code> with the name <code>gardener</code> in the project namespace.
If a <code>ResourceQuota</code> resource with the name <code>gardener</code> already exists, the controller will only update fields in <code>spec.hard</code> which are <strong>unavailable</strong> at that time.
This is done to configure a default <code>Quota</code> in all projects but to allow manual quota increases as the projects&rsquo; demands increase.
<code>spec.hard</code> fields in the <code>ResourceQuota</code> object that are not present in the configuration are removed from the object.
Labels and annotations on the <code>ResourceQuota</code> <code>config</code> get merged with the respective fields on existing <code>ResourceQuota</code>s.
An optional <code>projectSelector</code> narrows down the amount of projects that are equipped with the given <code>config</code>.
If multiple configs match for a project, then only the first match in the list is applied to the project namespace.</p><p>The <code>.status.phase</code> of the <code>Project</code> resources is set to <code>Ready</code> or <code>Failed</code> by the reconciler to indicate whether the reconciliation loop was performed successfully.
Also, it generates <code>Event</code>s to provide further information about its operations.</p><p>When a <code>Project</code> is marked for deletion, the controller ensures that there are no <code>Shoots</code> left in the project namespace.
Once all <code>Shoots</code> are gone, the <code>Namespace</code> and <code>Project</code> are released.</p><h4 id=stale-projects-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerprojectstale><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/project/stale>&ldquo;Stale Projects&rdquo; Reconciler</a></h4><p>As Gardener is a large-scale Kubernetes as a Service, it is designed for being used by a large amount of end-users.
Over time, it is likely to happen that some of the hundreds or thousands of <code>Project</code> resources are no longer actively used.</p><p>Gardener offers the &ldquo;stale projects&rdquo; reconciler which will take care of identifying such stale projects, marking them with a &ldquo;warning&rdquo;, and eventually deleting them after a certain time period.
This reconciler is enabled by default and works as follows:</p><ol><li>Projects are considered as &ldquo;stale&rdquo;/not actively used when all of the following conditions apply: The namespace associated with the <code>Project</code> does not have any&mldr;<ol><li><code>Shoot</code> resources.</li><li><code>BackupEntry</code> resources.</li><li><code>Secret</code> resources that are referenced by a <code>SecretBinding</code> that is in use by a <code>Shoot</code> (not necessarily in the same namespace).</li><li><code>Quota</code> resources that are referenced by a <code>SecretBinding</code> that is in use by a <code>Shoot</code> (not necessarily in the same namespace).</li><li>The time period when the project was used for the last time (<code>status.lastActivityTimestamp</code>) is longer than the configured <code>minimumLifetimeDays</code></li></ol></li></ol><p>If a project is considered &ldquo;stale&rdquo;, then its <code>.status.staleSinceTimestamp</code> will be set to the time when it was first detected to be stale.
If it gets actively used again, this timestamp will be removed.
After some time, the <code>.status.staleAutoDeleteTimestamp</code> will be set to a timestamp after which Gardener will auto-delete the <code>Project</code> resource if it still is not actively used.</p><p>The component configuration of the <code>gardener-controller-manager</code> offers to configure the following options:</p><ul><li><code>minimumLifetimeDays</code>: Don&rsquo;t consider newly created <code>Project</code>s as &ldquo;stale&rdquo; too early to give people/end-users some time to onboard and get familiar with the system. The &ldquo;stale project&rdquo; reconciler won&rsquo;t set any timestamp for <code>Project</code>s younger than <code>minimumLifetimeDays</code>. When you change this value, then projects marked as &ldquo;stale&rdquo; may be no longer marked as &ldquo;stale&rdquo; in case they are young enough, or vice versa.</li><li><code>staleGracePeriodDays</code>: Don&rsquo;t compute auto-delete timestamps for stale <code>Project</code>s that are unused for less than <code>staleGracePeriodDays</code>. This is to not unnecessarily make people/end-users nervous &ldquo;just because&rdquo; they haven&rsquo;t actively used their <code>Project</code> for a given amount of time. When you change this value, then already assigned auto-delete timestamps may be removed if the new grace period is not yet exceeded.</li><li><code>staleExpirationTimeDays</code>: Expiration time after which stale <code>Project</code>s are finally auto-deleted (after <code>.status.staleSinceTimestamp</code>). If this value is changed and an auto-delete timestamp got already assigned to the projects, then the new value will only take effect if it&rsquo;s increased. Hence, decreasing the <code>staleExpirationTimeDays</code> will not decrease already assigned auto-delete timestamps.</li></ul><blockquote><p>Gardener administrators/operators can exclude specific <code>Project</code>s from the stale check by annotating the related <code>Namespace</code> resource with <code>project.gardener.cloud/skip-stale-check=true</code>.</p></blockquote><h4 id=activity-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerprojectactivity><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/project/activity>&ldquo;Activity&rdquo; Reconciler</a></h4><p>Since the other two reconcilers are unable to actively monitor the relevant objects that are used in a <code>Project</code> (<code>Shoot</code>, <code>Secret</code>, etc.), there could be a situation where the user creates and deletes objects in a short period of time. In that case, the <code>Stale Project Reconciler</code> could not see that there was any activity on that project and it will still mark it as a <code>Stale</code>, even though it is actively used.</p><p>The <code>Project Activity Reconciler</code> is implemented to take care of such cases. An event handler will notify the reconciler for any acitivity and then it will update the <code>status.lastActivityTimestamp</code>. This update will also trigger the <code>Stale Project Reconciler</code>.</p><h3 id=secretbinding-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollersecretbinding><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/secretbinding><code>SecretBinding</code> Controller</a></h3><p><code>SecretBinding</code>s reference <code>Secret</code>s and <code>Quota</code>s and are themselves referenced by <code>Shoot</code>s.
The controller adds finalizers to the referenced objects to ensure they don&rsquo;t get deleted while still being referenced.
Similarly, to ensure that <code>SecretBinding</code>s in-use are always present in the system until the last referring <code>Shoot</code> gets deleted, the controller adds a finalizer which is only released when there is no <code>Shoot</code> referencing the <code>SecretBinding</code> anymore.</p><p>Referenced <code>Secret</code>s will also be labeled with <code>provider.shoot.gardener.cloud/&lt;type>=true</code>, where <code>&lt;type></code> is the value of the <code>.provider.type</code> of the <code>SecretBinding</code>.
Also, all referenced <code>Secret</code>s, as well as <code>Quota</code>s, will be labeled with <code>reference.gardener.cloud/secretbinding=true</code> to allow for easily filtering for objects referenced by <code>SecretBinding</code>s.</p><h3 id=seed-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerseed><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/seed><code>Seed</code> Controller</a></h3><p>The Seed controller in the <code>gardener-controller-manager</code> reconciles <code>Seed</code> objects with the help of the following reconcilers.</p><h4 id=main-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerseedsecrets><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/seed/secrets>&ldquo;Main&rdquo; Reconciler</a></h4><p>This reconciliation loop takes care of seed related operations in the garden cluster. When a new <code>Seed</code> object is created,
the reconciler creates a new <code>Namespace</code> in the garden cluster <code>seed-&lt;seed-name></code>. <code>Namespaces</code> dedicated to single
seed clusters allow us to segregate access permissions i.e., a <code>gardenlet</code> must not have permissions to access objects in
all <code>Namespaces</code> in the garden cluster.
There are objects in a Garden environment which are created once by the operator e.g., default domain secret,
alerting credentials, and are required for operations happening in the <code>gardenlet</code>. Therefore, we not only need a seed specific
<code>Namespace</code> but also a copy of these &ldquo;shared&rdquo; objects.</p><p>The &ldquo;main&rdquo; reconciler takes care about this replication:</p><table><thead><tr><th>Kind</th><th>Namespace</th><th>Label Selector</th></tr></thead><tbody><tr><td>Secret</td><td>garden</td><td>gardener.cloud/role</td></tr></tbody></table><h4 id=backup-buckets-check-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerseedbackupbucketscheck><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/seed/backupbucketscheck>&ldquo;Backup Buckets Check&rdquo; Reconciler</a></h4><p>Every time a <code>BackupBucket</code> object is created or updated, the referenced <code>Seed</code> object is enqueued for reconciliation.
It&rsquo;s the reconciler&rsquo;s task to check the <code>status</code> subresource of all existing <code>BackupBucket</code>s that reference this <code>Seed</code>.
If at least one <code>BackupBucket</code> has <code>.status.lastError != nil</code>, the <code>BackupBucketsReady</code> condition on the <code>Seed</code> will be set to <code>False</code>, and consequently the <code>Seed</code> is considered as <code>NotReady</code>.
If the <code>SeedBackupBucketsCheckControllerConfiguration</code> (which is part of <code>gardener-controller-manager</code>s component configuration) contains a <code>conditionThreshold</code> for the <code>BackupBucketsReady</code>, the condition will instead first be set to <code>Progressing</code> and eventually to <code>False</code> once the <code>conditionThreshold</code> expires. See <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-controller-manager.yaml>the example config file</a> for details.
Once the <code>BackupBucket</code> is healthy again, the seed will be re-queued and the condition will turn <code>true</code>.</p><h4 id=extensions-check-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerseedextensionscheck><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/seed/extensionscheck>&ldquo;Extensions Check&rdquo; Reconciler</a></h4><p>This reconciler reconciles <code>Seed</code> objects and checks whether all <code>ControllerInstallation</code>s referencing them are in a healthy state.
Concretely, all three conditions <code>Valid</code>, <code>Installed</code>, and <code>Healthy</code> must have status <code>True</code> and the <code>Progressing</code> condition must have status <code>False</code>.
Based on this check, it maintains the <code>ExtensionsReady</code> condition in the respective <code>Seed</code>&rsquo;s <code>.status.conditions</code> list.</p><h4 id=lifecycle-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollerseedlifecycle><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/seed/lifecycle>&ldquo;Lifecycle&rdquo; Reconciler</a></h4><p>The &ldquo;Lifecycle&rdquo; reconciler processes <code>Seed</code> objects which are enqueued every 10 seconds in order to check if the responsible
<code>gardenlet</code> is still responding and operable. Therefore, it checks renewals via <code>Lease</code> objects of the seed in the garden cluster
which are renewed regularly by the <code>gardenlet</code>.</p><p>In case a <code>Lease</code> is not renewed for the configured amount in <code>config.controllers.seed.monitorPeriod.duration</code>:</p><ol><li>The reconciler assumes that the <code>gardenlet</code> stopped operating and updates the <code>GardenletReady</code> condition to <code>Unknown</code>.</li><li>Additionally, the conditions and constraints of all <code>Shoot</code> resources scheduled on the affected seed are set to <code>Unknown</code> as well,
because a striking <code>gardenlet</code> won&rsquo;t be able to maintain these conditions any more.</li><li>If the gardenlet&rsquo;s client certificate has expired (identified based on the <code>.status.clientCertificateExpirationTimestamp</code> field in the <code>Seed</code> resource) and if it is managed by a <code>ManagedSeed</code>, then this will be triggered for a reconciliation. This will trigger the bootstrapping process again and allows gardenlets to obtain a fresh client certificate.</li></ol><h3 id=shoot-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollershoot><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/shoot><code>Shoot</code> Controller</a></h3><h4 id=conditions-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollershootconditions><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/shoot/conditions>&ldquo;Conditions&rdquo; Reconciler</a></h4><p>In case the reconciled <code>Shoot</code> is registered via a <code>ManagedSeed</code> as a seed cluster, this reconciler merges the conditions in the respective <code>Seed</code>&rsquo;s <code>.status.conditions</code> into the <code>.status.conditions</code> of the <code>Shoot</code>.
This is to provide a holistic view on the status of the registered seed cluster by just looking at the <code>Shoot</code> resource.</p><h4 id=hibernation-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollershoothibernation><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/shoot/hibernation>&ldquo;Hibernation&rdquo; Reconciler</a></h4><p>This reconciler is responsible for hibernating or awakening shoot clusters based on the schedules defined in their <code>.spec.hibernation.schedules</code>.
It ignores <a href=/docs/gardener/shoot_status/#last-operation>failed <code>Shoot</code>s</a> and those marked for deletion.</p><h4 id=maintenance-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollershootmaintenance><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/shoot/maintenance>&ldquo;Maintenance&rdquo; Reconciler</a></h4><p>This reconciler is responsible for maintaining shoot clusters based on the time window defined in their <code>.spec.maintenance.timeWindow</code>.
It might auto-update the Kubernetes version or the operating system versions specified in the worker pools (<code>.spec.provider.workers</code>).
It could also add some operation or task annotations. For more information, see <a href=/docs/gardener/shoot_maintenance/>Shoot Maintenance</a>.</p><h4 id=quota-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollershootquota><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/shoot/quota>&ldquo;Quota&rdquo; Reconciler</a></h4><p>This reconciler might auto-delete shoot clusters in case their referenced <code>SecretBinding</code> is itself referencing a <code>Quota</code> with <code>.spec.clusterLifetimeDays != nil</code>.
If the shoot cluster is older than the configured lifetime, then it gets deleted.
It maintains the expiration time of the <code>Shoot</code> in the value of the <code>shoot.gardener.cloud/expiration-timestamp</code> annotation.
This annotation might be overridden, however only by at most twice the value of the <code>.spec.clusterLifetimeDays</code>.</p><h4 id=reference-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollershootreference><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/shoot/reference>&ldquo;Reference&rdquo; Reconciler</a></h4><p>Shoot objects may specify references to other objects in the garden cluster which are required for certain features.
For example, users can configure various DNS providers via <code>.spec.dns.providers</code> and usually need to refer to a corresponding <code>Secret</code> with valid DNS provider credentials inside.
Such objects need a special protection against deletion requests as long as they are still being referenced by one or multiple shoots.</p><p>Therefore, this reconciler checks <code>Shoot</code>s for referenced objects and adds the finalizer <code>gardener.cloud/reference-protection</code> to their <code>.metadata.finalizers</code> list.
The reconciled <code>Shoot</code> also gets this finalizer to enable a proper garbage collection in case the <code>gardener-controller-manager</code> is offline at the moment of an incoming deletion request.
When an object is not actively referenced anymore because the <code>Shoot</code> specification has changed or all related shoots were deleted (are in deletion), the controller will remove the added finalizer again so that the object can safely be deleted or garbage collected.</p><p>This reconciler inspects the following references:</p><ul><li>DNS provider secrets (<code>.spec.dns.provider</code>)</li><li>Audit policy configmaps (<code>.spec.kubernetes.kubeAPIServer.auditConfig.auditPolicy.configMapRef</code>)</li></ul><p>Further checks might be added in the future.</p><h4 id=retry-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollershootretry><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/shoot/retry>&ldquo;Retry&rdquo; Reconciler</a></h4><p>This reconciler is responsible for retrying certain failed <code>Shoot</code>s.
Currently, the reconciler retries only failed <code>Shoot</code>s with an error code <code>ERR_INFRA_RATE_LIMITS_EXCEEDED</code>. See <a href=/docs/gardener/shoot_status/#error-codes>Shoot Status</a> for more details.</p><h4 id=status-label-reconcilerhttpsgithubcomgardenergardenertreemasterpkgcontrollermanagercontrollershootstatuslabel><a href=https://github.com/gardener/gardener/tree/master/pkg/controllermanager/controller/shoot/statuslabel>&ldquo;Status Label&rdquo; Reconciler</a></h4><p>This reconciler is responsible for maintaining the <code>shoot.gardener.cloud/status</code> label on <code>Shoot</code>s. See <a href=/docs/gardener/shoot_status/#status-label>Shoot Status</a> for more details.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-30e7b65c22f3f4eef68dc3f72ee90cfe>9 - Gardener Node Agent</h1><div class=lead>How Gardener bootstraps machines into worker nodes and how it installs and maintains gardener-managed node-specific components</div><h2 id=overview>Overview</h2><p>The goal of the <code>gardener-node-agent</code> is to bootstrap a machine into a worker node and maintain node-specific components, which run on the node and are unmanaged by Kubernetes (e.g. the <code>kubelet</code> service, systemd units, &mldr;).</p><p>It effectively is a Kubernetes controller deployed onto the worker node.</p><h2 id=architecture-and-basic-design>Architecture and Basic Design</h2><p><img src=/__resources/gardener-nodeagent-architecture_d1eaf4.svg alt=Design></p><p>This figure visualizes the overall architecture of the <code>gardener-node-agent</code>. On the left side, it starts with an <a href=/docs/gardener/extensions/operatingsystemconfig/><code>OperatingSystemConfig</code></a> resource (<code>OSC</code>) with a corresponding worker pool specific <code>cloud-config-&lt;worker-pool></code> secret being passed by reference through the userdata to a machine by the <code>machine-controller-manager</code> (MCM).</p><p>On the right side, the <code>cloud-config</code> secret will be extracted and used by the <code>gardener-node-agent</code> after being installed. Details on this can be found in the next section.</p><p>Finally, the <code>gardener-node-agent</code> runs a systemd service watching on secret resources located in the <code>kube-system</code> namespace like our <code>cloud-config</code> secret that contains the <code>OperatingSystemConfig</code>. When <code>gardener-node-agent</code> applies the OSC, it installs the <code>kubelet</code> + configuration on the worker node.</p><h2 id=installation-and-bootstrapping>Installation and Bootstrapping</h2><p>This section describes how the <code>gardener-node-agent</code> is initially installed onto the worker node.</p><p>In the beginning, there is a very small bash script called <a href=https://github.com/gardener/gardener/blob/master/pkg/component/extensions/operatingsystemconfig/original/components/containerd/templates/scripts/init.tpl.sh><code>gardener-node-init.sh</code></a>, which will be copied to <code>/var/lib/gardener-node-agent/init.sh</code> on the node with cloud-init data.
This script&rsquo;s sole purpose is downloading and starting the <code>gardener-node-agent</code>.
The binary artifact is extracted from an <a href=https://github.com/opencontainers/image-spec/blob/main/manifest.md>OCI artifact</a> and lives at <code>/opt/bin/gardener-node-agent</code>.</p><p>Along with the init script, a configuration for the <code>gardener-node-agent</code> is carried over to the worker node at <code>/var/lib/gardener-node-agent/config.yaml</code>.
This configuration contains things like the shoot&rsquo;s <code>kube-apiserver</code> endpoint, the according certificates to communicate with it, and controller configuration.</p><p>In a bootstrapping phase, the <code>gardener-node-agent</code> sets itself up as a systemd service.
It also executes tasks that need to be executed before any other components are installed, e.g. formatting the data device for the <code>kubelet</code>.</p><h2 id=controllers>Controllers</h2><p>This section describes the controllers in more details.</p><h3 id=lease-controllerhttpsgithubcomgardenergardenertreemasterpkgnodeagentcontrollerlease><a href=https://github.com/gardener/gardener/tree/master/pkg/nodeagent/controller/lease><code>Lease</code> Controller</a></h3><p>This controller creates a <code>Lease</code> for <code>gardener-node-agent</code> in <code>kube-system</code> namespace of the shoot cluster.
Each instance of <code>gardener-node-agent</code> creates its own <code>Lease</code> when its corresponding <code>Node</code> was created.
It renews the <code>Lease</code> resource every 10 seconds. This indicates a heartbeat to the external world.</p><h3 id=node-controllerhttpsgithubcomgardenergardenertreemasterpkgnodeagentcontrollernode><a href=https://github.com/gardener/gardener/tree/master/pkg/nodeagent/controller/node><code>Node</code> Controller</a></h3><p>This controller watches the <code>Node</code> object for the machine it runs on.
The correct <code>Node</code> is identified based on the hostname of the machine (<code>Node</code>s have the <code>kubernetes.io/hostname</code> label).
Whenever the <code>worker.gardener.cloud/restart-systemd-services</code> annotation changes, the controller performs the desired changes by restarting the specified systemd unit files.
See also <a href=/docs/gardener/shoot_operations/#restart-systemd-services-on-particular-worker-nodes>this document</a> for more information.
After restarting all units, the annotation is removed.</p><blockquote><p>ℹ️ When the <code>gardener-node-agent</code> systemd service itself is requested to be restarted, the annotation is removed first to ensure it does not restart itself indefinitely.</p></blockquote><h3 id=operating-system-config-controllerhttpsgithubcomgardenergardenertreemasterpkgnodeagentcontrolleroperatingsystemconfig><a href=https://github.com/gardener/gardener/tree/master/pkg/nodeagent/controller/operatingsystemconfig>Operating System Config Controller</a></h3><p>This controller contains the main logic of <code>gardener-node-agent</code>.
It watches <code>Secret</code>s whose <code>data</code> map contains the <a href=/docs/gardener/extensions/operatingsystemconfig/#reconcile-purpose><code>OperatingSystemConfig</code></a> which consists of all systemd units and files that are relevant for the node configuration.
Amongst others, a prominent example is the configuration file for <code>kubelet</code> and its unit file for the <code>kubelet.service</code>.</p><p>The controller decodes the configuration and computes the files and units that have changed since its last reconciliation.
It writes or update the files and units to the file system, removes no longer needed files and units, reloads the systemd daemon, and starts or stops the units accordingly.</p><p>After successful reconciliation, it persists the just applied <code>OperatingSystemConfig</code> into a file on the host.
This file will be used for future reconciliations to compute file/unit changes.</p><p>The controller also maintains two annotations on the <code>Node</code>:</p><ul><li><code>worker.gardener.cloud/kubernetes-version</code>, describing the version of the installed <code>kubelet</code>.</li><li><code>checksum/cloud-config-data</code>, describing the checksum of the applied <code>OperatingSystemConfig</code> (used in future reconciliations to determine whether it needs to reconcile, and to report that this node is up-to-date).</li></ul><h3 id=token-controllerhttpsgithubcomgardenergardenertreemasterpkgnodeagentcontrollertoken><a href=https://github.com/gardener/gardener/tree/master/pkg/nodeagent/controller/token>Token Controller</a></h3><p>This controller watches the access token <code>Secret</code>s in the <code>kube-system</code> namespace configured via the <code>gardener-node-agent</code>&rsquo;s component configuration (<code>.controllers.token.syncConfigs[]</code> field).
Whenever the <code>.data.token</code> field changes, it writes the new content to a file on the configured path on the host file system.
This mechanism is used to download its own access token for the shoot cluster, but also the access tokens of other <code>systemd</code> components (e.g., <code>valitail</code>).
Since the underlying client is based on <code>k8s.io/client-go</code> and the kubeconfig points to this token file, it is dynamically reloaded without the necessity of explicit configuration or code changes.
This procedure ensures that the most up-to-date tokens are always present on the host and used by the <code>gardener-node-agent</code> and the other <code>systemd</code> components.</p><h2 id=reasoning>Reasoning</h2><p>The <code>gardener-node-agent</code> is a replacement for what was called the <code>cloud-config-downloader</code> and the <code>cloud-config-executor</code>, both written in <code>bash</code>. The <code>gardener-node-agent</code> implements this functionality as a regular controller and feels more uniform in terms of maintenance.</p><p>With the new architecture we gain a lot, let&rsquo;s describe the most important gains here.</p><h3 id=developer-productivity>Developer Productivity</h3><p>Since the Gardener community develops in Go day by day, writing business logic in <code>bash</code> is difficult, hard to maintain, almost impossible to test. Getting rid of almost all <code>bash</code> scripts which are currently in use for this very important part of the cluster creation process will enhance the speed of adding new features and removing bugs.</p><h3 id=speed>Speed</h3><p>Until now, the <code>cloud-config-downloader</code> runs in a loop every <code>60s</code> to check if something changed on the shoot which requires modifications on the worker node. This produces a lot of unneeded traffic on the API server and wastes time, it will sometimes take up to <code>60s</code> until a desired modification is started on the worker node.
By writing a &ldquo;real&rdquo; Kubernetes controller, we can watch for the <code>Node</code>, the <code>OSC</code> in the <code>Secret</code>, and the shoot-access token in the <code>secret</code>. If any of these object changed, and only then, the required action will take effect immediately.
This will speed up operations and will reduce the load on the API server of the shoot especially for large clusters.</p><h2 id=scalability>Scalability</h2><p>The <code>cloud-config-downloader</code> adds a random wait time before restarting the <code>kubelet</code> in case the <code>kubelet</code> was updated or a configuration change was made to it. This is required to reduce the load on the API server and the traffic on the internet uplink. It also reduces the overall downtime of the services in the cluster because every <code>kubelet</code> restart transforms a node for several seconds into <code>NotReady</code> state which potentionally interrupts service availability.</p><p>Decision was made to keep the existing jitter mechanism which calculates the <code>kubelet-download-and-restart-delay-seconds</code> on the controller itself.</p><h3 id=correctness>Correctness</h3><p>The configuration of the <code>cloud-config-downloader</code> is actually done by placing a file for every configuration item on the disk on the worker node. This was done because parsing the content of a single file and using this as a value in <code>bash</code> reduces to something like <code>VALUE=$(cat /the/path/to/the/file)</code>. Simple, but it lacks validation, type safety and whatnot.
With the <code>gardener-node-agent</code> we introduce a new API which is then stored in the <code>gardener-node-agent</code> <code>secret</code> and stored on disk in a single YAML file for comparison with the previous known state. This brings all benefits of type safe configuration.
Because actual and previous configuration are compared, removed files and units are also removed and stopped on the worker if removed from the <code>OSC</code>.</p><h3 id=availability>Availability</h3><p>Previously, the <code>cloud-config-downloader</code> simply restarted the systemd units on every change to the <code>OSC</code>, regardless which of the services changed. The <code>gardener-node-agent</code> first checks which systemd unit was changed, and will only restart these. This will prevent unneeded <code>kubelet</code> restarts.</p><h3 id=future-development>Future Development</h3><p>The <code>gardener-node-agent</code> opens up the possibilty for further improvements.</p><p>Necessary restarts of the <code>kubelet</code> could be deterministic instead of the aforementioned random jittering. In that case, the <code>gardenlet</code> could add annotations across all nodes. As the <code>gardener-node-agent</code> watches the <code>Node</code> object, it could wait with <code>kubelet</code> restarts, OSC changes or react immediately. Critical changes could be performed in chunks of nodes in serial order, but an equal time spread is possible, too.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b482c2c8a27582776a6072670e48a64d>10 - Gardener Operator</h1><div class=lead>Understand the component responsible for the garden cluster environment and its various features</div><h2 id=overview>Overview</h2><p>The <code>gardener-operator</code> is responsible for the garden cluster environment.
Without this component, users must deploy ETCD, the Gardener control plane, etc., manually and with separate mechanisms (not maintained in this repository).
This is quite unfortunate since this requires separate tooling, processes, etc.
A lot of production- and enterprise-grade features were built into Gardener for managing the seed and shoot clusters, so it makes sense to re-use them as much as possible also for the garden cluster.</p><h2 id=deployment>Deployment</h2><p>There is a <a href=https://github.com/gardener/gardener/tree/master/charts/gardener/operator>Helm chart</a> which can be used to deploy the <code>gardener-operator</code>.
Once deployed and ready, you can create a <code>Garden</code> resource.
Note that there can only be one <code>Garden</code> resource per system at a time.</p><blockquote><p>ℹ️ Similar to seed clusters, garden runtime clusters require a <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>VPA</a>, see <a href=/docs/gardener/concepts/operator/#vertical-pod-autoscaler>this section</a>.
By default, <code>gardener-operator</code> deploys the VPA components.
However, when there already is a VPA available, then set <code>.spec.runtimeCluster.settings.verticalPodAutoscaler.enabled=false</code> in the <code>Garden</code> resource.</p></blockquote><h2 id=garden-resources><code>Garden</code> Resources</h2><p>Please find an exemplary <code>Garden</code> resource <a href=https://github.com/gardener/gardener/blob/master/example/operator/20-garden.yaml>here</a>.</p><h3 id=configuration-for-runtime-cluster>Configuration For Runtime Cluster</h3><h4 id=settings>Settings</h4><p>The <code>Garden</code> resource offers a few settings that are used to control the behaviour of <code>gardener-operator</code> in the runtime cluster.
This section provides an overview over the available settings in <code>.spec.runtimeCluster.settings</code>:</p><h5 id=load-balancer-services>Load Balancer Services</h5><p><code>gardener-operator</code> deploys Istio and relevant resources to the runtime cluster in order to expose the <code>virtual-garden-kube-apiserver</code> service (similar to how the <code>kube-apiservers</code> of shoot clusters are exposed).
In most cases, the <code>cloud-controller-manager</code> (responsible for managing these load balancers on the respective underlying infrastructure) supports certain customization and settings via annotations.
<a href=https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer>This document</a> provides a good overview and many examples.</p><p>By setting the <code>.spec.runtimeCluster.settings.loadBalancerServices.annotations</code> field the Gardener administrator can specify a list of annotations which will be injected into the <code>Service</code>s of type <code>LoadBalancer</code>.</p><h5 id=vertical-pod-autoscaler>Vertical Pod Autoscaler</h5><p><code>gardener-operator</code> heavily relies on the Kubernetes <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler><code>vertical-pod-autoscaler</code> component</a>.
By default, the <code>Garden</code> controller deploys the VPA components into the <code>garden</code> namespace of the respective runtime cluster.
In case you want to manage the VPA deployment on your own or have a custom one, then you might want to disable the automatic deployment of <code>gardener-operator</code>.
Otherwise, you might end up with two VPAs which will cause erratic behaviour.
By setting the <code>.spec.runtimeCluster.settings.verticalPodAutoscaler.enabled=false</code> you can disable the automatic deployment.</p><p>⚠️ In any case, there must be a VPA available for your runtime cluster.
Using a runtime cluster without VPA is not supported.</p><h5 id=topology-aware-traffic-routing>Topology-Aware Traffic Routing</h5><p>Refer to the <a href=/docs/gardener/topology_aware_routing/>Topology-Aware Traffic Routing documentation</a> as this document contains the documentation for the topology-aware routing setting for the garden runtime cluster.</p><h4 id=volumes>Volumes</h4><p>It is possible to define the minimum size for <code>PersistentVolumeClaim</code>s in the runtime cluster created by <code>gardener-operator</code> via the <code>.spec.runtimeCluster.volume.minimumSize</code> field.
This can be relevant in case the runtime cluster runs on an infrastructure that does only support disks of at least a certain size.</p><h3 id=configuration-for-virtual-cluster>Configuration For Virtual Cluster</h3><h4 id=etcd-encryption-config>ETCD Encryption Config</h4><p>The <code>spec.virtualCluster.kubernetes.kubeAPIServer.encryptionConfig</code> field in the Garden API allows operators to customize encryption configurations for the <code>kube-apiserver</code> of the virtual cluster. It provides options to specify additional resources for encryption. Similarly <code>spec.virtualCluster.gardener.gardenerAPIServer.encryptionConfig</code> field allows operators to customize encryption configurations for the <code>gardener-apiserver</code>.</p><ul><li>The resources field can be used to specify resources that should be encrypted in addition to secrets. Secrets are always encrypted for the <code>kube-apiserver</code>. For the <code>gardener-apiserver</code>, the following resources are always encrypted:<ul><li><code>controllerdeployments.core.gardener.cloud</code></li><li><code>controllerregistrations.core.gardener.cloud</code></li><li><code>internalsecrets.core.gardener.cloud</code></li><li><code>shootstates.core.gardener.cloud</code></li></ul></li><li>Adding an item to any of the lists will cause patch requests for all the resources of that kind to encrypt them in the etcd. See <a href=https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data>Encrypting Confidential Data at Rest</a> for more details.</li><li>Removing an item from any of these lists will cause patch requests for all the resources of that type to decrypt and rewrite the resource as plain text. See <a href=https://kubernetes.io/docs/tasks/administer-cluster/decrypt-data/>Decrypt Confidential Data that is Already Encrypted at Rest</a> for more details.</li></ul><blockquote><p>ℹ️ Note that configuring encryption for a custom resource for the <code>kube-apiserver</code> is only supported for Kubernetes versions >= 1.26.</p></blockquote><h2 id=controllers>Controllers</h2><p>As of today, the <code>gardener-operator</code> only has two controllers which are now described in more detail.</p><h3 id=garden-controllerhttpsgithubcomgardenergardenertreemasterpkgoperatorcontrollergarden><a href=https://github.com/gardener/gardener/tree/master/pkg/operator/controller/garden><code>Garden</code> Controller</a></h3><p>The Garden controller in the operator reconciles Garden objects with the help of the following reconcilers.</p><h4 id=main-reconcilerhttpsgithubcomgardenergardenertreemasterpkgoperatorcontrollergardengarden><a href=https://github.com/gardener/gardener/tree/master/pkg/operator/controller/garden/garden><code>Main</code> Reconciler</a></h4><p>The reconciler first generates a general CA certificate which is valid for ~<code>30d</code> and auto-rotated when 80% of its lifetime is reached.
Afterwards, it brings up the so-called &ldquo;garden system components&rdquo;.
The <a href=/docs/gardener/concepts/resource-manager/><code>gardener-resource-manager</code></a> is deployed first since its <code>ManagedResource</code> controller will be used to bring up the remainders.</p><p>Other system components are:</p><ul><li>runtime garden system resources (<a href=/docs/gardener/priority-classes/><code>PriorityClass</code>es</a> for the workload resources)</li><li>virtual garden system resources (RBAC rules)</li><li>Vertical Pod Autoscaler (if enabled via <code>.spec.runtimeCluster.settings.verticalPodAutoscaler.enabled=true</code> in the <code>Garden</code>)</li><li>HVPA Controller (when <code>HVPA</code> feature gate is enabled)</li><li>ETCD Druid</li><li>Istio</li></ul><p>As soon as all system components are up, the reconciler deploys the virtual garden cluster.
It comprises out of two ETCDs (one &ldquo;main&rdquo; etcd, one &ldquo;events&rdquo; etcd) which are managed by ETCD Druid via <code>druid.gardener.cloud/v1alpha1.Etcd</code> custom resources.
The whole management works similar to how it works for <code>Shoot</code>s, so you can take a look at <a href=/docs/gardener/concepts/etcd/>this document</a> for more information in general.</p><p>The virtual garden control plane components are:</p><ul><li><code>virtual-garden-etcd-main</code></li><li><code>virtual-garden-etcd-events</code></li><li><code>virtual-garden-kube-apiserver</code></li><li><code>virtual-garden-kube-controller-manager</code></li><li><code>virtual-garden-gardener-resource-manager</code></li></ul><p>If the <code>.spec.virtualCluster.controlPlane.highAvailability={}</code> is set then these components will be deployed in a &ldquo;highly available&rdquo; mode.
For ETCD, this means that there will be 3 replicas each.
This works similar like for <code>Shoot</code>s (see <a href=/docs/guides/high-availability/control-plane/>this document</a>) except for the fact that there is no failure tolerance type configurability.
The <code>gardener-resource-manager</code>&rsquo;s <a href=/docs/gardener/concepts/resource-manager/#high-availability-config>HighAvailabilityConfig webhook</a> makes sure that all pods with multiple replicas are spread on nodes, and if there are at least two zones in <code>.spec.runtimeCluster.provider.zones</code> then they also get spread across availability zones.</p><blockquote><p>If once set, removing <code>.spec.virtualCluster.controlPlane.highAvailability</code> again is not supported.</p></blockquote><p>The <code>virtual-garden-kube-apiserver</code> <code>Deployment</code> is exposed via Istio, similar to how the <code>kube-apiservers</code> of shoot clusters are exposed.</p><p>Similar to the <code>Shoot</code> API, the version of the virtual garden cluster is controlled via <code>.spec.virtualCluster.kubernetes.version</code>.
Likewise, specific configuration for the control plane components can be provided in the same section, e.g. via <code>.spec.virtualCluster.kubernetes.kubeAPIServer</code> for the <code>kube-apiserver</code> or <code>.spec.virtualCluster.kubernetes.kubeControllerManager</code> for the <code>kube-controller-manager</code>.</p><p>The <code>kube-controller-manager</code> only runs a few controllers that are necessary in the scenario of the virtual garden.
Most prominently, <strong>the <code>serviceaccount-token</code> controller is unconditionally disabled</strong>.
Hence, the usage of static <code>ServiceAccount</code> secrets is not supported generally.
Instead, the <a href=https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-request-v1/><code>TokenRequest</code> API</a> should be used.
Third-party components that need to communicate with the virtual cluster can leverage the <a href=/docs/gardener/concepts/resource-manager/#tokenrequestor-controller><code>gardener-resource-manager</code>&rsquo;s <code>TokenRequestor</code> controller</a> and the generic kubeconfig, just like it works for <code>Shoot</code>s.
Please note, that this functionality is restricted to the <code>garden</code> namespace. The current <code>Secret</code> name of the generic kubeconfig can be found in the annotations (key: <code>generic-token-kubeconfig.secret.gardener.cloud/name</code>) of the <code>Garden</code> resource.</p><p>For the virtual cluster, it is essential to provide at least one DNS domain via <code>.spec.virtualCluster.dns.domains</code>.
<strong>The respective DNS records are not managed by <code>gardener-operator</code> and should be created manually.
They should point to the load balancer IP of the <code>istio-ingressgateway</code> <code>Service</code> in namespace <code>virtual-garden-istio-ingress</code>.
The DNS records must be prefixed with both <code>gardener.</code> and <code>api.</code> for all domains in <code>.spec.virtualCluster.dns.domains</code>.</strong></p><p>The first DNS domain in this list is used for the <code>server</code> in the kubeconfig, and for configuring the <code>--external-hostname</code> flag of the API server.</p><p>Apart from the control plane components of the virtual cluster, the reconcile also deploys the control plane components of Gardener.
<code>gardener-apiserver</code> reuses the same ETCDs like the <code>virtual-garden-kube-apiserver</code>, so all data related to the &ldquo;the garden cluster&rdquo; is stored together and &ldquo;isolated&rdquo; from ETCD data related to the runtime cluster.
This drastically simplifies backup and restore capabilities (e.g., moving the virtual garden cluster from one runtime cluster to another).</p><p>The Gardener control plane components are:</p><ul><li><code>gardener-apiserver</code></li><li><code>gardener-admission-controller</code></li><li><code>gardener-controller-manager</code></li><li><code>gardener-scheduler</code></li></ul><p>Besides those, the optional <a href=https://github.com/gardener/dashboard>Gardener Dashboard</a> will also get deployed when <code>.spec.virtualCluster.gardener.gardenerDashboard</code> is set.
You can read more about it and its configuration in <a href=/docs/gardener/concepts/operator/#dashboard>this section</a>.</p><p>The reconciler also manages a few observability-related components (more planned as part of <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/19-migrating-observability-stack-to-operators.md>GEP-19</a>):</p><ul><li><code>fluent-operator</code></li><li><code>fluent-bit</code></li><li><code>gardener-metrics-exporter</code></li><li><code>kube-state-metrics</code></li><li><code>plutono</code></li><li><code>vali</code></li><li><code>prometheus-operator</code></li><li><code>alertmanager-garden</code> (read more <a href=/docs/gardener/concepts/operator/#observability>here</a>)</li><li><code>prometheus-garden</code> (read more <a href=/docs/gardener/concepts/operator/#observability>here</a>)</li><li><code>blackbox-exporter</code></li></ul><p>It is also mandatory to provide an IPv4 CIDR for the service network of the virtual cluster via <code>.spec.virtualCluster.networking.services</code>.
This range is used by the API server to compute the cluster IPs of <code>Service</code>s.</p><p>The controller maintains the <code>.status.lastOperation</code> which indicates the status of an operation.</p><h5 id=gardener-dashboardhttpsgithubcomgardenerdashboard><a href=https://github.com/gardener/dashboard>Gardener Dashboard</a></h5><p><code>.spec.virtualCluster.gardener.gardenerDashboard</code> serves a few configuration options for the dashboard.
This section highlights the most prominent fields:</p><ul><li><code>oidcConfig</code>: The general OIDC configuration is part of <code>.spec.virtualCluster.kubernetes.kubeAPIServer.oidcConfig</code>.
This section allows you to define a few specific settings for the dashboard.
<code>sessionLifetime</code> is the duration after which a session is terminated (i.e., after which a user is automatically logged out).
<code>additionalScopes</code> allows to extend the list of scopes of the JWT token that are to be recognized.
You must reference a <code>Secret</code> in the <code>garden</code> namespace containing the client ID/secret for the dashboard:<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardener-dashboard-oidc
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>stringData:
</span></span><span style=display:flex><span>  client_id: &lt;secret&gt;
</span></span><span style=display:flex><span>  client_secret: &lt;secret&gt;
</span></span></code></pre></div></li><li><code>enableTokenLogin</code>: This is enabled by default and allows logging into the dashboard with a JWT token.
You can disable it in case you want to only allow OIDC-based login.
However, at least one of the both login methods must be enabled.</li><li><code>frontendConfigMapRef</code>: Reference a <code>ConfigMap</code> in the <code>garden</code> namespace containing the frontend configuration in the data with key <code>frontend-config.yaml</code>, for example<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardener-dashboard-frontend
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  frontend-config.yaml: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    helpMenuItems:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - title: Homepage
</span></span></span><span style=display:flex><span><span style=color:#a31515>      icon: mdi-file-document
</span></span></span><span style=display:flex><span><span style=color:#a31515>      url: https://gardener.cloud</span>    
</span></span></code></pre></div>Please take a look at <a href=https://github.com/gardener/dashboard/blob/64516ede9110065c24c61ab67f06c866fef10f3c/charts/gardener-dashboard/values.yaml#L154-L376>this file</a> to get an idea of which values are configurable.
This configuration can also include branding, themes, and colors.
Read more about it <a href=/docs/dashboard/customization/>here</a>.
Assets (logos/icons) are configured in a separate <code>ConfigMap</code>, see below.</li><li><code>assetsConfigMapRef</code>: Reference a <code>ConfigMap</code> in the <code>garden</code> namespace containing the assets, for example<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardener-dashboard-assets
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>binaryData:
</span></span><span style=display:flex><span>  favicon-16x16.png: base64(favicon-16x16.png)
</span></span><span style=display:flex><span>  favicon-32x32.png: base64(favicon-32x32.png)
</span></span><span style=display:flex><span>  favicon-96x96.png: base64(favicon-96x96.png)
</span></span><span style=display:flex><span>  favicon.ico: base64(favicon.ico)
</span></span><span style=display:flex><span>  logo.svg: base64(logo.svg)
</span></span></code></pre></div>Note that the assets must be provided base64-encoded, hence <code>binaryData</code> (instead of <code>data</code>) must be used.
Please take a look at <a href=/docs/dashboard/customization/#logos-and-icons>this file</a> to get more information.</li><li><code>gitHub</code>: You can connect a GitHub repository that can be used to create issues for shoot clusters in the cluster details page.
You have to reference a <code>Secret</code> in the <code>garden</code> namespace that contains the GitHub credentials, for example:<div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardener-dashboard-github
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>stringData:
</span></span><span style=display:flex><span>  <span style=color:green># This is for GitHub token authentication:</span>
</span></span><span style=display:flex><span>  authentication.token: &lt;secret&gt;
</span></span><span style=display:flex><span>  <span style=color:green># Alternatively, this is for GitHub app authentication:</span>
</span></span><span style=display:flex><span>  authentication.appId: &lt;secret&gt;
</span></span><span style=display:flex><span>  authentication.clientId: &lt;secret&gt;
</span></span><span style=display:flex><span>  authentication.clientSecret: &lt;secret&gt;
</span></span><span style=display:flex><span>  authentication.installationId: &lt;secret&gt;
</span></span><span style=display:flex><span>  authentication.privateKey: &lt;secret&gt;
</span></span><span style=display:flex><span>  <span style=color:green># This is the webhook secret, see explanation below</span>
</span></span><span style=display:flex><span>  webhookSecret: &lt;secret&gt;
</span></span></code></pre></div>Note that you can also set up a GitHub webhook to the dashboard such that it receives updates when somebody changes the GitHub issue.
The <code>webhookSecret</code> field is the secret that you enter in GitHub in the <a href=https://docs.github.com/en/webhooks/using-webhooks/validating-webhook-deliveries#creating-a-secret-token>webhook configuration</a>.
The dashboard uses it to verify that received traffic is indeed originated from GitHub.
If you don&rsquo;t want to set up such webhook, or if the dashboard is not reachable by the GitHub webhook (e.g., in restricted environments) you can also configure <code>gitHub.pollInterval</code>.
It is the interval of how often the GitHub API is polled for issue updates.
This field is used as a fallback mechanism to ensure state synchronization, even when there is a GitHub webhook configuration.
If a webhook event is missed or not successfully delivered, the polling will help catch up on any missed updates.
If this field is not provided and there is no <code>webhookSecret</code> key in the referenced secret, it will be implicitly defaulted to <code>15m</code>.
The dashboard will use this to regularly poll the GitHub API for updates on issues.</li><li><code>terminal</code>: This enables the web terminal feature, read more about it <a href=/docs/dashboard/webterminals/>here</a>.
The <code>allowedHosts</code> field is explained <a href=/docs/dashboard/webterminals/#configuration>here</a>.
The <code>container</code> section allows you to specify a container image and a description that should be used for the web terminals.</li></ul><h5 id=observability>Observability</h5><h6 id=garden-prometheus>Garden Prometheus</h6><p><code>gardener-operator</code> deploys a Prometheus instance in the <code>garden</code> namespace (called &ldquo;Garden Prometheus&rdquo;) which fetches metrics and data from garden system components, cAdvisors, the virtual cluster control plane, and the Seeds&rsquo; aggregate Prometheus instances.
Its purpose is to provide an entrypoint for operators when debugging issues with components running in the garden cluster.
It also serves as the top-level aggregator of metering across a Gardener landscape.</p><p>If you would like to extend the configuration for this Garden Prometheus, you can create the <a href="https://github.com/prometheus-operator/prometheus-operator?tab=readme-ov-file#customresourcedefinitions"><code>prometheus-operator</code>&rsquo;s custom resources</a> and label them with <code>prometheus=garden</code>, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: monitoring.coreos.com/v1
</span></span><span style=display:flex><span>kind: ServiceMonitor
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    prometheus: garden
</span></span><span style=display:flex><span>  name: garden-my-component
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: my-component
</span></span><span style=display:flex><span>  endpoints:
</span></span><span style=display:flex><span>  - metricRelabelings:
</span></span><span style=display:flex><span>    - action: keep
</span></span><span style=display:flex><span>      regex: ^(metric1|metric2|...)$
</span></span><span style=display:flex><span>      sourceLabels:
</span></span><span style=display:flex><span>      - __name__
</span></span><span style=display:flex><span>    port: metrics
</span></span></code></pre></div><h6 id=alertmanager>Alertmanager</h6><p>By default, the <code>alertmanager-garden</code> deployed by <code>gardener-operator</code> does not come with any configuration.
It is the responsibility of the human operators to design and provide it.
This can be done by creating <code>monitoring.coreos.com/v1alpha1.AlertmanagerConfig</code> resources labeled with <code>alertmanager=garden</code> (read more about them <a href=https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/design.md#alertmanagerconfig>here</a>), for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: monitoring.coreos.com/v1alpha1
</span></span><span style=display:flex><span>kind: AlertmanagerConfig
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: config
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    alertmanager: garden
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  route:
</span></span><span style=display:flex><span>    receiver: dev-null
</span></span><span style=display:flex><span>    groupBy:
</span></span><span style=display:flex><span>    - alertname
</span></span><span style=display:flex><span>    - landscape
</span></span><span style=display:flex><span>    routes:
</span></span><span style=display:flex><span>    - continue: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      groupWait: 3m
</span></span><span style=display:flex><span>      groupInterval: 5m
</span></span><span style=display:flex><span>      repeatInterval: 12h
</span></span><span style=display:flex><span>      routes:
</span></span><span style=display:flex><span>      - receiver: ops
</span></span><span style=display:flex><span>        matchers:
</span></span><span style=display:flex><span>        - name: severity
</span></span><span style=display:flex><span>          value: warning
</span></span><span style=display:flex><span>          matchType: =
</span></span><span style=display:flex><span>        - name: topology
</span></span><span style=display:flex><span>          value: garden
</span></span><span style=display:flex><span>          matchType: =
</span></span><span style=display:flex><span>  receivers:
</span></span><span style=display:flex><span>  - name: dev-null
</span></span><span style=display:flex><span>  - name: ops
</span></span><span style=display:flex><span>    slackConfigs:
</span></span><span style=display:flex><span>    - apiURL: https://&lt;slack-api-url&gt;
</span></span><span style=display:flex><span>      channel: &lt;channel-name&gt;
</span></span><span style=display:flex><span>      username: Gardener-Alertmanager
</span></span><span style=display:flex><span>      iconEmoji: <span style=color:#a31515>&#34;:alert:&#34;</span>
</span></span><span style=display:flex><span>      title: <span style=color:#a31515>&#34;[{{ .Status | toUpper }}] Gardener Alert(s)&#34;</span>
</span></span><span style=display:flex><span>      text: <span style=color:#a31515>&#34;{{ range .Alerts }}*{{ .Annotations.summary }} ({{ .Status }})*\n{{ .Annotations.description }}\n\n{{ end }}&#34;</span>
</span></span><span style=display:flex><span>      sendResolved: <span style=color:#00f>true</span>
</span></span></code></pre></div><h4 id=care-reconcilerhttpsgithubcomgardenergardenertreemasterpkgoperatorcontrollergardencare><a href=https://github.com/gardener/gardener/tree/master/pkg/operator/controller/garden/care><code>Care</code> Reconciler</a></h4><p>This reconciler performs four &ldquo;care&rdquo; actions related to <code>Garden</code>s.</p><p>It maintains the following conditions:</p><ul><li><code>VirtualGardenAPIServerAvailable</code>: The <code>/healthz</code> endpoint of the garden&rsquo;s <code>virtual-garden-kube-apiserver</code> is called and considered healthy when it responds with <code>200 OK</code>.</li><li><code>RuntimeComponentsHealthy</code>: The conditions of the <code>ManagedResource</code>s applied to the runtime cluster are checked (e.g., <code>ResourcesApplied</code>).</li><li><code>VirtualComponentsHealthy</code>: The virtual components are considered healthy when the respective <code>Deployment</code>s (for example <code>virtual-garden-kube-apiserver</code>,<code>virtual-garden-kube-controller-manager</code>), and <code>Etcd</code>s (for example <code>virtual-garden-etcd-main</code>) exist and are healthy. Additionally, the conditions of the <code>ManagedResource</code>s applied to the virtual cluster are checked (e.g., <code>ResourcesApplied</code>).</li><li><code>ObservabilityComponentsHealthy</code>: This condition is considered healthy when the respective <code>Deployment</code>s (for example <code>plutono</code>) and <code>StatefulSet</code>s (for example <code>prometheus</code>, <code>vali</code>) exist and are healthy.</li></ul><p>If all checks for a certain condition are succeeded, then its <code>status</code> will be set to <code>True</code>.
Otherwise, it will be set to <code>False</code> or <code>Progressing</code>.</p><p>If at least one check fails and there is threshold configuration for the conditions (in <code>.controllers.gardenCare.conditionThresholds</code>), then the status will be set:</p><ul><li>to <code>Progressing</code> if it was <code>True</code> before.</li><li>to <code>Progressing</code> if it was <code>Progressing</code> before and the <code>lastUpdateTime</code> of the condition does not exceed the configured threshold duration yet.</li><li>to <code>False</code> if it was <code>Progressing</code> before and the <code>lastUpdateTime</code> of the condition exceeds the configured threshold duration.</li></ul><p>The condition thresholds can be used to prevent reporting issues too early just because there is a rollout or a short disruption.
Only if the unhealthiness persists for at least the configured threshold duration, then the issues will be reported (by setting the status to <code>False</code>).</p><p>In order to compute the condition statuses, this reconciler considers <code>ManagedResource</code>s (in the <code>garden</code> and <code>istio-system</code> namespace) and their status, see <a href=/docs/gardener/concepts/resource-manager/#conditions>this document</a> for more information.
The following table explains which <code>ManagedResource</code>s are considered for which condition type:</p><table><thead><tr><th>Condition Type</th><th><code>ManagedResource</code>s are considered when</th></tr></thead><tbody><tr><td><code>RuntimeComponentsHealthy</code></td><td><code>.spec.class=seed</code> and <code>care.gardener.cloud/condition-type</code> label either unset, or set to <code>RuntimeComponentsHealthy</code></td></tr><tr><td><code>VirtualComponentsHealthy</code></td><td><code>.spec.class</code> unset or <code>care.gardener.cloud/condition-type</code> label set to <code>VirtualComponentsHealthy</code></td></tr><tr><td><code>ObservabilityComponentsHealthy</code></td><td><code>care.gardener.cloud/condition-type</code> label set to <code>ObservabilityComponentsHealthy</code></td></tr></tbody></table><h4 id=reference-reconcilerhttpsgithubcomgardenergardenertreemasterpkgoperatorcontrollergardenreference><a href=https://github.com/gardener/gardener/tree/master/pkg/operator/controller/garden/reference><code>Reference</code> Reconciler</a></h4><p><code>Garden</code> objects may specify references to other objects in the Garden cluster which are required for certain features.
For example, operators can configure a secret for ETCD backup via <code>.spec.virtualCluster.etcd.main.backup.secretRef.name</code> or an audit policy <code>ConfigMap</code> via <code>.spec.virtualCluster.kubernetes.kubeAPIServer.auditConfig.auditPolicy.configMapRef.name</code>.
Such objects need a special protection against deletion requests as long as they are still being referenced by the <code>Garden</code>.</p><p>Therefore, this reconciler checks <code>Garden</code>s for referenced objects and adds the finalizer <code>gardener.cloud/reference-protection</code> to their <code>.metadata.finalizers</code> list.
The reconciled <code>Garden</code> also gets this finalizer to enable a proper garbage collection in case the <code>gardener-operator</code> is offline at the moment of an incoming deletion request.
When an object is not actively referenced anymore because the <code>Garden</code> specification has changed is in deletion, the controller will remove the added finalizer again so that the object can safely be deleted or garbage collected.</p><p>This reconciler inspects the following references:</p><ul><li>ETCD backup <code>Secret</code>s (<code>.spec.virtualCluster.etcd.main.backup.secretRef</code>)</li><li>Admission plugin kubeconfig <code>Secret</code>s (<code>.spec.virtualCluster.kubernetes.kubeAPIServer.admissionPlugins[].kubeconfigSecretName</code> and <code>.spec.virtualCluster.gardener.gardenerAPIServer.admissionPlugins[].kubeconfigSecretName</code>)</li><li>Authentication webhook kubeconfig <code>Secret</code>s (<code>.spec.virtualCluster.kubernetes.kubeAPIServer.authentication.webhook.kubeconfigSecretName</code>)</li><li>Audit webhook kubeconfig <code>Secret</code>s (<code>.spec.virtualCluster.kubernetes.kubeAPIServer.auditWebhook.kubeconfigSecretName</code> and <code>.spec.virtualCluster.gardener.gardenerAPIServer.auditWebhook.kubeconfigSecretName</code>)</li><li>SNI <code>Secret</code>s (<code>.spec.virtualCluster.kubernetes.kubeAPIServer.sni.secretName</code>)</li><li>Audit policy <code>ConfigMap</code>s (<code>.spec.virtualCluster.kubernetes.kubeAPIServer.auditConfig.auditPolicy.configMapRef.name</code> and <code>.spec.virtualCluster.gardener.gardenerAPIServer.auditConfig.auditPolicy.configMapRef.name</code>)</li></ul><p>Further checks might be added in the future.</p><h3 id=networkpolicy-controller-registrarhttpsgithubcomgardenergardenertreemasterpkgcontrollernetworkpolicy><a href=https://github.com/gardener/gardener/tree/master/pkg/controller/networkpolicy><code>NetworkPolicy</code> Controller Registrar</a></h3><p>This controller registers the same <code>NetworkPolicy</code> controller which is also used in <code>gardenlet</code>, please read it up <a href=/docs/gardener/concepts/gardenlet/#networkpolicy-controllerpkggardenletcontrollernetworkpolicy>here</a> for more details.</p><p>The registration happens as soon as the <code>Garden</code> resource is created.
It contains the networking information of the garden runtime cluster which is required configuration for the <code>NetworkPolicy</code> controller.</p><h2 id=webhooks>Webhooks</h2><p>As of today, the <code>gardener-operator</code> only has one webhook handler which is now described in more detail.</p><h3 id=validation>Validation</h3><p>This webhook handler validates <code>CREATE</code>/<code>UPDATE</code>/<code>DELETE</code> operations on <code>Garden</code> resources.
Simple validation is performed via <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation>standard CRD validation</a>.
However, more advanced validation is hard to express via these means and is performed by this webhook handler.</p><p>Furthermore, for deletion requests, it is validated that the <code>Garden</code> is annotated with a deletion confirmation annotation, namely <code>confirmation.gardener.cloud/deletion=true</code>.
Only if this annotation is present it allows the <code>DELETE</code> operation to pass.
This prevents users from accidental/undesired deletions.</p><p>Another validation is to check that there is only one <code>Garden</code> resource at a time.
It prevents creating a second <code>Garden</code> when there is already one in the system.</p><h3 id=defaulting>Defaulting</h3><p>This webhook handler mutates the <code>Garden</code> resource on <code>CREATE</code>/<code>UPDATE</code>/<code>DELETE</code> operations.
Simple defaulting is performed via <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#defaulting>standard CRD defaulting</a>.
However, more advanced defaulting is hard to express via these means and is performed by this webhook handler.</p><h2 id=using-garden-runtime-cluster-as-seed-cluster>Using Garden Runtime Cluster As Seed Cluster</h2><p>In production scenarios, you probably wouldn&rsquo;t use the Kubernetes cluster running <code>gardener-operator</code> and the Gardener control plane (called &ldquo;runtime cluster&rdquo;) as seed cluster at the same time.
However, such setup is technically possible and might simplify certain situations (e.g., development, evaluation, &mldr;).</p><p>If the runtime cluster is a seed cluster at the same time, <a href=/docs/gardener/concepts/gardenlet/#seed-controller><code>gardenlet</code>&rsquo;s <code>Seed</code> controller</a> will not manage the components which were already deployed (and reconciled) by <code>gardener-operator</code>.
As of today, this applies to:</p><ul><li><code>gardener-resource-manager</code></li><li><code>vpa-{admission-controller,recommender,updater}</code></li><li><code>hvpa-controller</code> (when <code>HVPA</code> feature gate is enabled)</li><li><code>etcd-druid</code></li><li><code>istio</code> control-plane</li><li><code>nginx-ingress-controller</code></li></ul><p>Those components are so-called &ldquo;seed system components&rdquo;.
In addition, there are a few observability components:</p><ul><li><code>fluent-operator</code></li><li><code>fluent-bit</code></li><li><code>vali</code></li><li><code>plutono</code></li><li><code>kube-state-metrics</code></li><li><code>prometheus-operator</code></li></ul><p>As all of these components are managed by <code>gardener-operator</code> in this scenario, the <code>gardenlet</code> just skips them.</p><blockquote><p>ℹ️ There is no need to configure anything - the <code>gardenlet</code> will automatically detect when its seed cluster is the garden runtime cluster at the same time.</p></blockquote><p>⚠️ Note that such setup requires that you upgrade the versions of <code>gardener-operator</code> and <code>gardenlet</code> in lock-step.
Otherwise, you might experience unexpected behaviour or issues with your seed or shoot clusters.</p><h2 id=credentials-rotation>Credentials Rotation</h2><p>The credentials rotation works in the same way as it does for <code>Shoot</code> resources, i.e. there are <code>gardener.cloud/operation</code> annotation values for starting or completing the rotation procedures.</p><p>For certificate authorities, <code>gardener-operator</code> generates one which is automatically rotated roughly each month (<code>ca-garden-runtime</code>) and several CAs which are <strong>NOT</strong> automatically rotated but only on demand.</p><p><strong>🚨 Hence, it is the responsibility of the (human) operator to regularly perform the credentials rotation.</strong></p><p>Please refer to <a href=/docs/gardener/shoot_credentials_rotation/#gardener-provided-credentials>this document</a> for more details. As of today, <code>gardener-operator</code> only creates the following types of credentials (i.e., some sections of the document don&rsquo;t apply for <code>Garden</code>s and can be ignored):</p><ul><li>certificate authorities (and related server and client certificates)</li><li>ETCD encryption key</li><li>observability password For Plutono</li><li><code>ServiceAccount</code> token signing key</li></ul><p>⚠️ Rotation of static <code>ServiceAccount</code> secrets is not supported since the <code>kube-controller-manager</code> does not enable the <code>serviceaccount-token</code> controller.</p><p>When the <code>ServiceAccount</code> token signing key rotation is in <code>Preparing</code> phase, then <code>gardener-operator</code> annotates all <code>Seed</code>s with <code>gardener.cloud/operation=renew-garden-access-secrets</code>.
This causes <code>gardenlet</code> to populate new <code>ServiceAccount</code> tokens for the garden cluster to all extensions, which are now signed with the new signing key.
Read more about it <a href=/docs/gardener/extensions/garden-api-access/#renewing-all-garden-access-secrets>here</a>.</p><p>Similarly, when the CA certificate rotation is in <code>Preparing</code> phase, then <code>gardener-operator</code> annotates all <code>Seed</code>s with <code>gardener.cloud/operation=renew-kubeconfig</code>.
This causes <code>gardenlet</code> to request a new client certificate for its garden cluster kubeconfig, which is now signed with the new client CA, and which also contains the new CA bundle for the server certificate verification.
Read more about it <a href=/docs/gardener/concepts/gardenlet/#rotate-certificates-using-bootstrap-kubeconfig>here</a>.</p><h2 id=migrating-an-existing-gardener-landscape-to-gardener-operator>Migrating an Existing Gardener Landscape to <code>gardener-operator</code></h2><p>Since <code>gardener-operator</code> was only developed in 2023, six years after the Gardener project initiation, most users probably already have an existing Gardener landscape.
The most prominent installation procedure is <a href=https://github.com/gardener/garden-setup>garden-setup</a>, however experience shows that most community members have developed their own tooling for managing the garden cluster and the Gardener control plane components.</p><blockquote><p>Consequently, providing a general migration guide is not possible since the detailed steps vary heavily based on how the components were set up previously.
As a result, this section can only highlight the most important caveats and things to know, while the concrete migration steps must be figured out individually based on the existing installation.</p><p>Please test your migration procedure thoroughly.
Note that in some cases it can be easier to set up a fresh landscape with <code>gardener-operator</code>, restore the ETCD data, switch the DNS records, and issue new credentials for all clients.</p></blockquote><p>Please make sure that you configure all your desired fields in the <a href=/docs/gardener/concepts/operator/#garden-resources><code>Garden</code> resource</a>.</p><h3 id=etcd>ETCD</h3><p><code>gardener-operator</code> leverages <code>etcd-druid</code> for managing the <code>virtual-garden-etcd-main</code> and <code>virtual-garden-etcd-events</code>, similar to how shoot cluster control planes are handled.
The <code>PersistentVolumeClaim</code> names differ slightly - for <code>virtual-garden-etcd-events</code> it&rsquo;s <code>virtual-garden-etcd-events-virtual-garden-etcd-events-0</code>, while for <code>virtual-garden-etcd-main</code> it&rsquo;s <code>main-virtual-garden-etcd-virtual-garden-etcd-main-0</code>.
The easiest approach for the migration is to make your existing ETCD volumes follow the same naming scheme.
Alternatively, backup your data, let <code>gardener-operator</code> take over ETCD, and then <a href=https://github.com/gardener/etcd-backup-restore/blob/master/docs/operations/manual_restoration.md>restore</a> your data to the new volume.</p><p>The backup bucket must be created separately, and its name as well as the respective credentials must be provided via the <code>Garden</code> resource in <code>.spec.virtualCluster.etcd.main.backup</code>.</p><h3 id=virtual-garden-kube-apiserver-deployment><code>virtual-garden-kube-apiserver</code> Deployment</h3><p><code>gardener-operator</code> deploys a <code>virtual-garden-kube-apiserver</code> into the runtime cluster.
This <code>virtual-garden-kube-apiserver</code> spans a new cluster, called the virtual cluster.
There are a few certificates and other credentials that should not change during the migration.
You have to prepare the environment accordingly by leveraging the <a href=/docs/gardener/secrets_management/#migrating-existing-secrets-to-secretsmanager>secret&rsquo;s manager capabilities</a>.</p><ul><li>The existing Cluster CA <code>Secret</code> should be labeled with <code>secrets-manager-use-data-for-name=ca</code>.</li><li>The existing Client CA <code>Secret</code> should be labeled with <code>secrets-manager-use-data-for-name=ca-client</code>.</li><li>The existing Front Proxy CA <code>Secret</code> should be labeled with <code>secrets-manager-use-data-for-name=ca-front-proxy</code>.</li><li>The existing Service Account Signing Key <code>Secret</code> should be labeled with <code>secrets-manager-use-data-for-name=service-account-key</code>.</li><li>The existing ETCD Encryption Key <code>Secret</code> should be labeled with <code>secrets-manager-use-data-for-name=kube-apiserver-etcd-encryption-key</code>.</li></ul><h3 id=virtual-garden-kube-apiserver-exposure><code>virtual-garden-kube-apiserver</code> Exposure</h3><p>The <code>virtual-garden-kube-apiserver</code> is exposed via a dedicated <code>istio-ingressgateway</code> deployed to namespace <code>virtual-garden-istio-ingress</code>.
The <code>virtual-garden-kube-apiserver</code> <code>Service</code> in the <code>garden</code> namespace is only of type <code>ClusterIP</code>.
Consequently, DNS records for this API server must target the load balancer IP of the <code>istio-ingressgateway</code>.</p><h3 id=virtual-garden-kubeconfig>Virtual Garden Kubeconfig</h3><p><code>gardener-operator</code> does not generate any static token or likewise for access to the virtual cluster.
Ideally, human users access it via OIDC only.
Alternatively, you can create an auto-rotated token that you can use for automation like CI/CD pipelines:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: shoot-access-virtual-garden
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    resources.gardener.cloud/purpose: token-requestor
</span></span><span style=display:flex><span>    resources.gardener.cloud/class: shoot
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    serviceaccount.resources.gardener.cloud/name: virtual-garden-user
</span></span><span style=display:flex><span>    serviceaccount.resources.gardener.cloud/namespace: kube-system
</span></span><span style=display:flex><span>    serviceaccount.resources.gardener.cloud/token-expiration-duration: 3h
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: managedresource-virtual-garden-access
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>stringData:
</span></span><span style=display:flex><span>  clusterrolebinding____gardener.cloud.virtual-garden-access.yaml: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    apiVersion: rbac.authorization.k8s.io/v1
</span></span></span><span style=display:flex><span><span style=color:#a31515>    kind: ClusterRoleBinding
</span></span></span><span style=display:flex><span><span style=color:#a31515>    metadata:
</span></span></span><span style=display:flex><span><span style=color:#a31515>      name: gardener.cloud.sap:virtual-garden
</span></span></span><span style=display:flex><span><span style=color:#a31515>    roleRef:
</span></span></span><span style=display:flex><span><span style=color:#a31515>      apiGroup: rbac.authorization.k8s.io
</span></span></span><span style=display:flex><span><span style=color:#a31515>      kind: ClusterRole
</span></span></span><span style=display:flex><span><span style=color:#a31515>      name: cluster-admin
</span></span></span><span style=display:flex><span><span style=color:#a31515>    subjects:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - kind: ServiceAccount
</span></span></span><span style=display:flex><span><span style=color:#a31515>      name: virtual-garden-user
</span></span></span><span style=display:flex><span><span style=color:#a31515>      namespace: kube-system</span>    
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: resources.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ManagedResource
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: virtual-garden-access
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  secretRefs:
</span></span><span style=display:flex><span>  - name: managedresource-virtual-garden-access
</span></span></code></pre></div><p>The <code>shoot-access-virtual-garden</code> <code>Secret</code> will get a <code>.data.token</code> field which can be used to authenticate against the virtual garden cluster.
See also <a href=/docs/gardener/concepts/resource-manager/#tokenrequestor-controller>this document</a> for more information about the <code>TokenRequestor</code>.</p><h3 id=gardener-apiserver><code>gardener-apiserver</code></h3><p>Similar to the <a href=/docs/gardener/concepts/operator/#virtual-garden-kube-apiserver-deployment><code>virtual-garden-kube-apiserver</code></a>, the <code>gardener-apiserver</code> also uses a few certificates and other credentials that should not change during the migration.
Again, you have to prepare the environment accordingly by leveraging the <a href=/docs/gardener/secrets_management/#migrating-existing-secrets-to-secretsmanager>secret&rsquo;s manager capabilities</a>.</p><ul><li>The existing ETCD Encryption Key <code>Secret</code> should be labeled with <code>secrets-manager-use-data-for-name=gardener-apiserver-etcd-encryption-key</code>.</li></ul><p>Also note that <code>gardener-operator</code> manages the <code>Service</code> and <code>Endpoints</code> resources for the <code>gardener-apiserver</code> in the virtual cluster within the <code>kube-system</code> namespace (<code>garden-setup</code> uses the <code>garden</code> namespace).</p><h2 id=local-development>Local Development</h2><p>The easiest setup is using a local <a href=https://kind.sigs.k8s.io/>KinD</a> cluster and the <a href=https://skaffold.dev/>Skaffold</a> based approach to deploy and develop the <code>gardener-operator</code>.</p><h3 id=setting-up-the-kind-cluster-runtime-cluster>Setting Up the KinD Cluster (runtime cluster)</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make kind-operator-up
</span></span></code></pre></div><p>This command sets up a new KinD cluster named <code>gardener-local</code> and stores the kubeconfig in the <code>./example/gardener-local/kind/operator/kubeconfig</code> file.</p><blockquote><p>It might be helpful to copy this file to <code>$HOME/.kube/config</code>, since you will need to target this KinD cluster multiple times.
Alternatively, make sure to set your <code>KUBECONFIG</code> environment variable to <code>./example/gardener-local/kind/operator/kubeconfig</code> for all future steps via <code>export KUBECONFIG=$PWD/example/gardener-local/kind/operator/kubeconfig</code>.</p></blockquote><p>All the following steps assume that you are using this kubeconfig.</p><h3 id=setting-up-gardener-operator>Setting Up Gardener Operator</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make operator-up
</span></span></code></pre></div><p>This will first build the base images (which might take a bit if you do it for the first time).
Afterwards, the Gardener Operator resources will be deployed into the cluster.</p><h3 id=developing-gardener-operator-optional>Developing Gardener Operator (Optional)</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make operator-dev
</span></span></code></pre></div><p>This is similar to <code>make operator-up</code> but additionally starts a <a href=https://skaffold.dev/docs/workflows/dev/>skaffold dev loop</a>.
After the initial deployment, skaffold starts watching source files.
Once it has detected changes, press any key to trigger a new build and deployment of the changed components.</p><h3 id=debugging-gardener-operator-optional>Debugging Gardener Operator (Optional)</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make operator-debug
</span></span></code></pre></div><p>This is similar to <code>make gardener-debug</code> but for Gardener Operator component. Please check <a href=/docs/gardener/deployment/getting_started_locally/#debugging-gardener>Debugging Gardener</a> for details.</p><h3 id=creating-a-garden>Creating a <code>Garden</code></h3><p>In order to create a garden, just run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f example/operator/20-garden.yaml
</span></span></code></pre></div><p>You can wait for the <code>Garden</code> to be ready by running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>./hack/usage/wait-for.sh garden local Reconciled
</span></span></code></pre></div><p>Alternatively, you can run <code>kubectl get garden</code> and wait for the <code>RECONCILED</code> status to reach <code>True</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME     RECONCILED    AGE
</span></span><span style=display:flex><span>garden   Progressing   1s
</span></span></code></pre></div><p>(Optional): Instead of creating above <code>Garden</code> resource manually, you could execute the e2e tests by running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make test-e2e-local-operator
</span></span></code></pre></div><h4 id=accessing-the-virtual-garden-cluster>Accessing the Virtual Garden Cluster</h4><p>⚠️ Please note that in this setup, the virtual garden cluster is not accessible by default when you download the kubeconfig and try to communicate with it.
The reason is that your host most probably cannot resolve the DNS name of the cluster.
Hence, if you want to access the virtual garden cluster, you have to run the following command which will extend your <code>/etc/hosts</code> file with the required information to make the DNS names resolvable:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat <span style=color:#a31515>&lt;&lt;EOF | sudo tee -a /etc/hosts
</span></span></span><span style=display:flex><span><span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515># Manually created to access local Gardener virtual garden cluster.
</span></span></span><span style=display:flex><span><span style=color:#a31515># TODO: Remove this again when the virtual garden cluster access is no longer required.
</span></span></span><span style=display:flex><span><span style=color:#a31515>127.0.0.1 api.virtual-garden.local.gardener.cloud
</span></span></span><span style=display:flex><span><span style=color:#a31515>EOF</span>
</span></span></code></pre></div><p>To access the virtual garden, you can acquire a <code>kubeconfig</code> by</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl -n garden get secret gardener -o jsonpath={.data.kubeconfig} | base64 -d &gt; /tmp/virtual-garden-kubeconfig
</span></span><span style=display:flex><span>kubectl --kubeconfig /tmp/virtual-garden-kubeconfig get namespaces
</span></span></code></pre></div><p>Note that this kubeconfig uses a token that has validity of <code>12h</code> only, hence it might expire and causing you to re-download the kubeconfig.</p><h3 id=deleting-the-garden>Deleting the <code>Garden</code></h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>./hack/usage/delete garden local
</span></span></code></pre></div><h3 id=tear-down-the-gardener-operator-environment>Tear Down the Gardener Operator Environment</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make operator-down
</span></span><span style=display:flex><span>make kind-operator-down
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-41370c41a8d3d04ad03bad8e1b4c465e>11 - Gardener Resource Manager</h1><div class=lead>Set of controllers with different responsibilities running once per seed and once per shoot</div><h2 id=overview>Overview</h2><p>Initially, the <code>gardener-resource-manager</code> was a project similar to the <a href=https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager>kube-addon-manager</a>.
It manages Kubernetes resources in a target cluster which means that it creates, updates, and deletes them.
Also, it makes sure that manual modifications to these resources are reconciled back to the desired state.</p><p>In the Gardener project we were using the kube-addon-manager since more than two years.
While we have progressed with our <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>extensibility story</a> (moving cloud providers out-of-tree), we had decided that the kube-addon-manager is no longer suitable for this use-case.
The problem with it is that it needs to have its managed resources on its file system.
This requires storing the resources in <code>ConfigMap</code>s or <code>Secret</code>s and mounting them to the kube-addon-manager pod during deployment time.
The <code>gardener-resource-manager</code> uses <code>CustomResourceDefinition</code>s which allows to dynamically add, change, and remove resources with immediate action and without the need to reconfigure the volume mounts/restarting the pod.</p><p>Meanwhile, the <code>gardener-resource-manager</code> has evolved to a more generic component comprising several controllers and webhook handlers.
It is deployed by gardenlet once per seed (in the <code>garden</code> namespace) and once per shoot (in the respective shoot namespaces in the seed).</p><h2 id=component-configuration>Component Configuration</h2><p>Similar to other Gardener components, the <code>gardener-resource-manager</code> uses a so-called component configuration file.
It allows specifying certain central settings like log level and formatting, client connection configuration, server ports and bind addresses, etc.
In addition, controllers and webhooks can be configured and sometimes even disabled.</p><p>Note that the very basic <code>ManagedResource</code> and health controllers cannot be disabled.</p><p>You can find an example configuration file <a href=https://github.com/gardener/gardener/blob/master/example/resource-manager/10-componentconfig.yaml>here</a>.</p><h2 id=controllers>Controllers</h2><h3 id=managedresource-controllerhttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollermanagedresource><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/managedresource><code>ManagedResource</code> Controller</a></h3><p>This controller watches custom objects called <code>ManagedResource</code>s in the <code>resources.gardener.cloud/v1alpha1</code> API group.
These objects contain references to secrets, which itself contain the resources to be managed.
The reason why a <code>Secret</code> is used to store the resources is that they could contain confidential information like credentials.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: managedresource-example1
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  objects.yaml: YXBpVmVyc2lvbjogdjEKa2luZDogQ29uZmlnTWFwCm1ldGFkYXRhOgogIG5hbWU6IHRlc3QtMTIzNAogIG5hbWVzcGFjZTogZGVmYXVsdAotLS0KYXBpVmVyc2lvbjogdjEKa2luZDogQ29uZmlnTWFwCm1ldGFkYXRhOgogIG5hbWU6IHRlc3QtNTY3OAogIG5hbWVzcGFjZTogZGVmYXVsdAo=
</span></span><span style=display:flex><span>    <span style=color:green># apiVersion: v1</span>
</span></span><span style=display:flex><span>    <span style=color:green># kind: ConfigMap</span>
</span></span><span style=display:flex><span>    <span style=color:green># metadata:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   name: test-1234</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   namespace: default</span>
</span></span><span style=display:flex><span>    <span style=color:green># ---</span>
</span></span><span style=display:flex><span>    <span style=color:green># apiVersion: v1</span>
</span></span><span style=display:flex><span>    <span style=color:green># kind: ConfigMap</span>
</span></span><span style=display:flex><span>    <span style=color:green># metadata:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   name: test-5678</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   namespace: default</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: resources.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ManagedResource
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  secretRefs:
</span></span><span style=display:flex><span>  - name: managedresource-example1
</span></span></code></pre></div><p>In the above example, the controller creates two <code>ConfigMap</code>s in the <code>default</code> namespace.
When a user is manually modifying them, they will be reconciled back to the desired state stored in the <code>managedresource-example</code> secret.</p><p>It is also possible to inject labels into all the resources:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: managedresource-example2
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  other-objects.yaml: YXBpVmVyc2lvbjogYXBwcy92MSAjIGZvciB2ZXJzaW9ucyBiZWZvcmUgMS45LjAgdXNlIGFwcHMvdjFiZXRhMgpraW5kOiBEZXBsb3ltZW50Cm1ldGFkYXRhOgogIG5hbWU6IG5naW54LWRlcGxveW1lbnQKc3BlYzoKICBzZWxlY3RvcjoKICAgIG1hdGNoTGFiZWxzOgogICAgICBhcHA6IG5naW54CiAgcmVwbGljYXM6IDIgIyB0ZWxscyBkZXBsb3ltZW50IHRvIHJ1biAyIHBvZHMgbWF0Y2hpbmcgdGhlIHRlbXBsYXRlCiAgdGVtcGxhdGU6CiAgICBtZXRhZGF0YToKICAgICAgbGFiZWxzOgogICAgICAgIGFwcDogbmdpbngKICAgIHNwZWM6CiAgICAgIGNvbnRhaW5lcnM6CiAgICAgIC0gbmFtZTogbmdpbngKICAgICAgICBpbWFnZTogbmdpbng6MS43LjkKICAgICAgICBwb3J0czoKICAgICAgICAtIGNvbnRhaW5lclBvcnQ6IDgwCg==
</span></span><span style=display:flex><span>    <span style=color:green># apiVersion: apps/v1</span>
</span></span><span style=display:flex><span>    <span style=color:green># kind: Deployment</span>
</span></span><span style=display:flex><span>    <span style=color:green># metadata:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   name: nginx-deployment</span>
</span></span><span style=display:flex><span>    <span style=color:green># spec:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   selector:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     matchLabels:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#       app: nginx</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   replicas: 2 # tells deployment to run 2 pods matching the template</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   template:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     metadata:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#       labels:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#         app: nginx</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     spec:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#       containers:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#       - name: nginx</span>
</span></span><span style=display:flex><span>    <span style=color:green>#         image: nginx:1.7.9</span>
</span></span><span style=display:flex><span>    <span style=color:green>#         ports:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#         - containerPort: 80</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: resources.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ManagedResource
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  secretRefs:
</span></span><span style=display:flex><span>  - name: managedresource-example2
</span></span><span style=display:flex><span>  injectLabels:
</span></span><span style=display:flex><span>    foo: bar
</span></span></code></pre></div><p>In this example, the label <code>foo=bar</code> will be injected into the <code>Deployment</code>, as well as into all created <code>ReplicaSet</code>s and <code>Pod</code>s.</p><h4 id=preventing-reconciliations>Preventing Reconciliations</h4><p>If a <code>ManagedResource</code> is annotated with <code>resources.gardener.cloud/ignore=true</code>, then it will be skipped entirely by the controller (no reconciliations or deletions of managed resources at all).
However, when the <code>ManagedResource</code> itself is deleted (for example when a shoot is deleted), then the annotation is not respected and all resources will be deleted as usual.
This feature can be helpful to temporarily patch/change resources managed as part of such <code>ManagedResource</code>.
Condition checks will be skipped for such <code>ManagedResource</code>s.</p><h4 id=modes>Modes</h4><p>The <code>gardener-resource-manager</code> can manage a resource in the following supported modes:</p><ul><li><code>Ignore</code><ul><li>The corresponding resource is removed from the <code>ManagedResource</code> status (<code>.status.resources</code>). No action is performed on the cluster.</li><li>The resource is no longer &ldquo;managed&rdquo; (updated or deleted).</li><li>The primary use case is a migration of a resource from one <code>ManagedResource</code> to another one.</li></ul></li></ul><p>The mode for a resource can be specified with the <code>resources.gardener.cloud/mode</code> annotation. The annotation should be specified in the encoded resource manifest in the Secret that is referenced by the <code>ManagedResource</code>.</p><h4 id=resource-class-and-reconcilation-scope>Resource Class and Reconcilation Scope</h4><p>By default, the <code>gardener-resource-manager</code> controller watches for <code>ManagedResource</code>s in all namespaces.
The <code>.sourceClientConnection.namespace</code> field in the component configuration restricts the watch to <code>ManagedResource</code>s in a single namespace only.
Note that this setting also affects all other controllers and webhooks since it&rsquo;s a central configuration.</p><p>A <code>ManagedResource</code> has an optional <code>.spec.class</code> field that allows it to indicate that it belongs to a given class of resources.
The <code>.controllers.resourceClass</code> field in the component configuration restricts the watch to <code>ManagedResource</code>s with the given <code>.spec.class</code>.
A default class is assumed if no class is specified.</p><p>For instance, the <code>gardener-resource-manager</code> which is deployed in the Shoot’s control plane namespace in the Seed does not specify a <code>.spec.class</code> and watches only for resources in the control plane namespace by specifying it in the <code>.sourceClientConnection.namespace</code> field.</p><p>If the <code>.spec.class</code> changes this means that the resources have to be handled by a different Gardener Resource Manager. That is achieved by:</p><ol><li>Cleaning all referenced resources by the Gardener Resource Manager that was responsible for the old class in its target cluster.</li><li>Creating all referenced resources by the Gardener Resource Manager that is responsible for the new class in its target cluster.</li></ol><h4 id=conditionshttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollerhealth><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/health>Conditions</a></h4><p>A <code>ManagedResource</code> has a <code>ManagedResourceStatus</code>, which has an array of Conditions. Conditions currently include:</p><table><thead><tr><th>Condition</th><th>Description</th></tr></thead><tbody><tr><td><code>ResourcesApplied</code></td><td><code>True</code> if all resources are applied to the target cluster</td></tr><tr><td><code>ResourcesHealthy</code></td><td><code>True</code> if all resources are present and healthy</td></tr><tr><td><code>ResourcesProgressing</code></td><td><code>False</code> if all resources have been fully rolled out</td></tr></tbody></table><p><code>ResourcesApplied</code> may be <code>False</code> when:</p><ul><li>the resource <code>apiVersion</code> is not known to the target cluster</li><li>the resource spec is invalid (for example the label value does not match the required regex for it)</li><li>&mldr;</li></ul><p><code>ResourcesHealthy</code> may be <code>False</code> when:</p><ul><li>the resource is not found</li><li>the resource is a Deployment and the Deployment does not have the minimum availability.</li><li>&mldr;</li></ul><p><code>ResourcesProgressing</code> may be <code>True</code> when:</p><ul><li>a <code>Deployment</code>, <code>StatefulSet</code> or <code>DaemonSet</code> has not been fully rolled out yet, i.e. not all replicas have been updated with the latest changes to <code>spec.template</code>.</li><li>there are still old <code>Pod</code>s belonging to an older <code>ReplicaSet</code> of a <code>Deployment</code> which are not terminated yet.</li></ul><p>Each Kubernetes resources has different notion for being healthy. For example, a Deployment is considered healthy if the controller observed its current revision and if the number of updated replicas is equal to the number of replicas.</p><p>The following <code>status.conditions</code> section describes a healthy <code>ManagedResource</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>conditions:
</span></span><span style=display:flex><span>- lastTransitionTime: <span style=color:#a31515>&#34;2022-05-03T10:55:39Z&#34;</span>
</span></span><span style=display:flex><span>  lastUpdateTime: <span style=color:#a31515>&#34;2022-05-03T10:55:39Z&#34;</span>
</span></span><span style=display:flex><span>  message: All resources are healthy.
</span></span><span style=display:flex><span>  reason: ResourcesHealthy
</span></span><span style=display:flex><span>  status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>  type: ResourcesHealthy
</span></span><span style=display:flex><span>- lastTransitionTime: <span style=color:#a31515>&#34;2022-05-03T10:55:36Z&#34;</span>
</span></span><span style=display:flex><span>  lastUpdateTime: <span style=color:#a31515>&#34;2022-05-03T10:55:36Z&#34;</span>
</span></span><span style=display:flex><span>  message: All resources have been fully rolled out.
</span></span><span style=display:flex><span>  reason: ResourcesRolledOut
</span></span><span style=display:flex><span>  status: <span style=color:#a31515>&#34;False&#34;</span>
</span></span><span style=display:flex><span>  type: ResourcesProgressing
</span></span><span style=display:flex><span>- lastTransitionTime: <span style=color:#a31515>&#34;2022-05-03T10:55:18Z&#34;</span>
</span></span><span style=display:flex><span>  lastUpdateTime: <span style=color:#a31515>&#34;2022-05-03T10:55:18Z&#34;</span>
</span></span><span style=display:flex><span>  message: All resources are applied.
</span></span><span style=display:flex><span>  reason: ApplySucceeded
</span></span><span style=display:flex><span>  status: <span style=color:#a31515>&#34;True&#34;</span>
</span></span><span style=display:flex><span>  type: ResourcesApplied
</span></span></code></pre></div><h4 id=ignoring-updates>Ignoring Updates</h4><p>In some cases, it is not desirable to update or re-apply some of the cluster components (for example, if customization is required or needs to be applied by the end-user).
For these resources, the annotation &ldquo;resources.gardener.cloud/ignore&rdquo; needs to be set to &ldquo;true&rdquo; or a truthy value (Truthy values are &ldquo;1&rdquo;, &ldquo;t&rdquo;, &ldquo;T&rdquo;, &ldquo;true&rdquo;, &ldquo;TRUE&rdquo;, &ldquo;True&rdquo;) in the corresponding managed resource secrets.
This can be done from the components that create the managed resource secrets, for example Gardener extensions or Gardener. Once this is done, the resource will be initially created and later ignored during reconciliation.</p><h4 id=finalizing-deletion-of-resources-after-grace-period>Finalizing Deletion of Resources After Grace Period</h4><p>When a <code>ManagedResource</code> is deleted, the controller deletes all managed resources from the target cluster.
In case the resources still have entries in their <code>.metadata.finalizers[]</code> list, they will remain stuck in the system until another entity removes the finalizers.
If you want the controller to forcefully finalize the deletion after some grace period (i.e., setting <code>.metadata.finalizers=null</code>), you can annotate the managed resources with <code>resources.gardener.cloud/finalize-deletion-after=&lt;duration></code>, e.g., <code>resources.gardener.cloud/finalize-deletion-after=1h</code>.</p><h4 id=preserving-replicas-or-resources-in-workload-resources>Preserving <code>replicas</code> or <code>resources</code> in Workload Resources</h4><p>The objects which are part of the <code>ManagedResource</code> can be annotated with:</p><ul><li><code>resources.gardener.cloud/preserve-replicas=true</code> in case the <code>.spec.replicas</code> field of workload resources like <code>Deployment</code>s, <code>StatefulSet</code>s, etc., shall be preserved during updates.</li><li><code>resources.gardener.cloud/preserve-resources=true</code> in case the <code>.spec.containers[*].resources</code> fields of all containers of workload resources like <code>Deployment</code>s, <code>StatefulSet</code>s, etc., shall be preserved during updates.</li></ul><blockquote><p>This can be useful if there are non-standard horizontal/vertical auto-scaling mechanisms in place.
Standard mechanisms like <code>HorizontalPodAutoscaler</code> or <code>VerticalPodAutoscaler</code> will be auto-recognized by <code>gardener-resource-manager</code>, i.e., in such cases the annotations are not needed.</p></blockquote><h4 id=origin>Origin</h4><p>All the objects managed by the resource manager get a dedicated annotation
<code>resources.gardener.cloud/origin</code> describing the <code>ManagedResource</code> object that describes
this object. The default format is <code>&lt;namespace>/&lt;objectname></code>.</p><p>In multi-cluster scenarios (the <code>ManagedResource</code> objects are maintained in a
cluster different from the one the described objects are managed), it might
be useful to include the cluster identity, as well.</p><p>This can be enforced by setting the <code>.controllers.clusterID</code> field in the component configuration.
Here, several possibilities are supported:</p><ul><li>given a direct value: use this as id for the source cluster.</li><li><code>&lt;cluster></code>: read the cluster identity from a <code>cluster-identity</code> config map
in the <code>kube-system</code> namespace (attribute <code>cluster-identity</code>). This is
automatically maintained in all clusters managed or involved in a gardener landscape.</li><li><code>&lt;default></code>: try to read the cluster identity from the config map. If not found,
no identity is used.</li><li>empty string: no cluster identity is used (completely cluster local scenarios).</li></ul><p>By default, cluster id is not used. If cluster id is specified, the format is <code>&lt;cluster id>:&lt;namespace>/&lt;objectname></code>.</p><p>In addition to the origin annotation, all objects managed by the resource manager get a dedicated label <code>resources.gardener.cloud/managed-by</code>. This label can be used to describe these objects with a <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/>selector</a>. By default it is set to &ldquo;gardener&rdquo;, but this can be overwritten by setting the <code>.conrollers.managedResources.managedByLabelValue</code> field in the component configuration.</p><h3 id=health-controllerhttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollerhealth><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/health><code>health</code> Controller</a></h3><p>This controller processes <code>ManagedResource</code>s that were reconciled by the main <a href=/docs/gardener/concepts/resource-manager/#managedResource-controller>ManagedResource Controller</a> at least once.
Its main job is to perform checks for maintaining the well <a href=/docs/gardener/concepts/resource-manager/#conditions>known conditions</a> <code>ResourcesHealthy</code> and <code>ResourcesProgressing</code>.</p><h4 id=progressing-checks>Progressing Checks</h4><p>In Kubernetes, applied changes must usually be rolled out first, e.g. when changing the base image in a <code>Deployment</code>.
Progressing checks detect ongoing roll-outs and report them in the <code>ResourcesProgressing</code> condition of the corresponding <code>ManagedResource</code>.</p><p>The following object kinds are considered for progressing checks:</p><ul><li><code>DaemonSet</code></li><li><code>Deployment</code></li><li><code>StatefulSet</code></li><li><a href=https://github.com/prometheus-operator/prometheus-operator><code>Prometheus</code></a></li><li><a href=https://github.com/prometheus-operator/prometheus-operator><code>Alertmanager</code></a></li><li><a href=https://github.com/gardener/cert-management><code>Certificate</code></a></li><li><a href=https://github.com/gardener/cert-management><code>Issuer</code></a></li></ul><h4 id=health-checks>Health Checks</h4><p><code>gardener-resource-manager</code> can evaluate the health of specific resources, often by consulting their conditions.
Health check results are regularly updated in the <code>ResourcesHealthy</code> condition of the corresponding <code>ManagedResource</code>.</p><p>The following object kinds are considered for health checks:</p><ul><li><code>CustomResourceDefinition</code></li><li><code>DaemonSet</code></li><li><code>Deployment</code></li><li><code>Job</code></li><li><code>Pod</code></li><li><code>ReplicaSet</code></li><li><code>ReplicationController</code></li><li><code>Service</code></li><li><code>StatefulSet</code></li><li><a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler><code>VerticalPodAutoscaler</code></a></li><li><a href=https://github.com/prometheus-operator/prometheus-operator><code>Prometheus</code></a></li><li><a href=https://github.com/prometheus-operator/prometheus-operator><code>Alertmanager</code></a></li><li><a href=https://github.com/gardener/cert-management><code>Certificate</code></a></li><li><a href=https://github.com/gardener/cert-management><code>Issuer</code></a></li></ul><h4 id=skipping-health-check>Skipping Health Check</h4><p>If a resource owned by a <code>ManagedResource</code> is annotated with <code>resources.gardener.cloud/skip-health-check=true</code>, then the resource will be skipped during health checks by the <code>health</code> controller. The <code>ManagedResource</code> conditions will not reflect the health condition of this resource anymore. The <code>ResourcesProgressing</code> condition will also be set to <code>False</code>.</p><h3 id=garbage-collector-for-immutable-configmapssecretshttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollergarbagecollector><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/garbagecollector>Garbage Collector For Immutable <code>ConfigMap</code>s/<code>Secret</code>s</a></h3><p>In Kubernetes, workload resources (e.g., <code>Pod</code>s) can mount <code>ConfigMap</code>s or <code>Secret</code>s or reference them via environment variables in containers.
Typically, when the content of such a <code>ConfigMap</code>/<code>Secret</code> gets changed, then the respective workload is usually not dynamically reloading the configuration, i.e., a restart is required.
The most commonly used approach is probably having the so-called <a href=https://helm.sh/docs/howto/charts_tips_and_tricks/#automatically-roll-deployments>checksum annotations in the pod template</a>, which makes Kubernetes recreate the pod if the checksum changes.
However, it has the downside that old, still running versions of the workload might not be able to properly work with the already updated content in the <code>ConfigMap</code>/<code>Secret</code>, potentially causing application outages.</p><p>In order to protect users from such outages (and also to improve the performance of the cluster), the Kubernetes community provides the <a href=https://kubernetes.io/docs/concepts/configuration/configmap/#configmap-immutable>&ldquo;immutable <code>ConfigMap</code>s/<code>Secret</code>s feature&rdquo;</a>.
Enabling immutability requires <code>ConfigMap</code>s/<code>Secret</code>s to have unique names.
Having unique names requires the client to delete <code>ConfigMap</code>s/<code>Secret</code>s no longer in use.</p><p>In order to provide a similarly lightweight experience for clients (compared to the well-established checksum annotation approach), the <code>gardener-resource-manager</code> features an optional garbage collector controller (disabled by default).
The purpose of this controller is cleaning up such immutable <code>ConfigMap</code>s/<code>Secret</code>s if they are no longer in use.</p><h4 id=how-does-the-garbage-collector-work>How Does the Garbage Collector Work?</h4><p>The following algorithm is implemented in the GC controller:</p><ol><li>List all <code>ConfigMap</code>s and <code>Secret</code>s labeled with <code>resources.gardener.cloud/garbage-collectable-reference=true</code>.</li><li>List all <code>Deployment</code>s, <code>StatefulSet</code>s, <code>DaemonSet</code>s, <code>Job</code>s, <code>CronJob</code>s, <code>Pod</code>s, <code>ManagedResource</code>s and for each of them:<ul><li>iterate over the <code>.metadata.annotations</code> and for each of them:<ul><li>If the annotation key follows the <code>reference.resources.gardener.cloud/{configmap,secret}-&lt;hash></code> scheme and the value equals <code>&lt;name></code>, then consider it as &ldquo;in-use&rdquo;.</li></ul></li></ul></li><li>Delete all <code>ConfigMap</code>s and <code>Secret</code>s not considered as &ldquo;in-use&rdquo;.</li></ol><p>Consequently, clients need to:</p><ol><li><p>Create immutable <code>ConfigMap</code>s/<code>Secret</code>s with unique names (e.g., a checksum suffix based on the <code>.data</code>).</p></li><li><p>Label such <code>ConfigMap</code>s/<code>Secret</code>s with <code>resources.gardener.cloud/garbage-collectable-reference=true</code>.</p></li><li><p>Annotate their workload resources with <code>reference.resources.gardener.cloud/{configmap,secret}-&lt;hash>=&lt;name></code> for all <code>ConfigMap</code>s/<code>Secret</code>s used by the containers of the respective <code>Pod</code>s.</p><p>⚠️ Add such annotations to <code>.metadata.annotations</code>, as well as to all templates of other resources (e.g., <code>.spec.template.metadata.annotations</code> in <code>Deployment</code>s or <code>.spec.jobTemplate.metadata.annotations</code> and <code>.spec.jobTemplate.spec.template.metadata.annotations</code> for <code>CronJob</code>s.
This ensures that the GC controller does not unintentionally consider <code>ConfigMap</code>s/<code>Secret</code>s as &ldquo;not in use&rdquo; just because there isn&rsquo;t a <code>Pod</code> referencing them anymore (e.g., they could still be used by a <code>Deployment</code> scaled down to <code>0</code>).</p></li></ol><p>ℹ️ For the last step, there is a helper function <code>InjectAnnotations</code> in the <code>pkg/controller/garbagecollector/references</code>, which you can use for your convenience.</p><p><strong>Example:</strong></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: test-1234
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    resources.gardener.cloud/garbage-collectable-reference: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: test-5678
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    resources.gardener.cloud/garbage-collectable-reference: <span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    reference.resources.gardener.cloud/configmap-82a3537f: test-5678
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: nginx
</span></span><span style=display:flex><span>    image: nginx:1.14.2
</span></span><span style=display:flex><span>    terminationGracePeriodSeconds: 2
</span></span></code></pre></div><p>The GC controller would delete the <code>ConfigMap/test-1234</code> because it is considered as not &ldquo;in-use&rdquo;.</p><p>ℹ️ If the GC controller is activated then the <code>ManagedResource</code> controller will no longer delete <code>ConfigMap</code>s/<code>Secret</code>s having the above label.</p><h4 id=how-to-activate-the-garbage-collector>How to Activate the Garbage Collector?</h4><p>The GC controller can be activated by setting the <code>.controllers.garbageCollector.enabled</code> field to <code>true</code> in the component configuration.</p><h3 id=tokeninvalidator-controllerhttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollertokeninvalidator><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/tokeninvalidator>TokenInvalidator Controller</a></h3><p>The Kubernetes community is slowly transitioning from static <code>ServiceAccount</code> token <code>Secret</code>s to <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection><code>ServiceAccount</code> Token Volume Projection</a>.
Typically, when you create a <code>ServiceAccount</code></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ServiceAccount
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: default
</span></span></code></pre></div><p>then the <a href=https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/serviceaccount/tokens_controller.go><code>serviceaccount-token</code></a> controller (part of <code>kube-controller-manager</code>) auto-generates a <code>Secret</code> with a static token:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>   annotations:
</span></span><span style=display:flex><span>      kubernetes.io/service-account.name: default
</span></span><span style=display:flex><span>      kubernetes.io/service-account.uid: 86e98645-2e05-11e9-863a-b2d4d086dd5a)
</span></span><span style=display:flex><span>   name: default-token-ntxs9
</span></span><span style=display:flex><span>type: kubernetes.io/service-account-token
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>   ca.crt: base64(cluster-ca-cert)
</span></span><span style=display:flex><span>   namespace: base64(namespace)
</span></span><span style=display:flex><span>   token: base64(static-jwt-token)
</span></span></code></pre></div><p>Unfortunately, when using <code>ServiceAccount</code> Token Volume Projection in a <code>Pod</code>, this static token is actually not used at all:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nginx
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  serviceAccountName: default
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - image: nginx
</span></span><span style=display:flex><span>    name: nginx
</span></span><span style=display:flex><span>    volumeMounts:
</span></span><span style=display:flex><span>    - mountPath: /var/run/secrets/tokens
</span></span><span style=display:flex><span>      name: token
</span></span><span style=display:flex><span>  volumes:
</span></span><span style=display:flex><span>  - name: token
</span></span><span style=display:flex><span>    projected:
</span></span><span style=display:flex><span>      sources:
</span></span><span style=display:flex><span>      - serviceAccountToken:
</span></span><span style=display:flex><span>          path: token
</span></span><span style=display:flex><span>          expirationSeconds: 7200
</span></span></code></pre></div><p>While the <code>Pod</code> is now using an expiring and auto-rotated token, the static token is still generated and valid.</p><p>There is neither a way of preventing <code>kube-controller-manager</code> to generate such static tokens, nor a way to proactively remove or invalidate them:</p><ul><li><a href=https://github.com/kubernetes/kubernetes/issues/77599>https://github.com/kubernetes/kubernetes/issues/77599</a></li><li><a href=https://github.com/kubernetes/kubernetes/issues/77600>https://github.com/kubernetes/kubernetes/issues/77600</a></li></ul><p>Disabling the <code>serviceaccount-token</code> controller is an option, however, especially in the Gardener context it may either break end-users or it may not even be possible to control such settings.
Also, even if a future Kubernetes version supports native configuration of the above behaviour, Gardener still supports older versions which won&rsquo;t get such features but need a solution as well.</p><p>This is where the <em>TokenInvalidator</em> comes into play:
Since it is not possible to prevent <code>kube-controller-manager</code> from generating static <code>ServiceAccount</code> <code>Secret</code>s, the <em>TokenInvalidator</em> is, as its name suggests, just invalidating these tokens.
It considers all such <code>Secret</code>s belonging to <code>ServiceAccount</code>s with <code>.automountServiceAccountToken=false</code>.
By default, all namespaces in the target cluster are watched, however, this can be configured by specifying the <code>.targetClientConnection.namespace</code> field in the component configuration.
Note that this setting also affects all other controllers and webhooks since it&rsquo;s a central configuration.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ServiceAccount
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-serviceaccount
</span></span><span style=display:flex><span>automountServiceAccountToken: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>This will result in a static <code>ServiceAccount</code> token secret whose <code>token</code> value is invalid:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    kubernetes.io/service-account.name: my-serviceaccount
</span></span><span style=display:flex><span>    kubernetes.io/service-account.uid: 86e98645-2e05-11e9-863a-b2d4d086dd5a
</span></span><span style=display:flex><span>  name: my-serviceaccount-token-ntxs9
</span></span><span style=display:flex><span>type: kubernetes.io/service-account-token
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  ca.crt: base64(cluster-ca-cert)
</span></span><span style=display:flex><span>  namespace: base64(namespace)
</span></span><span style=display:flex><span>  token: AAAA
</span></span></code></pre></div><p>Any attempt to regenerate the token or creating a new such secret will again make the component invalidating it.</p><blockquote><p>You can opt-out of this behaviour for <code>ServiceAccount</code>s setting <code>.automountServiceAccountToken=false</code> by labeling them with <code>token-invalidator.resources.gardener.cloud/skip=true</code>.</p></blockquote><p>In order to enable the <em>TokenInvalidator</em> you have to set both <code>.controllers.tokenValidator.enabled=true</code> and <code>.webhooks.tokenValidator.enabled=true</code> in the component configuration.</p><p>The below graphic shows an overview of the Token Invalidator for Service account secrets in the Shoot cluster.
<img src=/__resources/resource-manager-token-invalidator_cff92f.jpg alt=image></p><h3 id=tokenrequestor-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollertokenrequestor><a href=https://github.com/gardener/gardener/tree/master/pkg/controller/tokenrequestor>TokenRequestor Controller</a></h3><p>This controller provides the service to create and auto-renew tokens via the <a href=https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-request-v1/><code>TokenRequest</code> API</a>.</p><p>It provides a functionality similar to the kubelet&rsquo;s <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a>.
It was created to handle the special case of issuing tokens to pods that run in a different cluster than the API server they communicate with (hence, using the native token volume projection feature is not possible).</p><p>The controller differentiates between <code>source cluster</code> and <code>target cluster</code>.
The <code>source cluster</code> hosts the <code>gardener-resource-manager</code> pod. Secrets in this cluster are watched and modified by the controller.
The <code>target cluster</code> <em>can</em> be configured to point to another cluster. The existence of ServiceAccounts are ensured and token requests are issued against the target.
When the <code>gardener-resource-manager</code> is deployed next to the Shoot&rsquo;s controlplane in the Seed, the <code>source cluster</code> is the Seed while the <code>target cluster</code> points to the Shoot.</p><h4 id=reconciliation-loop>Reconciliation Loop</h4><p>This controller reconciles <code>Secret</code>s in all namespaces in the source cluster with the label: <code>resources.gardener.cloud/purpose=token-requestor</code>.
See <a href=https://github.com/gardener/gardener/blob/master/example/resource-manager/30-secret-tokenrequestor.yaml>this YAML file</a> for an example of the secret.</p><p>The controller ensures a <code>ServiceAccount</code> exists in the target cluster as specified in the annotations of the <code>Secret</code> in the source cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>serviceaccount.resources.gardener.cloud/name: &lt;sa-name&gt;
</span></span><span style=display:flex><span>serviceaccount.resources.gardener.cloud/namespace: &lt;sa-namespace&gt;
</span></span></code></pre></div><p>You can optionally annotate the <code>Secret</code> with <code>serviceaccount.resources.gardener.cloud/labels</code>, e.g. <code>serviceaccount.resources.gardener.cloud/labels={"some":"labels","foo":"bar"}</code>.
This will make the <code>ServiceAccount</code> getting labelled accordingly.</p><p>The requested tokens will act with the privileges which are assigned to this <code>ServiceAccount</code>.</p><p>The controller will then request a token via the <a href=https://kubernetes.io/docs/reference/kubernetes-api/authentication-resources/token-request-v1/><code>TokenRequest</code> API</a> and populate it into the <code>.data.token</code> field to the <code>Secret</code> in the source cluster.</p><p>Alternatively, the client can provide a raw kubeconfig (in YAML or JSON format) via the <code>Secret</code>&rsquo;s <code>.data.kubeconfig</code> field.
The controller will then populate the requested token in the kubeconfig for the user used in the <code>.current-context</code>.
For example, if <code>.data.kubeconfig</code> is</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: AAAA
</span></span><span style=display:flex><span>    server: some-server-url
</span></span><span style=display:flex><span>  name: shoot--foo--bar
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: shoot--foo--bar
</span></span><span style=display:flex><span>    user: shoot--foo--bar-token
</span></span><span style=display:flex><span>  name: shoot--foo--bar
</span></span><span style=display:flex><span>current-context: shoot--foo--bar
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>preferences: {}
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: shoot--foo--bar-token
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    token: <span style=color:#a31515>&#34;&#34;</span>
</span></span></code></pre></div><p>then the <code>.users[0].user.token</code> field of the kubeconfig will be updated accordingly.</p><p>The controller also adds an annotation to the <code>Secret</code> to keep track when to renew the token before it expires.
By default, the tokens are issued to expire after 12 hours. The expiration time can be set with the following annotation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>serviceaccount.resources.gardener.cloud/token-expiration-duration: 6h
</span></span></code></pre></div><p>It automatically renews once 80% of the lifetime is reached, or after <code>24h</code>.</p><p>Optionally, the controller can also populate the token into a <code>Secret</code> in the target cluster. This can be requested by annotating the <code>Secret</code> in the source cluster with:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>token-requestor.resources.gardener.cloud/target-secret-name: <span style=color:#a31515>&#34;foo&#34;</span>
</span></span><span style=display:flex><span>token-requestor.resources.gardener.cloud/target-secret-namespace: <span style=color:#a31515>&#34;bar&#34;</span>
</span></span></code></pre></div><p>Overall, the TokenRequestor controller provides credentials with limited lifetime (JWT tokens)
used by Shoot control plane components running in the Seed to talk to the Shoot API Server.
Please see the graphic below:</p><p><img src=/__resources/resource-manager-projected-token-controlplane-to-shoot-apiserver_9561b5.jpg alt=image></p><blockquote><p>ℹ️ Generally, the controller can run with multiple instances in different components.
For example, <code>gardener-resource-manager</code> might run the <code>TokenRequestor</code> controller, but <code>gardenlet</code> might run it, too.
In order to differentiate which instance of the controller is responsible for a <code>Secret</code>, it can be labeled with <code>resources.gardener.cloud/class=&lt;class></code>.
The <code>&lt;class></code> must be configured in the respective controller, otherwise it will be responsible for all <code>Secret</code>s no matter whether they have the label or not.</p></blockquote><h3 id=kubelet-server-certificatesigningrequest-approverhttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollercsrapprover><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/csrapprover>Kubelet Server <code>CertificateSigningRequest</code> Approver</a></h3><p>Gardener configures the kubelets such that they request two certificates via the <code>CertificateSigningRequest</code> API:</p><ol><li>client certificate for communicating with the <code>kube-apiserver</code></li><li>server certificate for serving its HTTPS server</li></ol><p>For client certificates, the <code>kubernetes.io/kube-apiserver-client-kubelet</code> signer is used (see <a href=https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#kubernetes-signers>Certificate Signing Requests</a> for more details).
The <code>kube-controller-manager</code>&rsquo;s <code>csrapprover</code> controller is responsible for auto-approving such <code>CertificateSigningRequest</code>s so that the respective certificates can be issued.</p><p>For server certificates, the <code>kubernetes.io/kubelet-serving</code> signer is used.
Unfortunately, the <code>kube-controller-manager</code> is not able to auto-approve such <code>CertificateSigningRequest</code>s (see <a href=https://github.com/kubernetes/kubernetes/issues/73356>kubernetes/kubernetes#73356</a> for details).</p><p>That&rsquo;s the motivation for having this controller as part of <code>gardener-resource-manager</code>.
It watches <code>CertificateSigningRequest</code>s with the <code>kubernetes.io/kubelet-serving</code> signer and auto-approves them when all the following conditions are met:</p><ul><li>The <code>.spec.username</code> is prefixed with <code>system:node:</code>.</li><li>There must be at least one DNS name or IP address as part of the certificate SANs.</li><li>The common name in the CSR must match the <code>.spec.username</code>.</li><li>The organization in the CSR must only contain <code>system:nodes</code>.</li><li>There must be a <code>Node</code> object with the same name in the shoot cluster.</li><li>There must be exactly one <code>Machine</code> for the node in the seed cluster.</li><li>The DNS names part of the SANs must be equal to all <code>.status.addresses[]</code> of type <code>Hostname</code> in the <code>Node</code>.</li><li>The IP addresses part of the SANs must be equal to all <code>.status.addresses[]</code> of type <code>InternalIP</code> in the <code>Node</code>.</li></ul><p>If any one of these requirements is violated, the <code>CertificateSigningRequest</code> will be denied.
Otherwise, once approved, the <code>kube-controller-manager</code>&rsquo;s <code>csrsigner</code> controller will issue the requested certificate.</p><h3 id=networkpolicy-controllerhttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollernetworkpolicy><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/networkpolicy><code>NetworkPolicy</code> Controller</a></h3><p>This controller reconciles <code>Service</code>s with a non-empty <code>.spec.podSelector</code>.
It creates two <code>NetworkPolicy</code>s for each port in the <code>.spec.ports[]</code> list.
For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardener-resource-manager
</span></span><span style=display:flex><span>  namespace: a
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    app: gardener-resource-manager
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>  - name: server
</span></span><span style=display:flex><span>    port: 443
</span></span><span style=display:flex><span>    protocol: TCP
</span></span><span style=display:flex><span>    targetPort: 10250
</span></span></code></pre></div><p>leads to</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    gardener.cloud/description: Allows ingress TCP traffic to port 10250 for pods
</span></span><span style=display:flex><span>      selected by the a/gardener-resource-manager service selector from pods running
</span></span><span style=display:flex><span>      in namespace a labeled with map[networking.resources.gardener.cloud/to-gardener-resource-manager-tcp-10250:allowed].
</span></span><span style=display:flex><span>  name: ingress-to-gardener-resource-manager-tcp-10250
</span></span><span style=display:flex><span>  namespace: a
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>  - from:
</span></span><span style=display:flex><span>    - podSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          networking.resources.gardener.cloud/to-gardener-resource-manager-tcp-10250: allowed
</span></span><span style=display:flex><span>    ports:
</span></span><span style=display:flex><span>    - port: 10250
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: gardener-resource-manager
</span></span><span style=display:flex><span>  policyTypes:
</span></span><span style=display:flex><span>  - Ingress
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    gardener.cloud/description: Allows egress TCP traffic to port 10250 from pods
</span></span><span style=display:flex><span>      running in namespace a labeled with map[networking.resources.gardener.cloud/to-gardener-resource-manager-tcp-10250:allowed]
</span></span><span style=display:flex><span>      to pods selected by the a/gardener-resource-manager service selector.
</span></span><span style=display:flex><span>  name: egress-to-gardener-resource-manager-tcp-10250
</span></span><span style=display:flex><span>  namespace: a
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  egress:
</span></span><span style=display:flex><span>  - to:
</span></span><span style=display:flex><span>    - podSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          app: gardener-resource-manager
</span></span><span style=display:flex><span>    ports:
</span></span><span style=display:flex><span>    - port: 10250
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      networking.resources.gardener.cloud/to-gardener-resource-manager-tcp-10250: allowed
</span></span><span style=display:flex><span>  policyTypes:
</span></span><span style=display:flex><span>  - Egress
</span></span></code></pre></div><p>A component that initiates the connection to <code>gardener-resource-manager</code>&rsquo;s <code>tcp/10250</code> port can now be labeled with <code>networking.resources.gardener.cloud/to-gardener-resource-manager-tcp-10250=allowed</code>.
That&rsquo;s all this component needs to do - it does not need to create any <code>NetworkPolicy</code>s itself.</p><h4 id=cross-namespace-communication>Cross-Namespace Communication</h4><p>Apart from this &ldquo;simple&rdquo; case where both communicating components run in the same namespace <code>a</code>, there is also the cross-namespace communication case.
With above example, let&rsquo;s say there are components running in another namespace <code>b</code>, and they would like to initiate the communication with <code>gardener-resource-manager</code> in <code>a</code>.
To cover this scenario, the <code>Service</code> can be annotated with <code>networking.resources.gardener.cloud/namespace-selectors='[{"matchLabels":{"kubernetes.io/metadata.name":"b"}}]'</code>.</p><blockquote><p>Note that you can specify multiple namespace selectors in this annotation which are OR-ed.</p></blockquote><p>This will make the controller create additional <code>NetworkPolicy</code>s as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    gardener.cloud/description: Allows ingress TCP traffic to port 10250 for pods selected
</span></span><span style=display:flex><span>      by the a/gardener-resource-manager service selector from pods running in namespace b
</span></span><span style=display:flex><span>      labeled with map[networking.resources.gardener.cloud/to-a-gardener-resource-manager-tcp-10250:allowed].
</span></span><span style=display:flex><span>  name: ingress-to-gardener-resource-manager-tcp-10250-from-b
</span></span><span style=display:flex><span>  namespace: a
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>  - from:
</span></span><span style=display:flex><span>    - namespaceSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          kubernetes.io/metadata.name: b
</span></span><span style=display:flex><span>      podSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          networking.resources.gardener.cloud/to-a-gardener-resource-manager-tcp-10250: allowed
</span></span><span style=display:flex><span>    ports:
</span></span><span style=display:flex><span>    - port: 10250
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: gardener-resource-manager
</span></span><span style=display:flex><span>  policyTypes:
</span></span><span style=display:flex><span>  - Ingress
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    gardener.cloud/description: Allows egress TCP traffic to port 10250 from pods running in
</span></span><span style=display:flex><span>      namespace b labeled with map[networking.resources.gardener.cloud/to-a-gardener-resource-manager-tcp-10250:allowed]
</span></span><span style=display:flex><span>      to pods selected by the a/gardener-resource-manager service selector.
</span></span><span style=display:flex><span>  name: egress-to-a-gardener-resource-manager-tcp-10250
</span></span><span style=display:flex><span>  namespace: b
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  egress:
</span></span><span style=display:flex><span>  - to:
</span></span><span style=display:flex><span>    - namespaceSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          kubernetes.io/metadata.name: a
</span></span><span style=display:flex><span>      podSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          app: gardener-resource-manager
</span></span><span style=display:flex><span>    ports:
</span></span><span style=display:flex><span>    - port: 10250
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      networking.resources.gardener.cloud/to-a-gardener-resource-manager-tcp-10250: allowed
</span></span><span style=display:flex><span>  policyTypes:
</span></span><span style=display:flex><span>  - Egress
</span></span></code></pre></div><p>The components in namespace <code>b</code> now need to be labeled with <code>networking.resources.gardener.cloud/to-a-gardener-resource-manager-tcp-10250=allowed</code>, but that&rsquo;s already it.</p><blockquote><p>Obviously, this approach also works for namespace selectors different from <code>kubernetes.io/metadata.name</code> to cover scenarios where the namespace name is not known upfront or where multiple namespaces with a similar label are relevant.
The controller creates two dedicated policies for each namespace matching the selectors.</p></blockquote><h4 id=service-targets-in-multiple-namespaces><code>Service</code> Targets In Multiple Namespaces</h4><p>Finally, let&rsquo;s say there is a <code>Service</code> called <code>example</code> which exists in different namespaces whose names are not static (e.g., <code>foo-1</code>, <code>foo-2</code>), and a component in namespace <code>bar</code> wants to initiate connections with all of them.</p><p>The <code>example</code> <code>Service</code>s in these namespaces can now be annotated with <code>networking.resources.gardener.cloud/namespace-selectors='[{"matchLabels":{"kubernetes.io/metadata.name":"bar"}}]'</code>.
As a consequence, the component in namespace <code>bar</code> now needs to be labeled with <code>networking.resources.gardener.cloud/to-foo-1-example-tcp-8080=allowed</code>, <code>networking.resources.gardener.cloud/to-foo-2-example-tcp-8080=allowed</code>, etc.
This approach does not work in practice, however, since the namespace names are neither static nor known upfront.</p><p>To overcome this, it is possible to specify an alias for the concrete namespace in the pod label selector via the <code>networking.resources.gardener.cloud/pod-label-selector-namespace-alias</code> annotation.</p><p>In above case, the <code>example</code> <code>Service</code> in the <code>foo-*</code> namespaces could be annotated with <code>networking.resources.gardener.cloud/pod-label-selector-namespace-alias=all-foos</code>.
This would modify the label selector in all <code>NetworkPolicy</code>s related to cross-namespace communication, i.e. instead of <code>networking.resources.gardener.cloud/to-foo-{1,2,...}-example-tcp-8080=allowed</code>, <code>networking.resources.gardener.cloud/to-all-foos-example-tcp-8080=allowed</code> would be used.
Now the component in namespace <code>bar</code> only needs this single label and is able to talk to all such <code>Service</code>s in the different namespaces.</p><blockquote><p>Real-world examples for this scenario are the <code>kube-apiserver</code> <code>Service</code> (which exists in all shoot namespaces), or the <code>istio-ingressgateway</code> <code>Service</code> (which exists in all <code>istio-ingress*</code> namespaces).
In both cases, the names of the namespaces are not statically known and depend on user input.</p></blockquote><h4 id=overwriting-the-pod-selector-label>Overwriting The Pod Selector Label</h4><p>For a component which initiates the connection to many other components, it&rsquo;s sometimes impractical to specify all the respective labels in its pod template.
For example, let&rsquo;s say a component <code>foo</code> talks to <code>bar{0..9}</code> on ports <code>tcp/808{0..9}</code>.
<code>foo</code> would need to have the ten <code>networking.resources.gardener.cloud/to-bar{0..9}-tcp-808{0..9}=allowed</code> labels.</p><p>As an alternative and to simplify this, it is also possible to annotate the targeted <code>Service</code>s with <code>networking.resources.gardener.cloud/from-&lt;some-alias>-allowed-ports</code>.
For our example, <code>&lt;some-alias></code> could be <code>all-bars</code>.</p><p>As a result, component <code>foo</code> just needs to have the label <code>networking.resources.gardener.cloud/to-all-bars=allowed</code> instead of all the other ten explicit labels.</p><p>⚠️ Note that this also requires to specify the list of allowed container ports as annotation value since the pod selector label will no longer be specific for a dedicated service/port.
For our example, the <code>Service</code> for <code>barX</code> with <code>X</code> in <code>{0..9}</code> needs to be annotated with <code>networking.resources.gardener.cloud/from-all-bars-allowed-ports=[{"port":808X,"protocol":"TCP"}]</code> in addition.</p><blockquote><p>Real-world examples for this scenario are the <code>Prometheis</code> in seed clusters which initiate the communication to a lot of components in order to scrape their metrics.
Another example is the <code>kube-apiserver</code> which initiates the communication to webhook servers (potentially of extension components that are not known by Gardener itself).</p></blockquote><h4 id=ingress-from-everywhere>Ingress From Everywhere</h4><p>All above scenarios are about components initiating connections to some targets.
However, some components also receive incoming traffic from sources outside the cluster.
This traffic requires adequate ingress policies so that it can be allowed.</p><p>To cover this scenario, the <code>Service</code> can be annotated with <code>networking.resources.gardener.cloud/from-world-to-ports=[{"port":"10250","protocol":"TCP"}]</code>.
As a result, the controller creates the following <code>NetworkPolicy</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: ingress-to-gardener-resource-manager-from-world
</span></span><span style=display:flex><span>  namespace: a
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>  - from:
</span></span><span style=display:flex><span>    - namespaceSelector: {}
</span></span><span style=display:flex><span>      podSelector: {}
</span></span><span style=display:flex><span>    - ipBlock:
</span></span><span style=display:flex><span>        cidr: 0.0.0.0/0
</span></span><span style=display:flex><span>    - ipBlock:
</span></span><span style=display:flex><span>        cidr: ::/0
</span></span><span style=display:flex><span>    ports:
</span></span><span style=display:flex><span>    - port: 10250
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: gardener-resource-manager
</span></span><span style=display:flex><span>  policyTypes:
</span></span><span style=display:flex><span>  - Ingress
</span></span></code></pre></div><p>The respective pods don&rsquo;t need any additional labels.
If the annotation&rsquo;s value is empty (<code>[]</code>) then all ports are allowed.</p><h4 id=services-exposed-via-ingress-resources>Services Exposed via <code>Ingress</code> Resources</h4><p>The controller can optionally be configured to watch <code>Ingress</code> resources by specifying the pod and namespace selectors for the <code>Ingress</code> controller.
If this information is provided, it automatically creates <code>NetworkPolicy</code> resources allowing the respective ingress/egress traffic for the backends exposed by the <code>Ingress</code>es.
This way, neither custom <code>NetworkPolicy</code>s nor custom labels must be provided.</p><p>The needed configuration is part of the component configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>controllers:
</span></span><span style=display:flex><span>  networkPolicy:
</span></span><span style=display:flex><span>    enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    concurrentSyncs: 5
</span></span><span style=display:flex><span>  <span style=color:green># namespaceSelectors:</span>
</span></span><span style=display:flex><span>  <span style=color:green># - matchLabels:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     kubernetes.io/metadata.name: default</span>
</span></span><span style=display:flex><span>    ingressControllerSelector:
</span></span><span style=display:flex><span>      namespace: default
</span></span><span style=display:flex><span>      podSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          foo: bar
</span></span></code></pre></div><p>As an example, let&rsquo;s assume that above <code>gardener-resource-manager</code> <code>Service</code> was exposed via the following <code>Ingress</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gardener-resource-manager
</span></span><span style=display:flex><span>  namespace: a
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: grm.foo.example.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: gardener-resource-manager
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 443
</span></span><span style=display:flex><span>        path: /
</span></span><span style=display:flex><span>        pathType: Prefix
</span></span></code></pre></div><p>As a result, the controller would automatically create the following <code>NetworkPolicy</code>s:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    gardener.cloud/description: Allows ingress TCP traffic to port 10250 for pods
</span></span><span style=display:flex><span>      selected by the a/gardener-resource-manager service selector from ingress controller
</span></span><span style=display:flex><span>      pods running in the default namespace labeled with map[foo:bar].
</span></span><span style=display:flex><span>  name: ingress-to-gardener-resource-manager-tcp-10250-from-ingress-controller
</span></span><span style=display:flex><span>  namespace: a
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ingress:
</span></span><span style=display:flex><span>  - from:
</span></span><span style=display:flex><span>    - podSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          foo: bar
</span></span><span style=display:flex><span>      namespaceSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          kubernetes.io/metadata.name: default
</span></span><span style=display:flex><span>    ports:
</span></span><span style=display:flex><span>    - port: 10250
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: gardener-resource-manager
</span></span><span style=display:flex><span>  policyTypes:
</span></span><span style=display:flex><span>  - Ingress
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: NetworkPolicy
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    gardener.cloud/description: Allows egress TCP traffic to port 10250 from pods
</span></span><span style=display:flex><span>      running in the default namespace labeled with map[foo:bar] to pods selected by
</span></span><span style=display:flex><span>      the a/gardener-resource-manager service selector.
</span></span><span style=display:flex><span>  name: egress-to-a-gardener-resource-manager-tcp-10250-from-ingress-controller
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  egress:
</span></span><span style=display:flex><span>  - to:
</span></span><span style=display:flex><span>    - podSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          app: gardener-resource-manager
</span></span><span style=display:flex><span>      namespaceSelector:
</span></span><span style=display:flex><span>        matchLabels:
</span></span><span style=display:flex><span>          kubernetes.io/metadata.name: a
</span></span><span style=display:flex><span>    ports:
</span></span><span style=display:flex><span>    - port: 10250
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>  podSelector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      foo: bar
</span></span><span style=display:flex><span>  policyTypes:
</span></span><span style=display:flex><span>  - Egress
</span></span></code></pre></div><blockquote><p>ℹ️ Note that <code>Ingress</code> resources reference the service port while <code>NetworkPolicy</code>s reference the target port/container port.
The controller automatically translates this when reconciling the <code>NetworkPolicy</code> resources.</p></blockquote><h3 id=node-controllerhttpsgithubcomgardenergardenertreemasterpkgresourcemanagercontrollernode><a href=https://github.com/gardener/gardener/tree/master/pkg/resourcemanager/controller/node><code>Node</code> Controller</a></h3><p>Gardenlet configures kubelet of shoot worker nodes to register the <code>Node</code> object with the <code>node.gardener.cloud/critical-components-not-ready</code> taint (effect <code>NoSchedule</code>).
This controller watches newly created <code>Node</code> objects in the shoot cluster and removes the taint once all node-critical components are scheduled and ready.
If the controller finds node-critical components that are not scheduled or not ready yet, it checks the <code>Node</code> again after the duration configured in <code>ResourceManagerConfiguration.controllers.node.backoff</code>
Please refer to the <a href=/docs/gardener/node-readiness/>feature documentation</a> or <a href=https://github.com/gardener/gardener/issues/7117>proposal issue</a> for more details.</p><h2 id=webhooks>Webhooks</h2><h3 id=mutating-webhooks>Mutating Webhooks</h3><h4 id=high-availability-config>High Availability Config</h4><p>This webhook is used to conveniently apply the configuration to make components deployed to seed or shoot clusters highly available.
The details and scenarios are described in <a href=/docs/gardener/high-availability/>High Availability Of Deployed Components</a>.</p><p>The webhook reacts on creation/update of <code>Deployment</code>s, <code>StatefulSet</code>s, <code>HorizontalPodAutoscaler</code>s and <code>HVPA</code>s in namespaces labeled with <code>high-availability-config.resources.gardener.cloud/consider=true</code>.</p><p>The webhook performs the following actions:</p><ol><li><p>The <code>.spec.replicas</code> (or <code>spec.minReplicas</code> respectively) field is mutated based on the <code>high-availability-config.resources.gardener.cloud/type</code> label of the resource and the <code>high-availability-config.resources.gardener.cloud/failure-tolerance-type</code> annotation of the namespace:</p><table><thead><tr><th>Failure Tolerance Type ➡️<br>/<br>⬇️ Component Type️ ️</th><th>unset</th><th>empty</th><th>non-empty</th></tr></thead><tbody><tr><td><code>controller</code></td><td><code>2</code></td><td><code>1</code></td><td><code>2</code></td></tr><tr><td><code>server</code></td><td><code>2</code></td><td><code>2</code></td><td><code>2</code></td></tr></tbody></table><ul><li>The replica count values can be overwritten by the <code>high-availability-config.resources.gardener.cloud/replicas</code> annotation.</li><li>It does NOT mutate the replicas when:<ul><li>the replicas are already set to <code>0</code> (hibernation case), or</li><li>when the resource is scaled horizontally by <code>HorizontalPodAutoscaler</code> or <code>Hvpa</code>, and the current replica count is higher than what was computed above.</li></ul></li></ul></li><li><p>When the <code>high-availability-config.resources.gardener.cloud/zones</code> annotation is NOT empty and either the <code>high-availability-config.resources.gardener.cloud/failure-tolerance-type</code> annotation is set or the <code>high-availability-config.resources.gardener.cloud/zone-pinning</code> annotation is set to <code>true</code>, then it adds a <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity>node affinity</a> to the pod template spec:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  affinity:
</span></span><span style=display:flex><span>    nodeAffinity:
</span></span><span style=display:flex><span>      requiredDuringSchedulingIgnoredDuringExecution:
</span></span><span style=display:flex><span>        nodeSelectorTerms:
</span></span><span style=display:flex><span>        - matchExpressions:
</span></span><span style=display:flex><span>          - key: topology.kubernetes.io/zone
</span></span><span style=display:flex><span>            operator: In
</span></span><span style=display:flex><span>            values:
</span></span><span style=display:flex><span>            - &lt;zone1&gt;
</span></span><span style=display:flex><span>          <span style=color:green># - ...</span>
</span></span></code></pre></div><p>This ensures that all pods are pinned to only nodes in exactly those concrete zones.</p></li><li><p><a href=https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/>Topology Spread Constraints</a> are added to the pod template spec when the <code>.spec.replicas</code> are greater than <code>1</code>. When the <code>high-availability-config.resources.gardener.cloud/zones</code> annotation &mldr;</p><ul><li><p>&mldr; contains only one zone, then the following is added:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - topologyKey: kubernetes.io/hostname
</span></span><span style=display:flex><span>    maxSkew: 1
</span></span><span style=display:flex><span>    whenUnsatisfiable: ScheduleAnyway
</span></span><span style=display:flex><span>    labelSelector: ...
</span></span></code></pre></div><p>This ensures that the (multiple) pods are scheduled across nodes on best-effort basis.</p></li><li><p>&mldr; contains at least two zones, then the following is added:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - topologyKey: kubernetes.io/hostname
</span></span><span style=display:flex><span>    maxSkew: 1
</span></span><span style=display:flex><span>    whenUnsatisfiable: ScheduleAnyway
</span></span><span style=display:flex><span>    labelSelector: ...
</span></span><span style=display:flex><span>  - topologyKey: topology.kubernetes.io/zone
</span></span><span style=display:flex><span>    minDomains: 2 <span style=color:green># lower value of max replicas or number of zones</span>
</span></span><span style=display:flex><span>    maxSkew: 1
</span></span><span style=display:flex><span>    whenUnsatisfiable: DoNotSchedule
</span></span><span style=display:flex><span>    labelSelector: ...
</span></span></code></pre></div><p>This enforces that the (multiple) pods are scheduled across zones.
It circumvents a known limitation in Kubernetes for clusters &lt; 1.26 (ref <a href=https://github.com/kubernetes/kubernetes/issues/109364>kubernetes/kubernetes#109364</a>.
In case the number of replicas is larger than twice the number of zones, then the <code>maxSkew=2</code> for the second spread constraints.
The <code>minDomains</code> calculation is based on whatever value is lower - (maximum) replicas or number of zones. This is the number of minimum domains required to schedule pods in a highly available manner.</p></li></ul><p>Independent on the number of zones, when one of the following conditions is true, then the field <code>whenUnsatisfiable</code> is set to <code>DoNotSchedule</code> for the constraint with <code>topologyKey=kubernetes.io/hostname</code> (which enforces the node-spread):</p><ul><li>The <code>high-availability-config.resources.gardener.cloud/host-spread</code> annotation is set to <code>true</code>.</li><li>The <code>high-availability-config.resources.gardener.cloud/failure-tolerance-type</code> annotation is set and NOT empty.</li></ul></li><li><p>Adds default tolerations for <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions>taint-based evictions</a>:</p><p>Tolerations for taints <code>node.kubernetes.io/not-ready</code> and <code>node.kubernetes.io/unreachable</code> are added to the handled <code>Deployment</code> and <code>StatefulSet</code> if their <code>podTemplate</code>s do not already specify them.
The <code>TolerationSeconds</code> are taken from the respective configuration section of the webhook&rsquo;s configuration (see <a href=https://github.com/gardener/gardener/blob/master/example/resource-manager/10-componentconfig.yaml>example</a>)).</p><p>We consider fine-tuned values for those tolerations a matter of high-availability because they often help to reduce recovery times in case of node or zone outages, also see <a href=/docs/guides/high-availability/best-practices/>High-Availability Best Practices</a>.
In addition, this webhook handling helps to set defaults for many but not all workload components in a cluster. For instance, Gardener can use this webhook to set defaults for nearly every component in seed clusters but only for the system components in shoot clusters. Any customer workload remains unchanged.</p></li></ol><h4 id=kubernetes-service-host-injection>Kubernetes Service Host Injection</h4><p>By default, when <code>Pod</code>s are created, Kubernetes implicitly injects the <code>KUBERNETES_SERVICE_HOST</code> environment variable into all containers.
The value of this variable points it to the default Kubernetes service (i.e., <code>kubernetes.default.svc.cluster.local</code>).
This allows pods to conveniently talk to the API server of their cluster.</p><p>In shoot clusters, this network path involves the <code>apiserver-proxy</code> <code>DaemonSet</code> which eventually forwards the traffic to the API server.
Hence, it results in additional network hop.</p><p>The purpose of this webhook is to explicitly inject the <code>KUBERNETES_SERVICE_HOST</code> environment variable into all containers and setting its value to the FQDN of the API server.
This way, the additional network hop is avoided.</p><h4 id=auto-mounting-projected-serviceaccount-tokens>Auto-Mounting Projected <code>ServiceAccount</code> Tokens</h4><p>When this webhook is activated, then it automatically injects projected <code>ServiceAccount</code> token volumes into <code>Pod</code>s and all its containers if all of the following preconditions are fulfilled:</p><ol><li>The <code>Pod</code> is NOT labeled with <code>projected-token-mount.resources.gardener.cloud/skip=true</code>.</li><li>The <code>Pod</code>&rsquo;s <code>.spec.serviceAccountName</code> field is NOT empty and NOT set to <code>default</code>.</li><li>The <code>ServiceAccount</code> specified in the <code>Pod</code>&rsquo;s <code>.spec.serviceAccountName</code> sets <code>.automountServiceAccountToken=false</code>.</li><li>The <code>Pod</code>&rsquo;s <code>.spec.volumes[]</code> DO NOT already contain a volume with a name prefixed with <code>kube-api-access-</code>.</li></ol><p>The projected volume will look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  volumes:
</span></span><span style=display:flex><span>  - name: kube-api-access-gardener
</span></span><span style=display:flex><span>    projected:
</span></span><span style=display:flex><span>      defaultMode: 420
</span></span><span style=display:flex><span>      sources:
</span></span><span style=display:flex><span>      - serviceAccountToken:
</span></span><span style=display:flex><span>          expirationSeconds: 43200
</span></span><span style=display:flex><span>          path: token
</span></span><span style=display:flex><span>      - configMap:
</span></span><span style=display:flex><span>          items:
</span></span><span style=display:flex><span>          - key: ca.crt
</span></span><span style=display:flex><span>            path: ca.crt
</span></span><span style=display:flex><span>          name: kube-root-ca.crt
</span></span><span style=display:flex><span>      - downwardAPI:
</span></span><span style=display:flex><span>          items:
</span></span><span style=display:flex><span>          - fieldRef:
</span></span><span style=display:flex><span>              apiVersion: v1
</span></span><span style=display:flex><span>              fieldPath: metadata.namespace
</span></span><span style=display:flex><span>            path: namespace
</span></span></code></pre></div><blockquote><p>The <code>expirationSeconds</code> are defaulted to <code>12h</code> and can be overwritten with the <code>.webhooks.projectedTokenMount.expirationSeconds</code> field in the component configuration, or with the <code>projected-token-mount.resources.gardener.cloud/expiration-seconds</code> annotation on a <code>Pod</code> resource.</p></blockquote><p>The volume will be mounted into all containers specified in the <code>Pod</code> to the path <code>/var/run/secrets/kubernetes.io/serviceaccount</code>.
This is the default location where client libraries expect to find the tokens and mimics the <a href=https://github.com/kubernetes/kubernetes/tree/v1.22.2/plugin/pkg/admission/serviceaccount>upstream <code>ServiceAccount</code> admission plugin</a>. See <a href=https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/#serviceaccount-admission-controller>Managing Service Accounts</a> for more information.</p><p>Overall, this webhook is used to inject projected service account tokens into pods running in the Shoot and the Seed cluster.
Hence, it is served from the Seed GRM and each Shoot GRM.
Please find an overview below for pods deployed in the Shoot cluster:</p><p><img src=/__resources/resource-manager-projected-token-shoot-to-shoot-apiserver_14af0d.jpg alt=image></p><h4 id=pod-topology-spread-constraints>Pod Topology Spread Constraints</h4><p>When this webhook is enabled, then it mimics the <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#spread-constraint-definition>topologyKey feature</a> for <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints>Topology Spread Constraints (TSC)</a> on the label <code>pod-template-hash</code>.
Concretely, when a pod is labelled with <code>pod-template-hash</code>, the handler of this webhook extends any topology spread constraint in the pod:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    pod-template-hash: 123abc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  topologySpreadConstraints:
</span></span><span style=display:flex><span>  - maxSkew: 1
</span></span><span style=display:flex><span>    topologyKey: topology.kubernetes.io/zone
</span></span><span style=display:flex><span>    whenUnsatisfiable: DoNotSchedule
</span></span><span style=display:flex><span>    labelSelector:
</span></span><span style=display:flex><span>      matchLabels:
</span></span><span style=display:flex><span>        pod-template-hash: 123abc <span style=color:green># added by webhook</span>
</span></span></code></pre></div><p>The procedure circumvents a <a href=https://github.com/kubernetes/kubernetes/issues/98215>known limitation</a> with TSCs which leads to imbalanced deployments after rolling updates.
Gardener enables this webhook to schedule pods of deployments across nodes and zones.</p><p>Please note that the <code>gardener-resource-manager</code> itself as well as pods labelled with <code>topology-spread-constraints.resources.gardener.cloud/skip</code> are excluded from any mutations.</p><h4 id=system-components-webhook>System Components Webhook</h4><p>If enabled, this webhook handles scheduling concerns for system components <code>Pod</code>s (except those managed by <code>DaemonSet</code>s).
The following tasks are performed by this webhook:</p><ol><li>Add <code>pod.spec.nodeSelector</code> as given in the webhook configuration.</li><li>Add <code>pod.spec.tolerations</code> as given in the webhook configuration.</li><li>Add <code>pod.spec.tolerations</code> for any existing nodes matching the node selector given in the webhook configuration. Known taints and tolerations used for <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions>taint based evictions</a> are disregarded.</li></ol><p>Gardener enables this webhook for <code>kube-system</code> and <code>kubernetes-dashboard</code> namespaces in shoot clusters, selecting <code>Pod</code>s being labelled with <code>resources.gardener.cloud/managed-by: gardener</code>.
It adds a configuration, so that <code>Pod</code>s will get the <code>worker.gardener.cloud/system-components: true</code> node selector (step 1) as well as tolerate any custom taint (step 2) that is added to system component worker nodes (<code>shoot.spec.provider.workers[].systemComponents.allow: true</code>).
In addition, the webhook merges these tolerations with the ones required for at that time available system component <code>Node</code>s in the cluster (step 3).
Both is required to ensure system component <code>Pod</code>s can be <em>scheduled</em> or <em>executed</em> during an active shoot reconciliation that is happening due to any modifications to <code>shoot.spec.provider.workers[].taints</code>, e.g. <code>Pod</code>s must be scheduled while there are still <code>Node</code>s not having the updated taint configuration.</p><blockquote><p>You can opt-out of this behaviour for <code>Pod</code>s by labeling them with <code>system-components-config.resources.gardener.cloud/skip=true</code>.</p></blockquote><h4 id=endpointslice-hints>EndpointSlice Hints</h4><p>This webhook mutates <a href=https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/><code>EndpointSlice</code>s</a>. For each endpoint in the EndpointSlice, it sets the endpoint&rsquo;s hints to the endpoint&rsquo;s zone.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: discovery.k8s.io/v1
</span></span><span style=display:flex><span>kind: EndpointSlice
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: example-hints
</span></span><span style=display:flex><span>endpoints:
</span></span><span style=display:flex><span>- addresses:
</span></span><span style=display:flex><span>  - <span style=color:#a31515>&#34;10.1.2.3&#34;</span>
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>    ready: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  hostname: pod-1
</span></span><span style=display:flex><span>  zone: zone-a
</span></span><span style=display:flex><span>  hints:
</span></span><span style=display:flex><span>    forZones:
</span></span><span style=display:flex><span>    - name: <span style=color:#a31515>&#34;zone-a&#34;</span> <span style=color:green># added by webhook</span>
</span></span><span style=display:flex><span>- addresses:
</span></span><span style=display:flex><span>  - <span style=color:#a31515>&#34;10.1.2.4&#34;</span>
</span></span><span style=display:flex><span>  conditions:
</span></span><span style=display:flex><span>    ready: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  hostname: pod-2
</span></span><span style=display:flex><span>  zone: zone-b
</span></span><span style=display:flex><span>  hints:
</span></span><span style=display:flex><span>    forZones:
</span></span><span style=display:flex><span>    - name: <span style=color:#a31515>&#34;zone-b&#34;</span> <span style=color:green># added by webhook</span>
</span></span></code></pre></div><p>The webhook aims to circumvent issues with the Kubernetes <code>TopologyAwareHints</code> feature that currently does not allow to achieve a deterministic topology-aware traffic routing. For more details, see the following issue <a href=https://github.com/kubernetes/kubernetes/issues/113731>kubernetes/kubernetes#113731</a> that describes drawbacks of the <code>TopologyAwareHints</code> feature for our use case.
If the above-mentioned issue gets resolved and there is a native support for deterministic topology-aware traffic routing in Kubernetes, then this webhook can be dropped in favor of the native Kubernetes feature.</p><h3 id=validating-webhooks>Validating Webhooks</h3><h4 id=unconfirmed-deletion-prevention-for-custom-resources-and-definitions>Unconfirmed Deletion Prevention For Custom Resources And Definitions</h4><p>As part of Gardener&rsquo;s <a href=/docs/gardener/extensions/>extensibility concepts</a>, a lot of <code>CustomResourceDefinition</code>s are deployed to the seed clusters that serve as extension points for provider-specific controllers.
For example, the <a href=/docs/gardener/extensions/infrastructure/><code>Infrastructure</code> CRD</a> triggers the provider extension to prepare the IaaS infrastructure of the underlying cloud provider for a to-be-created shoot cluster.
Consequently, these extension CRDs have a lot of power and control large portions of the end-user&rsquo;s shoot cluster.
Accidental or undesired deletions of those resource can cause tremendous and hard-to-recover-from outages and should be prevented.</p><p>When this webhook is activated, it reacts for <code>CustomResourceDefinition</code>s and most of the custom resources in the <code>extensions.gardener.cloud/v1alpha1</code> API group.
It also reacts for the <code>druid.gardener.cloud/v1alpha1.Etcd</code> resources.</p><p>The webhook prevents <code>DELETE</code> requests for those <code>CustomResourceDefinition</code>s labeled with <code>gardener.cloud/deletion-protected=true</code>, and for all mentioned custom resources if they were not previously annotated with the <code>confirmation.gardener.cloud/deletion=true</code>.
This prevents that undesired <code>kubectl delete &lt;...></code> requests are accepted.</p><h4 id=extension-resource-validation>Extension Resource Validation</h4><p>When this webhook is activated, it reacts for most of the custom resources in the <code>extensions.gardener.cloud/v1alpha1</code> API group.
It also reacts for the <code>druid.gardener.cloud/v1alpha1.Etcd</code> resources.</p><p>The webhook validates the resources specifications for <code>CREATE</code> and <code>UPDATE</code> requests.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d13c2ec04fc608d6d65a3a24ab54fbf9>12 - Gardener Scheduler</h1><div class=lead>Understand the configuration and flow of the controller that assigns a seed cluster to newly created shoots</div><h2 id=overview>Overview</h2><p>The Gardener Scheduler is in essence a controller that watches newly created shoots and assigns a seed cluster to them.
Conceptually, the task of the Gardener Scheduler is very similar to the task of the Kubernetes Scheduler: finding a seed for a shoot instead of a node for a pod.</p><p>Either the scheduling strategy or the shoot cluster purpose hereby determines how the scheduler is operating.
The following sections explain the configuration and flow in greater detail.</p><h2 id=why-is-the-gardener-scheduler-needed>Why Is the Gardener Scheduler Needed?</h2><h3 id=1-decoupling>1. Decoupling</h3><p>Previously, an admission plugin in the Gardener API server conducted the scheduling decisions.
This implies changes to the API server whenever adjustments of the scheduling are needed.
Decoupling the API server and the scheduler comes with greater flexibility to develop these components independently.</p><h3 id=2-extensibility>2. Extensibility</h3><p>It should be possible to easily extend and tweak the scheduler in the future.
Possibly, similar to the Kubernetes scheduler, hooks could be provided which influence the scheduling decisions.
It should be also possible to completely replace the standard Gardener Scheduler with a custom implementation.</p><h2 id=algorithm-overview>Algorithm Overview</h2><p>The following <strong>sequence</strong> describes the steps involved to determine a seed candidate:</p><ol><li>Determine usable seeds with &ldquo;usable&rdquo; defined as follows:<ul><li>no <code>.metadata.deletionTimestamp</code></li><li><code>.spec.settings.scheduling.visible</code> is <code>true</code></li><li><code>.status.lastOperation</code> is not <code>nil</code></li><li>conditions <code>GardenletReady</code>, <code>BackupBucketsReady</code> (if available) are <code>true</code></li></ul></li><li>Filter seeds:<ul><li>matching <code>.spec.seedSelector</code> in <code>CloudProfile</code> used by the <code>Shoot</code></li><li>matching <code>.spec.seedSelector</code> in <code>Shoot</code></li><li>having no network intersection with the <code>Shoot</code>&rsquo;s networks (due to the VPN connectivity between seeds and shoots their networks must be disjoint)</li><li>whose taints (<code>.spec.taints</code>) are tolerated by the <code>Shoot</code> (<code>.spec.tolerations</code>)</li><li>whose capacity for shoots would not be exceeded if the shoot is scheduled onto the seed, see <a href=/docs/gardener/concepts/scheduler/#ensuring-seeds-capacity-for-shoots-is-not-exceeded>Ensuring seeds capacity for shoots is not exceeded</a></li><li>which have at least three zones in <code>.spec.provider.zones</code> if shoot requests a high available control plane with failure tolerance type <code>zone</code>.</li></ul></li><li>Apply active <a href=/docs/gardener/concepts/scheduler/#strategies>strategy</a> e.g., <em>Minimal Distance strategy</em></li><li>Choose least utilized seed, i.e., the one with the least number of shoot control planes, will be the winner and written to the <code>.spec.seedName</code> field of the <code>Shoot</code>.</li></ol><p>In order to put the scheduling decision into effect, the scheduler sends an update request for the <code>Shoot</code> resource to
the API server. After validation, the <code>gardener-apiserver</code> updates the <code>Shoot</code> to have the <code>spec.seedName</code> field set.
Subsequently, the <code>gardenlet</code> picks up and starts to create the cluster on the specified seed.</p><h2 id=configuration>Configuration</h2><p>The Gardener Scheduler configuration has to be supplied on startup. It is a mandatory and also the only available flag.
<a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-scheduler.yaml>This yaml file</a> holds an example scheduler configuration.</p><p>Most of the configuration options are the same as in the Gardener Controller Manager (leader election, client connection, &mldr;).
However, the Gardener Scheduler on the other hand does not need a TLS configuration, because there are currently no webhooks configurable.</p><h2 id=strategies>Strategies</h2><p>The scheduling strategy is defined in the <em><strong>candidateDeterminationStrategy</strong></em> of the scheduler&rsquo;s configuration and can have the possible values <code>SameRegion</code> and <code>MinimalDistance</code>.
The <code>SameRegion</code> strategy is the default strategy.</p><h3 id=same-region-strategy>Same Region strategy</h3><p>The Gardener Scheduler reads the <code>spec.provider.type</code> and <code>.spec.region</code> fields from the <code>Shoot</code> resource.
It tries to find a seed that has the identical <code>.spec.provider.type</code> and <code>.spec.provider.region</code> fields set.
If it cannot find a suitable seed, it adds an event to the shoot stating that it is unschedulable.</p><h3 id=minimal-distance-strategy>Minimal Distance strategy</h3><p>The Gardener Scheduler tries to find a valid seed with minimal distance to the shoot&rsquo;s intended region.
Distances are configured via <code>ConfigMap</code>(s), usually per cloud provider in a Gardener landscape.
The configuration is structured like this:</p><ul><li>It refers to one or multiple <code>CloudProfile</code>s via annotation <code>scheduling.gardener.cloud/cloudprofiles</code>.</li><li>It contains the declaration as <code>region-config</code> via label <code>scheduling.gardener.cloud/purpose</code>.</li><li>If a <code>CloudProfile</code> is referred by multiple <code>ConfigMap</code>s, only the first one is considered.</li><li>The <code>data</code> fields configure actual distances, where <em>key</em> relates to the <code>Shoot</code> region and <em>value</em> contains distances to <code>Seed</code> regions.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: &lt;name&gt;
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    scheduling.gardener.cloud/cloudprofiles: cloudprofile-name-1{,optional-cloudprofile-name-2,...}
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    scheduling.gardener.cloud/purpose: region-config
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  region-1: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    region-2: 10
</span></span></span><span style=display:flex><span><span style=color:#a31515>    region-3: 20
</span></span></span><span style=display:flex><span><span style=color:#a31515>    ...</span>    
</span></span><span style=display:flex><span>  region-2: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>    region-1: 10
</span></span></span><span style=display:flex><span><span style=color:#a31515>    region-3: 10
</span></span></span><span style=display:flex><span><span style=color:#a31515>    ...</span>    
</span></span></code></pre></div><blockquote><p>Gardener provider extensions for public cloud providers usually have an example weight <code>ConfigMap</code> in their repositories.
We suggest to check them out before defining your own data.</p></blockquote><p>If a valid seed candidate cannot be found after consulting the distance configuration, the scheduler will fall back to
the Levenshtein distance to find the closest region. Therefore, the region name
is split into a base name and an orientation. Possible orientations are <code>north</code>, <code>south</code>, <code>east</code>, <code>west</code> and <code>central</code>.
The distance then is twice the Levenshtein distance of the region&rsquo;s base name plus a correction value based on the
orientation and the provider.</p><p>If the orientations of shoot and seed candidate match, the correction value is 0, if they differ it is 2 and if
either the seed&rsquo;s or the shoot&rsquo;s region does not have an orientation it is 1.
If the provider differs, the correction value is additionally incremented by 2.</p><p>Because of this, a matching region with a matching provider is always prefered.</p><h3 id=special-handling-based-on-shoot-cluster-purpose>Special handling based on shoot cluster purpose</h3><p>Every shoot cluster can have a purpose that describes what the cluster is used for, and also influences how the cluster is setup (see <a href=/docs/gardener/shoot_purposes/>Shoot Cluster Purpose</a> for more information).</p><p>In case the shoot has the <code>testing</code> purpose, then the scheduler only reads the <code>.spec.provider.type</code> from the <code>Shoot</code> resource and tries to find a <code>Seed</code> that has the identical <code>.spec.provider.type</code>.
The region does not matter, i.e., <code>testing</code> shoots may also be scheduled on a seed in a complete different region if it is better for balancing the whole Gardener system.</p><h2 id=shootsbinding-subresource><code>shoots/binding</code> Subresource</h2><p>The <code>shoots/binding</code> subresource is used to bind a <code>Shoot</code> to a <code>Seed</code>. On creation of a shoot cluster/s, the scheduler updates the binding automatically if an appropriate seed cluster is available.
Only an operator with the necessary RBAC can update this binding manually. This can be done by changing the <code>.spec.seedName</code> of the shoot. However, if a different seed is already assigned to the shoot, this will trigger a control-plane migration. For required steps, please see <a href=/docs/gardener/control_plane_migration/#triggering-the-migration>Triggering the Migration</a>.</p><h2 id=specschedulername-field-in-the-shoot-specification><code>spec.schedulerName</code> Field in the <code>Shoot</code> Specification</h2><p>Similar to the <code>spec.schedulerName</code> field in <code>Pod</code>s, the <code>Shoot</code> specification has an optional <code>.spec.schedulerName</code> field. If this field is set on creation, only the scheduler which relates to the configured name is responsible for scheduling the shoot.
The <code>default-scheduler</code> name is reserved for the default scheduler of Gardener.
Affected Shoots will remain in <code>Pending</code> state if the mentioned scheduler is not present in the landscape.</p><h2 id=specseedname-field-in-the-shoot-specification><code>spec.seedName</code> Field in the <code>Shoot</code> Specification</h2><p>Similar to the <code>.spec.nodeName</code> field in <code>Pod</code>s, the <code>Shoot</code> specification has an optional <code>.spec.seedName</code> field. If this field is set on creation, the shoot will be scheduled to this seed. However, this field can only be set by users having RBAC for the <code>shoots/binding</code> subresource. If this field is not set, the <code>scheduler</code> will assign a suitable seed automatically and populate this field with the seed name.</p><h2 id=seedselector-field-in-the-shoot-specification><code>seedSelector</code> Field in the <code>Shoot</code> Specification</h2><p>Similar to the <code>.spec.nodeSelector</code> field in <code>Pod</code>s, the <code>Shoot</code> specification has an optional <code>.spec.seedSelector</code> field.
It allows the user to provide a label selector that must match the labels of the <code>Seed</code>s in order to be scheduled to one of them.
The labels on the <code>Seed</code>s are usually controlled by Gardener administrators/operators - end users cannot add arbitrary labels themselves.
If provided, the Gardener Scheduler will only consider as &ldquo;suitable&rdquo; those seeds whose labels match those provided in the <code>.spec.seedSelector</code> of the <code>Shoot</code>.</p><p>By default, only seeds with the same provider as the shoot are selected. By adding a <code>providerTypes</code> field to the <code>seedSelector</code>,
a dedicated set of possible providers (<code>*</code> means all provider types) can be selected.</p><h2 id=ensuring-a-seeds-capacity-for-shoots-is-not-exceeded>Ensuring a Seed&rsquo;s Capacity for Shoots Is Not Exceeded</h2><p>Seeds have a practical limit of how many shoots they can accommodate. Exceeding this limit is undesirable, as the system performance will be noticeably impacted. Therefore, the scheduler ensures that a seed&rsquo;s capacity for shoots is not exceeded by taking into account a maximum number of shoots that can be scheduled onto a seed.</p><p>This mechanism works as follows:</p><ul><li>The <code>gardenlet</code> is configured with certain <em>resources</em> and their total <em>capacity</em> (and, for certain resources, the amount <em>reserved</em> for Gardener), see <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>/example/20-componentconfig-gardenlet.yaml</a>. Currently, the only such resource is the maximum number of shoots that can be scheduled onto a seed.</li><li>The <code>gardenlet</code> seed controller updates the <code>capacity</code> and <code>allocatable</code> fields in the Seed status with the capacity of each resource and how much of it is actually available to be consumed by shoots. The <code>allocatable</code> value of a resource is equal to <code>capacity</code> minus <code>reserved</code>.</li><li>When scheduling shoots, the scheduler filters out all candidate seeds whose allocatable capacity for shoots would be exceeded if the shoot is scheduled onto the seed.</li></ul><h2 id=failure-to-determine-a-suitable-seed>Failure to Determine a Suitable Seed</h2><p>In case the scheduler fails to find a suitable seed, the operation is being retried with exponential backoff.
The reason for the failure will be reported in the <code>Shoot</code>&rsquo;s <code>.status.lastOperation</code> field as well as a Kubernetes event (which can be retrieved via <code>kubectl -n &lt;namespace> describe shoot &lt;shoot-name></code>).</p><h2 id=current-limitation--future-plans>Current Limitation / Future Plans</h2><ul><li>Azure unfortunately has a geographically non-hierarchical naming pattern and does not start with the continent. This is the reason why we will exchange the implementation of the <code>MinimalDistance</code> strategy with a more suitable one in the future.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-899a4a43944208c044aaee6e6023bbba>13 - gardenlet</h1><div class=lead>Understand how the gardenlet, the primary &ldquo;agent&rdquo; on every seed cluster, works and learn more about the different Gardener components</div><h2 id=overview>Overview</h2><p>Gardener is implemented using the <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/operator/>operator pattern</a>:
It uses custom controllers that act on our own custom resources,
and apply Kubernetes principles to manage clusters instead of containers.
Following this analogy, you can recognize components of the Gardener architecture
as well-known Kubernetes components, for example, shoot clusters can be compared with pods,
and seed clusters can be seen as worker nodes.</p><p>The following Gardener components play a similar role as the corresponding components
in the Kubernetes architecture:</p><table><thead><tr><th>Gardener Component</th><th>Kubernetes Component</th></tr></thead><tbody><tr><td><code>gardener-apiserver</code></td><td><code>kube-apiserver</code></td></tr><tr><td><code>gardener-controller-manager</code></td><td><code>kube-controller-manager</code></td></tr><tr><td><code>gardener-scheduler</code></td><td><code>kube-scheduler</code></td></tr><tr><td><code>gardenlet</code></td><td><code>kubelet</code></td></tr></tbody></table><p>Similar to how the <code>kube-scheduler</code> of Kubernetes finds an appropriate node
for newly created pods, the <code>gardener-scheduler</code> of Gardener finds an appropriate seed cluster
to host the control plane for newly ordered clusters.
By providing multiple seed clusters for a region or provider, and distributing the workload,
Gardener also reduces the blast radius of potential issues.</p><p>Kubernetes runs a primary &ldquo;agent&rdquo; on every node, the kubelet,
which is responsible for managing pods and containers on its particular node.
Decentralizing the responsibility to the kubelet has the advantage that the overall system
is scalable. Gardener achieves the same for cluster management by using a <strong>gardenlet</strong>
as а primary &ldquo;agent&rdquo; on every seed cluster, and is only responsible for shoot clusters
located in its particular seed cluster:</p><p><img src=/__resources/gardenlet-architecture-similarities_3babfa.png alt="Counterparts in the Gardener Architecture and the Kubernetes Architecture"></p><p>The <code>gardener-controller-manager</code> has controllers to manage resources of the Gardener API. However, instead of letting the <code>gardener-controller-manager</code> talk directly to seed clusters or shoot clusters, the responsibility isn’t only delegated to the gardenlet, but also managed using a reversed control flow: It&rsquo;s up to the gardenlet to contact the Gardener API server, for example, to share a status for its managed seed clusters.</p><p>Reversing the control flow allows placing seed clusters or shoot clusters behind firewalls without the necessity of direct access via VPN tunnels anymore.</p><p><img src=/__resources/gardenlet-architecture-detailed_cae249.png alt="Reversed Control Flow Using a gardenlet"></p><h2 id=tls-bootstrapping>TLS Bootstrapping</h2><p>Kubernetes doesn’t manage worker nodes itself, and it’s also not
responsible for the lifecycle of the kubelet running on the workers.
Similarly, Gardener doesn’t manage seed clusters itself,
so it is also not responsible for the lifecycle of the gardenlet running on the seeds.
As a consequence, both the gardenlet and the kubelet need to prepare
a trusted connection to the Gardener API server
and the Kubernetes API server correspondingly.</p><p>To prepare a trusted connection between the gardenlet
and the Gardener API server, the gardenlet initializes
a bootstrapping process after you deployed it into your seed clusters:</p><ol><li><p>The gardenlet starts up with a bootstrap <code>kubeconfig</code>
having a bootstrap token that allows to create <code>CertificateSigningRequest</code> (CSR) resources.</p></li><li><p>After the CSR is signed, the gardenlet downloads
the created client certificate, creates a new <code>kubeconfig</code> with it,
and stores it inside a <code>Secret</code> in the seed cluster.</p></li><li><p>The gardenlet deletes the bootstrap <code>kubeconfig</code> secret,
and starts up with its new <code>kubeconfig</code>.</p></li><li><p>The gardenlet starts normal operation.</p></li></ol><p>The <code>gardener-controller-manager</code> runs a control loop
that automatically signs CSRs created by gardenlets.</p><blockquote><p>The gardenlet bootstrapping process is based on the
kubelet bootstrapping process. More information:
<a href=https://kubernetes.io/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/>Kubelet&rsquo;s TLS bootstrapping</a>.</p></blockquote><p>If you don&rsquo;t want to run this bootstrap process, you can create
a <code>kubeconfig</code> pointing to the garden cluster for the gardenlet yourself,
and use the field <code>gardenClientConnection.kubeconfig</code> in the
gardenlet configuration to share it with the gardenlet.</p><h2 id=gardenlet-certificate-rotation>gardenlet Certificate Rotation</h2><p>The certificate used to authenticate the gardenlet against the API server
has a certain validity based on the configuration of the garden cluster
(<code>--cluster-signing-duration</code> flag of the <code>kube-controller-manager</code> (default <code>1y</code>)).</p><blockquote><p>You can also configure the validity for the client certificate by specifying <code>.gardenClientConnection.kubeconfigValidity.validity</code> in the gardenlet&rsquo;s component configuration.
Note that changing this value will only take effect when the kubeconfig is rotated again (it is not picked up immediately).
The minimum validity is <code>10m</code> (that&rsquo;s what is enforced by the <code>CertificateSigningRequest</code> API in Kubernetes which is used by the gardenlet).</p></blockquote><p>By default, after about 70-90% of the validity has expired, the gardenlet tries to automatically replace
the current certificate with a new one (certificate rotation).</p><blockquote><p>You can change these boundaries by specifying <code>.gardenClientConnection.kubeconfigValidity.autoRotationJitterPercentage{Min,Max}</code> in the gardenlet&rsquo;s component configuration.</p></blockquote><p>To use a certificate rotation, you need to specify the secret to store
the <code>kubeconfig</code> with the rotated certificate in the field
<code>.gardenClientConnection.kubeconfigSecret</code> of the
gardenlet <a href=/docs/gardener/concepts/gardenlet/#component-configuration>component configuration</a>.</p><h3 id=rotate-certificates-using-bootstrap-kubeconfig>Rotate Certificates Using Bootstrap <code>kubeconfig</code></h3><p>If the gardenlet created the certificate during the initial TLS Bootstrapping
using the Bootstrap <code>kubeconfig</code>, certificates can be rotated automatically.
The same control loop in the <code>gardener-controller-manager</code> that signs
the CSRs during the initial TLS Bootstrapping also automatically signs
the CSR during a certificate rotation.</p><p>ℹ️ You can trigger an immediate renewal by annotating the <code>Secret</code> in the seed
cluster stated in the <code>.gardenClientConnection.kubeconfigSecret</code> field with
<code>gardener.cloud/operation=renew</code>. Within <code>10s</code>, gardenlet detects this and terminates
itself to request new credentials. After it has booted up again, gardenlet will issue a
new certificate independent of the remaining validity of the existing one.</p><p>ℹ️ Alternatively, annotate the respective <code>Seed</code> with <code>gardener.cloud/operation=renew-kubeconfig</code>.
This will make gardenlet annotate its own kubeconfig secret with <code>gardener.cloud/operation=renew</code>
and triggers the process described in the previous paragraph.</p><h3 id=rotate-certificates-using-custom-kubeconfig>Rotate Certificates Using Custom <code>kubeconfig</code></h3><p>When trying to rotate a custom certificate that wasn’t created by gardenlet
as part of the TLS Bootstrap, the x509 certificate&rsquo;s <code>Subject</code> field
needs to conform to the following:</p><ul><li>the Common Name (CN) is prefixed with <code>gardener.cloud:system:seed:</code></li><li>the Organization (O) equals <code>gardener.cloud:system:seeds</code></li></ul><p>Otherwise, the <code>gardener-controller-manager</code> doesn’t automatically
sign the CSR.
In this case, an external component or user needs to approve the CSR manually,
for example, using the command <code>kubectl certificate approve seed-csr-&lt;...></code>).
If that doesn’t happen within 15 minutes,
the gardenlet repeats the process and creates another CSR.</p><h2 id=configuring-the-seed-to-work-with-gardenlet>Configuring the Seed to Work with gardenlet</h2><p>The gardenlet works with a single seed, which must be configured in the
<code>GardenletConfiguration</code> under <code>.seedConfig</code>. This must be a copy of the
<code>Seed</code> resource, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: gardenlet.config.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: GardenletConfiguration
</span></span><span style=display:flex><span>seedConfig:
</span></span><span style=display:flex><span>  metadata:
</span></span><span style=display:flex><span>    name: my-seed
</span></span><span style=display:flex><span>  spec:
</span></span><span style=display:flex><span>    provider:
</span></span><span style=display:flex><span>      type: aws
</span></span><span style=display:flex><span>    <span style=color:green># ...</span>
</span></span><span style=display:flex><span>    settings:
</span></span><span style=display:flex><span>      scheduling:
</span></span><span style=display:flex><span>        visible: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>(see <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>this yaml file</a> for a more complete example)</p><p>On startup, gardenlet registers a <code>Seed</code> resource using the given template
in the <code>seedConfig</code> if it&rsquo;s not present already.</p><h2 id=component-configuration>Component Configuration</h2><p>In the component configuration for the gardenlet, it’s possible to define:</p><ul><li>settings for the Kubernetes clients interacting with the various clusters</li><li>settings for the controllers inside the gardenlet</li><li>settings for leader election and log levels, feature gates, and seed selection or seed configuration.</li></ul><p>More information: <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>Example gardenlet Component Configuration</a>.</p><h2 id=heartbeats>Heartbeats</h2><p>Similar to how Kubernetes uses <code>Lease</code> objects for node heart beats
(see <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/589-efficient-node-heartbeats/README.md>KEP</a>),
the gardenlet is using <code>Lease</code> objects for heart beats of the seed cluster.
Every two seconds, the gardenlet checks that the seed cluster&rsquo;s <code>/healthz</code>
endpoint returns HTTP status code 200.
If that is the case, the gardenlet renews the lease in the Garden cluster in the <code>gardener-system-seed-lease</code> namespace and updates
the <code>GardenletReady</code> condition in the <code>status.conditions</code> field of the <code>Seed</code> resource. For more information, see <a href=/docs/gardener/concepts/gardenlet/#lease-reconciler>this section</a>.</p><p>Similar to the <code>node-lifecycle-controller</code> inside the <code>kube-controller-manager</code>,
the <code>gardener-controller-manager</code> features a <code>seed-lifecycle-controller</code> that sets
the <code>GardenletReady</code> condition to <code>Unknown</code> in case the gardenlet fails to renew the lease.
As a consequence, the <code>gardener-scheduler</code> doesn’t consider this seed cluster for newly created shoot clusters anymore.</p><h3 id=healthz-endpoint><code>/healthz</code> Endpoint</h3><p>The gardenlet includes an HTTP server that serves a <code>/healthz</code> endpoint.
It’s used as a liveness probe in the <code>Deployment</code> of the gardenlet.
If the gardenlet fails to renew its lease,
then the endpoint returns <code>500 Internal Server Error</code>, otherwise it returns <code>200 OK</code>.</p><p>Please note that the <code>/healthz</code> only indicates whether the gardenlet
could successfully probe the Seed&rsquo;s API server and renew the lease with
the Garden cluster.
It does <em>not</em> show that the Gardener extension API server (with the Gardener resource groups)
is available.
However, the gardenlet is designed to withstand such connection outages and
retries until the connection is reestablished.</p><h2 id=controllers>Controllers</h2><p>The gardenlet consists out of several controllers which are now described in more detail.</p><h3 id=backupbucket-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollerbackupbucket><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/backupbucket><code>BackupBucket</code> Controller</a></h3><p>The <code>BackupBucket</code> controller reconciles those <code>core.gardener.cloud/v1beta1.BackupBucket</code> resources whose <code>.spec.seedName</code> value is equal to the name of the <code>Seed</code> the respective <code>gardenlet</code> is responsible for.
A <code>core.gardener.cloud/v1beta1.BackupBucket</code> resource is created by the <code>Seed</code> controller if <code>.spec.backup</code> is defined in the <code>Seed</code>.</p><p>The controller adds finalizers to the <code>BackupBucket</code> and the secret mentioned in the <code>.spec.secretRef</code> of the <code>BackupBucket</code>. The controller also copies this secret to the seed cluster. Additionally, it creates an <code>extensions.gardener.cloud/v1alpha1.BackupBucket</code> resource (non-namespaced) in the seed cluster and waits until the responsible extension controller reconciles it (see <a href=/docs/gardener/extensions/backupbucket/>Contract: BackupBucket Resource</a> for more details).
The status from the reconciliation is reported in the <code>.status.lastOperation</code> field. Once the extension resource is ready and the <code>.status.generatedSecretRef</code> is set by the extension controller, the <code>gardenlet</code> copies the referenced secret to the <code>garden</code> namespace in the garden cluster. An owner reference to the <code>core.gardener.cloud/v1beta1.BackupBucket</code> is added to this secret.</p><p>If the <code>core.gardener.cloud/v1beta1.BackupBucket</code> is deleted, the controller deletes the generated secret in the garden cluster and the <code>extensions.gardener.cloud/v1alpha1.BackupBucket</code> resource in the seed cluster and it waits for the respective extension controller to remove its finalizers from the <code>extensions.gardener.cloud/v1alpha1.BackupBucket</code>. Then it deletes the secret in the seed cluster and finally removes the finalizers from the <code>core.gardener.cloud/v1beta1.BackupBucket</code> and the referred secret.</p><h3 id=backupentry-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollerbackupentry><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/backupentry><code>BackupEntry</code> Controller</a></h3><p>The <code>BackupEntry</code> controller reconciles those <code>core.gardener.cloud/v1beta1.BackupEntry</code> resources whose <code>.spec.seedName</code> value is equal to the name of a <code>Seed</code> the respective gardenlet is responsible for.
Those resources are created by the <code>Shoot</code> controller (only if backup is enabled for the respective <code>Seed</code>) and there is exactly one <code>BackupEntry</code> per <code>Shoot</code>.</p><p>The controller creates an <code>extensions.gardener.cloud/v1alpha1.BackupEntry</code> resource (non-namespaced) in the seed cluster and waits until the responsible extension controller reconciled it (see <a href=/docs/gardener/extensions/backupentry/>Contract: BackupEntry Resource</a> for more details).
The status is populated in the <code>.status.lastOperation</code> field.</p><p>The <code>core.gardener.cloud/v1beta1.BackupEntry</code> resource has an owner reference pointing to the corresponding <code>Shoot</code>.
Hence, if the <code>Shoot</code> is deleted, the <code>BackupEntry</code> resource also gets deleted.
In this case, the controller deletes the <code>extensions.gardener.cloud/v1alpha1.BackupEntry</code> resource in the seed cluster and waits until the responsible extension controller has deleted it.
Afterwards, the finalizer of the <code>core.gardener.cloud/v1beta1.BackupEntry</code> resource is released so that it finally disappears from the system.</p><p>If the <code>spec.seedName</code> and <code>.status.seedName</code> of the <code>core.gardener.cloud/v1beta1.BackupEntry</code> are different, the controller will migrate it by annotating the <code>extensions.gardener.cloud/v1alpha1.BackupEntry</code> in the <code>Source Seed</code> with <code>gardener.cloud/operation: migrate</code>, waiting for it to be migrated successfully and eventually deleting it from the <code>Source Seed</code> cluster. Afterwards, the controller will recreate the <code>extensions.gardener.cloud/v1alpha1.BackupEntry</code> in the <code>Destination Seed</code>, annotate it with <code>gardener.cloud/operation: restore</code> and wait for the restore operation to finish. For more details about control plane migration, please read <a href=/docs/gardener/control_plane_migration/#shoot-control-plane-migration>Shoot Control Plane Migration</a>.</p><h5 id=keep-backup-for-deleted-shoots>Keep Backup for Deleted Shoots</h5><p>In some scenarios it might be beneficial to not immediately delete the <code>BackupEntry</code>s (and with them, the etcd backup) for deleted <code>Shoot</code>s.</p><p>In this case you can configure the <code>.controllers.backupEntry.deletionGracePeriodHours</code> field in the component configuration of the gardenlet.
For example, if you set it to <code>48</code>, then the <code>BackupEntry</code>s for deleted <code>Shoot</code>s will only be deleted <code>48</code> hours after the <code>Shoot</code> was deleted.</p><p>Additionally, you can limit the <a href=/docs/gardener/shoot_purposes/>shoot purposes</a> for which this applies by setting <code>.controllers.backupEntry.deletionGracePeriodShootPurposes[]</code>.
For example, if you set it to <code>[production]</code> then only the <code>BackupEntry</code>s for <code>Shoot</code>s with <code>.spec.purpose=production</code> will be deleted after the configured grace period. All others will be deleted immediately after the <code>Shoot</code> deletion.</p><p>In case a <code>BackupEntry</code> is scheduled for future deletion but you want to delete it immediately, add the annotation <code>backupentry.core.gardener.cloud/force-deletion=true</code>.</p><h3 id=bastion-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollerbastion><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/bastion><code>Bastion</code> Controller</a></h3><p>The <code>Bastion</code> controller reconciles those <code>operations.gardener.cloud/v1alpha1.Bastion</code> resources whose <code>.spec.seedName</code> value is equal to the name of a <code>Seed</code> the respective gardenlet is responsible for.</p><p>The controller creates an <code>extensions.gardener.cloud/v1alpha1.Bastion</code> resource in the seed cluster in the shoot namespace with the same name as <code>operations.gardener.cloud/v1alpha1.Bastion</code>. Then it waits until the responsible extension controller has reconciled it (see <a href=/docs/gardener/extensions/bastion/>Contract: Bastion Resource</a> for more details). The status is populated in the <code>.status.conditions</code> and <code>.status.ingress</code> fields.</p><p>During the deletion of <code>operations.gardener.cloud/v1alpha1.Bastion</code> resources, the controller first sets the <code>Ready</code> condition to <code>False</code> and then deletes the <code>extensions.gardener.cloud/v1alpha1.Bastion</code> resource in the seed cluster.
Once this resource is gone, the finalizer of the <code>operations.gardener.cloud/v1alpha1.Bastion</code> resource is released, so it finally disappears from the system.</p><h3 id=controllerinstallation-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollercontrollerinstallation><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/controllerinstallation><code>ControllerInstallation</code> Controller</a></h3><p>The <code>ControllerInstallation</code> controller in the <code>gardenlet</code> reconciles <code>ControllerInstallation</code> objects with the help of the following reconcilers.</p><h4 id=main-reconcilerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollercontrollerinstallationcontrollerinstallation><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/controllerinstallation/controllerinstallation>&ldquo;Main&rdquo; Reconciler</a></h4><p>This reconciler is responsible for <code>ControllerInstallation</code>s referencing a <code>ControllerDeployment</code> whose <code>type=helm</code>.</p><p>For each <code>ControllerInstallation</code>, it creates a namespace on the seed cluster named <code>extension-&lt;controller-installation-name></code>.
Then, it creates a generic garden kubeconfig and garden access secret for the extension for <a href=/docs/gardener/extensions/garden-api-access/>accessing the garden cluster</a>.</p><p>After that, it unpacks the Helm chart tarball in the <code>ControllerDeployment</code>s <code>.providerConfig.chart</code> field and deploys the rendered resources to the seed cluster.
The Helm chart values in <code>.providerConfig.values</code> will be used and extended with some information about the Gardener environment and the seed cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>gardener:
</span></span><span style=display:flex><span>  version: &lt;gardenlet-version&gt;
</span></span><span style=display:flex><span>  garden:
</span></span><span style=display:flex><span>    clusterIdentity: &lt;identity-of-garden-cluster&gt;
</span></span><span style=display:flex><span>    genericKubeconfigSecretName: &lt;secret-name&gt;
</span></span><span style=display:flex><span>  gardenlet:
</span></span><span style=display:flex><span>    featureGates:
</span></span><span style=display:flex><span>      Foo: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      Bar: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      <span style=color:green># ...</span>
</span></span><span style=display:flex><span>  seed:
</span></span><span style=display:flex><span>    name: &lt;seed-name&gt;
</span></span><span style=display:flex><span>    clusterIdentity: &lt;identity-of-seed-cluster&gt;
</span></span><span style=display:flex><span>    annotations: &lt;seed-annotations&gt;
</span></span><span style=display:flex><span>    labels: &lt;seed-labels&gt;
</span></span><span style=display:flex><span>    spec: &lt;seed-specification&gt;
</span></span></code></pre></div><p>As of today, there are a few more fields in <code>.gardener.seed</code>, but it is recommended to use the <code>.gardener.seed.spec</code> if the Helm chart needs more information about the seed configuration.</p><p>The rendered chart will be deployed via a <code>ManagedResource</code> created in the <code>garden</code> namespace of the seed cluster.
It is labeled with <code>controllerinstallation-name=&lt;name></code> so that one can easily find the owning <code>ControllerInstallation</code> for an existing <code>ManagedResource</code>.</p><p>The reconciler maintains the <code>Installed</code> condition of the <code>ControllerInstallation</code> and sets it to <code>False</code> if the rendering or deployment fails.</p><h4 id=care-reconcilerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollercontrollerinstallationcare><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/controllerinstallation/care>&ldquo;Care&rdquo; Reconciler</a></h4><p>This reconciler reconciles <code>ControllerInstallation</code> objects and checks whether they are in a healthy state.
It checks the <code>.status.conditions</code> of the backing <code>ManagedResource</code> created in the <code>garden</code> namespace of the seed cluster.</p><ul><li>If the <code>ResourcesApplied</code> condition of the <code>ManagedResource</code> is <code>True</code>, then the <code>Installed</code> condition of the <code>ControllerInstallation</code> will be set to <code>True</code>.</li><li>If the <code>ResourcesHealthy</code> condition of the <code>ManagedResource</code> is <code>True</code>, then the <code>Healthy</code> condition of the <code>ControllerInstallation</code> will be set to <code>True</code>.</li><li>If the <code>ResourcesProgressing</code> condition of the <code>ManagedResource</code> is <code>True</code>, then the <code>Progressing</code> condition of the <code>ControllerInstallation</code> will be set to <code>True</code>.</li></ul><p>A <code>ControllerInstallation</code> is considered &ldquo;healthy&rdquo; if <code>Applied=Healthy=True</code> and <code>Progressing=False</code>.</p><h4 id=required-reconcilerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollercontrollerinstallationrequired><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/controllerinstallation/required>&ldquo;Required&rdquo; Reconciler</a></h4><p>This reconciler watches all resources in the <code>extensions.gardener.cloud</code> API group in the seed cluster.
It is responsible for maintaining the <code>Required</code> condition on <code>ControllerInstallation</code>s.
Concretely, when there is at least one extension resource in the seed cluster a <code>ControllerInstallation</code> is responsible for, then the status of the <code>Required</code> condition will be <code>True</code>.
If there are no extension resources anymore, its status will be <code>False</code>.</p><p>This condition is taken into account by the <code>ControllerRegistration</code> controller part of <code>gardener-controller-manager</code> when it computes which extensions have to be deployed to which seed cluster. See <a href=/docs/gardener/concepts/controller-manager/#controllerregistration-controller>Gardener Controller Manager</a> for more details.</p><h3 id=managedseed-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollermanagedseed><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/managedseed><code>ManagedSeed</code> Controller</a></h3><p>The <code>ManagedSeed</code> controller in the <code>gardenlet</code> reconciles <code>ManagedSeed</code> that refers to <code>Shoot</code> scheduled on <code>Seed</code> the gardenlet is responsible for. Additionally, the controller monitors <code>Seed</code>s, which are owned by <code>ManagedSeed</code>s for which the gardenlet is responsible.</p><p>On <code>ManagedSeed</code> reconciliation, the controller first waits for the referenced Shoot to undergo a reconciliation process. Once the Shoot is successfully reconciled, the controller sets the <code>ShootReconciled</code> status of the ManagedSeed to <code>true</code>. Then, it creates <code>garden</code> namespace within the target Shoot cluster. The controller also manages secrets related to Seeds, such as the <code>backup</code> and <code>kubeconfig</code> secrets. It ensures that these secrets are created and updated according to the ManagedSeed spec. Finally, it deploys the <code>gardenlet</code> within the specified Shoot cluster which registers the <code>Seed</code> cluster.</p><p>On <code>ManagedSeed</code> deletion, the controller first deletes the corresponding <code>Seed</code> that was originally created by the controller. Subsequently, it deletes the <code>gardenlet</code> instance within the Shoot cluster. The controller also ensures the deletion of related Seed secrets. Finally, the dedicated <code>garden</code> namespace within the Shoot cluster is deleted.</p><h3 id=networkpolicy-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollernetworkpolicy><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/networkpolicy><code>NetworkPolicy</code> Controller</a></h3><p>The <code>NetworkPolicy</code> controller reconciles <code>NetworkPolicy</code>s in all relevant namespaces in the seed cluster and provides so-called &ldquo;general&rdquo; policies for access to the runtime cluster&rsquo;s API server, DNS, public networks, etc.</p><p>The controller resolves the IP address of the Kubernetes service in the <code>default</code> namespace and creates an egress <code>NetworkPolicy</code>s for it.</p><p>For more details about <code>NetworkPolicy</code>s in Gardener, please see <a href=/docs/gardener/network_policies/><code>NetworkPolicy</code>s In Garden, Seed, Shoot Clusters</a>.</p><h3 id=seed-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollerseed><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/seed><code>Seed</code> Controller</a></h3><p>The <code>Seed</code> controller in the <code>gardenlet</code> reconciles <code>Seed</code> objects with the help of the following reconcilers.</p><h4 id=main-reconcilerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollerseedseed><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/seed/seed>&ldquo;Main Reconciler&rdquo;</a></h4><p>This reconciler is responsible for managing the seed&rsquo;s system components.
Those comprise CA certificates, the various <code>CustomResourceDefinition</code>s, the logging and monitoring stacks, and few central components like <code>gardener-resource-manager</code>, <code>etcd-druid</code>, <code>istio</code>, etc.</p><p>The reconciler also deploys a <code>BackupBucket</code> resource in the garden cluster in case the <code>Seed'</code>s <code>.spec.backup</code> is set.
It also checks whether the seed cluster&rsquo;s Kubernetes version is at least the <a href=/docs/gardener/supported_k8s_versions/#seed-cluster-versions>minimum supported version</a> and errors in case this constraint is not met.</p><p>This reconciler maintains the <code>.status.lastOperation</code> field, i.e. it sets it:</p><ul><li>to <code>state=Progressing</code> before it executes its reconciliation flow.</li><li>to <code>state=Error</code> in case an error occurs.</li><li>to <code>state=Succeeded</code> in case the reconciliation succeeded.</li></ul><h4 id=care-reconcilerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollerseedcare><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/seed/care>&ldquo;Care&rdquo; Reconciler</a></h4><p>This reconciler checks whether the seed system components (deployed by the &ldquo;main&rdquo; reconciler) are healthy.
It checks the <code>.status.conditions</code> of the backing <code>ManagedResource</code> created in the <code>garden</code> namespace of the seed cluster.
A <code>ManagedResource</code> is considered &ldquo;healthy&rdquo; if the conditions <code>ResourcesApplied=ResourcesHealthy=True</code> and <code>ResourcesProgressing=False</code>.</p><p>If all <code>ManagedResource</code>s are healthy, then the <code>SeedSystemComponentsHealthy</code> condition of the <code>Seed</code> will be set to <code>True</code>.
Otherwise, it will be set to <code>False</code>.</p><p>If at least one <code>ManagedResource</code> is unhealthy and there is threshold configuration for the conditions (in <code>.controllers.seedCare.conditionThresholds</code>), then the status of the <code>SeedSystemComponentsHealthy</code> condition will be set:</p><ul><li>to <code>Progressing</code> if it was <code>True</code> before.</li><li>to <code>Progressing</code> if it was <code>Progressing</code> before and the <code>lastUpdateTime</code> of the condition does not exceed the configured threshold duration yet.</li><li>to <code>False</code> if it was <code>Progressing</code> before and the <code>lastUpdateTime</code> of the condition exceeds the configured threshold duration.</li></ul><p>The condition thresholds can be used to prevent reporting issues too early just because there is a rollout or a short disruption.
Only if the unhealthiness persists for at least the configured threshold duration, then the issues will be reported (by setting the status to <code>False</code>).</p><p>In order to compute the condition statuses, this reconciler considers <code>ManagedResource</code>s (in the <code>garden</code> and <code>istio-system</code> namespace) and their status, see <a href=/docs/gardener/concepts/resource-manager/#conditions>this document</a> for more information.
The following table explains which <code>ManagedResource</code>s are considered for which condition type:</p><table><thead><tr><th>Condition Type</th><th><code>ManagedResource</code>s are considered when</th></tr></thead><tbody><tr><td><code>SeedSystemComponentsHealthy</code></td><td><code>.spec.class</code> is set</td></tr></tbody></table><h4 id=lease-reconcilerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollerseedlease><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/seed/lease>&ldquo;Lease&rdquo; Reconciler</a></h4><p>This reconciler checks whether the connection to the seed cluster&rsquo;s <code>/healthz</code> endpoint works.
If this succeeds, then it renews a <code>Lease</code> resource in the garden cluster&rsquo;s <code>gardener-system-seed-lease</code> namespace.
This indicates a heartbeat to the external world, and internally the <code>gardenlet</code> sets its health status to <code>true</code>.
In addition, the <code>GardenletReady</code> condition in the <code>status</code> of the <code>Seed</code> is set to <code>True</code>.
The whole process is similar to what the <code>kubelet</code> does to report heartbeats for its <code>Node</code> resource and its <code>KubeletReady</code> condition. For more information, see <a href=/docs/gardener/concepts/gardenlet/#heartbeats>this section</a>.</p><p>If the connection to the <code>/healthz</code> endpoint or the update of the <code>Lease</code> fails, then the internal health status of <code>gardenlet</code> is set to <code>false</code>.
Also, this internal health status is set to <code>false</code> automatically after some time, in case the controller gets stuck for whatever reason.
This internal health status is available via the <code>gardenlet</code>&rsquo;s <code>/healthz</code> endpoint and is used for the <code>livenessProbe</code> in the <code>gardenlet</code> pod.</p><h3 id=shoot-controllerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollershoot><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/shoot><code>Shoot</code> Controller</a></h3><p>The <code>Shoot</code> controller in the <code>gardenlet</code> reconciles <code>Shoot</code> objects with the help of the following reconcilers.</p><h4 id=main-reconcilerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollershootshoot><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/shoot/shoot>&ldquo;Main&rdquo; Reconciler</a></h4><p>This reconciler is responsible for managing all shoot cluster components and implements the core logic for creating, updating, hibernating, deleting, and migrating shoot clusters.
It is also responsible for syncing the <a href=/docs/gardener/extensions/cluster/><code>Cluster</code> cluster</a> to the seed cluster before and after each successful shoot reconciliation.</p><p>The main reconciliation logic is performed in 3 different task flows dedicated to specific operation types:</p><ul><li><code>reconcile</code> (operations: create, reconcile, restore): this is the main flow responsible for creation and regular reconciliation of shoots. Hibernating a shoot also triggers this flow. It is also used for restoration of the shoot control plane on the new seed (second half of a <a href=/docs/gardener/control_plane_migration/#shoot-control-plane-migration>Control Plane Migration</a>)</li><li><code>migrate</code>: this flow is triggered when <code>spec.seedName</code> specifies a different seed than <code>status.seedName</code>. It performs the first half of the <a href=/docs/gardener/control_plane_migration/#shoot-control-plane-migration>Control Plane Migration</a>, i.e., a backup (<code>migrate</code> operation) of all control plane components followed by a &ldquo;shallow delete&rdquo;.</li><li><code>delete</code>: this flow is triggered when the shoot&rsquo;s <code>deletionTimestamp</code> is set, i.e., when it is deleted.</li></ul><p>The gardenlet takes special care to prevent unnecessary shoot reconciliations.
This is important for several reasons, e.g., to not overload the seed API servers and to not exhaust infrastructure rate limits too fast.
The gardenlet performs shoot reconciliations according to the following rules:</p><ul><li>If <code>status.observedGeneration</code> is less than <code>metadata.generation</code>: this is the case, e.g., when the spec was changed, a <a href=/docs/gardener/shoot_operations/>manual reconciliation operation</a> was triggered, or the shoot was deleted.</li><li>If the <a href=/docs/gardener/shoot_status/>last operation</a> was not successful.</li><li>If the shoot is in a <a href=/docs/gardener/shoot_status/>failed state</a>, the gardenlet does not perform any reconciliation on the shoot (unless the retry operation was triggered). However, it syncs the <code>Cluster</code> resource to the seed in order to inform the extension controllers about the failed state.</li><li>Regular reconciliations are performed with every <code>GardenletConfiguration.controllers.shoot.syncPeriod</code> (defaults to <code>1h</code>).</li><li>Shoot reconciliations are not performed if the assigned seed cluster is not healthy or has not been reconciled by the current gardenlet version yet (determined by the <code>Seed.status.gardener</code> section). This is done to make sure that shoots are reconciled with fully rolled out seed system components after a Gardener upgrade. Otherwise, the gardenlet might perform operations of the new version that doesn&rsquo;t match the old version of the deployed seed system components, which might lead to unspecified behavior.</li></ul><p>There are a few special cases that overwrite or confine how often and under which circumstances periodic shoot reconciliations are performed:</p><ul><li>In case the gardenlet config allows it (<code>controllers.shoot.respectSyncPeriodOverwrite</code>, disabled by default), the sync period for a shoot can be increased individually by setting the <code>shoot.gardener.cloud/sync-period</code> annotation. This is always allowed for shoots in the <code>garden</code> namespace. Shoots are not reconciled with a higher frequency than specified in <code>GardenletConfiguration.controllers.shoot.syncPeriod</code>.</li><li>In case the gardenlet config allows it (<code>controllers.shoot.respectSyncPeriodOverwrite</code>, disabled by default), shoots can be marked as &ldquo;ignored&rdquo; by setting the <code>shoot.gardener.cloud/ignore</code> annotation. In this case, the gardenlet does not perform any reconciliation for the shoot.</li><li>In case <code>GardenletConfiguration.controllers.shoot.reconcileInMaintenanceOnly</code> is enabled (disabled by default), the gardenlet performs regular shoot reconciliations only once in the respective maintenance time window (<code>GardenletConfiguration.controllers.shoot.syncPeriod</code> is ignored). The gardenlet randomly distributes shoot reconciliations over the maintenance time window to avoid high bursts of reconciliations (see <a href=/docs/gardener/shoot_maintenance/#cluster-reconciliation>Shoot Maintenance</a>).</li><li>In case <code>Shoot.spec.maintenance.confineSpecUpdateRollout</code> is enabled (disabled by default), changes to the shoot specification are not rolled out immediately but only during the respective maintenance time window (see <a href=/docs/gardener/shoot_maintenance/>Shoot Maintenance</a>).</li></ul><h4 id=care-reconcilerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollershootcare><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/shoot/care>&ldquo;Care&rdquo; Reconciler</a></h4><p>This reconciler performs three &ldquo;care&rdquo; actions related to <code>Shoot</code>s.</p><h5 id=conditions>Conditions</h5><p>It maintains the following conditions:</p><ul><li><code>APIServerAvailable</code>: The <code>/healthz</code> endpoint of the shoot&rsquo;s <code>kube-apiserver</code> is called and considered healthy when it responds with <code>200 OK</code>.</li><li><code>ControlPlaneHealthy</code>: The control plane is considered healthy when the respective <code>Deployment</code>s (for example <code>kube-apiserver</code>,<code>kube-controller-manager</code>), and <code>Etcd</code>s (for example <code>etcd-main</code>) exist and are healthy.</li><li><code>ObservabilityComponentsHealthy</code>: This condition is considered healthy when the respective <code>Deployment</code>s (for example <code>plutono</code>) and <code>StatefulSet</code>s (for example <code>prometheus</code>,<code>vali</code>) exist and are healthy.</li><li><code>EveryNodyReady</code>: The conditions of the worker nodes are checked (e.g., <code>Ready</code>, <code>MemoryPressure</code>). Also, it&rsquo;s checked whether the Kubernetes version of the installed <code>kubelet</code> matches the desired version specified in the <code>Shoot</code> resource.</li><li><code>SystemComponentsHealthy</code>: The conditions of the <code>ManagedResource</code>s are checked (e.g., <code>ResourcesApplied</code>). Also, it is verified whether the VPN tunnel connection is established (which is required for the <code>kube-apiserver</code> to communicate with the worker nodes).</li></ul><p>Sometimes, <code>ManagedResource</code>s can have both <code>Healthy</code> and <code>Progressing</code> conditions set to <code>True</code> (e.g., when a <code>DaemonSet</code> rolls out one-by-one on a large cluster with many nodes) while this is not reflected in the <code>Shoot</code> status. In order to catch issues where the rollout gets stuck, one can set <code>.controllers.shootCare.managedResourceProgressingThreshold</code> in the <code>gardenlet</code>&rsquo;s component configuration. If the <code>Progressing</code> condition is still <code>True</code> for more than the configured duration, the <code>SystemComponentsHealthy</code> condition in the <code>Shoot</code> is set to <code>False</code>, eventually.</p><p>Each condition can optionally also have error <code>codes</code> in order to indicate which type of issue was detected (see <a href=/docs/gardener/shoot_status/>Shoot Status</a> for more details).</p><p>Apart from the above, extension controllers can also contribute to the <code>status</code> or error <code>codes</code> of these conditions (see <a href=/docs/gardener/extensions/shoot-health-status-conditions/>Contributing to Shoot Health Status Conditions</a> for more details).</p><p>If all checks for a certain conditions are succeeded, then its <code>status</code> will be set to <code>True</code>.
Otherwise, it will be set to <code>False</code>.</p><p>If at least one check fails and there is threshold configuration for the conditions (in <code>.controllers.seedCare.conditionThresholds</code>), then the status will be set:</p><ul><li>to <code>Progressing</code> if it was <code>True</code> before.</li><li>to <code>Progressing</code> if it was <code>Progressing</code> before and the <code>lastUpdateTime</code> of the condition does not exceed the configured threshold duration yet.</li><li>to <code>False</code> if it was <code>Progressing</code> before and the <code>lastUpdateTime</code> of the condition exceeds the configured threshold duration.</li></ul><p>The condition thresholds can be used to prevent reporting issues too early just because there is a rollout or a short disruption.
Only if the unhealthiness persists for at least the configured threshold duration, then the issues will be reported (by setting the status to <code>False</code>).</p><p>Besides directly checking the status of <code>Deployment</code>s, <code>Etcd</code>s, <code>StatefulSet</code>s in the shoot namespace, this reconciler also considers <code>ManagedResource</code>s (in the shoot namespace) and their status in order to compute the condition statuses, see <a href=/docs/gardener/concepts/resource-manager/#conditions>this document</a> for more information.
The following table explains which <code>ManagedResource</code>s are considered for which condition type:</p><table><thead><tr><th>Condition Type</th><th><code>ManagedResource</code>s are considered when</th></tr></thead><tbody><tr><td><code>ControlPlaneHealthy</code></td><td><code>.spec.class=seed</code> and <code>care.gardener.cloud/condition-type</code> label either unset, or set to <code>ControlPlaneHealthy</code></td></tr><tr><td><code>ObservabilityComponentsHealthy</code></td><td><code>care.gardener.cloud/condition-type</code> label set to <code>ObservabilityComponentsHealthy</code></td></tr><tr><td><code>SystemComponentsHealthy</code></td><td><code>.spec.class</code> unset or <code>care.gardener.cloud/condition-type</code> label set to <code>SystemComponentsHealthy</code></td></tr></tbody></table><h5 id=constraints-and-automatic-webhook-remediation>Constraints And Automatic Webhook Remediation</h5><p>Please see <a href=/docs/gardener/shoot_status/#constraints>Shoot Status</a> for more details.</p><h5 id=garbage-collection>Garbage Collection</h5><p>Stale pods in the shoot namespace in the seed cluster and in the <code>kube-system</code> namespace in the shoot cluster are deleted.
A pod is considered stale when:</p><ul><li>it was terminated with reason <code>Evicted</code>.</li><li>it was terminated with reason starting with <code>OutOf</code> (e.g., <code>OutOfCpu</code>).</li><li>it is stuck in termination (i.e., if its <code>deletionTimestamp</code> is more than <code>5m</code> ago).</li></ul><h4 id=state-reconcilerhttpsgithubcomgardenergardenertreemasterpkggardenletcontrollershootstate><a href=https://github.com/gardener/gardener/tree/master/pkg/gardenlet/controller/shoot/state>&ldquo;State&rdquo; Reconciler</a></h4><p>This reconciler periodically (default: every <code>6h</code>) performs backups of the state of <code>Shoot</code> clusters and persists them into <code>ShootState</code> resources into the same namespace as the <code>Shoot</code>s in the garden cluster.
It is only started in case the <code>gardenlet</code> is responsible for an unmanaged <code>Seed</code>, i.e. a <code>Seed</code> which is not backed by a <code>seedmanagement.gardener.cloud/v1alpha1.ManagedSeed</code> object.
Alternatively, it can be disabled by setting the <code>concurrentSyncs=0</code> for the controller in the <code>gardenlet</code>&rsquo;s component configuration.</p><p>Please refer to <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/22-improved-usage-of-shootstate-api.md>GEP-22: Improved Usage of the <code>ShootState</code> API</a> for all information.</p><h3 id=tokenrequestor-controllerhttpsgithubcomgardenergardenertreemasterpkgcontrollertokenrequestor><a href=https://github.com/gardener/gardener/tree/master/pkg/controller/tokenrequestor><code>TokenRequestor</code> Controller</a></h3><p>The <code>gardenlet</code> uses an instance of the <code>TokenRequestor</code> controller which initially was developed in the context of the <code>gardener-resource-manager</code>, please read <a href=/docs/gardener/concepts/resource-manager/#tokenrequestor-controller>this document</a> for further information.</p><p><code>gardenlet</code> uses it for requesting tokens for components running in the seed cluster that need to communicate with the garden cluster.
The mechanism works the same way as for shoot control plane components running in the seed which need to communicate with the shoot cluster.
However, <code>gardenlet</code>&rsquo;s instance of the <code>TokenRequestor</code> controller is restricted to <code>Secret</code>s labeled with <code>resources.gardener.cloud/class=garden</code>.
Furthermore, it doesn&rsquo;t respect the <code>serviceaccount.resources.gardener.cloud/namespace</code> annotation. Instead, it always uses the seed&rsquo;s namespace in the garden cluster for managing <code>ServiceAccounts</code> and their tokens.</p><h2 id=managed-seeds>Managed Seeds</h2><p>Gardener users can use shoot clusters as seed clusters, so-called &ldquo;managed seeds&rdquo; (aka &ldquo;shooted seeds&rdquo;),
by creating <code>ManagedSeed</code> resources.
By default, the gardenlet that manages this shoot cluster then automatically
creates a clone of itself with the same version and the same configuration
that it currently has.
Then it deploys the gardenlet clone into the managed seed cluster.</p><p>For more information, see <a href=/docs/gardener/managed_seed/>Register Shoot as Seed</a>.</p><h2 id=migrating-from-previous-gardener-versions>Migrating from Previous Gardener Versions</h2><p>If your Gardener version doesn’t support gardenlets yet,
no special migration is required, but the following prerequisites must be met:</p><ul><li>Your Gardener version is at least 0.31 before upgrading to v1.</li><li>You have to make sure that your garden cluster is exposed in a way
that it’s reachable from all your seed clusters.</li></ul><p>With previous Gardener versions, you had deployed the Gardener Helm chart
(incorporating the API server, <code>controller-manager</code>, and scheduler).
With v1, this stays the same, but you now have to deploy the gardenlet Helm chart as well
into all of your seeds (if they aren’t managed, as mentioned earlier).</p><p>See <a href=/docs/gardener/deployment/deploy_gardenlet/>Deploy a gardenlet</a> for all instructions.</p><h2 id=related-links>Related Links</h2><ul><li><a href=/docs/gardener/concepts/architecture/>Gardener Architecture</a></li><li><a href=https://github.com/gardener/gardener/issues/356>#356: Implement Gardener Scheduler</a></li><li><a href=https://github.com/gardener/gardener/pull/2309>#2309: Add /healthz endpoint for gardenlet</a></li></ul></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2023 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.11dbee029dba1a98021fb7be4d7405a7392afb38ff5640a21ff4f4c4c5057b2f.js integrity="sha256-EdvuAp26GpgCH7e+TXQFpzkq+zj/VkCiH/T0xMUFey8=" crossorigin=anonymous></script></body></html>