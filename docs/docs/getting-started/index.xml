<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener â€“ Getting Started</title><link>https://gardener.cloud/docs/getting-started/</link><description>Recent content in Getting Started on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><atom:link href="https://gardener.cloud/docs/getting-started/index.xml" rel="self" type="application/rss+xml"/><item><title>Docs: Introduction to Gardener</title><link>https://gardener.cloud/docs/getting-started/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/introduction/</guid><description>
&lt;h2 id="problem-space">Problem Space&lt;/h2>
&lt;p>Let&amp;rsquo;s discuss the problem space first. Why does anyone need something like Gardener?&lt;/p>
&lt;h3 id="running-software">Running Software&lt;/h3>
&lt;p>The starting point is this rather simple question: Why would you want to run some software?&lt;/p>
&lt;p>Typically, software is run with a purpose and not just for the sake of running it. Whether it is a digital ledger, a company&amp;rsquo;s inventory or a blog - software provides a service to its user.&lt;/p>
&lt;p>Which brings us to the way this software is being consumed. Traditionally, software has been shipped on physical / digital media to the customer or end user. There, someone had to install, configure, and operate it. In recent times, the pattern has shifted. More and more solutions are operated by the vendor or a hosting partner and sold as a service ready to be used.&lt;/p>
&lt;p>But still, someone needs to install, configure, and maintain it - regardless of where it is installed. And of course, it will run forever once started and is generally resilient to any kind of failures.&lt;/p>
&lt;p>For smaller installations things like maintenance, scaling, debugging or configuration can be done in a semi-automatic way. It&amp;rsquo;s probably no fun and most importantly, only a limited amount of instances can be taken care of - similar to how one would take care of a pet.&lt;/p>
&lt;p>But when hosting services at scale, there is no way someone can do all this manually at acceptable costs. So we need some vehicle to easily spin up new instances, do lifecycle operations, get some basic failure resilience, and more. How can we achieve that?&lt;/p>
&lt;h2 id="solution-space-1---kubernetes">Solution Space 1 - Kubernetes&lt;/h2>
&lt;p>Let&amp;rsquo;s start solving some of the problems described earlier with Container technology and Kubernetes.&lt;/p>
&lt;h3 id="containers">Containers&lt;/h3>
&lt;p>Container technology is at the core of the solution space. A container forms a vehicle that is shippable, can easily run in any supported environment and generally adds a powerful abstraction layer to the infrastructure.&lt;/p>
&lt;p>However, plain containers do not help with resilience or scaling. Therefore, we need another system for orchestration.&lt;/p>
&lt;h3 id="orchestration">Orchestration&lt;/h3>
&lt;p>&amp;ldquo;Classical&amp;rdquo; orchestration that just follows the &amp;ldquo;notes&amp;rdquo; and moves from &lt;code>state A&lt;/code> to &lt;code>state B&lt;/code> doesn&amp;rsquo;t solve all of our problems. We need something else.&lt;/p>
&lt;p>Kubernetes operates on the principle of &amp;ldquo;desired state&amp;rdquo;. With it, you write a construction plan, then have controllers cycle through &amp;ldquo;observe -&amp;gt; analyze -&amp;gt; act&amp;rdquo; and transition the actual to the desired state. Those reconciliations ensure that whatever breaks there is a path back to a healthy state.&lt;/p>
&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>Containers (famously brought to the mainstream as &amp;ldquo;Docker&amp;rdquo;) and Kubernetes are the ingredients of a fundamental shift in IT. Similar to how the Operating System layer enabled the decoupling of software and hardware, container-related technologies provide an abstract interface to any kind of infrastructure platform for the next-generation of applications.&lt;/p>
&lt;h2 id="solution-space-2---gardener">Solution Space 2 - Gardener&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/operating-apps_954ffc.png" alt="">&lt;/p>
&lt;p>So, Kubernetes solves a lot of problems. But how do you get a Kubernetes cluster?&lt;/p>
&lt;p>Either:&lt;/p>
&lt;ul>
&lt;li>Buy a cluster as a service from an external vendor&lt;/li>
&lt;li>Run a Gardener instance and host yourself a cluster with its help&lt;/li>
&lt;/ul>
&lt;p>Essentially, it was a &amp;ldquo;make or buy&amp;rdquo; decision that led to the founding of Gardener.&lt;/p>
&lt;h3 id="the-reason-why-we-choose-to-make-it">The Reason Why We Choose to &amp;ldquo;Make It&amp;rdquo;&lt;/h3>
&lt;p>Gardener allows to run Kubernetes clusters on various hyperscalers. It offers the same set of basic configuration options independent of the choosen infrastructure. This kind of harmonization supports any mulit-vendor strategy while reducing adoption costs for the individual teams. Just imagine having to deal with multiple vendors all offering vastly differentl Kubernetes clusters.&lt;/p>
&lt;p>Of course, there are plenty more reasons - from acquiring operational knowledge to having influence on the developed features - that made the pendulum swing towards &amp;ldquo;make it&amp;rdquo;.&lt;/p>
&lt;h2 id="what-exactly-is-gardener">What exactly is Gardener?&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/universal-kubernetes_e472f3.png" alt="">&lt;/p>
&lt;p>Gardener is a system to manage Kubernetes clusters. It is driven by the same &amp;ldquo;desired state&amp;rdquo; pattern as Kubernetes itself. In fact, it is using Kubernetes to run Kubernetes.&lt;/p>
&lt;p>A user may &amp;ldquo;desire&amp;rdquo; clusters with specific configuration on infrastructures such as GCP, AWS, Azure, Alicloud, Openstack, vsphere, &amp;hellip; and Gardener will make sure to create such a cluster and keep it running.&lt;/p>
&lt;p>If you take this rather simplistic principle of reconciliation and add the feature-richness of Gardener to it, you end up with universal Kubernetes at scale.&lt;/p>
&lt;p>Whether you need fleet management at minimal TCO or to look for a highly customizable control plane - we have it all.&lt;/p>
&lt;p>On top of that, Gardener-managed Kubernetes clusters fulfill the conformance standard set out by the CNCF and we submit our test results for certification.&lt;/p>
&lt;p>Have a look at the &lt;a href="https://cncf.landscape2.io/?item=platform--certified-kubernetes--installer--gardener">CNCF map&lt;/a> for more information or dive into the &lt;a href="https://testgrid.k8s.io/conformance-gardener">testgrid&lt;/a> directly.&lt;/p>
&lt;p>Gardener itself is open-source. Under the umbrella of &lt;a href="https://github.com/gardener">github.com/gardener&lt;/a> we develop the core functionalities as well as the extensions and you are welcome to contribute (by opening issues, feature requests or submitting code).&lt;/p>
&lt;p>Last time we counted, there were already 131 projects. That&amp;rsquo;s actually more projects than members of the organization.&lt;/p>
&lt;p>As of today, Gardener is mainly developed by SAP employees and SAP is an &amp;ldquo;adopter&amp;rdquo; as well, among StackIT, Telekom, Finanz Informatik Technologie Services GmbH and others. For a full list of adopters, see the &lt;a href="https://gardener.cloud/adopter/">Adopters page&lt;/a>.&lt;/p></description></item><item><title>Docs: Architecture</title><link>https://gardener.cloud/docs/getting-started/architecture/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/architecture/</guid><description>
&lt;h2 id="kubeception">Kubeception&lt;/h2>
&lt;p>Kubeception - Kubernetes in Kubernetes in Kubernetes&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kubeception_d7bdc4.png" alt="">&lt;/p>
&lt;p>In the classic setup, there is a dedicated host / VM to host the master components / control plane of a Kubernetes cluster. However, these are just normal programs that can easily be put into containers. Once in containers, Kubernetes Deployments and StatefulSets (for the etcd) can be made to watch over them. And by putting all that into a separate, dedicated Kubernetes cluster you get Kubernetes on Kubernetes, aka Kubeception (named after the famous movie Inception with Leonardo DiCaprio).&lt;/p>
&lt;p>But what are the advantages of running Kubernetes on Kubernetes? For one, it makes use of resources more reasonably. Instead of providing a dedicated computer or virtual machine for the control plane of a Kubernetes cluster - which will probably never be the right size but either too small or too big - you can dynamically scale the individual control plane components based on demand and maximize resource usage by combining the control planes of multiple Kubernetes clusters.&lt;/p>
&lt;p>In addition to that, it helps introducing a first layer of high availability. What happens if the API server suddenly stops responding to requests? In a traditional setup, someone would have to find out and manually restart the API server. In the Kubeception model, the API server is a Kubernetes Deployment and of course, it has sophisticated liveness- and readiness-probes. Should the API server fail, its liveness-probe will fail too and the pod in question simply gets restarted automatically - sometimes even before anybody would have noticed about the API server being unresponsive.&lt;/p>
&lt;p>In Gardener&amp;rsquo;s terminology, the cluster hosting the control plane components is called a seed cluster. The cluster that end users actually use (and whose control plane is hosted in the seed) is called a shoot cluster.&lt;/p>
&lt;p>The worker nodes of a shoot cluster are plain, simple virtual machines in a hyperscaler (EC2 instances in AWS, GCE instances in GCP or ECS instances in Alibaba Cloud). They run an operating system, a container runtime (e.g., containerd), and the kubelet that gets configured during node bootstrap to connect to the shoot&amp;rsquo;s API server. The API server in turn runs in the seed cluster and is exposed through an ingress. This connection happens over public internet and is - of course - TLS encrypted.&lt;/p>
&lt;p>In other terms: you use Kubernetes to run Kubernetes.&lt;/p>
&lt;h2 id="cluster-hierarchy-in-gardener">Cluster Hierarchy in Gardener&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/cluster-hierarchy_75e575.png" alt="">&lt;/p>
&lt;p>Gardener uses many Kubernetes clusters to eventually provide you with your very own shoot cluster.&lt;/p>
&lt;p>At the heart of Gardener&amp;rsquo;s cluster hierarchy is the garden cluster. Since Gardener is 100% Kubernetes native, a Kubernetes cluster is needed to store all Gardener related resources. The garden cluster is actually nodeless - it only consist of a control plane, an API server (actually two), an etcd, and a bunch of controllers. The garden cluster is the central brain of a Gardener landscape and the one you connect to in order to create, modify or delete shoot clusters - either with kubectl and a dedicated kubeconfig or though the Gardener dashboard.&lt;/p>
&lt;p>The seed clusters are next in the hierarchy - they are the clusters which will host the &amp;ldquo;kubeceptioned&amp;rdquo; control planes of the shoot clusters. For every hyperscaler supported in a Gardener landscpae, there would be at least one seed cluster. However, to reduce latencies as well as for scaling, Gardener landscapes have several different seeds in different regions across the globe to keep the distance between control planes and actual worker nodes small.&lt;/p>
&lt;p>Finally, there are the shoot clusters - what Gardener is all about. Shoot clusters are the clusters which you create through Gardener and which your workload gets deployed to.&lt;/p>
&lt;h2 id="gardener-components-overview">Gardener Components Overview&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/components_8e9e5c.png" alt="">&lt;/p>
&lt;p>From a very high level point of view, the important components of Gardener are:&lt;/p>
&lt;h3 id="the-gardener-api-endpoint">The Gardener API Endpoint&lt;/h3>
&lt;p>You can connect to the Gardener API Endpoint (i.e., the API server in the garden cluster) either through the dashboard or with kubectl, given that you have a proper kubeconfig for it.&lt;/p>
&lt;h3 id="the-seeds-running-the-shoot-cluster-control-planes">The Seeds Running the Shoot Cluster Control Planes&lt;/h3>
&lt;p>Inside each seed is one of the most important controllers in Gardener - the gardenlet. It spawns many other controllers, which will eventually create all resources for a shoot cluster, including all resources on the cloud providers such as virtual networks, security groups, and virtual machines.&lt;/p>
&lt;h2 id="gardeners-api-endpoint">Gardener&amp;rsquo;s API Endpoint&lt;/h2>
&lt;p>Kubernetes&amp;rsquo; API can be extended - either by CRDs or by API aggregation.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/api-endpoint-1_dd8f0f.png" alt="">&lt;/p>
&lt;p>API aggregation involves setting up a so called extension-API-server and registering it with the main Kubernetes API server. The extension API server will then serve resources of custom-defined API groups on its own. While the main Kubernetes API server is still used to handle RBAC, authorization, namespacing, quotas, limits, etc., all custom resources will be delegated to the extenstion-API-server. This is done through an APIService resource in the main API server - it specifies that, e.g., the API group &lt;code>core.gardener.cloud&lt;/code> is served by a dedicated extension-API-server and all requests concerning this API group should be forwarded the specified IP address or Kubernetes service name. Extension API servers can persist their resources in their very own etcd but they do not have to - instead, they can use the main API servers etcd as well.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/api-endpoint-2_595eff.png" alt="">&lt;/p>
&lt;p>Gardener uses its very own extension API server for its resources like Shoot, Seed, CloudProfile, SecretBinding, etc&amp;hellip; However, Gardener does not set up a dedicated etcd for its own extension API server - instead, it reuses the existing etcd of the main Kubernetes API server. This is absolutely possible since the resources of Gardener&amp;rsquo;s API are part of the API group &lt;code>gardener.cloud&lt;/code> and thus will not interfere with any resources of the main Kubernetes API in etcd.&lt;/p>
&lt;p>In case you are interested, you can read more on:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/setup-extension-api-server/">API Extension&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation">API Aggregation &lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/configure-aggregation-layer/#register-apiservice-objects">APIService Resource&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="gardener-api-resources">Gardener API Resources&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/api-resources_33c935.png" alt="">&lt;/p>
&lt;p>Since Gardener&amp;rsquo;s API endpoint is a regular Kubernetes cluster, it would theoretically serve all resources from the Kubernetes core API, including Pods, Deployments, etc. However, Gardener implements RBAC rules and disables certain controllers that make these resources inaccessible. Objects like Secrets, Namespaces, and ResourceQuotas are still available, though, as they play a vital role in Gardener.&lt;/p>
&lt;p>In addition, through Gardener&amp;rsquo;s extension API server, the API endpoint also serves Gardener&amp;rsquo;s custom resources like Projects, Shoots, CloudProfiles, Seeds, SecretBindings (those are relevant for users), ControllerRegistrations, ControllerDeployments, BackupBuckets, BackupEntries (those are relevant to an operator), etc.&lt;/p></description></item><item><title>Docs: Gardener Projects</title><link>https://gardener.cloud/docs/getting-started/project/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/project/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/overview_999502.png" alt="">&lt;/p>
&lt;p>Gardener is all about Kubernetes clusters, which we call shoots. However, Gardener also does user management, delicate permission management and offers technical accounts to integrate its services into other infrastructure. It allows you to create several quotas and it needs credentials to connect to cloud providers. All of these are arranged in multiple fully contained projects, each of which belongs to a dedicated user and / or group.&lt;/p>
&lt;h2 id="projects-on-yaml-level">Projects on YAML Level&lt;/h2>
&lt;p>Projects are a Kubernetes resource which can be expressed by YAML. The resource specification can be found in the &lt;a href="https://github.com/gardener/gardener/blob/master/docs/api-reference/core.md/#core.gardener.cloud/v1beta1.Project">API reference documentation&lt;/a>.&lt;/p>
&lt;p>A project&amp;rsquo;s specification defines a name, a description (which is a free-text field), a purpose (again, a free-text field), an owner, and members. In Gardener, user management is done on a project level. Therefore, projects can have different members with certain roles.&lt;/p>
&lt;p>In Gardener, a user can have one of five different roles: &lt;code>owner&lt;/code>, &lt;code>admin&lt;/code>, &lt;code>viewer&lt;/code>, &lt;code>UAM&lt;/code>, and &lt;code>service account manager&lt;/code>. A member with the &lt;code>viewer&lt;/code> role can see and list all clusters but cannot create, delete or modify them. For that, a member would need the &lt;code>admin&lt;/code> role. Another important role would be the &lt;code>uam&lt;/code> role - members with that role are allowed to manage members and technical users for a project. The &lt;code>owner&lt;/code> of a project is allowed to do all of that, regardless of what other roles might be assigned to him.&lt;/p>
&lt;p>Projects are getting reconciled by Gardener&amp;rsquo;s project-controller, a component of Gardener&amp;rsquo;s controller manager. The status of the last reconcilation, along with any potential failures, will be recorded in the project&amp;rsquo;s &lt;code>status&lt;/code> field.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/projects/">Projects&lt;/a>.&lt;/p>
&lt;p>In case you are interested, you can also view the source code for:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/gardener/gardener/blob/master/pkg/apis/core/types_project.go">The structure of a project API object&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/gardener/blob/master/pkg/controllermanager/controller/project/project/reconciler.go">Reconciling a project&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="gardener-projects-and-kubernetes-namespaces">Gardener Projects and Kubernetes Namespaces&lt;/h2>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
Each Gardener project corresponds to a Kubernetes namespace and all project specific resources are placed into it.
&lt;/div>
&lt;p>Even though projects are a dedicated Kubernetes resource, every project also corresponds to a dedicated namespace in the garden cluster. All project resources - including shoots - are placed into this namespace.&lt;/p>
&lt;p>You can ask Gardener to use a specific namespace name in the project manifest but usually, this field should be left empty. The namespace then gets created automatically by Gardener&amp;rsquo;s project-controller, with its name getting generated from the project&amp;rsquo;s name, prefixed by &amp;ldquo;garden-&amp;rdquo;.&lt;/p>
&lt;p>ResourceQuotas - if any - will be enforced on the project namespace.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Quotas&lt;/h4>
Since all Gardener resources are custom Kubernetes resources, the usual and well established concept of &lt;code>resourceQuotas&lt;/code> in Kubernetes can also be applied to Gardener resources. With a &lt;code>resourceQuota&lt;/code> that sets a hard limit on (e.g., &lt;code>count/shoots.core.gardener.cloud&lt;/code>) you can restrict the number of shoot clusters that can be created in a project.
&lt;/div>
&lt;h2 id="service-accounts">Service Accounts&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/service-account_c93395.png" alt="">&lt;/p>
&lt;p>Since Gardener is 100% Kubernetes, it can be easily used in a programmatic way - by just sending the resource manifest of a Gardener resource to its API server. To do so, a kubeconfig file and a (technical) user that the kubeconfig maps to are required.&lt;/p>
&lt;p>Next to project members, a project can have several service accounts - simple Kubernetes service accounts that are created in a project&amp;rsquo;s namespace. Consequently, every service account will also have its own, dedicated kubeconfig and they can be granted different roles through RoleBindings.&lt;/p>
&lt;p>To integrate Gardener with other infrastructure or CI/CD platforms, one can create a service account, obtain its kubeconfig and then automatically send shoot manifests to the Gardener API server. With that, Kubernetes clusters can be created, modified or deleted on the fly whenever they are needed.&lt;/p>
&lt;h2 id="infrastructure-secrets">Infrastructure Secrets&lt;/h2>
&lt;p>For Gardener to create all relevant infrastructure that a shoot cluster needs inside a cloud provider, it needs to know how to authenticate to the cloud provider&amp;rsquo;s API. This is done through regular secrets.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/secret_53cd82.png" alt="">&lt;/p>
&lt;p>Through the Gardener dashboard, secrets can be created for each supported cloud provider (using the dashboard is the preferred way, as it provides interactive help on what information needs to be placed into the secret and how the corresponding user account on the cloud provider should be configured). All of that is stored in a standard, opaque Kubernetes secret.&lt;/p>
&lt;p>Inside of a shoot manifest, a reference to that secret is given so that Gardener knows which secret to use for a given shoot. Consequently, different shoots, even though they are in the same project, can be created on multiple different cloud provider accounts. However, instead of referring to the secret directly, Gardener introduces another layer of indirection called a SecretBinding.&lt;/p>
&lt;p>In the shoot manifest, we refer to a SecretBinding and the SecretBinding in turn refers to the actual secret.&lt;/p></description></item><item><title>Docs: Control Plane Components</title><link>https://gardener.cloud/docs/getting-started/ca-components/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/ca-components/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>A cluster has a data plane and a control plane. The data plane is like a space station. It has certain components which keep everyone / everything alive and can operate autonomously to a certain extent. However, without mission control (and the occasional delivery of supplies) it cannot share information or receive new instructions.&lt;/p>
&lt;p>So let&amp;rsquo;s see what the mission control (control plane) of a Kubernetes cluster looks like.&lt;/p>
&lt;h2 id="kubeception">Kubeception&lt;/h2>
&lt;p>&lt;a href="https://gardener.cloud/docs/getting-started/architecture/#kubeception">Kubeception - Kubernetes in Kubernetes in Kubernetes&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kubeception_d7bdc4.png" alt="">&lt;/p>
&lt;p>In the classic setup, there is a dedicated host / VM to host the master components / control plane of a Kubernetes cluster. However, these are just normal programs that can easily be put into containers. Once in containers, we can make Kubernetes Deployments and StatefulSets (for the etcd) watch over them. And now we put all that into a separate, dedicated Kubernetes cluster - et voilÃ , we have Kubernetes in Kubernetes, aka Kubeception (named after the famous movie Inception with Leonardo DiCaprio).&lt;/p>
&lt;p>In Gardener&amp;rsquo;s terminology, the cluster hosting the control plane components is called a seed cluster. The cluster that end users actually use (and whose control plane is hosted in the seed) is called a shoot cluster.&lt;/p>
&lt;h2 id="control-plane-components-on-the-seed">Control Plane Components on the Seed&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/control-plane-components-1_461774.png" alt="">&lt;/p>
&lt;p>All control-plane components of a shoot cluster run in a dedicated namespace on the seed.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/control-plane-components-2_a23322.png" alt="">&lt;/p>
&lt;p>A control plane has lots of components:&lt;/p>
&lt;ul>
&lt;li>Everything needed to run vanilla Kubernetes&lt;/li>
&lt;li>etcd main &amp;amp; events (split for performance reasons)&lt;/li>
&lt;li>Kube-.*-manager&lt;/li>
&lt;li>CSI driver&lt;/li>
&lt;/ul>
&lt;p>Additionally, we deploy components needed to manage the cluster:&lt;/p>
&lt;ul>
&lt;li>Gardener Resource Manager (GRM)&lt;/li>
&lt;li>Machine Controller Manager (MCM)&lt;/li>
&lt;li>DNS Management&lt;/li>
&lt;li>VPN&lt;/li>
&lt;/ul>
&lt;p>There is also a set of components making our life easier (logging, monitoring) or adding additional features (cert manager).&lt;/p>
&lt;h2 id="core-components">Core Components&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/core-components-1_4ea211.png" alt="">&lt;/p>
&lt;p>Let&amp;rsquo;s take a close look at the API server as well as etcd.&lt;/p>
&lt;p>Secrets are encrypted at rest. When asking etcd for the data, the reply is still encrypted. Decryption is done by the API server which knows the necessary key.&lt;/p>
&lt;p>For non-HA clusters etcd has only 1 replica, while for HA clusters there are 3 replicas.&lt;/p>
&lt;p>One special remark is needed for Gardener&amp;rsquo;s deployment of etcd. The pods coming from the etcd-main StatefulSet contain two containers - one runs etcd, the other runs a program that periodically backs up etcd&amp;rsquo;s contents to an object store that is set up per seed cluster to make sure no data is lost. After all, etcd is the Achilles heel of all Kubernetes clusters. The backup container is also capable of performing a restore from the object store as well as defragment and compact the etcd datastore. For performance reasons, Gardener stores Kubernetes events in a separate etcd instance. By default, events are retained for 1h but can be kept longer if defined in the &lt;code>shoot.spec&lt;/code>.&lt;/p>
&lt;p>The kube API server (often called &amp;ldquo;kapi&amp;rdquo;) scales both horizontally and vertically.&lt;/p>
&lt;p>The kube API server is not directly exposed / reachable via its public hostname. Instead, Gardener runs a single LoadBalancer service backed by an istio gateway / envoy, which uses SNI to forward traffic.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/core-components-2_510a57.png" alt="">&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/overview/components/#kube-controller-manager">kube-controller-manager&lt;/a> (aka &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/">KCM&lt;/a>) is the component that contains all the controllers for the core Kubernetes objects such as Deployments, Services, PVCs, etc.&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/">Kubernetes scheduler&lt;/a> will assign pods to nodes.&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/">Cloud Controller Manager&lt;/a> (aka CCM) is the component that contains all functionality to talk to Cloud environments (e.g., create LoadBalancer services).&lt;/p>
&lt;p>The CSI driver is the storage subsystem of Kubernetes. It provisions and manages anything related to persistence.&lt;/p>
&lt;p>Without the cluster autoscaler, nodes could not be added or removed based on current pressure on the cluster resources. Without the VPA, pods would have fixed resource limits that could not change on demand.&lt;/p>
&lt;h2 id="gardener-specific-components">Gardener-Specific Components&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/gardener-specific-components_80c7db.png" alt="">&lt;/p>
&lt;p>&lt;strong>Shoot DNS service:&lt;/strong> External DNS management for resources within the cluster.&lt;/p>
&lt;p>&lt;strong>Machine Controller Manager:&lt;/strong> Responsible for managing VMs which will become nodes in the cluster.&lt;/p>
&lt;p>&lt;strong>Virtual Private Network deployments&lt;/strong> (aka &lt;a href="https://github.com/gardener/vpn">VPN&lt;/a>): Almost every communication between Kubernetes controllers and the API server is unidirectional - the controllers are given a kubeconfig and will establish a connection to the API server, which is exposed to all nodes of the cluster through a LoadBalancer. However, there are a few operations that require the API server to connect to the kubelet instead (e.g., for every webhook, when using kubectl exec or kubectl logs). Since every good Kubernetes cluster will have its worker nodes shielded behind firewalls to reduce the attack surface, Gardener establishes a VPN connection from the shoot&amp;rsquo;s internal network to the API server in the seed. For that, every shoot, as well as every control plane namespace in the seed, have openVPN pods in them that connect to each other (with the connection being established from the shoot to the seed).&lt;/p>
&lt;p>&lt;strong>Gardener Resource Manager:&lt;/strong> Tooling to deploy and manage Kubernetes resources required for cluster functionality.&lt;/p>
&lt;h2 id="machines">Machines&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/machines_b2fde8.png" alt="">&lt;/p>
&lt;p>&lt;strong>Machine Controller Manager&lt;/strong> (aka &lt;a href="https://github.com/gardener/machine-controller-manager">MCM&lt;/a>):&lt;/p>
&lt;p>The machine controller manager, which lives on the seed in a shoot&amp;rsquo;s control plane namespace, is the key component responsible for provisioning and removing worker nodes for a Kubernetes cluster. It acts on MachineClass, MachineDeployment, and MachineSet resources in the seed (think of them as the equivalent of Deployments and ReplicaSets) and controls the lifecycle of machine objects. Through a system of plugins, the MCM is the component that phones to the cloud provider&amp;rsquo;s API and bootstraps virtual machines.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/other-components/machine-controller-manager/faq/">MCM&lt;/a> and &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md">Cluster-autoscaler&lt;/a>.&lt;/p>
&lt;h2 id="managedresources">ManagedResources&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/managed-resources_2f5571.png" alt="">&lt;/p>
&lt;p>&lt;strong>Gardener Resource Manager&lt;/strong> (aka &lt;a href="https://gardener.cloud/docs/gardener/concepts/resource-manager/">GRM&lt;/a>):&lt;/p>
&lt;p>Gardener not only deploys components into the control plane namespace of the seed but also to the shoot (e.g., the counterpart of the VPN). Together with the components in the seed, Gardener needs to have a way to reconcile them.&lt;/p>
&lt;p>Enter the GRM - it reconciles on ManagedResources objects, which are descriptions of Kubernetes resources which are deployed into the seed or shoot by GRM. If any of these resources are modified or deleted by accident, the usual observe-analyze-act cycle will revert these potentially malicious changes back to the values that Gardener envisioned. In fact, all the components found in a shoot&amp;rsquo;s kube-system namespace are ManagedResources governed by the GRM. The actual resource definition is contained in secrets (as they may contain &amp;ldquo;secret&amp;rdquo; data), while the ManagedResources contain a reference to the secret containing the actual resource to be deployed and reconciled.&lt;/p>
&lt;h2 id="dns-records---internal-and-external">DNS Records - &amp;ldquo;Internal&amp;rdquo; and &amp;ldquo;External&amp;rdquo;&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/dns-records_2ce62f.png" alt="">&lt;/p>
&lt;p>The internal domain name is used by all Gardener components to talk to the API server. Even though it is called &amp;ldquo;internal&amp;rdquo;, it is still publicly routable.&lt;/p>
&lt;p>But most importantly, it is pre-defined and not configurable by the end user.&lt;/p>
&lt;p>Therefore, the &amp;ldquo;external&amp;rdquo; domain name exists. It is either a user owned domain or can be pre-defined for a Gardener landscape. It is used by any end user accessing the cluster&amp;rsquo;s API server.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/extensions/dnsrecord/">Contract: DNSRecord Resources&lt;/a>.&lt;/p>
&lt;h2 id="features-and-observability">Features and Observability&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/features-observability_ba6fa5.png" alt="">&lt;/p>
&lt;p>Gardener runs various health checks to ensure that the cluster works properly. The Network Problem Detector gives information about connectivity within the cluster and to the API server.&lt;/p>
&lt;p>&lt;strong>Certificate Management:&lt;/strong> allows to request certificates via the ACME protocol (e.g., issued by Let&amp;rsquo;s Encrypt) from within the cluster. For detailed information, have a look at the &lt;a href="https://github.com/gardener/cert-management#certificate-management">cert-manager project&lt;/a>.&lt;/p>
&lt;p>&lt;strong>Observability stack:&lt;/strong> Gardener deploys observability components and gathers logs and metrics for the control-plane &amp;amp; kube-system namespace. Also provided out-of-the-box is a UI based on Plutono (fork of Grafana) with pre-defined dashboards to access and query the monitoring data. For more information, see &lt;a href="https://gardener.cloud/docs/getting-started/observability/">Observability&lt;/a>.&lt;/p>
&lt;h2 id="ha-control-plane">HA Control Plane&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/ha-control-plane_7fec08.png" alt="">&lt;/p>
&lt;p>As the title indicates, the HA control plane feature is only about the control plane. Setting up the data plane to span multiple zones is part of the worker spec of a shoot.&lt;/p>
&lt;p>HA control planes can be configured as part of the shoot&amp;rsquo;s spec. The available types are:&lt;/p>
&lt;ul>
&lt;li>Node&lt;/li>
&lt;li>Zone&lt;/li>
&lt;/ul>
&lt;p>Both work similarly and just differ in the failure domain the concepts are applied to.&lt;/p>
&lt;p>For detailed guidance and more information, see the &lt;a href="https://gardener.cloud/docs/guides/high-availability/">High Availability Guides&lt;/a>.&lt;/p>
&lt;h2 id="zonal-ha-control-planes">Zonal HA Control Planes&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/zonal-ha-control-planes_90c628.png" alt="">&lt;/p>
&lt;p>Zonal HA is the most likely setup for shoots with &lt;code>purpose: production&lt;/code>.&lt;/p>
&lt;p>The starting point is a regular (non-HA) control plane. etcd and most controllers are singletons and the kube-apiserver might have been scaled up to several replicas.&lt;/p>
&lt;p>To get to an HA setup we need:&lt;/p>
&lt;ul>
&lt;li>A minimum of 3 replicas of the API server&lt;/li>
&lt;li>3 replicas for etcd (both main and events)&lt;/li>
&lt;li>A second instance for each controller (e.g., controller manager, csi-driver, scheduler, etc.) that can take over in case of failure (active / passive).&lt;/li>
&lt;/ul>
&lt;p>To distribute those pods across zones, well-known concepts like PodTopologySpreadConstraints or Affinities are applied.&lt;/p>
&lt;h2 id="kube-system-namespace">kube-system Namespace&lt;/h2>
&lt;img src="https://gardener.cloud/__resources/kube-system-namespace_1d0e42.png" alt="kube-system-namespace" width="50%"/>
&lt;p>For a fully functional cluster, a few components need to run on the data plane side of the diagram. They all exist in the kube-system namespace. Let&amp;rsquo;s have a closer look at them.&lt;/p>
&lt;h3 id="networking">Networking&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kube-system-namespace-2_1cbd3b.png" alt="">&lt;/p>
&lt;p>On each node we need a CNI (container network interface) plugin. Gardener offers Calico or Cilium as network provider for a shoot. When using Calico, a kube-proxy is deployed. Cilium does not need a kube-proxy, as it takes care of its tasks as well.&lt;/p>
&lt;p>The CNI plugin ensures pod-to-pod communication within the cluster. As part of it, it assigns cluster-internal IP addresses to the pods and manages the network devices associated with them. When an overlay network is enabled, calico will also manage the routing of pod traffic between different nodes.&lt;/p>
&lt;p>On the other hand, kube-proxy implements the actual service routing (cilium can do this as well and no kube-proxy is needed). Whenever packets go to a service&amp;rsquo;s IP address, they are re-routed based on IPtables rules maintained by kube-proxy to reach the actual pods backing the service. kube-proxy operates on endpoint-slices and manages IPtables on EVERY node. In addition, kube-proxy provides a health check endpoint for services with &lt;code>externalTrafficPolicy=local&lt;/code>, where traffic only gets to nodes that run a pod matching the selector of the service.&lt;/p>
&lt;p>The egress filter implements basic filtering of outgoing traffic to be compliant with SAP&amp;rsquo;s policies.&lt;/p>
&lt;p>And what happens if the pods crashloop, are missing or otherwise broken?&lt;/p>
&lt;p>Well, in case kube-proxy is broken, service traffic will degrade over time (depending on the pod churn rate and how many kube-proxy pods are broken).&lt;/p>
&lt;p>When calico is failing on a node, no new pods can start there as they don&amp;rsquo;t get any IP address assigned. It might also fail to add routes to newly added nodes. Depending on the error, deleting the pod might help.&lt;/p>
&lt;h3 id="dns-system">DNS System&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kube-system-namespace-3_237a52.png" alt="">&lt;/p>
&lt;p>For a normal service in Kubernetes, a cluster-internal DNS record that resolves to the service&amp;rsquo;s ClusterIP address is being created. In Gardener (similar to most other Kubernetes offerings) CoreDNS takes care of this aspect. To reduce the load when it comes to upstream DNS queries, Gardener deploys a DNS cache to each node by default. It will also forward queries outside the cluster&amp;rsquo;s search domain directly to the upstream DNS server. For more information, see &lt;a href="https://gardener.cloud/docs/gardener/node-local-dns/">NodeLocalDNS Configuration&lt;/a> and &lt;a href="https://gardener.cloud/docs/gardener/dns-autoscaling/">DNS autoscaling&lt;/a>.&lt;/p>
&lt;p>In addition to this optimization, Gardener allows &lt;a href="https://gardener.cloud/docs/gardener/custom-dns-config/">custom DNS configuration to be added to CoreDNS&lt;/a> via a dedicated ConfigMap.&lt;/p>
&lt;p>In case this customization is related to non-Kubernetes entities, you may configure the shoot&amp;rsquo;s NodeLocalDNS to forward to CoreDNS instead of upstream (&lt;code>disableForwardToUpstreamDNS: true&lt;/code>).&lt;/p>
&lt;p>A broken DNS system on any level will cause disruption / service degradation for applications within the cluster.&lt;/p>
&lt;h3 id="health-checks-and-metrics">Health Checks and Metrics&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kube-system-namespace-4_f5796b.png" alt="">&lt;/p>
&lt;p>Gardener deploys probes checking the health of individual nodes. In a similar fashion, a network health check probes connectivity within the cluster (node to node, pod to pod, pod to api-server, &amp;hellip;).&lt;/p>
&lt;p>They provide the data foundation for Gardener&amp;rsquo;s monitoring stack together with the metrics collecting / exporting components.&lt;/p>
&lt;h3 id="connectivity-components">Connectivity Components&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kube-system-namespace-5_0ad4c6.png" alt="">&lt;/p>
&lt;p>From the perspective of the data plane, the shoot&amp;rsquo;s API server is reachable via the cluster-internal service &lt;code>kubernetes.default.svc.cluster.local&lt;/code>. The apiserver-proxy intercepts connections to this destination and changes it so that the traffic is forwarded to the kube-apiserver service in the seed cluster. For more information, see &lt;a href="https://github.com/gardener/gardener/blob/764df0ee5ebc13b2634eba98169b409244f19bfe/docs/usage/control-plane-endpoints-and-ports.md#kube-apiserver-via-apiserver-proxy">kube-apiserver via apiserver-proxy&lt;/a>.&lt;/p>
&lt;p>The second component here is the VPN shoot. It initiates a VPN connection to its counterpart in the seed. This way, there is no open port / Loadbalancer needed on the data plane. The VPN connection is used for any traffic flowing from the control plane to the data plane. If the VPN connection is broken, port-forwarding or log querying with kubectl will not work. In addition, webhooks will stop functioning properly.&lt;/p>
&lt;h3 id="csi-driver">csi-driver&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kube-system-namespace-6_19a863.png" alt="">&lt;/p>
&lt;p>The last component to mention here is the csi-driver that is deployed as a Daemonset to all nodes. It registers with the kubelet and takes care of the mounting of volume types it is responsible for.&lt;/p></description></item><item><title>Docs: Gardener Shoots</title><link>https://gardener.cloud/docs/getting-started/shoots/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/shoots/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/k8s-cluster_c316bb.png" alt="">&lt;/p>
&lt;p>A Kubernetes cluster consists of a control plane and a data plane. The data plane runs the actual containers on worker nodes (which translate to physical or virtual machines). For the control and data plane to work together properly, lots of components need matching configuration.&lt;/p>
&lt;p>Some configurations are standardized but some are also very specific to the needs of a cluster&amp;rsquo;s user / workload. Ideally, you want a properly configured cluster with the possibility to fine-tune some settings.&lt;/p>
&lt;h2 id="concept-of-a-shoot">Concept of a &amp;ldquo;Shoot&amp;rdquo;&lt;/h2>
&lt;p>In Gardener, Kubernetes clusters (with their control plane and their data plane) are called shoot clusters or simply shoots.
For Gardener, a shoot is just another Kubernetes resource. Gardener components watch it and act upon changes (e.g., creation). It comes with reasonable default settings but also allows fine-tuned configuration. And on top of it, you get a status providing health information, information about ongoing operations, and so on.&lt;/p>
&lt;p>Luckily there is a dashboard to get started.&lt;/p>
&lt;h2 id="basic-configuration-options">Basic Configuration Options&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/basic_configurations_1_7c6894.png" alt="">&lt;/p>
&lt;p>Every cluster needs a name - after all, it is a Kubernetes resource and therefore unique within a namespace.&lt;/p>
&lt;p>The Kubernetes version will be used as a starting point. Once a newer version is available, you can always update your existing clusters (but not downgrade, as this is not supported by Kubernetes in general).&lt;/p>
&lt;p>The &amp;ldquo;purpose&amp;rdquo; affects some configuration (like automatic deployment of a monitoring stack or setting up certain alerting rules) and generally indicates the importance of a cluster.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/basic_configurations_2_e640a3.png" alt="">&lt;/p>
&lt;p>Start by selecting the infrastructure you want to use. The choice will be mapped to a cloud profile that contains provider specific information like the available (actual) OS images, zones and regions or machine types.&lt;/p>
&lt;p>Each data plane runs in an infrastructure account owned by the end user. By selecting the infrastructure secret containing the accounts credentials, you are granting Gardener access to the respective account to create / manage resources.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Note&lt;/h4>
&lt;p>Changing the account after the creation of a cluster is not possible. The credentials can be updated with a new key or even user but have to stay within the same account.&lt;/p>
&lt;p>Currently, there is no way to move a single cluster to a different account. You would rather have to re-create a cluster and migrate workloads by different means.&lt;/p>
&lt;/div>
&lt;p>As part of the infrastructure you chose, the region for data plane has to be chosen as well. The Gardener scheduler will try to place the control plane on a seed cluster based on a minimal distance strategy. See &lt;a href="https://gardener.cloud/docs/gardener/concepts/scheduler/">Gardener Scheduler&lt;/a> for more details.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/basic_configurations_3_827a18.png" alt="">&lt;/p>
&lt;p>Up next, the networking provider (CNI) for the cluster has to be selected. At the point of writing, it is possible to choose between Calico and Cilium. If not specified in the shoot&amp;rsquo;s manifest, default CIDR ranges for nodes, services, and pods will be used.&lt;/p>
&lt;p>In order to run any workloads in your cluster, you need nodes. The worker section lets you specify the most important configuration options. For beginners, the machine type is probably the most relevant field, together with the machine image (operating system).&lt;/p>
&lt;p>The machine type is provider-specific and configured in the cloud profile. Check your respective cloud profile if you&amp;rsquo;re missing a machine type. Maybe it is available in general but unavailable in your selected region.&lt;/p>
&lt;p>The operating system your machines will run is the next thing to choose. Debian-based &lt;a href="https://github.com/gardenlinux/gardenlinux">GardenLinux&lt;/a> is the best choice for most use cases.&lt;/p>
&lt;p>Other specifications for the workers include the volume type and size. These settings affect the root disk of each node. Therefore we would always recommend to use an SSD-based type to avoid i/o issues.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Caveat&lt;/h4>
Some machine types (e.g., bare-metal machine types on OpenStack) require you to omit the volume type and volume size settings.
&lt;/div>
&lt;p>The autoscaler parameter defines the initial elasticity / scalability of your cluster. The cluster-autoscaler will add more nodes up to the maximum defined here when your workload grows and remove nodes in case your workload shrinks. The minimum number of nodes should be equal to or higher than the number of zones. You can distribute the nodes of a worker pool among all zones available to your cluster. This is the first step in running HA workloads.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/basic_configurations_4_6139e7.png" alt="">&lt;/p>
&lt;p>Once per day, all clusters reconcile. This means all controllers will check if there are any updates they have to apply (e.g., new image version for ETCD). The maintenance window defines when this daily operation will be triggered. It is important to understand that there is no opt-out for reconciliation.&lt;/p>
&lt;p>It is also possible to confine updates to the shoot spec to be applied only during this time. This can come in handy when you want to bundle changes or prevent changes to be applied outside a well-known time window.&lt;/p>
&lt;p>You can allow Gardener to automatically update your cluster&amp;rsquo;s Kubernetes patch version and/or OS version (of the nodes). Take this decision consciously! Whenever a new Kubernetes patch version or OS version is set to &lt;code>supported&lt;/code> in the respective cloud profile, auto update will upgrade your cluster during the next maintenance window. If you fail to (manually) upgrade the Kubernetes or OS version before they expire, force-upgrades will take place during the maintenance window.&lt;/p>
&lt;h3 id="result">Result&lt;/h3>
&lt;p>The result of your provided inputs and a set of conscious default values is a shoot resource that, once applied, will be acted upon by various Gardener components. The status section represents the intermediate steps / results of these operations. A typical shoot creation flow would look like this:&lt;/p>
&lt;ol>
&lt;li>Assign control plane to a seed.&lt;/li>
&lt;li>Create infrastructure resources in the data plane account (e.g., VPC, gateways, &amp;hellip;)&lt;/li>
&lt;li>Deploy control plane incl. DNS records.&lt;/li>
&lt;li>Create nodes (VMs) and bootstrap kubelets.&lt;/li>
&lt;li>Deploy kube-system components to nodes.&lt;/li>
&lt;/ol>
&lt;h2 id="how-to-access-a-shoot">How to Access a Shoot&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/access-shoot_955308.png" alt="">&lt;/p>
&lt;p>Static credentials for shoots were discontinued in Gardener with Kubernetes v1.27. Short lived credentials need to be used instead. You can create/request tokens directly via Gardener or delegate authentication to an identity provider.&lt;/p>
&lt;p>A short-lived admin kubeconfig can be requested by using kubectl. If this is something you do frequently, consider switching to &lt;a href="https://github.com/gardener/gardenlogin">gardenlogin&lt;/a>, which helps you with it.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/access-shoot-2_6937da.png" alt="">&lt;/p>
&lt;p>An alternative is to use an identity provider and issue OIDC tokens.&lt;/p>
&lt;h2 id="what-can-you-configure">What can you configure?&lt;/h2>
&lt;p>With the basic configuration options having been introduced, it is time to discuss more possibilities. Gardener offers a variety of options to tweak the control plane&amp;rsquo;s behavior - like defining an event TTL (default 1h), adding an OIDC configuration or activating some feature gates. You could alter the scheduling profile and define an audit logging policy. In addition, the control plane can be configured to run in HA mode (applied on a node or zone level), but keep in mind that once you enable HA, you cannot go back.&lt;/p>
&lt;p>In case you have specific requirements for the cluster internal DNS, Gardener offers a plugin mechanism for custom core DNS rules or optimization with node-local DNS. For more information, see &lt;a href="https://gardener.cloud/docs/gardener/custom-dns-config/">Custom DNS Configuration&lt;/a> and &lt;a href="https://gardener.cloud/docs/gardener/node-local-dns/">NodeLocalDNS Configuration&lt;/a>.&lt;/p>
&lt;p>Another category of configuration options is dedicated to the nodes and the infrastructure they are running on. Every provider has their own perks and some of them are exposed. Check the detailed documentation of the relevant extension for your infrastructure provider.&lt;/p>
&lt;p>You can fine-tune the cluster-autoscaler or help the kubelet to cope better with your workload.&lt;/p>
&lt;h2 id="worker-pools">Worker Pools&lt;/h2>
&lt;p>There are a couple of ways to configure a worker pool. One of them is to set everything in the Gardener dashboard. However, only a subset of options is presented there.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/worker-pools-1_ba1993.png" alt="">&lt;/p>
&lt;p>A slightly more complex way is to set the configuration through the yaml file itself.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/worker-pools-2_d4c2d9.png" alt="">&lt;/p>
&lt;p>This allows you to configure much more properties of a worker pool, like the timeout after which an unhealthy machine is getting replaced. For more options, see the &lt;a href="https://gardener.cloud/docs/gardener/api-reference/core/#worker">Worker&lt;/a> API reference.&lt;/p>
&lt;h2 id="how-to-change-things">How to Change Things&lt;/h2>
&lt;p>Since a shoot is just another Kubernetes resource, changes can be applied via kubectl. For convenience, the basic settings are configurable via the dashboard&amp;rsquo;s UI. It also has a &amp;ldquo;yaml&amp;rdquo; tab where you can alter all of the shoot&amp;rsquo;s specification in your browser. Once applied, the cluster will reconcile eventually and your changes become active (or cause an error).&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/change-things_ea1b03.png" alt="">&lt;/p>
&lt;h2 id="immutability-in-a-shoot">Immutability in a Shoot&lt;/h2>
&lt;p>While Gardener allows you to modify existing shoot clusters, it is important to remember that not all properties of a shoot can be changed after it is created.&lt;/p>
&lt;p>For example, it is not possible to move a shoot to a different infrastructure account. This is mainly rooted in the fact that discs and network resources are bound to your account.&lt;/p>
&lt;p>Another set of options that become immutable are most of the network aspects of a cluster. On an infrastructure level the VPC cannot be changed and on a cluster level things like the pod / service cidr ranges, together with the nodeCIDRmask, are set for the lifetime of the cluster.&lt;/p>
&lt;p>Some other things can be changed, but not reverted. While it is possible to add more zones to a cluster on an infrastructure level (assuming that an appropriate CIDR range is available), removing zones is not supported. Similarly, upgrading Kubernetes versions is comparable to a one-way ticket. As of now, Kubernetes does not support downgrading. Lastly, the HA setting of the control plane is immutable once specified.&lt;/p>
&lt;h2 id="crazy-botany">Crazy Botany&lt;/h2>
&lt;p>Since remembering all these options can be quite challenging, here is very helpful resource - an &lt;a href="https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml">example shoot&lt;/a> with all the latest options ðŸŽ‰&lt;/p></description></item><item><title>Docs: Shoot Lifecycle</title><link>https://gardener.cloud/docs/getting-started/lifecycle/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/lifecycle/</guid><description>
&lt;h2 id="reconciliation-in-kubernetes-and-gardener">Reconciliation in Kubernetes and Gardener&lt;/h2>
&lt;p>The starting point of all reconciliation cycles is the constant observation of both the desired and actual state. A component would analyze any differences between the two states and try to converge the actual towards the desired state using appropriate actions. Typically, a component is responsible for a single resource type but it also watches others that have an implication on it.&lt;/p>
&lt;p>As an example, the Kubernetes controller for ReplicaSets will watch pods belonging to it in order to ensure that the specified replica count is fulfilled. If one pod gets deleted, the controller will create a new pod to enforce the desired over the actual state.&lt;/p>
&lt;p>This is all standard behaviour, as Gardener is following the native Kubernetes approach. All elements of a shoot cluster have a representation in Kubernetes resources and controllers are watching / acting upon them.&lt;/p>
&lt;p>If we pick up the example of the ReplicaSet - a user typically creates a &lt;code>deployment&lt;/code> resource and the ReplicaSet is implicitly generated on the way to create the pods. Similarly, Gardener takes the user&amp;rsquo;s intent (shoot) and creates lots of domain specific resources on the way. They all reconcile and make sure their actual and desired states match.&lt;/p>
&lt;h2 id="updating-the-desired-state-of-a-shoot">Updating the Desired State of a Shoot&lt;/h2>
&lt;p>Based on the shoot&amp;rsquo;s specifications, Gardener will create network resources on a hyperscaler, backup resources for the ETCD, credentials, and other resources, but also representations of the worker pools. Eventually, this process will result in a fully functional Kubernetes cluster.&lt;/p>
&lt;p>If you change the desired state, Gardener will reconcile the shoot and run through the same cycle to ensure the actual state matches the desired state.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/update-shoot-state_ed8700.png" alt="">&lt;/p>
&lt;p>For example, the (infrastructure-specific) machine type can be changed within the shoot resource. The following reconciliation will pick up the change and initiate the creation of new nodes with a different machine type and the removal of the old nodes.&lt;/p>
&lt;h2 id="maintenance-window-and-daily-reconciliation">Maintenance Window and Daily Reconciliation&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/maintenance-window_5f1928.png" alt="">&lt;/p>
&lt;p>EVERY shoot cluster reconciles once per day during the so-called &amp;ldquo;maintenance window&amp;rdquo;. You can confine the rollout of spec changes to this window.&lt;/p>
&lt;p>Additionally, the daily reconciliation will help pick up all kind of version changes. When a new Gardener version was rolled out to the landscape, shoot clusters will pick up any changes during their next reconciliation. For example, if a new Calico version is introduced to fix some bug, it will automatically reach all shoots.&lt;/p>
&lt;h2 id="impact-of-a-change">Impact of a Change&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/change-impact_4ce151.png" alt="">&lt;/p>
&lt;p>It is important to be aware of the impacts that a change can have on a cluster and the workloads within it.&lt;/p>
&lt;p>An operator pushing a new Gardener version with a new calico image to a landscape will cause all calico pods to be re-created. Another example would be the rollout of a new etcd backup-restore image. This would cause etcd pods to be re-created, rendering a non-HA control plane unavailable until etcd is up and running again.&lt;/p>
&lt;p>When you change the shoot spec, it can also have significant impact on the cluster. Imagine that you have changes the machine type of a worker pool. This will cause new machines to be created and old machines to be deleted. Or in other words: all nodes will be drained, the pods will be evicted and then re-created on newly created nodes.&lt;/p>
&lt;h2 id="kubernetes-version-update-minor--patch">Kubernetes Version Update (Minor + Patch)&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/Kubernetes-version-update_ea24c1.png" alt="">&lt;/p>
&lt;p>Some operations are rather common and have to be performed on a regular basis. Updating the Kubernetes version is one them. Patch updates cause relatively little disruption, as only the control-plane pods will be re-created with new images and the kubelets on all nodes will restart.&lt;/p>
&lt;p>A minor version update is more impactful - it will cause all nodes to be recreated and rolls components of the control plane.&lt;/p>
&lt;h2 id="os-version-update">OS Version Update&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/os-update_1652c2.png" alt="">&lt;/p>
&lt;p>The OS version is defined for each worker pool and can be changed per worker pool. You can freely switch back and forth. However, as there is no in-place update, each change will cause the entire worker pool to roll and nodes will be replaced.
For OS versions different update strategies can be configured. Please check the &lt;a href="https://github.com/gardener/gardener/blob/master/docs/usage/shoot_versions.md/#update-path-for-machine-image-versions">documentation&lt;/a> for details.&lt;/p>
&lt;h2 id="version-classifications">Version Classifications&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/version-classifications_f6b6f1.png" alt="">&lt;/p>
&lt;p>Gardener has the following classifications for Kubernetes and OS image versions:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>preview&lt;/code>: still in testing phase (several versions can be in preview at the same time)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>supported&lt;/code>: recommended version&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>deprecated&lt;/code>: a new version has been set to &amp;ldquo;supported&amp;rdquo;, updating is recommended (might have an expiration date)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;code>expired&lt;/code>: cannot be used anymore, clusters using this version will be force-upgraded&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Version information is maintained in the relevant cloud profile resource. There might be circumstances where a version will never become &lt;code>supported&lt;/code> but instead move to &lt;code>deprecated&lt;/code> directly. Similarly, a version might be directly introduced as &lt;code>supported&lt;/code>.&lt;/p>
&lt;h2 id="autoupdate--forced-updates">AutoUpdate / Forced Updates&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/auto-update_814bee.png" alt="">&lt;/p>
&lt;p>AutoUpdate for a machine image version will update all node pools to the latest supported version based on the defined update strategy. Whenever a new version is set to &lt;code>supported&lt;/code>, the cluster will pick it up during its next maintenance window.&lt;/p>
&lt;p>For Kubernetes versions the mechanism is the same, but only applied to patch version. This means that the cluster will be kept on the latest supported patch version of a specific minor version.&lt;/p>
&lt;p>In case a version used in a cluster expires, there is a force update during the next maintenance window. In a worst case scenario, 2 minor versions expire simultaneously. Then there will be two consecutive minor updates enforced.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/shoot_versions/">Shoot Kubernetes and Operating System Versioning in Gardener&lt;/a>.&lt;/p>
&lt;h2 id="applying-changes-to-a-seed">Applying Changes to a Seed&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/seeds-change_37b290.png" alt="">&lt;/p>
&lt;p>It is important to keep in mind that a seed is just another Kubernetes cluster. As such, it has its own lifecycle (daily reconciliation, maintenance, etc.) and is also a subject to change.&lt;/p>
&lt;p>From time to time changes need to be applied to the seed as well. Some (like updating the OS version) cause the node pool to roll. In turn, this will cause the eviction of ALL pods running on the affected node. If your etcd is evicted and you don&amp;rsquo;t have a highly available control plane, it will cause downtime for your cluster. Your workloads will continue to run ,of course, but your cluster&amp;rsquo;s API server will not function until the etcd is up and running again.&lt;/p></description></item><item><title>Docs: Observability</title><link>https://gardener.cloud/docs/getting-started/observability/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/observability/</guid><description>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>Gardener offers out-of-the-box observability for the control plane, Gardener managed system-components, and the nodes of a shoot cluster.&lt;/p>
&lt;p>Having your workload survive on day 2 can be a challenge. The goal of this topic is to give you the tools with which to observe, analyze, and alert when the control plane or system components of your cluster become unhealthy. This will let you guide your containers through the storm of operating in a production environment.&lt;/p></description></item><item><title>Docs: Features</title><link>https://gardener.cloud/docs/getting-started/features/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/features/</guid><description/></item><item><title>Docs: Common Pitfalls</title><link>https://gardener.cloud/docs/getting-started/common-pitfalls/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://gardener.cloud/docs/getting-started/common-pitfalls/</guid><description>
&lt;h2 id="architecture">Architecture&lt;/h2>
&lt;h3 id="containers-will-not-fix-a-broken-architecture">Containers will NOT fix a broken architecture!&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/microservices_0c8923.png" alt="">&lt;/p>
&lt;p>Running a highly distributed system has advantages, but of course, those come at a cost. In order to succeed, one would need:&lt;/p>
&lt;ul>
&lt;li>Logging&lt;/li>
&lt;li>Tracing&lt;/li>
&lt;li>No singleton&lt;/li>
&lt;li>Tolerance to failure of individual instances&lt;/li>
&lt;li>Automated config / change management&lt;/li>
&lt;li>Kubernetes knowledge&lt;/li>
&lt;/ul>
&lt;h2 id="scalability">Scalability&lt;/h2>
&lt;p>Most scalability dimensions are interconnected with others. If a cluster grows beyond reasonable defaults, it can still function very well. But tuning it comes at the cost of time and can influence stability negatively.&lt;/p>
&lt;p>Take the number of nodes and pods, for example. Both are connected and you cannot grow both towards their individual limits, as you would face issues way before reaching any theoretical limits.&lt;/p>
&lt;p>Reading the &lt;a href="https://gardener.cloud/docs/guides/administer-shoots/scalability/">Scalability of Gardener Managed Kubernetes Clusters&lt;/a> guide is strongly recommended in order to understand the topic of scalability within Kubernetes and Gardener.&lt;/p>
&lt;h3 id="a-small-sample-of-things-that-can-grow-beyond-reasonable-limits">A Small Sample of Things That Can Grow Beyond Reasonable Limits&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/hibernation-1_54d110.png" alt="">&lt;/p>
&lt;p>When scaling a cluster, there are plenty of resources that can be exhausted or reach a limit:&lt;/p>
&lt;ul>
&lt;li>The API server will be scaled horizontally and vertically by Gardener. However, it can still consume too much resources to fit onto a single node on the seed. In this case, you can only reduce the load on the API server. This should not happen with regular usage patterns though.&lt;/li>
&lt;li>ETCD disk space: 8GB is the limit. If you have too many resources or a high churn rate, a cluster can run out of ETCD capacity. In such a scenario it will stop working until defragmented, compacted, and cleaned up.&lt;/li>
&lt;li>The number of nodes is limited by the network configuration (pod cidr range &amp;amp; node cidr mask). Also, there is a reasonable number of nodes (300) that most workloads should not exceed. It is possible to go beyond but doing so requires careful tuning and consideration of connected scaling dimensions (like the number of pods per node).&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>The availability of your cluster is directly impacted by the way you use it.&lt;/strong>&lt;/p>
&lt;h3 id="infrastructure-capacity-and-quotas">Infrastructure Capacity and Quotas&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/capacity_030ae5.png" alt="">&lt;/p>
&lt;p>Sometimes requests cannot be fulfilled due to shortages on the infrastructure side. For example, a certain instance type might not be available and new Kubernetes nodes of this type cannot be added. It is a good practice to use the &lt;a href="https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/expander/priority/readme.md">cluster-autoscaler&amp;rsquo;s priority expander&lt;/a> and have a secondary node pool.&lt;/p>
&lt;p>Sometimes, it is not the physical capacity but exhausted quotas within an infrastructure account that result in limits. Obviously, there should be sufficient quota to create as many VMs as needed. But there are also other resources that are created in the infrastructure that need proper quotas:&lt;/p>
&lt;ul>
&lt;li>Loadbalancers&lt;/li>
&lt;li>VPC&lt;/li>
&lt;li>Disks&lt;/li>
&lt;li>Routes (often forgotten, but very important for clusters without overlay network; typically defaults to around 50 routes, meaning that 50 nodes is the maximum a cluster can have)&lt;/li>
&lt;li>&amp;hellip;&lt;/li>
&lt;/ul>
&lt;h3 id="nodecidrmasksize">NodeCIDRMaskSize&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/mask-size_6e1bbd.png" alt="">&lt;/p>
&lt;p>Upon cluster creation, there are several settings that are network related. For example, the address space for Pods has to be defined. In this case, it is a &lt;code>/16&lt;/code> subnet that includes a total of 65.536 hosts. However, that does not imply that you can easily use all addresses at the same point in time.&lt;/p>
&lt;p>As part of the Kubernetes network setup, the &lt;code>/16&lt;/code> network is divided into smaller subnets and each node gets a distinct subnet. The size of this subnet defaults to &lt;code>/24&lt;/code>. It can also be specified (but not changed later).&lt;/p>
&lt;p>Now, as you create more nodes, you have a total of 256 subnets that can be assigned to nodes, thus limiting the total number of nodes of this cluster to 256.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/shoot_networking/">Shoot Networking&lt;/a>.&lt;/p>
&lt;h2 id="overlapping-vpcs">Overlapping VPCs&lt;/h2>
&lt;h3 id="avoid-overlapping-cidr-ranges-in-vpcs">Avoid Overlapping CIDR Ranges in VPCs&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/cidr-ranges_eba6d1.png" alt="">&lt;/p>
&lt;p>Gardener can create shoot cluster resources in an existing / user-created VPC. However, you have to make sure that the CIDR ranges used by the shoots nodes or subnets for zones do not overlap with other shoots deployed to the same VPC.&lt;/p>
&lt;p>In case of an overlap, there might be strange routing effects, and packets ending up at a wrong location.&lt;/p>
&lt;h2 id="expired-credentials">Expired Credentials&lt;/h2>
&lt;p>&lt;img src="https://gardener.cloud/__resources/expired-credentials_e0e111.png" alt="">&lt;/p>
&lt;p>Credentials expire or get revoked. When this happens to the actively used infrastructure credentials of a shoot, the cluster will stop working after a while. New nodes cannot be added, LoadBalancers cannot be created, and so on.&lt;/p>
&lt;p>You can update the credentials stored in the project namespace and reconcile the cluster to replicate the new keys to all relevant controllers. Similarly, when doing a planned rotation one should wait until the shoot reconciled successfully before invalidating the old credentials.&lt;/p>
&lt;h2 id="autoupdate-breaking-clusters">AutoUpdate Breaking Clusters&lt;/h2>
&lt;p>Gardener can automatically update a shoot&amp;rsquo;s Kubernetes patch version, when a new patch version is labeled as &amp;ldquo;supported&amp;rdquo;. Automatically updating of the OS images works in a similar way. Both are triggered by the &amp;ldquo;supported&amp;rdquo; classification in the respective cloud profile and can be enabled / disabled as part a shoot&amp;rsquo;s spec.&lt;/p>
&lt;p>Additionally, when a minor Kubernetes / OS version expires, Gardener will force-update the shoot to the next supported version.&lt;/p>
&lt;p>Turning on AutoUpdate for a shoot may be convenient but comes at the risk of potentially unwanted changes. While it is possible to switch to another OS version, updates to the Kubernetes version are a one way operation and cannot be reverted.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Recommendation&lt;/h4>
Control the version lifecycle separately for any cluster that hosts important workload.
&lt;/div>
&lt;h2 id="node-draining">Node Draining&lt;/h2>
&lt;h3 id="node-draining-and-pod-disruption-budget">Node Draining and Pod Disruption Budget&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/node-draining_d59d8d.gif" alt="">&lt;/p>
&lt;p>Typically, nodes are drained when:&lt;/p>
&lt;ul>
&lt;li>There is a update of the OS / Kubernetes minor version&lt;/li>
&lt;li>An Operator cordons &amp;amp; drains a node&lt;/li>
&lt;li>The cluster-autoscaler wants to scale down&lt;/li>
&lt;/ul>
&lt;p>Without a PodDistruptionBudget, pods will be terminated as fast as possible. If an application has 2 out of 2 replicas running on the drained node, this will probably cause availability issues.&lt;/p>
&lt;h3 id="node-draining-with-pdb">Node Draining with PDB&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/node-draining-pdb_b4128a.gif" alt="">&lt;/p>
&lt;p>PodDisruptionBudgets can help to manage a graceful node drain. However, if no disruptions are allowed there, the node drain will be blocked until it reaches a timeout. Only then will the nodes be terminated but without respecting PDB thresholds.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Recommendation&lt;/h4>
Configure PDBs and allow disruptions.
&lt;/div>
&lt;h2 id="pod-resource-requests-and-limits">Pod Resource Requests and Limits&lt;/h2>
&lt;h3 id="resource-consumption">Resource Consumption&lt;/h3>
&lt;p>Pods consume resources and, of course, there are only so many resources available on a single node. Setting requests will make the scheduling much better, as the scheduler has more information available.&lt;/p>
&lt;p>Specifying limits can help, but can also limit an application in unintended ways. A recommendation to start with:&lt;/p>
&lt;ul>
&lt;li>Do not set CPU limits (CPU is compressible and throttling is really hard to detect)&lt;/li>
&lt;li>Set memory limits and monitor OOM kills / restarts of workload (typically detectable by container status exit code 137 and corresponding events). This will decrease the likelihood of OOM situations on the node itself. However, for critical workloads it might be better to have uncapped growth and rather risk a node going OOM.&lt;/li>
&lt;/ul>
&lt;p>Next, consider if assigning the workload to quality of service class &lt;code>guaranteed&lt;/code> is needed. Again - this can help or be counterproductive. It is important to be aware of its implications. For more information, see &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/">Pod Quality of Service Classes&lt;/a>.&lt;/p>
&lt;p>Tune &lt;code>shoot.spec.Kubernetes.kubeReserved&lt;/code> to protect the node (kubelet) in case of a workload pod consuming too much resources. It is very helpful to ensure a high level of stability.&lt;/p>
&lt;p>If the usage profile changes over time, the VPA can help a lot to adapt the resource requests / limits automatically.&lt;/p>
&lt;h2 id="webhooks">Webhooks&lt;/h2>
&lt;h3 id="user-deployed-webhooks-in-kubernetes">User-Deployed Webhooks in Kubernetes&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/user-webhook_f00e79.gif" alt="">&lt;/p>
&lt;p>By default, any request to the API server will go through a chain of checks. Let&amp;rsquo;s take the example of creating a pod.&lt;/p>
&lt;p>When the resource is submitted to the API server, it will be checked against the following validations:&lt;/p>
&lt;ul>
&lt;li>Is the user authorized to perform this action?&lt;/li>
&lt;li>Is the pod definitionactually valid?&lt;/li>
&lt;li>Are the specified values allowed?&lt;/li>
&lt;/ul>
&lt;p>Additionally, there is the defaulting - like the injection of the &lt;code>default&lt;/code> service account&amp;rsquo;s name, if nothing else is specified.&lt;/p>
&lt;p>This chain of admission control and mutation can be enhanced by the user. Read about &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">dynamic admission control&lt;/a> for more details.&lt;/p>
&lt;p>&lt;code>ValidatingWebhookConfiguration&lt;/code>: allow or deny requests based on custom rules&lt;/p>
&lt;p>&lt;code>MutatingWebhookConfiguration&lt;/code>: change Ð° resource before it is actually stored in etcd (that is, before any other controller acts upon)&lt;/p>
&lt;p>Both &lt;code>ValidatingWebhookConfiguration&lt;/code> as well as &lt;code>MutatingWebhookConfiguration&lt;/code> resources:&lt;/p>
&lt;ul>
&lt;li>specify for which resources and operations these checks should be executed.&lt;/li>
&lt;li>specify how to reach the webhook server (typically a service running on the data plane of a cluster)&lt;/li>
&lt;li>rely on a webhook server performing a review and reply to the &lt;code>admissionReview&lt;/code> request&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://gardener.cloud/__resources/user-webhook-fail_a3c301.gif" alt="">&lt;/p>
&lt;p>What could possibly go wrong?
Due to the separation of control plane and data plane in Gardener&amp;rsquo;s architecture, webhooks have the potential to break a cluster.
If the webhook server is not responding in time with a valid answer, the request should timeout and the failure policy is invoked. Depending on the scope of the webhook, frequent failures may cause downtime for applications.
Common causes for failure are:&lt;/p>
&lt;ul>
&lt;li>The call to the webhook is made through the VPN tunnel. VPN / connection issues can happen both on the side of the seed as well as the shoot and would render the webhook unavailable from the perspective of the control plane.&lt;/li>
&lt;li>The traffic cannot reach the pod (network issue, pod not available)&lt;/li>
&lt;li>The pod is processing too slow (e.g., because there are too many requests)&lt;/li>
&lt;/ul>
&lt;h3 id="timeout">Timeout&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/timeout_b55c76.png" alt="">&lt;/p>
&lt;p>Webhooks are a very helpful feature of Kubernetes. However, they can easily be configured to break a shoot cluster. Take the timeout, for example. High timeouts (&amp;gt;15s) can lead to blocking requests of control plane components. That&amp;rsquo;s because most control-plane API calls are made with a client-side timeout of 30s, so if a webhook has &lt;code>timeoutSeconds=30&lt;/code>, the overall request might still fail as there is overhead in communication with the API server and other potential webhooks.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Recommendation&lt;/h4>
Webhooks (esp. mutating) may be called sequentially and thus adding up their individual timeouts. Even with a &lt;code>faliurePolicy=ignore&lt;/code> the timeout will stop the request.
&lt;/div>
&lt;h3 id="recommendations">Recommendations&lt;/h3>
&lt;p>Problematic webhooks are reported as part of a shoot&amp;rsquo;s status. In addition to timeouts, it is crucial to exclude the &lt;code>kube-system&lt;/code> namespace and (potentially non-namespaced) resources that are necessary for the cluster to function properly. Those should not be subject to a user-defined webhook.&lt;/p>
&lt;p>In particular, a webhook should not operate on:&lt;/p>
&lt;ul>
&lt;li>the &lt;code>kube-system&lt;/code> namespace&lt;/li>
&lt;li>&lt;code>Endpoints&lt;/code> or &lt;code>EndpointSlices&lt;/code>&lt;/li>
&lt;li>&lt;code>Nodes&lt;/code>&lt;/li>
&lt;li>&lt;code>PodSecurityPolicies&lt;/code>&lt;/li>
&lt;li>&lt;code>ClusterRoles&lt;/code>&lt;/li>
&lt;li>&lt;code>ClusterRoleBindings&lt;/code>&lt;/li>
&lt;li>&lt;code>CustomResourceDefinitions&lt;/code>&lt;/li>
&lt;li>&lt;code>ApiServices&lt;/code>&lt;/li>
&lt;li>&lt;code>CertificateSigningRequests&lt;/code>&lt;/li>
&lt;li>&lt;code>PriorityClasses&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Example:&lt;/strong>&lt;/p>
&lt;p>A webhook checks node objects upon creation and has a &lt;code>failurePolicy: fail&lt;/code>. If the webhook does not answer in time (either due to latency or because there is no pod serving it), new nodes cannot join the cluster.&lt;/p>
&lt;p>For more information, see &lt;a href="https://gardener.cloud/docs/gardener/shoot_status/#constraints">Shoot Status&lt;/a>.&lt;/p>
&lt;h2 id="conversion-webhooks">Conversion Webhooks&lt;/h2>
&lt;h3 id="who-installs-a-conversion-webhook">Who installs a conversion webhook?&lt;/h3>
&lt;p>If you have written your own &lt;code>CustomResourceDefinition&lt;/code> (CRD) and made a version upgrade, you will also have consciously written &amp;amp; deployed the conversion webhook.&lt;/p>
&lt;p>However, sometimes, you simply use helm or kustomize to install a (third-party) dependency that contains CRDs. Of course, those can contain conversion webhooks as well. As a user of a cluster, please make sure to be aware what you deploy.&lt;/p>
&lt;h3 id="crd-with-a-conversion-webhook">CRD with a Conversion Webhook&lt;/h3>
&lt;p>&lt;img src="https://gardener.cloud/__resources/conversion-webhook-crd_887cd7.png" alt="">&lt;/p>
&lt;p>Conversion webhooks are tricky. Similarly to regular webhooks, they should have a low timeout. However, they cannot be remediated automatically and can cause errors in the control plane. For example, if a webhook is invoked but not available, it can block the garbage collection run by the kube-controller-manager.&lt;/p>
&lt;p>In turn, when deleting something like a &lt;code>deployment&lt;/code>, dependent resources like &lt;code>pods&lt;/code> will not be deleted automatically.&lt;/p>
&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Recommendation&lt;/h4>
Try to avoid conversion webhooks. They are valid and can be used, but should not stay in place forever. Complete the upgrade to a new version of the CRD as soon as possible.
&lt;/div>
&lt;p>For more information, see the &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#webhook-conversion">Webhook Conversion&lt;/a>, &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#upgrade-existing-objects-to-a-new-stored-version">Upgrade Existing Objects to a New Stored Version&lt;/a>, and &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#version-priority">Version Priority&lt;/a> topics in the Kubernetes documentation.&lt;/p></description></item></channel></rss>