<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/gardener/operations/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/gardener/operations/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Operations | Gardener</title><meta name=description content><meta property="og:title" content="Operations"><meta property="og:description" content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/gardener/operations/"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Operations"><meta itemprop=description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Operations"><meta name=twitter:description content="Project Gardener Website - A Managed Kubernetes Service Done Right"><link rel=preload href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css as=style><link href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7N3XF5XLGV"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7N3XF5XLGV",{anonymize_ip:!1})}</script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.c16b2fa65f2a285b7fddc42db82cadf0.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/gardener/operations/>Return to the regular view of this page</a>.</p></div><h1 class=title>Operations</h1><div class=content></div></div><div class=td-content><h1 id=pg-9c04e3b14be52ecef60a80a497b4d880>1 - Configuration</h1><h1 id=gardener-configuration-and-usage>Gardener Configuration and Usage</h1><p>Gardener automates the full lifecycle of Kubernetes clusters as a service.
Additionally, it has several extension points allowing external controllers to plug-in to the lifecycle.
As a consequence, there are several configuration options for the various custom resources that are partially required.</p><p>This document describes the:</p><ol><li><a href=#configuration-and-usage-of-gardener-as-operatoradministrator>Configuration and usage of Gardener as operator/administrator</a>.</li><li><a href=#configuration-and-usage-of-gardener-as-end-userstakeholdercustomer>Configuration and usage of Gardener as end-user/stakeholder/customer</a>.</li></ol><h2 id=configuration-and-usage-of-gardener-as-operatoradministrator>Configuration and Usage of Gardener as Operator/Administrator</h2><p>When we use the terms &ldquo;operator/administrator&rdquo;, we refer to both the people deploying and operating Gardener.
Gardener consists of the following components:</p><ol><li><code>gardener-apiserver</code>, a Kubernetes-native API extension that serves custom resources in the Kubernetes-style (like <code>Seed</code>s and <code>Shoot</code>s), and a component that contains multiple admission plugins.</li><li><code>gardener-admission-controller</code>, an HTTP(S) server with several handlers to be used in a <a href=https://github.com/gardener/gardener/blob/master/charts/gardener/controlplane/charts/application/templates/validatingwebhook-admission-controller.yaml>ValidatingWebhookConfiguration</a>.</li><li><code>gardener-controller-manager</code>, a component consisting of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for maintaining <code>Shoot</code>s, reconciling <code>Project</code>s).</li><li><code>gardener-scheduler</code>, a component that assigns newly created <code>Shoot</code> clusters to appropriate <code>Seed</code> clusters.</li><li><code>gardenlet</code>, a component running in seed clusters and consisting out of multiple controllers that implement reconciliation and deletion flows for some of the custom resources (e.g., it contains the logic for reconciliation and deletion of <code>Shoot</code>s).</li></ol><p>Each of these components have various configuration options.
The <code>gardener-apiserver</code> uses the standard API server library maintained by the Kubernetes community, and as such it mainly supports command line flags.
Other components use so-called componentconfig files that describe their configuration in a Kubernetes-style versioned object.</p><h3 id=configuration-file-for-gardener-admission-controller>Configuration File for Gardener Admission Controller</h3><p>The Gardener admission controller only supports one command line flag, which should be a path to a valid admission-controller configuration file.
Please take a look at this <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-admission-controller.yaml>example configuration</a>.</p><h3 id=configuration-file-for-gardener-controller-manager>Configuration File for Gardener Controller Manager</h3><p>The Gardener controller manager only supports one command line flag, which should be a path to a valid controller-manager configuration file.
Please take a look at this <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-controller-manager.yaml>example configuration</a>.</p><h3 id=configuration-file-for-gardener-scheduler>Configuration File for Gardener Scheduler</h3><p>The Gardener scheduler also only supports one command line flag, which should be a path to a valid scheduler configuration file.
Please take a look at this <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardener-scheduler.yaml>example configuration</a>.
Information about the concepts of the Gardener scheduler can be found at <a href=/docs/gardener/concepts/scheduler/>Gardener Scheduler</a>.</p><h3 id=configuration-file-for-gardenlet>Configuration File for gardenlet</h3><p>The gardenlet also only supports one command line flag, which should be a path to a valid gardenlet configuration file.
Please take a look at this <a href=https://github.com/gardener/gardener/blob/master/example/20-componentconfig-gardenlet.yaml>example configuration</a>.
Information about the concepts of the Gardenlet can be found at <a href=/docs/gardener/concepts/gardenlet/>gardenlet</a>.</p><h3 id=system-configuration>System Configuration</h3><p>After successful deployment of the four components, you need to setup the system.
Let&rsquo;s first focus on some &ldquo;static&rdquo; configuration.
When the <code>gardenlet</code> starts, it scans the <code>garden</code> namespace of the garden cluster for <code>Secret</code>s that have influence on its reconciliation loops, mainly the <code>Shoot</code> reconciliation:</p><ul><li><p><strong>Internal domain secret</strong> - contains the DNS provider credentials (having appropriate privileges) which will be used to create/delete the so-called &ldquo;internal&rdquo; DNS records for the Shoot clusters, please see this <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-internal-domain.yaml>yaml file</a> for an example.</p><ul><li>This secret is used in order to establish a stable endpoint for shoot clusters, which is used internally by all control plane components.</li><li>The DNS records are normal DNS records but called &ldquo;internal&rdquo; in our scenario because only the kubeconfigs for the control plane components use this endpoint when talking to the shoot clusters.</li><li>It is forbidden to change the internal domain secret if there are existing shoot clusters.</li></ul></li><li><p><strong>Default domain secrets</strong> (optional) - contain the DNS provider credentials (having appropriate privileges) which will be used to create/delete DNS records for a default domain for shoots (e.g., <code>example.com</code>), please see this <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-default-domain.yaml>yaml file</a> for an example.</p><ul><li>Not every end-user/stakeholder/customer has its own domain, however, Gardener needs to create a DNS record for every shoot cluster.</li><li>As landscape operator you might want to define a default domain owned and controlled by you that is used for all shoot clusters that don&rsquo;t specify their own domain.</li><li>If you have multiple default domain secrets defined you can add a priority as an annotation (<code>dns.gardener.cloud/domain-default-priority</code>) to select which domain should be used for new shoots during creation. The domain with the highest priority is selected during shoot creation. If there is no annotation defined, the default priority is <code>0</code>, also all non integer values are considered as priority <code>0</code>.</li></ul></li><li><p><strong>Alerting secrets</strong> (optional) - contain the alerting configuration and credentials for the <a href=https://prometheus.io/docs/alerting/alertmanager/>AlertManager</a> to send email alerts. It is also possible to configure the monitoring stack to send alerts to an AlertManager not deployed by Gardener to handle alerting. Please see this <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-alerting.yaml>yaml file</a> for an example.</p><ul><li>If email alerting is configured:<ul><li>An AlertManager is deployed into each seed cluster that handles the alerting for all shoots on the seed cluster.</li><li>Gardener will inject the SMTP credentials into the configuration of the AlertManager.</li><li>The AlertManager will send emails to the configured email address in case any alerts are firing.</li></ul></li><li>If an external AlertManager is configured:<ul><li>Each shoot has a <a href=https://prometheus.io/docs/introduction/overview/>Prometheus</a> responsible for monitoring components and sending out alerts. The alerts will be sent to a URL configured in the alerting secret.</li><li>This external AlertManager is not managed by Gardener and can be configured however the operator sees fit.</li><li>Supported authentication types are no authentication, basic, or mutual TLS.</li></ul></li></ul></li><li><p><strong>OpenVPN Diffie-Hellmann Key secret</strong> (optional) - contains the self-generated Diffie-Hellmann key used by OpenVPN in your landscape, please see this <a href=https://github.com/gardener/gardener/blob/master/example/10-secret-openvpn-diffie-hellman.yaml>yaml file</a> for an example.</p><ul><li>If you don&rsquo;t specify a custom key, then a default key is used, but for productive landscapes it&rsquo;s recommend to create a landscape-specific key and define it.</li></ul></li><li><p><strong>Global monitoring secrets</strong> (optional) - contains basic authentication credentials for the Prometheus aggregating metrics for all clusters.</p><ul><li>These secrets are synced to each seed cluster and used to gain access to the aggregate monitoring components.</li></ul></li></ul><p>Apart from this &ldquo;static&rdquo; configuration there are several custom resources extending the Kubernetes API and used by Gardener.
As an operator/administrator, you have to configure some of them to make the system work.</p><h3 id=configuration-and-usage-of-gardener-as-end-userstakeholdercustomer>Configuration and Usage of Gardener as End-User/Stakeholder/Customer</h3><p>As an end-user/stakeholder/customer, you are using a Gardener landscape that has been setup for you by another team.
You don&rsquo;t need to care about how Gardener itself has to be configured or how it has to be deployed.
Take a look at <a href=/docs/gardener/concepts/apiserver/>Gardener API Server</a> - the topic describes which resources are offered by Gardener.
You may want to have a more detailed look for <code>Project</code>s, <code>SecretBinding</code>s, <code>Shoot</code>s, and <code>(Cluster)OpenIDConnectPreset</code>s.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-aff8a0e855c667b461c3138baa093be0>2 - Control Plane Migration</h1><h1 id=control-plane-migration>Control Plane Migration</h1><h2 id=prerequisites>Prerequisites</h2><p>Also, the involved Seeds need to have enabled <code>BackupBucket</code>s.</p><h2 id=shootstate>ShootState</h2><p><code>ShootState</code> is an API resource which stores non-reconstructible state and data required to completely recreate a <code>Shoot</code>&rsquo;s control plane on a new <code>Seed</code>. The <code>ShootState</code> resource is created on <code>Shoot</code> creation in its <code>Project</code> namespace and the required state/data is persisted during <code>Shoot</code> creation or reconciliation.</p><h2 id=shoot-control-plane-migration>Shoot Control Plane Migration</h2><p>Triggering the migration is done by changing the <code>Shoot</code>&rsquo;s <code>.spec.seedName</code> to a <code>Seed</code> that differs from the <code>.status.seedName</code>, we call this <code>Seed</code> a <code>"Destination Seed"</code>. This action can only be performed by an operator with the necessary RBAC. If the Destination <code>Seed</code> does not have a backup and restore configuration, the change to <code>spec.seedName</code> is rejected. Additionally, this Seed must not be set for deletion and must be healthy.</p><p>If the <code>Shoot</code> has different <code>.spec.seedName</code> and <code>.status.seedName</code>, a process is started to prepare the Control Plane for migration:</p><ol><li><code>.status.lastOperation</code> is changed to <code>Migrate</code>.</li><li>Kubernetes API Server is stopped and the extension resources are annotated with <code>gardener.cloud/operation=migrate</code>.</li><li>Full snapshot of the ETCD is created and terminating of the Control Plane in the <code>Source Seed</code> is initiated.</li></ol><p>If the process is successful, we update the status of the <code>Shoot</code> by setting the <code>.status.seedName</code> to the null value. That way, a restoration is triggered in the <code>Destination Seed</code> and <code>.status.lastOperation</code> is changed to <code>Restore</code>. The control plane migration is completed when the <code>Restore</code> operation has completed successfully.</p><p>The etcd backups will be copied over to the <code>BackupBucket</code> of the <code>Destination Seed</code> during control plane migration and any future backups will be uploaded there.</p><h2 id=triggering-the-migration>Triggering the Migration</h2><p>For controlplane migration, operators with the necessary RBAC can use the <a href=/docs/gardener/concepts/scheduler/#shootsbinding-subresource><code>shoots/binding</code></a> subresource to change the <code>.spec.seedName</code>, with the following commands:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export NAMESPACE=my-namespace
</span></span><span style=display:flex><span>export SHOOT_NAME=my-shoot
</span></span><span style=display:flex><span>kubectl get --raw /apis/core.gardener.cloud/v1beta1/namespaces/<span style=color:#a31515>${</span>NAMESPACE<span style=color:#a31515>}</span>/shoots/<span style=color:#a31515>${</span>SHOOT_NAME<span style=color:#a31515>}</span> | jq -c <span style=color:#a31515>&#39;.spec.seedName = &#34;&lt;destination-seed&gt;&#34;&#39;</span> | kubectl replace --raw /apis/core.gardener.cloud/v1beta1/namespaces/<span style=color:#a31515>${</span>NAMESPACE<span style=color:#a31515>}</span>/shoots/<span style=color:#a31515>${</span>SHOOT_NAME<span style=color:#a31515>}</span>/binding -f - | jq -r <span style=color:#a31515>&#39;.spec.seedName&#39;</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-f1b7d259d2662f6282ed4c300db9e27d>3 - Istio</h1><h1 id=istio>Istio</h1><p><a href=https://istio.io>Istio</a> offers a service mesh implementation with focus on several important features - traffic, observability, security, and policy.</p><h2 id=prerequisites>Prerequisites</h2><ul><li>Third-party JWT is used, therefore each Seed cluster where this feature is enabled must have <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> enabled.</li><li>Kubernetes 1.16+</li></ul><h2 id=differences-with-istios-default-profile>Differences with Istio&rsquo;s Default Profile</h2><p>The <a href=https://istio.io/docs/setup/additional-setup/config-profiles/>default profile</a> which is recommended for production deployment, is not suitable for the Gardener use case, as it offers more functionality than desired. The current installation goes through heavy refactorings due to the <code>IstioOperator</code> and the mixture of Helm values + Kubernetes API specification makes configuring and fine-tuning it very hard. A more simplistic deployment is used by Gardener. The differences are the following:</p><ul><li>Telemetry is not deployed.</li><li><code>istiod</code> is deployed.</li><li><code>istio-ingress-gateway</code> is deployed in a separate <code>istio-ingress</code> namespace.</li><li><code>istio-egress-gateway</code> is not deployed.</li><li>None of the Istio addons are deployed.</li><li>Mixer (deprecated) is not deployed.</li><li>Mixer CDRs are not deployed.</li><li>Kubernetes <code>Service</code>, Istio&rsquo;s <code>VirtualService</code> and <code>ServiceEntry</code> are <strong>NOT</strong> advertised in the service mesh. This means that if a <code>Service</code> needs to be accessed directly from the Istio Ingress Gateway, it should have <code>networking.istio.io/exportTo: "*"</code> annotation. <code>VirtualService</code> and <code>ServiceEntry</code> must have <code>.spec.exportTo: ["*"]</code> set on them respectively.</li><li>Istio injector is not enabled.</li><li>mTLS is enabled by default.</li></ul><h2 id=handling-multiple-availability-zones-with-istio>Handling Multiple Availability Zones with Istio</h2><p>For various reasons, e.g., improved resiliency to certain failures, it may be beneficial to use multiple availability zones in a seed cluster. While availability zones have advantages in being able to cover some failure domains, they also come with some additional challenges. Most notably, the latency across availability zone boundaries is higher than within an availability zone. Furthermore, there might be additional cost implied by network traffic crossing an availability zone boundary. Therefore, it may be useful to try to keep traffic within an availability zone if possible. The istio deployment as part of Gardener has been adapted to allow this.</p><p>A seed cluster spanning multiple availability zones may be used for <a href=/docs/gardener/usage/shoot_high_availability/>highly-available shoot control planes</a>. Those control planes may use a single or multiple availability zones. In addition to that, ordinary non-highly-available shoot control planes may be scheduled to such a seed cluster as well. The result is that the seed cluster may have control planes spanning multiple availability zones and control planes that are pinned to exactly one availability zone. These two types need to be handled differently when trying to prevent unnecessary cross-zonal traffic.</p><p>The goal is achieved by using multiple istio ingress gateways. The default istio ingress gateway spans all availability zones. It is used for multi-zonal shoot control planes. For each availability zone, there is an additional istio ingress gateway, which is utilized only for single-zone shoot control planes pinned to this availability zone. This is illustrated in the following diagram.</p><p><img src=/__resources/multi-zonal-istio_8c8fa9.png alt="Multi Availability Zone Handling in Istio"></p><p>Please note that operators may need to perform additional tuning to prevent cross-zonal traffic completely. The <a href=/docs/gardener/operations/seed_settings/#load-balancer-services>loadbalancer settings in the seed specification</a> offer various options, e.g., by setting the external traffic policy to <code>local</code> or using infrastructure specific loadbalancer annotations.</p><p>Furthermore, note that this approach is also taken in case <a href=/docs/gardener/usage/exposureclasses/><code>ExposureClass</code>es</a> are used. For each exposure class, additional zonal istio ingress gateways may be deployed to cover for single-zone shoot control planes using the exposure class.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-73a31cc4dd448862d8d593d06861c54c>4 - Managed Seed</h1><h1 id=register-shoot-as-seed>Register Shoot as Seed</h1><p>An existing shoot can be registered as a seed by creating a <code>ManagedSeed</code> resource. This resource contains:</p><ul><li>The name of the shoot that should be registered as seed.</li><li>A <code>gardenlet</code> section that contains:<ul><li><code>gardenlet</code> deployment parameters, such as the number of replicas, the image, etc.</li><li>The <code>GardenletConfiguration</code> resource that contains controllers configuration, feature gates, and a <code>seedConfig</code> section that contains the <code>Seed</code> spec and parts of its metadata.</li><li>Additional configuration parameters, such as the garden connection bootstrap mechanism (see <a href=/docs/gardener/concepts/gardenlet/#tls-bootstrapping>TLS Bootstrapping</a>), and whether to merge the provided configuration with the configuration of the parent <code>gardenlet</code>.</li></ul></li></ul><p><code>gardenlet</code> is deployed to the shoot, and it registers a new seed upon startup based on the <code>seedConfig</code> section.</p><blockquote><p><strong>Note:</strong> Earlier Gardener allowed specifying a <code>seedTemplate</code> directly in the <code>ManagedSeed</code> resource. This feature is discontinued, any seed configuration must be via the <code>GardenletConfiguration</code>.</p></blockquote><p>Note the following important aspects:</p><ul><li>Unlike the <code>Seed</code> resource, the <code>ManagedSeed</code> resource is namespaced. Currently, managed seeds are restricted to the <code>garden</code> namespace.</li><li>The newly created <code>Seed</code> resource always has the same name as the <code>ManagedSeed</code> resource. Attempting to specify a different name in the <code>seedConfig</code> will fail.</li><li>The <code>ManagedSeed</code> resource must always refer to an existing shoot. Attempting to create a <code>ManagedSeed</code> referring to a non-existing shoot will fail.</li><li>A shoot that is being referred to by a <code>ManagedSeed</code> cannot be deleted. Attempting to delete such a shoot will fail.</li><li>You can omit practically everything from the <code>gardenlet</code> section, including all or most of the <code>Seed</code> spec fields. Proper defaults will be supplied in all cases, based either on the most common use cases or the information already available in the <code>Shoot</code> resource.</li><li>Also, if your seed is configured to host HA shoot control planes, then <code>gardenlet</code> will be deployed with multiple replicas across nodes or availability zones by default.</li><li>Some <code>Seed</code> spec fields, for example the provider type and region, networking CIDRs for pods, services, and nodes, etc., must be the same as the corresponding <code>Shoot</code> spec fields of the shoot that is being registered as seed. Attempting to use different values (except empty ones, so that they are supplied by the defaulting mechanims) will fail.</li></ul><h2 id=deploying-gardenlet-to-the-shoot>Deploying gardenlet to the Shoot</h2><p>To register a shoot as a seed and deploy <code>gardenlet</code> to the shoot using a default configuration, create a <code>ManagedSeed</code> resource similar to the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: seedmanagement.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ManagedSeed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-managed-seed
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  shoot:
</span></span><span style=display:flex><span>    name: crazy-botany
</span></span><span style=display:flex><span>  gardenlet: {}
</span></span></code></pre></div><p>For an example that uses non-default configuration, see <a href=https://github.com/gardener/gardener/blob/master/example/55-managedseed-gardenlet.yaml>55-managed-seed-gardenlet.yaml</a></p><h3 id=renewing-the-gardenlet-kubeconfig-secret>Renewing the Gardenlet Kubeconfig Secret</h3><p>In order to make the <code>ManagedSeed</code> controller renew the gardenlet&rsquo;s kubeconfig secret, annotate the <code>ManagedSeed</code> with <code>gardener.cloud/operation=renew-kubeconfig</code>. This will trigger a reconciliation during which the kubeconfig secret is deleted and the bootstrapping is performed again (during which gardenlet obtains a new client certificate).</p><p>It is also possible to trigger the renewal on the secret directly, see <a href=/docs/gardener/concepts/gardenlet/#rotate-certificates-using-bootstrap-kubeconfig>Rotate Certificates Using Bootstrap kubeconfig</a>.</p><h3 id=specifying-apiserver-replicas-and-autoscaler-options>Specifying <code>apiServer</code> <code>replicas</code> and <code>autoscaler</code> Options</h3><p>There are few configuration options that are not supported in a <code>Shoot</code> resource but due to backward compatibility reasons it is possible to specify them for a <code>Shoot</code> that is referred by a <code>ManagedSeed</code>. These options are:</p><table><thead><tr><th>Option</th><th>Description</th></tr></thead><tbody><tr><td><code>apiServer.autoscaler.minReplicas</code></td><td>Controls the minimum number of <code>kube-apiserver</code> replicas for the shoot registered as seed cluster.</td></tr><tr><td><code>apiServer.autoscaler.maxReplicas</code></td><td>Controls the maximum number of <code>kube-apiserver</code> replicas for the shoot registered as seed cluster.</td></tr><tr><td><code>apiServer.replicas</code></td><td>Controls how many <code>kube-apiserver</code> replicas the shoot registered as seed cluster gets by default.</td></tr></tbody></table><p>It is possible to specify these options via the <code>shoot.gardener.cloud/managed-seed-api-server</code> annotation on the Shoot resource. Example configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    shoot.gardener.cloud/managed-seed-api-server: <span style=color:#a31515>&#34;apiServer.replicas=3,apiServer.autoscaler.minReplicas=3,apiServer.autoscaler.maxReplicas=6&#34;</span>
</span></span></code></pre></div><h3 id=enforced-configuration-options>Enforced Configuration Options</h3><p>The following configuration options are enforced by Gardener API server for the ManagedSeed resources:</p><ol><li><p>The vertical pod autoscaler should be enabled from the Shoot specification.</p><p>The vertical pod autoscaler is a prerequisite for a Seed cluster. It is possible to enable the VPA feature for a Seed <a href=/docs/gardener/operations/seed_settings/#vertical-pod-autoscaler>(using the Seed spec)</a> and for a Shoot <a href=/docs/gardener/usage/shoot_autoscaling/#vertical-pod-auto-scaling>(using the Shoot spec)</a>. In context of <code>ManagedSeed</code>s, enabling the VPA in the Seed spec (instead of the Shoot spec) offers less flexibility and increases the network transfer and cost. Due to these reasons, the Gardener API server enforces the vertical pod autoscaler to be enabled from the Shoot specification.</p></li><li><p>The nginx-ingress addon should not be enabled for a Shoot referred by a ManagedSeed.</p><p>An Ingress controller is also a prerequisite for a Seed cluster. For a Seed cluster, it is possible to enable Gardener managed Ingress controller or to deploy self-managed Ingress controller. There is also the nginx-ingress addon that can be enabled for a Shoot (using the Shoot spec). However, the Shoot nginx-ingress addon is in deprecated mode and it is not recommended for production clusters. Due to these reasons, the Gardener API server does not allow the Shoot nginx-ingress addon to be enabled for ManagedSeeds.</p></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-8e9a6dc089917b10cb1841164169028f>5 - Network Policies</h1><h1 id=networkpolicys-in-garden-seed-shoot-clusters><code>NetworkPolicy</code>s In Garden, Seed, Shoot Clusters</h1><p>This document describes which <a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>Kubernetes <code>NetworkPolicy</code>s</a> deployed by Gardener into the various clusters.</p><h2 id=garden-cluster>Garden Cluster</h2><p><em>(via <code>gardener-operator</code> and <code>gardener-resource-manager</code>)</em></p><p>The <code>gardener-operator</code> runs a <a href=/docs/gardener/concepts/operator/#networkpolicy-controller-registrar><code>NetworkPolicy</code> controller</a> which is responsible for the following namespaces:</p><ul><li><code>garden</code></li><li><code>istio-system</code></li><li><code>*istio-ingress-*</code></li><li><code>shoot-*</code></li><li><code>extension-*</code> (in case the garden cluster is a seed cluster at the same time)</li></ul><p>It deploys the following so-called &ldquo;general <code>NetworkPolicy</code>s&rdquo;:</p><table><thead><tr><th>Name</th><th>Purpose</th></tr></thead><tbody><tr><td><code>deny-all</code></td><td><a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-ingress-and-all-egress-traffic>Denies all ingress and egress traffic</a> for all pods in this namespace. Hence, all traffic must be explicitly allowed.</td></tr><tr><td><code>allow-to-dns</code></td><td>Allows egress traffic from pods labeled with <code>networking.gardener.cloud/to-dns=allowed</code> to DNS pods running in the <code>kube-sytem</code> namespace. In practice, most of the pods performing network egress traffic need this label.</td></tr><tr><td><code>allow-to-runtime-apiserver</code></td><td>Allows egress traffic from pods labeled with <code>networking.gardener.cloud/to-runtime-apiserver=allowed</code> to the API server of the runtime cluster.</td></tr><tr><td><code>allow-to-blocked-cidrs</code></td><td>Allows egress traffic from pods labeled with <code>networking.gardener.cloud/to-blocked-cidrs=allowed</code> to explicitly blocked addresses configured by human operators (configured via <code>.spec.networking.blockedCIDRs</code> in the <code>Seed</code>). For instance, this can be used to block the cloud provider&rsquo;s metadata service.</td></tr><tr><td><code>allow-to-public-networks</code></td><td>Allows egress traffic from pods labeled with <code>networking.gardener.cloud/to-public-networks=allowed</code> to all public network IPs, except for private networks (RFC1918), carrier-grade NAT (RFC6598), and explicitly blocked addresses configured by human operators for all pods labeled with <code>networking.gardener.cloud/to-public-networks=allowed</code>. In practice, this blocks egress traffic to all networks in the cluster and only allows egress traffic to public IPv4 addresses.</td></tr><tr><td><code>allow-to-private-networks</code></td><td>Allows egress traffic from pods labeled with <code>networking.gardener.cloud/to-private-networks=allowed</code> to the private networks (RFC1918) and carrier-grade NAT (RFC6598) except for cluster-specific networks (configured via <code>.spec.networks</code> in the <code>Seed</code>).</td></tr><tr><td><code>allow-to-shoot-networks</code></td><td>Allows egress traffic from pods labeled with <code>networking.gardener.cloud/to-shoot-networks=allowed</code> to IPv4 blocks belonging to the shoot networks (configured via <code>.spec.networking</code> in the <code>Shoot</code>). In practice, this should be used by components which use VPN tunnel to communicate to pods in the shoot cluster. Note that this policy only exists in <code>shoot-*</code> namespaces.</td></tr></tbody></table><p>Apart from those, the <code>gardener-operator</code> also enables the <a href=/docs/gardener/concepts/resource-manager/#networkpolicy-controller><code>NetworkPolicy</code> controller of <code>gardener-resource-manager</code></a>.
Please find more information in the linked document.
In summary, most of the pods that initiate connections with other pods will have labels with <code>networking.resources.gardener.cloud/</code> prefixes.
This way, they leverage the automatically created <code>NetworkPolicy</code>s by the controller.
As a result, in most cases no special/custom-crafted <code>NetworkPolicy</code>s must be created anymore.</p><h2 id=seed-cluster>Seed Cluster</h2><p><em>(via <code>gardenlet</code> and <code>gardener-resource-manager</code>)</em></p><p>In seed clusters it works the same way as in the garden cluster managed by <code>gardener-operator</code>.
When a seed cluster is the garden cluster at the same time, <code>gardenlet</code> does not enable the <code>NetworkPolicy</code> controller (since <code>gardener-operator</code> already runs it).
Otherwise, it uses the exact same controller and code like <code>gardener-operator</code>, resulting in the same behaviour in both garden and seed clusters.</p><h3 id=logging--monitoring>Logging & Monitoring</h3><h4 id=seed-system-namespaces>Seed System Namespaces</h4><p>As part of the seed reconciliation flow, the <code>gardenlet</code> deploys various Prometheus instances into the <code>garden</code> namespace.
See also <a href=/docs/gardener/development/monitoring-stack/>this document</a> for more information.
Each pod that should be scraped for metrics by these instances must have a <code>Service</code> which is annotated with</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>annotations:
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/from-all-seed-scrape-targets-allowed-ports: <span style=color:#a31515>&#39;[{&#34;port&#34;:&lt;metrics-port-on-pod&gt;,&#34;protocol&#34;:&#34;&lt;protocol, typically TCP&gt;&#34;}]&#39;</span>
</span></span></code></pre></div><p>If the respective pod is not running in the <code>garden</code> namespace, the <code>Service</code> needs these annotations in addition:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>annotations:
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/namespace-selectors: <span style=color:#a31515>&#39;[{&#34;matchLabels&#34;:{&#34;kubernetes.io/metadata.name&#34;:&#34;garden&#34;}}]&#39;</span>
</span></span></code></pre></div><p>If the respective pod is running in an <code>extension-*</code> namespace, the <code>Service</code> needs this annotation in addition:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>annotations:
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/pod-label-selector-namespace-alias: extensions
</span></span></code></pre></div><p>This automatically allows the needed network traffic from the respective Prometheus pods.</p><h4 id=shoot-namespaces>Shoot Namespaces</h4><p>As part of the shoot reconciliation flow, the <code>gardenlet</code> deploys a shoot-specific Prometheus into the shoot namespace.
Each pod that should be scraped for metrics must have a <code>Service</code> which is annotated with</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>annotations:
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/from-all-scrape-targets-allowed-ports: <span style=color:#a31515>&#39;[{&#34;port&#34;:&lt;metrics-port-on-pod&gt;,&#34;protocol&#34;:&#34;&lt;protocol, typically TCP&gt;&#34;}]&#39;</span>
</span></span></code></pre></div><p>This automatically allows the network traffic from the Prometheus pod.</p><h3 id=webhook-servers>Webhook Servers</h3><p>Components serving webhook handlers that must be reached by <code>kube-apiserver</code>s of the virtual garden cluster or shoot clusters just need to annotate their <code>Service</code> as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>annotations:
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/from-all-webhook-targets-allowed-ports: <span style=color:#a31515>&#39;[{&#34;port&#34;:&lt;server-port-on-pod&gt;,&#34;protocol&#34;:&#34;&lt;protocol, typically TCP&gt;&#34;}]&#39;</span>
</span></span></code></pre></div><p>This automatically allows the network traffic from the API server pods.</p><p>In case the servers run in a different namespace than the <code>kube-apiserver</code>s, the following annotations are needed:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>annotations:
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/from-all-webhook-targets-allowed-ports: <span style=color:#a31515>&#39;[{&#34;port&#34;:&lt;server-port-on-pod&gt;,&#34;protocol&#34;:&#34;&lt;protocol, typically TCP&gt;&#34;}]&#39;</span>
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/pod-label-selector-namespace-alias: extensions
</span></span><span style=display:flex><span>  <span style=color:green># for the virtual garden cluster:</span>
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/namespace-selectors: <span style=color:#a31515>&#39;[{&#34;matchLabels&#34;:{&#34;kubernetes.io/metadata.name&#34;:&#34;garden&#34;}}]&#39;</span>
</span></span><span style=display:flex><span>  <span style=color:green># for shoot clusters:</span>
</span></span><span style=display:flex><span>  networking.resources.gardener.cloud/namespace-selectors: <span style=color:#a31515>&#39;[{&#34;matchLabels&#34;:{&#34;gardener.cloud/role&#34;:&#34;shoot&#34;}}]&#39;</span>
</span></span></code></pre></div><h2 id=additional-namespace-coverage-in-gardenseed-cluster>Additional Namespace Coverage in Garden/Seed Cluster</h2><p>In some cases, garden or seed clusters might run components in dedicated namespaces which are not covered by the controller by default (see list above).
Still, it might(/should) be desired to also include such &ldquo;custom namespaces&rdquo; into the control of the <code>NetworkPolicy</code> controllers.</p><p>In order to do so, human operators can adapt the component configs of <code>gardener-operator</code> or <code>gardenlet</code> by providing label selectors for additional namespaces:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>controllers:
</span></span><span style=display:flex><span>  networkPolicy:
</span></span><span style=display:flex><span>    additionalNamespaceSelectors:
</span></span><span style=display:flex><span>    - matchLabels:
</span></span><span style=display:flex><span>        foo: bar
</span></span></code></pre></div><h3 id=communication-with-kube-apiserver-for-components-in-custom-namespaces>Communication With <code>kube-apiserver</code> For Components In Custom Namespaces</h3><h3 id=egress-traffic>Egress Traffic</h3><p>Component running in such custom namespaces might need to initiate the communication with the <code>kube-apiserver</code>s of the virtual garden cluster or a shoot cluster.
In order to achieve this, their custom namespace must be labeled with <code>networking.gardener.cloud/access-target-apiserver=allowed</code>.
This will make the <code>NetworkPolicy</code> controllers automatically provisioning the required policies into their namespace.</p><p>As a result, the respective component pods just need to be labeled with</p><ul><li><code>networking.resources.gardener.cloud/to-garden-virtual-garden-kube-apiserver-tcp-443=allowed</code> (virtual garden cluster)</li><li><code>networking.resources.gardener.cloud/to-all-shoots-kube-apiserver-tcp-443=allowed</code> (shoot clusters)</li></ul><h3 id=ingress-traffic>Ingress Traffic</h3><p>Components running in such custom namespaces might serve webhook handlers that must be reached by the <code>kube-apiserver</code>s of the virtual garden cluster or a shoot cluster.
In order to achieve this, their <code>Service</code> must be annotated.
Please refer to <a href=#webhook-servers>this section</a> for more information.</p><h2 id=shoot-cluster>Shoot Cluster</h2><p><em>(via <code>gardenlet</code>)</em></p><p>For shoot clusters, the concepts mentioned above don&rsquo;t apply and are not enabled.
Instead, <code>gardenlet</code> only deploys a few &ldquo;custom&rdquo; <code>NetworkPolicy</code>s for the shoot system components running in the <code>kube-system</code> namespace.
All other namespaces in the shoot cluster do not contain network policies deployed by <code>gardenlet</code>.</p><p>As a best practice, every pod deployed into the <code>kube-system</code> namespace should use appropriate <code>NetworkPolicy</code> in order to only allow <strong>required</strong> network traffic.
Therefore, pods should have labels matching to the selectors of the available network policies.</p><p><code>gardenlet</code> deploys the following <code>NetworkPolicy</code>s:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME                                       POD-SELECTOR
</span></span><span style=display:flex><span>gardener.cloud--allow-dns                  k8s-app in (kube-dns)
</span></span><span style=display:flex><span>gardener.cloud--allow-from-seed            networking.gardener.cloud/from-seed=allowed
</span></span><span style=display:flex><span>gardener.cloud--allow-to-dns               networking.gardener.cloud/to-dns=allowed
</span></span><span style=display:flex><span>gardener.cloud--allow-to-apiserver         networking.gardener.cloud/to-apiserver=allowed
</span></span><span style=display:flex><span>gardener.cloud--allow-to-from-nginx        app=nginx-ingress
</span></span><span style=display:flex><span>gardener.cloud--allow-to-kubelet           networking.gardener.cloud/to-kubelet=allowed
</span></span><span style=display:flex><span>gardener.cloud--allow-to-public-networks   networking.gardener.cloud/to-public-networks=allowed
</span></span><span style=display:flex><span>gardener.cloud--allow-vpn                  app=vpn-shoot
</span></span></code></pre></div><p>Note that a <code>deny-all</code> policy will not be created by <code>gardenlet</code>.
Shoot owners can create it manually if needed/desired.
Above listed <code>NetworkPolicy</code>s ensure that the traffic for the shoot system components is allowed in case such <code>deny-all</code> policies is created.</p><h2 id=implications-for-gardener-extensions>Implications for Gardener Extensions</h2><p>Gardener extensions sometimes need to deploy additional components into the shoot namespace in the seed cluster hosting the control plane.
For example, the <a href=https://github.com/gardener/gardener-extension-provider-aws><code>gardener-extension-provider-aws</code></a> deploys the <code>cloud-controller-manager</code> into the shoot namespace.
In most cases, such pods require network policy labels to allow the traffic they are initiating.</p><p>For components deployed in the <code>kube-system</code> namespace of the shoots (e.g., CNI plugins or CSI drivers, etc.), custom <code>NetworkPolicy</code>s might be required to ensure the respective components can still communicate in case the user creates a <code>deny-all</code> policy.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0b0b191c41815b5790bc7d9e61ba2f0a>6 - Seed Bootstrapping</h1><h1 id=seed-bootstrapping>Seed Bootstrapping</h1><p>Whenever the gardenlet is responsible for a new <code>Seed</code> resource its &ldquo;seed controller&rdquo; is being activated.
One part of this controller&rsquo;s reconciliation logic is deploying certain components into the <code>garden</code> namespace of the seed cluster itself.
These components are required to spawn and manage control planes for shoot clusters later on.
This document is providing an overview which actions are performed during this bootstrapping phase, and it explains the rationale behind them.</p><h2 id=dependency-watchdog>Dependency Watchdog</h2><p>The dependency watchdog (abbreviation: DWD) is a component developed separately in the <a href=https://github.com/gardener/dependency-watchdog>gardener/dependency-watchdog</a> GitHub repository.
Gardener is using it for two purposes:</p><ol><li>Prevention of melt-down situations when the load balancer used to expose the kube-apiserver of shoot clusters goes down while the kube-apiserver itself is still up and running.</li><li>Fast recovery times for crash-looping pods when depending pods are again available.</li></ol><p>For the sake of separating these concerns, two instances of the DWD are deployed by the seed controller.</p><h3 id=prober>Prober</h3><p>The <code>dependency-watchdog-prober</code> deployment is responsible for above-mentioned first point.</p><p>The <code>kube-apiserver</code> of shoot clusters is exposed via a load balancer, usually with an attached public IP, which serves as the main entry point when it comes to interaction with the shoot cluster (e.g., via <code>kubectl</code>).
While end-users are talking to their clusters via this load balancer, other control plane components like the <code>kube-controller-manager</code> or <code>kube-scheduler</code> run in the same namespace/same cluster, so they can communicate via the in-cluster <code>Service</code> directly instead of using the detour with the load balancer.
However, the worker nodes of shoot clusters run in isolated, distinct networks.
This means that the <code>kubelet</code>s and <code>kube-proxy</code>s also have to talk to the control plane via the load balancer.</p><p>The <code>kube-controller-manager</code> has a special control loop called <a href=https://github.com/kubernetes/kubernetes/tree/master/pkg/controller/nodelifecycle><code>nodelifecycle</code></a> which will set the status of <code>Node</code>s to <code>NotReady</code> in case the kubelet stops to regularly renew its lease/to send its heartbeat.
This will trigger other self-healing capabilities of Kubernetes, for example, the eviction of pods from such &ldquo;unready&rdquo; nodes to healthy nodes.
Similarly, the <code>cloud-controller-manager</code> has a control loop that will disconnect load balancers from &ldquo;unready&rdquo; nodes, i.e., such workload would no longer be accessible until moved to a healthy node.
Furthermore, the <code>machine-controller-manager</code> removes &ldquo;unready&rdquo; nodes after <code>health-timeout</code> (default 10min).</p><p>While these are awesome Kubernetes features on their own, they have a dangerous drawback when applied in the context of Gardener&rsquo;s architecture:
When the <code>kube-apiserver</code> load balancer fails for whatever reason, then the <code>kubelet</code>s can&rsquo;t talk to the <code>kube-apiserver</code> to renew their lease anymore.
After a minute or so the <code>kube-controller-manager</code> will get the impression that all nodes have died and will mark them as <code>NotReady</code>.
This will trigger above mentioned eviction as well as detachment of load balancers.
As a result, the customer&rsquo;s workload will go down and become unreachable.</p><p>This is exactly the situation that the DWD prevents:
It regularly tries to talk to the <code>kube-apiserver</code>s of the shoot clusters, once by using their load balancer, and once by talking via the in-cluster <code>Service</code>.
If it detects that the <code>kube-apiserver</code> is reachable internally but not externally, it scales down <code>machine-controller-manager</code>, <code>cluster-autoscaler</code> (if enabled) and <code>kube-controller-manager</code> to <code>0</code>.
This will prevent it from marking the shoot worker nodes as &ldquo;unready&rdquo;. This will also prevent the <code>machine-controller-manager</code> from deleting potentially healthy nodes.
As soon as the <code>kube-apiserver</code> is reachable externally again, <code>kube-controller-manager</code>, <code>machine-controller-manager</code> and <code>cluster-autoscaler</code> are restored to the state prior to scale-down.</p><h3 id=weeder>Weeder</h3><p>The <code>dependency-watchdog-weeder</code> deployment is responsible for above mentioned second point.</p><p>Kubernetes is restarting failing pods with an exponentially increasing backoff time.
While this is a great strategy to prevent system overloads, it has the disadvantage that the delay between restarts is increasing up to multiple minutes very fast.</p><p>In the Gardener context, we are deploying many components that are depending on other components.
For example, the <code>kube-apiserver</code> is depending on a running <code>etcd</code>, or the <code>kube-controller-manager</code> and <code>kube-scheduler</code> are depending on a running <code>kube-apiserver</code>.
In case such a &ldquo;higher-level&rdquo; component fails for whatever reason, the dependent pods will fail and end-up in crash-loops.
As Kubernetes does not know anything about these hierarchies, it won&rsquo;t recognize that such pods can be restarted faster as soon as their dependents are up and running again.</p><p>This is exactly the situation in which the DWD will become active:
If it detects that a certain <code>Service</code> is available again (e.g., after the <code>etcd</code> was temporarily down while being moved to another seed node), then DWD will restart all crash-looping dependant pods.
These dependant pods are detected via a pre-configured label selector.</p><p>As of today, the DWD is configured to restart a crash-looping <code>kube-apiserver</code> after <code>etcd</code> became available again, or any pod depending on the <code>kube-apiserver</code> that has a <code>gardener.cloud/role=controlplane</code> label (e.g., <code>kube-controller-manager</code>, <code>kube-scheduler</code>).</p></div><div class=td-content style=page-break-before:always><h1 id=pg-8cb8151808ff96f4fb35a6a2a1b971ed>7 - Seed Settings</h1><h1 id=settings-for-seeds>Settings for <code>Seed</code>s</h1><p>The <code>Seed</code> resource offers a few settings that are used to control the behaviour of certain Gardener components.
This document provides an overview over the available settings:</p><h2 id=dependency-watchdog>Dependency Watchdog</h2><p>Gardenlet can deploy two instances of the <a href=https://github.com/gardener/dependency-watchdog>dependency-watchdog</a> into the <code>garden</code> namespace of the seed cluster.
One instance only activates the weeder while the second instance only activates the prober.</p><h3 id=weeder>Weeder</h3><p>The weeder helps to alleviate the delay where control plane components remain unavailable by finding the respective pods in <code>CrashLoopBackoff</code> status and restarting them once their dependants become ready and available again.
For example, if <code>etcd</code> goes down then also <code>kube-apiserver</code> goes down (and into a <code>CrashLoopBackoff</code> state). If <code>etcd</code> comes up again then (without the <code>endpoint</code> controller) it might take some time until <code>kube-apiserver</code> gets restarted as well.</p><p>⚠️ <code>.spec.settings.dependencyWatchdog.endpoint.enabled</code> is deprecated and will be removed in a future version of Gardener. Use <code>.spec.settings.dependencyWatchdog.weeder.enabled</code> instead.</p><p>It can be enabled/disabled via the <code>.spec.settings.dependencyWatchdog.endpoint.enabled</code> field.
It defaults to <code>true</code>.</p><h3 id=prober>Prober</h3><p>The <code>probe</code> controller scales down the <code>kube-controller-manager</code> of shoot clusters in case their respective <code>kube-apiserver</code> is not reachable via its external ingress.
This is in order to avoid melt-down situations, since the <code>kube-controller-manager</code> uses in-cluster communication when talking to the <code>kube-apiserver</code>, i.e., it wouldn&rsquo;t be affected if the external access to the <code>kube-apiserver</code> is interrupted for whatever reason.
The <code>kubelet</code>s on the shoot worker nodes, however, would indeed be affected since they typically run in different networks and use the external ingress when talking to the <code>kube-apiserver</code>.
Hence, without scaling down <code>kube-controller-manager</code>, the nodes might be marked as <code>NotReady</code> and eventually replaced (since the <code>kubelet</code>s cannot report their status anymore).
To prevent such unnecessary turbulences, <code>kube-controller-manager</code> is being scaled down until the external ingress becomes available again. In addition, as a precautionary measure, <code>machine-controller-manager</code> is also scaled down, along with <code>cluster-autoscaler</code> which depends on <code>machine-controller-manager</code>.</p><p>⚠️ <code>.spec.settings.dependencyWatchdog.probe.enabled</code> is deprecated and will be removed in a future version of Gardener. Use <code>.spec.settings.dependencyWatchdog.prober.enabled</code> instead.</p><p>It can be enabled/disabled via the <code>.spec.settings.dependencyWatchdog.probe.enabled</code> field.
It defaults to <code>true</code>.</p><h2 id=reserve-excess-capacity>Reserve Excess Capacity</h2><p>If the excess capacity reservation is enabled, then the gardenlet will deploy a special <code>Deployment</code> into the <code>garden</code> namespace of the seed cluster.
This <code>Deployment</code>&rsquo;s pod template has only one container, the <code>pause</code> container, which simply runs in an infinite loop.
The priority of the deployment is very low, so any other pod will preempt these <code>pause</code> pods.
This is especially useful if new shoot control planes are created in the seed.
In case the seed cluster runs at its capacity, then there is no waiting time required during the scale-up.
Instead, the low-priority <code>pause</code> pods will be preempted and allow newly created shoot control plane pods to be scheduled fast.
In the meantime, the cluster-autoscaler will trigger the scale-up because the preempted <code>pause</code> pods want to run again.
However, this delay doesn&rsquo;t affect the important shoot control plane pods, which will improve the user experience.</p><p>Use <code>.spec.settings.excessCapacityReservation.configs</code> to create excess capacity reservation deployments which allow to specify custom values for <code>resources</code>, <code>nodeSelector</code> and <code>tolerations</code>. Each config creates a deployment with a minium number of 2 replicas and a maximum equal to the number of zones configured for this seed.<br>It defaults to a config reserving 2 CPUs and 6Gi of memory for each pod with no <code>nodeSelector</code> and no <code>tolerations</code>.</p><p>Excess capacity reservation is enabled when <code>.spec.settings.excessCapacityReservation.enabled</code> is <code>true</code> or not specified while <code>configs</code> are present. It can be disabled by setting the field to <code>false</code>.</p><h2 id=scheduling>Scheduling</h2><p>By default, the Gardener Scheduler will consider all seed clusters when a new shoot cluster shall be created.
However, administrators/operators might want to exclude some of them from being considered by the scheduler.
Therefore, seed clusters can be marked as &ldquo;invisible&rdquo;.
In this case, the scheduler simply ignores them as if they wouldn&rsquo;t exist.
Shoots can still use the invisible seed but only by explicitly specifying the name in their <code>.spec.seedName</code> field.</p><p>Seed clusters can be marked visible/invisible via the <code>.spec.settings.scheduling.visible</code> field.
It defaults to <code>true</code>.</p><p>ℹ️ In previous Gardener versions (&lt; 1.5) these settings were controlled via taint keys (<code>seed.gardener.cloud/{disable-capacity-reservation,invisible}</code>).
The taint keys are no longer supported and removed in version 1.12.
The rationale behind it is the implementation of tolerations similar to Kubernetes tolerations.
More information about it can be found in <a href=https://github.com/gardener/gardener/issues/2193>#2193</a>.</p><h2 id=load-balancer-services>Load Balancer Services</h2><p>Gardener creates certain Kubernetes <code>Service</code> objects of type <code>LoadBalancer</code> in the seed cluster.
Most prominently, they are used for exposing the shoot control planes, namely the kube-apiserver of the shoot clusters.
In most cases, the cloud-controller-manager (responsible for managing these load balancers on the respective underlying infrastructure) supports certain customization and settings via annotations.
<a href=https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer>This document</a> provides a good overview and many examples.</p><p>By setting the <code>.spec.settings.loadBalancerServices.annotations</code> field the Gardener administrator can specify a list of annotations, which will be injected into the <code>Service</code>s of type <code>LoadBalancer</code>.</p><h3 id=external-traffic-policy>External Traffic Policy</h3><p>Setting the <a href=https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip>external traffic policy</a> to <code>Local</code> can be beneficial as it
preserves the source IP address of client requests. In addition to that, it removes one hop in the data path and hence reduces request latency. On some cloud infrastructures, it can furthermore be
used in conjunction with <code>Service</code> annotations as described above to prevent cross-zonal traffic from the load balancer to the backend pod.</p><p>The default external traffic policy is <code>Cluster</code>, meaning that all traffic from the load balancer will be sent to any cluster node, which then itself will redirect the traffic to the actual receiving pod.
This approach adds a node to the data path, may cross the zone boundaries twice, and replaces the source IP with one of the cluster nodes.</p><p><img src=/__resources/external-traffic-policy-cluster_d7d2ed.png alt="External Traffic Policy Cluster"></p><p>Using external traffic policy <code>Local</code> drops the additional node, i.e., only cluster nodes with corresponding backend pods will be in the list of backends of the load balancer. However, this has multiple implications.
The health check port in this scenario is exposed by <code>kube-proxy</code> , i.e., if <code>kube-proxy</code> is not working on a node a corresponding pod on the node will not receive traffic from
the load balancer as the load balancer will see a failing health check. (This is quite different from ordinary service routing where <code>kube-proxy</code> is only responsible for setup, but does not need to
run for its operation.) Furthermore, load balancing may become imbalanced if multiple pods run on the same node because load balancers will split the load equally among the nodes and not among the pods. This is mitigated by corresponding node anti affinities.</p><p><img src=/__resources/external-traffic-policy-local_09608f.png alt="External Traffic Policy Local"></p><p>Operators need to take these implications into account when considering switching external traffic policy to <code>Local</code>.</p><h3 id=zone-specific-settings>Zone-Specific Settings</h3><p>In case a seed cluster is configured to use multiple zones via <code>.spec.provider.zones</code>, it may be necessary to configure the load balancers in individual zones in different way, e.g., by utilizing
different annotations. One reason may be to reduce cross-zonal traffic and have zone-specific load balancers in place. Zone-specific load balancers may then be bound to zone-specific subnets or
availability zones in the cloud infrastructure.</p><p>Besides the load balancer annotations, it is also possible to set the <a href=#external-traffic-policy>external traffic policy</a> for each zone-specific load balancer individually.</p><h2 id=vertical-pod-autoscaler>Vertical Pod Autoscaler</h2><p>Gardener heavily relies on the Kubernetes <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler><code>vertical-pod-autoscaler</code> component</a>.
By default, the seed controller deploys the VPA components into the <code>garden</code> namespace of the respective seed clusters.
In case you want to manage the VPA deployment on your own or have a custom one, then you might want to disable the automatic deployment of Gardener.
Otherwise, you might end up with two VPAs, which will cause erratic behaviour.
By setting the <code>.spec.settings.verticalPodAutoscaler.enabled=false</code>, you can disable the automatic deployment.</p><p>⚠️ In any case, there must be a VPA available for your seed cluster. Using a seed without VPA is not supported.</p><h2 id=topology-aware-traffic-routing>Topology-Aware Traffic Routing</h2><p>Refer to the <a href=/docs/gardener/operations/topology_aware_routing/>Topology-Aware Traffic Routing documentation</a> as this document contains the documentation for the topology-aware routing Seed setting.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0764aaff8e1c6b8735fed3db1b990493>8 - Topology Aware Routing</h1><h1 id=topology-aware-traffic-routing>Topology-Aware Traffic Routing</h1><h2 id=motivation>Motivation</h2><p>The enablement of <a href=/docs/gardener/usage/shoot_high_availability/>highly available shoot control-planes</a> requires multi-zone seed clusters. A garden runtime cluster can also be a multi-zone cluster. The topology-aware routing is introduced to reduce costs and to improve network performance by avoiding the cross availability zone traffic, if possible. The cross availability zone traffic is charged by the cloud providers and it comes with higher latency compared to the traffic within the same zone. The topology-aware routing feature enables topology-aware routing for <code>Service</code>s deployed in a seed or garden runtime cluster. For the clients consuming these topology-aware services, <code>kube-proxy</code> favors the endpoints which are located in the same zone where the traffic originated from. In this way, the cross availability zone traffic is avoided.</p><h2 id=how-it-works>How it works</h2><p>The topology-aware routing feature relies on the Kubernetes feature <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/><code>TopologyAwareHints</code></a>.</p><h5 id=endpointslice-hints-mutating-webhook>EndpointSlice Hints Mutating Webhook</h5><p>The component that is responsible for providing hints in the EndpointSlices resources is the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/>kube-controller-manager</a>, in particular this is the <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/>EndpointSlice controller</a>. However, there are several drawbacks with the TopologyAwareHints feature that don&rsquo;t allow us to use it in its native way:</p><ul><li><p>The algorithm in the EndpointSlice controller is based on a CPU-balance heuristic. From the TopologyAwareHints documentation:</p><blockquote><p>The controller allocates a proportional amount of endpoints to each zone. This proportion is based on the allocatable CPU cores for nodes running in that zone. For example, if one zone had 2 CPU cores and another zone only had 1 CPU core, the controller would allocate twice as many endpoints to the zone with 2 CPU cores.</p></blockquote><p>In case it is not possible to achieve a balanced distribution of the endpoints, as a safeguard mechanism the controller removes hints from the EndpointSlice resource.
In our setup, the clients and the servers are well-known and usually the traffic a component receives does not depend on the zone&rsquo;s allocatable CPU.
Many components deployed by Gardener are scaled automatically by VPA. In case of an overload of a replica, the VPA should provide and apply enhanced CPU and memory resources. Additionally, Gardener uses the cluster-autoscaler to upscale/downscale Nodes dynamically. Hence, it is not possible to ensure a balanced allocatable CPU across the zones.</p></li><li><p>The TopologyAwareHints feature does not work at low-endpoint counts. It falls apart for a Service with less than 10 Endpoints.</p></li><li><p>Hints provided by the EndpointSlice controller are not deterministic. With cluster-autoscaler running and load increasing, hints can be removed in the next moment. There is no option to enforce the zone-level topology.</p></li></ul><p>For more details, see the following issue <a href=https://github.com/kubernetes/kubernetes/issues/113731>kubernetes/kubernetes#113731</a>.</p><p>To circumvent these issues with the EndpointSlice controller, a mutating webhook in the gardener-resource-manager assigns hints to EndpointSlice resources. For each endpoint in the EndpointSlice, it sets the endpoint&rsquo;s hints to the endpoint&rsquo;s zone. The webhook overwrites the hints provided by the EndpointSlice controller in kube-controller-manager. For more details, see the <a href=/docs/gardener/concepts/resource-manager/#endpointslice-hints>webhook&rsquo;s documentation</a>.</p><h5 id=kube-proxy>kube-proxy</h5><p>By default, with kube-proxy running in <code>iptables</code> mode, traffic is distributed randomly across all endpoints, regardless of where it originates from. In a cluster with 3 zones, traffic is more likely to go to another zone than to stay in the current zone.
With the topology-aware routing feature, kube-proxy filters the endpoints it routes to based on the hints in the EndpointSlice resource. In most of the cases, kube-proxy will prefer the endpoint(s) in the same zone. For more details, see the <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/#implementation-kube-proxy>Kubernetes documentation</a>.</p><h2 id=how-to-make-a-service-topology-aware>How to make a Service topology-aware?</h2><p>To make a Service topology-aware, the following annotation and label have to be added to the Service:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    service.kubernetes.io/topology-aware-hints: <span style=color:#a31515>&#34;auto&#34;</span>
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    endpoint-slice-hints.resources.gardener.cloud/consider: <span style=color:#a31515>&#34;true&#34;</span>
</span></span></code></pre></div><blockquote><p>Note: In Kubernetes 1.27 the <code>service.kubernetes.io/topology-aware-hints=auto</code> annotation is deprecated in favor of the newly introduced <code>service.kubernetes.io/topology-mode=auto</code>. When the runtime cluster&rsquo;s K8s version is >= 1.27, use the <code>service.kubernetes.io/topology-mode=auto</code> annotation. For more details, see the <a href=https://github.com/kubernetes/kubernetes/pull/116522>corresponding upstream PR</a>.</p></blockquote><p>The <code>service.kubernetes.io/topology-aware-hints=auto</code> annotation is needed for kube-proxy. One of the prerequisites on kube-proxy side for using topology-aware routing is the corresponding Service to be annotated with the <code>service.kubernetes.io/topology-aware-hints=auto</code>. For more details, see the following <a href=https://github.com/kubernetes/kubernetes/blob/b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d/pkg/proxy/topology.go#L140-L186>kube-proxy function</a>.
The <code>endpoint-slice-hints.resources.gardener.cloud/consider=true</code> label is needed for gardener-resource-manager to prevent the EndpointSlice hints mutating webhook from selecting all EndpointSlice resources but only the ones that are labeled with the consider label.</p><p>The Gardener extensions can use this approach to make a Service they deploy topology-aware.</p><p>Prerequisites for making a Service topology-aware:</p><ol><li>The Pods backing the Service should be spread on most of the available zones. This constraint should be ensured with appropriate scheduling constraints (topology spread constraints, (anti-)affinity). Enabling the feature for a Service with a single backing Pod or Pods all located in the same zone does not lead to a benefit.</li><li>The component should be scaled up by <code>VerticalPodAutoscaler</code>. In case of an overload (a large portion of the of the traffic is originating from a given zone), the <code>VerticalPodAutoscaler</code> should provide better resource recommendations for the overloaded backing Pods.</li><li>Consider the <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/#constraints><code>TopologyAwareHints</code> constraints</a>.</li></ol><blockquote><p>Note: The topology-aware routing feature is considered as alpha feature. Use it only for evaluation purposes.</p></blockquote><h2 id=topology-aware-services-in-the-seed-cluster>Topology-aware Services in the Seed cluster</h2><h5 id=etcd-main-client-and-etcd-events-client>etcd-main-client and etcd-events-client</h5><p>The <code>etcd-main-client</code> and <code>etcd-events-client</code> Services are topology-aware. They are consumed by the kube-apiserver.</p><h5 id=kube-apiserver>kube-apiserver</h5><p>The <code>kube-apiserver</code> Service is topology-aware. It is consumed by the controllers running in the Shoot control plane.</p><blockquote><p>Note: The <code>istio-ingressgateway</code> component routes traffic in topology-aware manner - if possible, it routes traffic to the target <code>kube-apiserver</code> Pods in the same zone. If there is no healthy <code>kube-apiserver</code> Pod available in the same zone, the traffic is routed to any of the healthy Pods in the other zones. This behaviour is unconditionally enabled.</p></blockquote><h5 id=gardener-resource-manager>gardener-resource-manager</h5><p>The <code>gardener-resource-manager</code> Service that is part of the Shoot control plane is topology-aware. The resource-manager serves webhooks and the Service is consumed by the kube-apiserver for the webhook communication.</p><h5 id=vpa-webhook>vpa-webhook</h5><p>The <code>vpa-webhook</code> Service that is part of the Shoot control plane is topology-aware. It is consumed by the kube-apiserver for the webhook communication.</p><h2 id=topology-aware-services-in-the-garden-runtime-cluster>Topology-aware Services in the garden runtime cluster</h2><h5 id=virtual-garden-etcd-main-client-and-virtual-garden-etcd-events-client>virtual-garden-etcd-main-client and virtual-garden-etcd-events-client</h5><p>The <code>virtual-garden-etcd-main-client</code> and <code>virtual-garden-etcd-events-client</code> Services are topology-aware. <code>virtual-garden-etcd-main-client</code> is consumed by <code>virtual-garden-kube-apiserver</code> and <code>gardener-apiserver</code>, <code>virtual-garden-etcd-events-client</code> is consumed by <code>virtual-garden-kube-apiserver</code>.</p><h5 id=virtual-garden-kube-apiserver>virtual-garden-kube-apiserver</h5><p>The <code>virtual-garden-kube-apiserver</code> Service is topology-aware. It is consumed by <code>virtual-garden-kube-controller-manager</code>, <code>gardener-controller-manager</code>, <code>gardener-scheduler</code>, <code>gardener-admission-controller</code>, extension admission components, <code>gardener-dashboard</code> and other components.</p><blockquote><p>Note: Unlike the other Services, the <code>virtual-garden-kube-apiserver</code> Service is of type LoadBalancer. In-cluster components consuming the <code>virtual-garden-kube-apiserver</code> Service by its Service name will have benefit from the topology-aware routing. However, the TopologyAwareHints feature cannot help with external traffic routed to load balancer&rsquo;s address - such traffic won&rsquo;t be routed in a topology-aware manner and will be routed according to the cloud-provider specific implementation.</p></blockquote><h5 id=gardener-apiserver>gardener-apiserver</h5><p>The <code>gardener-apiserver</code> Service is topology-aware. It is consumed by <code>virtual-garden-kube-apiserver</code>. The aggregation layer in <code>virtual-garden-kube-apiserver</code> proxies requests sent for the Gardener API types to the <code>gardener-apiserver</code>.</p><h5 id=gardener-admission-controller>gardener-admission-controller</h5><p>The <code>gardener-admission-controller</code> Service is topology-aware. It is consumed by <code>virtual-garden-kube-apiserver</code> and <code>gardener-apiserver</code> for the webhook communication.</p><h2 id=how-to-enable-the-topology-aware-routing-for-a-seed-cluster>How to enable the topology-aware routing for a Seed cluster?</h2><p>For a Seed cluster the topology-aware routing functionality can be enabled in the Seed specification:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span><span style=color:green># ...</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  settings:
</span></span><span style=display:flex><span>    topologyAwareRouting:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The topology-aware routing setting can be only enabled for a Seed cluster with more than one zone.
gardenlet enables topology-aware Services only for Shoot control planes with failure tolerance type <code>zone</code> (<code>.spec.controlPlane.highAvailability.failureTolerance.type=zone</code>). Control plane Pods of non-HA Shoots and HA Shoots with failure tolerance type <code>node</code> are pinned to single zone. For more details, see <a href=/docs/gardener/development/high-availability/>High Availability Of Deployed Components</a>.</p><p>⚠️ For K8s &lt; 1.24 Seed clusters, the topology-aware routing setting requires the Kubernetes <code>TopologyAwareHints</code> feature gate to be enabled for kube-apiserver, kube-controller-manager and kube-proxy. This is required because the <code>TopologyAwareHints</code> feature gate is disabled by default in K8s &lt; 1.24. When <code>TopologyAwareHints</code> is disabled, the kube-apiserver does not allow anything to be persisted in the <code>.endpoints[].hints</code> field in the EndpointSlice resource. Also, the kube-controller-manager removes the hints, hence kube-proxy is not using topology-aware routing.</p><h2 id=how-to-enable-the-topology-aware-routing-for-a-garden-runtime-cluster>How to enable the topology-aware routing for a garden runtime cluster?</h2><p>For a garden runtime cluster the topology-aware routing functionality can be enabled in the Garden resource specification:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: operator.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Garden
</span></span><span style=display:flex><span><span style=color:green># ...</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  runtimeCluster:
</span></span><span style=display:flex><span>    settings:
</span></span><span style=display:flex><span>      topologyAwareRouting:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The topology-aware routing setting can be only enabled for a garden runtime cluster with more than one zone.</p><p>⚠️ For K8s &lt; 1.24 garden runtime clusters, the topology-aware routing setting requires the Kubernetes <code>TopologyAwareHints</code> feature gate to be enabled for kube-apiserver, kube-controller-manager and kube-proxy. For more details, see the above section.</p></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2023 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.7b24c0fb082ffb2de6cb14d6c95e9f8053053709ffcf8c761ef8e9ad2f8021e4.js integrity="sha256-eyTA+wgv+y3myxTWyV6fgFMFNwn/z4x2HvjprS+AIeQ=" crossorigin=anonymous></script></body></html>