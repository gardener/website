<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener – 2024</title><link>https://gardener.cloud/blog/2024/</link><description>Recent content in 2024 on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 22 Apr 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://gardener.cloud/blog/2024/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Gardener's Registry Cache Extension: Another Cost Saving Win and More</title><link>https://gardener.cloud/blog/2024/04-22-gardeners-registry-cache-extension-another-cost-saving-win-and-more/</link><pubDate>Mon, 22 Apr 2024 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2024/04-22-gardeners-registry-cache-extension-another-cost-saving-win-and-more/</guid><description>
&lt;h2 id="use-cases">Use Cases&lt;/h2>
&lt;p>In Kubernetes, on every Node the container runtime daemon pulls the container images that are configured in the Pods&amp;rsquo; specifications running on the corresponding Node. Although these container images are cached on the Node&amp;rsquo;s file system after the initial pull operation, there are imperfections with this setup.&lt;/p>
&lt;p>New Nodes are often created due to events such as auto-scaling (scale up), rolling updates, or replacements of unhealthy Nodes. A new Node would need to pull the images running on it from the container registry because the Node&amp;rsquo;s cache is initially empty. Pulling an image from a registry incurs network traffic and registry costs.&lt;/p>
&lt;p>To reduce network traffic and registry costs for your Shoot cluster, it is recommended to enable the Gardener&amp;rsquo;s Registry Cache extension to run a registry as pull-through cache in the Shoot cluster.&lt;/p>
&lt;p>The use cases of using a pull-through cache are not only limited to cost savings. Using a pull-through cache makes the Kubernetes cluster resilient to failures with the upstream registry - outages, failures due to rate limiting.&lt;/p>
&lt;h2 id="solution">Solution&lt;/h2>
&lt;p>Gardener&amp;rsquo;s Registry Cache extension deploys and manages a pull-through cache registry in the Shoot cluster.&lt;/p>
&lt;p>A pull-through cache registry is a registry that caches container images in its storage The first time when an image is requested from the pull-through cache, it pulls the image from the upstream registry, returns it to the client and stores it in its local storage. On subsequent requests for the same image, the pull-through cache serves the image from its storage. In this way network traffic to the upstream registry is avoided.&lt;/p>
&lt;p>Imagine that you have a DaemonSet in your Kubernetes cluster. In a cluster without a pull-through cache, every Node must pull the same container image from the upstream registry. In a cluster with a pull-through cache, the image is pulled once from the upstream registry and served later for all Nodes.&lt;/p>
&lt;p>&lt;img src="https://gardener.cloud/__resources/shoot-cluster-with-registry-cache_36d211.png" alt="A Shoot cluster setup with a registry cache for Docker Hub (docker.io)" title="A Shoot cluster setup with a registry cache for Docker Hub (docker.io)">&lt;/p>
&lt;p style="text-align: center; font-style: italic;">A Shoot cluster setup with a registry cache for Docker Hub (docker.io).&lt;/p>
&lt;h2 id="cost-considerations">Cost Considerations&lt;/h2>
&lt;p>An image pull represents ingress traffic for a virtual machine (data is entering to the system from outside) and egress traffic for the upstream registry (data is leaving the system).&lt;/p>
&lt;p>Ingress traffic from the internet to a virtual machine is free of charge on AWS, GCP and Azure. However, the cloud providers charge NAT gateway costs for inbound and outbound data processed by the NAT gateway based on the processed data volume (per GB). The container registry offering on the cloud providers charge for egress traffic - again, based on the data volume (per GB).&lt;/p>
&lt;p>Having all of this in mind, the Registry Cache extension reduces NAT gateway costs for the Shoot cluster and container registry costs.&lt;/p>
&lt;h2 id="try-it-out">Try It Out!&lt;/h2>
&lt;p>We would also like to encourage you to try it! As a Gardener user you can also reduce your infrastructure costs and increase resilience by enabling the Registry Cache for your Shoot clusters. The Registry Cache extension is a great fit for long running Shoot clusters that have high image pull rate.&lt;/p>
&lt;p>For more information, refer to the Registry Cache extension documentation!&lt;/p></description></item><item><title>Blog: SpinKube on Gardener - Serverless WASM on Kubernetes</title><link>https://gardener.cloud/blog/2024/04-18-spinkube-gardener-shoot-cluster/</link><pubDate>Thu, 18 Apr 2024 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2024/04-18-spinkube-gardener-shoot-cluster/</guid><description>
&lt;p>With the rising popularity of &lt;a href="https://webassembly.org/">WebAssembly (WASM)&lt;/a> and &lt;a href="https://wasi.dev/">WebAssembly System Interface (WASI)&lt;/a> comes a variety of integration possibilities. WASM is now not only suitable for the browser, but can be also utilized for running workloads on the server. In this post we will explore how you can get started writing serverless applications powered by &lt;a href="https://www.spinkube.dev/">SpinKube&lt;/a> on a Gardener Shoot cluster. This post is inspired by a similar tutorial that goes through the steps of &lt;a href="https://www.spinkube.dev/docs/spin-operator/tutorials/deploy-on-azure-kubernetes-service/">Deploying the Spin Operator on Azure Kubernetes Service&lt;/a>. Keep in mind that this post does not aim to define a production environment. It is meant to show that Gardener Shoot clusters are able to run WebAssembly workloads, giving users the chance to experiment and explore this cutting-edge technology.&lt;/p>
&lt;h2 id="prerequisites">Prerequisites&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubectl/">kubectl&lt;/a> - the Kubernetes command line tool&lt;/li>
&lt;li>&lt;a href="https://helm.sh/">helm&lt;/a> - the package manager for Kubernetes&lt;/li>
&lt;li>A running Gardener Shoot cluster&lt;/li>
&lt;/ul>
&lt;h2 id="gardener-shoot-cluster">Gardener Shoot Cluster&lt;/h2>
&lt;p>For this showcase I am using a Gardener Shoot cluster on AWS infrastructure with nodes powered by &lt;a href="https://github.com/gardenlinux/gardenlinux">Garden Linux&lt;/a>, although the steps should be applicable for other infrastructures as well, since Gardener aims to provide a homogenous Kubernetes experience.&lt;/p>
&lt;p>As a prerequisite for next steps, verify that you have access to your Gardener Shoot cluster.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Verify the access to the Gardener Shoot cluster&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl get ns
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME STATUS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default Active 4m1s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-node-lease Active 4m1s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-public Active 4m1s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kube-system Active 4m1s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you are having troubles accessing the Gardener Shoot cluster, please consult the &lt;a href="https://gardener.cloud/docs/gardener/shoot_access/">Accessing Shoot Clusters&lt;/a> documentation page.&lt;/p>
&lt;h2 id="deploy-the-spin-operator">Deploy the Spin Operator&lt;/h2>
&lt;p>As a first step, we will install the Spin Operator Custom Resource Definitions and the Runtime Class needed by &lt;code>wasmtime-spin-v2&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Install Spin Operator CRDs&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f https://github.com/spinkube/spin-operator/releases/download/v0.1.0/spin-operator.crds.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Install the Runtime Class&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f https://github.com/spinkube/spin-operator/releases/download/v0.1.0/spin-operator.runtime-class.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, we will install &lt;a href="https://github.com/cert-manager/cert-manager">cert-manager&lt;/a>, which is required for provisioning TLS certificates used by the admission webhook of the Spin Operator. If you face issues installing &lt;code>cert-manager&lt;/code>, please consult the &lt;a href="https://cert-manager.io/docs/installation/helm/">cert-manager installation&lt;/a> documentation.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Add and update the Jetstack repository&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm repo add jetstack https://charts.jetstack.io
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm repo update
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Install the cert-manager chart alongside with CRDs needed by cert-manager&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm install &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> cert-manager jetstack/cert-manager &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --namespace cert-manager &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --create-namespace &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --version v1.14.4 &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --set installCRDs=true
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In order to install the &lt;code>containerd-wasm-shim&lt;/code> on the Kubernetes nodes we will use the &lt;a href="https://kwasm.sh/">kwasm-operator&lt;/a>. There is also a successor of &lt;code>kwasm-operator&lt;/code> - &lt;a href="https://github.com/spinkube/runtime-class-manager">runtime-class-manager&lt;/a> which aims to address some of the limitations of &lt;code>kwasm-operator&lt;/code> and provide a production grade implementation for deploying &lt;code>containerd&lt;/code> shims on Kubernetes nodes. Since &lt;code>kwasm-operator&lt;/code> is easier to install, for the purpose of this post we will use it instead of the &lt;code>runtime-class-manager&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Add the kwasm helm repository&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm repo add kwasm http://kwasm.sh/kwasm-operator/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm repo update
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Install KWasm operator&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm install &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> kwasm-operator kwasm/kwasm-operator &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --namespace kwasm &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --create-namespace &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --set kwasmOperator.installerImage=ghcr.io/spinkube/containerd-shim-spin/node-installer:v0.13.1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Annotate all nodes in the cluster so kwasm can select them and provision the required containerd shim&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl annotate node --all kwasm.sh/kwasm-node=true
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can see that a pod has started and completed in the &lt;code>kwasm&lt;/code> namespace.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n kwasm get pod
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ip-10-180-7-60.eu-west-1.compute.internal-provision-kwasm-qhr8r 0/1 Completed 0 8s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kwasm-operator-6c76c5f94b-8zt4s 1/1 Running 0 15s
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The logs of the &lt;code>kwasm-operator&lt;/code> also indicate that the node was provisioned with the required shim.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl -n kwasm logs kwasm-operator-6c76c5f94b-8zt4s
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{&lt;span style="color:#a31515">&amp;#34;level&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;info&amp;#34;&lt;/span>,&lt;span style="color:#a31515">&amp;#34;node&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;ip-10-180-7-60.eu-west-1.compute.internal&amp;#34;&lt;/span>,&lt;span style="color:#a31515">&amp;#34;time&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;2024-04-18T05:44:25Z&amp;#34;&lt;/span>,&lt;span style="color:#a31515">&amp;#34;message&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;Trying to Deploy on ip-10-180-7-60.eu-west-1.compute.internal&amp;#34;&lt;/span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{&lt;span style="color:#a31515">&amp;#34;level&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;info&amp;#34;&lt;/span>,&lt;span style="color:#a31515">&amp;#34;time&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;2024-04-18T05:44:31Z&amp;#34;&lt;/span>,&lt;span style="color:#a31515">&amp;#34;message&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;Job ip-10-180-7-60.eu-west-1.compute.internal-provision-kwasm is still Ongoing&amp;#34;&lt;/span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>{&lt;span style="color:#a31515">&amp;#34;level&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;info&amp;#34;&lt;/span>,&lt;span style="color:#a31515">&amp;#34;time&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;2024-04-18T05:44:31Z&amp;#34;&lt;/span>,&lt;span style="color:#a31515">&amp;#34;message&amp;#34;&lt;/span>:&lt;span style="color:#a31515">&amp;#34;Job ip-10-180-7-60.eu-west-1.compute.internal-provision-kwasm is Completed. Happy WASMing&amp;#34;&lt;/span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally we can deploy the &lt;code>spin-operator&lt;/code> alongside with a &lt;a href="https://www.spinkube.dev/docs/glossary/#spin-app-executor-crd">shim-executor&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>helm install spin-operator &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --namespace spin-operator &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --create-namespace &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --version 0.1.0 &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> --wait &lt;span style="color:#a31515">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a31515">&lt;/span> oci://ghcr.io/spinkube/charts/spin-operator
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f https://github.com/spinkube/spin-operator/releases/download/v0.1.0/spin-operator.shim-executor.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="deploy-a-spin-app">Deploy a Spin App&lt;/h2>
&lt;p>Let&amp;rsquo;s deploy a sample Spin application using the following command:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/spinkube/spin-operator/main/config/samples/simple.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After the CRD has been picked up by the &lt;code>spin-operator&lt;/code>, a pod will be created running the sample application. Let&amp;rsquo;s explore its logs.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl logs simple-spinapp-56687588d9-nbrtq
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Serving http://0.0.0.0:80
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Available Routes:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> hello: http://0.0.0.0:80/hello
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> go-hello: http://0.0.0.0:80/go-hello
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can see the available routes served by the application. Let&amp;rsquo;s port forward to the application &lt;code>service&lt;/code> and test them out.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl port-forward services/simple-spinapp 8000:80
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Forwarding from 127.0.0.1:8000 -&amp;gt; 80
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Forwarding from [::1]:8000 -&amp;gt; 80
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In another terminal, we can verify that the application returns a response.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>curl http://localhost:8000/hello
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Hello world from Spin!%
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This sets the ground for further experimentation and testing. What the &lt;code>SpinApp&lt;/code> CRD provides as capabilities and API can be explored through the &lt;a href="https://www.spinkube.dev/docs/spin-operator/reference/spin-app/">SpinApp CRD reference&lt;/a>.&lt;/p>
&lt;h2 id="cleanup">Cleanup&lt;/h2>
&lt;p>Let&amp;rsquo;s clean all deployed resources so far.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Delete the spin app and its executor&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl delete spinapp simple-spinapp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl delete spinappexecutors.core.spinoperator.dev containerd-shim-spin
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Uninstall the spin-operator chart&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm -n spin-operator uninstall spin-operator
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Remove the kwasm.sh/kwasm-node annotation from nodes&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl annotate node --all kwasm.sh/kwasm-node-
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Uninstall the kwasm-operator chart&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm -n kwasm uninstall kwasm-operator
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Uninstall the cert-manager chart&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>helm -n cert-manager uninstall cert-manager
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000"># Delete the runtime class and SpinApp CRDs&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl delete runtimeclass wasmtime-spin-v2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl delete crd spinappexecutors.core.spinoperator.dev
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl delete crd spinapps.core.spinoperator.dev
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In my opinion, WASM on the server is here to stay. Communities are expressing more and more interest in integrating Kubernetes with WASM workloads. As shown Gardener clusters are perfectly capable of supporting this use case. This setup is a great way to start exploring the capabilities that WASM can bring to the server. As stated in the introduction, bear in mind that this post does not define a production environment, but is rather meant to define a playground suitable for exploring and trying out ideas.&lt;/p></description></item><item><title>Blog: KubeCon / CloudNativeCon Europe 2024 Highlights</title><link>https://gardener.cloud/blog/2024/04-05-kubecon-cloudnativecon-europe-2024-highlights/</link><pubDate>Fri, 05 Apr 2024 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2024/04-05-kubecon-cloudnativecon-europe-2024-highlights/</guid><description>
&lt;p>&lt;img src="https://gardener.cloud/__resources/kubecon-eu2024_1207d8.png" alt="KubeCon EU 2024 Keynote Room" title="KubeCon EU 2024 Keynote Room">&lt;/p>
&lt;p>KubeCon + CloudNativeCon Europe 2024, recently held in Paris, was a testament to the robustness of the open-source community and its pivotal role in driving advancements in AI and cloud-native technologies. With a record attendance of over +12,000 participants, the conference underscored the ubiquity of cloud-native architectures and the business opportunities they provide.&lt;/p>
&lt;h2 id="ai-everywhere">AI Everywhere&lt;/h2>
&lt;p>LLMs and GenAI took center stage at the event, with discussions on challenges such as security, data management, and energy consumption. A popular quote stated, &amp;ldquo;If #inference is the new web application, #kubernetes is the new web server&amp;rdquo;. The conference emphasized the need for more open data models for AI to democratize the technology. Cloud-native platforms offer advantages for AI innovation, such as packaging models and dependencies as Docker packages and enhancing resource management for proper model execution. The community is exploring AI workload management, including using CPUs for inferencing and preprocessing data before handing it over to GPUs. CNCF took the initiative and put together an &lt;a href="https://www.cncf.io/reports/cloud-native-artificial-intelligence-whitepaper/">AI whitepaper&lt;/a> outlining the apparent synergy between cloud-native technologies and AI.&lt;/p>
&lt;h2 id="cluster-autopilot">Cluster Autopilot&lt;/h2>
&lt;p>The conference showcased popular projects in the cloud-native ecosystem, including Kubernetes, Istio, and OpenTelemetry. Kubernetes was highlighted as a platform for running massive AI workloads. The UXL Foundation aims to enable multi-vendor AI workloads on Kubernetes, allowing developers to move AI workloads without being locked into a specific infrastructure. Every vendor we interacted with has assembled an AI-powered chatbot, which performs various functions – from assessing cluster health through analyzing cost efficiency and proposing workload optimizations to troubleshooting issues and alerting for potential challenges with upcoming Kubernetes version upgrades. Sysdig went even further with a chatbot, which answers the popular question, &amp;ldquo;Do any of my products have critical CVEs in production?&amp;rdquo; and analyzes workloads&amp;rsquo; structure and configuration. Some chatbots leveraged the &lt;a href="https://k8sgpt.ai/">k8sgpt project&lt;/a>, which joined the CNCF sandbox earlier this year.&lt;/p>
&lt;h2 id="sophisticated-fleet-management">Sophisticated Fleet Management&lt;/h2>
&lt;p>The ecosystem showcased maturity in observability, platform engineering, security, and optimization, which will help operationalize AI workloads. Data demands and costs were also in focus, touching on data observability and cloud-cost management. Cloud-native technologies, also going beyond Kubernetes, are expected to play a crucial role in managing the increasing volume of data and scaling AI. Google showcased fleet management in their Google Hosted Cloud offering (ex-Anthos). It allows for defining teams and policies at the fleet level, later applied to all the Kubernetes clusters in the fleet, irrespective of the infrastructure they run on (GCP and beyond).&lt;/p>
&lt;h2 id="wasm-everywhere">WASM Everywhere&lt;/h2>
&lt;p>The conference also highlighted the growing interest in WebAssembly (WASM) as a portable binary instruction format for executable programs and its integration with Kubernetes and other functions. The topic here started with a dedicated WASM pre-conference day, the sessions of which are available in the &lt;a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2MQteKoXxICTWiUdZYEw6RI">following playlist&lt;/a>. WASM is positioned as the smoother approach to software distribution and modularity, providing more lightweight runtime execution options and an easier way for app developers to enter.&lt;/p>
&lt;h2 id="rust-on-the-rise">Rust on the Rise&lt;/h2>
&lt;p>Several talks were promoting Rust as an ideal &lt;a href="https://youtu.be/2q3RLffSvEc">programming language for cloud-native workloads&lt;/a>. It was even promoted as suitable for &lt;a href="https://youtu.be/rXS-3hFYVjc">writing Kubernetes controllers&lt;/a>.&lt;/p>
&lt;h2 id="internal-developer-platforms">Internal Developer Platforms&lt;/h2>
&lt;p>The event showcased the importance of Internal Developer Platforms (IDPs), both commercial and open-source, in facilitating the development process across all types of organizations – from Allianz to Mercedes. &lt;a href="https://backstage.io/">Backstage&lt;/a> leads the pack by a large margin, with all relevant sessions being full or at capacity. Much effort goes into the modularization of Backstage, which was also a notable highlight at the conference.&lt;/p>
&lt;h2 id="sustainability">Sustainability&lt;/h2>
&lt;p>Sustainability was a key theme, with discussions on the role of cloud-native technologies in promoting green practices. The &lt;a href="https://github.com/kubecost">KubeCost application&lt;/a> folks put a lot of effort into emphasizing the large amount of wasted money, which hyperscalers benefit from. In parallel – the &lt;a href="https://kube-green.dev/">kube-green project&lt;/a> emphasized optimizing your cluster footprint to minimize CO2 emissions. The conference also highlighted the importance of open source in creating a level playing field for multiple players to compete, fostering diverse participation, and solving global challenges.&lt;/p>
&lt;h2 id="customer-stories">Customer Stories&lt;/h2>
&lt;p>In contrast to the Chicago KubeCon in 2023, the one in Paris outlined multiple case studies, best practices, and reference scenarios. Many enterprises and their IT teams were well represented at KubeCon - regarding sessions, sponsorships, and participation. These companies strive to excel forward, reaping the efficiency and flexibility benefits cloud-native architectures provide.
We came across multiple companies using &lt;a href="https://gardener.cloud/">Gardener&lt;/a> as their Kubernetes management underlay – including FUGA Cloud, StackIT, and metal-stack Cloud. We eagerly anticipate more companies embracing Gardener at future events. The consistent feedback from these companies has been overwhelmingly positive—they absolutely love using Gardener and our shared excitement grows as the community thrives!&lt;/p>
&lt;h2 id="notable-talks">Notable Talks&lt;/h2>
&lt;p>Notable talks from leaders in the cloud-native world, including Solomon Hykes, Bob Wise, and representatives from KCP for Platforms and the United Nations, provided valuable insights into the future of AI and cloud-native technologies. All the talks are now uploaded to YouTube in the following &lt;a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0">playlist&lt;/a>. Those do not include the various pre-conference days, available as &lt;a href="https://www.youtube.com/@cncf/playlists?view=1&amp;amp;sort=dd&amp;amp;flow=grid">separate playlists&lt;/a> by CNCF.&lt;/p>
&lt;h2 id="in-conclusion">In Conclusion&amp;hellip;&lt;/h2>
&lt;p>In conclusion, KubeCon 2024 showcased the intersection of AI and cloud-native technologies, the open-source community&amp;rsquo;s growth, and the cloud-native ecosystem&amp;rsquo;s maturity. Many enterprises are actively engaged there, innovating, trying, and growing their internal expertise. They&amp;rsquo;re using KubeCon as a recruiting event, expanding their internal talent pool and taking more of their internal operations and processes into their own hands. The event served as a platform for global collaboration, cross-company alignments, innovation, and the exchange of ideas, setting the stage for the future of cloud-native computing.&lt;/p></description></item></channel></rss>