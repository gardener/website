<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://gardener.cloud/blog/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/blog/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Blogs | Gardener</title><meta name=description content="OverviewHere you can find a variety of articles related to Gardener and keep up to date with the latest community calls, features, and highlights!
How to ContributeIf you’d like to create a new blog post, simply follow the steps outlined in the Documentation Contribution Guide and add the topic to the corresponding folder."><meta property="og:url" content="https://gardener.cloud/blog/"><meta property="og:site_name" content="Gardener"><meta property="og:title" content="Blogs"><meta property="og:description" content="OverviewHere you can find a variety of articles related to Gardener and keep up to date with the latest community calls, features, and highlights!
How to ContributeIf you’d like to create a new blog post, simply follow the steps outlined in the Documentation Contribution Guide and add the topic to the corresponding folder."><meta property="og:locale" content="en_US"><meta property="og:type" content="website"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Blogs"><meta itemprop=description content="OverviewHere you can find a variety of articles related to Gardener and keep up to date with the latest community calls, features, and highlights!
How to ContributeIf you’d like to create a new blog post, simply follow the steps outlined in the Documentation Contribution Guide and add the topic to the corresponding folder."><meta itemprop=datePublished content="2025-05-21T00:00:00+00:00"><meta itemprop=dateModified content="2025-05-21T00:00:00+00:00"><meta itemprop=wordCount content="53"><meta itemprop=image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Blogs"><meta name=twitter:description content="OverviewHere you can find a variety of articles related to Gardener and keep up to date with the latest community calls, features, and highlights!
How to ContributeIf you’d like to create a new blog post, simply follow the steps outlined in the Documentation Contribution Guide and add the topic to the corresponding folder."><link rel=preload href=/scss/main.min.64d56283aba037cc3a217d684edadfb4e3c57ca54122947d2f030f74bcd28a27.css as=style integrity="sha256-ZNVig6ugN8w6IX1oTtrftOPFfKVBIpR9LwMPdLzSiic=" crossorigin=anonymous><link href=/scss/main.min.64d56283aba037cc3a217d684edadfb4e3c57ca54122947d2f030f74bcd28a27.css rel=stylesheet integrity="sha256-ZNVig6ugN8w6IX1oTtrftOPFfKVBIpR9LwMPdLzSiic=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class="td-section td-blog"><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"><svg width="90" height="90" viewBox="0 0 90 90" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#FFF" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009F76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=navbar-brand__name>Gardener</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=https://demo.gardener.cloud target=_blank rel=noopener><span>Demo</span></a></li><li class=nav-item><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class=nav-item><a class=nav-link href=/docs><span>Documentation</span></a></li><li class=nav-item><a class=nav-link href=/blog><span>Blogs</span></a></li><li class=nav-item><a class=nav-link href=/community><span>Community</span></a></li><li class=nav-item><a class=nav-link href=https://join.slack.com/t/gardener-cloud/shared_invite/zt-33c9daems-3oOorhnqOSnldZPWqGmIBw target=_blank rel=noopener><span>Join us on</span></a></li></ul></div><div class="d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.d12b965d996396eb34a8ddab918f76e1.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><div class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"></div><div class="d-none d-xl-block col-xl-2 td-toc d-print-none"></div><main class="col-12 col-md-9 col-xl-8 ps-md-5 pe-md-4" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/blog/>Return to the regular view of this page</a>.</p></div><h1 class=title>Blogs</h1><div class=content><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>Here you can find a variety of articles related to Gardener and keep up to date with the latest community calls, features, and highlights!</p><h2 id=how-to-contribute>How to Contribute<a class=td-heading-self-link href=#how-to-contribute aria-label="Heading self-link"></a></h2><p>If you&rsquo;d like to create a new blog post, simply follow the steps outlined in the <a href=/docs/contribute/documentation/>Documentation Contribution Guide</a> and add the topic to the <a href=https://github.com/gardener/documentation/tree/master/website/blog>corresponding folder</a>.</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-74698c4da71724c2daf3f0a937845373>2025</h1><div class="td-byline mb-4"><time datetime=2025-05-21 class=text-body-secondary>Wednesday, May 21, 2025</time></div></div><div class=td-content><h1 id=pg-ff1457f66818b35c95a0a8ac6e54bda7>May</h1><div class="td-byline mb-4"><time datetime=2025-05-21 class=text-body-secondary>Wednesday, May 21, 2025</time></div></div><div class=td-content><h1 id=pg-c9f1fffb57fbf0b00ef79ab5f034da98>Fine-Tuning kube-proxy Readiness: Ensuring Accurate Health Checks During Node Scale-Down</h1><div class="td-byline mb-4"><time datetime=2025-05-21 class=text-body-secondary>Wednesday, May 21, 2025</time></div><p>Gardener has recently refined how it determines the readiness of <code>kube-proxy</code> components within managed Kubernetes clusters. This adjustment leads to more accurate system health reporting, especially during node scale-down operations orchestrated by <code>cluster-autoscaler</code>.</p><h3 id=the-challenge-kube-proxy-readiness-during-node-scale-down>The Challenge: kube-proxy Readiness During Node Scale-Down<a class=td-heading-self-link href=#the-challenge-kube-proxy-readiness-during-node-scale-down aria-label="Heading self-link"></a></h3><p>Previously, Gardener utilized <code>kube-proxy</code>&rsquo;s <code>/healthz</code> endpoint for its readiness probe. While generally effective, this endpoint&rsquo;s behavior changed in Kubernetes 1.28 (as part of <a href="https://github.com/alexanderConstantinescu/kubernetes-enhancements/blob/e3d8adae9cf79338add2149db0900e47a4c64338/keps/sig-network/3836-kube-proxy-improved-ingress-connectivity-reliability/README.md?plain=1#L105-L107">KEP-3836</a> and implemented in <a href=https://github.com/kubernetes/kubernetes/pull/116470>kubernetes/kubernetes#116470</a>). The <code>/healthz</code> endpoint now reports <code>kube-proxy</code> as unhealthy if its node is marked for deletion by <code>cluster-autoscaler</code> (e.g., via a specific taint) or has a deletion timestamp.</p><p>This behavior is intended to help external load balancers (particularly those using <code>externalTrafficPolicy: Cluster</code> on infrastructures like GCP) avoid sending <em>new</em> traffic to nodes that are about to be terminated. However, for Gardener&rsquo;s internal system component health checks, this meant that <code>kube-proxy</code> could appear unready for extended periods if node deletion was delayed due to <code>PodDisruptionBudgets</code> or long <code>terminationGracePeriodSeconds</code>. This could lead to misleading &ldquo;unhealthy&rdquo; states for the cluster&rsquo;s system components.</p><h3 id=the-solution-aligning-with-upstream-kube-proxy-enhancements>The Solution: Aligning with Upstream kube-proxy Enhancements<a class=td-heading-self-link href=#the-solution-aligning-with-upstream-kube-proxy-enhancements aria-label="Heading self-link"></a></h3><p>To address this, Gardener now leverages the <code>/livez</code> endpoint for <code>kube-proxy</code>&rsquo;s readiness probe in clusters running Kubernetes version 1.28 and newer. The <code>/livez</code> endpoint, also introduced as part of the aforementioned <code>kube-proxy</code> improvements, checks the actual liveness of the <code>kube-proxy</code> process itself, without considering the node&rsquo;s termination status.</p><p>For clusters running Kubernetes versions 1.27.x and older (where <code>/livez</code> is not available), Gardener will continue to use the <code>/healthz</code> endpoint for the readiness probe.</p><p>This change, detailed in <a href=https://github.com/gardener/gardener/pull/12015>gardener/gardener#12015</a>, ensures that Gardener&rsquo;s readiness check for <code>kube-proxy</code> accurately reflects <code>kube-proxy</code>&rsquo;s operational status rather than the node&rsquo;s lifecycle state. It&rsquo;s important to note that this adjustment does not interfere with the goals of KEP-3836; cloud controller managers can still utilize the <code>/healthz</code> endpoint for their load balancer health checks as intended.</p><h3 id=benefits-for-gardener-operators>Benefits for Gardener Operators<a class=td-heading-self-link href=#benefits-for-gardener-operators aria-label="Heading self-link"></a></h3><p>This enhancement brings a key benefit to Gardener operators:</p><ul><li><strong>More Accurate System Health:</strong> The system components health check will no longer report <code>kube-proxy</code> as unhealthy simply because its node is being gracefully terminated by <code>cluster-autoscaler</code>. This reduces false alarms and provides a clearer view of the cluster&rsquo;s actual health.</li><li><strong>Smoother Operations:</strong> Operations teams will experience fewer unnecessary alerts related to <code>kube-proxy</code> during routine scale-down events, allowing them to focus on genuine issues.</li></ul><p>By adapting its <code>kube-proxy</code> readiness checks, Gardener continues to refine its operational robustness, providing a more stable and predictable management experience.</p><h3 id=further-information>Further Information<a class=td-heading-self-link href=#further-information aria-label="Heading self-link"></a></h3><ul><li><strong>GitHub Pull Request:</strong> <a href=https://github.com/gardener/gardener/pull/12015>gardener/gardener#12015</a></li><li><strong>Recording of the presentation segment:</strong> <a href="https://youtu.be/ssvXpPliOY0?t=1151">Watch on YouTube (starts at the relevant section)</a></li><li><strong>Upstream KEP:</strong> <a href="https://github.com/alexanderConstantinescu/kubernetes-enhancements/blob/e3d8adae9cf79338add2149db0900e47a4c64338/keps/sig-network/3836-kube-proxy-improved-ingress-connectivity-reliability/README.md?plain=1#L105-L107">KEP-3836: Kube-proxy improved ingress connectivity reliability</a></li><li><strong>Upstream Kubernetes PR:</strong> <a href=https://github.com/kubernetes/kubernetes/pull/116470>kubernetes/kubernetes#116470</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-584d5f8b980f6c0ea31f0083ecf96c90>New in Gardener: Forceful Redeployment of gardenlets for Enhanced Operational Control</h1><div class="td-byline mb-4"><time datetime=2025-05-21 class=text-body-secondary>Wednesday, May 21, 2025</time></div><p>Gardener continues to enhance its operational capabilities, and a recent improvement introduces a much-requested feature for managing gardenlets: the ability to forcefully trigger their redeployment. This provides operators with greater control and a streamlined recovery path for specific scenarios.</p><h3 id=the-standard-gardenlet-lifecycle>The Standard gardenlet Lifecycle<a class=td-heading-self-link href=#the-standard-gardenlet-lifecycle aria-label="Heading self-link"></a></h3><p>gardenlets, crucial components in the Gardener architecture, are typically deployed into seed clusters. For setups utilizing the <code>seedmanagement.gardener.cloud/v1alpha1.Gardenlet</code> resource, particularly in unmanaged seeds (those not backed by a shoot cluster and <code>ManagedSeed</code> resource), the <code>gardener-operator</code> handles the initial deployment of the gardenlet.</p><p>Once this initial deployment is complete, the gardenlet takes over its own lifecycle, leveraging a self-upgrade strategy to keep itself up-to-date. Under normal circumstances, the <code>gardener-operator</code> does not intervene further after this initial phase.</p><h3 id=when-things-go-awry-the-need-for-intervention>When Things Go Awry: The Need for Intervention<a class=td-heading-self-link href=#when-things-go-awry-the-need-for-intervention aria-label="Heading self-link"></a></h3><p>While the self-upgrade mechanism is robust, certain situations can arise where a gardenlet might require a more direct intervention. For example:</p><ul><li>The gardenlet&rsquo;s client certificate to the virtual garden cluster might have expired or become invalid.</li><li>The gardenlet <code>Deployment</code> in the seed cluster might have been accidentally deleted or become corrupted.</li></ul><p>In such cases, because the <code>gardener-operator</code>&rsquo;s responsibility typically ends after the initial deployment, the gardenlet might not be able to recover on its own, potentially leading to operational issues.</p><h3 id=empowering-operators-the-force-redeploy-annotation>Empowering Operators: The Force-Redeploy Annotation<a class=td-heading-self-link href=#empowering-operators-the-force-redeploy-annotation aria-label="Heading self-link"></a></h3><p>To address these challenges, Gardener now allows operators to instruct the <code>gardener-operator</code> to forcefully redeploy a gardenlet. This is achieved by annotating the specific <code>Gardenlet</code> resource with:</p><pre tabindex=0><code>gardener.cloud/operation=force-redeploy
</code></pre><p>When this annotation is applied, it signals the <code>gardener-operator</code> to re-initiate the deployment process for the targeted gardenlet, effectively overriding the usual hands-off approach after initial setup.</p><h3 id=how-it-works>How It Works<a class=td-heading-self-link href=#how-it-works aria-label="Heading self-link"></a></h3><p>The process for a forceful redeployment is straightforward:</p><ol><li>An operator identifies a gardenlet that requires redeployment due to issues like an expired certificate or a missing deployment.</li><li>The operator applies the <code>gardener.cloud/operation=force-redeploy</code> annotation to the corresponding <code>seedmanagement.gardener.cloud/v1alpha1.Gardenlet</code> resource in the virtual garden cluster.</li><li><strong>Important:</strong> If the gardenlet is for a remote cluster and its kubeconfig <code>Secret</code> was previously removed (a standard cleanup step after initial deployment), this <code>Secret</code> must be recreated, and its reference (<code>.spec.kubeconfigSecretRef</code>) must be re-added to the <code>Gardenlet</code> specification.</li><li>The <code>gardener-operator</code> detects the annotation and proceeds to redeploy the gardenlet, applying its configurations and charts anew.</li><li>Once the redeployment is successfully completed, the <code>gardener-operator</code> automatically removes the <code>gardener.cloud/operation=force-redeploy</code> annotation from the <code>Gardenlet</code> resource. Similar to the initial deployment, it will also clean up the referenced kubeconfig <code>Secret</code> and set <code>.spec.kubeconfigSecretRef</code> to <code>nil</code> if it was provided.</li></ol><h3 id=benefits>Benefits<a class=td-heading-self-link href=#benefits aria-label="Heading self-link"></a></h3><p>This new feature offers significant advantages for Gardener operators:</p><ul><li><strong>Enhanced Recovery:</strong> Provides a clear and reliable mechanism to recover gardenlets from specific critical failure states.</li><li><strong>Improved Operational Flexibility:</strong> Offers more direct control over the gardenlet lifecycle when exceptional circumstances demand it.</li><li><strong>Reduced Manual Effort:</strong> Streamlines the process of restoring a misbehaving gardenlet, minimizing potential downtime or complex manual recovery procedures.</li></ul><p>This enhancement underscores Gardener&rsquo;s commitment to operational excellence and responsiveness to the needs of its user community.</p><h3 id=dive-deeper>Dive Deeper<a class=td-heading-self-link href=#dive-deeper aria-label="Heading self-link"></a></h3><p>To learn more about this feature, you can explore the following resources:</p><ul><li><strong>GitHub Pull Request:</strong> <a href=https://github.com/gardener/gardener/pull/11972>gardener/gardener#11972</a></li><li><strong>Official Documentation:</strong> <a href=https://github.com/gardener/gardener/tree/master/docs/deployment/deploy_gardenlet_via_operator.md#forceful-re-deployment>Forceful Re-Deployment of gardenlets</a></li><li><strong>Community Meeting Recording (starts at the relevant segment):</strong> <a href="https://youtu.be/ssvXpPliOY0?t=338">Gardener Review Meeting on YouTube</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7f61ecaa1cf2430641f2c9565b2c53d1>Streamlined Node Onboarding: Introducing `gardenadm token` and `gardenadm join`</h1><div class="td-byline mb-4"><time datetime=2025-05-21 class=text-body-secondary>Wednesday, May 21, 2025</time></div><p>Gardener continues to enhance its <code>gardenadm</code> tool, simplifying the management of autonomous Shoot clusters. Recently, new functionalities have been introduced to streamline the process of adding worker nodes to these clusters: the <code>gardenadm token</code> command suite and the corresponding <code>gardenadm join</code> command. These additions offer a more convenient and Kubernetes-native experience for cluster expansion.</p><h3 id=managing-bootstrap-tokens-with-gardenadm-token>Managing Bootstrap Tokens with <code>gardenadm token</code><a class=td-heading-self-link href=#managing-bootstrap-tokens-with-gardenadm-token aria-label="Heading self-link"></a></h3><p>A key aspect of securely joining nodes to a Kubernetes cluster is the use of bootstrap tokens. The new <code>gardenadm token</code> command provides a set of subcommands to manage these tokens effectively within your autonomous Shoot cluster&rsquo;s control plane node. This functionality is analogous to the familiar <code>kubeadm token</code> commands.</p><p>The available subcommands include:</p><ul><li><strong><code>gardenadm token list</code></strong>: Displays all current bootstrap tokens. You can also use the <code>--with-token-secrets</code> flag to include the token secrets in the output for easier inspection.</li><li><strong><code>gardenadm token generate</code></strong>: Generates a cryptographically random bootstrap token. This command only prints the token; it does not create it on the server.</li><li><strong><code>gardenadm token create [token]</code></strong>: Creates a new bootstrap token on the server. If you provide a token (in the format <code>[a-z0-9]{6}.[a-z0-9]{16}</code>), it will be used. If no token is supplied, <code>gardenadm</code> will automatically generate a random one and create it.<ul><li>A particularly helpful option for this command is <code>--print-join-command</code>. When used, instead of just outputting the token, it prints the complete <code>gardenadm join</code> command, ready to be copied and executed on the worker node you intend to join. You can also specify flags like <code>--description</code>, <code>--validity</code>, and <code>--worker-pool-name</code> to customize the token and the generated join command.</li></ul></li><li><strong><code>gardenadm token delete &lt;token-value...></code></strong>: Deletes one or more bootstrap tokens from the server. You can specify tokens by their ID, the full token string, or the name of the Kubernetes Secret storing the token (e.g., <code>bootstrap-token-&lt;id></code>).</li></ul><p>These commands provide comprehensive control over the lifecycle of bootstrap tokens, enhancing security and operational ease.</p><h3 id=joining-worker-nodes-with-gardenadm-join>Joining Worker Nodes with <code>gardenadm join</code><a class=td-heading-self-link href=#joining-worker-nodes-with-gardenadm-join aria-label="Heading self-link"></a></h3><p>Once a bootstrap token is created (ideally using <code>gardenadm token create --print-join-command</code> on a control plane node), the new <code>gardenadm join</code> command facilitates the process of adding a new worker node to the autonomous Shoot cluster.</p><p>The command is executed on the prospective worker machine and typically looks like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>gardenadm join --bootstrap-token &lt;token_id.token_secret&gt; --ca-certificate &lt;base64_encoded_ca_bundle&gt; --gardener-node-agent-secret-name &lt;os_config_secret_name&gt; &lt;control_plane_api_server_address&gt;
</span></span></code></pre></div><p>Key parameters include:</p><ul><li><code>--bootstrap-token</code>: The token obtained from the <code>gardenadm token create</code> command.</li><li><code>--ca-certificate</code>: The base64-encoded CA certificate bundle of the cluster&rsquo;s API server.</li><li><code>--gardener-node-agent-secret-name</code>: The name of the Secret in the <code>kube-system</code> namespace of the control plane that contains the OperatingSystemConfig (OSC) for the <code>gardener-node-agent</code>. This OSC dictates how the node should be configured.</li><li><code>&lt;control_plane_api_server_address></code>: The address of the Kubernetes API server of the autonomous cluster.</li></ul><p>Upon execution, <code>gardenadm join</code> performs several actions:</p><ol><li>It discovers the Kubernetes version of the control plane using the provided bootstrap token and CA certificate.</li><li>It checks if the <code>gardener-node-agent</code> has already been initialized on the machine.</li><li>If not already joined, it prepares the <code>gardener-node-init</code> configuration. This involves setting up a systemd service (<code>gardener-node-init.service</code>) which, in turn, downloads and runs the <code>gardener-node-agent</code>.</li><li>The <code>gardener-node-agent</code> then uses the bootstrap token to securely download its specific OperatingSystemConfig from the control plane.</li><li>Finally, it applies this configuration, setting up the kubelet and other necessary components, thereby officially joining the node to the cluster.</li></ol><p>After the node has successfully joined, the bootstrap token used for the process will be automatically deleted by the <code>kube-controller-manager</code> once it expires. However, it can also be manually deleted immediately using <code>gardenadm token delete</code> on the control plane node for enhanced security.</p><p>These new <code>gardenadm</code> commands significantly simplify the expansion of autonomous Shoot clusters, providing a robust and user-friendly mechanism for managing bootstrap tokens and joining worker nodes.</p><h3 id=further-information>Further Information<a class=td-heading-self-link href=#further-information aria-label="Heading self-link"></a></h3><ul><li><strong><code>gardenadm token</code> Pull Request:</strong> <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/28-autonomous-shoot-clusters.md>GEP-28</a> <code>gardenadm token</code> (<a href=https://github.com/gardener/gardener/pull/11934>#11934</a>)</li><li><strong><code>gardenadm join</code> Pull Request:</strong> <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/28-autonomous-shoot-clusters.md>GEP-28</a> <code>gardenadm join</code> (<a href=https://github.com/gardener/gardener/pull/11942>#11942</a>)</li><li><strong>Recording of the demo:</strong> Watch the demo starting at <a href="https://youtu.be/ssvXpPliOY0?t=768">12m48s</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3d3c662dbe65fcd8da97232e9214b1e0>Enhanced Network Flexibility: Gardener Now Supports CIDR Overlap for Non-HA Shoots</h1><div class="td-byline mb-4"><time datetime=2025-05-19 class=text-body-secondary>Monday, May 19, 2025</time></div><p>Gardener is continually evolving to offer greater flexibility and efficiency in managing Kubernetes clusters. A significant enhancement has been introduced that addresses a common networking challenge: the requirement for completely disjoint network CIDR blocks between a shoot cluster and its seed cluster. Now, Gardener allows for IPv4 network overlap in specific scenarios, providing users with more latitude in their network planning.</p><h3 id=addressing-ip-address-constraints>Addressing IP Address Constraints<a class=td-heading-self-link href=#addressing-ip-address-constraints aria-label="Heading self-link"></a></h3><p>Previously, all shoot cluster networks (pods, services, nodes) had to be distinct from the seed cluster&rsquo;s networks. This could be challenging in environments with limited IP address space or complex network topologies. With this new feature, IPv4 or dual-stack shoot clusters can now define pod, service, and node networks that overlap with the IPv4 networks of their seed cluster.</p><h3 id=how-it-works-nat-for-seamless-connectivity>How It Works: NAT for Seamless Connectivity<a class=td-heading-self-link href=#how-it-works-nat-for-seamless-connectivity aria-label="Heading self-link"></a></h3><p>This capability is enabled through a double Network Address Translation (NAT) mechanism within the VPN connection established between the shoot and seed clusters. When IPv4 network overlap is configured, Gardener intelligently maps the overlapping shoot and seed networks to a dedicated set of newly reserved IPv4 ranges. These ranges are used exclusively within the VPN pods to ensure seamless communication, effectively resolving any conflicts that would arise from the overlapping IPs.</p><p>The reserved mapping ranges are:</p><ul><li><code>241.0.0.0/8</code>: Seed Pod Mapping Range</li><li><code>242.0.0.0/8</code>: Shoot Node Mapping Range</li><li><code>243.0.0.0/8</code>: Shoot Service Mapping Range</li><li><code>244.0.0.0/8</code>: Shoot Pod Mapping Range</li></ul><h3 id=conditions-for-utilizing-overlapping-networks>Conditions for Utilizing Overlapping Networks<a class=td-heading-self-link href=#conditions-for-utilizing-overlapping-networks aria-label="Heading self-link"></a></h3><p>To leverage this new network flexibility, the following conditions must be met:</p><ol><li><strong>Non-Highly-Available VPN:</strong> The shoot cluster must utilize a non-highly-available (non-HA) VPN. This is typically the configuration for shoots with a non-HA control plane.</li><li><strong>IPv4 or Dual-Stack Shoots:</strong> The shoot cluster must be configured as either single-stack IPv4 or dual-stack (IPv4/IPv6). The overlap feature specifically pertains to IPv4 networks.</li><li><strong>Non-Use of Reserved Ranges:</strong> The shoot cluster&rsquo;s own defined networks (for pods, services, and nodes) must not utilize any of the Gardener-reserved IP ranges, including the newly introduced mapping ranges listed above, or the existing <code>240.0.0.0/8</code> range (Kube-ApiServer Mapping Range).</li></ol><p>It&rsquo;s important to note that Gardener will prevent the migration of a non-HA shoot to an HA setup if its network ranges currently overlap with the seed, as this feature is presently limited to non-HA VPN configurations. For single-stack IPv6 shoots, Gardener continues to enforce non-overlapping IPv6 networks to avoid any potential issues, although IPv6 address space exhaustion is less common.</p><h3 id=benefits-for-gardener-users>Benefits for Gardener Users<a class=td-heading-self-link href=#benefits-for-gardener-users aria-label="Heading self-link"></a></h3><p>This enhancement offers increased flexibility in IP address management, particularly beneficial for users operating numerous shoot clusters or those in environments with constrained IPv4 address availability. By relaxing the strict disjointedness requirement for non-HA shoots, Gardener simplifies network allocation and reduces the operational overhead associated with IP address planning.</p><h3 id=explore-further>Explore Further<a class=td-heading-self-link href=#explore-further aria-label="Heading self-link"></a></h3><p>To dive deeper into this feature, you can review the original pull request and the updated documentation:</p><ul><li><strong>GitHub PR:</strong> <a href=https://github.com/gardener/gardener/pull/11582>feat: Allow CIDR overlap for non-HA VPN shoots (#11582)</a></li><li><strong>Gardener Documentation:</strong> <a href=/docs/gardener/networking/shoot_networking/#overlapping-ipv4-networks-between-seed-and-shoot>Shoot Networking</a></li><li><strong>Developer Talk Recording:</strong> <a href="https://youtu.be/ZwurVm1IJ7o?t=0">Gardener Development - Sprint Review #131</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0ccde31ce45ecbf3c0a91d5eed0a85e4>Enhanced Node Management: Introducing In-Place Updates in Gardener</h1><div class="td-byline mb-4"><time datetime=2025-05-19 class=text-body-secondary>Monday, May 19, 2025</time></div><p>Gardener is committed to providing efficient and flexible Kubernetes cluster management. Traditionally, updates to worker pool configurations, such as machine image or Kubernetes minor version changes, trigger a rolling update. This process involves replacing existing nodes with new ones, which is a robust approach for many scenarios. However, for environments with physical or bare-metal nodes, or stateful workloads sensitive to node replacement, or if the virtual machine type is scarce, this can introduce challenges like extended update times and potential disruptions.</p><p>To address these needs, Gardener now introduces <strong>In-Place Node Updates</strong>. This new capability allows certain updates to be applied directly to existing worker nodes without requiring their replacement, significantly reducing disruption and speeding up update processes for compatible changes.</p><h3 id=new-update-strategies-for-worker-pools>New Update Strategies for Worker Pools<a class=td-heading-self-link href=#new-update-strategies-for-worker-pools aria-label="Heading self-link"></a></h3><p>Gardener now supports three distinct update strategies for your worker pools, configurable via the <code>updateStrategy</code> field in the <code>Shoot</code> specification&rsquo;s worker pool definition:</p><ul><li><strong><code>AutoRollingUpdate</code></strong>: This is the classic and default strategy. When updates occur, nodes are cordoned, drained, terminated, and replaced with new nodes incorporating the changes.</li><li><strong><code>AutoInPlaceUpdate</code></strong>: With this strategy, compatible updates are applied directly to the existing nodes. The MachineControllerManager (MCM) automatically selects nodes, cordons and drains them, and then signals the Gardener Node Agent (GNA) to perform the update. Once GNA confirms success, MCM uncordons the node.</li><li><strong><code>ManualInPlaceUpdate</code></strong>: This strategy also applies updates directly to existing nodes but gives operators fine-grained control. After an update is specified, MCM marks all nodes in the pool as candidates. Operators must then manually label individual nodes to select them for the in-place update process, which then proceeds similarly to the <code>AutoInPlaceUpdate</code> strategy.</li></ul><p>The <code>AutoInPlaceUpdate</code> and <code>ManualInPlaceUpdate</code> strategies are available when the <code>InPlaceNodeUpdates</code> feature gate is enabled in the <code>gardener-apiserver</code>.</p><h3 id=what-can-be-updated-in-place>What Can Be Updated In-Place?<a class=td-heading-self-link href=#what-can-be-updated-in-place aria-label="Heading self-link"></a></h3><p>In-place updates are designed to handle a variety of common operational tasks more efficiently:</p><ul><li><strong>Machine Image Updates</strong>: Newer versions of a machine image can be rolled out by executing an update command directly on the node, provided the image and cloud profile are configured to support this.</li><li><strong>Kubernetes Minor Version Updates</strong>: Updates to the Kubernetes minor version of worker nodes can be applied in-place.</li><li><strong>Kubelet Configuration Changes</strong>: Modifications to the Kubelet configuration can be applied directly.</li><li><strong>Credentials Rotation</strong>: Critical for security, rotation of Certificate Authorities (CAs) and ServiceAccount signing keys can now be performed on existing nodes without replacement.</li></ul><p>However, some changes still necessitate a rolling update (node replacement):</p><ul><li>Changing the machine image name (e.g., switching from Ubuntu to Garden Linux).</li><li>Modifying the machine type.</li><li>Altering volume types or sizes.</li><li>Changing the Container Runtime Interface (CRI) name (e.g., from Docker to containerd).</li><li>Enabling or disabling node-local DNS.</li></ul><h3 id=key-api-and-component-adaptations>Key API and Component Adaptations<a class=td-heading-self-link href=#key-api-and-component-adaptations aria-label="Heading self-link"></a></h3><p>Several Gardener components and APIs have been enhanced to support in-place updates:</p><ul><li><strong>CloudProfile</strong>: The <code>CloudProfile</code> API now allows specifying <code>inPlaceUpdates</code> configuration within <code>machineImage.versions</code>. This includes a boolean <code>supported</code> field to indicate if a version supports in-place updates and an optional <code>minVersionForUpdate</code> string to define the minimum OS version from which an in-place update to the current version is permissible.</li><li><strong>Shoot Specification</strong>: As mentioned, the <code>spec.provider.workers[].updateStrategy</code> field allows selection of the desired update strategy. Additionally, <code>spec.provider.workers[].machineControllerManagerSettings</code> now includes <code>machineInPlaceUpdateTimeout</code> and <code>disableHealthTimeout</code> (which defaults to <code>true</code> for in-place strategies to prevent premature machine deletion during lengthy updates). For <code>ManualInPlaceUpdate</code>, <code>maxSurge</code> defaults to <code>0</code> and <code>maxUnavailable</code> to <code>1</code>.</li><li><strong>OperatingSystemConfig (OSC)</strong>: The OSC resource, managed by OS extensions, now includes <code>status.inPlaceUpdates.osUpdate</code> where extensions can specify the <code>command</code> and <code>args</code> for the Gardener Node Agent to execute for machine image (Operating System) updates. The <code>spec.inPlaceUpdates</code> field in the OSC will carry information like the target Operating System version, Kubelet version, and credential rotation status to the node.</li><li><strong>Gardener Node Agent (GNA)</strong>: GNA is responsible for executing the in-place updates on the node. It watches for a specific node condition ( <code>InPlaceUpdate</code> with reason <code>ReadyForUpdate</code>) set by MCM, performs the OS update, Kubelet updates, or credentials rotation, restarts necessary pods (like DaemonSets), and then labels the node with the update outcome.</li><li><strong>MachineControllerManager (MCM)</strong>: MCM orchestrates the in-place update process. For in-place strategies, while new machine classes and machine sets are created to reflect the desired state, the actual machine objects are not deleted and recreated. Instead, their ownership is transferred to the new machine set. MCM handles cordoning, draining, and setting node conditions to coordinate with GNA.</li><li><strong>Shoot Status & Constraints</strong>: To provide visibility, the <code>status.inPlaceUpdates.pendingWorkerUpdates</code> field in the <code>Shoot</code> now lists worker pools pending <code>autoInPlaceUpdate</code> or <code>manualInPlaceUpdate</code>. A new <code>ShootManualInPlaceWorkersUpdated</code> constraint is added if any manual in-place updates are pending, ensuring users are aware.</li><li><strong>Worker Status</strong>: The <code>Worker</code> extension resource now includes <code>status.inPlaceUpdates.workerPoolToHashMap</code> to track the configuration hash of worker pools that have undergone in-place updates. This helps Gardener determine if a pool is up-to-date.</li><li><strong>Forcing Updates</strong>: If an in-place update is stuck, the <code>gardener.cloud/operation=force-in-place-update</code> annotation can be added to the Shoot to allow subsequent changes or retries.</li></ul><h3 id=benefits-of-in-place-updates>Benefits of In-Place Updates<a class=td-heading-self-link href=#benefits-of-in-place-updates aria-label="Heading self-link"></a></h3><ul><li><strong>Reduced Disruption</strong>: Minimizes workload interruptions by avoiding full node replacements for compatible updates.</li><li><strong>Faster Updates</strong>: Applying changes directly can be quicker than provisioning new nodes, especially for OS patches or configuration changes.</li><li><strong>Bare-Metal Efficiency</strong>: Particularly beneficial for bare-metal environments where node provisioning is more time-consuming and complex.</li><li><strong>Stateful Workload Friendly</strong>: Lessens the impact on stateful applications that might be sensitive to node churn.</li></ul><p>In-place node updates represent a significant step forward in Gardener&rsquo;s operational flexibility, offering a more nuanced and efficient approach to managing node lifecycles, especially in demanding or specialized environments.</p><h3 id=dive-deeper>Dive Deeper<a class=td-heading-self-link href=#dive-deeper aria-label="Heading self-link"></a></h3><p>To explore the technical details and contributions that made this feature possible, refer to the following resources:</p><ul><li><strong>Parent Issue for &ldquo;[GEP-31] Support for In-Place Node Updates&rdquo;</strong>: <a href=https://github.com/gardener/gardener/issues/10219>Issue #10219</a></li><li><strong>GEP-31: In-Place Node Updates of Shoot Clusters</strong>: <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/31-inplace-node-update.md>GEP-31: In-Place Node Updates of Shoot Clusters</a></li><li><strong>Developer Talk Recording (starting at 39m37s)</strong>: <a href="https://youtu.be/ZwurVm1IJ7o?t=2377">Youtube</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d120589c63a3e999d70ebea3e76f10bf>Gardener Dashboard 1.80: Streamlined Credentials, Enhanced Cluster Views, and Real-Time Updates</h1><div class="td-byline mb-4"><time datetime=2025-05-19 class=text-body-secondary>Monday, May 19, 2025</time></div><p>Gardener Dashboard version 1.80 introduces several significant enhancements aimed at improving user experience, credentials management, and overall operational efficiency. These updates bring more clarity to credential handling, a smoother experience for managing large numbers of clusters, and a move towards a more reactive interface.</p><h3 id=unified-and-enhanced-credentials-management>Unified and Enhanced Credentials Management<a class=td-heading-self-link href=#unified-and-enhanced-credentials-management aria-label="Heading self-link"></a></h3><p>The management of secrets and credentials has been significantly revamped for better clarity and functionality:</p><ul><li><strong>Introducing CredentialsBindings:</strong> The dashboard now fully supports <code>CredentialsBinding</code> resources alongside the existing <code>SecretBinding</code> resources. This allows for referencing both Secrets and, in the future, Workload Identities more explicitly. While <code>CredentialsBindings</code> referencing Workload Identity resources are visible for cluster creation, editing or deleting them via the dashboard is not yet supported.</li><li><strong>&ldquo;Credentials&rdquo; Page:</strong> The former &ldquo;Secrets&rdquo; page has been renamed to &ldquo;Credentials.&rdquo; It features a new &ldquo;Kind&rdquo; column and distinct icons to clearly differentiate between <code>SecretBinding</code> and <code>CredentialsBinding</code> types, especially useful when resources share names. The column showing the referenced credential resource name has been removed as this information is part of the binding&rsquo;s details.</li><li><strong>Contextual Information and Safeguards:</strong> When editing a secret, all its associated data is now displayed, providing better context. If an underlying secret is referenced by multiple bindings, a hint is shown to prevent unintended impacts. Deletion of a binding is prevented if the underlying secret is still in use by another binding.</li><li><strong>Simplified Creation and Editing:</strong> New secrets created via the dashboard will now automatically generate a <code>CredentialsBinding</code>. While existing <code>SecretBindings</code> remain updatable, the creation of new <code>SecretBindings</code> through the dashboard is no longer supported, encouraging the adoption of the more versatile <code>CredentialsBinding</code>. The edit dialog for secrets now pre-fills current data, allowing for easier modification of specific fields.</li><li><strong>Handling Missing Secrets:</strong> The UI now provides clear information and guidance if a <code>CredentialsBinding</code> or <code>SecretBinding</code> references a secret that no longer exists.</li></ul><h3 id=revamped-cluster-list-for-improved-scalability>Revamped Cluster List for Improved Scalability<a class=td-heading-self-link href=#revamped-cluster-list-for-improved-scalability aria-label="Heading self-link"></a></h3><p>Navigating and managing a large number of clusters is now more efficient:</p><ul><li><strong>Virtual Scrolling:</strong> The cluster list has adopted virtual scrolling. Rows are rendered dynamically as you scroll, replacing the previous pagination system. This significantly improves performance and provides a smoother browsing experience, especially for environments with hundreds or thousands of clusters.</li><li><strong>Optimized Row Display:</strong> The height of individual rows in the cluster list has been reduced, allowing more clusters to be visible on the screen at once. Additionally, expandable content within a row (like worker details or ticket labels) now has a maximum height with internal scrolling, ensuring consistent row sizes and smooth virtual scrolling performance.</li></ul><h3 id=real-time-updates-for-projects>Real-Time Updates for Projects<a class=td-heading-self-link href=#real-time-updates-for-projects aria-label="Heading self-link"></a></h3><p>The dashboard is becoming more dynamic with the introduction of real-time updates:</p><ul><li><strong>Instant Project Changes:</strong> Modifications to projects, such as creation or deletion, are now reflected instantly in the project list and interface without requiring a page reload. This is achieved through WebSocket communication.</li><li><strong>Foundation for Future Reactivity:</strong> This enhancement for projects lays the groundwork for bringing real-time updates to other resources within the dashboard, such as Seeds and the Garden resource, in future releases.</li></ul><h3 id=other-notable-enhancements>Other Notable Enhancements<a class=td-heading-self-link href=#other-notable-enhancements aria-label="Heading self-link"></a></h3><ul><li><strong>Kubeconfig Update:</strong> The kubeconfig generated for garden cluster access via the &ldquo;Account&rdquo; page now uses the <code>--oidc-pkce-method</code> flag, replacing the deprecated <code>--oidc-use-pkce</code> flag. Users encountering deprecation messages should redownload their kubeconfig.</li><li><strong>Notification Behavior:</strong> Kubernetes warning notifications are now automatically dismissed after 5 seconds. However, all notifications will remain visible as long as the mouse cursor is hovering over them, giving users more time to read important messages.</li><li><strong>API Server URL Path:</strong> Support has been added for kubeconfigs that include a path in the API server URL.</li></ul><p>These updates in Gardener Dashboard 1.80 collectively enhance usability, provide better control over credentials, and improve performance for large-scale operations.</p><p>For a comprehensive list of all features, bug fixes, and contributor acknowledgments, please refer to the <a href=https://github.com/gardener/dashboard/releases/tag/1.80.0>official release notes</a>.
You can also view the segment of the community call discussing these dashboard updates <a href="https://youtu.be/ZwurVm1IJ7o?t=1793">here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bf04bed69dc8e0a4cc4625a9d8e34906>Gardener: Powering Enterprise Kubernetes at Scale and Europe's Sovereign Cloud Future</h1><div class="td-byline mb-4"><time datetime=2025-05-12 class=text-body-secondary>Monday, May 12, 2025</time></div><p>The Kubernetes ecosystem is dynamic, offering a wealth of tools to manage the complexities of modern cloud-native applications. For enterprises seeking to provision and manage Kubernetes clusters efficiently, securely, and at scale, a robust and comprehensive solution is paramount. Gardener, born from years of managing tens of thousands of clusters efficiently across diverse platforms and in demanding environments, stands out as a fully open-source choice for delivering fully managed Kubernetes Clusters as a Service. It already empowers organizations like SAP, STACKIT, T-Systems, and others (see <a href=https://gardener.cloud/adopter>adopters</a>) and has become a core technology for <a href=https://neonephos.org/projects>NeoNephos</a>, a project aimed at advancing digital autonomy in Europe (see <a href="https://www.youtube.com/watch?v=85MDID9Ju04&amp;t=621s">KubeCon London 2025 Keynote</a> and <a href=https://neonephos.org/2025/03/31/the-linux-foundation-announces-the-launch-of-neonephos-to-advance-digital-autonomy-in-europe>press announcement</a>).</p><h3 id=the-gardener-approach-an-architecture-forged-by-experience>The Gardener Approach: An Architecture Forged by Experience<a class=td-heading-self-link href=#the-gardener-approach-an-architecture-forged-by-experience aria-label="Heading self-link"></a></h3><p>At the heart of Gardener&rsquo;s architecture is the concept of &ldquo;Kubeception&rdquo; (see <a href="https://github.com/gardener/gardener?tab=readme-ov-file#gardener">readme</a> and <a href=/docs/gardener/concepts/architecture/>architecture</a>). This approach involves using Kubernetes to manage Kubernetes. Gardener runs on a Kubernetes cluster (called a <strong>runtime cluster</strong>), facilitates access through a self-managed node-less Kubernetes cluster (the <strong>garden cluster</strong>), manages Kubernetes control planes as pods within other self-managed Kubernetes clusters that provide high scalability (called <strong>seed clusters</strong>), and ultimately provisions end-user Kubernetes clusters (called <strong>shoot clusters</strong>).</p><p>This multi-layered architecture isn&rsquo;t complexity for its own sake. Gardener&rsquo;s design and extensive feature set are the product of over eight years of continuous development and refinement, directly shaped by the high-scale, security-sensitive, and enterprise-grade requirements of its users. Experience has shown that such a sophisticated structure is key to addressing significant challenges in scalability, security, and operational manageability. For instance:</p><ul><li><strong>Scalability:</strong> Gardener achieves considerable scalability through its use of <strong>seed clusters</strong>, which it also manages. This allows for the distribution of control planes, preventing bottlenecks. The design even envisions leveraging Gardener to host its own management components (as an <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/28-autonomous-shoot-clusters.md><strong>autonomous cluster</strong></a>), showcasing its resilience without risking circular dependencies.</li><li><strong>Security:</strong> A fundamental principle in Gardener is the strict isolation of control planes from data planes. This extends to Gardener itself, which runs in a dedicated management cluster but exposes its API to end-users through a workerless virtual cluster. This workerless cluster acts as an isolated access point, presenting no compute surface for potentially malicious pods, thereby significantly enhancing security.</li><li><strong>API Power & User Experience:</strong> Gardener utilizes the full capabilities of the Kubernetes API server. This enables advanced functionalities and sophisticated API change management. Crucially, for the end-user, interaction remains 100% Kubernetes-native. Users employ standard custom resources to instruct Gardener, meaning any tool, library, or language binding that supports Kubernetes CRDs inherently supports Gardener.</li></ul><h3 id=delivering-fully-managed-kubernetes-clusters-as-a-service>Delivering Fully Managed Kubernetes Clusters as a Service<a class=td-heading-self-link href=#delivering-fully-managed-kubernetes-clusters-as-a-service aria-label="Heading self-link"></a></h3><p>Gardener provides a comprehensive &ldquo;fully managed Kubernetes Clusters as a Service&rdquo; offering. This means it handles much more than just spinning up a cluster; it manages the entire lifecycle and operational aspects. Here’s a glimpse into its capabilities:</p><ol><li><p><strong>Full Cluster Lifecycle Management:</strong></p><ul><li><strong>Infrastructure Provisioning:</strong> Gardener takes on the provisioning and management of underlying cloud infrastructure, including VPCs, subnets, NAT gateways, security groups, IAM roles, and virtual machines across a wide range of providers like AWS, Azure, GCP, OpenStack, and more.</li><li><strong>Worker Node Management:</strong> It meticulously manages worker pools, covering OS images, machine types, autoscaling configurations (min/max/surge), update strategies, volume management, CRI configuration, and provider-specific settings.</li></ul></li><li><p><strong>Enterprise Platform Governance:</strong></p><ul><li><strong>Cloud Profiles:</strong> Gardener is designed with the comprehensive needs of enterprise platform operators in mind. Managing a fleet of clusters for an organization requires more than just provisioning; it demands clear governance over available resources, versions, and their lifecycle. Gardener addresses this through its declarative API, allowing platform administrators to define and enforce policies such as which Kubernetes versions are &ldquo;supported,&rdquo; &ldquo;preview,&rdquo; or &ldquo;deprecated,&rdquo; along with their expiration dates. Similarly, it allows control over available machine images, their versions, and lifecycle status. This level of granular control and lifecycle management for the underlying components of a Kubernetes service is crucial for enterprise adoption and stable operations. This is a key consideration often left as an additional implementation burden for platform teams using other cluster provisioning tools, where such governance features must be built on top. Gardener, by contrast, integrates these concerns directly into its API and operational model, simplifying the task for platform operators.</li></ul></li><li><p><strong>Advanced Networking:</strong></p><ul><li><strong>CNI Plugin Management:</strong> Gardener manages the deployment and configuration of CNI plugins such as Calico or Cilium.</li><li><strong>Dual-Stack Networking:</strong> It offers comprehensive support for IPv4, IPv6, and dual-stack configurations for pods, services, and nodes.</li><li><strong>NodeLocal DNS Cache:</strong> To enhance DNS performance and reliability, Gardener can deploy and manage NodeLocal DNS.</li></ul></li><li><p><strong>Comprehensive Autoscaling:</strong></p><ul><li><strong>Cluster Autoscaler:</strong> Gardener manages the Cluster Autoscaler for worker nodes, enabling dynamic scaling based on pod scheduling demands.</li><li><strong>Horizontal and Vertical Pod Autoscaler (VPA):</strong> It manages HPA/VPA for workloads and applies it to control plane components, optimizing resource utilization (see <a href=https://gardener.cloud/blog/2025/04-17-leaner-clusters-lower-bills>blog</a>).</li></ul></li><li><p><strong>Operational Excellence & Maintenance:</strong></p><ul><li><strong>Automated Kubernetes Upgrades:</strong> Gardener handles automated Kubernetes version upgrades for both control plane and worker nodes, with configurable maintenance windows.</li><li><strong>Automated OS Image Updates:</strong> It manages automated machine image updates for worker nodes.</li><li><strong>Cluster Hibernation:</strong> To optimize costs, Gardener supports hibernating clusters, scaling down components during inactivity.</li><li><strong>Scheduled Maintenance:</strong> It allows defining specific maintenance windows for predictability.</li><li><strong>Robust Credentials Rotation:</strong> Gardener features automated mechanisms for rotating <strong>all</strong> credentials. It provisions fine-grained, dedicated, and individual CAs, certificates, credentials, and secrets for each component — whether Kubernetes-related (such as service account keys or etcd encryption keys) or Gardener-specific (such as opt-in SSH keys or observability credentials). The Gardener installation, the seeds, and all shoots have their own distinct sets of credentials — amounting to more than 150 per shoot cluster control plane and hundreds of thousands for larger Gardener installations overall. All these credentials are rotated automatically and without downtime — most continuously, while some (like the API server CA) require user initiation to ensure operational awareness. For a deeper dive into Gardener&rsquo;s credential rotation, see our <a href="https://www.youtube.com/watch?v=3V8oFQ16mTg&amp;t=29s">Cloud Native Rejekts talk</a>). This granular approach effectively prevents lateral movement, significantly strengthening the security posture.</li></ul></li><li><p><strong>Enhanced Security & Access Control:</strong></p><ul><li><strong>OIDC Integration:</strong> Gardener supports OIDC configuration for the <code>kube-apiserver</code> for secure user authentication.</li><li><strong>Customizable Audit Policies:</strong> It allows specifying custom audit policies for detailed logging.</li><li><strong>Managed Service Account Issuers:</strong> Gardener can manage service account issuers, enhancing workload identity security.</li><li><strong>SSH Access Control:</strong> It provides mechanisms to manage SSH access to worker nodes securely if opted in (Gardener itself doesn&rsquo;t require SSH access to worker nodes).</li><li><strong>Workload Identity:</strong> Gardener supports workload identity features, allowing pods to securely authenticate to cloud provider services.</li></ul></li><li><p><strong>Powerful Extensibility:</strong></p><ul><li><strong>Extension Framework and Ecosystem:</strong> Gardener features a robust extension mechanism for deep integration of cloud providers, operating systems, container runtimes, or services like DNS management, certificate management, registry caches, network filtering, image signature verification, and more.</li><li><strong>Catered to Platform Builders:</strong> This extensibility also allows platform builders to deploy custom extensions into the self-managed seed cluster infrastructure that hosts shoot cluster control planes. This offers robust isolation for these custom components from the user&rsquo;s shoot cluster worker nodes, enhancing both security and operational stability.</li></ul></li><li><p><strong>Integrated DNS and Certificate Management:</strong></p><ul><li><strong>External DNS Management:</strong> Gardener can manage DNS records for the cluster&rsquo;s API server and services via its <code>shoot-dns-service</code> extension.</li><li><strong>Automated Certificate Management:</strong> Through extensions like <code>shoot-cert-service</code>, it manages TLS certificates, including ACME integration. Gardener also provides its own robust DNS (<code>dns-management</code>) and certificate (<code>cert-management</code>) solutions designed for enterprise scale. These custom solutions were developed because, at the scale Gardener operates, many deep optimizations were necessary, e.g., to avoid being rate-limited by upstream providers.</li></ul></li></ol><h3 id=a-kubernetes-native-foundation-for-sovereign-cloud>A Kubernetes-Native Foundation for Sovereign Cloud<a class=td-heading-self-link href=#a-kubernetes-native-foundation-for-sovereign-cloud aria-label="Heading self-link"></a></h3><p>The modern IT landscape is rapidly evolving away from primitive virtual machines towards distributed systems. Kubernetes has emerged as the de facto standard for deploying and managing these modern, cloud-native applications and services at scale. Gardener is squarely positioned at the forefront of this shift, offering a Kubernetes-native approach to managing Kubernetes clusters themselves. It possesses a mature, declarative, Kubernetes-native API for full cluster lifecycle management. Unlike services that might expose proprietary APIs, Gardener’s approach is inherently Kubernetes-native and multi-cloud. This unified API is comprehensive, offering a consistent way to manage diverse cluster landscapes.</p><p>Its nature as a fully open-source project is particularly relevant for initiatives like NeoNephos, which aim to build sovereign cloud solutions. All core features, stable releases, and essential operational components are available to the community. This inherent cloud-native, Kubernetes-centric design, coupled with its open-source nature and ability to run on diverse infrastructures (including on-premise and local cloud providers), provides the transparency, control, and technological independence crucial for digital sovereignty. Gardener delivers full sovereign control <em>today</em>, enabling organizations to run all modern applications and services at scale with complete authority over their infrastructure and data. This is a significant reason why many cloud providers and enterprises that champion sovereignty are choosing Gardener as their foundation and actively contributing to its ecosystem.</p><h3 id=operational-depth-reflecting-real-world-scale>Operational Depth Reflecting Real-World Scale<a class=td-heading-self-link href=#operational-depth-reflecting-real-world-scale aria-label="Heading self-link"></a></h3><p>Gardener&rsquo;s operational maturity is a direct reflection of its long evolution, shaped by the demands of enterprise users and real-world, large-scale deployments. This maturity translates into statistical evidence and track records of uptime for end-users and their critical services. For instance, Gardener includes fully automated, incremental etcd backups with a recovery point objective (RPO) of five minutes and supports autonomous, hands-off restoration workflows via <code>etcd-druid</code>. Features like Vertical Pod Autoscalers (VPAs), PodDisruptionBudgets (PDBs), NetworkPolicies, PriorityClasses, and sophisticated pod placement strategies are integral to Gardener&rsquo;s offering, ensuring high availability and fault tolerance. Gardener&rsquo;s automation deals with many of the usual exceptions and does not require human DevOps intervention for most operational tasks. Gardener&rsquo;s commitment to robust security is evident in <a href=https://gardener.cloud/blog/2021/09.12-navigating-cloud-native-security/#gardeners-proactive-security-posture>Gardener&rsquo;s proactive security posture</a>, which has proven effective in real-world scenarios. This depth of experience and automation ultimately translates into first-class Service Level Agreements (SLAs) that businesses can trust and rely on. As a testament to this, SAP entrusts Gardener with its Systems of Record. This level of operational excellence enables Gardener to meet the expectations of today’s most demanding Kubernetes use cases.</p><h3 id=conclusion-a-solid-foundation-for-your-kubernetes-strategy>Conclusion: A Solid Foundation for Your Kubernetes Strategy<a class=td-heading-self-link href=#conclusion-a-solid-foundation-for-your-kubernetes-strategy aria-label="Heading self-link"></a></h3><p>For enterprises and organizations seeking a comprehensive, truly open-source solution for managing the full lifecycle of Kubernetes clusters at scale, Gardener offers a compelling proposition. Its mature architecture, rich feature set, operational robustness, built-in enterprise governance capabilities, and commitment to the open-source community provide a solid foundation for running demanding Kubernetes workloads with confidence. This makes it a suitable technical underpinning for ambitious projects like NeoNephos, contributing to a future of greater digital autonomy.</p><p>We invite you to explore <a href=https://gardener.cloud/>Gardener</a> and discover how it can empower your enterprise-grade and -scale Kubernetes journey.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-58f0aa5a00b559e8bf3d94f8b205a4a9>April</h1><div class="td-byline mb-4"><time datetime=2025-04-17 class=text-body-secondary>Thursday, April 17, 2025</time></div></div><div class=td-content><h1 id=pg-8494dc13a2a0383601c075af04a527f9>Leaner Clusters, Lower Bills: How Gardener Optimized Kubernetes Compute Costs</h1><div class="td-byline mb-4"><time datetime=2025-04-17 class=text-body-secondary>Thursday, April 17, 2025</time></div><p>As organizations embrace Kubernetes for managing containerized applications at scale, the underlying infrastructure costs, particularly for compute resources, become a critical factor. Gardener, the open-source Kubernetes management platform, empowers organizations like SAP, STACKIT, T-Systems, and others (see <a href=https://gardener.cloud/adopter>adopters</a>) to operate tens of thousands of Kubernetes clusters efficiently across diverse environments. Gardener&rsquo;s role as a core technology in initiatives like <a href=https://neonephos.org/projects>NeoNephos</a>, aimed at advancing digital autonomy in Europe (see <a href="https://www.youtube.com/watch?v=85MDID9Ju04&amp;t=621s">KubeCon London 2025 Keynote</a> and <a href=https://neonephos.org/2025/03/31/the-linux-foundation-announces-the-launch-of-neonephos-to-advance-digital-autonomy-in-europe>press announcement</a>), further underscores the need for cost-effective and sustainable operations.</p><p>At the heart of Gardener&rsquo;s architecture is the concept of &ldquo;Kubeception&rdquo; (see <a href="https://github.com/gardener/gardener?tab=readme-ov-file#gardener">readme</a> and <a href=/docs/gardener/concepts/architecture/>architecture</a>): Gardener runs <em>on</em> Kubernetes (called a <strong>runtime cluster</strong>), facilitates access <em>through</em> a self-managed node-less Kubernetes cluster (called the <strong>garden cluster</strong>), manages Kubernetes control planes as pods <em>within</em> self-managed Kubernetes clusters that provide high scalability to Gardener (called <strong>seed clusters</strong>), and <em>provisions</em> end-user Kubernetes clusters (called <strong>shoot clusters</strong>). Therefore, optimizing Gardener&rsquo;s own Kubernetes-related resource consumption directly translates into cost savings across all these layers, benefiting both Gardener service providers and the end-users consuming the managed clusters.</p><p>While infrastructure costs span compute, storage, and networking, compute resources (the virtual machines running Kubernetes nodes) typically represent the largest share of the bill. Over the past years, the Gardener team has undertaken a significant effort to optimize these costs. This blog post details our journey, focusing heavily on the compute optimizations that go beyond standard autoscaling practices, ultimately delivering substantial savings that benefit the entire Gardener ecosystem.</p><p>We&rsquo;ll build upon the foundations laid out in our <a href=/docs/guides/applications/shoot-pod-autoscaling-best-practices/>Pod Autoscaling Best Practices Guide</a>. You may want to check it out beforehand, as we&rsquo;ll only touch upon a few key recommendations from it in this blog post, not delving into the full depth required for effective pod autoscaling – a prerequisite for the compute optimizations discussed here.</p><h2 id=visibility-and-initial-measures>Visibility and Initial Measures<a class=td-heading-self-link href=#visibility-and-initial-measures aria-label="Heading self-link"></a></h2><h3 id=know-your-spending-leveraging-observability-and-iaas-cost-tools>Know Your Spending: Leveraging Observability and IaaS Cost Tools<a class=td-heading-self-link href=#know-your-spending-leveraging-observability-and-iaas-cost-tools aria-label="Heading self-link"></a></h3><p>You can&rsquo;t optimize what you can&rsquo;t measure. Our first step was to gain deep visibility into our spending patterns. We leveraged:</p><ul><li><strong>IaaS Cost Reports & Alerts:</strong> Regularly analyzing detailed cost breakdowns from cloud providers (AWS Cost Explorer, Azure Cost Management, GCP Billing Reports) helped us identify major cost drivers across compute, storage, and network usage. Setting up alerts for cost anomalies makes us aware of regressions and unexpected budget overruns.</li><li><strong>Cloud Provider Recommendation Tools:</strong> Tools like AWS Trusted Advisor, Azure Advisor&rsquo;s Cost recommendations, and Google Cloud&rsquo;s machine type rightsizing recommendations provided initial, manual pointers towards obvious inefficiencies like underutilized virtual machines or suboptimal instance types.</li><li><strong>Internal Usage Reports:</strong> We generated custom reports detailing our own resource consumption. This helped identify and drive down the number and uptime of development and other non-production clusters. Automating the configuration of Gardener&rsquo;s <a href=/docs/gardener/shoot/shoot_hibernate/>cluster hibernation feature</a> or reporting on clusters with poor hibernation schedules further curbed unnecessary spending. These insights are now integrated into the Gardener Dashboard (our GUI).</li></ul><h3 id=the-reserved-instance--savings-plan-imperative-planning-for-discounts>The Reserved Instance / Savings Plan Imperative: Planning for Discounts<a class=td-heading-self-link href=#the-reserved-instance--savings-plan-imperative-planning-for-discounts aria-label="Heading self-link"></a></h3><p>Cloud providers offer significant discounts for commitment: Reserved Instances (RIs) on AWS/Azure, Savings Plans (SPs) on AWS/Azure, and Committed Use Discounts (CUDs) on GCP. However, maximizing their benefit requires careful planning, which is not the primary subject of this blog post. Companies typically have tools that generate recommendations from cost reports, suggesting the purchase of new RIs, SPs, or CUDs if on-demand usage consistently increases. Two key learnings emerged in this context, though:</p><ul><li><strong>Coordination between Operations and Controlling:</strong> We discovered that technical optimizations and discount commitment purchases <em>must</em> go hand-in-hand. A significant 20% utilization improvement can be completely negated if the remaining workload runs on expensive on-demand instances because the RI/SP/CUD purchase didn&rsquo;t account for the change. On-demand pricing can easily be twice or more expensive than committed pricing.</li><li><strong>Commitments vs. Spot Pricing:</strong> While Spot Instances/Preemptible virtual machines offer deep discounts, their ephemeral nature makes them unsuitable for critical control plane components. For predictable baseline workloads, well-planned RIs/SPs/CUDs provide substantial, reliable savings and are often more beneficial overall. Spot Instance/Preemptible VM discounts are generally not higher than, and often less than, RI/SP/CUD discounts for comparable commitment levels.</li></ul><h3 id=early-wins-finding-and-eliminating-resource-waste>Early Wins: Finding and Eliminating Resource Waste<a class=td-heading-self-link href=#early-wins-finding-and-eliminating-resource-waste aria-label="Heading self-link"></a></h3><p>We also actively looked for waste, specifically orphaned resources. Development and experimentation inevitably lead to forgotten resources (virtual machines, disks, load balancers, etc.). We implemented processes like requiring all resources to include a personal identifier in the name or as a label/tag to facilitate later cleanup. Initially, we generated simple reports, but it became clear that this task required a more professional approach. Unaccounted-for resources aren&rsquo;t just costly; they can also pose security risks or indicate security incidents. Therefore, we developed the <a href=https://github.com/gardener/inventory><code>gardener/inventory</code></a> tool. This tool understands Gardener installations and cross-references expected cloud provider resources (based on Gardener&rsquo;s desired state and implementation) against actually existing resources. It acts as an additional safety net, alerting on discrepancies (e.g., unexpected load balancers for a seed, unmanaged virtual machines in a VPC) which could indicate either cost leakage or a potential security issue, complementing Gardener&rsquo;s existing security measures like high-frequency credentials rotation, image signing and admission, network policies, Falco, etc.</p><h3 id=consolidation-avoiding-a-fragmented-seed-landscape>Consolidation: Avoiding a Fragmented Seed Landscape<a class=td-heading-self-link href=#consolidation-avoiding-a-fragmented-seed-landscape aria-label="Heading self-link"></a></h3><p>If possible, avoid operating too many small seeds unless required by regulations or driven by end-user demand. As Gardener supports control plane migration, you can consolidate your control planes into fewer, larger seeds where reasonable. Since starting Gardener in production in 2017, we&rsquo;ve encountered technological advancements (e.g., Azure Availability Sets to Zones) and corrected initial misconfigurations (e.g., too-small CIDR ranges limiting pod/node counts) that necessitated recreating seeds. While hard conflicts (like seed/shoot cluster IP address overlaps) can sometimes block migration to differently configured seeds, you can often at least merge multiple seeds into one or fewer. The key takeaway is that a less fragmented seed landscape generally leads to better efficiency.</p><p>However, there is a critical caveat: Gardener allows control planes to reside in different regions (or even different cloud providers) than their worker nodes. This flexibility comes at the cost of inter-regional or internet network traffic. These additional network-related costs can easily negate efficiency gains from seed consolidation. Therefore, consolidate thoughtfully, being mindful that excessive consolidation across regions can significantly increase network costs (intra-region traffic is cheaper than inter-region traffic, and internet traffic is usually the most expensive).</p><h2 id=quick-wins-in-networking-and-storage>Quick Wins in Networking and Storage<a class=td-heading-self-link href=#quick-wins-in-networking-and-storage aria-label="Heading self-link"></a></h2><p>While compute was our main focus, we also addressed significant cost drivers in networking and storage early on.</p><h3 id=centralized-ingress--caching>Centralized Ingress & Caching<a class=td-heading-self-link href=#centralized-ingress--caching aria-label="Heading self-link"></a></h3><ul><li><strong>Centralized Ingress:</strong> In Gardener&rsquo;s early days, each shoot control plane had its own Load Balancer (LB), plus another for the reverse tunnel connection to worker nodes (to reach webhooks, scrape metrics, stream logs, <code>exec</code> into pods, etc.). This proliferation of LBs was expensive. We transitioned to a model using a central Istio ingress-gateway per seed cluster with a single LB, leveraging SNI (Server Name Indication) routing to direct traffic to the correct control plane API servers. We also reversed the connection direction: shoots now connect <em>to</em> seed clusters, and seeds connect <em>to</em> the garden cluster. This reduced the need for LBs exposing seed components and enabled <em>private</em> shoots or even <em>private</em> seeds behind firewalls.</li><li><strong>Registry Cache:</strong> Pulling container images for essential components (like CNI, CSI drivers, kube-proxy) on every new node startup generated significant network traffic and costs. We implemented a <a href=https://github.com/gardener/gardener-extension-registry-cache>registry cache extension</a>, drastically reducing external image pulls (see <a href=https://gardener.cloud/blog/2024/04-22-gardeners-registry-cache-extension-another-cost-saving-win-and-more>blog post</a>).</li></ul><h3 id=smarter-networking-habits>Smarter Networking Habits<a class=td-heading-self-link href=#smarter-networking-habits aria-label="Heading self-link"></a></h3><ul><li><strong>Efficient API Usage:</strong> Well-implemented controllers use <code>watch</code> requests rather than frequent <code>list</code> requests to minimize API server load and improve responsiveness. Leveraging server-side filtering via <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors>label selectors</a> and <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors>field selectors</a> reduces the amount of data transferred.</li><li><strong>Reducing Cross-Zonal Traffic:</strong> Data transfer between availability zones, necessary for highly available control planes, is generally more expensive than within a single zone. We enabled Kubernetes&rsquo; <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints>Topology Aware Routing</a> to help route API server traffic within the same zone where possible, reducing cross-zonal traffic and therefore costs (see <a href=https://github.com/gardener/gardener/issues/6718>Gardener Issue #6718</a>).</li><li><strong>Avoiding Large Resources:</strong> Storing large amounts of data directly in Kubernetes resources (ConfigMaps, Secrets) is inefficient and strains etcd and the network. We utilize blob stores for large payloads, such as control plane etcd or state backups used for automated restoration or control plane migration (with data compressed and encrypted in transit and at rest).</li><li><strong>Regression Monitoring:</strong> Implementing regression monitoring for network traffic helped catch seemingly innocent code changes that could inadvertently cause massive spikes in data transfer costs.</li></ul><h3 id=conscious-storage-consumption>Conscious Storage Consumption<a class=td-heading-self-link href=#conscious-storage-consumption aria-label="Heading self-link"></a></h3><p>Storage costs were addressed by being mindful of Persistent Volume Claim (PVC) size and performance tiers (e.g., standard HDD vs. premium SSD). Choosing the right storage class based on actual workload needs prevents overspending on unused capacity or unnecessary IOPS.</p><h2 id=deep-dive-into-compute-cost-optimization>Deep Dive into Compute Cost Optimization<a class=td-heading-self-link href=#deep-dive-into-compute-cost-optimization aria-label="Heading self-link"></a></h2><p>This is where the most significant savings were realized. Optimizing compute utilization in Kubernetes is a multi-faceted challenge involving the interplay of several components.</p><h3 id=understanding-utilization-the-interplay-of-scheduler-cluster-autoscaler-hpa-and-vpa>Understanding Utilization: The Interplay of Scheduler, Cluster Autoscaler, HPA, and VPA<a class=td-heading-self-link href=#understanding-utilization-the-interplay-of-scheduler-cluster-autoscaler-hpa-and-vpa aria-label="Heading self-link"></a></h3><p>We think of utilization optimization in two stages:</p><ol><li><strong>Packing Pods onto Nodes (Requests vs. Allocatable):</strong> How efficiently are the resource <em>requests</em> of your pods filling up the <em>allocatable</em> capacity of your nodes? This is primarily influenced by the Kube-Scheduler and the Cluster Autoscaler (CA).</li><li><strong>Right-Sizing Pods (Usage vs. Requests):</strong> How closely does the actual resource <em>usage</em> of your pods match their <em>requests</em>? This is where Horizontal Pod Autoscaler (HPA) and Vertical Pod Autoscaler (VPA) come in.</li></ol><p>You need to optimize <em>both</em> stages for maximum efficiency.</p><h3 id=optimizing-scheduling-bin-packing-and-pod-priorities-with-kube-scheduler>Optimizing Scheduling: Bin-Packing and Pod Priorities with Kube-Scheduler<a class=td-heading-self-link href=#optimizing-scheduling-bin-packing-and-pod-priorities-with-kube-scheduler aria-label="Heading self-link"></a></h3><ul><li><strong>Bin-Packing:</strong> By default, Kube-Scheduler tries to spread pods across nodes (using the <code>LeastAllocated</code> strategy). For cost optimization, <em>packing</em> pods tightly onto fewer nodes (using the <code>MostAllocated</code> strategy, often called bin-packing) is more effective. Gardener runs Kubernetes control planes as pods on seed clusters. Switching the Kube-Scheduler profile in our seed clusters to prioritize bin-packing yielded over 20% reduction in machine costs for these clusters simply by requiring fewer nodes. We also made this scheduling profile available for shoot clusters (see <a href=https://github.com/gardener/gardener/pull/6251>Gardener PR #6251</a>).</li><li><strong>Pod Priorities:</strong> Assigning proper <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption>Pod Priorities</a> is important not just for stability but also for cost. High-priority pods (like control plane components) can preempt lower-priority pods if necessary, reducing the need to maintain excess capacity just in case a critical pod needs scheduling space. This avoids unnecessary over-provisioning.</li></ul><h3 id=voluntary-disruptions-pod-disruption-budgets>Voluntary Disruptions: Pod Disruption Budgets<a class=td-heading-self-link href=#voluntary-disruptions-pod-disruption-budgets aria-label="Heading self-link"></a></h3><ul><li><strong>Pod Disruption Budgets:</strong> Defining proper <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb>Pod Disruption Budgets (PDBs)</a> helps manage and steer voluntary disruptions safely. We define them consistently for all Gardener components. This provides the necessary control to rebalance, compact, or generally replace underlying machines as needed by us or our automation, contributing to cost efficiency by enabling node consolidation.</li></ul><h3 id=enabling-higher-pod-density-per-node>Enabling Higher Pod Density per Node<a class=td-heading-self-link href=#enabling-higher-pod-density-per-node aria-label="Heading self-link"></a></h3><ul><li><strong>Node Configuration:</strong> To effectively utilize larger instance types and enable better bin-packing, nodes must be configured to handle more pods. We observed nodes becoming pod-bound (unable to schedule more pods despite available CPU/memory). To prevent this, ensure you provide:<ul><li>A large enough <code>--node-cidr-mask-size</code> (e.g., <code>/22</code> for ~1024 IPs, though assume ~80% effective due to IP reuse; see <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager>kube-controller-manager docs</a>) to allocate sufficient IPs per node.</li><li>Sufficient <code>--kube-reserved</code> resources (see <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet>kubelet docs</a>) to account for system overhead.</li><li>An increased <code>--max-pods</code> value (again, see <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet>kubelet docs</a>) to inform the kubelet and scheduler of the node&rsquo;s actual pod capacity.</li></ul></li></ul><h3 id=fine-tuning-the-cluster-autoscaler-scaling-nodes-efficiently>Fine-Tuning the Cluster Autoscaler: Scaling Nodes Efficiently<a class=td-heading-self-link href=#fine-tuning-the-cluster-autoscaler-scaling-nodes-efficiently aria-label="Heading self-link"></a></h3><p>The cluster autoscaler (CA) adds or removes nodes based on pending pods and node utilization. We tuned its behavior for better cost efficiency:</p><ul><li><code>--scale-down-unneeded-time=15m</code>: Time a node must be underutilized before CA considers it for removal, allowing removal of persistently unneeded capacity.</li><li><code>--scale-down-delay-after-add=30m</code>: Prevents CA from removing a node too soon after adding one, reducing potential node thrashing during fluctuating load.</li><li><code>--scale-down-utilization-threshold=0.9</code>: We significantly increased this threshold (default is 0.5). It instructs CA to attempt removing any node running below 90% utilization <em>if</em> it can safely reschedule the existing pods onto other available nodes; otherwise, it does nothing. We have run with this setting successfully for a long time, supported by properly tuned pod priorities, PDBs managing voluntary disruptions, highly available control planes, and Kubernetes&rsquo; level-triggered, asynchronous nature.</li></ul><h3 id=mastering-pod-autoscaling-hpa-vpa-and-best-practices>Mastering Pod Autoscaling: HPA, VPA, and Best Practices<a class=td-heading-self-link href=#mastering-pod-autoscaling-hpa-vpa-and-best-practices aria-label="Heading self-link"></a></h3><p>Right-sizing pods dynamically is key. Kubernetes offers HPA and VPA:</p><ul><li><strong>Horizontal Pod Autoscaling (HPA):</strong> Scales the <em>number</em> of pod replicas based on metrics (CPU/memory utilization, custom metrics). Ideal for stateless applications handling variable request loads.</li><li><strong>Vertical Pod Autoscaler (VPA):</strong> Adjusts the CPU/memory <em>requests</em> of existing pods. Ideal for stateless and also stateful applications or workloads with fluctuating resource needs over time, without changing replica count.</li></ul><h3 id=our-best-practices--learnings>Our Best Practices & Learnings:<a class=td-heading-self-link href=#our-best-practices--learnings aria-label="Heading self-link"></a></h3><ul><li><strong>Combine HPA and VPA for API Servers Safely:</strong> You <em>can</em> use HPA and VPA together, even on the same metric (like CPU), but careful configuration is essential. The key is to configure HPA to scale based on the <em>average value</em> (<code>target.type: AverageValue</code>) rather than <em>utilization percentage</em> (<code>target.type: Utilization</code>). This prevents conflicts where VPA changes the requests, which would otherwise immediately invalidate HPA&rsquo;s utilization calculation.<ul><li><em>Example HPA targeting average CPU/Memory values:</em><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  minReplicas: 3
</span></span><span style=display:flex><span>  maxReplicas: 12
</span></span><span style=display:flex><span>  metrics:
</span></span><span style=display:flex><span>  - resource:
</span></span><span style=display:flex><span>      name: cpu
</span></span><span style=display:flex><span>      target:
</span></span><span style=display:flex><span>        averageValue: 6 # Target 6 cores average usage per pod (Note: String value often required)
</span></span><span style=display:flex><span>        type: AverageValue
</span></span><span style=display:flex><span>    type: Resource
</span></span><span style=display:flex><span>  - resource:
</span></span><span style=display:flex><span>      name: memory
</span></span><span style=display:flex><span>      target:
</span></span><span style=display:flex><span>        averageValue: 24Gi <span style=color:green># Target 24Gi average usage per pod</span>
</span></span><span style=display:flex><span>        type: AverageValue
</span></span><span style=display:flex><span>    type: Resource
</span></span><span style=display:flex><span>  behavior: <span style=color:green># Fine-tune scaling behavior</span>
</span></span><span style=display:flex><span>    scaleDown:
</span></span><span style=display:flex><span>      policies:
</span></span><span style=display:flex><span>      - periodSeconds: 300
</span></span><span style=display:flex><span>        type: Pods
</span></span><span style=display:flex><span>        value: 1
</span></span><span style=display:flex><span>      selectPolicy: Max
</span></span><span style=display:flex><span>      stabilizationWindowSeconds: 1800
</span></span><span style=display:flex><span>    scaleUp:
</span></span><span style=display:flex><span>      policies:
</span></span><span style=display:flex><span>      - periodSeconds: 60
</span></span><span style=display:flex><span>        type: Percent
</span></span><span style=display:flex><span>        value: 100
</span></span><span style=display:flex><span>      selectPolicy: Max
</span></span><span style=display:flex><span>      stabilizationWindowSeconds: 60
</span></span><span style=display:flex><span>  scaleTargetRef:
</span></span><span style=display:flex><span>    apiVersion: apps/v1
</span></span><span style=display:flex><span>    kind: Deployment
</span></span><span style=display:flex><span>    name: kube-apiserver
</span></span></code></pre></div></li></ul></li><li><strong>Tune VPA Configuration:</strong><ul><li>We adjusted VPA parameters like <code>--target-cpu-percentile</code> / <code>--target-memory-percentile</code> (determining the percentile of historical usage data to include in target recommendations, ignoring spikes above) and margin/bound parameters to make VPA less sensitive to tiny spikes and react faster and more accurately to sustained changes.</li><li>We also tuned parameters like <code>--cpu-histogram-decay-half-life</code> (from 24h to 15m) and <code>--recommendation-lower-bound-cpu-percentile</code> (from 0.5 to 0.7) to follow changes in CPU utilization more closely (work on memory is ongoing).</li><li><strong>VPA <code>minAllowed</code>:</strong> We set <code>minAllowed</code> (per VPA resource) based on observed usage patterns and historical outage data related to VPA scaling down too aggressively.</li><li><strong>VPA <code>maxAllowed</code>:</strong> We set <code>maxAllowed</code> (per VPA controller) to prevent request recommendations from exceeding node capacity. We found <code>maxAllowed</code> couldn&rsquo;t be configured centrally in the VPA controller, so we contributed this feature upstream (see <a href=https://github.com/kubernetes/autoscaler/issues/7147>Kubernetes Autoscaler Issue #7147</a> and <a href=https://github.com/kubernetes/autoscaler/pull/7560>corresponding PR</a>).</li></ul></li><li><strong>Set Pod Requests:</strong> We always set CPU and memory requests for our containers or let VPA manage those.</li><li><strong>Tune Pod Requests:</strong> We systematically processed hundreds of components:<ul><li>Some deployments were placed under VPA management. Others (very small, below VPA&rsquo;s resolution of ~10m cores / 10Mi memory) were removed from VPA and given static requests.</li><li><strong>&ldquo;Initial&rdquo; Requests:</strong> For pods managed by VPA, we set initial requests to the observed P5 (5th percentile) of historical usage. This provides a reasonable starting point for VPA.</li><li><strong>&ldquo;Static&rdquo; Requests:</strong> For pods not managed by VPA, we set requests to the P95 (95th percentile). This ensures they generally have enough resources; only exceptional spikes might cause issues, where VPA wouldn&rsquo;t typically help either.</li></ul></li><li><strong>Quality of Service (QoS):</strong> Prefer the <code>Burstable</code> QoS class (requests set, ideally no limits) for most workloads. Avoid <code>BestEffort</code> (no requests/limits), as these pods are the first to be evicted under pressure. Avoid <code>Guaranteed</code> (requests match limits), as limits often cause more harm than good. See our <a href=/docs/guides/applications/shoot-pod-autoscaling-best-practices/#quality-of-service-qos>Pod Autoscaling Best Practices Guide</a>. Pods in the <code>Guaranteed</code> QoS class, or generally those with limits, will be actively CPU-throttled and can be OOMKilled even if the node has ample spare capacity. Worse, if containers in the pod are under VPA, their CPU requests/limits often won&rsquo;t scale up appropriately because CPU throttling goes unnoticed by VPA.<ul><li><strong>Avoid Limits:</strong> In Gardener&rsquo;s context (and often also elsewhere), setting CPU limits offers few advantages and significant disadvantages, primarily unnecessary throttling. Setting memory limits <em>can</em> prevent runaway processes but may also prematurely kill pods. We generally avoid setting limits unless the theoretical maximum resource consumption of a component is well understood. When unsure, let VPA manage requests and rely on monitoring/alerting for excessive usage.</li></ul></li></ul><h2 id=data-driven-machine-type-selection>Data-Driven Machine Type Selection<a class=td-heading-self-link href=#data-driven-machine-type-selection aria-label="Heading self-link"></a></h2><h3 id=continuous-monitoring-understanding-how-well-our-machines-are-utilized>Continuous Monitoring: Understanding How Well Our Machines are Utilized<a class=td-heading-self-link href=#continuous-monitoring-understanding-how-well-our-machines-are-utilized aria-label="Heading self-link"></a></h3><p>Before optimizing machine type selection, we established comprehensive machine utilization monitoring. This was important during individual improvement steps to validate their effectiveness. We collect key metrics per Gardener installation, cloud provider, seed, and worker pool, and created dashboards to visualize and monitor our machine costs. These dashboards include:</p><ul><li>Total CPU [in thousand cores], Total Memory [in TB], Total Number of Control Planes [count]</li><li>Used Capacity CPU [%], Used Capacity Memory [%], Unused vs. Capacity Cost [Currency]</li><li>Requested Allocatable CPU [%], Requested Allocatable Memory [%], Unrequested vs. Allocatable Cost [Currency]</li><li>Used Requested CPU [%], Used Requested Memory [%], Unused vs. Requested Cost [Currency]</li><li>Used Reserved CPU [%, can exceed 100%], Used Reserved Memory [%, can exceed 100%], Unused vs. Reserved Cost [Currency]</li><li>Nodes with >99% filling levels, broken down by CPU, memory, volumes, and pods (to identify the most critical resource blocking further usage)</li><li>Effective CPU:memory ratio of the workload (more on that later)</li></ul><h3 id=why-machine-types-matter-size-ratios-generations-and-hidden-constraints>Why Machine Types Matter: Size, Ratios, Generations, and Hidden Constraints<a class=td-heading-self-link href=#why-machine-types-matter-size-ratios-generations-and-hidden-constraints aria-label="Heading self-link"></a></h3><p>Selecting the right machine type is critical for cost efficiency. Several factors come into play:</p><ul><li><strong>Size:</strong> Larger machines generally lead to less fragmentation (less wasted CPU/memory remainder per node) and better overhead efficiency (system components like kubelet/containerd consume a smaller percentage of total resources). However, smaller machines can be better for low-load scenarios while meeting high-availability constraints (e.g., needing to spread critical pods across 3 zones requires at least 3 nodes).</li><li><strong>CPU:Memory Ratio:</strong> Cloud providers offer instance families with different CPU:memory ratios (e.g., high-cpu 1:2, standard 1:4, high-memory 1:8). Matching the instance ratio to your workload&rsquo;s aggregate CPU:memory request ratio minimizes waste.</li><li><strong>Generations:</strong> Newer instance generations usually offer better performance and, crucially, better price-performance. This can also shift the effective CPU:memory ratio required by the workload due to performance differences.</li><li><strong>Hidden Constraints: Volume Limits:</strong> This proved to be a <em>major</em> factor, especially on AWS and Azure. Each instance type has a maximum number of network-attached volumes it can support. Gardener control planes, each with its own etcd cluster requiring persistent volumes for each replica, are heavily impacted. We often found ourselves limited by volume attachments long before hitting CPU or memory limits. Interestingly, ARM-based instance types on Azure support a slightly higher volume limit.</li></ul><h3 id=the-case-for-dedicated-pools-isolating-workloads>The Case for Dedicated Pools: Isolating Workloads<a class=td-heading-self-link href=#the-case-for-dedicated-pools-isolating-workloads aria-label="Heading self-link"></a></h3><p>While mixing diverse workloads seems efficient at first glance, dedicated node pools for specific workload types proved beneficial for several reasons:</p><ul><li><strong>Handling <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node><code>safe-to-evict: false</code></a>:</strong> Some pods (like single-replica stateful components for non-HA clusters) cannot be safely evicted by the Cluster Autoscaler. Mixing these with evictable pods on the same node can prevent the CA from scaling down that node, even if it&rsquo;s underutilized, negating cost savings. Placing these non-evictable pods in a dedicated pool (where scale-down might be disabled or carefully managed) isolates this behavior.</li><li><strong>Volume Concentration:</strong> Our &ldquo;etcd&rdquo; worker pools host primarily etcd pods (high volume count) and daemonsets, while &ldquo;standard&rdquo; pools host API servers, controllers, etc. (lower volume concentration). This difference influences the optimal machine type due to volume attachment limits.</li><li><strong>Preventing Scheduling Traps:</strong> Ensure critical, long-running pods (like Istio gateways) have node affinities/selectors to land only on their preferred, optimized node pools. Avoid them landing on temporary, large nodes spun up for short-lived bulky pods; if such a pod prevents the large node from scaling down (e.g., because the pool is at its minimum node count), the CA won&rsquo;t automatically replace the underutilized large node with a smaller one. That&rsquo;s a concept called &ldquo;workload consolidation&rdquo;, today only supported by <a href=https://github.com/kubernetes-sigs/karpenter>Karpenter</a>, which isn&rsquo;t supporting as many cloud providers as CA.</li></ul><h3 id=analyzing-workload-profiles-finding-the-optimal-instance-size-and-family>Analyzing Workload Profiles: Finding the Optimal Instance Size and Family<a class=td-heading-self-link href=#analyzing-workload-profiles-finding-the-optimal-instance-size-and-family aria-label="Heading self-link"></a></h3><p>Early on, we used a guide for operators to estimate a reasonable machine size for a seed cluster based on the number of hosted control planes, e.g.:</p><table><thead><tr><th>Optimal<br>Worker Pool (CPUxMem+Vols)</th><th>Very Low Seed Utilization<br>0 &lt;= |control planes| &lt; 15</th><th>Low Seed Utilization<br>5 &lt;= |control planes| &lt; 30</th><th>Medium Seed Utilization<br>10 &lt;= |control planes| &lt; 70</th><th>High Seed Utilization<br>30 &lt;= |control planes| &lt; 180</th><th>Very High Seed Utilization<br>120 &lt;= |control planes| &lt; ∞</th></tr></thead><tbody><tr><td>AWS</td><td><code>m5.large</code>(2x8+26)</td><td><code>r7i.large</code>(2x16+32)</td><td><code>r7i.xlarge</code>(4x32+32)</td><td><code>r7i.2xlarge</code>(8x64+32)</td><td><code>r7i.2xlarge</code>(8x64+32)</td></tr><tr><td>Azure</td><td><code>Standard_D2s_v5</code>(2x8+4)</td><td><code>Standard_D4s_v5</code>(4x16+8)</td><td><code>Standard_D8s_v5</code>(8x32+16)</td><td><code>Standard_D16s_v5</code>(16x64+32)</td><td><code>Standard_D16s_v5</code>(16x64+32)</td></tr><tr><td>GCP</td><td><code>n1-standard-2</code>(2x8+127)</td><td><code>n1-standard-4</code>(4x15+127)</td><td><code>n1-standard-8</code>(8x30+127)</td><td><code>n1-standard-16</code>(16x60+127)</td><td><code>n1-standard-16</code>(16x60+127)</td></tr></tbody></table><p>This guide also recommended specific instance families. Choosing the right family requires calculating the workload&rsquo;s aggregate CPU:memory ratio (total requested CPU : total requested memory across similar workloads). For example, 1000 cores and 6000 GB memory yields a 1:6 ratio.</p><p>Next, one must calculate the cost per core and per GB for different instance families and determine the break-even CPU:memory ratio – the point where the resource waste of two families is equal. The cluster autoscaler doesn&rsquo;t perform this cost-aware analysis; it always weights CPU and memory equally (1:1).</p><p>To find the optimal family manually, we followed these steps when adding new generations/families:</p><ul><li><strong>Cost per Resource Unit:</strong> Determine the effective cost per core and per GB. Example:<ul><li>Instance A (2 cores, 4 GB) costs €48/month.</li><li>Instance B (2 cores, 8 GB) costs €64/month.</li><li>Difference: 4 GB and €16 -> €4 per GB.</li><li>Cost of 2 cores = €48 - (4 GB * €4/GB) = €32 -> €16 per core.</li></ul></li><li><strong>Break-Even Analysis:</strong> Using the unit costs, calculate the break-even CPU:memory ratio where the cost of waste balances out between two families for your specific workload ratio.</li></ul><p>For instance, if the break-even ratio between standard (1:4) and high-memory (1:8) families is 1:5.7, and your workload runs at 1:6, the high-memory family is likely more cost-effective.</p><h3 id=automating-the-choice-a-machine-type-recommender>Automating the Choice: A Machine Type Recommender<a class=td-heading-self-link href=#automating-the-choice-a-machine-type-recommender aria-label="Heading self-link"></a></h3><p>This manual process was tedious, error-prone, and infrequently performed, leading to suboptimal machine types running in many seeds. To address this, we developed an automated pool recommender based on the following principles:</p><ol><li><p><strong>Comprehensive Data Collection:</strong> The recommender gathers metrics across the entire Gardener installation for specific seed sets (groups of seeds with similar configurations like provider and region). For every relevant seed, it collects:</p><ul><li><strong>Node Metadata & Specs:</strong> Instance type, pool, zone, capacity, allocatable resources.</li><li><strong>CSI Node Info:</strong> Maximum attachable volume counts per node.</li><li><strong>Pod Specs:</strong> Resource requests (CPU, memory) for all pods, distinguishing daemonset pods.</li><li><strong>Actual Node Usage:</strong> Detailed usage statistics obtained directly from the <a href=https://kubernetes.io/docs/reference/instrumentation/node-metrics>kubelet summary API</a> (<code>/api/v1/nodes/NODENAME/proxy/stats/summary</code>). This provides actual cgroup-level data on CPU and memory consumption for kubelet, container runtime, system overhead, and individual pods. Especially for memory, this was the only reliable method we found to get accurate working set bytes overall (simply summing pod metrics is inaccurate due to page cache/sharing; see kernel docs for <a href=https://www.kernel.org/doc/Documentation/cgroup-v1/memory.txt>cgroup-v1</a> and <a href=https://www.kernel.org/doc/Documentation/cgroup-v2.txt>cgroup-v2</a>).</li></ul></li><li><p><strong>Analyzing the Data:</strong> Before recommending <em>new</em> types, the recommender calculates key metrics that act as predictors and provide context:</p><ul><li><strong>Workload Ratios:</strong> Requested Core : Requested GB, Attached Volume : Requested GB, Scheduled Pod : Requested GB.</li><li><strong>Measured Overhead Ratios:</strong> <em>Measured</em> Reserved Core : Pod Count, <em>Measured</em> Reserved GB : Pod Count.</li><li><strong>Aggregation:</strong> Machines are grouped by pool within a seed set.</li><li><strong>Performance Normalization:</strong> CPU metrics (usage) are normalized based on relative performance indicators of the analyzed machine type.</li></ul></li><li><p><strong>Simulating Workload on Candidate Machines:</strong> This is the core recommendation logic:</p><ul><li><strong>Candidate Iteration:</strong> The system iterates through all <em>potential</em> machine types available for the specific provider and region(s).</li><li><strong>Resource Calculation per Candidate:</strong> For each candidate machine type:<ul><li>Calculate <code>kube-reserved</code>: Estimates CPU/memory needed for kubelet/runtime using our measurement-based model, tailored to the candidate&rsquo;s capacity (more on that later).</li><li>Account for DaemonSets: Subtracts the average CPU/memory <em>requests</em> of DaemonSet pods (derived from current aggregated pool data).</li><li>Performance Adjustment: Adjusts CPU calculations (reserved, daemonset, workload requests) based on the candidate&rsquo;s performance factor relative to a baseline.</li><li>Calculate Allocatable Resources: Determines CPU/memory available for workload pods after subtracting reserved and DaemonSet resources.</li><li>Unschedulable Buffer: Reduces allocatable resources slightly (e.g., by the equivalent of an &ldquo;average pod&rdquo;) to account for resource fragmentation and imperfect bin-packing, slightly favoring larger nodes.</li></ul></li><li><strong>Constraint Checking & Usable Resources:</strong> Projects how much of the <em>aggregated current workload</em> (total requests) could fit onto the candidate. It considers multiple dimensions, converting them to a common unit (GB-equivalent) using the measured workload ratios:<ul><li>Performance-adjusted Allocatable CPU (converted to GB-equivalent)</li><li>Allocatable Memory (GB)</li><li>Attachable Volumes (converted to GB-equivalent)</li><li>Schedulable Pods (converted to GB-equivalent)
The <em>minimum</em> of these values determines the actual usable resources for that candidate machine type under the given workload profile – identifying the <em><strong>true bottleneck</strong></em>, i.e. whether a candidate is CPU-, memory-, volume-, pod-, or load-bound and thus potentially suboptimal.</li></ul></li><li><strong>Cost & Waste Analysis:</strong><ul><li>Calculates the base <code>machine_costs</code> (<code>Cores * Cost per Core + GBs * Cost per GB</code>) for the candidate.</li><li>Estimates <code>excess_costs</code> (waste) per machine due to factors like:<ul><li><em>Imperfect Packing:</em> Assumes the &ldquo;last&rdquo; node in a zone is only half-utilized on average.</li><li><em>Scale-Down Disabled:</em> Increases estimated waste if scale-down is disabled.</li><li><em>Volume Packing:</em> Adds potential waste if the workload is heavily volume-constrained, assuming not all nodes can be packed efficiently with volumes.</li></ul></li></ul></li><li><strong>Efficiency Score Calculation:</strong> Computes a relative efficiency score for each candidate:
<code>Efficiency = (Cost_of_Usable_Resources) / (Base_Machine_Cost + Estimated_Excess_Cost)</code>
This score reflects how cost-effectively the candidate machine type can serve the workload, factoring in estimated waste.</li></ul></li><li><p><strong>Ranking & Selection:</strong></p><ul><li><strong>Sorting:</strong> Candidates are ranked primarily by <code>Efficiency / Cost per Core</code>. Dividing by cost per core helps prioritize newer/cheaper instance generations or those with better RI/SP coverage, while still heavily favoring the calculated efficiency.</li><li><strong>Preferred Type & Hysteresis:</strong> The top-ranked type is marked as <code>preferred</code> and receives the highest CA expander priority. A threshold (e.g., >5% efficiency improvement) prevents switching the preferred type too frequently, avoiding churn (flapping).</li><li><strong>Priority Assignment:</strong> Priorities are assigned for the cluster autoscaler expander, favoring the preferred type and then ranking others based on the sort order.</li><li><strong>Handling Existing/Legacy Pools:</strong> Ensures that pools with currently running nodes, even if suboptimal or using non-standard names, are preserved to avoid disruption. Legacy pools are tainted with a <code>NoSchedule</code> taint to allow workload to slowely migrate away from them.</li></ul></li></ol><p>This data-driven, simulation-based approach allowed us to abandon guides like above and manual operations and consistently select machine types that offer the best balance of performance and cost for the specific workloads running on our Gardener seeds.</p><h3 id=reserving-capacity-for-kubelet-and-container-runtime-tailoring-kube-reserved-beyond-workload-naive-formulas>Reserving Capacity for Kubelet and Container Runtime: Tailoring <code>kube-reserved</code> Beyond Workload-Naive Formulas<a class=td-heading-self-link href=#reserving-capacity-for-kubelet-and-container-runtime-tailoring-kube-reserved-beyond-workload-naive-formulas aria-label="Heading self-link"></a></h3><p>As pod packing density increases, accurately accounting for resources needed by the system itself (kubelet, container runtime, OS) becomes critical. Standard cloud provider formulas for <code>kube-reserved</code> (see <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet>kubelet options</a>) are often workload-naive, based only on total node CPU/memory capacity (see <a href=https://medium.com/@danielepolencic/reserved-cpu-and-memory-in-kubernetes-nodes-65aee1946afd>summary blog post</a>). They can either over-reserve (wasting resources) or under-reserve (risking node stability). Our experience showed that formulas considering only node capacity and potentially <code>maxPods</code> were often significantly inaccurate, leading to either waste or instability.</p><p>Therefore, instead of relying on static formulas, we adopted a measurement-based approach combined with predictive modeling:</p><ol><li><p><strong>Measure Actual Overhead:</strong> We utilize the data already retrieved via the kubelet summary API. By querying this endpoint across thousands of nodes for all our seeds, we collect the <em>actual</em> CPU (<code>usageNanoCores</code>) and memory (<code>workingSetBytes</code>) consumed by the <code>kubelet</code> and <code>runtime</code> system containers under various conditions (different machine types, workload profiles like ETCD pools, varying pod densities).</p></li><li><p><strong>Derive Workload-Aware Ratios:</strong> We then calculate key ratios that correlate overhead with workload characteristics, specifically pod density:</p><ul><li><code>ratio_1_used_reserved_core_to_pods</code>: Average number of pods running per actually <em>used</em> reserved core (performance-normalized across machine types).</li><li><code>ratio_1_used_reserved_gi_to_pods</code>: Average number of pods running per actually <em>used</em> reserved GB of memory.</li></ul><p>These ratios capture how much system overhead is typically generated <em>per pod</em> on average within a specific pool type for a given seed set. We explored other potential predictors (containers, probes) but found pod count to be the most useful predictor with acceptable standard deviation.</p></li><li><p><strong>Predict Expected <code>kube-reserved</code>:</strong> We use these measured ratios to <em>predict</em> the necessary <code>kube-reserved</code> for <em>any</em> candidate machine type considered by the Pool Recommender. The model works as follows:</p><ul><li><strong>Base Load:</strong> We observed a consistent base memory overhead even on lightly loaded nodes (e.g., ~200MiB with <a href="https://github.com/gardenlinux/gardenlinux?tab=readme-ov-file#garden-linux">Garden Linux</a>, Gardener&rsquo;s own Debian-based container-optimized OS) and negligible base CPU overhead.</li><li><strong>Estimate Pod-Driven Overhead:</strong> Using the predicted pod density for a candidate machine type (based on its capacity and the workload profile), we multiply this density by the measured <code>ratio_1_used_reserved_core_to_pods</code> and <code>ratio_1_used_reserved_gi_to_pods</code> to estimate the required <code>kube-reserved</code> CPU and memory, respectively. This tailors the reservation to the candidate&rsquo;s specific capacity and performance characteristics.</li></ul></li><li><p><strong>Apply Thresholds for Stability:</strong> To prevent minor fluctuations in calculated recommendations from causing constant configuration changes (increasing <code>kube-reserved</code> can trigger pod evictions), we apply thresholds (hysteresis).</p></li></ol><p>This tailored, data-driven approach to <code>kube-reserved</code> provides better cost optimization and enhanced stability compared to generic, workload-naive formulas.</p><p><em>Note on <code>system-reserved</code>:</em> You might wonder why we only discussed <code>kube-reserved</code> and not <code>system-reserved</code>. Similar to our reasoning against resource limits, configuring <code>system-reserved</code> can lead to unexpected CPU throttling or OOM kills for critical system processes outside Kubernetes&rsquo; direct management. Therefore, Gardener focuses on configuring <code>kube-reserved</code> and relies on the kubelet&rsquo;s eviction mechanisms to manage overall node pressure. See also <a href=https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#general-guidelines>Reserve Compute Resources for System Daemons</a>.</p><h2 id=looking-ahead-continuous-improvement-and-future-optimizations>Looking Ahead: Continuous Improvement and Future Optimizations<a class=td-heading-self-link href=#looking-ahead-continuous-improvement-and-future-optimizations aria-label="Heading self-link"></a></h2><p>Cost optimization is an ongoing process, not a one-time fix. We&rsquo;re actively exploring further improvements:</p><ul><li><strong>Addressing Load Imbalances:</strong> VPA assigns the same request to all pods in a managed group (Deployment/StatefulSet/DaemonSet). This is inefficient for workloads with inherent imbalances (e.g., controller leaders vs. followers, etcd leader vs. followers, uneven load distribution across DaemonSet pods).<ul><li><strong>Request-Based Load Balancing:</strong> For components like the <code>kube-apiserver</code>, default connection-based load balancing can lead to uneven load distribution that VPA handles poorly (resulting in over-provisioning for some pods, under-provisioning for others). We have implemented request-based load balancing to distribute load more evenly, allowing VPA to set more accurate requests (see <a href=https://github.com/gardener/gardener/pull/11085>related work</a>).</li><li><strong>In-place pod resource updates</strong> (a Kubernetes enhancement) would be particularly beneficial in the future, allowing VPA to adjust resources without requiring pod restarts, further improving efficiency and stability.</li></ul></li><li><strong>Exploring Cilium / Replacing kube-proxy:</strong> Initial tests suggest switching the CNI from Calico to Cilium could yield 5-10% CPU savings on worker nodes, partly because Cilium can replace kube-proxy, reducing overhead. Memory usage appears similar and Gardener has supported Cilium for years. Alternatively, to eliminate kube-proxy without changing CNIs, we could evaluate <a href=https://docs.tigera.io/calico/latest/operations/ebpf/enabling-ebpf>Calico&rsquo;s eBPF data plane</a>, which can also replace kube-proxy.</li><li><strong>ARM Architecture:</strong> We are evaluating ARM-based CPUs (AWS Graviton, Azure Cobalt, GCP Axion). They are generally cheaper per core. Even if slightly slower per core (but often with a better price-performance), they offer additional instance family options, potentially allowing a better match to the workload&rsquo;s CPU:memory ratio (e.g., a 1:6 workload x86 ratio might turn into a performance-adjusted 1:5 ARM ratio and thereby result in less waste than x86 instance families of either a 1:4 or 1:8 ratio). Additionally, Azure&rsquo;s ARM instances sometimes offer slightly higher volume attachment limits.</li></ul><h2 id=conclusion-sustainable-savings-and-key-takeaways>Conclusion: Sustainable Savings and Key Takeaways<a class=td-heading-self-link href=#conclusion-sustainable-savings-and-key-takeaways aria-label="Heading self-link"></a></h2><p>Optimizing Kubernetes compute costs at scale is a complex but rewarding endeavor. Our journey with Gardener involved a multi-pronged approach:</p><ol><li><strong>Establish Visibility:</strong> Use cloud cost tools and internal monitoring to understand spending.</li><li><strong>Strategic Purchasing:</strong> Tightly align RI/SP/CUD purchases with technical optimizations and workload forecasts.</li><li><strong>Clean Up Waste:</strong> Eliminate orphaned resources and leverage features like cluster hibernation.</li><li><strong>Tune Kubernetes Core Components:</strong> Utilize scheduler bin-packing, fine-tune cluster autoscaler parameters, and master HPA/VPA configurations, including safe combined usage.</li><li><strong>Data-Driven Machine Selection:</strong> Analyze workload profiles, use dedicated pools strategically, consider all constraints (especially non-obvious ones like volume limits), and automate machine type recommendations based on real data and simulation.</li><li><strong>Accurate Overheads:</strong> Measure and tailor <code>kube-reserved</code> based on actual system usage patterns rather than static formulas.</li></ol><p>These efforts have yielded substantial cost reductions for operating Gardener itself and, by extension, for all Gardener adopters running managed Kubernetes clusters. We hope sharing our journey provides valuable insights for your own optimization efforts, whether you&rsquo;re just starting or looking to refine your existing strategies.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3102f9ae3ddedc989ff766c7849bb8c2>March</h1><div class="td-byline mb-4"><time datetime=2025-03-18 class=text-body-secondary>Tuesday, March 18, 2025</time></div></div><div class=td-content><h1 id=pg-3f946694eaa084ffd57daad085a7d436>Gardener at KubeCon + CloudNativeCon Europe, London 2025</h1><div class="td-byline mb-4"><time datetime=2025-03-18 class=text-body-secondary>Tuesday, March 18, 2025</time></div><h1 id=gardener-at-kubecon--cloudnativecon-europe-london-2025>Gardener at KubeCon + CloudNativeCon Europe, London 2025<a class=td-heading-self-link href=#gardener-at-kubecon--cloudnativecon-europe-london-2025 aria-label="Heading self-link"></a></h1><p>The open-source project <a href=https://gardener.cloud/>Gardener</a> is set to showcase its cutting-edge Kubernetes-as-a-Service (KaaS) capabilities at <a href=https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/>KubeCon + CloudNativeCon Europe</a> 2025 in London.</p><p>Gardener has been pioneering hosted control planes long before they became mainstream and is now the default choice within SAP and <a href=https://gardener.cloud/adopter/>other organizations</a> for provisioning Kubernetes clusters.
Organizations looking to transform their Infrastructure-as-a-Service (IaaS) into a Kubernetes-as-a-Service (KaaS) platform can experience Gardener’s powerful automation, security, and multi-cloud extensibility firsthand at the event or <a href=https://demo.gardener.cloud/>directly in the browser</a>.</p><h2 id=revolutionizing-kubernetes-management-with-gardener>Revolutionizing Kubernetes Management with Gardener<a class=td-heading-self-link href=#revolutionizing-kubernetes-management-with-gardener aria-label="Heading self-link"></a></h2><p>Gardener provides a fully managed Kubernetes cluster solution that is:</p><ul><li><strong>Infinitely Extensible:</strong> Gardener offers limitless extensibility, supporting AWS, Azure, Alicloud, GCP, OpenStack, and other infrastructures. Run nodes with Garden Linux, SuSE, Ubuntu, or Flatcar OS while utilizing runc or gVisor for container runtime flexibility and Calico or Cilium for CNI. Explore the <a href=https://gardener.cloud/docs/extensions/>Gardener Extensions Library</a> for even more customization options.</li><li><strong>Automated Lifecycle Management:</strong> Simplified provisioning, scaling, and updates with built-in automation.</li><li><strong>Security & Compliance:</strong> Enforced policies and strict isolation to meet regulatory and enterprise security requirements, with support for automated credential rotation.</li><li><strong>Multi-Tenancy & Cost Efficiency:</strong> Designed for organizations running at scale, optimizing resources without sacrificing performance.</li></ul><p>Also, explore our other open-source projects:</p><p><strong><a href=https://github.com/gardener/etcd-druid/>etcd-druid</a></strong> – Our in-house operator responsible for managing etcd instances in Gardener’s hosted control planes, ensuring the stability and performance of Kubernetes clusters.</p><p><strong><a href=https://github.com/openmcp-project>openMCP</a></strong> – Our latest open-source offering that enables organizations to streamline application development using control plane methodology, making it easy to roll out, upgrade, and replicate cloud environments securely and seamlessly.</p><p>As Kubernetes adoption continues to accelerate, Gardener remains the go-to choice for managing Kubernetes at scale across multi-cloud and hybrid environments. Stop by our booth <em><strong>S561</strong></em> at KubeCon + CloudNativeCon Europe 2025 to experience Gardener firsthand and meet the team to see how Gardener empowers organizations to run secure, scalable, and efficient Kubernetes clusters with ease.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bb707c167ef21bc1707d196b2bfdf446>2024</h1><div class="td-byline mb-4"><time datetime=2024-12-08 class=text-body-secondary>Sunday, December 08, 2024</time></div></div><div class=td-content><h1 id=pg-9de4bd43643c99e60492539f9fb7666c>Unleashing Potential: Highlights from the 6th Gardener Community Hackathon</h1><div class="td-byline mb-4"><time datetime=2024-12-08 class=text-body-secondary>Sunday, December 08, 2024</time></div><p><img src=/blog/2024/images/hackathon202412-team.jpg alt="Hackathon 2024/12 Team" title="Hackathon 2024/12 Team"></p><p>The <a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md>6th Gardener Community Hackathon</a>, hosted at <a href=https://www.schlosshof-info.de/>Schlosshof Freizeitheim</a> in <a href=https://maps.app.goo.gl/28FZXpzZLjgaKNef9>Schelklingen, Germany</a> in December 2024, was a hub of creativity and collaboration. Developers of various companies joined forces to explore new frontiers of the Gardener project. Here&rsquo;s a rundown of the key outcomes:</p><ol><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-ipv6-support-on-ironcore>🌐 <strong>IPv6 Support on IronCore</strong>:</a> The team successfully created dual-stack shoot clusters on IronCore, although LoadBalancer services for IPv6 traffic still need some work.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-version-classification-lifecycle-in-cloudprofiles>🔁 <strong>Version Classification Lifecycle in <code>CloudProfile</code></strong>:</a> A Gardener Enhancement Proposal (GEP) was developed to predefine the timestamps for Kubernetes or machine image version classifications in <code>CloudProfile</code>s.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-gardener-slis-shoot-cluster-creationdeletion-times>💡 <strong>Gardener SLIs: Shoot Cluster Creation/Deletion Times</strong>:</a> Metrics for shoot cluster creation and deletion times were exposed, improving observability in end-to-end testing.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#%EF%B8%8F-enhanced-seed-authorizer-with-labelfield-selectors>🛡️ <strong>Enhanced <code>Seed</code> Authorizer With Label/Field Selectors</strong>:</a> The <code>Seed</code> Authorizer was upgraded to enforce label/field selectors, restricting <code>gardenlet</code> access to specific <code>Shoot</code> resources.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-bring-your-own-etcd-encryption-key-via-key-management-systems>🔑 <strong>Bring Your Own ETCD Encryption Key via Key Management Systems</strong>:</a> Users can now manage the encryption key for ETCD of shoot clusters using external key management systems like Vault or AWS KMS.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#%EF%B8%8F-load-balancing-for-calls-to-kube-apiservers>⚖️ <strong>Load Balancing for Calls to <code>kube-apiserver</code></strong>:</a> Scalability and load balancing of requests to <code>kube-apiserver</code> were improved by leveraging Istio.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-validate-poc-for-in-place-node-updates-of-shoot-clusters>🪴 <strong>Validate PoC For In-Place Node Updates Of Shoot Clusters</strong>:</a> A proof-of-concept for in-place updates of Kubernetes minor versions and machine image versions in shoot clusters was validated.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-prevent-pod-scheduling-issues-due-to-overscaling>🚀 <strong>Prevent <code>Pod</code> Scheduling Issues Due To Overscaling</strong>:</a> The issue of the Vertical Pod Autoscaler recommending resource requirements beyond the allocatable resources of the largest nodes was addressed.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-prevent-multiple-systemd-unit-restarts-on-reconciliation-errors>💪🏻 <strong>Prevent Multiple <code>systemd</code> Unit Restarts On Reconciliation Errors</strong>:</a> The reconciliation process of <code>gardener-node-agent</code> was improved to prevent multiple restarts of <code>systemd</code> units.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#%EF%B8%8F-trigger-nodes-rollout-individually-per-worker-pool-during-credentials-rotation>🤹‍♂️ <strong>Trigger Nodes Rollout Individually per Worker Pool During Credentials Rotation</strong>:</a> More control over the rollout of worker nodes during shoot cluster credentials rotation was introduced.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#%EF%B8%8F-e2e-test-skeleton-for-autonomous-shoot-clusters>⛓️‍💥 <strong>E2E Test Skeleton For Autonomous Shoot Clusters</strong>:</a> The e2e test infrastructure for managing autonomous shoot clusters was established.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#%EF%B8%8F-deploy-prow-via-flux>⬆️ <strong>Deploy Prow Via Flux</strong>:</a> Prow, Gardener&rsquo;s CI and automation system, was deployed using Flux, a cloud-native solution for continuous delivery based on GitOps.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-replace-topologyawarehints-with-servicetrafficdistribution>🚏 <strong>Replace <code>TopologyAwareHints</code> With <code>ServiceTrafficDistribution</code></strong>:</a> <code>TopologyAwareHints</code> were replaced with <code>ServiceTrafficDistribution</code>, eliminating custom code in Gardener.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-support-more-use-cases-for-tokenrequestor>🪪 <strong>Support More Use-Cases For <code>TokenRequestor</code></strong>:</a> The injection of the current CA bundle into access secrets was enabled, supporting more use cases.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-cluster-autoscalers-provisioningrequest-api>🫄 <strong><code>cluster-autoscaler</code>&rsquo;s <code>ProvisioningRequest</code> API</strong>:</a> The <code>ProvisioningRequest</code> API in <code>cluster-autoscaler</code> was introduced, allowing users to provision new nodes or check if a pod would fit in the existing cluster without scaling up.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-watch-managedresources-in-shoot-care-controller>👀 <strong>Watch <code>ManagedResource</code>s In <code>Shoot</code> Care Controller</strong>:</a> A watch for <code>ManagedResource</code>s in the <code>Shoot</code> care controller was introduced, re-evaluating health checks immediately when relevant conditions change.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-cluster-api-provider-for-gardener>🐢 <strong>Cluster API Provider For Gardener</strong>:</a> The cluster API in Gardener was supported, allowing for the deployment and deletion of shoot clusters via the cluster API.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-make-cluster-autoscaler-work-in-local-setup>👨🏼‍💻 <strong>Make <code>cluster-autoscaler</code> Work In Local Setup</strong>:</a> The <code>cluster-autoscaler</code> was made to work in the local setup, setting the <code>nodeTemplate</code> in the <code>MachineClass</code> for the <code>cluster-autoscaler</code> to get the resource capacity of the nodes.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-use-structured-authorization-in-local-kind-cluster>🧹 <strong>Use Structured Authorization In Local KinD Cluster</strong>:</a> Structured Authorization was used to enable the <code>Seed</code> Authorizer in the local KinD clusters, speeding up cluster creation.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-drop-internal-versions-from-component-configuration-apis>🧹 <strong>Drop Internal Versions From Component Configuration APIs</strong>:</a> The internal version of component configurations was removed, reducing maintenance effort during development.
15:55</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-fix-non-functional-shoot-node-logging-in-local-setup>🐛 <strong>Fix Non-Functional Shoot Node Logging In Local Setup</strong>:</a> The shoot node logging in the local development setup was fixed.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#-no-longer-generate-empty-secret-for-reconcile-operatingsystemconfigs>🧹 <strong>No Longer Generate Empty <code>Secret</code> For <code>reconcile</code> <code>OperatingSystemConfig</code>s</strong>:</a> The generation of an empty <code>Secret</code> for <code>reconcile</code> <code>OperatingSystemConfig</code>s was stopped.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-12_Schelklingen/README.md#%EF%B8%8F-generic-monitoring-extension>🖥️ <strong>Generic Monitoring Extension</strong>:</a> The requirements for externalizing the monitoring aspect of Gardener were discussed.</li></ol><p>These outcomes reflect the ongoing progress and collaborative spirit of the Gardener community. We&rsquo;re eager to see what the next Hackathon will bring. Keep an eye out for more updates on the Gardener project!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-23fe1fc5dc93a318cce24ad7b53561aa>Introducing the New Gardener Demo Environment: Your Hands-On Playground for Kubernetes Management</h1><div class="td-byline mb-4"><time datetime=2024-11-09 class=text-body-secondary>Saturday, November 09, 2024</time></div><p>We&rsquo;re thrilled to announce the launch of our <a href=https://demo.gardener.cloud>new Gardener demo environment</a>!
This interactive playground is designed to provide you with a hands-on experience of Gardener, our open-source project that offers a Kubernetes-based solution for managing Kubernetes clusters across various cloud providers uniformly.</p><h2 id=why-a-demo-environment>Why a Demo Environment?<a class=td-heading-self-link href=#why-a-demo-environment aria-label="Heading self-link"></a></h2><p>We understand that the best way to learn is by doing.
That&rsquo;s why we&rsquo;ve created this demo environment.
It&rsquo;s a space where you can experiment with Gardener, explore its features, and see firsthand how it can simplify your Kubernetes operations - all without having to install anything on your local machine.</p><p><img src=https://demo.gardener.cloud/assets/screenshot-light-3TO--vgb.png alt=Demo></p><h2 id=easy-access-quick-learning>Easy Access, Quick Learning<a class=td-heading-self-link href=#easy-access-quick-learning aria-label="Heading self-link"></a></h2><p>Getting started is as simple as logging in with your GitHub account.
Once you&rsquo;re in, you&rsquo;ll have access to a terminal session running a pre-configured Gardener system (based on the <a href=/docs/gardener/deployment/getting_started_locally/>local setup</a> which you can also install on your machine if preferred).
This means you can dive right in and start creating and managing Kubernetes clusters.
Whether you&rsquo;re a seasoned Kubernetes user or just starting your Kubernetes journey, this demo environment is a great way to learn about Gardener. It&rsquo;s designed to be intuitive and interactive, helping you learn at your own pace.</p><h2 id=discover-the-power-of-gardener>Discover the Power of Gardener<a class=td-heading-self-link href=#discover-the-power-of-gardener aria-label="Heading self-link"></a></h2><p>Gardener is all about making Kubernetes management easier and more efficient.
With its ability to manage Kubernetes clusters homogenously across different cloud providers, it&rsquo;s a powerful tool that can optimize your resources, improve performance, and enhance the resilience of your Kubernetes workloads.
But don&rsquo;t just take our word for it - try it out for yourself!
The Gardener demo environment is your playground to discover the power of Gardener.</p><h2 id=start-your-gardener-journey-today>Start Your Gardener Journey Today<a class=td-heading-self-link href=#start-your-gardener-journey-today aria-label="Heading self-link"></a></h2><p>We&rsquo;re excited to see what you&rsquo;ll create with Gardener.
Log in with your GitHub account and start your hands-on Gardener journey today.
Happy exploring!</p><p><a href=https://demo.gardener.cloud>Enter the demo environment here!</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-01f5049482ede32005789c0f7628ca5e>PromCon EU 2024 Highlights</h1><div class="td-byline mb-4"><time datetime=2024-11-01 class=text-body-secondary>Friday, November 01, 2024</time></div><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>Many innovative observability and application performance management (APM) products and services were released over the last few years. They often adopt or enhance concepts that Prometheus invented more than a decade ago. However, Prometheus, as an open-source project, has never lost its importance in this fast-moving industry and is the core of Gardener&rsquo;s monitoring stack.</p><p>On September 11th and 12th, Prometheus developers and users met for PromCon EU 2024 in Berlin. The single-track conference provided a good mix of talks about the latest Prometheus development and governance by many core contributors, as well as users who shared their experiences with monitoring in a cloud-native environment. The overarching topic of the event was the upcoming Prometheus 3.0 release. Many of the presented innovations will also be interesting for the future of Gardener&rsquo;s observability stack. We will take a closer look at the changes in the Prometheus and Perses projects in the remainder of this article.</p><p><img src=/blog/2024/images/promcon2024.jpeg alt="alt text"></p><h2 id=prometheus-30---opentelemetry-everywhere>Prometheus 3.0 - OpenTelemetry Everywhere<a class=td-heading-self-link href=#prometheus-30---opentelemetry-everywhere aria-label="Heading self-link"></a></h2><p>A first beta version of Prometheus 3.0 was released during the first day of the conference. The main focus of the new major release is compatibility with OpenTelemetry and full support for metric ingestion using the OpenTelemetry protocol (OTLP). While OpenTelemetry and Prometheus metrics have many things in common, a lot of incompatibilities still need to be sorted out.</p><p>One of the differences can be found in the naming conventions for metrics and label or attribute names. Instead of simply replacing dots with underscores, the Prometheus developers decided to introduce full UTF-8 support to achieve the best possible compatibility. This plan brought up interesting syntactical challenges for PromQL queries, as well as a demand for conversion when interacting with systems that still have the restrictions of previous Prometheus releases.</p><p>The development of native histograms in Prometheus has been ongoing for a while. The more efficient way to represent histograms also contributes to OpenTelemetry compatibility.</p><p>While Prometheus traditionally uses labels to enrich a time series with metadata, OpenTelemetry describes the concepts of metric attributes and resource attributes. While the new release contains a new <code>Info</code> PromQL function to ease the enrichment with resource attributes, the development of a new metadata store has been initiated to improve the experience even more in the long term.</p><p>A new UI makes the creation of ad-hoc queries easier and provides better feedback about the structure of a query and possible mistakes to the user.</p><p>The Prometheus project has reached a high level of maturity after more than 12 years of development by an active community of contributors. To ensure that the community can continue growing, a new governance model was proposed, including clearly defined roles and a new steering committee.</p><h2 id=perses---the-new-kid-in-the-sandbox>Perses - The New Kid in The Sandbox<a class=td-heading-self-link href=#perses---the-new-kid-in-the-sandbox aria-label="Heading self-link"></a></h2><p>Perses was of particular interest to us working on monitoring at Gardener. As you may know, Gardener is currently using Plutono, a fork of Grafana&rsquo;s most recent Apache-licensed version. This was introduced in <a href=https://github.com/gardener/gardener/pull/7318>g/g#7318</a> over a year ago and intended as a stop-gap solution. Since then, we have been interested in finding a more sustainable solution, as we&rsquo;re cut off from the enhancements to Grafana. If you&rsquo;ve ever made a change to a dashboard in Gardener, you&rsquo;ll know that there are some pain points in working with Plutono.
Therefore, we were looking forward especially to a talk on a project known as Perses. An open-source dashboard solution that recently joined the CNCF as a sandbox project. Driven by development from Amadeus, Chronosphere, and Red Hat Openshift, Perses already offers a variety of dashboard panels, supports dashboards-as-code workflows, and has data-source plugins for metrics and distributed traces. It also brings tooling to convert Grafana dashboards to Perses, which also works for Plutono!
Adjacent to the Perses project itself is a Perses operator which enables deployment of Perses on Kubernetes using Custom Resource Definitions (CRDs). This is not a new concept - we also use the Prometheus operator in Gardener and would like to use it for Perses as well. One of the areas in which Perses is still lacking is the ability to use it to visualize logs, a key feature of Plutono. With work on the plugin architecture continuing, we hope to be able to work on this feature in the not-too-distant future. We are excited to see how Perses will develop and are looking forward to contributing to the project.</p><h2 id=conclusion>Conclusion<a class=td-heading-self-link href=#conclusion aria-label="Heading self-link"></a></h2><p>PromCon EU 2024 brought together open-source developers, commercial vendors, and users of observability tools. The event provided excellent learning opportunities during many high-quality talks and the chance to network with peers from the community while enjoying a BBQ after the first conference day. The large and healthy Prometheus community shows that open-source observability tools can coexist with commercial solutions.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7da834c7828d84f249e544d60a46c3bc>Gardener at KubeCon + CloudNativeCon North America 2024</h1><div class="td-byline mb-4"><time datetime=2024-10-24 class=text-body-secondary>Thursday, October 24, 2024</time></div><p><a href=https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/>KubeCon + CloudNativeCon NA</a> is just around the corner, taking place this year amidst the stunning backdrop of the Rocky Mountains in Salt Lake City, Utah.</p><p>This year, we&rsquo;re thrilled to announce that the <a href=https://gardener.cloud/>Gardener open-source project</a> will have its own booth at the event.
If you’re passionate about multi-cloud Kubernetes clusters, optimizing for low TCO, built with enterprise-grade features, then Gardener should be on your must-visit list at KubeCon.</p><h2 id=why-you-should-visit-the-gardener-booth>Why You Should Visit the Gardener Booth<a class=td-heading-self-link href=#why-you-should-visit-the-gardener-booth aria-label="Heading self-link"></a></h2><p>The Gardener team will be presenting several features and enhancements that demonstrate the project’s continuous progress. Here are some highlights you&rsquo;ll be able to experience first-hand:</p><p><strong>Extensible and Automated Kubernetes Cluster Management:</strong> Learn how Gardener simplifies and automates Kubernetes cluster management across various infrastructures, allowing for consistent and reliable deployments.</p><p><strong>Hosted Control Planes with Enhanced Security:</strong> Get insights into Gardener’s security features for hosted control planes. Discover how Gardener ensures robust isolation, scalability, and performance while keeping security at the forefront.</p><p><strong>Multi-Cloud Strategy:</strong> Gardener has been built with multi-cloud in mind. Whether you&rsquo;re deploying clusters across AWS, Azure, Google Cloud, or on-prem, Gardener has the flexibility to manage them all seamlessly.</p><p><strong>etcd Cluster Operator:</strong> Explore our advancements with our in-house operator <a href=https://github.com/gardener/etcd-druid>etcd-druid</a>, a key component responsible for managing etcd instances of Gardener&rsquo;s hosted control planes, ensuring the stability and performance of Kubernetes clusters.</p><p><strong>Latest Project Developments and Roadmap:</strong> Stay ahead of the curve by getting a sneak peek at the latest updates, ongoing enhancements, and future plans for Gardener.</p><p><strong>Software Lifecycle Management with Open Component Model (OCM):</strong> Learn how <a href=https://ocm.software/>OCM</a> supports compliant and secure software lifecycle management, and how it can help streamline processes across various projects and environments.</p><h2 id=meet-the-gardener-team>Meet the Gardener Team<a class=td-heading-self-link href=#meet-the-gardener-team aria-label="Heading self-link"></a></h2><p>One of the best parts of KubeCon is the opportunity to meet the community behind the projects. At the Gardener booth, you&rsquo;ll have the chance to connect directly with the maintainers who are driving innovation in Kubernetes cluster management. Whether you&rsquo;re new to the project or already using Gardener in production, our team will be there to answer questions, provide demos, and discuss best practices.</p><h2 id=see-you-in-salt-lake-city>See You in Salt Lake City!<a class=td-heading-self-link href=#see-you-in-salt-lake-city aria-label="Heading self-link"></a></h2><p>Don’t miss the chance to explore how Gardener can streamline your Kubernetes management operations. Visit us at KubeCon + CloudNativeCon North America 2024. Our booth will be located at <strong>R39</strong>. We look forward to seeing you there!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-dcb240a9de77b5b4c133d008e3bfbb90>Innovation Unleashed: A Deep Dive into the 5th Gardener Community Hackathon</h1><div class="td-byline mb-4"><time datetime=2024-05-21 class=text-body-secondary>Tuesday, May 21, 2024</time></div><p><img src=/blog/2024/images/hackathon202405-team.jpg alt="Hackathon 2024/05 Team" title="Hackathon 2024/05 Team"></p><p>The Gardener community recently concluded its <a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md>5th Hackathon</a>, a week-long event that brought together multiple companies to collaborate on common topics of interest. The Hackathon, held at <a href=https://www.schlosshof-info.de/>Schlosshof Freizeitheim</a> in <a href=https://maps.app.goo.gl/28FZXpzZLjgaKNef9>Schelklingen, Germany</a>, was a testament to the power of collective effort and open-source, producing a tremendous number of results in a short time and moving the Gardener project forward with innovative solutions.</p><h2 id=a-week-of-collaboration-and-innovation>A Week of Collaboration and Innovation<a class=td-heading-self-link href=#a-week-of-collaboration-and-innovation aria-label="Heading self-link"></a></h2><p>The Hackathon addressed a wide range of topics, from improving the maturity of the Gardener API to harmonizing development setups and automating additional preparation tasks for Gardener installations. The event also saw the introduction of new resources and configurations, the rewriting of VPN components from Bash to Golang, and the exploration of a Tailscale-based VPN to secure shoot clusters.</p><h3 id=key-achievements>Key Achievements<a class=td-heading-self-link href=#key-achievements aria-label="Heading self-link"></a></h3><ol><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#%EF%B8%8F-oci-helm-release-reference-for-controllerdeployments>🗃️ <strong>OCI Helm Release Reference for ControllerDeployment</strong></a>: The Hackathon introduced the <code>core.gardener.cloud/v1</code> API, which supports OCI repository-based Helm chart references. This innovation reduces operational complexity and enables reusability for other scenarios.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#-gardener-operator-local-development-setup-with-gardenlets>👨🏼‍💻 <strong>Local <code>gardener-operator</code> Development Setup with gardenlet</strong></a>: A new Skaffold configuration was created to harmonize the development setups for Gardener. This configuration deploys <code>gardener-operator</code> and its <code>Garden</code> CRD together with a deployment of <code>gardenlet</code> to register a seed cluster, allowing for a full-fledged Gardener setup.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#-extensions-for-garden-cluster-via-gardener-operator>👨🏻‍🌾 <strong>Extensions for Garden Cluster via <code>gardener-operator</code></strong></a>: The Hackathon focused on automating additional preparation tasks for Gardener installations. The <code>Garden</code> controller was augmented to deploy extensions as part of its reconciliation flow, reducing operational complexity.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#-gardenlet-self-upgrades-for-unmanaged-seeds>🪄 <strong>Gardenlet Self-Upgrades for Unmanaged <code>Seed</code>s</strong></a>: A new <code>Gardenlet</code> resource was introduced, allowing for the specification of deployment values and component configurations. A new controller within <code>gardenlet</code> watches these resources and updates the <code>gardenlet</code>&rsquo;s Helm chart and configuration accordingly, effectively implementing self-upgrades.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#-type-safe-configurability-in-operatingsystemconfig-for-containerd-dns-ntp-etc>🦺 <strong>Type-Safe Configurability in <code>OperatingSystemConfig</code></strong></a>: The Hackathon improved the configurability of the <code>OperatingSystemConfig</code> for <code>containerd</code>, DNS, NTP, etc. The <code>OperatingSystemConfig</code> API was augmented to support <code>containerd</code>-config related use-cases.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#-expose-shoot-api-server-in-tailscale-vpn>👮 <strong>Expose Shoot API Server in Tailscale VPN</strong></a>: The Hackathon explored the use of a <a href=https://tailscale.com/>Tailscale</a>-based VPN to secure shoot clusters. <a href=https://gardener.cloud/docs/guides/administer-shoots/tailscale/>A document was compiled</a> explaining how shoot owners can expose their API server within a Tailscale VPN.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#%EF%B8%8F-rewrite-gardenervpn2-from-bash-to-golang>⌨️ <strong>Rewrite <code>gardener/vpn2</code> from Bash to Golang</strong></a>: The Hackathon improved the VPN components by rewriting them in Golang. All functionality was successfully rewritten, and the pull requests have been opened for <code>gardener/vpn2</code> and the integration into <code>gardener/gardener</code>.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#%EF%B8%8F-pure-ipv6-based-vpn-tunnel>🕳️ <strong>Pure IPv6-Based VPN Tunnel</strong></a>: The Hackathon addressed the restriction of the VPN network CIDR by switching the VPN tunnel to a pure IPv6-based network (follow-up of <a href=https://github.com/gardener/gardener/pull/9597>gardener/gardener#9597</a>). This allows for more flexibility in network design.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#-harmonize-local-vpn-setup-with-real-world-scenario>👐 <strong>Harmonize Local VPN Setup with Real-World Scenario</strong></a>: The Hackathon aimed to align the local VPN setup with real-world scenarios regarding the VPN connection. <code>provider-local</code> was augmented to dynamically create Calico&rsquo;s <code>IPPool</code> resources to emulate the real-world&rsquo;s networking situation.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#-support-cilium-v115-for-ha-shoots>🐝 <strong>Support Cilium <code>v1.15+</code> for HA <code>Shoot</code>s</strong></a>: The Hackathon addressed the issue of <code>Cilium v1.15+</code> not considering <code>StatefulSet</code> labels in <code>NetworkPolicy</code>s. A prototype was developed to make the <code>Service</code> resources for <code>vpn-seed-server</code> <a href=https://kubernetes.io/docs/concepts/services-networking/service/#headless-services>headless</a>.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#-compression-for-managedresource-secrets>🍞 <strong>Compression for <code>ManagedResource</code> <code>Secret</code>s</strong></a>: The Hackathon focused on reducing the size of <code>Secret</code> related to <code>ManagedResource</code>s by leveraging the <a href=https://de.wikipedia.org/wiki/Brotli>Brotli compression algorithm</a>. This reduces network I/O and related costs, improving scalability and reducing load on the ETCD cluster.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#-making-shoot-flux-extension-production-ready>🚛 <strong>Making Shoot Flux Extension Production-Ready</strong></a>: The Hackathon aimed to promote the <a href=https://github.com/stackitcloud/gardener-extension-shoot-flux>Flux extension</a> to &ldquo;production-ready&rdquo; status. Features such as reconciliation sync mode, and the option to provide additional <code>Secret</code> resources were added.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#-move-machine-contoller-manager-provider-local-repository-into-gardenergardener>🧹 <strong>Move <code>machine-controller-manager-provider-local</code> Repository into gardener/gardener</strong></a>: The Hackathon focused on moving the <code>machine-controller-manager-provider-local</code> repository content into the <code>gardener/gardener</code> repository. This simplifies maintenance and development tasks.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#%EF%B8%8F-stop-vendoring-third-party-code-in-os-extensions>🗄️ <strong>Stop Vendoring Third-Party Code in OS Extensions</strong></a>: The Hackathon aimed to avoid vendoring third-party code in the OS extensions. Two out of the four OS extensions have been adapted.</li><li><a href=https://github.com/gardener-community/hackathon/blob/main/2024-05_Schelklingen/README.md#-consider-embedded-files-for-local-image-builds>📦 <strong>Consider Embedded Files for Local Image Builds</strong></a>: The Hackathon addressed the issue that changes to embedded files don&rsquo;t lead to automatic rebuilds of the Gardener images by <code>Skaffold</code> for local development. The related <code>hack</code> script was augmented to detect embedded files and make them part of the list of dependencies.</li></ol><p>Note that a significant portion of the above topics have been built on top of the achievements of <a href=https://github.com/gardener-community/hackathon>previous Hackathons</a>.This continuity and progression of these Hackathons, with each one building on the achievements of the last, is a testament to the power of sustained collaborative effort.</p><h2 id=looking-ahead>Looking Ahead<a class=td-heading-self-link href=#looking-ahead aria-label="Heading self-link"></a></h2><p>As we look towards the future, the Gardener community is already gearing up for the next Hackathon slated for the end of 2024. The anticipation is palpable, as these events have consistently proven to be a hotbed of creativity, innovation, and collaboration. The 5th Gardener Community Hackathon has once again demonstrated the remarkable outcomes that can be achieved when diverse minds unite to work on shared interests. The event has not only yielded an impressive array of results in a short span but has also sparked innovations that promise to propel the Gardener project to new heights. The community eagerly awaits the next Hackathon, ready to tackle new challenges and continue the journey of innovation and growth.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d7e3769c731a9cd873abfc85de9685a5>Gardener's Registry Cache Extension: Another Cost Saving Win and More</h1><div class="td-byline mb-4"><time datetime=2024-04-22 class=text-body-secondary>Monday, April 22, 2024</time></div><h2 id=use-cases>Use Cases<a class=td-heading-self-link href=#use-cases aria-label="Heading self-link"></a></h2><p>In Kubernetes, on every Node the container runtime daemon pulls the container images that are configured in the Pods&rsquo; specifications running on the corresponding Node. Although these container images are cached on the Node&rsquo;s file system after the initial pull operation, there are imperfections with this setup.</p><p>New Nodes are often created due to events such as auto-scaling (scale up), rolling updates, or replacements of unhealthy Nodes. A new Node would need to pull the images running on it from the container registry because the Node&rsquo;s cache is initially empty. Pulling an image from a registry incurs network traffic and registry costs.</p><p>To reduce network traffic and registry costs for your Shoot cluster, it is recommended to enable the Gardener&rsquo;s Registry Cache extension to run a registry as pull-through cache in the Shoot cluster.</p><p>The use cases of using a pull-through cache are not only limited to cost savings. Using a pull-through cache makes the Kubernetes cluster resilient to failures with the upstream registry - outages, failures due to rate limiting.</p><h2 id=solution>Solution<a class=td-heading-self-link href=#solution aria-label="Heading self-link"></a></h2><p>Gardener&rsquo;s Registry Cache extension deploys and manages a pull-through cache registry in the Shoot cluster.</p><p>A pull-through cache registry is a registry that caches container images in its storage. The first time when an image is requested from the pull-through cache, it pulls the image from the upstream registry, returns it to the client, and stores it in its local storage. On subsequent requests for the same image, the pull-through cache serves the image from its storage, avoiding network traffic to the upstream registry.</p><p>Imagine that you have a DaemonSet in your Kubernetes cluster. In a cluster without a pull-through cache, every Node must pull the same container image from the upstream registry. In a cluster with a pull-through cache, the image is pulled once from the upstream registry and served later for all Nodes.</p><p><img src=/blog/2024/images/shoot-cluster-with-registry-cache.png alt="A Shoot cluster setup with a registry cache for Docker Hub (docker.io)"></p><p style=text-align:center;font-style:italic>A Shoot cluster setup with a registry cache for Docker Hub (docker.io).</p><h2 id=cost-considerations>Cost Considerations<a class=td-heading-self-link href=#cost-considerations aria-label="Heading self-link"></a></h2><p>An image pull represents ingress traffic for a virtual machine (data is entering to the system from outside) and egress traffic for the upstream registry (data is leaving the system).</p><p>Ingress traffic from the internet to a virtual machine is free of charge on AWS, GCP, and Azure. However, the cloud providers charge NAT gateway costs for inbound and outbound data processed by the NAT gateway based on the processed data volume (per GB). The container registry offerings on the cloud providers charge for egress traffic - again, based on the data volume (per GB).</p><p>Having all of this in mind, the Registry Cache extension reduces NAT gateway costs for the Shoot cluster and container registry costs.</p><h2 id=try-it-out>Try It Out!<a class=td-heading-self-link href=#try-it-out aria-label="Heading self-link"></a></h2><p>We would also like to encourage you to try it! As a Gardener user, you can also reduce your infrastructure costs and increase resilience by enabling the Registry Cache for your Shoot clusters. The Registry Cache extension is a great fit for long running Shoot clusters that have high image pull rate.</p><p>For more information, refer to the <a href=/docs/extensions/others/gardener-extension-registry-cache/>Registry Cache extension documentation</a>!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-65c2d2064014d4450b7d6dddfd52b7d4>SpinKube on Gardener - Serverless WASM on Kubernetes</h1><div class="td-byline mb-4"><time datetime=2024-04-18 class=text-body-secondary>Thursday, April 18, 2024</time></div><p>With the rising popularity of <a href=https://webassembly.org/>WebAssembly (WASM)</a> and <a href=https://wasi.dev/>WebAssembly System Interface (WASI)</a> comes a variety of integration possibilities. WASM is now not only suitable for the browser, but can be also utilized for running workloads on the server. In this post we will explore how you can get started writing serverless applications powered by <a href=https://www.spinkube.dev/>SpinKube</a> on a Gardener Shoot cluster. This post is inspired by a similar tutorial that goes through the steps of <a href=https://www.spinkube.dev/docs/spin-operator/tutorials/deploy-on-azure-kubernetes-service/>Deploying the Spin Operator on Azure Kubernetes Service</a>. Keep in mind that this post does not aim to define a production environment. It is meant to show that Gardener Shoot clusters are able to run WebAssembly workloads, giving users the chance to experiment and explore this cutting-edge technology.</p><h2 id=prerequisites>Prerequisites<a class=td-heading-self-link href=#prerequisites aria-label="Heading self-link"></a></h2><ul><li><a href=https://kubernetes.io/docs/reference/kubectl/>kubectl</a> - the Kubernetes command line tool</li><li><a href=https://helm.sh/>helm</a> - the package manager for Kubernetes</li><li>A running Gardener Shoot cluster</li></ul><h2 id=gardener-shoot-cluster>Gardener Shoot Cluster<a class=td-heading-self-link href=#gardener-shoot-cluster aria-label="Heading self-link"></a></h2><p>For this showcase I am using a Gardener Shoot cluster on AWS infrastructure with nodes powered by <a href=https://github.com/gardenlinux/gardenlinux>Garden Linux</a>, although the steps should be applicable for other infrastructures as well, since Gardener aims to provide a homogenous Kubernetes experience.</p><p>As a prerequisite for next steps, verify that you have access to your Gardener Shoot cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Verify the access to the Gardener Shoot cluster</span>
</span></span><span style=display:flex><span>kubectl get ns
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>NAME              STATUS   AGE
</span></span><span style=display:flex><span>default           Active   4m1s
</span></span><span style=display:flex><span>kube-node-lease   Active   4m1s
</span></span><span style=display:flex><span>kube-public       Active   4m1s
</span></span><span style=display:flex><span>kube-system       Active   4m1s
</span></span></code></pre></div><p>If you are having troubles accessing the Gardener Shoot cluster, please consult the <a href=https://gardener.cloud/docs/gardener/shoot_access/>Accessing Shoot Clusters</a> documentation page.</p><h2 id=deploy-the-spin-operator>Deploy the Spin Operator<a class=td-heading-self-link href=#deploy-the-spin-operator aria-label="Heading self-link"></a></h2><p>As a first step, we will install the Spin Operator Custom Resource Definitions and the Runtime Class needed by <code>wasmtime-spin-v2</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Install Spin Operator CRDs</span>
</span></span><span style=display:flex><span>kubectl apply -f https://github.com/spinkube/spin-operator/releases/download/v0.1.0/spin-operator.crds.yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Install the Runtime Class</span>
</span></span><span style=display:flex><span>kubectl apply -f https://github.com/spinkube/spin-operator/releases/download/v0.1.0/spin-operator.runtime-class.yaml
</span></span></code></pre></div><p>Next, we will install <a href=https://github.com/cert-manager/cert-manager>cert-manager</a>, which is required for provisioning TLS certificates used by the admission webhook of the Spin Operator. If you face issues installing <code>cert-manager</code>, please consult the <a href=https://cert-manager.io/docs/installation/helm/>cert-manager installation</a> documentation.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Add and update the Jetstack repository</span>
</span></span><span style=display:flex><span>helm repo add jetstack https://charts.jetstack.io
</span></span><span style=display:flex><span>helm repo update
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Install the cert-manager chart alongside with CRDs needed by cert-manager</span>
</span></span><span style=display:flex><span>helm install <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  cert-manager jetstack/cert-manager <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --namespace cert-manager <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --create-namespace <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --version v1.14.4 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --set installCRDs=true
</span></span></code></pre></div><p>In order to install the <code>containerd-wasm-shim</code> on the Kubernetes nodes we will use the <a href=https://kwasm.sh/>kwasm-operator</a>. There is also a successor of <code>kwasm-operator</code> - <a href=https://github.com/spinkube/runtime-class-manager>runtime-class-manager</a> which aims to address some of the limitations of <code>kwasm-operator</code> and provide a production grade implementation for deploying <code>containerd</code> shims on Kubernetes nodes. Since <code>kwasm-operator</code> is easier to install, for the purpose of this post we will use it instead of the <code>runtime-class-manager</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Add the kwasm helm repository</span>
</span></span><span style=display:flex><span>helm repo add kwasm http://kwasm.sh/kwasm-operator/
</span></span><span style=display:flex><span>helm repo update
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Install KWasm operator</span>
</span></span><span style=display:flex><span>helm install <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  kwasm-operator kwasm/kwasm-operator <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --namespace kwasm <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --create-namespace <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --set kwasmOperator.installerImage=ghcr.io/spinkube/containerd-shim-spin/node-installer:v0.13.1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Annotate all nodes in the cluster so kwasm can select them and provision the required containerd shim</span>
</span></span><span style=display:flex><span>kubectl annotate node --all kwasm.sh/kwasm-node=true
</span></span></code></pre></div><p>We can see that a pod has started and completed in the <code>kwasm</code> namespace.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n kwasm get pod
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>NAME                                                              READY   STATUS      RESTARTS   AGE
</span></span><span style=display:flex><span>ip-10-180-7-60.eu-west-1.compute.internal-provision-kwasm-qhr8r   0/1     Completed   0          8s
</span></span><span style=display:flex><span>kwasm-operator-6c76c5f94b-8zt4s                                   1/1     Running     0          15s
</span></span></code></pre></div><p>The logs of the <code>kwasm-operator</code> also indicate that the node was provisioned with the required shim.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n kwasm logs kwasm-operator-6c76c5f94b-8zt4s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>{<span style=color:#a31515>&#34;level&#34;</span>:<span style=color:#a31515>&#34;info&#34;</span>,<span style=color:#a31515>&#34;node&#34;</span>:<span style=color:#a31515>&#34;ip-10-180-7-60.eu-west-1.compute.internal&#34;</span>,<span style=color:#a31515>&#34;time&#34;</span>:<span style=color:#a31515>&#34;2024-04-18T05:44:25Z&#34;</span>,<span style=color:#a31515>&#34;message&#34;</span>:<span style=color:#a31515>&#34;Trying to Deploy on ip-10-180-7-60.eu-west-1.compute.internal&#34;</span>}
</span></span><span style=display:flex><span>{<span style=color:#a31515>&#34;level&#34;</span>:<span style=color:#a31515>&#34;info&#34;</span>,<span style=color:#a31515>&#34;time&#34;</span>:<span style=color:#a31515>&#34;2024-04-18T05:44:31Z&#34;</span>,<span style=color:#a31515>&#34;message&#34;</span>:<span style=color:#a31515>&#34;Job ip-10-180-7-60.eu-west-1.compute.internal-provision-kwasm is still Ongoing&#34;</span>}
</span></span><span style=display:flex><span>{<span style=color:#a31515>&#34;level&#34;</span>:<span style=color:#a31515>&#34;info&#34;</span>,<span style=color:#a31515>&#34;time&#34;</span>:<span style=color:#a31515>&#34;2024-04-18T05:44:31Z&#34;</span>,<span style=color:#a31515>&#34;message&#34;</span>:<span style=color:#a31515>&#34;Job ip-10-180-7-60.eu-west-1.compute.internal-provision-kwasm is Completed. Happy WASMing&#34;</span>}
</span></span></code></pre></div><p>Finally we can deploy the <code>spin-operator</code> alongside with a <a href=https://www.spinkube.dev/docs/glossary/#spin-app-executor-crd>shim-executor</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>helm install spin-operator <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --namespace spin-operator <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --create-namespace <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --version 0.1.0 <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  --wait <span style=color:#a31515>\
</span></span></span><span style=display:flex><span><span style=color:#a31515></span>  oci://ghcr.io/spinkube/charts/spin-operator
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubectl apply -f https://github.com/spinkube/spin-operator/releases/download/v0.1.0/spin-operator.shim-executor.yaml
</span></span></code></pre></div><h2 id=deploy-a-spin-app>Deploy a Spin App<a class=td-heading-self-link href=#deploy-a-spin-app aria-label="Heading self-link"></a></h2><p>Let&rsquo;s deploy a sample Spin application using the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f https://raw.githubusercontent.com/spinkube/spin-operator/main/config/samples/simple.yaml
</span></span></code></pre></div><p>After the CRD has been picked up by the <code>spin-operator</code>, a pod will be created running the sample application. Let&rsquo;s explore its logs.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl logs simple-spinapp-56687588d9-nbrtq
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Serving http://0.0.0.0:80
</span></span><span style=display:flex><span>Available Routes:
</span></span><span style=display:flex><span>  hello: http://0.0.0.0:80/hello
</span></span><span style=display:flex><span>  go-hello: http://0.0.0.0:80/go-hello
</span></span></code></pre></div><p>We can see the available routes served by the application. Let&rsquo;s port forward to the application <code>service</code> and test them out.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl port-forward services/simple-spinapp 8000:80
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Forwarding from 127.0.0.1:8000 -&gt; 80
</span></span><span style=display:flex><span>Forwarding from [::1]:8000 -&gt; 80
</span></span></code></pre></div><p>In another terminal, we can verify that the application returns a response.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl http://localhost:8000/hello
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Hello world from Spin!%
</span></span></code></pre></div><p>This sets the ground for further experimentation and testing. What the <code>SpinApp</code> CRD provides as capabilities and API can be explored through the <a href=https://www.spinkube.dev/docs/reference/spin-app/>SpinApp CRD reference</a>.</p><h2 id=cleanup>Cleanup<a class=td-heading-self-link href=#cleanup aria-label="Heading self-link"></a></h2><p>Let&rsquo;s clean all deployed resources so far.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># Delete the spin app and its executor</span>
</span></span><span style=display:flex><span>kubectl delete spinapp simple-spinapp
</span></span><span style=display:flex><span>kubectl delete spinappexecutors.core.spinoperator.dev containerd-shim-spin
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Uninstall the spin-operator chart</span>
</span></span><span style=display:flex><span>helm -n spin-operator uninstall spin-operator
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Remove the kwasm.sh/kwasm-node annotation from nodes</span>
</span></span><span style=display:flex><span>kubectl annotate node --all kwasm.sh/kwasm-node-
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Uninstall the kwasm-operator chart</span>
</span></span><span style=display:flex><span>helm -n kwasm uninstall kwasm-operator
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Uninstall the cert-manager chart</span>
</span></span><span style=display:flex><span>helm -n cert-manager uninstall cert-manager
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Delete the runtime class and SpinApp CRDs</span>
</span></span><span style=display:flex><span>kubectl delete runtimeclass wasmtime-spin-v2
</span></span><span style=display:flex><span>kubectl delete crd spinappexecutors.core.spinoperator.dev
</span></span><span style=display:flex><span>kubectl delete crd spinapps.core.spinoperator.dev
</span></span></code></pre></div><h2 id=conclusion>Conclusion<a class=td-heading-self-link href=#conclusion aria-label="Heading self-link"></a></h2><p>In my opinion, WASM on the server is here to stay. Communities are expressing more and more interest in integrating Kubernetes with WASM workloads. As shown Gardener clusters are perfectly capable of supporting this use case. This setup is a great way to start exploring the capabilities that WASM can bring to the server. As stated in the introduction, bear in mind that this post does not define a production environment, but is rather meant to define a playground suitable for exploring and trying out ideas.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-83da9d61cf34544484b5153e0deef706>KubeCon / CloudNativeCon Europe 2024 Highlights</h1><div class="td-byline mb-4"><time datetime=2024-04-05 class=text-body-secondary>Friday, April 05, 2024</time></div><p><img src=/blog/2024/images/kubecon-eu2024.png alt="KubeCon EU 2024 Keynote Room" title="KubeCon EU 2024 Keynote Room"></p><p>KubeCon + CloudNativeCon Europe 2024, recently held in Paris, was a testament to the robustness of the open-source community and its pivotal role in driving advancements in AI and cloud-native technologies. With a record attendance of over +12,000 participants, the conference underscored the ubiquity of cloud-native architectures and the business opportunities they provide.</p><h2 id=ai-everywhere>AI Everywhere<a class=td-heading-self-link href=#ai-everywhere aria-label="Heading self-link"></a></h2><p>LLMs and GenAI took center stage at the event, with discussions on challenges such as security, data management, and energy consumption. A popular quote stated, &ldquo;If #inference is the new web application, #kubernetes is the new web server&rdquo;. The conference emphasized the need for more open data models for AI to democratize the technology. Cloud-native platforms offer advantages for AI innovation, such as packaging models and dependencies as Docker packages and enhancing resource management for proper model execution. The community is exploring AI workload management, including using CPUs for inferencing and preprocessing data before handing it over to GPUs. CNCF took the initiative and put together an <a href=https://www.cncf.io/reports/cloud-native-artificial-intelligence-whitepaper/>AI whitepaper</a> outlining the apparent synergy between cloud-native technologies and AI.</p><h2 id=cluster-autopilot>Cluster Autopilot<a class=td-heading-self-link href=#cluster-autopilot aria-label="Heading self-link"></a></h2><p>The conference showcased popular projects in the cloud-native ecosystem, including Kubernetes, Istio, and OpenTelemetry. Kubernetes was highlighted as a platform for running massive AI workloads. The UXL Foundation aims to enable multi-vendor AI workloads on Kubernetes, allowing developers to move AI workloads without being locked into a specific infrastructure. Every vendor we interacted with has assembled an AI-powered chatbot, which performs various functions – from assessing cluster health through analyzing cost efficiency and proposing workload optimizations to troubleshooting issues and alerting for potential challenges with upcoming Kubernetes version upgrades. Sysdig went even further with a chatbot, which answers the popular question, &ldquo;Do any of my products have critical CVEs in production?&rdquo; and analyzes workloads&rsquo; structure and configuration. Some chatbots leveraged the <a href=https://k8sgpt.ai/>k8sgpt project</a>, which joined the CNCF sandbox earlier this year.</p><h2 id=sophisticated-fleet-management>Sophisticated Fleet Management<a class=td-heading-self-link href=#sophisticated-fleet-management aria-label="Heading self-link"></a></h2><p>The ecosystem showcased maturity in observability, platform engineering, security, and optimization, which will help operationalize AI workloads. Data demands and costs were also in focus, touching on data observability and cloud-cost management. Cloud-native technologies, also going beyond Kubernetes, are expected to play a crucial role in managing the increasing volume of data and scaling AI. Google showcased fleet management in their Google Hosted Cloud offering (ex-Anthos). It allows for defining teams and policies at the fleet level, later applied to all the Kubernetes clusters in the fleet, irrespective of the infrastructure they run on (GCP and beyond).</p><h2 id=wasm-everywhere>WASM Everywhere<a class=td-heading-self-link href=#wasm-everywhere aria-label="Heading self-link"></a></h2><p>The conference also highlighted the growing interest in WebAssembly (WASM) as a portable binary instruction format for executable programs and its integration with Kubernetes and other functions. The topic here started with a dedicated WASM pre-conference day, the sessions of which are available in the <a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2MQteKoXxICTWiUdZYEw6RI">following playlist</a>. WASM is positioned as the smoother approach to software distribution and modularity, providing more lightweight runtime execution options and an easier way for app developers to enter.</p><h2 id=rust-on-the-rise>Rust on the Rise<a class=td-heading-self-link href=#rust-on-the-rise aria-label="Heading self-link"></a></h2><p>Several talks were promoting Rust as an ideal <a href=https://youtu.be/2q3RLffSvEc>programming language for cloud-native workloads</a>. It was even promoted as suitable for <a href=https://youtu.be/rXS-3hFYVjc>writing Kubernetes controllers</a>.</p><h2 id=internal-developer-platforms>Internal Developer Platforms<a class=td-heading-self-link href=#internal-developer-platforms aria-label="Heading self-link"></a></h2><p>The event showcased the importance of Internal Developer Platforms (IDPs), both commercial and open-source, in facilitating the development process across all types of organizations – from Allianz to Mercedes. <a href=https://backstage.io/>Backstage</a> leads the pack by a large margin, with all relevant sessions being full or at capacity. Much effort goes into the modularization of Backstage, which was also a notable highlight at the conference.</p><h2 id=sustainability>Sustainability<a class=td-heading-self-link href=#sustainability aria-label="Heading self-link"></a></h2><p>Sustainability was a key theme, with discussions on the role of cloud-native technologies in promoting green practices. The <a href=https://github.com/kubecost>KubeCost application</a> folks put a lot of effort into emphasizing the large amount of wasted money, which hyperscalers benefit from. In parallel – the <a href=https://kube-green.dev/>kube-green project</a> emphasized optimizing your cluster footprint to minimize CO2 emissions. The conference also highlighted the importance of open source in creating a level playing field for multiple players to compete, fostering diverse participation, and solving global challenges.</p><h2 id=customer-stories>Customer Stories<a class=td-heading-self-link href=#customer-stories aria-label="Heading self-link"></a></h2><p>In contrast to the Chicago KubeCon in 2023, the one in Paris outlined multiple case studies, best practices, and reference scenarios. Many enterprises and their IT teams were well represented at KubeCon - regarding sessions, sponsorships, and participation. These companies strive to excel forward, reaping the efficiency and flexibility benefits cloud-native architectures provide.
We came across multiple companies using <a href=https://gardener.cloud/>Gardener</a> as their Kubernetes management underlay – including FUGA Cloud, STACKIT, and metal-stack Cloud. We eagerly anticipate more companies embracing Gardener at future events. The consistent feedback from these companies has been overwhelmingly positive—they absolutely love using Gardener and our shared excitement grows as the community thrives!</p><h2 id=notable-talks>Notable Talks<a class=td-heading-self-link href=#notable-talks aria-label="Heading self-link"></a></h2><p>Notable talks from leaders in the cloud-native world, including Solomon Hykes, Bob Wise, and representatives from KCP for Platforms and the United Nations, provided valuable insights into the future of AI and cloud-native technologies. All the talks are now uploaded to YouTube in the following <a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0">playlist</a>. Those do not include the various pre-conference days, available as <a href="https://www.youtube.com/@cncf/playlists?view=1&amp;sort=dd&amp;flow=grid">separate playlists</a> by CNCF.</p><h2 id=in-conclusion>In Conclusion&mldr;<a class=td-heading-self-link href=#in-conclusion aria-label="Heading self-link"></a></h2><p>In conclusion, KubeCon 2024 showcased the intersection of AI and cloud-native technologies, the open-source community&rsquo;s growth, and the cloud-native ecosystem&rsquo;s maturity. Many enterprises are actively engaged there, innovating, trying, and growing their internal expertise. They&rsquo;re using KubeCon as a recruiting event, expanding their internal talent pool and taking more of their internal operations and processes into their own hands. The event served as a platform for global collaboration, cross-company alignments, innovation, and the exchange of ideas, setting the stage for the future of cloud-native computing.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9974ffd96185d1107b35d87ea46b9b21>2023</h1><div class="td-byline mb-4"><time datetime=2023-03-27 class=text-body-secondary>Monday, March 27, 2023</time></div></div><div class=td-content><h1 id=pg-1ca23b5c125c3eb9ff43239a2d9eeba4>High Availability and Zone Outage Toleration</h1><div class="td-byline mb-4"><time datetime=2023-03-27 class=text-body-secondary>Monday, March 27, 2023</time></div><p>Developing highly available workload that can tolerate a zone outage is no trivial task. In this blog, we will explore various recommendations to get closer to that goal. While many recommendations are general enough, the examples are specific in how to achieve this in a <a href=https://gardener.cloud>Gardener</a>-managed cluster and where/how to tweak the different control plane components. If you do not use Gardener, it may be still a worthwhile read as most settings can be influenced with most of the Kubernetes providers.</p><p>First however, what is a zone outage? It sounds like a clear-cut &ldquo;thing&rdquo;, but it isn&rsquo;t. There are many things that can go haywire. Here are some examples:</p><ul><li>Elevated cloud provider API error rates for individual or multiple services</li><li>Network bandwidth reduced or latency increased, usually also effecting storage sub systems as they are network attached</li><li>No networking at all, no DNS, machines shutting down or restarting, &mldr;</li><li>Functional issues, of either the entire service (e.g., all block device operations) or only parts of it (e.g., LB listener registration)</li><li>All services down, temporarily or permanently (the proverbial burning down data center &#x1f525;)</li></ul><p>This and everything in between make it hard to prepare for such events, but you can still do a lot. The most important recommendation is to not target specific issues exclusively - tomorrow another service will fail in an unanticipated way. Also, focus more on <a href=https://research.google/pubs/pub50828>meaningful availability</a> than on internal signals (useful, but not as relevant as the former). Always prefer automation over manual intervention (e.g., leader election is a pretty robust mechanism, auto-scaling may be required as well, etc.).</p><p>Also remember that HA is costly - you need to balance it against the cost of an outage as silly as this may sound, e.g., running all this excess capacity &ldquo;just in case&rdquo; vs. &ldquo;going down&rdquo; vs. a risk-based approach in between where you have means that will kick in, but they are not guaranteed to work (e.g., if the cloud provider is out of resource capacity). Maybe some of your components must run at the highest possible availability level, but others not - that&rsquo;s a decision only you can make.</p><h2 id=control-plane>Control Plane<a class=td-heading-self-link href=#control-plane aria-label="Heading self-link"></a></h2><p>The Kubernetes cluster control plane is managed by Gardener (as pods in separate infrastructure clusters to which you have no direct access) and can be set up with no failure tolerance (control plane pods will be recreated best-effort when resources are available) or one of the <a href=/docs/guides/high-availability/control-plane/>failure tolerance types <code>node</code> or <code>zone</code></a>.</p><p>Strictly speaking, static workload does not depend on the (high) availability of the control plane, but static workload doesn&rsquo;t rhyme with Cloud and Kubernetes and also means, that when you possibly need it the most, e.g., during a zone outage, critical self-healing or auto-scaling functionality won&rsquo;t be available to you and your workload, if your control plane is down as well. That&rsquo;s why it&rsquo;s generally recommended to use the failure tolerance type <code>zone</code> for the control planes of productive clusters, at least in all regions that have 3+ zones. Regions that have only 1 or 2 zones don&rsquo;t support the failure tolerance type <code>zone</code> and then your second best option is the failure tolerance type <code>node</code>, which means a zone outage can still take down your control plane, but individual node outages won&rsquo;t.</p><p>In the <code>shoot</code> resource it&rsquo;s merely only this what you need to add:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  controlPlane:
</span></span><span style=display:flex><span>    highAvailability:
</span></span><span style=display:flex><span>      failureTolerance:
</span></span><span style=display:flex><span>        type: zone <span style=color:green># valid values are `node` and `zone` (only available if your control plane resides in a region with 3+ zones)</span>
</span></span></code></pre></div><p>This setting will scale out all control plane components for a Gardener cluster as necessary, so that no single zone outage can take down the control plane for longer than just a few seconds for the fail-over to take place (e.g., lease expiration and new leader election or readiness probe failure and endpoint removal). Components run highly available in either active-active (servers) or active-passive (controllers) mode at all times, the persistence (ETCD), which is consensus-based, will tolerate the loss of one zone and still maintain quorum and therefore remain operational. These are all patterns that we will revisit down below also for your own workload.</p><h2 id=worker-pools>Worker Pools<a class=td-heading-self-link href=#worker-pools aria-label="Heading self-link"></a></h2><p>Now that you have configured your Kubernetes cluster control plane in HA, i.e. spread it across multiple zones, you need to do the same for your own workload, but in order to do so, you need to spread your nodes across multiple zones first.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: ...
</span></span><span style=display:flex><span>      minimum: 6
</span></span><span style=display:flex><span>      maximum: 60
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - ...
</span></span></code></pre></div><p>Prefer regions with at least 2, better 3+ zones and list the zones in the <code>zones</code> section for each of your worker pools. Whether you need 2 or 3 zones at a minimum depends on your fail-over concept:</p><ul><li>Consensus-based software components (like ETCD) depend on maintaining a quorum of <code>(n/2)+1</code>, so you need at least 3 zones to tolerate the outage of 1 zone.</li><li>Primary/Secondary-based software components need just 2 zones to tolerate the outage of 1 zone.</li><li>Then there are software components that can scale out horizontally. They are probably fine with 2 zones, but you also need to think about the load-shift and that the remaining zone must then pick up the work of the unhealthy zone. With 2 zones, the remaining zone must cope with an increase of 100% load. With 3 zones, the remaining zones must only cope with an increase of 50% load (per zone).</li></ul><p>In general, the question is also whether you have the fail-over capacity already up and running or not. If not, i.e. you depend on re-scheduling to a healthy zone or auto-scaling, be aware that during a zone outage, you will see a resource crunch in the healthy zones. If you have no automation, i.e. only human operators (a.k.a. &ldquo;red button approach&rdquo;), you probably will not get the machines you need and even with automation, it may be tricky. But holding the capacity available at all times is costly. In the end, that&rsquo;s a decision only you can make. If you made that decision, please adapt the <code>minimum</code> and <code>maximum</code> settings for your worker pools accordingly.</p><p>Also, consider fall-back worker pools (with different/alternative machine types) and <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders>cluster autoscaler expanders</a> using a <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/expander/priority/readme.md>priority-based strategy</a>.</p><p>Gardener-managed clusters deploy the <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>cluster autoscaler</a> or CA for short and you can <a href=/docs/gardener/api-reference/core/#clusterautoscaler>tweak the general CA knobs</a> for Gardener-managed clusters like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    clusterAutoscaler:
</span></span><span style=display:flex><span>      expander: <span style=color:#a31515>&#34;least-waste&#34;</span>
</span></span><span style=display:flex><span>      scanInterval: 10s
</span></span><span style=display:flex><span>      scaleDownDelayAfterAdd: 60m
</span></span><span style=display:flex><span>      scaleDownDelayAfterDelete: 0s
</span></span><span style=display:flex><span>      scaleDownDelayAfterFailure: 3m
</span></span><span style=display:flex><span>      scaleDownUnneededTime: 30m
</span></span><span style=display:flex><span>      scaleDownUtilizationThreshold: 0.5
</span></span></code></pre></div><p>If you want to be ready for a sudden spike or have some buffer in general, <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-can-i-configure-overprovisioning-with-cluster-autoscaler>over-provision nodes by means of &ldquo;placeholder&rdquo; pods</a> with <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption>low priority</a> and appropriate resource requests. This way, they will demand nodes to be provisioned for them, but if any pod comes up with a regular/higher priority, the low priority pods will be evicted to make space for the more important ones. Strictly speaking, this is not related to HA, but it may be important to keep this in mind as you generally want critical components to be rescheduled as fast as possible and if there is no node available, it may take 3 minutes or longer to do so (depending on the cloud provider). Besides, not only zones can fail, but also individual nodes.</p><h2 id=replicas-horizontal-scaling>Replicas (Horizontal Scaling)<a class=td-heading-self-link href=#replicas-horizontal-scaling aria-label="Heading self-link"></a></h2><p>Now let&rsquo;s talk about your workload. In most cases, this will mean to run multiple replicas. If you cannot do that (a.k.a. you have a singleton), that&rsquo;s a bad situation to be in. Maybe you can run a spare (secondary) as backup? If you cannot, you depend on quick detection and rescheduling of your singleton (more on that below).</p><p>Obviously, things get messier with persistence. If you have persistence, you should ideally replicate your data, i.e. let your spare (secondary) &ldquo;follow&rdquo; your main (primary). If your software doesn&rsquo;t support that, you have to deploy other means, e.g., <a href=https://kubernetes.io/docs/concepts/storage/volume-snapshots>volume snapshotting</a> or side-backups (specific to the software you deploy; keep the backups regional, so that you can switch to another zone at all times). If you have to do those, your HA scenario becomes more a DR scenario and terms like RPO and RTO become relevant to you:</p><ul><li><strong>Recovery Point Objective (RPO)</strong>: Potential data loss, i.e. how much data will you lose at most (time between backups)</li><li><strong>Recovery Time Objective (RTO)</strong>: Time until recovery, i.e. how long does it take you to be operational again (time to restore)</li></ul><p>Also, keep in mind that your persistent volumes are usually zonal, i.e. once you have a volume in one zone, it&rsquo;s bound to that zone and you cannot get up your pod in another zone w/o first recreating the volume yourself (Kubernetes won&rsquo;t help you here directly).</p><p>Anyway, best avoid that, if you can (from technical and cost perspective). The best solution (and also the most costly one) is to run multiple replicas in multiple zones and keep your data replicated at all times, so that your RPO is always 0 (best). That&rsquo;s what we do for Gardener-managed cluster HA control planes (ETCD) as any data loss may be disastrous and lead to orphaned resources (in addition, we deploy side cars that do side-backups for disaster recovery, with full and incremental snapshots with an RPO of 5m).</p><p>So, how to run with multiple replicas? That&rsquo;s the easiest part in Kubernetes and the two most important resources, <code>Deployments</code> and <code>StatefulSet</code>, support that out of the box:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment | StatefulSet
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: ...
</span></span></code></pre></div><p>The problem comes with the number of replicas. It&rsquo;s easy only if the number is static, e.g., 2 for active-active/passive or 3 for consensus-based software components, but what with software components that can scale out horizontally? Here you usually do not set the number of replicas statically, but make use of the <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale>horizontal pod autoscaler</a> or HPA for short (built-in; part of the kube-controller-manager). There are also other options like the <a href=https://github.com/kubernetes-sigs/cluster-proportional-autoscaler>cluster proportional autoscaler</a>, but while the former works based on metrics, the latter is more a guestimate approach that derives the number of replicas from the number of nodes/cores in a cluster. Sometimes useful, but often blind to the actual demand.</p><p>So, HPA it is then for most of the cases. However, what is the resource (e.g., CPU or memory) that drives the number of desired replicas? Again, this is up to you, but not always are CPU or memory the best choices. In some cases, <a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#scaling-on-custom-metrics>custom metrics</a> may be more appropriate, e.g., requests per second (it was also for us).</p><p>You will have to create specific <code>HorizontalPodAutoscaler</code> resources for your scale target and can <a href=/docs/gardener/api-reference/core/#horizontalpodautoscalerconfig>tweak the general HPA knobs</a> for Gardener-managed clusters like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeControllerManager:
</span></span><span style=display:flex><span>      horizontalPodAutoscaler:
</span></span><span style=display:flex><span>        syncPeriod: 15s
</span></span><span style=display:flex><span>        tolerance: 0.1
</span></span><span style=display:flex><span>        downscaleStabilization: 5m0s
</span></span><span style=display:flex><span>        initialReadinessDelay: 30s
</span></span><span style=display:flex><span>        cpuInitializationPeriod: 5m0s
</span></span></code></pre></div><h2 id=resources-vertical-scaling>Resources (Vertical Scaling)<a class=td-heading-self-link href=#resources-vertical-scaling aria-label="Heading self-link"></a></h2><p>While it is important to set a sufficient number of replicas, it is also important to give the pods sufficient resources (CPU and memory). This is especially true when you think about HA. When a zone goes down, you might need to get up replacement pods, if you don&rsquo;t have them running already to take over the load from the impacted zone. Likewise, e.g., with active-active software components, you can expect the remaining pods to receive more load. If you cannot scale them out horizontally to serve the load, you will probably need to scale them out (or rather up) vertically. This is done by the <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler>vertical pod autoscaler</a> or VPA for short (not built-in; part of the <a href=https://github.com/kubernetes/autoscaler>kubernetes/autoscaler</a> repository).</p><p>A few caveats though:</p><ul><li>You cannot use HPA and VPA on the same metrics as they would influence each other, which would lead to pod trashing (more replicas require fewer resources; fewer resources require more replicas)</li><li>Scaling horizontally doesn&rsquo;t cause downtimes (at least not when out-scaling and only one replica is affected when in-scaling), but scaling vertically does (if the pod runs OOM anyway, but also when new recommendations are applied, resource requests for existing pods may be changed, which causes the pods to be rescheduled). Although the discussion is going on for a very long time now, that is still not supported in-place yet (see <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/1287-in-place-update-pod-resources/README.md>KEP 1287</a>, <a href=https://github.com/kubernetes/kubernetes/pull/102884>implementation in Kubernetes</a>, <a href=https://github.com/kubernetes/autoscaler/issues/4016>implementation in VPA</a>).</li></ul><p>VPA is a useful tool and Gardener-managed clusters deploy a VPA by default for you (HPA is supported anyway as it&rsquo;s built into the kube-controller-manager). You will have to create specific <code>VerticalPodAutoscaler</code> resources for your scale target and can <a href=/docs/gardener/api-reference/core/#verticalpodautoscaler>tweak the general VPA knobs</a> for Gardener-managed clusters like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    verticalPodAutoscaler:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      evictAfterOOMThreshold: 10m0s
</span></span><span style=display:flex><span>      evictionRateBurst: 1
</span></span><span style=display:flex><span>      evictionRateLimit: -1
</span></span><span style=display:flex><span>      evictionTolerance: 0.5
</span></span><span style=display:flex><span>      recommendationMarginFraction: 0.15
</span></span><span style=display:flex><span>      updaterInterval: 1m0s
</span></span><span style=display:flex><span>      recommenderInterval: 1m0s
</span></span></code></pre></div><p>While horizontal pod autoscaling is relatively straight-forward, it takes a long time to master vertical pod autoscaling. We saw <a href=https://github.com/kubernetes/autoscaler/issues/4498>performance issues</a>, hard-coded behavior (on OOM, memory is bumped by +20% and it may take a few iterations to reach a good level), unintended pod disruptions by applying new resource requests (after 12h all targeted pods will receive new requests even though individually they would be fine without, which also drives active-passive resource consumption up), difficulties to deal with spiky workload in general (due to the algorithmic approach it takes), recommended requests may exceed node capacity, limit scaling is proportional and therefore often questionable, and more. VPA is a double-edged sword: useful and necessary, but not easy to handle.</p><p>For the Gardener-managed components, we mostly removed limits. Why?</p><ul><li>CPU limits have almost always only downsides. They cause needless CPU throttling, which is not even easily visible. CPU requests turn into <code>cpu shares</code>, so if the node has capacity, the pod may consume the freely available CPU, but not if you have set limits, which curtail the pod by means of <code>cpu quota</code>. There are only certain scenarios in which they may make sense, e.g., if you set requests=limits and thereby define a pod with <code>guaranteed</code> <a href=https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod>QoS</a>, which influences your <code>cgroup</code> placement. However, that is difficult to do for the components you implement yourself and practically impossible for the components you just consume, because what&rsquo;s the correct value for requests/limits and will it hold true also if the load increases and what happens if a zone goes down or with the next update/version of this component? If anything, CPU limits caused outages, not helped prevent them.</li><li>As for memory limits, they are slightly more useful, because CPU is compressible and memory is not, so if one pod runs berserk, it may take others down (with CPU, <code>cpu shares</code> make it as fair as possible), depending on which OOM killer strikes (a complicated topic by itself). You don&rsquo;t want the operating system OOM killer to strike as the result is unpredictable. Better, it&rsquo;s the cgroup OOM killer or even the <code>kubelet</code>&rsquo;s eviction, if the consumption is slow enough as <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#interactions-of-pod-priority-and-qos>it takes priorities into consideration</a> even. If your component is critical and a singleton (e.g., node daemon set pods), you are better off also without memory limits, because letting the pod go OOM because of artificial/wrong memory limits can mean that the node becomes unusable. Hence, such components also better run only with no or a very high memory limit, so that you can catch the occasional memory leak (bug) eventually, but under normal operation, if you cannot decide about a true upper limit, rather not have limits and cause endless outages through them or when you need the pods the most (during a zone outage) where all your assumptions went out of the window.</li></ul><p>The downside of having poor or no limits and poor and no requests is that nodes may &ldquo;die&rdquo; more often. Contrary to the expectation, even for managed services, the managed service is not responsible or cannot guarantee the health of a node under all circumstances, since the end user defines what is run on the nodes (shared responsibility). If the workload exhausts any resource, it will be the end of the node, e.g., by compressing the CPU too much (so that the <code>kubelet</code> fails to do its work), exhausting the main memory too fast, disk space, file handles, or any other resource.</p><p>The <code>kubelet</code> allows for <a href=https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources>explicit reservation of resources</a> for operating system daemons (<code>system-reserved</code>) and Kubernetes daemons (<code>kube-reserved</code>) that are subtracted from the actual node resources and become the allocatable node resources for your workload/pods. All managed services configure these settings &ldquo;by rule of thumb&rdquo; (a balancing act), but cannot guarantee that the values won&rsquo;t waste resources or always will be sufficient. You will have to fine-tune them eventually and adapt them to your needs. In addition, you can configure <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction>soft and hard eviction thresholds</a> to give the <code>kubelet</code> some headroom to evict &ldquo;greedy&rdquo; pods in a controlled way. These settings can be configured for Gardener-managed clusters like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubelet:
</span></span><span style=display:flex><span>      systemReserved:                          <span style=color:green># explicit resource reservation for operating system daemons</span>
</span></span><span style=display:flex><span>        cpu: 100m
</span></span><span style=display:flex><span>        memory: 1Gi
</span></span><span style=display:flex><span>        ephemeralStorage: 1Gi
</span></span><span style=display:flex><span>        pid: 1000
</span></span><span style=display:flex><span>      kubeReserved:                            <span style=color:green># explicit resource reservation for Kubernetes daemons</span>
</span></span><span style=display:flex><span>        cpu: 100m
</span></span><span style=display:flex><span>        memory: 1Gi
</span></span><span style=display:flex><span>        ephemeralStorage: 1Gi
</span></span><span style=display:flex><span>        pid: 1000
</span></span><span style=display:flex><span>      evictionSoft:                            <span style=color:green># soft, i.e. graceful eviction (used if the node is about to run out of resources, avoiding hard evictions)</span>
</span></span><span style=display:flex><span>        memoryAvailable: 200Mi
</span></span><span style=display:flex><span>        imageFSAvailable: 10%
</span></span><span style=display:flex><span>        imageFSInodesFree: 10%
</span></span><span style=display:flex><span>        nodeFSAvailable: 10%
</span></span><span style=display:flex><span>        nodeFSInodesFree: 10%
</span></span><span style=display:flex><span>      evictionSoftGracePeriod:                 <span style=color:green># caps pod&#39;s `terminationGracePeriodSeconds` value during soft evictions (specific grace periods)</span>
</span></span><span style=display:flex><span>        memoryAvailable: 1m30s
</span></span><span style=display:flex><span>        imageFSAvailable: 1m30s
</span></span><span style=display:flex><span>        imageFSInodesFree: 1m30s
</span></span><span style=display:flex><span>        nodeFSAvailable: 1m30s
</span></span><span style=display:flex><span>        nodeFSInodesFree: 1m30s
</span></span><span style=display:flex><span>      evictionHard:                            <span style=color:green># hard, i.e. immediate eviction (used if the node is out of resources, avoiding the OS generally run out of resources fail processes indiscriminately)</span>
</span></span><span style=display:flex><span>        memoryAvailable: 100Mi
</span></span><span style=display:flex><span>        imageFSAvailable: 5%
</span></span><span style=display:flex><span>        imageFSInodesFree: 5%
</span></span><span style=display:flex><span>        nodeFSAvailable: 5%
</span></span><span style=display:flex><span>        nodeFSInodesFree: 5%
</span></span><span style=display:flex><span>      evictionMinimumReclaim:                  <span style=color:green># additional resources to reclaim after hitting the hard eviction thresholds to not hit the same thresholds soon after again</span>
</span></span><span style=display:flex><span>        memoryAvailable: 0Mi
</span></span><span style=display:flex><span>        imageFSAvailable: 0Mi
</span></span><span style=display:flex><span>        imageFSInodesFree: 0Mi
</span></span><span style=display:flex><span>        nodeFSAvailable: 0Mi
</span></span><span style=display:flex><span>        nodeFSInodesFree: 0Mi
</span></span><span style=display:flex><span>      evictionMaxPodGracePeriod: 90            <span style=color:green># caps pod&#39;s `terminationGracePeriodSeconds` value during soft evictions (general grace periods)</span>
</span></span><span style=display:flex><span>      evictionPressureTransitionPeriod: 5m0s   <span style=color:green># stabilization time window to avoid flapping of node eviction state</span>
</span></span></code></pre></div><p>You can tweak these settings also individually per worker pool (<code>spec.provider.workers.kubernetes.kubelet...</code>), which makes sense especially with different machine types (and also workload that you may want to schedule there).</p><p>Physical memory is not compressible, but you can overcome this issue to some degree (alpha since Kubernetes <code>v1.22</code> in combination with the feature gate <code>NodeSwap</code> on the <code>kubelet</code>) with swap memory. You can read more in this <a href=https://kubernetes.io/blog/2021/08/09/run-nodes-with-swap-alpha>introductory blog</a> and the <a href=https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory>docs</a>. If you chose to use it (still only alpha at the time of this writing) you may want to consider also the risks associated with swap memory:</p><ul><li>Reduced performance predictability</li><li>Reduced performance up to page trashing</li><li>Reduced security as secrets, normally held only in memory, could be swapped out to disk</li></ul><p>That said, the various options mentioned above are only remotely related to HA and will not be further explored throughout this document, but just to remind you: if a zone goes down, load patterns will shift, existing pods will probably receive more load and will require more resources (especially because it is often practically impossible to set &ldquo;proper&rdquo; resource requests, which drive node allocation - limits are always ignored by the scheduler) or more pods will/must be placed on the existing and/or new nodes and then these settings, which are generally critical (especially if you switch on <a href=/docs/gardener/shoot/shoot_scheduling_profiles/>bin-packing for Gardener-managed clusters</a> as a cost saving measure), will become even more critical during a zone outage.</p><h2 id=probes>Probes<a class=td-heading-self-link href=#probes aria-label="Heading self-link"></a></h2><p>Before we go down the rabbit hole even further and talk about how to spread your replicas, we need to talk about probes first, as they will become relevant later. Kubernetes supports three kinds of probes: <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes>startup, liveness, and readiness probes</a>. If you are a <a href=https://twitter.com/thockin/status/1615468485987143682>visual thinker</a>, also check out this <a href=https://speakerdeck.com/thockin/kubernetes-pod-probes>slide deck</a> by <a href=https://www.linkedin.com/in/tim-hockin-6501072>Tim Hockin</a> (Kubernetes networking SIG chair).</p><p>Basically, the <code>startupProbe</code> and the <code>livenessProbe</code> help you restart the container, if it&rsquo;s unhealthy for whatever reason, by letting the <code>kubelet</code> that orchestrates your containers on a node know, that it&rsquo;s unhealthy. The former is a special case of the latter and only applied at the startup of your container, if you need to handle the startup phase differently (e.g., with very slow starting containers) from the rest of the lifetime of the container.</p><p>Now, the <code>readinessProbe</code> helps you manage the ready status of your container and thereby pod (any container that is not ready turns the pod not ready). This again has impact on endpoints and pod disruption budgets:</p><ul><li>If the pod is not ready, the endpoint will be removed and the pod will not receive traffic anymore</li><li>If the pod is not ready, the pod counts into the pod disruption budget and if the budget is exceeded, no further voluntary pod disruptions will be permitted for the remaining ready pods (e.g., no eviction, no voluntary horizontal or vertical scaling, if the pod runs on a node that is about to be drained or in draining, draining will be paused until the max drain timeout passes)</li></ul><p>As you can see, all of these probes are (also) related to HA (mostly the <code>readinessProbe</code>, but depending on your workload, you can also leverage <code>livenessProbe</code> and <code>startupProbe</code> into your HA strategy). If Kubernetes doesn&rsquo;t know about the individual status of your container/pod, it won&rsquo;t do anything for you (right away). That said, later/indirectly something might/will happen via the node status that can also be ready or not ready, which influences the pods and load balancer listener registration (a not ready node will not receive cluster traffic anymore), but this process is worker pool global and reacts delayed and also doesn&rsquo;t discriminate between the containers/pods on a node.</p><p>In addition, Kubernetes also offers <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#pod-readiness-gate>pod readiness gates</a> to amend your pod readiness with additional custom conditions (normally, only the sum of the container readiness matters, but pod readiness gates additionally count into the overall pod readiness). This may be useful if you want to block (by means of pod disruption budgets that we will talk about next) the roll-out of your workload/nodes in case some (possibly external) condition fails.</p><h2 id=pod-disruption-budgets>Pod Disruption Budgets<a class=td-heading-self-link href=#pod-disruption-budgets aria-label="Heading self-link"></a></h2><p>One of the most important resources that help you on your way to HA are <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb>pod disruption budgets</a> or PDB for short. They tell Kubernetes how to deal with voluntary pod disruptions, e.g., during the deployment of your workload, when the nodes are rolled, or just in general when a pod shall be evicted/terminated. Basically, if the budget is reached, they block all voluntary pod disruptions (at least for a while until possibly other timeouts act or things happen that leave Kubernetes no choice anymore, e.g., the node is forcefully terminated). You should always define them for your workload.</p><p>Very important to note is that they are based on the <code>readinessProbe</code>, i.e. even if all of your replicas are <code>lively</code>, but not enough of them are <code>ready</code>, this blocks voluntary pod disruptions, so they are very critical and useful. Here an example (you can specify either <code>minAvailable</code> or <code>maxUnavailable</code> in absolute numbers or as percentage):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: policy/v1
</span></span><span style=display:flex><span>kind: PodDisruptionBudget
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  maxUnavailable: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      ...
</span></span></code></pre></div><p>And please do not specify a PDB of <code>maxUnavailable</code> being 0 or similar. That&rsquo;s pointless, even detrimental, as it blocks then even useful operations, forces always the hard timeouts that are less graceful and it doesn&rsquo;t make sense in the context of HA. You cannot &ldquo;force&rdquo; HA by preventing voluntary pod disruptions, you must work with the pod disruptions in a resilient way. Besides, PDBs are really only about voluntary pod disruptions - something bad can happen to a node/pod at any time and PDBs won&rsquo;t make this reality go away for you.</p><p>PDBs will not always work as expected and can also get in your way, e.g., if the PDB is violated or would be violated, it may possibly block whatever you are trying to do to salvage the situation, e.g., drain a node or deploy a patch version (if the PDB is or would be violated, not even unhealthy pods would be evicted as they could theoretically become healthy again, which Kubernetes doesn&rsquo;t know). In order to overcome this issue, it is now possible (alpha since Kubernetes <code>v1.26</code> in combination with the feature gate <code>PDBUnhealthyPodEvictionPolicy</code> on the API server) to configure the so-called <a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy>unhealthy pod eviction policy</a>. The default is still <code>IfHealthyBudget</code> as a change in default would have changed the behavior (as described above), but you can now also set <code>AlwaysAllow</code> at the PDB (<code>spec.unhealthyPodEvictionPolicy</code>). For more information, please check out <a href=https://github.com/kubernetes/kubernetes/issues/72320>this discussion</a>, <a href=https://github.com/kubernetes/kubernetes/pull/105296>the PR</a> and <a href="https://groups.google.com/g/kubernetes-sig-apps/c/_joO4swogKY?pli=1">this document</a> and balance the pros and cons for yourself. In short,
the new <code>AlwaysAllow</code> option is probably the better choice in most of the cases while <code>IfHealthyBudget</code> is useful only if you have frequent temporary transitions or for special cases where you have already implemented controllers that depend on the old behavior.</p><h2 id=pod-topology-spread-constraints>Pod Topology Spread Constraints<a class=td-heading-self-link href=#pod-topology-spread-constraints aria-label="Heading self-link"></a></h2><p><a href=https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints>Pod topology spread constraints</a> or PTSC for short (no official abbreviation exists, but we will use this in the following) are enormously helpful to distribute your replicas across multiple zones, nodes, or any other user-defined topology domain. They complement and improve on <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>pod (anti-)affinities</a> that still exist and can be used in combination.</p><p>PTSCs are an improvement, because they allow for <code>maxSkew</code> and <code>minDomains</code>. You can steer the &ldquo;level of tolerated imbalance&rdquo; with <code>maxSkew</code>, e.g., you probably want that to be at least 1, so that you can perform a rolling update, but this all depends on your <a href=https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment>deployment</a> (<code>maxUnavailable</code> and <code>maxSurge</code>), etc. <a href=https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#rolling-updates>Stateful sets</a> are a bit different (<code>maxUnavailable</code>) as they are bound to volumes and depend on them, so there usually cannot be 2 pods requiring the same volume. <code>minDomains</code> is a hint to tell the scheduler how far to spread, e.g., if all nodes in one zone disappeared because of a zone outage, it may &ldquo;appear&rdquo; as if there are only 2 zones in a 3 zones cluster and the scheduling decisions may end up wrong, so a <code>minDomains</code> of 3 will tell the scheduler to spread to 3 zones before adding another replica in one zone. Be careful with this setting as it also means, if one zone is down the &ldquo;spread&rdquo; is already at least 1, if pods run in the other zones. This is useful where you have exactly as many replicas as you have zones and you do not want any imbalance. Imbalance is critical as if you end up with one, nobody is going to do the (active) re-balancing for you (unless you deploy and configure additional non-standard components such as the <a href=https://github.com/kubernetes-sigs/descheduler>descheduler</a>). So, for instance, if you have something like a DBMS that you want to spread across 2 zones (active-passive) or 3 zones (consensus-based), you better specify <code>minDomains</code> of 2 respectively 3 to force your replicas into at least that many zones before adding more replicas to another zone (if supported).</p><p>Anyway, PTSCs are critical to have, but not perfect, so we saw (unsurprisingly, because that&rsquo;s how the scheduler works), that the scheduler may block the deployment of new pods because it takes the decision pod-by-pod (see for instance <a href=https://github.com/kubernetes/kubernetes/issues/109364>#109364</a>).</p><h2 id=pod-affinities-and-anti-affinities>Pod Affinities and Anti-Affinities<a class=td-heading-self-link href=#pod-affinities-and-anti-affinities aria-label="Heading self-link"></a></h2><p>As said, you can combine PTSCs with <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>pod affinities and/or anti-affinities</a>. Especially <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>inter-pod (anti-)affinities</a> may be helpful to place pods <em>apart</em>, e.g., because they are fall-backs for each other or you do not want multiple potentially resource-hungry <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#besteffort>&ldquo;best-effort&rdquo;</a> or <a href=https://kubernetes.io/docs/concepts/workloads/pods/pod-qos/#burstable>&ldquo;burstable&rdquo;</a> pods side-by-side (noisy neighbor problem), or <em>together</em>, e.g., because they form a unit and you want to reduce the failure domain, reduce the network latency, and reduce the costs.</p><h2 id=topology-aware-hints>Topology Aware Hints<a class=td-heading-self-link href=#topology-aware-hints aria-label="Heading self-link"></a></h2><p>While <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints>topology aware hints</a> are not directly related to HA, they are very relevant in the HA context. Spreading your workload across multiple zones may increase network latency and cost significantly, if the traffic is not shaped. Topology aware hints (beta since Kubernetes <code>v1.23</code>, replacing the now deprecated topology aware traffic routing with topology keys) help to route the traffic within the originating zone, if possible. Basically, they tell <code>kube-proxy</code> how to setup your routing information, so that clients can talk to endpoints that are located within the same zone.</p><p>Be aware however, that there are some limitations. Those are called <a href=https://kubernetes.io/docs/concepts/services-networking/topology-aware-hints/#safeguards>safeguards</a> and if they strike, the hints are off and traffic is routed again randomly. Especially controversial is the balancing limitation as there is the assumption, that the load that hits an endpoint is determined by the allocatable CPUs in that topology zone, but that&rsquo;s not always, if even often, the case (see for instance <a href=https://github.com/kubernetes/kubernetes/issues/113731>#113731</a> and <a href=https://github.com/kubernetes/kubernetes/issues/110714>#110714</a>). So, this limitation hits far too often and your hints are off, but then again, it&rsquo;s about network latency and cost optimization first, so it&rsquo;s better than nothing.</p><h2 id=networking>Networking<a class=td-heading-self-link href=#networking aria-label="Heading self-link"></a></h2><p>We have talked about networking only to some small degree so far (<code>readiness</code> probes, pod disruption budgets, topology aware hints). The most important component is probably your ingress load balancer - everything else is managed by Kubernetes. AWS, Azure, GCP, and also OpenStack offer multi-zonal load balancers, so make use of them. In Azure and GCP, LBs are regional whereas in AWS and OpenStack, they need to be bound to a zone, which the cloud-controller-manager does by observing the zone labels at the nodes (please note that this behavior is not always working as expected, see <a href=https://github.com/kubernetes/cloud-provider-aws/issues/569>#570</a> where the AWS cloud-controller-manager is not readjusting to newly observed zones).</p><p>Please be reminded that even if you use a service mesh like <a href=https://istio.io>Istio</a>, the off-the-shelf installation/configuration usually never comes with productive settings (to simplify first-time installation and improve first-time user experience) and you will have to fine-tune your installation/configuration, much like the rest of your workload.</p><h2 id=relevant-cluster-settings>Relevant Cluster Settings<a class=td-heading-self-link href=#relevant-cluster-settings aria-label="Heading self-link"></a></h2><p>Following now a summary/list of the more relevant settings you may like to tune for Gardener-managed clusters:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  controlPlane:
</span></span><span style=display:flex><span>    highAvailability:
</span></span><span style=display:flex><span>      failureTolerance:
</span></span><span style=display:flex><span>        type: zone <span style=color:green># valid values are `node` and `zone` (only available if your control plane resides in a region with 3+ zones)</span>
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeAPIServer:
</span></span><span style=display:flex><span>      defaultNotReadyTolerationSeconds: 300
</span></span><span style=display:flex><span>      defaultUnreachableTolerationSeconds: 300
</span></span><span style=display:flex><span>    kubelet:
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>    kubeScheduler:
</span></span><span style=display:flex><span>      featureGates:
</span></span><span style=display:flex><span>        MinDomainsInPodTopologySpread: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    kubeControllerManager:
</span></span><span style=display:flex><span>      nodeMonitorPeriod: 10s
</span></span><span style=display:flex><span>      nodeMonitorGracePeriod: 40s
</span></span><span style=display:flex><span>      horizontalPodAutoscaler:
</span></span><span style=display:flex><span>        syncPeriod: 15s
</span></span><span style=display:flex><span>        tolerance: 0.1
</span></span><span style=display:flex><span>        downscaleStabilization: 5m0s
</span></span><span style=display:flex><span>        initialReadinessDelay: 30s
</span></span><span style=display:flex><span>        cpuInitializationPeriod: 5m0s
</span></span><span style=display:flex><span>    verticalPodAutoscaler:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      evictAfterOOMThreshold: 10m0s
</span></span><span style=display:flex><span>      evictionRateBurst: 1
</span></span><span style=display:flex><span>      evictionRateLimit: -1
</span></span><span style=display:flex><span>      evictionTolerance: 0.5
</span></span><span style=display:flex><span>      recommendationMarginFraction: 0.15
</span></span><span style=display:flex><span>      updaterInterval: 1m0s
</span></span><span style=display:flex><span>      recommenderInterval: 1m0s
</span></span><span style=display:flex><span>    clusterAutoscaler:
</span></span><span style=display:flex><span>      expander: <span style=color:#a31515>&#34;least-waste&#34;</span>
</span></span><span style=display:flex><span>      scanInterval: 10s
</span></span><span style=display:flex><span>      scaleDownDelayAfterAdd: 60m
</span></span><span style=display:flex><span>      scaleDownDelayAfterDelete: 0s
</span></span><span style=display:flex><span>      scaleDownDelayAfterFailure: 3m
</span></span><span style=display:flex><span>      scaleDownUnneededTime: 30m
</span></span><span style=display:flex><span>      scaleDownUtilizationThreshold: 0.5
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: ...
</span></span><span style=display:flex><span>      minimum: 6
</span></span><span style=display:flex><span>      maximum: 60
</span></span><span style=display:flex><span>      maxSurge: 3
</span></span><span style=display:flex><span>      maxUnavailable: 0
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - ... <span style=color:green># list of zones you want your worker pool nodes to be spread across, see above</span>
</span></span><span style=display:flex><span>      kubernetes:
</span></span><span style=display:flex><span>        kubelet:
</span></span><span style=display:flex><span>          ... <span style=color:green># similar to `kubelet` above (cluster-wide settings), but here per worker pool (pool-specific settings), see above</span>
</span></span><span style=display:flex><span>      machineControllerManager: <span style=color:green># optional, it allows to configure the machine-controller settings.</span>
</span></span><span style=display:flex><span>        machineCreationTimeout: 20m
</span></span><span style=display:flex><span>        machineHealthTimeout: 10m
</span></span><span style=display:flex><span>        machineDrainTimeout: 60h
</span></span><span style=display:flex><span>  systemComponents:
</span></span><span style=display:flex><span>    coreDNS:
</span></span><span style=display:flex><span>      autoscaling:
</span></span><span style=display:flex><span>        mode: horizontal <span style=color:green># valid values are `horizontal` (driven by CPU load) and `cluster-proportional` (driven by number of nodes/cores)</span>
</span></span></code></pre></div><h4 id=on-speccontrolplanehighavailabilityfailuretolerancetype>On <code>spec.controlPlane.highAvailability.failureTolerance.type</code><a class=td-heading-self-link href=#on-speccontrolplanehighavailabilityfailuretolerancetype aria-label="Heading self-link"></a></h4><p>If set, determines the degree of failure tolerance for your control plane. <code>zone</code> is preferred, but only available if your control plane resides in a region with 3+ zones. See <a href=/blog/2023/03-27-high-availability-and-zone-outage-toleration/#control-plane>above</a> and the <a href=/docs/guides/high-availability/control-plane/>docs</a>.</p><h4 id=on-speckuberneteskubeapiserverdefaultunreachabletolerationseconds-and-defaultnotreadytolerationseconds>On <code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds</code> and <code>defaultNotReadyTolerationSeconds</code><a class=td-heading-self-link href=#on-speckuberneteskubeapiserverdefaultunreachabletolerationseconds-and-defaultnotreadytolerationseconds aria-label="Heading self-link"></a></h4><p>This is a very interesting <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver>API server setting</a> that lets Kubernetes decide how fast to evict pods from nodes whose status condition of type <code>Ready</code> is either <code>Unknown</code> (node status unknown, a.k.a unreachable) or <code>False</code> (<code>kubelet</code> not ready) (see <a href=https://kubernetes.io/docs/concepts/architecture/nodes/#condition>node status conditions</a>; please note that <code>kubectl</code> shows both values as <code>NotReady</code> which is a somewhat &ldquo;simplified&rdquo; visualization).</p><p>You can also override the cluster-wide API server settings <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-based-evictions>individually per pod</a>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tolerations:
</span></span><span style=display:flex><span>  - key: <span style=color:#a31515>&#34;node.kubernetes.io/unreachable&#34;</span>
</span></span><span style=display:flex><span>    operator: <span style=color:#a31515>&#34;Exists&#34;</span>
</span></span><span style=display:flex><span>    effect: <span style=color:#a31515>&#34;NoExecute&#34;</span>
</span></span><span style=display:flex><span>    tolerationSeconds: 0
</span></span><span style=display:flex><span>  - key: <span style=color:#a31515>&#34;node.kubernetes.io/not-ready&#34;</span>
</span></span><span style=display:flex><span>    operator: <span style=color:#a31515>&#34;Exists&#34;</span>
</span></span><span style=display:flex><span>    effect: <span style=color:#a31515>&#34;NoExecute&#34;</span>
</span></span><span style=display:flex><span>    tolerationSeconds: 0
</span></span></code></pre></div><p>This will evict pods on unreachable or not-ready nodes immediately, but be cautious: <code>0</code> is very aggressive and may lead to unnecessary disruptions. Again, you must decide for your own workload and balance out the pros and cons (e.g., long startup time).</p><p>Please note, these settings replace <code>spec.kubernetes.kubeControllerManager.podEvictionTimeout</code> that was deprecated with Kubernetes <code>v1.26</code> (and acted as an upper bound).</p><h4 id=on-speckuberneteskubeschedulerfeaturegatesmindomainsinpodtopologyspread>On <code>spec.kubernetes.kubeScheduler.featureGates.MinDomainsInPodTopologySpread</code><a class=td-heading-self-link href=#on-speckuberneteskubeschedulerfeaturegatesmindomainsinpodtopologyspread aria-label="Heading self-link"></a></h4><p>Required to be enabled for <code>minDomains</code> to work with PTSCs (beta since Kubernetes <code>v1.25</code>, but off by default). See <a href=/blog/2023/03-27-high-availability-and-zone-outage-toleration/#pod-topology-spread-constraints>above</a> and the <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/#topologyspreadconstraints-field>docs</a>. This tells the scheduler, how many topology domains to expect (=zones in the context of this document).</p><h4 id=on-speckuberneteskubecontrollermanagernodemonitorperiod-and-nodemonitorgraceperiod>On <code>spec.kubernetes.kubeControllerManager.nodeMonitorPeriod</code> and <code>nodeMonitorGracePeriod</code><a class=td-heading-self-link href=#on-speckuberneteskubecontrollermanagernodemonitorperiod-and-nodemonitorgraceperiod aria-label="Heading self-link"></a></h4><p>This is another very interesting <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager>kube-controller-manager setting</a> that can help you speed up or slow down how fast a node shall be considered <code>Unknown</code> (node status unknown, a.k.a unreachable) when the <code>kubelet</code> is not updating its status anymore (see <a href=https://kubernetes.io/docs/concepts/architecture/nodes/#condition>node status conditions</a>), which effects eviction (see <code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds</code> and <code>defaultNotReadyTolerationSeconds</code> above). The shorter the time window, the faster Kubernetes will act, but the higher the chance of flapping behavior and pod trashing, so you may want to balance that out according to your needs, otherwise stick to the default which is a reasonable compromise.</p><h4 id=on-speckuberneteskubecontrollermanagerhorizontalpodautoscaler>On <code>spec.kubernetes.kubeControllerManager.horizontalPodAutoscaler...</code><a class=td-heading-self-link href=#on-speckuberneteskubecontrollermanagerhorizontalpodautoscaler aria-label="Heading self-link"></a></h4><p>This configures horizontal pod autoscaling in Gardener-managed clusters. See <a href=/blog/2023/03-27-high-availability-and-zone-outage-toleration/#replicas-horizontal-scaling>above</a> and the <a href=https://kubernetes.io/de/docs/tasks/run-application/horizontal-pod-autoscale>docs</a> for the detailed fields.</p><h4 id=on-speckubernetesverticalpodautoscaler>On <code>spec.kubernetes.verticalPodAutoscaler...</code><a class=td-heading-self-link href=#on-speckubernetesverticalpodautoscaler aria-label="Heading self-link"></a></h4><p>This configures vertical pod autoscaling in Gardener-managed clusters. See <a href=/blog/2023/03-27-high-availability-and-zone-outage-toleration/#resources-vertical-scaling>above</a> and the <a href=https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/docs/faq.md>docs</a> for the detailed fields.</p><h4 id=on-speckubernetesclusterautoscaler>On <code>spec.kubernetes.clusterAutoscaler...</code><a class=td-heading-self-link href=#on-speckubernetesclusterautoscaler aria-label="Heading self-link"></a></h4><p>This configures node auto-scaling in Gardener-managed clusters. See <a href=/blog/2023/03-27-high-availability-and-zone-outage-toleration/#worker-pools>above</a> and the <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md>docs</a> for the detailed fields, especially about <a href=https://github.com/gardener/autoscaler/blob/machine-controller-manager-provider/cluster-autoscaler/FAQ.md#what-are-expanders>expanders</a>, which may become life-saving in case of a zone outage when a resource crunch is setting in and everybody rushes to get machines in the healthy zones.</p><p>In case of a zone outage, it may be interesting to understand how the cluster autoscaler will put a worker pool in one zone into &ldquo;back-off&rdquo;. Unfortunately, the official cluster autoscaler documentation does not explain these details, but you can find hints in the <a href=https://github.com/kubernetes/autoscaler/blob/b94f340af58eb063df9ebfcd65835f9a499a69a2/cluster-autoscaler/config/autoscaling_options.go#L214-L219>source code</a>:</p><p>If a node fails to come up, the node group (worker pool in that zone) will go into &ldquo;back-off&rdquo;, at first 5m, then <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/utils/backoff/exponential_backoff.go#L77-L82>exponentially longer</a> until the maximum of 30m is reached. The &ldquo;back-off&rdquo; is reset after 3 hours. This in turn means, that nodes must be first considered <code>Unknown</code>, which happens when <code>spec.kubernetes.kubeControllerManager.nodeMonitorPeriod.nodeMonitorGracePeriod</code> lapses. Then they must either remain in this state until <code>spec.provider.workers.machineControllerManager.machineHealthTimeout</code> lapses for them to be recreated, which will fail in the unhealthy zone, or <code>spec.kubernetes.kubeAPIServer.defaultUnreachableTolerationSeconds</code> lapses for the pods to be evicted (usually faster than node replacements, depending on your configuration), which will trigger the cluster autoscaler to create more capacity, but very likely in the same zone as it tries to balance its node groups at first, which will also fail in the unhealthy zone. It will be considered failed only when <code>maxNodeProvisionTime</code> lapses (usually close to <code>spec.provider.workers.machineControllerManager.machineCreationTimeout</code>) and only then put the node group into &ldquo;back-off&rdquo; and not retry for 5m at first and then exponentially longer. It&rsquo;s critical to keep that in mind and accommodate for it. If you have already capacity up and running, the reaction time is usually much faster with leases (whatever you set) or endpoints (<code>spec.kubernetes.kubeControllerManager.nodeMonitorPeriod.nodeMonitorGracePeriod</code>), but if you depend on new/fresh capacity, the above should inform you how long you will have to wait for it.</p><h4 id=on-specproviderworkersminimum-maximum-maxsurge-maxunavailable-zones-and-machinecontrollermanager>On <code>spec.provider.workers.minimum</code>, <code>maximum</code>, <code>maxSurge</code>, <code>maxUnavailable</code>, <code>zones</code>, and <code>machineControllerManager</code><a class=td-heading-self-link href=#on-specproviderworkersminimum-maximum-maxsurge-maxunavailable-zones-and-machinecontrollermanager aria-label="Heading self-link"></a></h4><p>Each worker pool in Gardener may be configured differently. Among many other settings like machine type, root disk, Kubernetes version, <code>kubelet</code> settings, and many more you can also specify the lower and upper bound for the number of machines (<code>minimum</code> and <code>maximum</code>), how many machines may be added additionally during a rolling update (<code>maxSurge</code>) and how many machines may be in termination/recreation during a rolling update (<code>maxUnavailable</code>), and of course across how many zones the nodes shall be spread (<code>zones</code>).</p><p>Interesting is also the configuration for Gardener&rsquo;s machine-controller-manager or MCM for short that provisions, monitors, terminates, replaces, or updates machines that back your nodes:</p><ul><li>The shorter <code>machineCreationTimeout</code> is, the faster MCM will retry to create a machine/node, if the process is stuck on cloud provider side. It is set to useful/practical timeouts for the different cloud providers and you probably don&rsquo;t want to change those (in the context of HA at least). Please align with the cluster autoscaler&rsquo;s <code>maxNodeProvisionTime</code>.</li><li>The shorter <code>machineHealthTimeout</code> is, the faster MCM will replace machines/nodes in case the kubelet isn&rsquo;t reporting back, which translates to <code>Unknown</code>, or reports back with <code>NotReady</code>, or the <a href=https://github.com/kubernetes/node-problem-detector>node-problem-detector</a> that Gardener deploys for you reports a non-recoverable issue/condition (e.g., read-only file system). If it is too short however, you risk node and pod trashing, so be careful.</li><li>The shorter <code>machineDrainTimeout</code> is, the faster you can get rid of machines/nodes that MCM decided to remove, but this puts a cap on the grace periods and PDBs. They are respected up until the drain timeout lapses - then the machine/node will be forcefully terminated, whether or not the pods are still in termination or not even terminated because of PDBs. Those PDBs will then be violated, so be careful here as well. Please align with the cluster autoscaler&rsquo;s <code>maxGracefulTerminationSeconds</code>.</li></ul><p>Especially the last two settings may help you recover faster from cloud provider issues.</p><h4 id=on-specsystemcomponentscorednsautoscaling>On <code>spec.systemComponents.coreDNS.autoscaling</code><a class=td-heading-self-link href=#on-specsystemcomponentscorednsautoscaling aria-label="Heading self-link"></a></h4><p>DNS is critical, in general and also within a Kubernetes cluster. Gardener-managed clusters deploy <a href=https://coredns.io>CoreDNS</a>, a graduated CNCF project. Gardener supports 2 auto-scaling modes for it, <code>horizontal</code> (using HPA based on CPU) and <code>cluster-proportional</code> (using <a href=https://github.com/kubernetes-sigs/cluster-proportional-autoscaler>cluster proportional autoscaler</a> that scales the number of pods based on the number of nodes/cores, not to be confused with the cluster autoscaler that scales nodes based on their utilization). Check out the <a href=/docs/gardener/autoscaling/dns-autoscaling/>docs</a>, especially the <a href=/docs/gardener/autoscaling/dns-autoscaling/#trade-offs-of-horizontal-and-cluster-proportional-dns-autoscaling>trade-offs</a> why you would chose one over the other (<code>cluster-proportional</code> gives you more configuration options, if CPU-based horizontal scaling is insufficient to your needs). Consider also Gardener&rsquo;s feature <a href=/docs/gardener/networking/node-local-dns/>node-local DNS</a> to decouple you further from the DNS pods and stabilize DNS. Again, that&rsquo;s not strictly related to HA, but may become important during a zone outage, when load patterns shift and pods start to initialize/resolve DNS records more frequently in bulk.</p><h2 id=more-caveats>More Caveats<a class=td-heading-self-link href=#more-caveats aria-label="Heading self-link"></a></h2><p>Unfortunately, there are a few more things of note when it comes to HA in a Kubernetes cluster that may be &ldquo;surprising&rdquo; and hard to mitigate:</p><ul><li>If the <code>kubelet</code> restarts, it will report all pods as <code>NotReady</code> on startup until it reruns its probes (<a href=https://github.com/kubernetes/kubernetes/issues/100277>#100277</a>), which leads to temporary endpoint and load balancer target removal (<a href=https://github.com/kubernetes/kubernetes/issues/102367>#102367</a>). This topic is somewhat controversial. Gardener uses rolling updates and a jitter to spread necessary <code>kubelet</code> restarts as good as possible.</li><li>If a <code>kube-proxy</code> pod on a node turns <code>NotReady</code>, all load balancer traffic to all pods (on this node) under services with <code>externalTrafficPolicy</code> <code>local</code> will cease as the load balancer will then take this node out of serving. This topic is somewhat controversial as well. So, please remember that <code>externalTrafficPolicy</code> <code>local</code> not only has the disadvantage of imbalanced traffic spreading, but also a dependency to the kube-proxy pod that may and will be unavailable during updates. Gardener uses rolling updates to spread necessary <code>kube-proxy</code> updates as good as possible.</li></ul><p>These are just a few additional considerations. They may or may not affect you, but other intricacies may. It&rsquo;s a reminder to be watchful as Kubernetes may have one or two relevant quirks that you need to consider (and will probably only find out over time and with extensive testing).</p><h2 id=meaningful-availability>Meaningful Availability<a class=td-heading-self-link href=#meaningful-availability aria-label="Heading self-link"></a></h2><p>Finally, let&rsquo;s go back to where we started. We recommended to measure <a href=https://research.google/pubs/pub50828>meaningful availability</a>. For instance, in Gardener, we do not trust only internal signals, but track also whether Gardener or the control planes that it manages are externally available through the external DNS records and load balancers, SNI-routing Istio gateways, etc. (the same path all users must take). It&rsquo;s a huge difference whether the API server&rsquo;s internal readiness probe passes or the user can actually reach the API server and it does what it&rsquo;s supposed to do. Most likely, you will be in a similar spot and can do the same.</p><p>What you do with these signals is another matter. Maybe there are some actionable metrics and you can trigger some active fail-over, maybe you can only use it to improve your HA setup altogether. In our case, we also use it to deploy mitigations, e.g., via our <a href=https://github.com/gardener/dependency-watchdog>dependency-watchdog</a> that watches, for instance, Gardener-managed API servers and shuts down components like the controller managers to avert cascading knock-off effects (e.g., melt-down if the <code>kubelets</code> cannot reach the API server, but the controller managers can and start taking down nodes and pods).</p><p>Either way, understanding how users perceive your service is key to the improvement process as a whole. Even if you are not struck by a zone outage, the measures above and tracking the meaningful availability will help you improve your service.</p><p>Thank you for your interest and we wish you no or a &ldquo;successful&rdquo; zone outage next time. 😊</p><h2 id=want-to-know-more-about-gardener>Want to know more about Gardener?<a class=td-heading-self-link href=#want-to-know-more-about-gardener aria-label="Heading self-link"></a></h2><p>The Gardener project is Open Source and <a href=https://github.com/gardener>hosted on GitHub</a>.</p><p>Feedback and contributions are always welcome!</p><p>All channels for getting in touch or learning about the project are listed on our <a href=/docs/contribute/#community>landing page</a>. We are cordially inviting interested parties to join our <a href=/docs/contribute/#bi-weekly-meetings>bi-weekly meetings</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ea4e888c31a16f912d612ee4ba1570cb>2022</h1><div class="td-byline mb-4"><time datetime=2022-10-20 class=text-body-secondary>Thursday, October 20, 2022</time></div></div><div class=td-content><h1 id=pg-36f7b690eb7845fa916c901da29f7490>Community Call - Get more computing power in Gardener by overcoming Kubelet limitations with CRI-resource-manager</h1><div class="td-byline mb-4"><time datetime=2022-10-20 class=text-body-secondary>Thursday, October 20, 2022</time></div><h2 id=presenters>Presenters<a class=td-heading-self-link href=#presenters aria-label="Heading self-link"></a></h2><p>This community call was led by <a href=https://github.com/ppalucki>Pawel Palucki</a> and <a href=https://github.com/kad>Alexander D. Kanevskiy</a>.</p><h2 id=topics>Topics<a class=td-heading-self-link href=#topics aria-label="Heading self-link"></a></h2><p>Alexander Kanevskiy begins the community call by giving an overview of <a href=https://github.com/intel/cri-resource-manager>CRI-resource-manager</a>, describing it as a &ldquo;hardware aware container runtime&rdquo;, and also going over what it brings to the user in terms of features and policies.</p><p>Pawel Palucki continues by giving details on the policy that will later be used in the demo and the use case demonstrated in it. He then goes over the &ldquo;must have&rdquo; features of any extension - observability and the ability to deploy and configure objects with it.</p><p>The demo then begins, mixed with slides giving further information at certain points regarding the installation process, static and dynamic configuration flow, healthchecks and recovery mode, and access to logs, among others.</p><p>The presentation is concluded by Pawel showcasing the new features coming to CRI-resource-manager with its next releases and sharing some tips for other extension developers.</p><p>If you are left with any questions regarding the content, you might find the answers at the Q&amp;A session and discussion held at the end, as well as the questions asked and answered throughout the meeting.</p><h2 id=recording>Recording<a class=td-heading-self-link href=#recording aria-label="Heading self-link"></a></h2><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/5a_A3furzlg?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="Get more computing power in Gardener with CRI-resource-manager"></iframe></div></div><div class=td-content style=page-break-before:always><h1 id=pg-89ebd16840f2aa089b5b301b124738c9>Community Call - Cilium / Isovalent Presentation</h1><div class="td-byline mb-4"><time datetime=2022-10-06 class=text-body-secondary>Thursday, October 06, 2022</time></div><h2 id=presenters>Presenters<a class=td-heading-self-link href=#presenters aria-label="Heading self-link"></a></h2><p>This community call was led by <a href=https://github.com/raymonddejong>Raymond de Jong</a>.</p><h2 id=topics>Topics<a class=td-heading-self-link href=#topics aria-label="Heading self-link"></a></h2><p>This meeting explores the uses of <a href=https://cilium.io/>Cilium</a>, an open source software used to secure the network connectivity between application services deployed using Kubernetes, and <a href=https://github.com/cilium/hubble/blob/master/README.md>Hubble</a>, the networking and security observability platform built on top of it.</p><p>Raymond de Jong begins the meeting by giving an introduction of Cillium and eBPF and how they are both used in Kubernetes networking and services. He then goes over the ways of running Cillium - either by using a supported cloud provider or by CNI chaining.</p><p>The next topic introduced is the Cluster Mesh and the different use cases for it, offering high availability, shared services, local and remote service affinity, and the ability to split services.</p><p>In regards to security, being an identity-based security solution utilizing API-aware authorization, Cillium implements Hubble in order to increase its observability. Hubble combines hubble UI, hubble API and hubble Metrics - Grafana and Prometheus, in order to provide service dependency maps, detailed flow visibility and built-in metrics for operations and applications stability.</p><p>The final topic covered is the Service Mesh, offering service maps and the ability to integrate Cluster Mesh features.</p><p>If you are left with any questions regarding the content, you might find the answers at the Q&amp;A session and discussion held at the end, as well as the questions asked and answered throughout the meeting.</p><h2 id=recording>Recording<a class=td-heading-self-link href=#recording aria-label="Heading self-link"></a></h2><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/46nCdVA-rsc?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="Cilium / Isovalent Presentation"></iframe></div></div><div class=td-content style=page-break-before:always><h1 id=pg-770b1a90a9f5e0750ddf7db982a425ca>Community Call - Gardener Extension Development</h1><div class="td-byline mb-4"><time datetime=2022-06-17 class=text-body-secondary>Friday, June 17, 2022</time></div><h2 id=presenters>Presenters<a class=td-heading-self-link href=#presenters aria-label="Heading self-link"></a></h2><p>This community call was led by <a href=https://github.com/jensac>Jens Schneider</a> and Lothar Gesslein.</p><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>Starting the development of a new Gardener extension can be challenging, when you are not an expert in the Gardener ecosystem yet. Therefore, the first half of this community call led by Jens Schneider aims to provide a &ldquo;getting started tutorial&rdquo; at a beginner level. <a href=https://23technologies.cloud/en>23Technologies</a> have developed a minimal working example for Gardener extensions, <a href=https://github.com/23technologies/gardener-extension-mwe>gardener-extension-mwe</a>, hosted in a Github repository. Jens is following the <a href=https://23technologies.cloud/en/blog/gardener-ext-dev>Getting started with Gardener extension development</a> tutorial, which aims to provide exactly that.</p><p>In the second part of the community call, Lothar Gesslein introduces the <a href=https://github.com/23technologies/gardener-extension-shoot-flux>gardener-extension-shoot-flux</a>, which allows for the automated installation of arbitrary Kubernetes resources into shoot clusters. As this extension relies on <a href=https://fluxcd.io/>Flux</a>, an overview of Flux&rsquo;s capabilities is also provided.</p><p>If you are left with any questions regarding the content, you might find the answers at the Q&amp;A session and discussion held at the end.</p><p>You can find the tutorials in this community call at:</p><ul><li><a href=https://23technologies.cloud/en/blog/gardener-ext-dev>Getting started with Gardener extension development</a></li><li><a href=https://23technologies.cloud/en/blog/gardener-ext-shoot-flux>A Gardener Extension for universal Shoot Configuration</a></li></ul><p>If you are left with any questions regarding the content, you might find the answers at the Q&amp;A session and discussion held at the end of the meeting.</p><h2 id=recording>Recording<a class=td-heading-self-link href=#recording aria-label="Heading self-link"></a></h2><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/nG2FRYL05mc?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="Gardener Extension Development - From scratch to the gardener-extension-shoot-flux"></iframe></div></div><div class=td-content style=page-break-before:always><h1 id=pg-fe89397d42482e6f63fb9940ca790fe2>Community Call - Deploying and Developing Gardener Locally</h1><div class="td-byline mb-4"><time datetime=2022-03-23 class=text-body-secondary>Wednesday, March 23, 2022</time></div><h2 id=presenters>Presenters<a class=td-heading-self-link href=#presenters aria-label="Heading self-link"></a></h2><p>This community call was led by <a href=https://github.com/timebertt>Tim Ebert</a> and <a href=https://github.com/rfranzke>Rafael Franzke</a>.</p><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>So far, deploying Gardener locally was not possible end-to-end. While you certainly could run the Gardener components in a minikube or kind cluster, creating shoot clusters always required to register seeds backed by cloud provider infrastructure like AWS, Azure, etc..</p><p>Consequently, developing Gardener locally was similarly complicated, and the entry barrier for new contributors was way too high.</p><p>In a previous community call (<a href="https://www.youtube.com/watch?v=ZPAisXqjoTI&amp;ab_channel=GardenerProject">Hackathon &ldquo;Hack The Metal&rdquo;</a>), we already presented a new approach for overcoming these hurdles and complexities.</p><p>Now we would like to present the <a href=/docs/gardener/extensions/provider-local/>Local Provider Extension</a> for Gardener and show how it can be used to deploy Gardener locally, allowing you to quickly get your feet wet with the project.</p><p>In this session, Tim Ebert goes through the process of setting up a local Gardener cluster. After his demonstration, Rafael Franzke showcases a different approach to building your clusters locally, which, while more complicated, offers a much faster build time.</p><p>You can find the tutorials in this community call at:</p><ul><li><a href=/docs/gardener/deployment/getting_started_locally/>Deploying Gardener locally</a></li><li><a href=/docs/gardener/getting_started_locally/>Running Gardener locally</a></li></ul><p>If you are left with any questions regarding the content, you might find the answers in the questions asked and answered throughout the meeting.</p><h2 id=recording>Recording<a class=td-heading-self-link href=#recording aria-label="Heading self-link"></a></h2><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/nV_JI8YWwY4?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title="Deploying and Developing Gardener Locally (Without Any External Infrastructure!)"></iframe></div></div><div class=td-content style=page-break-before:always><h1 id=pg-10ead6949381e30bc81d2a37d49d6005>Community Call - Gardenctl-v2</h1><div class="td-byline mb-4"><time datetime=2022-02-17 class=text-body-secondary>Thursday, February 17, 2022</time></div><h2 id=presenters>Presenters<a class=td-heading-self-link href=#presenters aria-label="Heading self-link"></a></h2><p>This community call was led by <a href=https://github.com/holgerkoser>Holger Kosser</a>, <a href=https://github.com/grolu>Lukas Gross</a> and <a href=https://github.com/petersutter>Peter Sutter</a>.</p><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>Watch the recording of our February 2022 Community call to see how to get started with the gardenctl-v2 and watch a walkthrough for gardenctl-v2 features. You&rsquo;ll learn about targeting, secure shoot cluster access, SSH, and how to use cloud provider CLIs natively.</p><p>The session is led by Lukas Gross, who begins by giving some information on the motivations behind creating a new version of gardenctl - providing secure access to shoot clustes, enabling direct usage of kubectl and cloud provider CLIs and managing cloud provider resources for SSH access.</p><p>Holger Kosser then takes over in order to delve deeper into the concepts behind the implementation of gardenctl-2, going over Targeting, Gardenlogin and Cloud Provider CLIs. After that, Peter Sutter does the first demo, where he presents the main features in gardenctl-2.</p><p>The next part details how to get started with gardenctl, followed by another demo. The landscape requirements are also discussed, as well as future plans and enhancement requests.</p><p>You can find the slides for this community call at <a href="https://docs.google.com/presentation/d/1WxvMwdJ1WT2YvTox8Ni_IyX4Qpsz3-87IS5M53FwSvU/edit#slide=id.p8">Google Slides</a>.</p><p>If you are left with any questions regarding the content, you might find the answers at the Q&amp;A session and discussion held at the end, as well as the questions asked and answered throughout the meeting.</p><h2 id=recording>Recording<a class=td-heading-self-link href=#recording aria-label="Heading self-link"></a></h2><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share; fullscreen" loading=eager referrerpolicy=strict-origin-when-cross-origin src="https://www.youtube.com/embed/U1VvyQiE3Jg?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 title=Gardenctl-v2></iframe></div></div><div class=td-content style=page-break-before:always><h1 id=pg-ae0214161ad8360d2ab5db06da986748>2021</h1><div class="td-byline mb-4"><time datetime=2021-09-12 class=text-body-secondary>Sunday, September 12, 2021</time></div></div><div class=td-content><h1 id=pg-75c3e4903ec95b7eb75f4e72e42ee553>Navigating Cloud-Native Security - Lessons from a Recent Container Service Vulnerability</h1><div class="td-byline mb-4"><time datetime=2021-09-12 class=text-body-secondary>Sunday, September 12, 2021</time></div><p>The cloud-native landscape is constantly evolving, bringing immense benefits in agility and scale. However, with this evolution comes a complex and ever-changing threat landscape. Recently, a <a href=https://unit42.paloaltonetworks.com/azure-container-instances>significant vulnerability was reported by Unit 42 concerning Azure Container Instances (ACI)</a>, a service designed to run containers in a multi-tenant environment. This incident offers valuable lessons for the entire community, and we at Gardener believe in sharing insights that can help strengthen collective security.</p><p>This particular vulnerability underscores the critical importance of vigilance, timely patching, and defense-in-depth, principles we have long championed within the Gardener project.</p><h3 id=understanding-the-aci-vulnerability>Understanding the ACI Vulnerability<a class=td-heading-self-link href=#understanding-the-aci-vulnerability aria-label="Heading self-link"></a></h3><p>As detailed in the Unit 42 report, the attack vector on ACI involved several stages, leveraging a combination of outdated software and architectural choices:</p><ol><li><strong>Outdated <code>runc</code>:</strong> The initial entry point exploited a version of <code>runc</code> from October 2016. This version was susceptible to <a href=https://nvd.nist.gov/vuln/detail/CVE-2019-5736>CVE-2019-5736</a>, a critical vulnerability allowing host takeover. This vulnerability was widely publicized in early 2019.</li><li><strong>Lateral Movement via Kubelet Impersonation:</strong> After gaining node access, the next step involved attempting to impersonate the Kubelet to interact with the Kubernetes API server. The ACI clusters were reportedly running Kubernetes versions (v1.8.x - v1.10.x) from 2017/2018, which were, in principle, vulnerable to such an attack.</li><li><strong>Exploiting a Custom Bridge Component:</strong> While the direct Kubelet impersonation didn&rsquo;t work as initially expected, investigators found that a custom bridge component, designed to abstract the underlying Kubernetes, became the next target. This proprietary component inadvertently held a service account token with <code>cluster-admin</code> privileges. By capturing this token, attackers could gain full control over the Kubernetes cluster.</li><li><strong>Control Plane Access:</strong> The report also noted that the API server appeared to be self-hosted, potentially allowing easier movement from the data plane to the control plane environment once cluster-admin privileges were obtained.</li><li><strong>Alternative Attack Vector:</strong> A second distinct attack vector, also leveraging the custom bridge component, was identified, pointing to another area where security hardening could have prevented compromise.</li></ol><h3 id=gardeners-proactive-security-posture>Gardener’s Proactive Security Posture<a class=td-heading-self-link href=#gardeners-proactive-security-posture aria-label="Heading self-link"></a></h3><p>The ACI incident highlights several threat vectors that the Gardener team has actively worked to mitigate over the years, often well in advance of them becoming widely exploited.</p><ul><li><strong>Timely Patching of Critical Vulnerabilities (e.g., <code>runc</code> CVE-2019-5736):</strong> When CVE-2019-5736 was pre-disclosed, the Gardener team treated it with utmost seriousness. We had announcements and patches prepared, rolling them out on the day of public disclosure. This rapid response is crucial for minimizing exposure to known high-severity vulnerabilities.</li><li><strong>Hardening Against Exploits:</strong> The Kubelet impersonation vector mentioned in the ACI report is particularly relevant to Gardener. The underlying Kubernetes vulnerability (CVE-2020-8558, tracked as Kubernetes issue <a href=https://github.com/kubernetes/kubernetes/issues/85867>#85867</a>) that could allow a compromised node/Kubelet to redirect API server traffic (like <code>kubectl exec</code>) was discovered and reported by Alban Crequy from Kinvolk. This discovery was made during a penetration test commissioned by the Gardener project, specifically asking to find loopholes in our seed clusters. We were able to implement mitigations in Gardener even before the upstream Kubernetes fix was available, further securing our seed cluster architecture. The second distinct attack vector was also discovered during such a penetration test and Gardener further hardened its network policies.</li><li><strong>Principle of Least Privilege and Secure Component Design:</strong> The ACI bridge component&rsquo;s <code>cluster-admin</code> token is a stark reminder of the dangers of overly privileged components, especially those interacting with user workloads. Within Gardener, we&rsquo;ve invested heavily in mechanisms like the Gardener Seed Authorizer (as discussed in Gardener issue <a href=https://github.com/gardener/gardener/issues/1723>#1723</a>). It goes beyond standard RBAC to strictly limit the capabilities of components and prevent lateral movement, ensuring that even if one part is compromised, the blast radius is contained. We also meticulously review and restrict permissions for all components.</li><li><strong>Strict Separation of Concerns:</strong> A core architectural principle in Gardener is the strict separation between the control plane and the data plane - at all levels. Being an administrator in a shoot cluster does not grant any access to the underlying seed cluster&rsquo;s control plane execution environment or the upper hierarchy of runtime and garden cluster, a critical defense against escalation.</li></ul><h3 id=learning-and-moving-forward>Learning and Moving Forward<a class=td-heading-self-link href=#learning-and-moving-forward aria-label="Heading self-link"></a></h3><p>The ACI vulnerability is a powerful reminder that security is not a one-time task but a continuous process of vigilance, proactive hardening, and learning from every incident, whether our own or others&rsquo;. No system is impenetrable, and the assumption that any single entity, regardless of size, has perfected security can lead to complacency.</p><p>At Gardener, we remain committed to:</p><ul><li><strong>Staying current:</strong> Diligently updating dependencies and core components.</li><li><strong>Defense-in-depth:</strong> Implementing multiple layers of security controls.</li><li><strong>Proactive discovery:</strong> Continuously testing and seeking out potential weaknesses.</li><li><strong>Community collaboration:</strong> Sharing knowledge and contributing to upstream security efforts.</li></ul><p>We believe that by fostering a culture of security awareness and investing in robust, layered defenses, we can build more resilient cloud-native systems for everyone. This recent industry event, while unfortunate for those affected, provides crucial learning points that reinforce our commitment to the security principles embedded in Gardener. We will continue to evolve Gardener&rsquo;s security posture, always striving to stay ahead of emerging threats.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-daf36bb623effc564b516c113aca74f8>Happy Anniversary, Gardener! Three Years of Open Source Kubernetes Management</h1><div class="td-byline mb-4"><time datetime=2021-02-01 class=text-body-secondary>Monday, February 01, 2021</time></div><p>Happy New Year Gardeners!
As we greet 2021, we also celebrate Gardener’s third anniversary. Gardener was born with its first open source
<a href=https://github.com/gardener/gardener/commit/d9619d01845db8c7105d27596fdb7563158effe1>commit</a>
on 10.1.2018 (its inception within SAP was of course some 9 months earlier):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>commit d9619d01845db8c7105d27596fdb7563158effe1
</span></span><span style=display:flex><span>Author: Gardener Development Community &lt;gardener.opensource@sap.com&gt;
</span></span><span style=display:flex><span>Date:   Wed Jan 10 13:07:09 2018 +0100
</span></span><span style=display:flex><span>    Initial version of gardener
</span></span><span style=display:flex><span>    This is the initial contribution to the Open Source Gardener project.
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>Looking back, three years down the line, the project initiators were working towards a special goal: Publishing Gardener as an open source project on Github.com.
Join us as we look back at how it all began, the challenges Gardener aims to solve, and why open source and the community was and is the project’s key enabler.</p><h2 id=gardener-kick-off-we-opted-to-build-ourselves>Gardener Kick-Off: “We opted to BUILD ourselves”<a class=td-heading-self-link href=#gardener-kick-off-we-opted-to-build-ourselves aria-label="Heading self-link"></a></h2><p>Early 2017, SAP put together a small, jelled team of experts with a clear mission: work out how SAP could serve Kubernetes based
environments (as a service) for all teams within the company. Later that same year, SAP also joined the <a href=https://www.cncf.io/>CNCF</a> as a platinum member.</p><p>We first deliberated intensively on the BUY options (including acquisitions, due to the size and estimated volume needed at SAP). There were some early products from commercial vendors and startups available that did not bind exclusively to one of the hyperscalers, but these products did not cover many of our crucial and immediate requirements for a multi-cloud environment.</p><p>Ultimately, we opted to BUILD ourselves. This decision was not made lightly, because right from the start, we knew that we would have to cover thousands of clusters, across the globe, on all kinds of infrastructures. We would have to be able to create them at scale as well as manage them 24x7. And thus, we predicted the need to invest into automation of all aspects, to keep the service TCO at a minimum, and to offer an enterprise worthy SLA early on. This particular endeavor grew into launching the project Gardener, first internally, and ultimately fulfilling all checks, externally based on open source.
Its mission statement, in a nutshell, is “Universal Kubernetes at scale”.
Now, that’s quite bold. But we also had a nifty innovation that helped us tremendously along the way. And we can openly reveal the secret here: Gardener was built, not only for creating Kubernetes at scale, but it was built (recursively) in Kubernetes itself.</p><h2 id=what-do-you-get-with-gardener>What Do You Get with Gardener?<a class=td-heading-self-link href=#what-do-you-get-with-gardener aria-label="Heading self-link"></a></h2><p>Gardener offers managed and homogenous Kubernetes clusters on IaaS providers like <em>AWS, Azure, GCP, AliCloud, Open Telekom Cloud, SCS, OVH</em> and more, but also covers versatile infrastructures like <em>OpenStack, VMware</em> or <em>bare metal</em>. Day-1 and Day-2 operations are an integral part of a cluster’s feature set. This means that Gardener is not only capable of provisioning or de-provisioning thousands of clusters, but also of monitoring your cluster’s health state, upgrading components in a rolling fashion, or scaling the control plane as well as worker nodes up and down depending on the current resource demand.</p><p>Some features mentioned above might sound familiar to you, simply because they’re squarely derived from Kubernetes. Concretely, if you explore a Gardener managed end-user cluster, you’ll never see the so-called “control plane components” (<em>Kube-Apiserver, Kube-Controller-Manager, Kube-Scheduler, etc.</em>) The reason is that they run as Pods inside another, hosting/seeding Kubernetes cluster. Speaking in Gardener terms, the latter is called a <em>Seed</em> cluster, and the end-user cluster is called a <em>Shoot</em> cluster; and thus the botanical naming scheme for Gardener was born. Further assets like infrastructure components or worker machines are modelled as managed Kubernetes objects too. This allows Gardener to leverage all the great and production proven features of Kubernetes for managing Kubernetes clusters. Our <a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>blog post</a> on Kubernetes.io reveals more details about the architectural refinements.</p><img title="Figure 1: Gardener architecture overview" src=/blog/2021/images/gardener-01.png style=width:90%;height:auto><figcaption style=text-align:center;margin-top:0;margin-bottom:30px;font-size:90%>Figure 1: Gardener architecture overview</figcaption><p>End-users directly benefit from Gardener’s recursive architecture. Many of the requirements that we identified for the Gardener service turned out to be highly convenient for shoot owners. For instance, Seed clusters are usually equipped with <em>DNS</em> and <em>x509</em> services. At the same time, these service offerings can be extended to requests coming from the Shoot clusters i.e., end-users get domain names and certificates for their applications out of the box.</p><h2 id=recognizing-the-power-of-open-source>Recognizing the Power of Open Source<a class=td-heading-self-link href=#recognizing-the-power-of-open-source aria-label="Heading self-link"></a></h2><p>The Gardener team immediately profited from open source: from Kubernetes obviously, and all its ecosystem projects. That all facilitated our project’s very fast and robust development. But it does not answer:</p><p>“Why would SAP open source a tool that clearly solves a monetizable enterprise requirement?"_</p><p>Short spoiler alert: it initially involved a leap of faith. If we just look at our own decision path, it is undeniable that developers, and with them entire industries, gravitate towards open source. We chose Linux, Containers, and Kubernetes exactly because they are open, and we could bet on network effects, especially around skills. The same decision process is currently replicated in thousands of companies, with the same results. Why? Because all companies are digitally transforming. They are becoming software companies as well to a certain extent. Many of them are also our customers and in many discussions, we recognized that they have the same challenges that we are solving with Gardener. This, in essence, was a key eye opener. We were confident that if we developed Gardener as open source, we’d not only seize the opportunity to shape a Kubernetes management tool that finds broad interest and adoption outside of our use case at SAP, but we could solve common challenges faster with the help of a community, and that in consequence would sustain continuous feature development.</p><p>Coincidently, that was also when the <em>SAP Open Source Program Office (OSPO)</em> was launched. It supported us making a case to develop Gardener completely as open source.
Today, we can witness that this strategy has unfolded. It opened the gates not only for adoption, but for co-innovation, investment security, and user feedback directly in code. Below you can see an example of how the Gardener project benefits from this external community power as contributions are submitted right away.</p><img title="Figure 2: Example immediate community contribution" src=/blog/2021/images/gardener-02.png style=width:90%;height:auto><figcaption style=text-align:center;margin-top:0;margin-bottom:30px;font-size:90%>Figure 2: Example immediate community contribution</figcaption><h2 id=differentiating-gardener-from-other-kubernetes-management-solutions>Differentiating Gardener from Other Kubernetes Management Solutions<a class=td-heading-self-link href=#differentiating-gardener-from-other-kubernetes-management-solutions aria-label="Heading self-link"></a></h2><p>Imagine that you have created a modern solid cloud native app or service, fully scalable, in containers. And the business case requires you to run the service on multiple clouds, like <em>AWS, AliCloud, Azure</em>, &mldr; maybe even on-premises like <em>OpenStack</em> or <em>VMware</em>. Your development team has done everything to ensure that the workload is highly portable. But they would need to qualify each providers’ managed Kubernetes offering and their custom <em>Bill-of-Material (BoM)</em>, their versions, their deprecation plan, roadmap etc. Your TCD would explode and this is exactly what teams at SAP experienced. Now, with Gardener you can, instead, roll out homogeneous clusters and stay in control of your versions and a single roadmap. Across all supported providers!</p><p>Also, teams that have serious, or say, more demanding workloads running on Kubernetes will come to the same conclusion: They require the full management control of the Kubernetes underlay. Not only that, they need access, visibility, and all the tuning options for the control plane to safeguard their service. This is a conclusion not only from teams at SAP, but also from our community members, like <em>PingCap</em>, who use Gardener to serve <em>TiDB Cloud service</em>. Whenever you need to get serious and need more than one or two clusters, Gardener is your friend.</p><h2 id=who-is-using-gardener>Who Is Using Gardener?<a class=td-heading-self-link href=#who-is-using-gardener aria-label="Heading self-link"></a></h2><p>Well, there is SAP itself of course, but also the number of Gardener adopters and companies interested in Gardener is growing (~1700 GitHub stars), as more are challenged by multi-cluster and multi-cloud requirements.</p><p><em>Flant, PingCap, STACKIT, T-Systems, Sky</em>, or <em>b’nerd</em> are among these companies, to name a few. They use Gardener to either run products they sell on top or offer managed Kubernetes clusters directly to their clients, or even only components that are re-usable from Gardener.</p><p>An interesting journey in the open source space started with <em>Finanz Informatik Technologie Service (FI-TS)</em>, an European Central Bank regulated and certified hoster for banks. They operate in very restricted environments, as you can imagine, and as such, they re-designed their datacenter for cloud native workloads from scratch, that is from cabling, racking and stacking to an API that serves bare metal servers.
For Kubernetes-as-a-Service, they evaluated and chose Gardener because it was open and a perfect candidate. With Gardener’s extension capabilities, it was possible to bring managed Kubernetes clusters to their very own bare metal stack, <a href=https://metal-stack.io/>metal-stack.io</a>.
Of course, this meant implementation effort. But by reusing the Gardener project, <em>FI-TS</em> was able to leverage our standard with minimal adjustments for their special use-case. Subsequently, with their contributions, SAP was able to make Gardener more open for the community.</p><h2 id=full-speed-ahead-with-the-community-in-2021>Full Speed Ahead with the Community in 2021<a class=td-heading-self-link href=#full-speed-ahead-with-the-community-in-2021 aria-label="Heading self-link"></a></h2><p>Some of the current and most active topics are about the installer (<a href=https://github.com/gardener/landscaper>Landscaper</a>),
<a href=https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md>control plane migration</a>,
<a href=https://github.com/gardener/gardener/blob/master/docs/proposals/13-automated-seed-management.md>automated seed management</a> and
documentation. Even though once you are into Kubernetes and then Gardener, all complexity falls into place, you can make all the semantic connections yourself. But beginners that join the community without much prior knowledge should experience a ramp-up with slighter slope. And that is currently a pain point. Experts directly ask questions about documentation not being up-to-date or clear enough. We prioritized the functionality of what you get with Gardener at the outset and need to catch up.
But here is the good part: Now that we are starting the installation subject, later we will have a much broader picture of what we need to install and maintain Gardener, and how we will build it.</p><span style=display:block;text-align:center><img title="Figure 3: Gardener Landscaper" src=/blog/2021/images/gardener-03.png style=width:10%;height:auto></span><p>In a <a href="https://www.youtube.com/watch?v=pGlHpJle7Zk">community call</a> last summer, we gave an overview of what we are building:
The Landscaper. With this tool, we will be able to not only install a full Gardener landscape, but we will also streamline patches, updates and upgrades with the Landscaper. Gardener adopters can then attach to a release train from the project and deploy Gardener into a dev, canary and multiple production environments sequentially. Like we do at SAP.</p><h2 id=key-takeaways-in-three-years-of-gardener>Key Takeaways in Three Years of Gardener<a class=td-heading-self-link href=#key-takeaways-in-three-years-of-gardener aria-label="Heading self-link"></a></h2><h3 id=1-open-source-is-strategic>#1 Open Source is Strategic<a class=td-heading-self-link href=#1-open-source-is-strategic aria-label="Heading self-link"></a></h3><p>Open Source is not just about using freely available libraries, components, or tools to optimize your own software production anymore. It is strategic, unfolds for projects like Gardener, and that in the meantime has also reached the Board Room.</p><h3 id=2-solving-concrete-challenges-by-co-innovation>#2 Solving Concrete Challenges by Co-Innovation<a class=td-heading-self-link href=#2-solving-concrete-challenges-by-co-innovation aria-label="Heading self-link"></a></h3><p>Users of a particular product or service increasingly vote/decide for open source variants, such as project Gardener, because that allows them to freely innovate and solve concrete challenges by developing exactly what they require (see FI-TS example). This user-centric process has tremendous advantages. It clears out the middleman and other vested interests. You have access to the full code. And lastly, if others start using and contributing to your innovation, it allows enterprises to secure their investments for the long term. And that re-enforces point #1 for enterprises that have yet to create a strategic Open Source Program Office.</p><h3 id=3-cloud-native-skills>#3 Cloud Native Skills<a class=td-heading-self-link href=#3-cloud-native-skills aria-label="Heading self-link"></a></h3><p>Gardener solves problems by applying Kubernetes and Kubernetes principles itself. Developers and operators who obtain familiarity with Kubernetes will immediately notice and appreciate our concept and can contribute intuitively. The Gardener maintainers feel responsible to facilitate community members and contributors. Barriers will further be reduced by our ongoing landscaper and documentation efforts. This is why we are so confident on <a href=/adopter/>Gardener adoption</a>.</p><p>The Gardener team is gladly welcoming new community members, especially regarding adoption and contribution. Feel invited to try out your very own Gardener installation, join our <a href=https://gardener-cloud.slack.com/>Slack workspace</a> or <a href=/docs/contribute/#bi-weekly-meetings>community calls</a>. We’re looking forward to seeing you there!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4be922e512e85f99936a577a28dfe062>Machine Controller Manager</h1><div class="td-byline mb-4"><time datetime=2021-01-25 class=text-body-secondary>Monday, January 25, 2021</time></div><p>Kubernetes is a cloud-native enabler built around the principles for a resilient, manageable, observable, highly automated, loosely coupled system. We know that Kubernetes is infrastructure agnostic with the help of a provider specific <a href=https://kubernetes.io/docs/concepts/architecture/cloud-controller/>Cloud Controller Manager</a>. But Kubernetes has explicitly externalized the management of the nodes. Once they appear - correctly configured - in the cluster, Kubernetes can use them. If nodes fail, Kubernetes can&rsquo;t do anything about it, external tooling is required. But every tool, every provider is different. So, why not elevate node management to a first class Kubernetes citizen? Why not create a Kubernetes native resource that manages machines just like pods? Such an approach is brought to you by the <a href=https://github.com/gardener/machine-controller-manager>Machine Controller Manager</a> (aka MCM), which, of course, is an open sourced project. MCM gives you the following benefits:</p><ul><li>seamlessly manage machines/nodes with a declarative API (of course, across different cloud providers)</li><li>integrate generically with the <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>cluster autoscaler</a></li><li>plugin with tools such as the <a href=https://github.com/kubernetes/node-problem-detector>node-problem-detector</a></li><li>transport the immutability design principle to machine/nodes</li><li>implement e.g. rolling upgrades of machines/nodes</li></ul><h2 id=machine-controller-manager-aka-mcm>Machine Controller Manager aka MCM<a class=td-heading-self-link href=#machine-controller-manager-aka-mcm aria-label="Heading self-link"></a></h2><p><a href=https://github.com/gardener/machine-controller-manager>Machine Controller Manager</a> is a group of cooperative controllers that manage the lifecycle of the worker machines. It is inspired by the design of Kube Controller Manager in which various sub controllers manage their respective Kubernetes Clients.</p><p>Machine Controller Manager reconciles a set of Custom Resources namely <code>MachineDeployment</code>, <code>MachineSet</code> and <code>Machines</code> which are managed & monitored by their controllers <code>MachineDeployment Controller</code>, <code>MachineSet Controller</code>, <code>Machine Controller</code> respectively along with another cooperative controller called the <code>Safety Controller</code>.</p><h2 id=understanding-the-sub-controllers-and-custom-resources-of-mcm>Understanding the sub-controllers and Custom Resources of MCM<a class=td-heading-self-link href=#understanding-the-sub-controllers-and-custom-resources-of-mcm aria-label="Heading self-link"></a></h2><p>The Custom Resources <code>MachineDeployment</code>, <code>MachineSet</code> and <code>Machines</code> are very much analogous to the native K8s resources of <code>Deployment</code>, <code>ReplicaSet</code> and <code>Pods</code> respectively. So, in the context of MCM:</p><ul><li><code>MachineDeployment</code> provides a declarative update for <code>MachineSet</code> and <code>Machines</code>. <code>MachineDeployment Controller</code> reconciles the <code>MachineDeployment</code> objects and manages the lifecycle of <code>MachineSet</code> objects. <code>MachineDeployment</code> consumes a provider specific <code>MachineClass</code> in its <code>spec.template.spec</code>, which is the template of the VM spec that would be spawned on the cloud by MCM.</li><li><code>MachineSet</code> ensures that the specified number of <code>Machine</code> replicas are running at a given point of time. <code>MachineSet Controller</code> reconciles the <code>MachineSet</code> objects and manages the lifecycle of <code>Machine</code> objects.</li><li><code>Machines</code> are the actual VMs running on the cloud platform provided by one of the supported cloud providers. <code>Machine Controller</code> is the controller that actually communicates with the cloud provider to create/update/delete machines on the cloud.</li><li>There is a <code>Safety Controller</code> responsible for handling the unidentified or unknown behaviours from the cloud providers.</li><li>Along with the above Custom Controllers and Resources, MCM requires the <code>MachineClass</code> to use K8s <code>Secret</code> that stores cloudconfig (initialization scripts used to create VMs) and cloud specific credentials.</li></ul><h2 id=workings-of-mcm>Workings of MCM<a class=td-heading-self-link href=#workings-of-mcm aria-label="Heading self-link"></a></h2><img title="Figure 1: In-Tree Machine Controller Manager" src=/blog/2021/images/mcm-00.png style=width:90%;height:auto><figcaption style=text-align:center;margin-top:0;margin-bottom:30px;font-size:90%>Figure 1: In-Tree Machine Controller Manager</figcaption><p>In MCM, there are two K8s clusters in the scope — a <em>Control Cluster</em> and a <em>Target Cluster</em>. The Control Cluster is the K8s cluster where the MCM is installed to manage the machine lifecycle of the Target Cluster. In other words, the Control Cluster is the one where the machine-* objects are stored. The Target Cluster is where all the node objects are registered. These clusters can be two distinct clusters or the same cluster, whichever fits.</p><p>When a <code>MachineDeployment</code> object is created, the <code>MachineDeployment Controller</code> creates the corresponding <code>MachineSet</code> object. The <code>MachineSet Controller</code> in-turn creates the <code>Machine</code> objects. The <code>Machine Controller</code> then talks to the cloud provider API and actually creates the VMs on the cloud.</p><p>The cloud initialization script that is introduced into the VMs via the K8s <code>Secret</code> consumed by the <code>MachineClasses</code> talks to the KCM (K8s Controller Manager) and creates the node objects. After registering themselves to the Target Cluster, nodes start sending health signals to the machine objects. That is when MCM updates the status of the machine object from <code>Pending</code> to <code>Running</code>.</p><h2 id=more-on-safety-controller>More on Safety Controller<a class=td-heading-self-link href=#more-on-safety-controller aria-label="Heading self-link"></a></h2><p>Safety Controller contains the following functions:</p><h3 id=orphan-vm-handling>Orphan VM Handling<a class=td-heading-self-link href=#orphan-vm-handling aria-label="Heading self-link"></a></h3><ul><li>It lists all the VMs in the cloud; matching the tag of given cluster name and maps the VMs with the <code>Machine</code> objects using the <code>ProviderID</code> field. VMs without any backing <code>Machine</code> objects are logged and deleted after confirmation.</li><li>This handler runs every 30 minutes and is configurable via <code>--machine-safety-orphan-vms-period</code> flag.</li></ul><h3 id=freeze-mechanism>Freeze Mechanism<a class=td-heading-self-link href=#freeze-mechanism aria-label="Heading self-link"></a></h3><ul><li><code>Safety Controller</code> freezes the <code>MachineDeployment</code> and <code>MachineSet controller</code> if the number of <code>Machine</code> objects goes beyond a certain threshold on top of the <code>Spec.Replicas</code>. It can be configured by the flag <code>--safety-up</code> or <code>--safety-down</code> and also <code>--machine-safety-overshooting-period</code>.</li><li><code>Safety Controller</code> freezes the functionality of the MCM if either of the <code>target-apiserver</code> or the <code>control-apiserver</code> is not reachable.</li><li><code>Safety Controller</code> unfreezes the MCM automatically once situation is resolved to normal. A <code>freeze</code> label is applied on <code>MachineDeployment</code>/<code>MachineSet</code> to enforce the freeze condition.</li></ul><h2 id=evolution-of-mcm-from-in-tree-to-out-of-tree-oot>Evolution of MCM from In-Tree to Out-of-Tree (OOT)<a class=td-heading-self-link href=#evolution-of-mcm-from-in-tree-to-out-of-tree-oot aria-label="Heading self-link"></a></h2><p>MCM supports declarative management of machines in a K8s Cluster on various cloud providers like AWS, Azure, GCP, AliCloud, OpenStack, Metal-stack, Packet, KubeVirt, VMWare, Yandex. It can, of course, be easily extended to support other cloud providers.</p><p>Going ahead, having the implementation of the Machine Controller Manager supporting too many cloud providers would be too much upkeep from both a development and a maintenance point of view. Which is why the <code>Machine Controller</code> component of MCM has been moved to Out-of-Tree design, where the <code>Machine Controller</code> for each respective cloud provider runs as an independent executable, even though typically packaged under the same deployment.</p><img title="Figure 2: Out-Of-Tree Machine Controller Manager" src=/blog/2021/images/mcm-01.png style=width:90%;height:auto><figcaption style=text-align:center;margin-top:0;margin-bottom:30px;font-size:90%>Figure 2: Out-Of-Tree (OOT) Machine Controller Manager</figcaption><p>This OOT Machine Controller will implement a common interface to manage the VMs on the respective cloud provider. Now, while the <code>Machine Controller</code> deals with the <code>Machine</code> objects, the Machine Controller Manager (MCM) deals with higher level objects such as the <code>MachineSet</code> and <code>MachineDeployment</code> objects.</p><p>A lot of contributions are already being made towards an OOT Machine Controller Manager for various cloud providers. Below are the links to the repositories:</p><ul><li><a href=https://github.com/gardener/machine-controller-manager-provider-alicloud>Out of Tree Machine Controller Manager for AliCloud</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-aws>Out of Tree Machine Controller Manager for AWS</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-azure>Out of Tree Machine Controller Manager for Azure</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-gcp>Out of Tree Machine Controller Manager for GCP</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-kubevirt>Out of Tree Machine Controller Manager for KubeVirt</a></li><li><a href=https://github.com/metal-stack/machine-controller-manager-provider-metal>Out of Tree Machine Controller Manager for Metal</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-vsphere>Out of Tree Machine Controller Manager for vSphere</a></li><li><a href=https://github.com/gardener/machine-controller-manager-provider-yandex>Out of Tree Machine Controller Manager for Yandex</a></li></ul><p>Watch the <a href=https://youtu.be/p9BJRpdkxjU>Out of Tree Machine Controller Manager</a> video on our <a href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw>Gardener Project</a> YouTube channel to understand more about OOT MCM.</p><h2 id=who-uses-mcm>Who Uses MCM?<a class=td-heading-self-link href=#who-uses-mcm aria-label="Heading self-link"></a></h2><p><strong><a href=http://gardener.cloud>Gardener</a></strong></p><p>MCM is originally developed and employed by a K8s Control Plane as a Service called Gardener. However, the MCM’s design is elegant enough to be employed when managing the machines of any independent K8s clusters, without having to necessarily associate it with Gardener.</p><p><strong><a href=https://metal-stack.io>Metal Stack</a></strong></p><p>Metal-stack is a set of microservices that implements Metal as a Service (MaaS). It enables you to turn your hardware into elastic cloud infrastructure. Metal-stack employs the <a href=https://github.com/metal-stack/machine-controller-manager-provider-metal>adopted</a> Machine Controller Manager to their Metal API. Check out an introduction to it in <a href="https://www.youtube.com/watch?v=XE-Kpyn8x2k">metal-stack - kubernetes on bare metal</a>.</p><p><strong><a href=http://sky.com>Sky UK Limited</a></strong></p><p>Sky UK Limited (a broadcaster) migrated their Kubernetes node management from Ansible to Machine Controller Manager. Check out the <a href=https://youtu.be/yF4wq7GAeEM>How Sky is using Machine Controller Manager (MCM) and autoscaler</a> video on our <a href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw>Gardener Project</a> YouTube channel.</p><p>Also, other interesting use cases with MCM are implemented by Kubernetes enthusiasts, who for example adjusted the Machine Controller Manager to provision machines in the cloud to extend a local Raspberry-Pi K3s cluster. This topic is covered in detail in the <a href="https://youtu.be/UuveyEOn4_o?t=60">2020-07-03 Gardener Community Meeting</a> on our <a href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw>Gardener Project</a> YouTube channel.</p><h2 id=conclusion>Conclusion<a class=td-heading-self-link href=#conclusion aria-label="Heading self-link"></a></h2><p>Machine Controller Manager is the leading automation tool for machine management for, and in, Kubernetes. And the best part is that it is open sourced. It is freely (and easily) usable and extensible, and the community more than welcomes contributions.</p><p>If you want to know more about Machine Controller Manager or find out about a similar scope for your solutions, feel free to visit the GitHub page <a href=https://github.com/gardener/machine-controller-manager>machine-controller-manager</a>. We are so excited to see what you achieve with Machine Controller Manager.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b3872a3cc0ac6391342f662d9d5d7287>2020</h1><div class="td-byline mb-4"><time datetime=2020-12-03 class=text-body-secondary>Thursday, December 03, 2020</time></div></div><div class=td-content><h1 id=pg-a1be1a6314d8a64289c7b8ee26aa6a98>STACKIT Kubernetes Engine with Gardener</h1><div class="td-byline mb-4"><time datetime=2020-12-03 class=text-body-secondary>Thursday, December 03, 2020</time></div><p><a href=https://stackit.de/en/>STACKIT</a> is a digital brand of Europe’s biggest retailer, the Schwarz Group, which consists of Lidl, Kaufland, as well as production and recycling companies. Following the industry trend, the Schwarz Group is in the process of a digital transformation. STACKIT enables this transformation by helping to modernize the internal IT of the company branches.</p><h2 id=what-is-stackit-and-the-stackit-kubernetes-engine-ske>What is STACKIT and the STACKIT Kubernetes Engine (SKE)?<a class=td-heading-self-link href=#what-is-stackit-and-the-stackit-kubernetes-engine-ske aria-label="Heading self-link"></a></h2><p>STACKIT started with colocation solutions for internal and external customers in Europe-based data centers, which was then expanded to a full cloud platform stack providing an IaaS layer with VMs, storage and network, as well as a PaaS layer including Cloud Foundry and a growing set of cloud services, like databases, messaging, etc.</p><p>With containers and Kubernetes becoming the lingua franca of the cloud, we are happy to announce the <em>STACKIT Kubernetes Engine (SKE)</em>, which has been released as Beta in November this year. We decided to use Gardener as the cluster management engine underneath SKE - for good reasons as you will see – and we would like to share our experiences with Gardener when working on the SKE Beta release, and serve as a testimonial for this technology.</p><img title="Figure 1: STACKIT Component Diagram" src=/blog/2020/images/00.png style=width:90%;height:auto><figcaption style=text-align:center;margin-top:0;margin-bottom:30px;font-size:90%>Figure 1: STACKIT Component Diagram</figcaption><h2 id=why-we-chose-gardener-as-a-cluster-management-tool>Why We Chose Gardener as a Cluster Management Tool<a class=td-heading-self-link href=#why-we-chose-gardener-as-a-cluster-management-tool aria-label="Heading self-link"></a></h2><p>We started with the Kubernetes endeavor in the beginning of 2020 with a newly formed agile team that consisted of software engineers, highly experienced in IT operations and development. After some exploration and a short conceptual phase, we had a clear-cut opinion on how the cluster management for STACKIT should look like: we were looking for a highly customizable tool that could be adapted to the specific needs of STACKIT and the Schwarz Group, e.g. in terms of network setup or the infrastructure layer it should be running on. Moreover, the tool should be scalable to a high number of managed Kubernetes clusters and should therefore provide a fully automated operation experience. As an open source project, contributing and influencing the tool, as well as collaborating with a larger community were important aspects that motivated us. Furthermore, we aimed to offer cluster management as a self-service in combination with an excellent user experience. Our objective was to have the managed clusters come with enterprise-grade SLAs – i.e. with “batteries included”, as some say.</p><p>With this mission, we started our quest through the world of Kubernetes and soon found Gardener to be a hot candidate of cluster management tools that seemed to fulfill our demands. We quickly got in contact and received a warm welcome from the Gardener community. As an interested potential adopter, but in the early days of the COVID-19 lockdown, we managed to organize an online workshop during which we got an introduction and deep dive into Gardener and discussed the STACKIT use cases. We learned that Gardener is extensible in many dimensions, and that contributions are always welcome and encouraged. Once we understood the basic Gardener concepts of Garden, Shoot and Seed clusters, its inception design and how this extends Kubernetes concepts in a natural way, we were eager to evaluate this tool in more detail.</p><p>After this evaluation, we were convinced that this tool fulfilled all our requirements - a decision was made and off we went.</p><h2 id=how-gardener-was-adapted-and-extended-by-ske>How Gardener was Adapted and Extended by SKE<a class=td-heading-self-link href=#how-gardener-was-adapted-and-extended-by-ske aria-label="Heading self-link"></a></h2><p>After becoming familiar with Gardener, we started to look into its code base to adapt it to the specific needs of the STACKIT OpenStack environment. Changes and extensions were made in order to get it integrated into the STACKIT environment, and whenever reasonable, we contributed those changes back:</p><ul><li>To run smoothly with the STACKIT OpenStack layer, the Gardener configuration was adapted in different places, e.g. to support CSI driver or to configure the domains of a shoot API server or ingress.</li><li>Gardener was extended to support shoots and shooted seeds in dual stack and dual home setup. This is used in SKE for the communication between shooted seeds and the Garden cluster.</li><li>SKE uses a private image registry for the Gardener installation in order to resolve dependencies to public image registries and to have more control over the used Gardener versions. To install and run Gardener with the private image registry, some new configurations need to be introduced into Gardener.</li><li>Gardener is a first-class API based service what allowed us to smoothly integrate it into the STACKIT User Interface. We were also able to jump-start and utilize the Gardener Dashboard for our Beta release by merely adjusting the look-&-feel, i.e. colors, labels and icons.</li></ul><img title="Figure 2: Gardener Dashboard adapted to STACKIT UI style" src=/blog/2020/images/01.png style=width:90%;height:auto><figcaption style=text-align:center;margin-top:0;margin-bottom:30px;font-size:90%>Figure 2: Gardener Dashboard adapted to STACKIT UI style</figcaption><h2 id=experience-with-gardener-operations>Experience with Gardener Operations<a class=td-heading-self-link href=#experience-with-gardener-operations aria-label="Heading self-link"></a></h2><p>As no OpenStack installation is identical to one another, getting Gardener to run stable on the STACKIT IaaS layer revealed some operational challenges. For instance, it was challenging to find the right configuration for Cinder CSI.</p><p>To test for its resilience, we tried to break the managed clusters with a Chaos Monkey test, e.g. by deleting services or components needed by Kubernetes and Gardener to work properly. The reconciliation feature of Gardener fixed all those problems automatically, so that damaged Shoot clusters became operational again after a short period of time. Thus, we were not able to break Shoot clusters from an end user perspective permanently, despite our efforts. Which again speaks for Gardener’s first-class cloud native design.</p><p>We also participated in a fruitful community support: For several challenges we contacted the community channel and help was provided in a timely manner. A lesson learned was that raising an issue in the community early on, before getting stuck too long on your own with an unresolved problem, is essential and efficient.</p><h2 id=summary>Summary<a class=td-heading-self-link href=#summary aria-label="Heading self-link"></a></h2><p>Gardener is used by SKE to provide a managed Kubernetes offering for internal use cases of the Schwarz Group as well as for the public cloud offering of STACKIT. Thanks to Gardener, it was possible to get from zero to a Beta release in only about half a year’s time – this speaks for itself. Within this period, we were able to integrate Gardener into the STACKIT environment, i.e. in its OpenStack IaaS layer, its management tools and its identity provisioning solution.</p><p>Gardener has become a vital building block in STACKIT&rsquo;s cloud native platform offering. For the future, the possibility to manage clusters also on other infrastructures and hyperscalers is seen as another great opportunity for extended use cases. The open co-innovation exchange with the Gardener community member companies has also opened the door to commercial co-operation.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cb063b2bc4e99b5fee018d6ebf27e0e8>Gardener v1.13 Released</h1><div class="td-byline mb-4"><time datetime=2020-11-23 class=text-body-secondary>Monday, November 23, 2020</time></div><p>Dear community, we&rsquo;re happy to announce a new minor release of Gardener, in fact, the 16th in 2020!
v1.13 came out just today after a couple of weeks of code improvements and feature implementations.
As usual, this blog post provides brief summaries for the most notable changes that we introduce with this version.
Behind the scenes (and not explicitly highlighted below) we are progressing on internal code restructurings and refactorings to ease further extensions and to enhance development productivity.
Speaking of those: You might be interested in watching <a href="https://www.youtube.com/watch?v=4sQs_Hj6xpY">the recording of the last Gardener Community Meeting</a> which includes a detailed session for <a href=https://github.com/gardener/terraformer/releases/tag/v2.0.0-rc.0>v2 of Terraformer</a>, a complete rewrite in Golang, and improved state handling.</p><h2 id=notable-changes-in-v113>Notable Changes in v1.13<a class=td-heading-self-link href=#notable-changes-in-v113 aria-label="Heading self-link"></a></h2><p>The main themes of Gardener&rsquo;s v1.13 release are increments for feature gate promotions, scalability and robustness, and cleanups and refactorings.
The community plans to continue on those and wants to deliver at least one more release in 2020.</p><h3 id=automatic-quotas-for-gardener-resources-gardenergardener3072>Automatic Quotas for Gardener Resources (<a href=https://github.com/gardener/gardener/pull/3072>gardener/gardener#3072</a>)<a class=td-heading-self-link href=#automatic-quotas-for-gardener-resources-gardenergardener3072 aria-label="Heading self-link"></a></h3><p>Gardener already supports <code>ResourceQuota</code>s since the last release, however, it was still up to operators/administrators to create these objects in project namespaces.
Obviously, in large Gardener installations with thousands of projects, this is a quite challenging task.
With this release, we are shipping an improvement in the <code>Project</code> controller in the gardener-controller-manager that allows operators to automatically create <code>ResourceQuota</code>s based on configuration.
Operators can distinguish via project label selectors which default quotas shall be defined for various projects.
Please find more details at <a href=https://github.com/gardener/gardener/blob/v1.13.0/docs/concepts/controller-manager.md#main-reconciler>Gardener Controller Manager</a>!</p><h3 id=resource-capacity-and-reservations-for-seeds-gardenergardener3075>Resource Capacity and Reservations for Seeds (<a href=https://github.com/gardener/gardener/pull/3075>gardener/gardener#3075</a>)<a class=td-heading-self-link href=#resource-capacity-and-reservations-for-seeds-gardenergardener3075 aria-label="Heading self-link"></a></h3><p>The larger the Gardener landscape, the more seed clusters you require.
Naturally, they have limits of how many shoots they can accommodate (based on constraints of the underlying infrastructure provider and/or seed cluster configuration).
Until this release, there were no means to prevent a seed cluster from becoming overloaded (and potentially die due to this load).
Now you define resource capacity and reservations in the <a href=https://github.com/gardener/gardener/blob/v1.13.0/example/20-componentconfig-gardenlet.yaml#L68-L70>gardenlet&rsquo;s component configuration</a>, similar to how the kubelet announces allocatable resources for <code>Node</code> objects.
We are <a href=https://github.com/gardener/gardener/blob/v1.13.0/charts/gardener/gardenlet/values.yaml#L100-L102>defaulting this to 250 shoots</a>, but you might want to adapt this value for your own environment.</p><h3 id=distributed-gardenlet-rollout-for-shooted-seeds-gardenergardener3135>Distributed Gardenlet Rollout for Shooted Seeds (<a href=https://github.com/gardener/gardener/pull/3135>gardener/gardener#3135</a>)<a class=td-heading-self-link href=#distributed-gardenlet-rollout-for-shooted-seeds-gardenergardener3135 aria-label="Heading self-link"></a></h3><p>With the same motivation, i.e., to improve catering with large landscapes, we allow operators to configure distributed rollouts of gardenlets for shooted seeds.
When a new Gardener version is being deployed in landscapes with a high number of shooted seeds, gardenlets of earlier versions were immediately re-deploying copies of themselves into the shooted seeds they manage.
This leads to a large number of new gardenlet pods that all roughly start at the same time.
Depending on the size of the landscape, this may trouble the gardener-apiservers as all of them are starting to fill their caches and create watches at the same time.
By default, this rollout is now randomized <a href=https://github.com/gardener/gardener/blob/v1.13.0/example/20-componentconfig-gardenlet.yaml#L63-L64>within a <code>5m</code> time window</a>, i.e., it may take up to <code>5m</code> until all gardenlets in all seeds have been updated.</p><h3 id=progressing-on-beta-promotion-for-apiserversni-feature-gate-gardenergardener3082-gardenergardener3143>Progressing on Beta-Promotion for <code>APIServerSNI</code> Feature Gate (<a href=https://github.com/gardener/gardener/pull/3082>gardener/gardener#3082</a>, <a href=https://github.com/gardener/gardener/pull/3143>gardener/gardener#3143</a>)<a class=td-heading-self-link href=#progressing-on-beta-promotion-for-apiserversni-feature-gate-gardenergardener3082-gardenergardener3143 aria-label="Heading self-link"></a></h3><p>The alpha <code>APIServerSNI</code> feature will drastically reduce the costs for load balancers in the seed clusters, thus, it is effectively contributing to Gardener&rsquo;s &ldquo;minimal TCO&rdquo; goal.
In this release we are introducing an important improvement that optimizes the connectivity when pods talk to their control plane by avoiding an extra network hop.
This is realized by a <code>MutatingWebhookConfiguration</code> whose server runs as a sidecar container in the kube-apiserver pod in the seed (only when the <code>APIServerSNI</code> feature gate is enabled).
The webhook injects a <code>KUBERNETES_SERVICE_HOST</code> environment variable into pods in the shoot which prevents the additional network hop to the <code>apiserver-proxy</code> on all worker nodes.
You can read more about it in <a href=https://github.com/gardener/gardener/blob/v1.13.0/docs/usage/apiserver-sni-injection.md>APIServerSNI environment variable injection</a>.</p><h3 id=more-control-plane-configurability-gardenergardener3141-gardenergardener3139>More Control Plane Configurability (<a href=https://github.com/gardener/gardener/pull/3141>gardener/gardener#3141</a>, <a href=https://github.com/gardener/gardener/pull/3139>gardener/gardener#3139</a>)<a class=td-heading-self-link href=#more-control-plane-configurability-gardenergardener3141-gardenergardener3139 aria-label="Heading self-link"></a></h3><p>A main capability beloved by Gardener users is its openness when it comes to configurability and fine-tuning of the Kubernetes control plane components.
Most managed Kubernetes offerings are not exposing options of the master components, but Gardener&rsquo;s <a href=https://github.com/gardener/gardener/blob/v1.13.0/example/90-shoot.yaml><code>Shoot</code> API</a> offers a selected set of settings.
With this release we are allowing to change the maximum number of (non-)mutating requests for the kube-apiserver of shoot clusters.
Similarly, the grace period before deleting pods on failed nodes can now be fine-grained for the kube-controller-manager.</p><h3 id=improved-project-resource-handling-gardenergardener3137-gardenergardener3136-gardenergardener3179>Improved <code>Project</code> Resource Handling (<a href=https://github.com/gardener/gardener/pull/3137>gardener/gardener#3137</a>, <a href=https://github.com/gardener/gardener/pull/3136>gardener/gardener#3136</a>, <a href=https://github.com/gardener/gardener/pull/3179>gardener/gardener#3179</a>)<a class=td-heading-self-link href=#improved-project-resource-handling-gardenergardener3137-gardenergardener3136-gardenergardener3179 aria-label="Heading self-link"></a></h3><p><code>Project</code>s are an important resource in the Gardener ecosystem as they enable collaboration with team members.
A couple of improvements have landed into this release.
Firstly, duplicates in the member list were not validated so far.
With this release, the gardener-apiserver is automatically merging them, and in future releases requests with duplicates will be denied.
Secondly, specific <code>Project</code>s may now be excluded from the <a href=https://github.com/gardener/gardener/blob/v1.13.0/docs/concepts/controller-manager.md#stale-projects-reconciler>stale checks</a> if desired.
Lastly, namespaces for <code>Project</code>s that were adopted (i.e., those that exist before the <code>Project</code> already) will now no longer be deleted when the <code>Project</code> is being deleted.
Please note that this only applies for newly created <code>Project</code>s.</p><h3 id=removal-of-deprecated-labels-and-annotations-gardenergardener3094>Removal of Deprecated Labels and Annotations (<a href=https://github.com/gardener/gardener/pull/3094>gardener/gardener#3094</a>)<a class=td-heading-self-link href=#removal-of-deprecated-labels-and-annotations-gardenergardener3094 aria-label="Heading self-link"></a></h3><p>The <code>core.gardener.cloud</code> API group succeeded the old <code>garden.sapcloud.io</code> API group in the beginning of 2020, however, a lot of labels and annotations with the old API group name were still supported.
We have continued with the process of removing those deprecated (but replaced with the new API group name) names.
Concretely, the project labels <code>garden.sapcloud.io/role=project</code> and <code>project.garden.sapcloud.io/name=&lt;project-name></code> are no longer supported now.
Similarly, the <code>shoot.garden.sapcloud.io/use-as-seed</code> and <code>shoot.garden.sapcloud.io/ignore-alerts</code> annotations got deleted.
We are not finished yet, but we do small increments and plan to progress on the topic until we finally get rid of all artifacts with the old API group name.</p><h3 id=nodelocaldns-network-policy-rules-adapted-gardenergardener3184><code>NodeLocalDNS</code> Network Policy Rules Adapted (<a href=https://github.com/gardener/gardener/pull/3184>gardener/gardener#3184</a>)<a class=td-heading-self-link href=#nodelocaldns-network-policy-rules-adapted-gardenergardener3184 aria-label="Heading self-link"></a></h3><p>The alpha <code>NodeLocalDNS</code> feature was already <a href=/blog/2020/08.06-gardener-v1.8.0-released/>introduced and explained with Gardener v1.8</a> with the motivation to overcome certain bottlenecks with the horizontally auto-scaled CoreDNS in all shoot clusters.
Unfortunately, due to a bug in the network policy rules, it was not working in all environments.
We have fixed this one now, so it should be ready for further tests and investigations.
Come give it a try!</p><p>Please bear in mind that this blog post only highlights the most noticeable changes and improvements, but there is a whole bunch more, including a ton of bug fixes in older versions! Come check out the <a href=https://github.com/gardener/gardener/releases/tag/v1.13.0>full release notes</a> and share your feedback in <a href=https://gardener-cloud.slack.com/>Slack</a>!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-db5cd4dd331a9047ae6f975b82b1d504>Case Study: Migrating ETCD Volumes in Production</h1><div class=lead>In this case study, our friends from metal-stack lead you through their journey of migrating Gardener ETCD volumes in their production environment.</div><div class="td-byline mb-4"><time datetime=2020-11-20 class=text-body-secondary>Friday, November 20, 2020</time></div><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>This is a guest commentary from <span style=white-space:nowrap><a href=https://metal-stack.io/>metal-stack</a></span>.<br><br>metal-stack is a software that provides an API for provisioning and managing physical servers in the data center. To categorize this product, the terms &ldquo;Metal-as-a-Service&rdquo; (MaaS) or &ldquo;bare metal cloud&rdquo; are commonly used.</p></blockquote><p>One reason that you stumbled upon this blog post could be that you saw errors like the following in your ETCD instances:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>etcd-main-0 etcd 2020-09-03 06:00:07.556157 W | etcdserver: read-only range request <span style=color:#a31515>&#34;key:\&#34;/registry/deployments/shoot--pwhhcd--devcluster2/kube-apiserver\&#34; &#34;</span> with result <span style=color:#a31515>&#34;range_response_count:1 size:9566&#34;</span> took too long (13.95374909s) to execute
</span></span></code></pre></div><p>As it turns out, 14 seconds are way too slow for running Kubernetes API servers. It makes them go into a crash loop (leader election fails). Even worse, this whole thing is self-amplifying: The longer a response takes, the more requests queue up, leading to response times increasing further and further. The system is very unlikely to recover. 😞</p><p>On Github, you can easily find the reason for this problem. Most probably your disks are too slow (see <a href=https://github.com/etcd-io/etcd/issues/10860>etcd-io/etcd#10860</a>). So, when you are (like in our case) on GKE and run your ETCD on their default persistent volumes, consider moving from standard disks to SSDs and the error messages should disappear. A guide on how to use SSD volumes on GKE can be found at <a href=https://cloud.google.com/kubernetes-engine/docs/how-to/persistent-volumes/ssd-pd>Using SSD persistent disks</a>.</p><p>Case closed? Well. For some people it might be. But when you are seeing this in your Gardener infrastructure, it&rsquo;s likely that there is something going wrong. The entire ETCD management is fully managed by Gardener, which makes the problem a bit more interesting to look at. This blog post strives to cover topics such as:</p><ul><li>Gardener operating principles</li><li>Gardener architecture and ETCD management</li><li>Pitfalls with multi-cloud environments</li><li>Migrating GCP volumes to a new storage class</li></ul><p>We from metal-stack learned quite a lot about the capabilities of Gardener through this problem. We are happy to share this experience with a broader audience. Gardener adopters and operators read on.</p><h2 id=how-gardener-manages-etcds>How Gardener Manages ETCDs<a class=td-heading-self-link href=#how-gardener-manages-etcds aria-label="Heading self-link"></a></h2><p>In our infrastructure, we use Gardener to provision Kubernetes clusters on bare metal machines in our own data centers using <span style=white-space:nowrap><a href=https://metal-stack.io/>metal-stack</a></span>. Even if the entire stack could be running on-premise, our initial <a href=/docs/gardener/concepts/apiserver/#seeds>seed cluster</a> and the <a href=https://docs.metal-stack.io/stable/overview/architecture/#Metal-Control-Plane>metal control plane</a> are hosted on GKE. This way, we do not need to manage a single Kubernetes cluster in our entire landscape manually. As soon as we have Gardener deployed on this initial cluster, we can spin up further Seeds in our own data centers through the concept of <a href=/docs/gardener/managed_seed/>ManagedSeeds</a>.</p><p>To make this easier to understand, let us give you a simplified picture of how our Gardener production setup looks like:</p><img title="Production Setup" src=/blog/2020/images/01-001.svg style=width:80vw;height:auto><figcaption style=text-align:center;margin-top:-25px;margin-bottom:30px;font-size:90%>Figure 1: Simplified View on Our Production Setup</figcaption><p>For every <a href=/docs/gardener/concepts/apiserver/#shoots>shoot cluster</a>, Gardener deploys an individual, standalone ETCD as a stateful set into a <em>shoot namespace</em>. The deployment of the ETCD stateful set is managed by a controller called <a href=https://github.com/gardener/etcd-druid>etcd-druid</a>, which reconciles a special resource of the kind <code>etcds.druid.gardener.cloud</code>. This <code>Etcd</code> resource is getting deployed during the shoot provisioning flow in the <a href=/docs/gardener/concepts/gardenlet/>gardenlet</a>.</p><p>For failure-safety, the etcd-druid deploys the official ETCD container image along with a sidecar project called <a href=https://github.com/gardener/etcd-backup-restore>etcd-backup-restore</a>. The sidecar automatically takes backups of the ETCD and stores them at a cloud provider, e.g. in S3 Buckets, Google Buckets, or similar. In case the ETCD comes up without or with corrupted data, the sidecar looks into the backup buckets and automatically restores the latest backup before ETCD starts up. This entire approach basically takes away the pain for operators to manually have to restore data in the event of data loss.</p><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>We found the etcd-backup-restore project very intriguing. It was the inspiration for us to come up with a similar sidecar for the databases we use with metal-stack. This project is called <a href=https://github.com/metal-stack/backup-restore-sidecar>backup-restore-sidecar</a>. We can cope with postgres and rethinkdb database at the moment and more to come. Feel free to check it out when you are interested.</p></blockquote><p>As it&rsquo;s the nature for multi-cloud applications to act upon a variety of cloud providers, with a single installation of Gardener, it is easily possible to spin up new Kubernetes clusters not only on GCP, but on other supported cloud platforms, too.</p><p>When the Gardenlet deploys a resource like the <code>Etcd</code> resource into a shoot namespace, a provider-specific extension-controller has the chance to manipulate it through a <a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook>mutating webhook</a>. This way, a cloud provider can adjust the generic Gardener resource to fit the provider-specific needs. For every cloud that Gardener supports, there is such an extension-controller. For metal-stack, we also maintain one, called <a href=https://github.com/metal-stack/gardener-extension-provider-metal>gardener-extension-provider-metal</a>.</p><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>A side note for cloud providers: Meanwhile, new cloud providers can be added <em>fully</em> out-of-tree, i.e. without touching any of Gardener&rsquo;s sources. This works through <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/>API extensions</a> and <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/>CRDs</a>. Gardener handles generic resources and backpacks provider-specific configuration through raw extensions. When you are a cloud provider on your own, this is really encouraging because you can integrate with Gardener without any burdens. You can find documentation on how to integrate your cloud into Gardener at <a href=/docs/gardener/new-cloud-provider/>Adding Cloud Providers</a> and <a href=/docs/gardener/extensions/>Extensibility Overview</a>.</p></blockquote><h2 id=the-mistake-is-in-the-deployment>The Mistake Is in the Deployment<a class=td-heading-self-link href=#the-mistake-is-in-the-deployment aria-label="Heading self-link"></a></h2><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>This section contains code examples from Gardener v1.8.</p></blockquote><p>Now that we know how the ETCDs are managed by Gardener, we can come back to the original problem from the beginning of this article. It turned out that the real problem was a misconfiguration in our deployment. Gardener actually <em>does</em> use SSD-backed storage on GCP for ETCDs by default. During reconciliation, the <a href=https://github.com/gardener/gardener-extension-provider-gcp>gardener-extension-controller-gcp</a> deploys a storage class called <code>gardener.cloud-fast</code> that enables accessing SSDs on GCP.</p><p>But for some reason, in our cluster we did not find such a storage class. And even more interesting, we did not use the <code>gardener-extension-provider-gcp</code> for any shoot reconciliation, only for ETCD backup purposes. And that was the big mistake we made: We reconciled the shoot control plane completely with <code>gardener-extension-provider-metal</code> even though our initial <code>Seed</code> actually runs on GKE and specific parts of the shoot control plane should be reconciled by the GCP extension-controller instead!</p><p>This is how the initial <code>Seed</code> resource looked like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: initial-seed
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    region: gke
</span></span><span style=display:flex><span>    type: metal
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>Surprisingly, this configuration was working pretty well for a long time. The initial seed properly produced the Kubernetes control planes of our managed seeds that looked like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ kubectl get controlplanes.extensions.gardener.cloud
</span></span><span style=display:flex><span>NAME                 TYPE    PURPOSE    STATUS      AGE
</span></span><span style=display:flex><span>fra-equ01            metal              Succeeded   85d
</span></span><span style=display:flex><span>fra-equ01-exposure   metal   exposure   Succeeded   85d
</span></span></code></pre></div><p>And this is another interesting observation: There are two <code>ControlPlane</code> resources. One regular resource and one with an <code>exposure</code> purpose. Gardener distinguishes between two types for this exact reason: Environments where the shoot control plane runs on a different cloud provider than the Kubernetes worker nodes. The regular <code>ControlPlane</code> resource gets reconciled by the provider configured in the <code>Shoot</code> resource, and the <code>exposure</code> type <code>ControlPlane</code> by the provider configured in the <code>Seed</code> resource.</p><p>With the existing configuration the <code>gardener-extension-provider-gcp</code> does not kick in and hence, it neither deploys the <code>gardener.cloud-fast</code> storage class nor does it mutate the <code>Etcd</code> resource to point to it. And in the end, we are left with ETCD volumes using the default storage class (which is what we do for ETCD stateful sets in the metal-stack seeds, because our default storage class uses <a href=https://github.com/metal-stack/csi-lvm>csi-lvm</a> that writes into logical volumes on the SSD disks in our physical servers).</p><p>The correction we had to make was a one-liner: Setting the provider type of the initial <code>Seed</code> resource to <code>gcp</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ kubectl get seed initial-seed -o yaml
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: initial-seed
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    region: gke
</span></span><span style=display:flex><span>    type: gcp <span style=color:green># &lt;-- here</span>
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>This change moved over the control plane exposure reconciliation to the <code>gardener-extension-provider-gcp</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ kubectl get -n &lt;shoot-namespace&gt; controlplanes.extensions.gardener.cloud
</span></span><span style=display:flex><span>NAME                 TYPE    PURPOSE    STATUS      AGE
</span></span><span style=display:flex><span>fra-equ01            metal              Succeeded   85d
</span></span><span style=display:flex><span>fra-equ01-exposure   gcp     exposure   Succeeded   85d
</span></span></code></pre></div><p>And boom, after some time of waiting for all sorts of magic reconciliations taking place in the background, the missing storage class suddenly appeared:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ kubectl get sc
</span></span><span style=display:flex><span>NAME                  PROVISIONER            
</span></span><span style=display:flex><span>gardener.cloud-fast   kubernetes.io/gce-pd
</span></span><span style=display:flex><span>standard (default)    kubernetes.io/gce-pd
</span></span></code></pre></div><p>Also, the <code>Etcd</code> resource was now configured properly to point to the new storage class:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>$ kubectl get -n &lt;shoot-namespace&gt; etcd etcd-main -o yaml
</span></span><span style=display:flex><span>apiVersion: druid.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Etcd
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  name: etcd-main
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  storageClass: gardener.cloud-fast <span style=color:green># &lt;-- was pointing to default storage class before!</span>
</span></span><span style=display:flex><span>  volumeClaimTemplate: main-etcd
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><blockquote class="alert alert-note"><div class=alert-title><svg viewBox="0 0 24 24" width="24" height="24"><title>information-outline</title><path d="M11 9h2V7H11m1 13c-4.41.0-8-3.59-8-8s3.59-8 8-8 8 3.59 8 8-3.59 8-8 8M12 2A10 10 0 002 12 10 10 0 0012 22 10 10 0 0022 12 10 10 0 0012 2M11 17h2V11H11v6z"/></svg><p>Note</p></div><p>Only the <code>etcd-main</code> storage class gets changed to <code>gardener.cloud-fast</code>. The <code>etcd-events</code> configuration will still point to standard disk storage because this ETCD is much less occupied as compared to the <code>etcd-main</code> stateful set.</p></blockquote><h2 id=the-migration>The Migration<a class=td-heading-self-link href=#the-migration aria-label="Heading self-link"></a></h2><p>Now that the deployment was in place such that this mistake would not repeat in the future, we still had the ETCDs running on the default storage class. The reconciliation does not delete the existing persistent volumes (PVs) on its own.</p><p>To bring production back up quickly, we temporarily moved the ETCD pods to other nodes in the GKE cluster. These were nodes which were less occupied, such that the disk throughput was a little higher than before. But surely that was not a final solution.</p><p>For a proper solution we had to move the ETCD data out of the standard disk PV into a SSD-based PV.</p><p>Even though we had the etcd-backup-restore sidecar, we did not want to fully rely on the restore mechanism to do the migration. The backup should only be there for emergency situations when something goes wrong. Thus, we came up with another approach to introduce the SSD volume: GCP disk snapshots. This is how we did the migration:</p><ol><li>Scale down etcd-druid to zero in order to prevent it from disturbing your migration</li><li>Scale down the kube-apiservers deployment to zero, then wait for the ETCD stateful to take another clean snapshot</li><li>Scale down the ETCD stateful set to zero as well</li><li>(in order to prevent Gardener from trying to bring up the downscaled resources, we used small shell constructs like <code>while true; do kubectl scale deploy etcd-druid --replicas 0 -n garden; sleep 1; done</code>)</li><li>Take a drive snapshot in GCP from the volume that is referenced by the ETCD PVC</li><li>Create a new disk in GCP from the snapshot on a SSD disk</li><li>Delete the existing PVC and PV of the ETCD (oops, data is now gone!)</li><li>Manually deploy a PV into your Kubernetes cluster that references this new SSD disk</li><li>Manually deploy a PVC with the name of the original PVC and let it reference the PV that you have just created</li><li>Scale up the ETCD stateful set and check that ETCD is running properly</li><li>(if something went terribly wrong, you still have the backup from the etcd-backup-restore sidecar, delete the PVC and PV again and let the sidecar bring up ETCD instead)</li><li>Scale up the kube-apiserver deployment again</li><li>Scale up etcd-druid again</li><li>(stop your shell hacks ;D)</li></ol><p>This approach worked very well for us and we were able to fix our production deployment issue. And what happened: We have never seen any crashing kube-apiservers again. 🎉</p><h2 id=conclusion>Conclusion<a class=td-heading-self-link href=#conclusion aria-label="Heading self-link"></a></h2><p>As bad as problems in production are, they are the best way for learning from your mistakes. For new users of Gardener it can be pretty overwhelming to understand the rich configuration possibilities that Gardener brings. However, once you get a hang of how Gardener works, the application offers an exceptional versatility that makes it very much suitable for production use-cases like ours.</p><p>This example has shown how Gardener:</p><ul><li>Can handle arbitrary layers of infrastructure hosted by different cloud providers.</li><li>Allows provider-specific tweaks to gain ideal performance for every cloud you want to support.</li><li>Leverages Kubernetes core principles across the entire project architecture, making it vastly extensible and resilient.</li><li>Brings useful disaster recovery mechanisms to your infrastructure (e.g. with etcd-backup-restore).</li></ul><p>We hope that you could take away something new through this blog post. With this article we also want to thank the SAP Gardener team for helping us to integrate Gardener with metal-stack. It&rsquo;s been a great experience so far. 😄 😍</p></div><div class=td-content style=page-break-before:always><h1 id=pg-fb62d145fea24a772ad79b0d17a1f655>Gardener v1.11 and v1.12 Released</h1><div class="td-byline mb-4"><time datetime=2020-11-04 class=text-body-secondary>Wednesday, November 04, 2020</time></div><p>Two months after our last Gardener release update, we are happy again to present release v1.11 and v1.12 in this blog post. Control plane migration, load balancer consolidation, and new security features are just a few topics we progressed with. As always, a detailed list of features, improvements, and bug fixes can be found in the <a href=https://github.com/gardener/gardener/releases>release notes</a> of each release. If you are going to update from a previous Gardener version, please take the time to go through the action items in the release notes.</p><h2 id=notable-changes-in-v112>Notable Changes in v1.12<a class=td-heading-self-link href=#notable-changes-in-v112 aria-label="Heading self-link"></a></h2><p>Release v1.12, fresh from the oven, is shipped with plenty of improvements, features, and some API changes we want to pick up in the next sections.</p><h3 id=drop-functionless-dns-providers-gardenergardener3036>Drop Functionless DNS Providers (<a href=https://github.com/gardener/gardener/pull/3036>gardener/gardener#3036</a>)<a class=td-heading-self-link href=#drop-functionless-dns-providers-gardenergardener3036 aria-label="Heading self-link"></a></h3><p>This release drops the support for the so-called functionless DNS providers. Those are providers in a shoot’s specification (<code>.spec.dns.providers</code>) which don’t serve the shoot’s domain (<code>.spec.dns.domain</code>), but are created by Gardener in the seed cluster to serve DNS requests coming from the shoot cluster. If such providers don’t specify a <code>type</code> or <code>secretName</code>, the creation or update request for the corresponding shoot is denied.</p><h3 id=seed-taints-gardenergardener2955>Seed Taints (<a href=https://github.com/gardener/gardener/pull/2955>gardener/gardener#2955</a>)<a class=td-heading-self-link href=#seed-taints-gardenergardener2955 aria-label="Heading self-link"></a></h3><p>In an earlier release, we reserved a dedicated section in <code>seed.spec.settings</code> as a replacement for <code>disable-capacity-reservation, disable-dns, invisible</code> taints. These already deprecated taints were still considered and synced, which gave operators enough time to switch their integration to the new <code>settings</code> field. As of version v1.12, support for them has been discontinued and they are automatically removed from seed objects. You may use the actual taint names in a future release of Gardener again.</p><h3 id=load-balancer-events-during-shoot-reconciliation-gardenergardener3028>Load Balancer Events During Shoot Reconciliation (<a href=https://github.com/gardener/gardener/pull/3028>gardener/gardener#3028</a>)<a class=td-heading-self-link href=#load-balancer-events-during-shoot-reconciliation-gardenergardener3028 aria-label="Heading self-link"></a></h3><p>As Gardener is capable of managing thousands of clusters, it is crucial to keep operation efforts at a minimum. This release demonstrates this endeavor by further improving error reporting to the end user. During a shoot’s reconciliation, Gardener creates <code>Services</code> of type <code>LoadBalancer</code> in the shoot cluster, e.g. for VPN or Nginx-Ingress addon, and waits for a successful creation. However, in the past we experienced that occurring issues caused by the party creating the load balancer (typically <a href=https://kubernetes.io/docs/concepts/architecture/cloud-controller/>Cloud-Controller-Manager</a>) are only exposed in the logs or as events. Gardener now fetches these event messages and propagates them to the shoot status in case of a failure. Users can then often fix the problem themselves, if for example the failure discloses an exhausted quota on the cloud provider.</p><h3 id=konnectivitytunnel-feature-per-shootgardenergardener3007>KonnectivityTunnel Feature per Shoot(<a href=https://github.com/gardener/gardener/pull/3007>gardener/gardener#3007</a>)<a class=td-heading-self-link href=#konnectivitytunnel-feature-per-shootgardenergardener3007 aria-label="Heading self-link"></a></h3><p>Since release <code>v1.6</code>, Gardener has been capable of <a href=/docs/gardener/reversed-vpn-tunnel/>reversing the tunnel direction</a> from the seed to the shoot via the <code>KonnectivityTunnel</code> feature gate. With this release we make it possible to control the feature per shoot. We recommend to selectively enable the <code>KonnectivityTunnel</code>, as it is still in <code>alpha</code> state.</p><h3 id=reference-protection-gardenergardener2771-gardenergardener-1708419>Reference Protection (<a href=https://github.com/gardener/gardener/pull/2771>gardener/gardener#2771</a>, <a href=https://github.com/gardener/gardener/commit/17084191c752c206537b9506b54828f4d723d9b7>gardener/gardener 1708419</a>)<a class=td-heading-self-link href=#reference-protection-gardenergardener2771-gardenergardener-1708419 aria-label="Heading self-link"></a></h3><p>Shoot clusters may refer to external objects, like <code>Secrets</code> for specified DNS providers or they have a reference to an audit policy <code>ConfigMap</code>. Deleting those objects while any shoot still references them causes server errors, often only recoverable by an immense amount of manual operations effort. To prevent such scenarios, Gardener now adds a new finalizer <code>gardener.cloud/reference-protection</code> to these objects and removes it as soon as the object itself becomes releasable. Due to compatibility reasons, we decided that the handling for the audit policy <code>ConfigMaps</code> is delivered as an opt-in feature first, so please familiarize yourself with the necessary settings in the Gardener Controller Manager <a href=https://github.com/gardener/gardener/blob/3db1c41726dc5f669e015f294b690d330b55bbf1/example/20-componentconfig-gardener-controller-manager.yaml#L28>component config</a> if you already plan to enable it.</p><h3 id=support-for-resource-quotas-gardenergardener2627>Support for Resource Quotas (<a href=https://github.com/gardener/gardener/pull/2627>gardener/gardener#2627</a>)<a class=td-heading-self-link href=#support-for-resource-quotas-gardenergardener2627 aria-label="Heading self-link"></a></h3><p>After the Kubernetes upstream change (<a href=https://github.com/kubernetes/kubernetes/pull/93537>kubernetes/kubernetes#93537</a>) for externalizing the backing admission plugin has been accepted, we are happy to announce the support of <a href=https://kubernetes.io/docs/concepts/policy/resource-quotas/>ResourceQuotas</a> for Gardener offered resource kinds. <a href=https://kubernetes.io/docs/concepts/policy/resource-quotas/>ResourceQuotas</a> allow you to specify a maximum number of objects per namespace, especially for end-user objects like <code>Shoots</code> or <code>SecretBindings</code> in a project namespace. Even though the admission plugin is enabled by default in the Gardener API Server, make sure the Kube Controller Manager runs the <code>resourcequota</code> controller as well.</p><h3 id=watch-out-developers-terraformer-v2-is-coming-gardenergardener3034>Watch Out Developers, Terraformer v2 is Coming! (<a href=https://github.com/gardener/gardener/pull/3034>gardener/gardener#3034</a>)<a class=td-heading-self-link href=#watch-out-developers-terraformer-v2-is-coming-gardenergardener3034 aria-label="Heading self-link"></a></h3><p>Although not related only to Gardener core, the preparation towards <a href=https://github.com/gardener/terraformer/pull/48>Terraformer v2</a> in the <a href=https://github.com/gardener/gardener/tree/master/extensions>extensions library</a> is still an important milestone to mention. With Terraformer v2, Gardener extensions using Terraform scripts will benefit from great consistency improvements. Please check out <a href=https://github.com/gardener/gardener/pull/3034>PR #3034</a>, which demonstrates necessary steps to transition to Terraformer v2 as soon as it’s released.</p><h2 id=notable-changes-in-v111>Notable Changes in v1.11<a class=td-heading-self-link href=#notable-changes-in-v111 aria-label="Heading self-link"></a></h2><p>The Gardener community worked eagerly to deliver plenty of improvements with version v1.11. Those help us to further progress with topics like <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md>control plane migration</a>, which is actively being worked on, or to harden our load balancer consolidation (<a href=https://github.com/gardener/gardener/blob/master/docs/proposals/08-shoot-apiserver-via-sni.md>APIServerSNI</a>) feature.
Besides improvements and fixes (full list available in release notes), this release contains major features as well, and we don’t want to miss a chance to walk you through them.</p><h3 id=gardener-admission-controller-gardenergardener2832-gardenergardener2781>Gardener Admission Controller (<a href=https://github.com/gardener/gardener/pull/2832>gardener/gardener#2832</a>), (<a href=https://github.com/gardener/gardener/pull/2781>gardener/gardener#2781</a>)<a class=td-heading-self-link href=#gardener-admission-controller-gardenergardener2832-gardenergardener2781 aria-label="Heading self-link"></a></h3><p>In this release, all admission related HTTP handlers moved from the Gardener Controller Manager (GCM) to the new component <a href=/docs/gardener/concepts/admission-controller/>Gardener Admission Controller</a>. The admission controller is rather a small component as opposed to GCM with regards to memory footprint and CPU consumption, and thus allows you to run multiple replicas of it much cheaper than it was before. We certainly recommend specifying the admission controller deployment with more than one replica, since it reduces the odds of a system-wide outage and increases the performance of your Gardener service.</p><p>Besides the already known <code>Namespace</code> and Kubeconfig <code>Secret</code> validation, a new admission handler <code>Resource-Size-Validator</code> was added to the admission controller. It allows operators to restrict the size for all kinds of Kubernetes objects, especially sent by end-users to the Kubernetes or Gardener API Server. We address a security concern with this feature to prevent denial of service attacks in which an attacker artificially increases the size of objects to exhaust your object store, API server caches, or to let Gardener and Kubernetes controllers run out-of-memory. The <a href=/docs/gardener/concepts/admission-controller/#resource-size-validator>documentation</a> reveals an approach of finding the right resource size for your setup and why you should create exceptions for technical users and operators.</p><h3 id=deferring-shoot-progress-reporting-gardenergardener2909>Deferring Shoot Progress Reporting (<a href=https://github.com/gardener/gardener/pull/2909>gardener/gardener#2909</a>)<a class=td-heading-self-link href=#deferring-shoot-progress-reporting-gardenergardener2909 aria-label="Heading self-link"></a></h3><p>Shoot progress reporting is the continuous update process of a shoot’s <code>.status.lastOperation</code> field while the shoot is being reconciled by Gardener. Many steps are involved during reconciliation and depending on the size of your setup, the updates might become an issue for the Gardener API Server, which will refrain from processing further requests for a certain period.
With <code>.controllers.shoot.progressReportPeriod</code> in Gardenlet’s component configuration, you can now delay these updates for the specified period.</p><h3 id=new-policy-for-controller-registrations-gardenergardener2896>New Policy for Controller Registrations (<a href=https://github.com/gardener/gardener/pull/2896>gardener/gardener#2896</a>)<a class=td-heading-self-link href=#new-policy-for-controller-registrations-gardenergardener2896 aria-label="Heading self-link"></a></h3><p>A while ago, we added support for different policies in <code>ControllerRegistrations</code> which determine under which circumstances the deployments of registration controllers happen in affected seed clusters. If you specify the new policy <code>AlwaysExceptNoShoots</code>, the respective extension controller will be deployed to all seed cluster hosting at least one shoot cluster. After all shoot clusters from a seed are gone, the extension deployment will be deleted again.
A full list of supported policies can be found at <a href=https://github.com/gardener/gardener/blob/master/docs/extensions/controllerregistration.md#deployment-configuration-options>Registering Extension Controllers</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3a516216418bba3edad3a6f0dc61d328>Gardener Integrates with KubeVirt</h1><div class="td-byline mb-4"><time datetime=2020-10-19 class=text-body-secondary>Monday, October 19, 2020</time></div><p>The Gardener team is happy to announce that <a href=https://gardener.cloud>Gardener</a> now offers support for an additional, often requested, infrastructure/virtualization technology, namely <a href=https://kubevirt.io/>KubeVirt</a>! Gardener can now provide <a href=https://github.com/cncf/k8s-conformance>Kubernetes-conformant</a> clusters using KubeVirt managed Virtual Machines in the environment of your choice. This integration has been tested and works with any qualified Kubernetes (provider) cluster that is compatibly configured to host the required KubeVirt components, in particular for example <a href=https://www.openshift.com/blog/openshift-virtualization-containers-kvm-and-your-vms>Red Hat OpenShift Virtualization</a>.</p><p>Gardener enables Kubernetes consumers to centralize and operate efficiently homogenous Kubernetes clusters across different IaaS providers and even private environments. This way the same cloud-based application version can be hosted and operated by its vendor or consumer on a variety of infrastructures. When a new customer or your development team demands for a new infrastructure provider, Gardener helps you to quickly and easily on-board your workload. Furthermore, on this new infrastructure, Gardener keeps the seamless Kubernetes management experience for your Kubernetes operators, while upholding the consistency of the CI/CD pipeline of your software development team.</p><h2 id=architecture-and-workflow>Architecture and Workflow<a class=td-heading-self-link href=#architecture-and-workflow aria-label="Heading self-link"></a></h2><p>Gardener is based on the idea of three types of clusters – <em>Garden cluster</em>, <em>Seed cluster</em> and <em>Shoot cluster</em> (see <strong>Figure 1</strong>). The Garden cluster is used to control the entire Kubernetes environment centrally in a highly scalable design. The highly available seed clusters are used to host the end users (shoot) clusters’ control planes. Finally, the shoot clusters consist only of worker nodes to host the cloud native applications.</p><img title="Gardener Architecture" src=/blog/2020/images/00-001.png style=width:90%;height:auto><figcaption style=text-align:center;margin-top:-25px;margin-bottom:30px;font-size:90%>Figure 1: Gardener Architecture</figcaption><p>An integration of the Gardener open source project with a new cloud provider follows a standard <a href=/docs/gardener/extensions/>Gardener extensibility</a> approach. The integration requires two new components: a <a href=/docs/gardener/extensions/>provider extension</a> and a <a href=/docs/other-components/machine-controller-manager/cp_support_new/>Machine Controller Manager (MCM) extension</a>. Both components together enable Gardener to instruct the new cloud provider. They run in the Gardener seed clusters that host the control planes of the shoots based on that cloud provider. The role of the provider extension is to manage the provider-specific aspects of the shoot clusters’ lifecycle, including infrastructure, control plane, worker nodes, and others. It works in cooperation with the MCM extension, which in particular is responsible to handle machines that are provisioned as worker nodes for the shoot clusters. To get this job done, the MCM extension leverages the VM management/API capabilities available with the respective cloud provider.</p><p>Setting up a Kubernetes cluster always involves a flow of interdependent steps (see <strong>Figure 2</strong>), beginning with the generation of certificates and preparation of the infrastructure, continuing with the provisioning of the control plane and the worker nodes, and ending with the deployment of system components. Gardener can be configured to utilize the KubeVirt extensions in its generic workflow at the right extension points, and deliver the desired outcome of a KubeVirt backed cluster.</p><img title="Gardener Architecture" src=/blog/2020/images/00-002.png style=width:90%;height:auto><figcaption style=text-align:center;margin-top:-25px;margin-bottom:30px;font-size:90%>Figure 2: Generic cluster reconciliation flow with extension points</figcaption><h3 id=gardener-integration-with-kubevirt-in-detail>Gardener Integration with KubeVirt in Detail<a class=td-heading-self-link href=#gardener-integration-with-kubevirt-in-detail aria-label="Heading self-link"></a></h3><p>Integration with KubeVirt follows the Gardener extensibility concept and introduces the two new components mentioned above: the <a href=https://github.com/gardener/gardener-extension-provider-kubevirt>KubeVirt Provider Extension</a> and the <a href=https://github.com/gardener/machine-controller-manager-provider-kubevirt>KubeVirt Machine Controller Manager (MCM) Extension</a>.</p><img title="Gardener integration with KubeVirt" src=/blog/2020/images/00-003.png style=width:80%;height:auto><figcaption style=text-align:center;margin-top:-25px;margin-bottom:30px;font-size:90%>Figure 3: Gardener integration with KubeVirt</figcaption><p>The KubeVirt Provider Extension consists of three separate controllers that handle respectively the infrastructure, the control plane, and the worker nodes of the shoot cluster.</p><p>The <strong>Infrastructure Controller</strong> configures the network communication between the shoot worker nodes. By default, shoot worker nodes only use the provider cluster’s pod network. To achieve higher level of network isolation and better performance, it is possible to add more networks and replace the default pod network with a different network using container network interface (CNI) plugins available in the provider cluster. This is currently based on <a href=https://github.com/intel/multus-cni/blob/master/README.md>Multus CNI</a> and <a href=https://github.com/k8snetworkplumbingwg/multus-cni/blob/master/docs/quickstart.md>NetworkAttachmentDefinitions</a>.</p><p>Example infrastructure configuration in a shoot definition:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>provider:
</span></span><span style=display:flex><span>  type: kubevirt
</span></span><span style=display:flex><span>  infrastructureConfig:
</span></span><span style=display:flex><span>    apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: InfrastructureConfig
</span></span><span style=display:flex><span>    networks:
</span></span><span style=display:flex><span>      tenantNetworks:
</span></span><span style=display:flex><span>      - name: network-1
</span></span><span style=display:flex><span>        config: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>          {
</span></span></span><span style=display:flex><span><span style=color:#a31515>            &#34;cniVersion&#34;: &#34;0.4.0&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>            &#34;name&#34;: &#34;bridge-firewall&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>            &#34;plugins&#34;: [
</span></span></span><span style=display:flex><span><span style=color:#a31515>              {
</span></span></span><span style=display:flex><span><span style=color:#a31515>                &#34;type&#34;: &#34;bridge&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>                &#34;isGateway&#34;: true,
</span></span></span><span style=display:flex><span><span style=color:#a31515>                &#34;isDefaultGateway&#34;: true,
</span></span></span><span style=display:flex><span><span style=color:#a31515>                &#34;ipMasq&#34;: true,
</span></span></span><span style=display:flex><span><span style=color:#a31515>                &#34;ipam&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#a31515>                  &#34;type&#34;: &#34;host-local&#34;,
</span></span></span><span style=display:flex><span><span style=color:#a31515>                  &#34;subnet&#34;: &#34;10.100.0.0/16&#34;
</span></span></span><span style=display:flex><span><span style=color:#a31515>                }
</span></span></span><span style=display:flex><span><span style=color:#a31515>              },
</span></span></span><span style=display:flex><span><span style=color:#a31515>              {
</span></span></span><span style=display:flex><span><span style=color:#a31515>                &#34;type&#34;: &#34;firewall&#34;
</span></span></span><span style=display:flex><span><span style=color:#a31515>              }
</span></span></span><span style=display:flex><span><span style=color:#a31515>            ]
</span></span></span><span style=display:flex><span><span style=color:#a31515>          }</span>
</span></span><span style=display:flex><span>        default: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The <strong>Control Plane Controller</strong> deploys a <em>Cloud Controller Manager (CCM)</em>. This is a Kubernetes control plane component that embeds cloud-specific control logic. As any other CCM, it runs the Node controller that is responsible for initializing Node objects, annotating and labeling them with cloud-specific information, obtaining the node’s hostname and IP addresses, and verifying the node’s health. It also runs the Service controller that is responsible for setting up load balancers and other infrastructure components for Service resources that require them.</p><p>Finally, the <strong>Worker Controller</strong> is responsible for managing the worker nodes of the Gardener shoot clusters.</p><p>Example worker configuration in a shoot definition:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>provider:
</span></span><span style=display:flex><span>  type: kubevirt
</span></span><span style=display:flex><span>  workers:
</span></span><span style=display:flex><span>  - name: cpu-worker
</span></span><span style=display:flex><span>    minimum: 1
</span></span><span style=display:flex><span>    maximum: 2
</span></span><span style=display:flex><span>    machine:
</span></span><span style=display:flex><span>      type: standard-1
</span></span><span style=display:flex><span>      image:
</span></span><span style=display:flex><span>        name: ubuntu
</span></span><span style=display:flex><span>        version: <span style=color:#a31515>&#34;18.04&#34;</span>
</span></span><span style=display:flex><span>    volume:
</span></span><span style=display:flex><span>      type: default
</span></span><span style=display:flex><span>      size: 20Gi
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - europe-west1-c
</span></span></code></pre></div><p>For more information about configuring the KubeVirt Provider Extension as an end-user, see <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/usage-as-end-user.md>Using the KubeVirt provider extension with Gardener as end-user</a>.</p><h3 id=enabling-your-gardener-setup-to-leverage-a-kubevirt-compatible-environment>Enabling Your Gardener Setup to Leverage a KubeVirt Compatible Environment<a class=td-heading-self-link href=#enabling-your-gardener-setup-to-leverage-a-kubevirt-compatible-environment aria-label="Heading self-link"></a></h3><p>The very first step required is to define the machine types (VM types) for VMs that will be available. This is achieved via the <code>CloudProfile</code> custom resource. The machine types configuration includes details such as CPU, GPU, memory, OS image, and more.</p><p>Example <code>CloudProfile</code> custom resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: kubevirt
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: kubevirt
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: kubevirt.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: ubuntu
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: <span style=color:#a31515>&#34;18.04&#34;</span>
</span></span><span style=display:flex><span>        sourceURL: <span style=color:#a31515>&#34;https://cloud-images.ubuntu.com/bionic/current/bionic-server-cloudimg-amd64.img&#34;</span>
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: <span style=color:#a31515>&#34;1.18.5&#34;</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: ubuntu
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: <span style=color:#a31515>&#34;18.04&#34;</span>
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: standard-1
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 4Gi
</span></span><span style=display:flex><span>  volumeTypes:
</span></span><span style=display:flex><span>  - name: default
</span></span><span style=display:flex><span>    class: default
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - name: europe-west1
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - name: europe-west1-b
</span></span><span style=display:flex><span>    - name: europe-west1-c
</span></span><span style=display:flex><span>    - name: europe-west1-d
</span></span></code></pre></div><p>Once a machine type is defined, it can be referenced in shoot definitions. This information is used by the KubeVirt Provider Extension to generate <code>MachineDeployment</code> and <code>MachineClass</code> custom resources required by the KubeVirt MCM extension for managing the worker nodes of the shoot clusters during the reconciliation process.</p><p>For more information about configuring the KubeVirt Provider Extension as an operator, see <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/blob/master/docs/usage-as-operator.md>Using the KubeVirt provider extension with Gardener as operator</a>.</p><h3 id=kubevirt-machine-controller-manager-mcm-extension>KubeVirt Machine Controller Manager (MCM) Extension<a class=td-heading-self-link href=#kubevirt-machine-controller-manager-mcm-extension aria-label="Heading self-link"></a></h3><p>The <a href=https://github.com/gardener/machine-controller-manager-provider-kubevirt>KubeVirt MCM Extension</a> is responsible for managing the VMs that are used as worker nodes of the Gardener shoot clusters using the virtualization capabilities of KubeVirt. This extension handles all necessary lifecycle management activities, such as machines creation, fetching, updating, listing, and deletion.</p><p>The KubeVirt MCM Extension implements the Gardener’s common <a href=https://github.com/gardener/machine-controller-manager/blob/master/pkg/util/provider/driver/driver.go>driver interface</a> for managing VMs in different cloud providers. As already mentioned, the KubeVirt MCM Extension is using the <code>MachineDeployments</code> and <code>MachineClasses</code> – an abstraction layer that follows the Kubernetes native declarative approach - to get instructions from the KubeVirt Provider Extension about the required machines for the shoot worker nodes. Also, the cluster austoscaler integrates with the <code>scale</code> subresource of the <code>MachineDeployment</code> resource. This way, Gardener offers a homogeneous autoscaling experience across all supported providers.</p><p>When a new shoot cluster is created or when a new worker node is needed for an existing shoot cluster, a new <a href=https://github.com/gardener/machine-controller-manager-provider-kubevirt/blob/master/kubernetes/machine.yaml>Machine</a> will be created, and at that time, the KubeVirt MCM extension will create a new KubeVirt <code>VirtualMachine</code> in the provider cluster. This <code>VirtualMachine</code> will be created based on a set of configurations in the <a href=https://github.com/gardener/machine-controller-manager-provider-kubevirt/blob/master/kubernetes/machine-class.yaml>MachineClass</a> that follows the <a href=https://github.com/gardener/machine-controller-manager-provider-kubevirt/blob/master/pkg/kubevirt/apis/provider_spec.go>specification</a> of the KubeVirt provider.</p><p>The KubeVirt MCM Extension has two main components. The <strong>MachinePlugin</strong> is responsible for handling the machine objects, and the <strong>PluginSPI</strong> is in charge of making calls to the cloud provider interface, to manage its resources.</p><img title="KubeVirt MCM extension workflow and architecture" src=/blog/2020/images/00-004.png style=width:60%;height:auto><figcaption style=text-align:center;margin-top:-25px;margin-bottom:30px;font-size:90%>Figure 4: KubeVirt MCM extension workflow and architecture</figcaption><p>As shown in <strong>Figure 4</strong>, the MachinePlugin receives a machine request from the MCM and starts its processing by decoding the request, doing partial validation, extracting the relevant information, and sending it to the PluginSPI.</p><p>The PluginSPI then creates, gets, or deletes <code>VirtualMachines</code> depending on the method called by the MachinePlugin. It extracts the kubeconfig of the provider cluster and handles all other required KubeVirt resources such as the secret that holds the <code>cloud-init</code> configurations, and <code>DataVolumes</code> that are mounted as disks to the VMs.</p><h3 id=supported-environments>Supported Environments<a class=td-heading-self-link href=#supported-environments aria-label="Heading self-link"></a></h3><p>The Gardener KubeVirt support is currently qualified on:</p><ul><li><a href=https://kubevirt.io/2020/changelog-v0.32.0.html>KubeVirt v0.32.0</a> (and later)</li><li><a href=https://docs.openshift.com/container-platform/4.4/welcome/index.html>Red Hat OpenShift Container Platform 4.4</a> (and later)</li></ul><p>There are also plans for further improvements and new features, for example integration with CSI drivers for storage management. Details about the implementation progress can be found in the <a href=https://github.com/gardener/gardener-extension-provider-kubevirt/issues>Gardener project on GitHub</a>.</p><p>You can find further resources about the open source project Gardener at <a href=https://gardener.cloud>https://gardener.cloud</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-65c71aebacd73a9bc77b9ff211888238>Shoot Reconciliation Details</h1><div class="td-byline mb-4"><time datetime=2020-10-19 class=text-body-secondary>Monday, October 19, 2020</time></div><p>Do you want to understand how Gardener creates and updates Kubernetes clusters (Shoots)?
Well, it&rsquo;s complicated, but if you are not afraid of large diagrams and are a visual learner like me, this might be useful to you.</p><h2 id=introduction>Introduction<a class=td-heading-self-link href=#introduction aria-label="Heading self-link"></a></h2><p>In this blog post I will share a technical diagram which attempts to tie together the various components involved when Gardener creates a Kubernetes cluster.
I have created and curated the diagram, which visualizes the Shoot reconciliation flow since I started developing on Gardener.
Aside from serving as a memory aid for myself, I created it in hopes that it may potentially help contributors to understand a core piece of the complex Gardener machinery.
Please be advised that the diagram and components involved are large.
Although it can be easily divided into multiple diagrams, I want to show all the components and connections in a single diagram to create an overview of the reconciliation flow.</p><p>The goal is to visualize the interactions of the components involved in the Shoot creation.
It is not intended to serve as a documentation of every component involved.</p><h2 id=background>Background<a class=td-heading-self-link href=#background aria-label="Heading self-link"></a></h2><p>Taking a step back, the Gardener <a href=https://github.com/gardener/gardener/blob/master/README.md>README</a> states:</p><blockquote><p>In essence, Gardener is an <a href=https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/>extension API server</a>
that comes along with a bundle of custom controllers.
It introduces new API objects in an existing Kubernetes cluster (which is called a <strong>garden</strong> cluster) in order to use them for the
management of end-user Kubernetes clusters (which are called <strong>shoot</strong> clusters).
These shoot clusters are described via <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>declarative cluster specifications</a> which are observed by the controllers.
They will bring up the clusters, reconcile their state, perform automated updates and make sure they are always up and running.</p></blockquote><p>This means that Gardener, just like any Kubernetes controller, creates Kubernetes clusters (Shoots) using a reconciliation loop.</p><p>The <a href=/docs/gardener/concepts/gardenlet/>Gardenlet</a> contains the controller and reconciliation loop responsible for the creation, update, deletion, and migration of Shoot clusters (there are more, but we spare them in this article).
In addition, the <a href=/docs/gardener/concepts/controller-manager/>Gardener Controller Manager</a> also reconciles Shoot resources, but only for seed-independent functionality such as Shoot hibernation, Shoot maintenance or quota control.</p><p>This blog post is about the reconciliation loop in the Gardenlet responsible for creating and updating Shoot clusters.
The code can be found in the <a href=https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/shoot/reconciler_reconcile.go>gardener/gardener repository</a>.
The reconciliation loops of the extension controllers can be found in their individual repositories.</p><h2 id=shoot-reconciliation-flow-diagram>Shoot Reconciliation Flow Diagram<a class=td-heading-self-link href=#shoot-reconciliation-flow-diagram aria-label="Heading self-link"></a></h2><p>When Gardner creates a Shoot cluster, there are three conceptual layers involved: the Garden cluster, the Seed cluster and the Shoot cluster.
Each layer represents a top-level section in the diagram (similar to a lane in a BPMN diagram).</p><p>It might seem confusing that the Shoot cluster itself is a layer, because the whole flow in the first place is about creating the Shoot cluster.
I decided to introduce this separate layer to make a clear distinction between which resources exist in the Seed API server (managed by Gardener) and which in the Shoot API server (accessible by the Shoot owner).</p><p>Each section contains several components.
Components are mostly Kubernetes resources in a Gardener installation (e.g. the gardenlet deployment in the Seed cluster).</p><p>This is the list of components:</p><p><strong>(Virtual) Garden Cluster</strong></p><ul><li>Gardener Extension API server</li><li>Validating Provider Webhooks</li><li>Project Namespace</li></ul><p><strong>Seed Cluster</strong></p><ul><li>Gardenlet</li><li>Seed API server<ul><li>every Shoot Control Plane has a dedicated namespace in the Seed.</li></ul></li><li>Cloud Provider (owned by Stakeholder)<ul><li>Arguably part of the Shoot cluster but used by components in the Seed cluster to create the infrastructure for the Shoot.</li></ul></li><li><a href=https://github.com/gardener/external-dns-management>Gardener DNS extension</a></li><li>Provider Extension (such as <a href=https://github.com/gardener/gardener-extension-provider-aws>gardener-extension-provider-aws</a>)</li><li><a href=https://github.com/gardener/etcd-druid>Gardener Extension ETCD Druid</a></li><li><a href=https://github.com/gardener/gardener-resource-manager>Gardener Resource Manager</a></li><li>Operating System Extension (such as <a href=https://github.com/gardener/gardener-extension-os-gardenlinux>gardener-extension-os-gardenlinux</a>)</li><li>Networking Extension (such as <a href=https://github.com/gardener/gardener-extension-networking-cilium>gardener-extension-networking-cilium</a>)</li><li><a href=https://github.com/gardener/machine-controller-manager>Machine Controller Manager</a></li><li>ContainerRuntime extension (such as <a href=https://github.com/gardener/gardener-extension-runtime-gvisor>gardener-extension-runtime-gvisor</a>)</li><li>Shoot API server (in the Shoot Namespace in the Seed cluster)</li></ul><p><strong>Shoot Cluster</strong></p><ul><li>Cloud Provider Compute API (owned by Stakeholder) - for VM/Node creation.</li><li>VM / Bare metal node hosted by Cloud Provider (in Stakeholder owned account).</li></ul><h3 id=how-to-use-the-diagram>How to Use the Diagram<a class=td-heading-self-link href=#how-to-use-the-diagram aria-label="Heading self-link"></a></h3><p>The diagram:</p><ul><li>should be read from top to bottom - starting in the top left corner with the creation of the Shoot resource via the Gardener Extension API server.</li><li>should not require an encompassing documentation / description.
More detailed documentation on the components itself can usually be found in the respective repository.</li><li>does not show which activities execute in parallel (many) and also does not describe the exact dependencies between the steps.
This can be found out by <a href=https://github.com/gardener/gardener/blob/master/pkg/gardenlet/controller/shoot/shoot/reconciler_reconcile.go>looking at the source code</a>.
It however tries to put the activities in a logical order of execution during the reconciliation flow.</li></ul><p>Occasionally, there is an info box with additional information next to parts in the diagram that in my point of view require further explanation.
Large example resource for the Gardener CRDs (e.g Worker CRD, Infrastructure CRD) are placed on the left side and are referenced by a dotted line (&mdash;&ndash;).</p><p>Be aware that Gardener is an evolving project, so the diagram will most likely be already outdated by the time you are reading this.
Nevertheless, it should give a solid starting point for further explorations into the details of Gardener.</p><h3 id=flow-diagram>Flow Diagram<a class=td-heading-self-link href=#flow-diagram aria-label="Heading self-link"></a></h3><p>The diagram can be found below and on <a href=https://github.com/danielfoehrKn/diagrams/tree/master/gardener/shoot-reconciliation>GitHub</a>.
There are multiple formats available (svg, vsdx, draw.io, html).</p><p>Please open an issue or open a PR in the repository if information is missing or is incorrect.
Thanks!</p><p><img style=width:300px;height:auto;margin:0;auto src=https://github.com/danielfoehrKn/diagrams/raw/master/gardener/shoot-reconciliation/gardener_reconcile_with_grid.png target=_blank></a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-8f6d4ea8a719e1717c27ea81082ed135>Gardener v1.9 and v1.10 Released</h1><div class="td-byline mb-4"><time datetime=2020-09-11 class=text-body-secondary>Friday, September 11, 2020</time></div><p>Summer holidays aren&rsquo;t over yet, still, the Gardener community was able to release two new minor versions in the past weeks.
Despite being limited in capacity these days, we were able to reach some major milestones, like adding Kubernetes v1.19 support and the long-delayed automated gardenlet certificate rotation.
Whilst we continue to work on topics related to scalability, robustness, and better observability, we agreed to adjust our focus a little more into the areas of development productivity, code quality and unit/integration testing for the upcoming releases.</p><h2 id=notable-changes-in-v110>Notable Changes in v1.10<a class=td-heading-self-link href=#notable-changes-in-v110 aria-label="Heading self-link"></a></h2><p><a href=https://github.com/gardener/gardener/releases/tag/v1.10.0>Gardener v1.10</a> was a comparatively small release (measured by the number of changes) but it comes with some major features!</p><h3 id=kubernetes-119-support-gardenergardener2799>Kubernetes 1.19 Support (<a href=https://github.com/gardener/gardener/pull/2799>gardener/gardener#2799</a>)<a class=td-heading-self-link href=#kubernetes-119-support-gardenergardener2799 aria-label="Heading self-link"></a></h3><p>The newest minor release of Kubernetes is now supported by Gardener (and all the maintained provider extensions)!
Predominantly, we have enabled CSI migration for OpenStack now that it got promoted to beta, i.e. 1.19 shoots will no longer use the in-tree Cinder volume provisioner.
The CSI migration enablement for Azure got postponed (to at least 1.20) due to some issues that the Kubernetes community is trying to fix in the 1.20 release cycle.
As usual, the <a href=https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md>1.19 release notes</a> should be considered before upgrading your shoot clusters.</p><h3 id=automated-certificate-rotation-for-gardenlet-gardenergardener2542>Automated Certificate Rotation for gardenlet (<a href=https://github.com/gardener/gardener/pull/2542>gardener/gardener#2542</a>)<a class=td-heading-self-link href=#automated-certificate-rotation-for-gardenlet-gardenergardener2542 aria-label="Heading self-link"></a></h3><p>Similar to the kubelet, the gardenlet supports TLS bootstrapping when deployed into a new seed cluster.
It will request a client certificate for the garden cluster using the <code>CertificateSigningRequest</code> API of Kubernetes and store the generated results in a <code>Secret</code> object in the <code>garden</code> namespace of its seed.
These certificates are usually valid for one year.
We have now added support for automatic renewals if the expiration dates are approaching.</p><h3 id=improved-monitoring-alerts-gardenergardener2776>Improved Monitoring Alerts (<a href=https://github.com/gardener/gardener/pull/2776>gardener/gardener#2776</a>)<a class=td-heading-self-link href=#improved-monitoring-alerts-gardenergardener2776 aria-label="Heading self-link"></a></h3><p>We have worked on a larger refactoring to improve reliability and accuracy of our monitoring alerts for both shoot control planes in the seed, as well as shoot system components running on worker nodes.
The improvements are primarily for operators and should result in less false positive alerts.
Also, the alerts should fire less frequently and are better grouped in order to reduce to overall amount of alerts.</p><h3 id=seed-deletion-protection-gardenergardener2732>Seed Deletion Protection (<a href=https://github.com/gardener/gardener/pull/2732>gardener/gardener#2732</a>)<a class=td-heading-self-link href=#seed-deletion-protection-gardenergardener2732 aria-label="Heading self-link"></a></h3><p>Our validation to improve robustness and countermeasures against accidental mistakes has been improved.
Earlier, it was possible to remove the <code>use-as-seed</code> annotation for shooted seeds or directly set the <code>deletionTimestamp</code> on <code>Seed</code> objects, despite of the fact that they might still run shoot control planes.
Seed deletion would not start in these cases, although, it would disrupt the system unnecessarily, and result in some unexpected behaviour.
The Gardener API server is now forbidding such requests if the seeds are not completely empty yet.</p><h3 id=logging-improvements-for-loki-multiple-prs>Logging Improvements for Loki (multiple PRs)<a class=td-heading-self-link href=#logging-improvements-for-loki-multiple-prs aria-label="Heading self-link"></a></h3><p>After we released our large logging stack refactoring (from EFK to Loki) with <a href=/blog/2020/08.06-gardener-v1.8.0-released/>Gardener v1.8</a>, we have continued to work on reliability, quality and user feedback in general.
We aren&rsquo;t done yet, though, Gardener v1.10 includes a bunch of improvements which will help to graduate the <code>Logging</code> feature gate to beta and GA, eventually.</p><h2 id=notable-changes-in-v19>Notable Changes in v1.9<a class=td-heading-self-link href=#notable-changes-in-v19 aria-label="Heading self-link"></a></h2><p>The <a href=https://github.com/gardener/gardener/releases/tag/v1.9.0>v1.9 release</a> contained tons of small improvements and adjustments in various areas of the code base and a little less new major features.
However, we don&rsquo;t want to miss the opportunity to highlight a few of them.</p><h3 id=cri-validation-in-cloudprofiles-gardenergardener2137>CRI Validation in <code>CloudProfile</code>s (<a href=https://github.com/gardener/gardener/pull/2137>gardener/gardener#2137</a>)<a class=td-heading-self-link href=#cri-validation-in-cloudprofiles-gardenergardener2137 aria-label="Heading self-link"></a></h3><p>A couple of releases back we have introduced support for <code>containerd</code> and the <code>ContainerRuntime</code> extension API.
The supported container runtimes are operating system specific, and until now it wasn&rsquo;t possible for end-users to easily figure out whether they can enable <code>containerd</code> or other <code>ContainerRuntime</code> extensions for their shoots.
With this change, Gardener administrators/operators can now provide that information in the <code>.spec.machineImages</code> section in the <code>CloudProfile</code> resource.
This also allows for enhanced validation and prevents misconfigurations.</p><h3 id=new-shoot-event-controller-gardenergardener2649>New Shoot Event Controller (<a href=https://github.com/gardener/gardener/pull/2649>gardener/gardener#2649</a>)<a class=td-heading-self-link href=#new-shoot-event-controller-gardenergardener2649 aria-label="Heading self-link"></a></h3><p>The shoot controllers in both the <code>gardener-controller-manager</code> and <code>gardenlet</code> fire several <code>Event</code>s for some important operations (e.g., automated hibernation/wake-up due to hibernation schedule, automated Kubernetes/machine image version update during maintenance, etc.).
Earlier, the only way to prolong the lifetime of these events was to modify the <code>--event-ttl</code> command line parameter of the garden cluster&rsquo;s <code>kube-apiserver</code>.
This came with the disadvantage that <em>all</em> events were kept for a longer time (not only those related to <code>Shoot</code>s that an operator is usually interested in and ideally wants to store for a couple of days).
The new shoot event controller allows to achieve this by deleting non-shoot events.
This helps operators and end-users to better understand which changes were applied to their shoots by Gardener.</p><h3 id=early-deployment-of-the-logging-stack-for-new-shoots-gardenergardener2750>Early Deployment of the Logging Stack for New Shoots (<a href=https://github.com/gardener/gardener/pull/2750>gardener/gardener#2750</a>)<a class=td-heading-self-link href=#early-deployment-of-the-logging-stack-for-new-shoots-gardenergardener2750 aria-label="Heading self-link"></a></h3><p>Since the first introduction of the <code>Logging</code> feature gate two years back, the logging stack was only deployed at the very end of the shoot creation.
This had the disadvantage that control plane pod logs were not kept in case the shoot creation flow is interrupted before the logging stack could be deployed.
In some situations, this was preventing fetching relevant information about why a certain control plane component crashed.
We now deploy the logging stack very early in the shoot creation flow to always have access to such information.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7243b09afa0754f2cc88295f9a92635d>Gardener v1.8.0 Released</h1><div class="td-byline mb-4"><time datetime=2020-08-06 class=text-body-secondary>Thursday, August 06, 2020</time></div><p>Even if we are in the midst of the summer holidays, a new Gardener release came out yesterday: v1.8.0! It&rsquo;s main themes are the large change of our logging stack to Loki (which was already explained in detail on a <a href=https://grafana.com/blog/2020/07/15/gardener-saps-kubernetes-as-a-service-open-source-project-is-moving-its-logging-stack-to-loki/>blog post on grafana.com</a>), more configuration options to optimize the utilization of a shoot, node-local DNS, new project roles, and significant improvements for the Kubernetes client that Gardener uses to interact with the many different clusters.</p><h2 id=notable-changes>Notable Changes<a class=td-heading-self-link href=#notable-changes aria-label="Heading self-link"></a></h2><h3 id=logging-20-efk-stack-replaced-by-loki-gardenergardener2515>Logging 2.0: EFK Stack Replaced by Loki (<a href=https://github.com/gardener/gardener/pull/2515>gardener/gardener#2515</a>)<a class=td-heading-self-link href=#logging-20-efk-stack-replaced-by-loki-gardenergardener2515 aria-label="Heading self-link"></a></h3><p>Since two years or so, Gardener could optionally provision a dedicated logging stack per seed and per shoot which was based on fluent-bit, fluentd, ElasticSearch and Kibana. This feature was still hidden behind an alpha-level feature gate and never got promoted to beta so far. Due to various limitations of this solution, we decided to replace the EFK stack with Loki. As we already have Prometheus and Grafana deployments for both users and operators by default for all clusters, the choice was just natural.
Please find out more on this topic at <a href=https://grafana.com/blog/2020/07/15/gardener-saps-kubernetes-as-a-service-open-source-project-is-moving-its-logging-stack-to-loki/>this dedicated blog post</a>.</p><h3 id=cluster-identities-and-dnsowner-objects-gardenergardener2471-gardenergardener2576>Cluster Identities and <code>DNSOwner</code> Objects (<a href=https://github.com/gardener/gardener/pull/2471>gardener/gardener#2471</a>, <a href=https://github.com/gardener/gardener/pull/2576>gardener/gardener#2576</a>)<a class=td-heading-self-link href=#cluster-identities-and-dnsowner-objects-gardenergardener2471-gardenergardener2576 aria-label="Heading self-link"></a></h3><p>The shoot control plane migration topic is ongoing since a few months already, and we are very much progressing with it. A first alpha version will probably make it out soon. As part of these endeavors, we introduced cluster identities and the usage of <code>DNSOwner</code> objects in this release. Both are needed to gracefully migrate the <code>DNSEntry</code> extension objects from the old seed to the new seed as part of the control plane migration process.
Please find out more on this topic at <a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/#control-plane-migration-between-seed-clusters>this blog post</a>.</p><h3 id=new-uam-role-for-project-members-to-limit-user-access-management-privileges-gardenergardener2611>New <code>uam</code> Role for <code>Project</code> Members to Limit User Access Management Privileges (<a href=https://github.com/gardener/gardener/pull/2611>gardener/gardener#2611</a>)<a class=td-heading-self-link href=#new-uam-role-for-project-members-to-limit-user-access-management-privileges-gardenergardener2611 aria-label="Heading self-link"></a></h3><p>In order to allow external user access management system to integrate with Gardener and to fulfil certain compliance aspects, we have introduced a new role called <code>uam</code> for <code>Project</code> members (next to <code>admin</code> and <code>viewer</code>). Only if a user has this role, then he/she is allowed to add/remove other human users to the respective <code>Project</code>. By default, all newly created <code>Project</code>s assign this role only to the owner while, for backwards-compatibility reasons, it will be assigned for all members for existing projects. Project owners can steadily revoke this access as desired.
Interestingly, the <code>uam</code> role is backed by a custom RBAC verb called <code>manage-members</code>, i.e., the Gardener API server is only admitting changes to the human <code>Project</code> members if the respective user is bound to this RBAC verb.</p><h3 id=new-node-local-dns-feature-for-shoots-gardenergardener2528>New Node-Local DNS Feature for Shoots (<a href=https://github.com/gardener/gardener/pull/2528>gardener/gardener#2528</a>)<a class=td-heading-self-link href=#new-node-local-dns-feature-for-shoots-gardenergardener2528 aria-label="Heading self-link"></a></h3><p>By default, we are using CoreDNS as DNS plugin in shoot clusters which we auto-scale horizontally using HPA. However, in some situations we are discovering certain bottlenecks with it, e.g., unreliable UDP connections, unnecessary node hopping, inefficient load balancing, etc.
To further optimize the DNS performance for shoot clusters, it is now possible to enable a new alpha-level feature gate in the gardenlet&rsquo;s componentconfig: <code>NodeLocalDNS</code>. If enabled, all shoots will get a new <code>DaemonSet</code> to run a DNS server on each node.</p><h3 id=more-kubelet-and-api-server-configurability-gardenergardener2574-gardenergardener2668>More kubelet and API Server Configurability (<a href=https://github.com/gardener/gardener/pull/2574>gardener/gardener#2574</a>, <a href=https://github.com/gardener/gardener/pull/2668>gardener/gardener#2668</a>)<a class=td-heading-self-link href=#more-kubelet-and-api-server-configurability-gardenergardener2574-gardenergardener2668 aria-label="Heading self-link"></a></h3><p>One large benefit of Gardener is that it allows you to optimize the usage of your control plane as well as worker nodes by exposing relevant configuration parameters in the <code>Shoot</code> API.
In this version, we are adding support to configure kubelet&rsquo;s values for <code>systemReserved</code> and <code>kubeReserved</code> resources as well as the kube-apiserver&rsquo;s watch cache sizes.
This allows end-users to get to better node utilization and/or performance for their shoot clusters.</p><h3 id=configurable-timeout-settings-for-machine-controller-manager-gardenergardener2563>Configurable Timeout Settings for machine-controller-manager (<a href=https://github.com/gardener/gardener/pull/2563>gardener/gardener#2563</a>)<a class=td-heading-self-link href=#configurable-timeout-settings-for-machine-controller-manager-gardenergardener2563 aria-label="Heading self-link"></a></h3><p>One very central component in Project Gardener is the <a href=https://github.com/gardener/machine-controller-manager>machine-controller-manager</a> for managing the worker nodes of shoot clusters. It has extensive qualities with respect to node lifecycle management and rolling updates. As such, it uses certain timeout values, e.g. when creating or draining nodes, or when checking their health.
Earlier, those were not customizable by end-users, but we are adding this possibility now. You can fine-grain these settings per worker pool in the <code>Shoot</code> API such that you can optimize the lifecycle management of your worker nodes even more!</p><h3 id=improved-usage-of-cached-client-to-reduce-network-io-gardenergardener2635-gardenergardener2637>Improved Usage of Cached Client to Reduce Network I/O (<a href=https://github.com/gardener/gardener/pull/2635>gardener/gardener#2635</a>, <a href=https://github.com/gardener/gardener/pull/2637>gardener/gardener#2637</a>)<a class=td-heading-self-link href=#improved-usage-of-cached-client-to-reduce-network-io-gardenergardener2635-gardenergardener2637 aria-label="Heading self-link"></a></h3><p>In the last Gardener release v1.7 we have introduced a huge refactoring the clients that we use to interact with the many different Kubernetes clusters. This is to further optimize the network I/O performed by leveraging watches and caches as good as possible. It&rsquo;s still an alpha-level feature that must be explicitly enabled in the Gardenlet&rsquo;s component configuration, though, with this release we have improved certain things in order to pave the way for beta promotion. For example, we were initially also using a cached client when interacting with shoots. However, as the gardenlet runs in the seed as well (and thus can communicate cluster-internally with the kube-apiservers of the respective shoots) this cache is not necessary and just memory overhead. We have removed it again and saw the memory usage getting lower again. More to come!</p><h3 id=aws-ebs-volume-encryption-by-default-gardenergardener-extension-provider-aws147>AWS EBS Volume Encryption by Default (<a href=https://github.com/gardener/gardener-extension-provider-aws/pull/147>gardener/gardener-extension-provider-aws#147</a>)<a class=td-heading-self-link href=#aws-ebs-volume-encryption-by-default-gardenergardener-extension-provider-aws147 aria-label="Heading self-link"></a></h3><p>The <code>Shoot</code> API already exposed the possibility to encrypt the root disks of worker nodes since quite a while, but it was disabled by default (for backwards-compatibility reasons). With this release we have change this default, so new shoot worker nodes will be provisioned with encrypted root disks out-of-the-box. However, the <code>g4dn</code> instance types of AWS don&rsquo;t support this encryption, so when you use them you have to explicitly disable the encryption in the worker pool configuration.</p><h3 id=liveness-probe-for-gardener-api-server-deployment-gardenergardener2647>Liveness Probe for Gardener API Server Deployment (<a href=https://github.com/gardener/gardener/pull/2647>gardener/gardener#2647</a>)<a class=td-heading-self-link href=#liveness-probe-for-gardener-api-server-deployment-gardenergardener2647 aria-label="Heading self-link"></a></h3><p>A small, but very valuable improvement is the introduction of a liveness probe for our Gardener API server. As it&rsquo;s built with the same library like the Kubernetes API server, it exposes two endpoints at <code>/livez</code> and <code>/readyz</code> which were created exactly for the purpose of live- and readiness probes.
With Gardener v1.8, the Helm chart contains a liveness probe configuration by default, and we are awaiting an upstream fix (<a href=https://github.com/kubernetes/kubernetes/issues/93599>kubernetes/kubernetes#93599</a>) to also enable the readiness probe. This will help in a smoother rolling update of the Gardener API server pods, i.e., preventing clients from talking to a not yet initialized or already terminating API server instance.</p><h3 id=webhook-ports-changed-to-enable-openshift-gardenergardener2660>Webhook Ports Changed to Enable OpenShift (<a href=https://github.com/gardener/gardener/pull/2660>gardener/gardener#2660</a>)<a class=td-heading-self-link href=#webhook-ports-changed-to-enable-openshift-gardenergardener2660 aria-label="Heading self-link"></a></h3><p>In order to make it possible to run Gardener on OpenShift clusters as well, we had to make a change in the port configuration for the webhooks we are using in both Gardener and the extension controllers. Earlier, all the webhook servers directly exposed port <code>443</code>, i.e., a system port which is a security concern and disallowed in OpenShift. We have changed this port now across all places and also adapted our network policies accordingly. This is most likely not the last necessary change to enable this scenario, however, it&rsquo;s a great improvement to push the project forward.</p><p>If you&rsquo;re interested in more details and even more improvements, you can read all the <a href=https://github.com/gardener/gardener/releases/tag/v1.8.0>release notes for Gardener v1.8.0</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5a00f0b84c6908a3bfc3ba8f837bfb40>PingCAP’s Experience in Implementing Their Managed TiDB Service with Gardener</h1><div class="td-byline mb-4"><time datetime=2020-05-27 class=text-body-secondary>Wednesday, May 27, 2020</time></div><p>Gardener is showing successful collaboration with its growing community of contributors and adopters. With this come some success stories, including PingCAP using Gardener to implement its managed service.</p><h3 id=about-pingcap-and-its-tidb-cloud>About PingCAP and Its TiDB Cloud<a class=td-heading-self-link href=#about-pingcap-and-its-tidb-cloud aria-label="Heading self-link"></a></h3><p><a href=https://pingcap.com/about/>PingCAP</a> started in 2015, when three seasoned infrastructure engineers working at leading Internet companies got sick and tired of the way databases were managed, scaled and maintained. Seeing no good solution on the market, they decided to build their own - the open-source way. With the help of a first-class team and hundreds of contributors from around the globe, PingCAP is building a distributed NewSQL, hybrid transactional and analytical processing (HTAP) database.</p><p>Its flagship project, <a href=https://en.wikipedia.org/wiki/TiDB>TiDB</a>, is a cloud-native distributed SQL database with MySQL compatibility, and one of the <a href=https://github.com/pingcap/tidb>most popular</a> open-source database projects - with 23.5K+ stars and 400+ contributors. Its sister project <a href=https://github.com/tikv/tikv>TiKV</a> is a <a href=https://landscape.cncf.io/card-mode>Cloud Native Interactive Landscape project</a>.</p><p>PingCAP envisioned their managed TiDB service, known as <a href=https://pingcap.com/tidb-cloud/sign-up/>TiDB Cloud</a>, to be multi-tenant, secure, cost-efficient, and to be compatible with different cloud providers. As a result, the company turned to Gardener to build their managed TiDB cloud service offering.</p><img title="TiDB Cloud Beta Preview" src=/blog/2020/images/00-001.png style=width:90%;height:auto><figcaption style=text-align:center;margin-top:-25px;margin-bottom:30px;font-size:90%>TiDB Cloud Beta Preview</figcaption><h3 id=limitations-with-other-public-managed-kubernetes-services>Limitations with Other Public Managed Kubernetes Services<a class=td-heading-self-link href=#limitations-with-other-public-managed-kubernetes-services aria-label="Heading self-link"></a></h3><p>Previously, PingCAP encountered issues while using other public managed K8s cluster services, to develop the first version of its TiDB Cloud. Their worst pain point was that they felt helpless when encountering certain malfunctions. PingCAP wasn’t able to do much to resolve these issues, except waiting for the providers’ help. More specifically, they experienced problems due to cloud-provider specific Kubernetes system upgrades, delays in the support response (which could be avoided in exchange of a costly support fee), and no control over when things got fixed.</p><p>There was also a lot of cloud-specific integration work needed to follow a multi-cloud strategy, which proved to be expensive both to produce and maintain. With one of these managed K8s services, you would have to integrate the instance API, as opposed to a solution like Gardener, which provides a unified API for all clouds. Such a unified API eliminates the need to worry about cloud specific-integration work altogether.</p><img src=/blog/2020/images/00-002.png style=width:60%;height:auto><h3 id=why-pingcap-chose-gardener-to-build-tidb-cloud>Why PingCAP Chose Gardener to Build TiDB Cloud<a class=td-heading-self-link href=#why-pingcap-chose-gardener-to-build-tidb-cloud aria-label="Heading self-link"></a></h3><blockquote><p>“Gardener has similar concepts to Kubernetes. Each Kubernetes cluster is just like a Kubernetes pod, so the similar concepts apply, and the controller pattern makes Gardener easy to manage. It was also easy to extend, as the team was already very familiar with Kubernetes, so it wasn’t hard for us to extend Gardener. We also saw that Gardener has a very active community, which is always a plus!”</p><p>- Aylei Wu, (Cloud Engineer) at PingCAP</p></blockquote><p>At first glance, PingCAP had initial reservations about using Gardener - mainly due to its adoption level (still at the beginning) and an apparent complexity of use. However, these were soon eliminated as they learned more about the solution. As Aylei Wu mentioned during the last <a href="https://www.youtube.com/watch?v=nqkzUylfIbU&amp;feature=youtu.be">Gardener community meeting</a>, <em><strong>“a good product speaks for itself”</strong></em>, and once the company got familiar with Gardener, they quickly noticed that the concepts were very similar to Kubernetes, which they were already familiar with.</p><p>They recognized that Gardener would be their best option, as it is highly extensible and provides a unified abstraction API layer. In essence, the machines can be managed via a machine controller manager for different cloud providers - without having to worry about the individual cloud APIs.</p><p>They agreed that Gardener’s solution, although complex, was definitely worth it. Even though it is a relatively new solution, meaning they didn’t have access to other user testimonials, they decided to go with the service since it checked all the boxes (and as SAP was running it productively with a huge fleet). PingCAP also came to the conclusion that building a managed Kubernetes service themselves would not be easy. Even if they were to build a managed K8s service, they would have to heavily invest in development and would still end up with an even more complex platform than Gardener’s. For all these reasons combined, PingCAP decided to go with Gardener to build its TiDB Cloud.</p><img src=/blog/2020/images/00-003.png style=width:70%;height:auto><p>Here are certain features of Gardener that PingCAP found appealing:</p><ul><li><strong>Cloud agnostic:</strong> Gardener’s abstractions for cloud-specific integrations dramatically reduce the investment in supporting more than one cloud infrastructure. Once the integration with Amazon Web Services was done, moving on to Google Cloud Platform proved to be relatively easy. (At the moment, TiDB Cloud has subscription plans available for both GCP and AWS, and they are planning to support Alibaba Cloud in the future.)</li><li><strong>Familiar concepts:</strong> Gardener is K8s native; its concepts are easily related to core Kubernetes concepts. As such, it was easy to onboard for a K8s experienced team like PingCAP’s SRE team.</li><li><strong>Easy to manage and extend:</strong> Gardener’s API and extensibility are easy to implement, which has a positive impact on the implementation, maintenance costs and time-to-market.</li><li><strong>Active community:</strong> Prompt and quality responses on Slack from the Gardener team tremendously helped to quickly onboard and produce an efficient solution.</li></ul><h3 id=how-pingcap-built-tidb-cloud-with-gardener>How PingCAP Built TiDB Cloud with Gardener<a class=td-heading-self-link href=#how-pingcap-built-tidb-cloud-with-gardener aria-label="Heading self-link"></a></h3><p>On a technical level, PingCAP’s set-up overview includes the following:</p><ul><li>A Base Cluster globally, which is the top-level control plane of TiDB Cloud</li><li>A Seed Cluster per cloud provider per region, which makes up the fundamental data plane of TiDB Cloud</li><li>A Shoot Cluster is dynamically provisioned per tenant per cloud provider per region when requested</li><li>A tenant may create one or more TiDB clusters in a Shoot Cluster</li></ul><p>As a real world example, PingCAP sets up the Base Cluster and Seed Clusters in advance. When a tenant creates its first TiDB cluster under the us-west-2 region of AWS, a Shoot Cluster will be dynamically provisioned in this region, and will host all the TiDB clusters of this tenant under us-west-2. Nevertheless, if another tenant requests a TiDB cluster in the same region, a <strong>new</strong> Shoot Cluster will be provisioned. Since different Shoot Clusters are located in different VPCs and can even be hosted under different AWS accounts, TiDB Cloud is able to achieve hard isolation between tenants and meet the critical security requirements for our customers.</p><p>To automate these processes, PingCAP creates a service in the Base Cluster, known as the TiDB Cloud “Central” service. The Central is responsible for managing shoots and the TiDB clusters in the Shoot Clusters. As shown in the following diagram, user operations go to the Central, being authenticated, authorized, validated, stored and then applied asynchronously in a controller manner. The Central will talk to the Gardener API Server to create and scale Shoot clusters. The Central will also access the Shoot API Service to deploy and reconcile components in the Shoot cluster, including control components (<a href=https://github.com/pingcap/tidb-operator>TiDB Operator</a>, API Proxy, Usage Reporter for billing, etc.) and the TiDB clusters.</p><img title="TiDB Cloud on Gardener Architecture Overview" src=/blog/2020/images/00-004.png style=width:90%;height:auto><figcaption style=text-align:center;margin-top:-25px;margin-bottom:30px;font-size:90%>TiDB Cloud on Gardener Architecture Overview</figcaption><h3 id=whats-next-for-pingcap-and-gardener>What’s Next for PingCAP and Gardener<a class=td-heading-self-link href=#whats-next-for-pingcap-and-gardener aria-label="Heading self-link"></a></h3><p>With the initial success of using the project to build TiDB Cloud, PingCAP is now working heavily on the stability and day-to-day operations of TiDB Cloud on Gardener. This includes writing Infrastructure-as-Code scripts/controllers with it to achieve GitOps, building tools to help diagnose problems across regions and clusters, as well as running chaos tests to identify and eliminate potential risks. After benefiting greatly from the community, PingCAP will continue to contribute back to Gardener.</p><p>In the future, PingCAP also plans to support more cloud providers like AliCloud and Azure. Moreover, PingCAP may explore the opportunity of running TiDB Cloud in on-premise data centers with the constantly expanding support this project provides. Engineers at PingCAP enjoy the ease of learning from Gardener’s Kubernetes-like concepts and being able to apply them everywhere. Gone are the days of heavy integrations with different clouds and worrying about vendor stability. With this project, PingCAP now sees broader opportunities to land TiDB Cloud on various infrastructures to meet the needs of their global user group.</p><p>Stay tuned, more <a href=/blog/>blog posts</a> to come on how Gardener is collaborating with its contributors and adopters to bring fully-managed clusters at scale everywhere! If you want to join in on the fun, <a href=/community/>connect with our community</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f5597fe94bd66c8df6289e7a6febba85>New Website, Same Green Flower</h1><div class="td-byline mb-4"><time datetime=2020-05-11 class=text-body-secondary>Monday, May 11, 2020</time></div><p>The <a href=https://gardener.cloud>Gardener project website</a> just received a serious facelift. Here are some of the highlights:</p><ul><li><strong>A completely new landing page</strong>, emphasizing both on Gardener&rsquo;s value proposition and the open community behind it.</li><li><strong>The Community page</strong> was reconstructed for quick access to the various community channels and will soon merge the Adopters page. It will provide a better insight into success stories from the community.</li><li><strong>Improved blogs</strong> layout. One-click sharing options are available starting with simple URL copy link and twitter button and others will closely follow up. While we are at it, give it a try. Spread the word.</li></ul><p><strong>Website builds</strong> also got to a new level with:</p><ul><li><strong>Containerization</strong>. The whole build environment is containerized now, eliminating differences between local and CI/CD setup and reducing content developers focus only to the <code>/documentation</code> repository. Running a local server for live preview of changes as you make them when developing content for the website, is now as easy as runing <code>make serve</code> in your local <code>/documentation</code> clone.</li><li><strong>Numerous improvements to the buld scripts</strong>. More <a href=https://github.com/gardener/website-generator#build-configuration>configuration options</a>, authenticated requests, fault tolerance and performance.</li><li><strong>Good news for Windows WSL users</strong> who will now enjoy a significantly support. See the updated <a href=https://github.com/gardener/website-generator#windows-10-users>README</a> for details on that.</li><li><strong>A number of improvements</strong> in layouts styles, site assets and hugo site-building techniques.</li></ul><p><strong>But hey, THAT&rsquo;S NOT ALL!</strong></p><p>Stay tuned for more improvements around the corner. The biggest ones are aligning the documentation with the new theme and restructuring it along, more emphasis on community success stories all around, more sharing options and more than a handful of shortcodes for content development and &mldr; let&rsquo;s cut the spoilers here.</p><p>I hope you will like it. Let us know what you think about it. Feel free to leave comments and discuss on <img src=/blog/2020/images/twitter-logo-green.svg class="icon inline"><a href=https://twitter.com/GardenerProject>Twitter</a> and <img src=/blog/2020/images/slack-logo-green.svg class="icon inline"><a href=https://gardener-cloud.slack.com/>Slack</a>, or in case of issues - on <img src=/blog/2020/images/github-mark-logo-green.svg class="icon inline"><a href=https://github.com/gardener/documentation/issues>GitHub</a>.</p><p><strong>Go ahead and help us spread the word: <a href=https://gardener.cloud><a href=https://gardener.cloud>https://gardener.cloud</a></a></strong></p><img src=/blog/2020/images/website-screen-l.png></div><div class=td-content style=page-break-before:always><h1 id=pg-936196286d8b2c724f189212da4526b8>2019</h1><div class="td-byline mb-4"><time datetime=2019-06-11 class=text-body-secondary>Tuesday, June 11, 2019</time></div></div><div class=td-content><h1 id=pg-d7ab4691426388a64b0dec284b3e78a4>Feature Flags in Kubernetes Applications</h1><div class="td-byline mb-4"><time datetime=2019-06-11 class=text-body-secondary>Tuesday, June 11, 2019</time></div><p>Feature flags are used to change the behavior of a program at runtime without forcing a restart.</p><p>Although they are essential in a native cloud environment, they cannot be implemented without significant effort on some platforms. Kubernetes has made this trivial. Here we will implement them through labels and annotations, but you can also implement them by connecting directly to the Kubernetes API Server.</p><p><img src=/blog/2019/images/teaser-2.gif alt=teaser></p><h2 id=possible-use-cases>Possible Use Cases<a class=td-heading-self-link href=#possible-use-cases aria-label="Heading self-link"></a></h2><ul><li>Turn on/off a specific instance</li><li>Turn on/off the profiling of a specific instance</li><li>Change the logging level, to capture detailed logs during a specific event</li><li>Change caching strategy at runtime</li><li>Change timeouts in production</li><li>Toggle on/off some special verification</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0714b96e96619738d515068f46295c82>Organizing Access Using kubeconfig Files</h1><div class="td-byline mb-4"><time datetime=2019-06-11 class=text-body-secondary>Tuesday, June 11, 2019</time></div><p>The kubectl command-line tool uses <code>kubeconfig</code> files to find the information it needs in order to choose a cluster and communicate with its API server.</p><p><img src=/blog/2019/images/teaser-1.svg alt=teaser></p><h3 id=what-happens-if-the-kubeconfig-file-of-your-production-cluster-is-leaked-or-published-by-accident>What happens if the kubeconfig file of your production cluster is leaked or published by accident?<a class=td-heading-self-link href=#what-happens-if-the-kubeconfig-file-of-your-production-cluster-is-leaked-or-published-by-accident aria-label="Heading self-link"></a></h3><p>Since there is no possibility to rotate or revoke the initial kubeconfig, there is only one way to protect your infrastructure or application if the kubeconfig has leaked - <strong>delete the cluster</strong>.</p><p>Learn more on <a href=/docs/guides/client-tools/working-with-kubeconfig/>Organizing Access Using kubeconfig Files</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f86342883e0673103ec7a5bf620a154b>KubeCon Rewind: SIG Cluster API & Gardener – Managing Machines Automatically</h1><div class="td-byline mb-4"><time datetime=2019-05-24 class=text-body-secondary>Friday, May 24, 2019</time></div><p>The KubeCon + CloudNativeCon Europe buzz might be settling, but the energy from our deep dive session with the incredible folks at <a href="https://github.com/kubernetes-sigs/cluster-api?tab=readme-ov-file#cluster-api"><strong>SIG Cluster API</strong></a> is still palpable! We, from the <strong>Gardener</strong> team, were absolutely thrilled to share the stage and explore the powerful, declarative world of Kubernetes cluster lifecycle management.</p><p>For those who don&rsquo;t know, Gardener has been on a mission since <strong>2017</strong> to provide a fully managed Kubernetes experience, uniquely running customer control planes as pods within dedicated &ldquo;seed&rdquo; clusters, a.k.a. &ldquo;Kubeception&rdquo;. This approach demanded robust automation for the underlying infrastructure. To solve this, we pioneered the <a href=https://github.com/gardener/machine-controller-manager><strong>Machine Controller Manager</strong></a>, introducing the core abstractions you might recognize: <code>Machine</code>, <code>MachineSet</code>, and <code>MachineDeployment</code>. These concepts were born out of real-world needs to declaratively manage VMs and their lifecycles as if they were just another Kubernetes resource.</p><p>It&rsquo;s exciting to see these foundational ideas embraced and extended by the wider community through <strong>Cluster API</strong>! Our joint talk was a fantastic opportunity to showcase how these abstractions, originally developed within Gardener, now form a cornerstone of Cluster API&rsquo;s approach to creating, configuring, and managing Kubernetes clusters in a standardized way, across any provider. Incidentally, this also made Gardener the <a href=https://github.com/kubernetes-sigs/cluster-api/commit/00b1ead264aea6f88585559056c180771cce3815>first adopter of the machine API</a>.</p><p>We dove into:</p><ul><li>The <strong>&ldquo;why&rdquo;</strong> behind Cluster API – the drive to stop reinventing the wheel for cluster provisioning.</li><li>The <strong>core API types</strong> (yes, including those familiar <code>Machine</code>, <code>MachineSet</code>, and <code>MachineDeployment</code> concepts!) that provide the declarative power.</li><li>How you can <strong>bootstrap</strong> Cluster API and get it managing your clusters.</li><li>The vibrant <strong>community</strong> and how everyone can get involved.</li></ul><p>This collaboration isn&rsquo;t just about shared code; it&rsquo;s about a shared vision for a more consistent, automated, and less painful Kubernetes world. Seeing Gardener&rsquo;s early innovations become part of a community-driven standard like Cluster API is a testament to the power of open source.</p><p>If you were there, thank you for the great questions and engagement! If you missed it, keep an eye out for the <a href="https://www.youtube.com/watch?v=Mtg8jygK3Hs">recording</a>. The journey to simplify cluster management is a collective one, and we&rsquo;re stoked to be building that future together with SIG Cluster API and all of you!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b8ccaca9b1a59904ce81ccfa43d1f16e>2018</h1><div class="td-byline mb-4"><time datetime=2018-12-25 class=text-body-secondary>Tuesday, December 25, 2018</time></div></div><div class=td-content><h1 id=pg-486244ec0498277b1c88be549bfbc5f4>Gardener Cookies</h1><div class="td-byline mb-4"><time datetime=2018-12-25 class=text-body-secondary>Tuesday, December 25, 2018</time></div><h1 id=green-tea-matcha-cookies>Green Tea Matcha Cookies<a class=td-heading-self-link href=#green-tea-matcha-cookies aria-label="Heading self-link"></a></h1><p>For a team event during the Christmas season we decided to completely reinterpret the topic <code>cookies</code>. :-)</p><img style=width:50%;height:auto;margin:0,auto src=/blog/2018/images/slider/cookie-00.jpg><p>Matcha cookies have the delicate flavor and color of green tea. These soft, pillowy and chewy green tea cookies
are perfect with tea. And of course they fit perfectly to our logo.</p><h2 id=ingredients>Ingredients<a class=td-heading-self-link href=#ingredients aria-label="Heading self-link"></a></h2><ul><li>1 stick butter, softened</li><li>⅞ cup of granulated sugar</li><li>1 cup + 2 tablespoons all-purpose flour</li><li>2 eggs</li><li>1¼ tablespoons culinary grade matcha powder</li><li>1 teaspoon baking powder</li><li>pinch of salt</li></ul><h2 id=instructions>Instructions<a class=td-heading-self-link href=#instructions aria-label="Heading self-link"></a></h2><ol><li>Cream together the butter and sugar in a large mixing bowl - it should be creamy colored and airy. A hand blender or stand mixer works well for this. This helps the cookie become fluffy and chewy.</li><li>Gently incorporate the eggs to the butter mixture one at a time.</li><li>In a separate bowl, sift together all the dry ingredients.</li><li>Add the dry ingredients to the wet by adding a little at a time and folding or gently mixing the batter together. Keep going until you&rsquo;ve incorporated all the remaining flour mixture. The dough should be a beautiful green color.</li><li>Chill the dough for at least an hour - up to overnight. The longer the better!</li><li>Preheat your oven to 325 F.</li><li>Roll the dough into balls the size of ping pong balls and place them on a non-stick cookie sheet.</li><li>Bake them for 12-15 minutes until the bottoms just start to become golden brown and the cookie no longer looks wet in the middle. Note: you can always bake them at 350 F for a less moist, fluffy cookie. It will bake faster by about 2-4 minutes 350 F so watch them closely.</li><li>Remove and let cool on a rack and enjoy!</li></ol><h2 id=note>Note<a class=td-heading-self-link href=#note aria-label="Heading self-link"></a></h2><p>Make sure you get culinary grade matcha powder. You should be able to find this in Asian or natural grocers.</p><img style=width:50%;height:auto src=/blog/2018/images/slider/cookie-01.jpg>
<img style=width:50%;height:auto src=/blog/2018/images/slider/cookie-02.jpg>
<img style=width:50%;height:auto src=/blog/2018/images/slider/cookie-03.jpg>
<img style=width:50%;height:auto src=/blog/2018/images/slider/cookie-05.jpg></div><div class=td-content style=page-break-before:always><h1 id=pg-868bdce99d5eded6025c09d01e1976ea>Cookies Are Dangerous...</h1><div class="td-byline mb-4"><time datetime=2018-12-22 class=text-body-secondary>Saturday, December 22, 2018</time></div><p><strong>&mldr;they mess up the figure.</strong></p><p><img src=/blog/2018/images/cookie.jpg alt=cookie></p><p>For a team event during the Christmas season we decided to completely reinterpret the topic <code>cookies</code>&mldr; since the vegetables have gone on a well-deserved vacation. :-)</p><p>Get the recipe at <a href=/blog/2018/12.25-gardener_cookies/>Gardener Cookies</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ae91555f579ea09dbf712ea3545addec>Hibernate a Cluster to Save Money</h1><div class="td-byline mb-4"><time datetime=2018-07-11 class=text-body-secondary>Wednesday, July 11, 2018</time></div><p>You want to experiment with Kubernetes or set up a customer scenario, but don&rsquo;t want to run the cluster 24 / 7 due to cost reasons?</p><p><img src=/blog/2018/images/teaser-patched-1.svg alt=teaser-patched-1></p><p>Gardener gives you the possibility to scale your cluster down to <strong>zero nodes</strong>.</p><p>Learn more on <a href=/docs/gardener/shoot/shoot_hibernate/>Hibernate a Cluster</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-eb76fdb04e7110115998d40f563102e2>Anti Patterns</h1><div class="td-byline mb-4"><time datetime=2018-06-11 class=text-body-secondary>Monday, June 11, 2018</time></div><p><img src=/blog/2018/images/blog-antipattern.png alt></p><h2 id=running-as-root-user>Running as Root User<a class=td-heading-self-link href=#running-as-root-user aria-label="Heading self-link"></a></h2><p>Whenever possible, do not run containers as root users. One could be tempted to say that in Kubernetes, the node and pods are well separated, however, the host and the container share the same kernel. If the container is compromised, a root user can damage the underlying node.</p><p>Instead of running a root user, use <code>RUN groupadd -r anygroup && useradd -r -g anygroup myuser</code> to create a group and a user in it. Use the <code>USER</code> command to switch to this user.</p><h2 id=storing-data-or-logs-in-containers>Storing Data or Logs in Containers<a class=td-heading-self-link href=#storing-data-or-logs-in-containers aria-label="Heading self-link"></a></h2><p>Containers are ideal for stateless applications and should be transient. This means that no data or logs should be stored in the container, as they are lost when the container is closed. If absolutely necessary, you can use persistence volumes instead to persist them outside the containers.</p><p>However, an ELK stack is preferred for storing and processing log files.</p><p>Learn more on <a href=/docs/guides/applications/antipattern/>Common Kubernetes Antipattern</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2cac7ad5c0a3f96aba02964baaa01ed3>Auditing Kubernetes for Secure Setup</h1><div class="td-byline mb-4"><time datetime=2018-06-11 class=text-body-secondary>Monday, June 11, 2018</time></div><p>In summer 2018, the <a href=https://github.com/gardener/gardener>Gardener project team</a> asked <a href=https://kinvolk.io/>Kinvolk</a> to execute several penetration tests in its role as a third-party contractor. The goal of this ongoing work is to increase the security of all Gardener stakeholders in the open source community. Following the Gardener architecture, the control plane of a Gardener managed shoot cluster resides in the corresponding seed cluster. This is a <a href=https://kubernetes.io/blog/2018/05/17/gardener/#kubernetes-control-plane>Control-Plane-as-a-Service</a> with a <a href=https://kubernetes.io/blog/2018/05/17/gardener/#network-air-gap>network air gap</a>.</p><p><img src=/blog/2018/images/teaser.svg alt=teaser></p><p>Along the way we found various kinds of security issues, for example, due to misconfiguration or missing isolation, as well as two special problems with upstream Kubernetes and its Control-Plane-as-a-Service architecture.</p><p>Learn more on <a href=/docs/guides/applications/insecure-configuration/>Auditing Kubernetes for Secure Setup</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9522eefd9bdd411630f132be1f15c22a>Big Things Come in Small Packages</h1><div class="td-byline mb-4"><time datetime=2018-06-11 class=text-body-secondary>Monday, June 11, 2018</time></div><p>Microservices tend to use smaller runtimes but you can <strong>use what you have</strong> today - and this can be a <strong>problem in Kubernetes</strong>.</p><p>Switching your architecture from a monolith to microservices has many advantages, both in the way you write software and the way it is used throughout its lifecycle. In this post, my attempt is to cover one problem which does not get as much attention and discussion - <strong>size of the technology stack</strong>.</p><h2 id=general-purpose-technology-stack>General Purpose Technology Stack<a class=td-heading-self-link href=#general-purpose-technology-stack aria-label="Heading self-link"></a></h2><p><img src=/blog/2018/images/blog-service-common-stack.png alt=service-common-stack></p><p>There is a tendency to be more generalized in development and to apply this pattern to all services. One feels that a homogeneous image of the technology stack is good if it is the same for all services.</p><p>One forgets, however, that a large percentage of the integrated infrastructure is not used by all services in the same way, and is therefore only a burden. Thus, resources are wasted and the entire application becomes expensive in operation and scales very badly.</p><h2 id=light-technology-stack>Light Technology Stack<a class=td-heading-self-link href=#light-technology-stack aria-label="Heading self-link"></a></h2><p>Due to the lightweight nature of your service, you can run more containers on a physical server and virtual machines. The result is higher resource utilization.</p><p><img src=/blog/2018/images/blog-service-service-stack.png alt=service-service-stack></p><p>Additionally, microservices are developed and deployed as containers independently of each another. This means that a development team can develop, optimize, and deploy a microservice without impacting other subsystems.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f242013ec2c69b611a1180f65727653b>Hardening the Gardener Community Setup</h1><div class="td-byline mb-4"><time datetime=2018-06-11 class=text-body-secondary>Monday, June 11, 2018</time></div><p>The <a href=https://github.com/gardener/gardener>Gardener project team</a> has analyzed the impact of the Gardener <a href=https://groups.google.com/forum/#!topic/gardener/Pom2Y70cDpw>CVE-2018-2475</a> and the <a href=https://groups.google.com/forum/#!topic/kubernetes-announce/GVllWCg6L88>Kubernetes CVE-2018-1002105</a> on the Gardener Community Setup. Following some recommendations it is possible to mitigate both vulnerabilities.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f0e497d5686cd6f2172880cef2025356>Kubernetes is Available in Docker for Mac 17.12 CE</h1><div class="td-byline mb-4"><time datetime=2018-06-11 class=text-body-secondary>Monday, June 11, 2018</time></div><table style=border:0><tr><td><img alt="Enabling Kubernetes in Docker image" src=/blog/2018/images/blog-kubernetes-enable.png title="Enabling Kubernetes in Docker image"></td><td valign=top><div><b>Kubernetes is only available in Docker for Mac 17.12 CE and higher</b> on the Edge channel. Kubernetes
support is not included in Docker for Mac Stable releases. To find out more about Stable and Edge channels
and how to switch between them, see
<a href=https://docs.docker.com/docker-for-mac/#general>general configuration</a>.</div></td></tr></table>Docker for Mac 17.12 CE (and higher) Edge includes a standalone Kubernetes server that runs on Mac, so that you can test deploying your Docker workloads on Kubernetes.<p>The Kubernetes client command, kubectl, is included and configured to connect to the local Kubernetes server. If you have kubectl already installed and pointing to some other environment, such as minikube or a GKE cluster, be sure to change the context so that kubectl is pointing to docker-for-desktop. Read more on <a href=https://docs.docker.com/docker-for-mac/#kubernetes>Docker.com</a>.</p><p>I recommend to <a href=/docs/guides/administer-shoots/oidc-login/>setup your shell</a> to see which KUBECONFIG is active.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-96ff114e6aa0e61b36f5f840dfe68647>Namespace Isolation</h1><div class="td-byline mb-4"><time datetime=2018-06-11 class=text-body-secondary>Monday, June 11, 2018</time></div><p>&mldr;or <strong>DENY all traffic from other namespaces</strong></p><p>You can configure a <strong>NetworkPolicy</strong> to deny all traffic from other namespaces while allowing all traffic coming from the same namespace the pod is deployed to. There are many reasons why you may choose to configure Kubernetes network policies:</p><ul><li>Isolate multi-tenant deployments</li><li>Regulatory compliance</li><li>Ensure containers assigned to different environments (e.g. dev/staging/prod) cannot interfere with each another</li></ul><p><img src=/blog/2018/images/blog-namespaceisolation.png alt=namespaceisolation></p><p>Learn more on <a href=/docs/guides/applications/network-isolation/>Namespace Isolation</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9e763e8598a206e9d2ca9ca012c81097>Namespace Scope</h1><div class="td-byline mb-4"><time datetime=2018-06-11 class=text-body-secondary>Monday, June 11, 2018</time></div><p><strong>Should I use:</strong></p><ul style=list-style:none><li>❌ one namespace per user/developer?</li><li>❌ one namespace per team?</li><li>❌ one per service type?</li><li>❌ one namespace per application type?</li><li>😄 <b>one namespace per running instance of your application?</b></li></ul><p><strong>Apply the <a href=https://en.wikipedia.org/wiki/Principle_of_least_privilege>Principle of Least Privilege</a></strong></p><p>All user accounts should run <strong>as few privileges as possible</strong> at all times, and also launch applications with as few privileges as possible. If you share a cluster for a different user separated by a <code>namespace</code>, <strong>the user has access to all <code>namespaces</code></strong> and services per default. It can happen that a user <strong>accidentally uses and destroys</strong> the <code>namespace</code> of a productive application or the <code>namespace</code> of another developer.</p><p><strong>Keep in mind - By default namespaces don&rsquo;t provide:</strong></p><ul><li>Network Isolation</li><li>Access Control</li><li>Audit Logging on user level</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6b0437ab0c60198e4b4ef4546de93d06>ReadWriteMany - Dynamically Provisioned Persistent Volumes Using Amazon EFS</h1><div class="td-byline mb-4"><time datetime=2018-06-11 class=text-body-secondary>Monday, June 11, 2018</time></div><p>The efs-provisioner allows you to mount EFS storage as PersistentVolumes in Kubernetes. It consists of a container that has access to an AWS EFS resource. The container reads a configmap containing the EFS filesystem ID, the AWS region and the name identifying the efs-provisioner. This name will be used later when you create a storage class.</p><p><img src=/blog/2018/images/blog-aws-efs.png alt></p><h2 id=why-efs>Why EFS<a class=td-heading-self-link href=#why-efs aria-label="Heading self-link"></a></h2><ol><li>When you have an application running on multiple nodes which require shared access to a file system.</li><li>When you have an application that requires multiple virtual machines to access the same file system at the same time, AWS EFS is a tool that you can use.</li><li>EFS supports encryption.</li><li>EFS is SSD based storage and its storage capacity and pricing will scale in or out as needed, so there is no need for the system administrator to do additional operations. It can grow to a petabyte scale.</li><li>EFS now supports NFSv4 lock upgrading and downgrading, so yes, you can use sqlite with EFS… even if it was possible before.</li><li>EFS is easy to setup.</li></ol><h2 id=why-not-efs>Why Not EFS<a class=td-heading-self-link href=#why-not-efs aria-label="Heading self-link"></a></h2><ol><li>Sometimes when you think about using a service like EFS, you may also think about <strong>vendor lock-in</strong> and its negative sides.</li><li>Making an EFS backup may decrease your production FS performance; the throughput used by backups counts towards your total file system throughput.</li><li>EFS is expensive when compared to EBS (roughly twice the price of EBS storage).</li><li>EFS is not the magical solution for all your distributed FS problems, it can be slow in many cases. Test, benchmark, and measure to ensure that EFS is a good solution for your use case.</li><li>EFS distributed architecture results in a latency overhead for each file read/write operation.</li><li>If you have the possibility to use a CDN, don’t use EFS, use it for the files which can&rsquo;t be stored in a CDN.</li><li>Don’t use EFS as a caching system, sometimes you could be doing this unintentionally.</li><li>Last but not least, even if EFS is a fully managed NFS, you will face performance problems in many cases, resolving them takes time and needs effort.</li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-c3da5fb402e9b425c8ab19987f15fd55>Shared Storage with S3 Backend</h1><div class="td-byline mb-4"><time datetime=2018-06-11 class=text-body-secondary>Monday, June 11, 2018</time></div><p>The storage is definitely the most complex and important part of an application setup. Once this part is completed, one of the most problematic parts could be solved.</p><p>Mounting an S3 bucket into a pod using <a href=https://github.com/libfuse/libfuse>FUSE</a> allows you to access data stored in S3 via the filesystem. The mount is a pointer to an S3 location, so the data is never synced locally. Once mounted, any pod can read or even write from that directory without the need for explicit keys.</p><p><img src=/blog/2018/images/blog-s3-shared-storage.png alt=s3-shared-storage></p><p>However, it can be used to import and parse large amounts of data into a database.</p><p>Learn more on <a href=https://github.com/freegroup/kube-s3/blob/master/README.md>Shared S3 Storage</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0b684b2e48a9e075f7112413186179eb>Watching Logs of Several Pods</h1><div class="td-byline mb-4"><time datetime=2018-06-11 class=text-body-secondary>Monday, June 11, 2018</time></div><p>One thing that always bothered me was that I couldn&rsquo;t get the logs of several pods at once with <code>kubectl</code>. A simple <code>tail -f &lt;path-to-logfile></code> isn&rsquo;t possible. Certainly, you can use <code>kubectl logs -f &lt;pod-id></code>, but it doesn&rsquo;t help if you want to monitor more than one pod at a time.</p><p>This is something you really need a lot, at least if you run several instances of a pod behind a <code>deployment</code>and you don&rsquo;t have a log viewer service like Kibana set up.</p><p><img src=/blog/2018/images/blog-kubetail.png alt=kubetail></p><p>In that case, kubetail comes to the rescue. It is a small bash script that allows you to aggregate the log files of several pods at the same time in a simple way. The script is called <code>kubetail</code> and is available at <a href=https://github.com/johanhaleby/kubetail>GitHub</a>.</p></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://demo.gardener.cloud>Demo</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://gardener-cloud.slack.com/><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://x.com/GardenerProject><img src=/images/branding/x-logo-white.svg class=media-icon><div class=media-text>X</div></a></li></ul><span class=copyright>Copyright 2019-2025 Gardener project authors.
<a href=https://www.sap.com/about/legal/terms-of-use.html>Terms of Use
<i class="fa fa-external-link" aria-hidden=true></i>
</a>|
<a href=https://www.sap.com/about/legal/terms-of-use.html>Privacy Statement
<i class="fa fa-external-link" aria-hidden=true></i>
</a>|
<a href=https://www.sap.com/about/legal/terms-of-use.html>Legal Disclosure
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=/js/main.min.403ff095218c662472dab60ed98ecbb19431682de5ac7c6159891241cd366af5.js integrity="sha256-QD/wlSGMZiRy2rYO2Y7LsZQxaC3lrHxhWYkSQc02avU=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script><script src=/js/navbar.js></script><script src=/js/filtering.js></script><script src=/js/page-content.js></script><script src=/js/community-index.js></script></body></html>