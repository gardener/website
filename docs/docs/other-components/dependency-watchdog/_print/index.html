<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://gardener.cloud/docs/other-components/dependency-watchdog/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/other-components/dependency-watchdog/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Dependency Watchdog | Gardener</title>
<meta name=description content="A watchdog which actively looks out for disruption and recovery of critical services"><meta property="og:url" content="https://gardener.cloud/docs/other-components/dependency-watchdog/"><meta property="og:site_name" content="Gardener"><meta property="og:title" content="Dependency Watchdog"><meta property="og:description" content="A watchdog which actively looks out for disruption and recovery of critical services"><meta property="og:locale" content="en_US"><meta property="og:type" content="website"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="Dependency Watchdog"><meta itemprop=description content="A watchdog which actively looks out for disruption and recovery of critical services"><meta itemprop=wordCount content="171"><meta itemprop=image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="Dependency Watchdog"><meta name=twitter:description content="A watchdog which actively looks out for disruption and recovery of critical services"><link rel=preload href=/scss/main.min.fbc210e652a04e9638febb26a9e0de86f3368657a8748b2db9afd7395efe70ef.css as=style integrity="sha256-+8IQ5lKgTpY4/rsmqeDehvM2hleodIstua/XOV7+cO8=" crossorigin=anonymous><link href=/scss/main.min.fbc210e652a04e9638febb26a9e0de86f3368657a8748b2db9afd7395efe70ef.css rel=stylesheet integrity="sha256-+8IQ5lKgTpY4/rsmqeDehvM2hleodIstua/XOV7+cO8=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script><script defer src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><style>.nav-link:hover{text-decoration:none}.ml-md-auto{margin-left:auto!important}.td-search__icon{color:#fff!important}.td-search__input.form-control::placeholder{color:#fff;border:1px;border-radius:20px}.td-search__input.form-control{border:1px;border-radius:20px}.td-search__input{max-width:90%}.td-search:not(:focus-within){color:#fff!important}</style><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=https://demo.gardener.cloud target=_blank><span>Demo</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><div class=dropdown><a href=/docs class=nav-link>Documentation</a><div class=dropdown-content><a class=taxonomy-term href=/docs>Users</a>
<a class=taxonomy-term href=/docs>Operators</a>
<a class=taxonomy-term href=/docs>Developers</a>
<a class=taxonomy-term href=/docs>All</a></div></div></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><div class="td-search td-search--offline"><div class=td-search__icon></div><input type=search class="td-search__input form-control" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.f4d8e374b6a35ded0a59eb22c687502b.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/other-components/dependency-watchdog/>Return to the regular view of this page</a>.</p></div><h1 class=title>Dependency Watchdog</h1><div class=lead>A watchdog which actively looks out for disruption and recovery of critical services</div><div class=content><h1 id=dependency-watchdog>Dependency Watchdog</h1><img src=/__resources/gardener-dwd_833554.png style=width:200px><p><a href=https://api.reuse.software/info/github.com/gardener/dependency-watchdog><img src=https://api.reuse.software/badge/github.com/gardener/dependency-watchdog alt="REUSE status"></a>
<a href=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/dependency-watchdog-master/jobs/master-head-update-job/><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/dependency-watchdog-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://testgrid.k8s.io/q/summary/gardener-dependency-watchdog/ci-dependency-watchdog-unit/tests_status><img src="https://testgrid.k8s.io/q/summary/gardener-dependency-watchdog/ci-dependency-watchdog-unit/tests_status?style=svg" alt="Unit Tests"></a>
<a href=https://goreportcard.com/report/github.com/gardener/dependency-watchdog><img src=https://goreportcard.com/badge/github.com/gardener/dependency-watchdog alt="Go Report Card"></a>
<a href=https://pkg.go.dev/github.com/gardener/dependency-watchdog><img src=https://godoc.org/github.com/gardener/dependency-watchdog?status.svg alt=GoDoc></a></p><h2 id=overview>Overview</h2><p>A watchdog which actively looks out for disruption and recovery of critical services. If there is a disruption then it will prevent cascading failure by conservatively scaling down dependent configured resources and if a critical service has just recovered then it will expedite the recovery of dependent services/pods.</p><p>Avoiding cascading failure is handled by <a href=/docs/other-components/dependency-watchdog/concepts/prober/>Prober</a> component and expediting recovery of dependent services/pods is handled by <a href=/docs/other-components/dependency-watchdog/concepts/weeder/>Weeder</a> component. These are separately deployed as individual pods.</p><h3 id=current-limitation--future-scope>Current Limitation & Future Scope</h3><p>Although in the current offering the <code>Prober</code> is tailored to handle one such use case of <code>kube-apiserver</code> connectivity, but the usage of prober can be extended to solve similar needs for other scenarios where the components involved might be different.</p><h2 id=start-using-or-developing-the-dependency-watchdog>Start using or developing the Dependency Watchdog</h2><p>See our documentation in the /docs repository, please <a href=https://github.com/gardener/dependency-watchdog/blob/master/docs/README.md>find the index here</a>.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>We always look forward to active community engagement.</p><p>Please report bugs or suggestions on how we can enhance <code>dependency-watchdog</code> to address additional recovery scenarios on <a href=https://github.com/gardener/dependency-watchdog/issues>GitHub issues</a></p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-75a0184996815280264f682a2b501338>1 - Concepts</h1></div><div class=td-content><h1 id=pg-37f31312dc8095d109349c3174ff97f5>1.1 - Prober</h1><h1 id=prober>Prober</h1><h2 id=overview>Overview</h2><p>Prober starts asynchronous and periodic probes for every shoot cluster. The first probe is the api-server probe which checks the reachability of the API Server from the control plane. The second probe is the lease probe which is done after the api server probe is successful and checks if the number of expired node leases is below a certain threshold.
If the lease probe fails, it will scale down the dependent kubernetes resources. Once the connectivity to <code>kube-apiserver</code> is reestablished and the number of expired node leases are within the accepted threshold, the prober will then proactively scale up the dependent kubernetes resources it had scaled down earlier. The failure threshold fraction for lease probe
and dependent kubernetes resources are defined in <a href=https://github.com/gardener/dependency-watchdog/blob/master/example/01-dwd-prober-configmap.yaml>configuration</a> that is passed to the prober.</p><h3 id=origin>Origin</h3><p>In a shoot cluster (a.k.a data plane) each node runs a kubelet which periodically renewes its lease. Leases serve as heartbeats informing Kube Controller Manager that the node is alive. The connectivity between the kubelet and the Kube ApiServer can break for different reasons and not recover in time.</p><p>As an example, consider a large shoot cluster with several hundred nodes. There is an issue with a NAT gateway on the shoot cluster which prevents the Kubelet from any node in the shoot cluster to reach its control plane Kube ApiServer. As a consequence, Kube Controller Manager transitioned the nodes of this shoot cluster to <code>Unknown</code> state.</p><p><a href=https://github.com/gardener/machine-controller-manager>Machine Controller Manager</a> which also runs in the shoot control plane reacts to any changes to the Node status and then takes action to recover backing VMs/machine(s). It waits for a grace period and then it will begin to replace the unhealthy machine(s) with new ones.</p><p>This replacement of healthy machines due to a broken connectivity between the worker nodes and the control plane Kube ApiServer results in undesired downtimes for customer workloads that were running on these otherwise healthy nodes. It is therefore required that there be an actor which detects the connectivity loss between the the kubelet and shoot cluster&rsquo;s Kube ApiServer and proactively scales down components in the shoot control namespace which could exacerbate the availability of nodes in the shoot cluster.</p><h2 id=dependency-watchdog-prober-in-gardener>Dependency Watchdog Prober in Gardener</h2><p>Prober is a central component which is deployed in the <code>garden</code> namespace in the seed cluster. Control plane components for a shoot are deployed in a dedicated shoot namespace for the shoot within the seed cluster.</p><img src=/__resources/prober-components.excalidraw_dd1d01.png><blockquote><p>NOTE: If you are not familiar with what gardener components like seed, shoot then please see the <a href=/docs/other-components/dependency-watchdog/concepts/prober/#appendix>appendix</a> for links.</p></blockquote><p>Prober periodically probes Kube ApiServer via two separate probes:</p><ol><li>API Server Probe: Local cluster DNS name which resolves to the ClusterIP of the Kube Apiserver</li><li>Lease Probe: Checks for number of expired leases to be within the specified threshold. The threshold defines the limit after which DWD can say that the kubelets are not able to reach the API server.</li></ol><h2 id=behind-the-scene>Behind the scene</h2><p>For all active shoot clusters (which have not been hibernated or deleted or moved to another seed via control-plane-migration), prober will schedule a probe to run periodically. During each run of a probe it will do the following:</p><ol><li>Checks if the Kube ApiServer is reachable via local cluster DNS. This should always succeed and will fail only when the Kube ApiServer has gone down. If the Kube ApiServer is down then there can be no further damage to the existing shoot cluster (barring new requests to the Kube Api Server).</li><li>Only if the probe is able to reach the Kube ApiServer via local cluster DNS, will it attempt to check the number of expired node leases in the shoot. The node lease renewal is done by the Kubelet, and so we can say that the lease probe is checking if the kubelet is able to reach the API server. If the number of expired node leases reaches
the threshold, then the probe fails.</li><li>If and when a lease probe fails, then it will initiate a scale-down operation for dependent resources as defined in the prober configuration.</li><li>In subsequent runs it will keep performing the lease probe. If it is successful, then it will start the scale-up operation for dependent resources as defined in the configuration.</li></ol><h3 id=prober-lifecycle>Prober lifecycle</h3><p>A reconciler is registered to listen to all events for <a href=/docs/gardener/api-reference/extensions/#extensions.gardener.cloud/v1alpha1.Cluster>Cluster</a> resource.</p><p>When a <code>Reconciler</code> receives a request for a <code>Cluster</code> change, it will query the extension kube-api server to get the <code>Cluster</code> resource.</p><p>In the following cases it will either remove an existing probe for this cluster or skip creating a new probe:</p><ol><li>Cluster is marked for deletion.</li><li>Hibernation has been enabled for the cluster.</li><li>There is an ongoing seed migration for this cluster.</li><li>If a new cluster is created with no workers.</li><li>If an update is made to the cluster by removing all workers (in other words making it worker-less).</li></ol><p>If none of the above conditions are true and there is no existing probe for this cluster then a new probe will be created, registered and started.</p><h3 id=probe-failure-identification>Probe failure identification</h3><p>DWD probe can either be a success or it could return an error. If the API server probe fails, the lease probe is not done and the probes will be retried. If the error is a <code>TooManyRequests</code> error due to requests to the Kube-API-Server being throttled,
then the probes are retried after a backOff of <code>backOffDurationForThrottledRequests</code>.</p><p>If the lease probe fails, then the error could be due to failure in listing the leases. In this case, no scaling operations are performed. If the error in listing the leases is a <code>TooManyRequests</code> error due to requests to the Kube-API-Server being throttled,
then the probes are retried after a backOff of <code>backOffDurationForThrottledRequests</code>.</p><p>If there is no error in listing the leases, then the Lease probe fails if the number of expired leases reaches the threshold fraction specified in the <a href=https://github.com/gardener/dependency-watchdog/blob/master/example/01-dwd-prober-configmap.yaml>configuration</a>.
A lease is considered expired in the following scenario:-</p><pre tabindex=0><code>	time.Now() &gt;= lease.Spec.RenewTime + (p.config.KCMNodeMonitorGraceDuration.Duration * expiryBufferFraction)
</code></pre><p>Here, <code>lease.Spec.RenewTime</code> is the time when current holder of a lease has last updated the lease. <code>config</code> is the probe config generated from the <a href=https://github.com/gardener/dependency-watchdog/blob/master/example/01-dwd-prober-configmap.yaml>configuration</a> and
<code>KCMNodeMonitorGraceDuration</code> is amount of time which KCM allows a running Node to be unresponsive before marking it unhealthy (See <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/#:~:text=Amount%20of%20time%20which%20we%20allow%20running%20Node%20to%20be%20unresponsive%20before%20marking%20it%20unhealthy.%20Must%20be%20N%20times%20more%20than%20kubelet%27s%20nodeStatusUpdateFrequency%2C%20where%20N%20means%20number%20of%20retries%20allowed%20for%20kubelet%20to%20post%20node%20status.">ref</a>)
. <code>expiryBufferFraction</code> is a hard coded value of <code>0.75</code>. Using this fraction allows the prober to intervene before KCM marks a node as unknown, but at the same time allowing kubelet sufficient retries to renew the node lease (Kubelet renews the lease every <code>10s</code> See <a href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#:~:text=The%20lease%20is%20currently%20renewed%20every%2010s%2C%20per%20KEP%2D0009.">ref</a>).</p><h2 id=appendix>Appendix</h2><ul><li><a href=https://github.com/gardener/gardener/blob/master/docs>Gardener</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/14-reversed-cluster-vpn.md>Reverse Cluster VPN</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-68d260224b4548925f479aa69121626d>1.2 - Weeder</h1><h1 id=weeder>Weeder</h1><h2 id=overview>Overview</h2><p>Weeder watches for an update to service endpoints and on receiving such an event it will create a time-bound watch for all configured dependent pods that need to be actively recovered in case they have not yet recovered from <code>CrashLoopBackoff</code> state. In a nutshell it accelerates recovery of pods when an upstream service recovers.</p><p>An interference in automatic recovery for dependent pods is required because kubernetes pod restarts a container with an exponential backoff when the pod is in <code>CrashLoopBackOff</code> state. This backoff could become quite large if the service stays down for long. Presence of weeder would not let that happen as it&rsquo;ll restart the pod.</p><h2 id=prerequisites>Prerequisites</h2><p>Before we understand how Weeder works, we need to be familiar with kubernetes <a href=https://kubernetes.io/docs/concepts/services-networking/service/>services & endpoints</a>.</p><blockquote><p>NOTE: If a kubernetes service is created with selectors then kubernetes will create corresponding endpoint resource which will have the same name as that of the service. In weeder implementation service and endpoint name is used interchangeably.</p></blockquote><h2 id=config>Config</h2><p>Weeder can be configured via command line arguments and a weeder configuration. See <a href=/docs/other-components/dependency-watchdog/deployment/configure/#weeder>configure weeder</a>.</p><h2 id=internals>Internals</h2><p>Weeder keeps a watch on the events for the specified endpoints in the config. For every endpoints a list of <code>podSelectors</code> can be specified. It cretes a weeder object per endpoints resource when it receives a satisfactory <code>Create</code> or <code>Update</code> event. Then for every podSelector it creates a goroutine. This goroutine keeps a watch on the pods with labels as per the podSelector and kills any pod which turn into <code>CrashLoopBackOff</code>. Each weeder lives for <code>watchDuration</code> interval which has a default value of 5 mins if not explicitly set.</p><p>To understand the actions taken by the weeder lets use the following diagram as a reference.
<img src=/__resources/weeder-components.excalidraw_931119.png>
Let us also assume the following configuration for the weeder:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>watchDuration: 2m0s
</span></span><span style=display:flex><span>servicesAndDependantSelectors:
</span></span><span style=display:flex><span>  etcd-main-client: <span style=color:green># name of the service/endpoint for etcd statefulset that weeder will receive events for.</span>
</span></span><span style=display:flex><span>    podSelectors: <span style=color:green># all pods matching the label selector are direct dependencies for etcd service</span>
</span></span><span style=display:flex><span>      - matchExpressions:
</span></span><span style=display:flex><span>          - key: gardener.cloud/role
</span></span><span style=display:flex><span>            operator: In
</span></span><span style=display:flex><span>            values:
</span></span><span style=display:flex><span>              - controlplane
</span></span><span style=display:flex><span>          - key: role
</span></span><span style=display:flex><span>            operator: In
</span></span><span style=display:flex><span>            values:
</span></span><span style=display:flex><span>              - apiserver
</span></span><span style=display:flex><span>  kube-apiserver: <span style=color:green># name of the service/endpoint for kube-api-server pods that weeder will receive events for. </span>
</span></span><span style=display:flex><span>    podSelectors: <span style=color:green># all pods matching the label selector are direct dependencies for kube-api-server service</span>
</span></span><span style=display:flex><span>      - matchExpressions:
</span></span><span style=display:flex><span>          - key: gardener.cloud/role
</span></span><span style=display:flex><span>            operator: In
</span></span><span style=display:flex><span>            values:
</span></span><span style=display:flex><span>              - controlplane
</span></span><span style=display:flex><span>          - key: role
</span></span><span style=display:flex><span>            operator: NotIn
</span></span><span style=display:flex><span>            values:
</span></span><span style=display:flex><span>              - main
</span></span><span style=display:flex><span>              - apiserver
</span></span></code></pre></div><p>Only for the sake of demonstration lets pick the first service -> dependent pods tuple (<code>etcd-main-client</code> as the service endpoint).</p><blockquote><ol><li>Assume that there are 3 replicas for etcd statefulset.</li><li>Time here is just for showing the series of events</li></ol></blockquote><ul><li><code>t=0</code> -> all etcd pods go down</li><li><code>t=10</code> -> kube-api-server pods transition to CrashLoopBackOff</li><li><code>t=100</code> -> all etcd pods recover together</li><li><code>t=101</code> -> Weeder sees <code>Update</code> event for <code>etcd-main-client</code> endpoints resource</li><li><code>t=102</code> -> go routine created to keep watch on kube-api-server pods</li><li><code>t=103</code> -> Since kube-api-server pods are still in CrashLoopBackOff, weeder deletes the pods to accelerate the recovery.</li><li><code>t=104</code> -> new kube-api-server pod created by replica-set controller in kube-controller-manager</li></ul><h3 id=points-to-note>Points to Note</h3><ul><li>Weeder only respond on <code>Update</code> events where a <code>notReady</code> endpoints resource turn to <code>Ready</code>. Thats why there was no weeder action at time <code>t=10</code> in the example above.<ul><li><code>notReady</code> -> no backing pod is Ready</li><li><code>Ready</code> -> atleast one backing pod is Ready</li></ul></li><li>Weeder doesn&rsquo;t respond on <code>Delete</code> events</li><li>Weeder will always wait for the entire <code>watchDuration</code>. If the dependent pods transition to CrashLoopBackOff after the watch duration or even after repeated deletion of these pods they do not recover then weeder will exit. Quality of service offered via a weeder is only Best-Effort.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7c43b00ba717958d1829ad1c780cc625>2 - Deployment</h1></div><div class=td-content><h1 id=pg-922ffeaf5b813bbead3438c478d3ffa4>2.1 - Configure</h1><h1 id=configure-dependency-watchdog-components>Configure Dependency Watchdog Components</h1><h2 id=prober>Prober</h2><p>Dependency watchdog prober command takes command-line-flags which are meant to fine-tune the prober. In addition a <code>ConfigMap</code> is also mounted to the container which provides tuning knobs for the all probes that the prober starts.</p><h3 id=command-line-arguments>Command line arguments</h3><p>Prober can be configured via the following flags:</p><table><thead><tr><th>Flag Name</th><th>Type</th><th>Required</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>kube-api-burst</td><td>int</td><td>No</td><td>10</td><td>Burst to use while talking with kubernetes API server. The number must be >= 0. If it is 0 then a default value of 10 will be used</td></tr><tr><td>kube-api-qps</td><td>float</td><td>No</td><td>5.0</td><td>Maximum QPS (queries per second) allowed when talking with kubernetes API server. The number must be >= 0. If it is 0 then a default value of 5.0 will be used</td></tr><tr><td>concurrent-reconciles</td><td>int</td><td>No</td><td>1</td><td>Maximum number of concurrent reconciles</td></tr><tr><td>config-file</td><td>string</td><td>Yes</td><td>NA</td><td>Path of the config file containing the configuration to be used for all probes</td></tr><tr><td>metrics-bind-addr</td><td>string</td><td>No</td><td>&ldquo;:9643&rdquo;</td><td>The TCP address that the controller should bind to for serving prometheus metrics</td></tr><tr><td>health-bind-addr</td><td>string</td><td>No</td><td>&ldquo;:9644&rdquo;</td><td>The TCP address that the controller should bind to for serving health probes</td></tr><tr><td>enable-leader-election</td><td>bool</td><td>No</td><td>false</td><td>In case prober deployment has more than 1 replica for high availability, then it will be setup in a active-passive mode. Out of many replicas one will become the leader and the rest will be passive followers waiting to acquire leadership in case the leader dies.</td></tr><tr><td>leader-election-namespace</td><td>string</td><td>No</td><td>&ldquo;garden&rdquo;</td><td>Namespace in which leader election resource will be created. It should be the same namespace where DWD pods are deployed</td></tr><tr><td>leader-elect-lease-duration</td><td>time.Duration</td><td>No</td><td>15s</td><td>The duration that non-leader candidates will wait after observing a leadership renewal until attempting to acquire leadership of a led but unrenewed leader slot. This is effectively the maximum duration that a leader can be stopped before it is replaced by another candidate. This is only applicable if leader election is enabled.</td></tr><tr><td>leader-elect-renew-deadline</td><td>time.Duration</td><td>No</td><td>10s</td><td>The interval between attempts by the acting master to renew a leadership slot before it stops leading. This must be less than or equal to the lease duration. This is only applicable if leader election is enabled.</td></tr><tr><td>leader-elect-retry-period</td><td>time.Duration</td><td>No</td><td>2s</td><td>The duration the clients should wait between attempting acquisition and renewal of a leadership. This is only applicable if leader election is enabled.</td></tr></tbody></table><p>You can view an example kubernetes prober <a href=https://github.com/gardener/dependency-watchdog/blob/master/example/03-dwd-prober-deployment.yaml>deployment</a> YAML to see how these command line args are configured.</p><h3 id=prober-configuration>Prober Configuration</h3><p>A probe configuration is mounted as <code>ConfigMap</code> to the container. The path to the config file is configured via <code>config-file</code> command line argument as mentioned above. Prober will start one probe per Shoot control plane hosted within the Seed cluster. Each such probe will run asynchronously and will periodically connect to the Kube ApiServer of the Shoot. Configuration below will influence each such probe.</p><p>You can view an example YAML configuration provided as <code>data</code> in a <code>ConfigMap</code> <a href=https://github.com/gardener/dependency-watchdog/blob/master/example/01-dwd-prober-configmap.yaml>here</a>.</p><table><thead><tr><th>Name</th><th>Type</th><th>Required</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>kubeConfigSecretName</td><td>string</td><td>Yes</td><td>NA</td><td>Name of the kubernetes Secret which has the encoded KubeConfig required to connect to the Shoot control plane Kube ApiServer via an internal domain. This typically uses the local cluster DNS.</td></tr><tr><td>probeInterval</td><td>metav1.Duration</td><td>No</td><td>10s</td><td>Interval with which each probe will run.</td></tr><tr><td>initialDelay</td><td>metav1.Duration</td><td>No</td><td>30s</td><td>Initial delay for the probe to become active. Only applicable when the probe is created for the first time.</td></tr><tr><td>probeTimeout</td><td>metav1.Duration</td><td>No</td><td>30s</td><td>In each run of the probe it will attempt to connect to the Shoot Kube ApiServer. probeTimeout defines the timeout after which a single run of the probe will fail.</td></tr><tr><td>backoffJitterFactor</td><td>float64</td><td>No</td><td>0.2</td><td>Jitter with which a probe is run.</td></tr><tr><td>dependentResourceInfos</td><td>[]prober.DependentResourceInfo</td><td>Yes</td><td>NA</td><td>Detailed below.</td></tr><tr><td>kcmNodeMonitorGraceDuration</td><td>metav1.Duration</td><td>Yes</td><td>NA</td><td>It is the node-monitor-grace-period set in the kcm flags. Used to determine whether a node lease can be considered expired.</td></tr><tr><td>nodeLeaseFailureFraction</td><td>float64</td><td>No</td><td>0.6</td><td>is used to determine the maximum number of leases that can be expired for a lease probe to succeed.</td></tr></tbody></table><h3 id=dependentresourceinfo>DependentResourceInfo</h3><p>If a lease probe fails, then it scales down the dependent resources defined by this property. Similarly, if the lease probe is now successful, then it scales up the dependent resources defined by this property.</p><p>Each dependent resource info has the following properties:</p><table><thead><tr><th>Name</th><th>Type</th><th>Required</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>ref</td><td>autoscalingv1.CrossVersionObjectReference</td><td>Yes</td><td>NA</td><td>It is a collection of ApiVersion, Kind and Name for a kubernetes resource thus serving as an identifier.</td></tr><tr><td>optional</td><td>bool</td><td>Yes</td><td>NA</td><td>It is possible that a dependent resource is optional for a Shoot control plane. This property enables a probe to determine the correct behavior in case it is unable to find the resource identified via <code>ref</code>.</td></tr><tr><td>scaleUp</td><td>prober.ScaleInfo</td><td>No</td><td></td><td>Captures the configuration to scale up this resource. Detailed below.</td></tr><tr><td>scaleDown</td><td>prober.ScaleInfo</td><td>No</td><td></td><td>Captures the configuration to scale down this resource. Detailed below.</td></tr></tbody></table><blockquote><p>NOTE: Since each dependent resource is a target for scale up/down, therefore it is mandatory that the resource reference points a kubernetes resource which has a <code>scale</code> subresource.</p></blockquote><h3 id=scaleinfo>ScaleInfo</h3><p>How to scale a <code>DependentResourceInfo</code> is captured in <code>ScaleInfo</code>. It has the following properties:</p><table><thead><tr><th>Name</th><th>Type</th><th>Required</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>level</td><td>int</td><td>Yes</td><td>NA</td><td>Detailed below.</td></tr><tr><td>initialDelay</td><td>metav1.Duration</td><td>No</td><td>0s (No initial delay)</td><td>Once a decision is taken to scale a resource then via this property a delay can be induced before triggering the scale of the dependent resource.</td></tr><tr><td>timeout</td><td>metav1.Duration</td><td>No</td><td>30s</td><td>Defines the timeout for the scale operation to finish for a dependent resource.</td></tr></tbody></table><p><strong>Determining target replicas</strong></p><p>Prober cannot assume any target replicas during a scale-up operation for the following reasons:</p><ol><li>Kubernetes resources could be set to provide highly availability and the number of replicas could wary from one shoot control plane to the other. In gardener the number of replicas of pods in shoot namespace are controlled by the <a href=/docs/guides/high-availability/control-plane/>shoot control plane configuration</a>.</li><li>If Horizontal Pod Autoscaler has been configured for a kubernetes dependent resource then it could potentially change the <code>spec.replicas</code> for a deployment/statefulset.</li></ol><p>Given the above constraint lets look at how prober determines the target replicas during scale-down or scale-up operations.</p><ol><li><p><code>Scale-Up</code>: Primary responsibility of a probe while performing a scale-up is to restore the replicas of a kubernetes dependent resource prior to scale-down. In order to do that it updates the following for each dependent resource that requires a scale-up:</p><ol><li><code>spec.replicas</code>: Checks if <code>dependency-watchdog.gardener.cloud/replicas</code> is set. If it is, then it will take the value stored against this key as the target replicas. To be a valid value it should always be greater than 0.</li><li>If <code>dependency-watchdog.gardener.cloud/replicas</code> annotation is not present then it falls back to the hard coded default value for scale-up which is set to 1.</li><li>Removes the annotation <code>dependency-watchdog.gardener.cloud/replicas</code> if it exists.</li></ol></li><li><p><code>Scale-Down</code>: To scale down a dependent kubernetes resource it does the following:</p><ol><li>Adds an annotation <code>dependency-watchdog.gardener.cloud/replicas</code> and sets its value to the current value of <code>spec.replicas</code>.</li><li>Updates <code>spec.replicas</code> to 0.</li></ol></li></ol><p><strong>Level</strong></p><p>Each dependent resource that should be scaled up or down is associated to a level. Levels are ordered and processed in ascending order (starting with 0 assigning it the highest priority). Consider the following configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>dependentResourceInfos:
</span></span><span style=display:flex><span>  - ref: 
</span></span><span style=display:flex><span>      kind: <span style=color:#a31515>&#34;Deployment&#34;</span>
</span></span><span style=display:flex><span>      name: <span style=color:#a31515>&#34;kube-controller-manager&#34;</span>
</span></span><span style=display:flex><span>      apiVersion: <span style=color:#a31515>&#34;apps/v1&#34;</span>
</span></span><span style=display:flex><span>    scaleUp: 
</span></span><span style=display:flex><span>      level: 1 
</span></span><span style=display:flex><span>    scaleDown: 
</span></span><span style=display:flex><span>      level: 0 
</span></span><span style=display:flex><span>  - ref:
</span></span><span style=display:flex><span>      kind: <span style=color:#a31515>&#34;Deployment&#34;</span>
</span></span><span style=display:flex><span>      name: <span style=color:#a31515>&#34;machine-controller-manager&#34;</span>
</span></span><span style=display:flex><span>      apiVersion: <span style=color:#a31515>&#34;apps/v1&#34;</span>
</span></span><span style=display:flex><span>    scaleUp:
</span></span><span style=display:flex><span>      level: 1
</span></span><span style=display:flex><span>    scaleDown:
</span></span><span style=display:flex><span>      level: 1
</span></span><span style=display:flex><span>  - ref:
</span></span><span style=display:flex><span>      kind: <span style=color:#a31515>&#34;Deployment&#34;</span>
</span></span><span style=display:flex><span>      name: <span style=color:#a31515>&#34;cluster-autoscaler&#34;</span>
</span></span><span style=display:flex><span>      apiVersion: <span style=color:#a31515>&#34;apps/v1&#34;</span>
</span></span><span style=display:flex><span>    scaleUp:
</span></span><span style=display:flex><span>      level: 0
</span></span><span style=display:flex><span>    scaleDown:
</span></span><span style=display:flex><span>      level: 2
</span></span></code></pre></div><p>Let us order the dependent resources by their respective levels for both scale-up and scale-down. We get the following order:</p><p><em>Scale Up Operation</em></p><p>Order of scale up will be:</p><ol><li>cluster-autoscaler</li><li>kube-controller-manager and machine-controller-manager will be scaled up concurrently after cluster-autoscaler has been scaled up.</li></ol><p><em>Scale Down Operation</em></p><p>Order of scale down will be:</p><ol><li>kube-controller-manager</li><li>machine-controller-manager after (1) has been scaled down.</li><li>cluster-autoscaler after (2) has been scaled down.</li></ol><h3 id=disableignore-scaling>Disable/Ignore Scaling</h3><p>A probe can be configured to ignore scaling of configured dependent kubernetes resources.
To do that one must set <code>dependency-watchdog.gardener.cloud/ignore-scaling</code> annotation to <code>true</code> on the scalable resource for which scaling should be ignored.</p><h2 id=weeder>Weeder</h2><p>Dependency watchdog weeder command also (just like the prober command) takes command-line-flags which are meant to fine-tune the weeder. In addition a <code>ConfigMap</code> is also mounted to the container which helps in defining the dependency of pods on endpoints.</p><h3 id=command-line-arguments-1>Command Line Arguments</h3><p>Weeder can be configured with the same flags as that for prober described under <a href=/docs/other-components/dependency-watchdog/deployment/configure/#command-line-arguments>command-line-arguments</a> section
You can find an example weeder <a href=https://github.com/gardener/dependency-watchdog/blob/master/example/04-dwd-weeder-deployment.yaml>deployment</a> YAML to see how these command line args are configured.</p><h3 id=weeder-configuration>Weeder Configuration</h3><p>Weeder configuration is mounted as <code>ConfigMap</code> to the container. The path to the config file is configured via <code>config-file</code> command line argument as mentioned above. Weeder will start one go routine per podSelector per endpoint on an endpoint event as described in <a href=/docs/other-components/dependency-watchdog/concepts/weeder/#internals>weeder internal concepts</a>.</p><p>You can view the example YAML configuration provided as <code>data</code> in a <code>ConfigMap</code> <a href=https://github.com/gardener/dependency-watchdog/blob/master/example/02-dwd-weeder-configmap.yaml>here</a>.</p><table><thead><tr><th>Name</th><th>Type</th><th>Required</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>watchDuration</td><td>*metav1.Duration</td><td>No</td><td>5m0s</td><td>The time duration for which watch is kept on dependent pods to see if anyone turns to <code>CrashLoopBackoff</code></td></tr><tr><td>servicesAndDependantSelectors</td><td>map[string]DependantSelectors</td><td>Yes</td><td>NA</td><td>Endpoint name and its corresponding dependent pods. More info below.</td></tr></tbody></table><h3 id=dependantselectors>DependantSelectors</h3><p>If the service recovers from downtime, then weeder starts to watch for CrashLoopBackOff pods. These pods are identified by info stored in this property.</p><table><thead><tr><th>Name</th><th>Type</th><th>Required</th><th>Default Value</th><th>Description</th></tr></thead><tbody><tr><td>podSelectors</td><td>[]*metav1.LabelSelector</td><td>Yes</td><td>NA</td><td>This is a list of <a href=https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1@v0.24.3#LabelSelector>Label selector</a></td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-f28b0a628f77e282807d7fe76d71d2a7>2.2 - Monitor</h1><h1 id=monitoring>Monitoring</h1><h2 id=work-in-progress><em>Work In Progress</em></h2><p>We will be introducing metrics for <code>Dependency-Watchdog-Prober</code> and <code>Dependency-Watchdog-Weeder</code>. These metrics will be pushed to prometheus. Once that is completed we will provide details on all the metrics that will be supported here.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9ca8f65686a5d016a3852ad7e840d061>3 - Contribution</h1><h1 id=how-to-contribute>How to contribute?</h1><p>Contributions are always welcome!</p><p>In order to contribute ensure that you have the development environment setup and you familiarize yourself with required steps to build, verify-quality and test.</p><h2 id=setting-up-development-environment>Setting up development environment</h2><h3 id=installing-go>Installing Go</h3><p>Minimum Golang version required: <code>1.18</code>.
On MacOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install go
</span></span></code></pre></div><p>For other OS, follow the <a href=https://go.dev/doc/install>installation instructions</a>.</p><h3 id=installing-git>Installing Git</h3><p>Git is used as version control for dependency-watchdog. On MacOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install git
</span></span></code></pre></div><p>If you do not have git installed already then please follow the <a href=https://git-scm.com/downloads>installation instructions</a>.</p><h3 id=installing-docker>Installing Docker</h3><p>In order to test dependency-watchdog containers you will need a local kubernetes setup. Easiest way is to first install Docker. This becomes a pre-requisite to setting up either a vanilla KIND/minikube cluster or a local Gardener cluster.</p><p>On MacOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install -cash docker
</span></span></code></pre></div><p>For other OS, follow the <a href=https://docs.docker.com/get-docker/>installation instructions</a>.</p><h3 id=installing-kubectl>Installing Kubectl</h3><p>To interact with the local Kubernetes cluster you will need kubectl. On MacOS run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew install kubernetes-cli
</span></span></code></pre></div><p>For other IS, follow the <a href=https://kubernetes.io/docs/tasks/tools/install-kubectl/>installation instructions</a>.</p><h2 id=get-the-sources>Get the sources</h2><p>Clone the repository from Github:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/gardener/dependency-watchdog.git
</span></span></code></pre></div><h2 id=using-makefile>Using Makefile</h2><p>For every change following make targets are recommended to run.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># build the code changes</span>
</span></span><span style=display:flex><span>&gt; make build
</span></span><span style=display:flex><span><span style=color:green># ensure that all required checks pass</span>
</span></span><span style=display:flex><span>&gt; make verify <span style=color:green># this will check formatting, linting and will run unit tests</span>
</span></span><span style=display:flex><span><span style=color:green># if you do not wish to run tests then you can use the following make target.</span>
</span></span><span style=display:flex><span>&gt; make check
</span></span></code></pre></div><p>All tests should be run and the test coverage should ideally not reduce.
Please ensure that you have read <a href=/docs/other-components/dependency-watchdog/testing/>testing guidelines</a>.</p><p>Before raising a pull request ensure that if you are introducing any new file then you must add licesence header to all new files. To add license header you can run this make target:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&gt; make add-license-headers
</span></span><span style=display:flex><span><span style=color:green># This will add license headers to any file which does not already have it.</span>
</span></span></code></pre></div><blockquote><p>NOTE: Also have a look at the Makefile as it has other targets that are not mentioned here.</p></blockquote><h2 id=raising-a-pull-request>Raising a Pull Request</h2><p>To raise a pull request do the following:</p><ol><li>Create a fork of <a href=https://github.com/gardener/dependency-watchdog>dependency-watchdog</a></li><li>Add <a href=https://github.com/gardener/dependency-watchdog>dependency-watchdog</a> as upstream remote via</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>   git remote add upstream https://github.com/gardener/dependency-watchdog
</span></span></code></pre></div><ol start=3><li>It is recommended that you create a git branch and push all your changes for the pull-request.</li><li>Ensure that while you work on your pull-request, you continue to rebase the changes from upstream to your branch. To do that execute the following command:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>   git pull --rebase upstream master
</span></span></code></pre></div><ol start=5><li>We prefer clean commits. If you have multiple commits in the pull-request, then squash the commits to a single commit. You can do this via <code>interactive git rebase</code> command. For example if your PR branch is ahead of remote origin HEAD by 5 commits then you can execute the following command and pick the first commit and squash the remaining commits.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>   git rebase -i HEAD~5 <span style=color:green>#actual number from the head will depend upon how many commits your branch is ahead of remote origin master</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-aaeabd99918759e49fd4224faf002bcd>4 - Dwd Using Local Garden</h1><h1 id=dependency-watchdog-with-local-garden-cluster>Dependency Watchdog with Local Garden Cluster</h1><h2 id=setting-up-local-garden-cluster>Setting up Local Garden cluster</h2><p>A convenient way to test local dependency-watchdog changes is to use a local garden cluster.
To setup a local garden cluster you can follow the <a href=/docs/gardener/deployment/getting_started_locally/>setup-guide</a>.</p><h2 id=dependency-watchdog-resources>Dependency Watchdog resources</h2><p>As part of the local garden installation, a <code>local</code> seed will be available.</p><h3 id=dependency-watchdog-resources-created-in-the-seed>Dependency Watchdog resources created in the seed</h3><h4 id=namespaced-resources>Namespaced resources</h4><p>In the <code>garden</code> namespace of the seed cluster, following resources will be created:</p><table><thead><tr><th>Resource (GVK)</th><th>Name</th></tr></thead><tbody><tr><td>{apiVersion: v1, Kind: ServiceAccount}</td><td>dependency-watchdog-prober</td></tr><tr><td>{apiVersion: v1, Kind: ServiceAccount}</td><td>dependency-watchdog-weeder</td></tr><tr><td>{apiVersion: apps/v1, Kind: Deployment}</td><td>dependency-watchdog-prober</td></tr><tr><td>{apiVersion: apps/v1, Kind: Deployment}</td><td>dependency-watchdog-weeder</td></tr><tr><td>{apiVersion: v1, Kind: ConfigMap}</td><td>dependency-watchdog-prober-*</td></tr><tr><td>{apiVersion: v1, Kind: ConfigMap}</td><td>dependency-watchdog-weeder-*</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: Role}</td><td>gardener.cloud:dependency-watchdog-prober:role</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: Role}</td><td>gardener.cloud:dependency-watchdog-weeder:role</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: RoleBinding}</td><td>gardener.cloud:dependency-watchdog-prober:role-binding</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: RoleBinding}</td><td>gardener.cloud:dependency-watchdog-weeder:role-binding</td></tr><tr><td>{apiVersion: resources.gardener.cloud/v1alpha1, Kind: ManagedResource}</td><td>dependency-watchdog-prober</td></tr><tr><td>{apiVersion: resources.gardener.cloud/v1alpha1, Kind: ManagedResource}</td><td>dependency-watchdog-weeder</td></tr><tr><td>{apiVersion: v1, Kind: Secret}</td><td>managedresource-dependency-watchdog-weeder</td></tr><tr><td>{apiVersion: v1, Kind: Secret}</td><td>managedresource-dependency-watchdog-prober</td></tr></tbody></table><h4 id=cluster-resources>Cluster resources</h4><table><thead><tr><th>Resource (GVK)</th><th>Name</th></tr></thead><tbody><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: ClusterRole}</td><td>gardener.cloud:dependency-watchdog-prober:cluster-role</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: ClusterRole}</td><td>gardener.cloud:dependency-watchdog-weeder:cluster-role</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: ClusterRoleBinding}</td><td>gardener.cloud:dependency-watchdog-prober:cluster-role-binding</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: ClusterRoleBinding}</td><td>gardener.cloud:dependency-watchdog-weeder:cluster-role-binding</td></tr></tbody></table><h3 id=dependency-watchdog-resources-created-in-shoot-control-namespace>Dependency Watchdog resources created in Shoot control namespace</h3><table><thead><tr><th>Resource (GVK)</th><th>Name</th></tr></thead><tbody><tr><td>{apiVersion: v1, Kind: Secret}</td><td>dependency-watchdog-prober</td></tr><tr><td>{apiVersion: resources.gardener.cloud/v1alpha1, Kind: ManagedResource}</td><td>shoot-core-dependency-watchdog</td></tr></tbody></table><h3 id=dependency-watchdog-resources-created-in-the-kube-node-lease-namespace-of-the-shoot>Dependency Watchdog resources created in the kube-node-lease namespace of the shoot</h3><table><thead><tr><th>Resource (GVK)</th><th>Name</th></tr></thead><tbody><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: Role}</td><td>gardener.cloud:target:dependency-watchdog</td></tr><tr><td>{apiVersion: rbac.authorization.k8s.io/v1, Kind: RoleBinding}</td><td>gardener.cloud:target:dependency-watchdog</td></tr></tbody></table><p>These will be created by the GRM and will have a managed resource named <code>shoot-core-dependency-watchdog</code> in the shoot namespace in the seed.</p><h2 id=update-gardener-with-custom-dependency-watchdog-docker-images>Update Gardener with custom Dependency Watchdog Docker images</h2><h3 id=build-tag-and-push-docker-images>Build, Tag and Push docker images</h3><p>To build dependency watchdog docker images run the following make target:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&gt; make docker-build
</span></span></code></pre></div><p>Local gardener hosts a docker registry which can be access at <code>localhost:5001</code>. To enable local gardener to be able to access the custom docker images you need to tag and push these images to the embedded docker registry. To do that execute the following commands:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&gt; docker images
</span></span><span style=display:flex><span><span style=color:green># Get the IMAGE ID of the dependency watchdog images that were built using docker-build make target.</span>
</span></span><span style=display:flex><span>&gt; docker tag &lt;IMAGE-ID&gt; localhost:5001/europe-docker.pkg.dev/gardener-project/public/gardener/dependency-watchdog-prober:&lt;TAGNAME&gt;
</span></span><span style=display:flex><span>&gt; docker push localhost:5001/europe-docker.pkg.dev/gardener-project/public/gardener/dependency-watchdog-prober:&lt;TAGNAME&gt;
</span></span></code></pre></div><h3 id=update-managedresource>Update ManagedResource</h3><p>Garden resource manager will revert back any changes that are done to the kubernetes deployment for dependency watchdog. This is quite useful in live landscapes where only tested and qualified images are used for all gardener managed components. Any change therefore is automatically reverted.</p><p>However, during development and testing you will need to use custom docker images. To prevent garden resource manager from reverting the changes done to the kubernetes deployment for dependency watchdog components you must update the respective managed resources first.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># List the managed resources</span>
</span></span><span style=display:flex><span>&gt; kubectl get mr -n garden | grep dependency
</span></span><span style=display:flex><span><span style=color:green># Sample response</span>
</span></span><span style=display:flex><span>dependency-watchdog-weeder            seed    True      True      False         26h
</span></span><span style=display:flex><span>dependency-watchdog-prober            seed    True      True      False         26h
</span></span><span style=display:flex><span><span style=color:green># Lets assume that you are currently testing prober and would like to use a custom docker image</span>
</span></span><span style=display:flex><span>&gt; kubectl edit mr dependency-watchdog-prober -n garden
</span></span><span style=display:flex><span><span style=color:green># This will open the resource YAML for editing. Add the annotation resources.gardener.cloud/ignore=true</span>
</span></span><span style=display:flex><span><span style=color:green># Reference: https://github.com/gardener/gardener/blob/master/docs/concepts/resource-manager.md</span>
</span></span><span style=display:flex><span><span style=color:green># Save the YAML file.</span>
</span></span></code></pre></div><p>When you are done with your testing then you can again edit the ManagedResource and remove the annotation. Garden resource manager will revert back to the image with which gardener was initially built and started.</p><h3 id=update-kubernetes-deployment>Update Kubernetes Deployment</h3><p>Find and update the kubernetes deployment for dependency watchdog.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&gt; kubectl get deploy -n garden | grep dependency
</span></span><span style=display:flex><span><span style=color:green># Sample response</span>
</span></span><span style=display:flex><span>dependency-watchdog-weeder            1/1     1            1           26h
</span></span><span style=display:flex><span>dependency-watchdog-prober            1/1     1            1           26h
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Lets assume that you are currently testing prober and would like to use a custom docker image</span>
</span></span><span style=display:flex><span>&gt; kubectl edit deploy dependency-watchdog-prober -n garden
</span></span><span style=display:flex><span><span style=color:green># This will open the resource YAML for editing. Change the image or any other changes and save.</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-65baffe8a57fa26fd40648819522e688>5 - Testing</h1><h1 id=testing-strategy-and-developer-guideline>Testing Strategy and Developer Guideline</h1><p>Intent of this document is to introduce you (the developer) to the following:</p><ul><li>Category of tests that exists.</li><li>Libraries that are used to write tests.</li><li>Best practices to write tests that are correct, stable, fast and maintainable.</li><li>How to run each category of tests.</li></ul><p>For any new contributions <strong>tests are a strict requirement</strong>. <code>Boy Scouts Rule</code> is followed: If you touch a code for which either no tests exist or coverage is insufficient then it is expected that you will add relevant tests.</p><h2 id=tools-used-for-writing-tests>Tools Used for Writing Tests</h2><p>These are the following tools that were used to write all the tests (unit + envtest + vanilla kind cluster tests), it is preferred not to introduce any additional tools / test frameworks for writing tests:</p><h3 id=gomega>Gomega</h3><p>We use gomega as our matcher or assertion library. Refer to Gomega&rsquo;s <a href=https://onsi.github.io/gomega/>official documentation</a> for details regarding its installation and application in tests.</p><h3 id=testing-package-from-standard-library><code>Testing</code> Package from Standard Library</h3><p>We use the <code>Testing</code> package provided by the standard library in golang for writing all our tests. Refer to its <a href=https://pkg.go.dev/testing>official documentation</a> to learn how to write tests using <code>Testing</code> package. You can also refer to <a href=https://go.dev/doc/tutorial/add-a-test>this</a> example.</p><h2 id=writing-tests>Writing Tests</h2><h3 id=common-for-all-kinds>Common for All Kinds</h3><ul><li>For naming the individual tests (<code>TestXxx</code> and <code>testXxx</code> methods) and helper methods, make sure that the name describes the implementation of the method. For eg: <code>testScalingWhenMandatoryResourceNotFound</code> tests the behaviour of the <code>scaler</code> when a mandatory resource (KCM deployment) is not present.</li><li>Maintain proper logging in tests. Use <code>t.log()</code> method to add appropriate messages wherever necessary to describe the flow of the test. See <a href=https://github.com/gardener/dependency-watchdog/blob/master/controllers/endpoint/endpoints_controller_test.go>this</a> for examples.</li><li>Make use of the <code>testdata</code> directory for storing arbitrary sample data needed by tests (YAML manifests, etc.). See <a href=https://github.com/gardener/dependency-watchdog/tree/master/controllers>this</a> package for examples.<ul><li>From <a href=https://pkg.go.dev/cmd/go/internal/test>https://pkg.go.dev/cmd/go/internal/test</a>:<blockquote><p>The go tool will ignore a directory named &ldquo;testdata&rdquo;, making it available to hold ancillary data needed by the tests.</p></blockquote></li></ul></li></ul><h3 id=table-driven-tests>Table-driven tests</h3><p>We need a tabular structure in two cases:</p><ul><li><strong>When we have multiple tests which require the same kind of setup</strong>:- In this case we have a <code>TestXxxSuite</code> method which will do the setup and run all the tests. We have a slice of <code>test</code> struct which holds all the tests (typically a <code>title</code> and <code>run</code> method). We use a <code>for</code> loop to run all the tests one by one. See <a href=https://github.com/gardener/dependency-watchdog/blob/master/controllers/cluster/cluster_controller_test.go>this</a> for examples.</li><li><strong>When we have the same code path and multiple possible values to check</strong>:- In this case we have the arguments and expectations in a struct. We iterate through the slice of all such structs, passing the arguments to appropriate methods and checking if the expectation is met. See <a href=https://github.com/gardener/dependency-watchdog/blob/master/internal/prober/scaler/scaler_test.go>this</a> for examples.</li></ul><h3 id=env-tests>Env Tests</h3><p>Env tests in Dependency Watchdog use the <code>sigs.k8s.io/controller-runtime/pkg/envtest</code> package. It sets up a temporary control plane (etcd + kube-apiserver) and runs the test against it. The code to set up and teardown the environment can be checked out <a href=https://github.com/gardener/dependency-watchdog/blob/master/internal/test/testenv.go>here</a>.</p><p>These are the points to be followed while writing tests that use <code>envtest</code> setup:</p><ul><li><p>All tests should be divided into two top level partitions:</p><ol><li>tests with common environment (<code>testXxxCommonEnvTests</code>)</li><li>tests which need a dedicated environment for each one. (<code>testXxxDedicatedEnvTests</code>)</li></ol><p>They should be contained within the <code>TestXxxSuite</code> method. See <a href=https://github.com/gardener/dependency-watchdog/blob/master/controllers/cluster/cluster_controller_test.go>this</a> for examples. If all tests are of one kind then this is not needed.</p></li><li><p>Create a method named <code>setUpXxxTest</code> for performing setup tasks before all/each test. It should either return a method or have a separate method to perform teardown tasks. See <a href=https://github.com/gardener/dependency-watchdog/blob/master/controllers/cluster/cluster_controller_test.go>this</a> for examples.</p></li><li><p>The tests run by the suite can be table-driven as well.</p></li><li><p>Use the <code>envtest</code> setup when there is a need of an environment close to an actual setup. Eg: start controllers against a real Kubernetes control plane to catch bugs that can only happen when talking to a real API server.</p></li></ul><blockquote><p>NOTE: It is currently not possible to bring up more than one envtest environments. See <a href=https://github.com/kubernetes-sigs/controller-runtime/issues/1363>issue#1363</a>. We enforce running serial execution of test suites each of which uses a different envtest environments. See <a href=https://github.com/gardener/dependency-watchdog/blob/master/hack/test.sh>hack/test.sh</a>.</p></blockquote><h3 id=vanilla-kind-cluster-tests>Vanilla Kind Cluster Tests</h3><p>There are some tests where we need a vanilla kind cluster setup, for eg:- The <code>scaler.go</code> code in the <code>prober</code> package uses the <code>scale</code> subresource to scale the deployments mentioned in the prober config. But the <code>envtest</code> setup does not support the <code>scale</code> subresource as of now. So we need this setup to test if the deployments are scaled as per the config or not.
You can check out the code for this setup <a href=https://github.com/gardener/dependency-watchdog/blob/master/internal/test/kind.go>here</a>. You can add utility methods for different kubernetes and custom resources in there.</p><p>These are the points to be followed while writing tests that use <code>Vanilla Kind Cluster</code> setup:</p><ul><li>Use this setup only if there is a need of an actual Kubernetes cluster(api server + control plane + etcd) to write the tests. (Because this is slower than your normal <code>envTest</code> setup)</li><li>Create <code>setUpXxxTest</code> similar to the one in <code>envTest</code>. Follow the same structural pattern used in <code>envTest</code> for writing these tests. See <a href=https://github.com/gardener/dependency-watchdog/blob/master/internal/prober/scaler/scaler_test.go>this</a> for examples.</li></ul><h2 id=run-tests>Run Tests</h2><p>To run unit tests, use the following Makefile target</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make test
</span></span></code></pre></div><p>To run KIND cluster based tests, use the following Makefile target</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>make kind-tests <span style=color:green># these tests will be slower as it brings up a vanilla KIND cluster</span>
</span></span></code></pre></div><p>To view coverage after running the tests, run :</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>go tool cover -html=cover.out
</span></span></code></pre></div><h2 id=flaky-tests>Flaky tests</h2><p>If you see that a test is flaky then you can use <code>make stress</code> target which internally uses <a href=https://pkg.go.dev/golang.org/x/tools/cmd/stress>stress tool</a></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make stress test-package=&lt;test-package&gt; test-func=&lt;test-func&gt; tool-params=<span style=color:#a31515>&#34;&lt;tool-params&gt;&#34;</span>
</span></span></code></pre></div><p>An example invocation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make stress test-package=./internal/util test-func=TestRetryUntilPredicateWithBackgroundContext tool-params=<span style=color:#a31515>&#34;-p 10&#34;</span>
</span></span></code></pre></div><p>The make target will do the following:</p><ol><li>It will create a test binary for the package specified via <code>test-package</code> at <code>/tmp/pkg-stress.test</code> directory.</li><li>It will run <code>stress</code> tool passing the <code>tool-params</code> and targets the function <code>test-func</code>.</li></ol></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2025 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=/js/main.min.69e2c1ae9320465ab10236d9ef752c6a4442c54b48b883b17c497b7c7d96a796.js integrity="sha256-aeLBrpMgRlqxAjbZ73UsakRCxUtIuIOxfEl7fH2Wp5Y=" crossorigin=anonymous></script><script defer src=/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/js/tabpane-persist.js></script></body></html>