<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/debug-a-pod/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/debug-a-pod/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>How to Debug a Pod | Gardener</title><meta name=description content="Your pod doesn't run as expected. Are there any log files? Where? How could I debug a pod?"><meta property="og:title" content="How to Debug a Pod"><meta property="og:description" content="Your pod doesn't run as expected. Are there any log files? Where? How could I debug a pod?"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/guides/monitoring-and-troubleshooting/debug-a-pod/"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="How to Debug a Pod"><meta itemprop=description content="Your pod doesn't run as expected. Are there any log files? Where? How could I debug a pod?"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="How to Debug a Pod"><meta name=twitter:description content="Your pod doesn't run as expected. Are there any log files? Where? How could I debug a pod?"><link rel=preload href=/scss/main.min.52b703b92d167c14e82f904cd88f9dbe92798d607a8949235304e48c7cd0a116.css as=style><link href=/scss/main.min.52b703b92d167c14e82f904cd88f9dbe92798d607a8949235304e48c7cd0a116.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.86c9c9e208d0f943f6df073b6b9682be.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/guides/monitoring-and-troubleshooting/debug-a-pod/>Return to the regular view of this page</a>.</p></div><h1 class=title>How to Debug a Pod</h1><div class=lead>Your pod doesn&rsquo;t run as expected. Are there any log files? Where? How could I debug a pod?</div><div class=content><h2 id=introduction>Introduction</h2><p>Kubernetes offers powerful options to get more details about startup or runtime failures of pods as e.g. described in
<a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/>Application Introspection and Debugging</a>
or <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/>Debug Pods and Replication Controllers</a>.</p><p>In order to identify pods with potential issues, you could e.g. run <code>kubectl get pods --all-namespaces | grep -iv Running</code> to filter
out the pods which are not in the state <code>Running</code>. One of frequent error state is <code>CrashLoopBackOff</code>, which tells that
a pod crashes right after the start. Kubernetes then tries to restart the pod again, but often the pod startup fails again.</p><p>Here is a short list of possible reasons which might lead to a pod crash:</p><ol><li>Error during image pull caused by e.g. wrong/missing secrets or wrong/missing image</li><li>The app runs in an error state caused e.g. by missing environmental variables (ConfigMaps) or secrets</li><li>Liveness probe failed</li><li>Too high resource consumption (memory and/or CPU) or too strict quota settings</li><li>Persistent volumes can&rsquo;t be created/mounted</li><li>The container image is not updated</li></ol><p>Basically, the commands <code>kubectl logs ...</code> and <code>kubectl describe ...</code> with different parameters are used to get more
detailed information. By calling e.g. <code>kubectl logs --help</code> you can get more detailed information about the command and its
parameters.</p><p>In the next sections you&rsquo;ll find some basic approaches to get some ideas what went wrong.</p><p>Remarks:</p><ul><li>Even if the pods seem to be running, as the status <code>Running</code> indicates, a high counter of the <code>Restarts</code> shows potential problems</li><li>You can get a good overview of the troubleshooting process with the interactive tutorial <a href=https://kubernetes.io/docs/tutorials/kubernetes-basics/explore-intro/>Troubleshooting with Kubectl</a> available which explains basic debugging activities</li><li>The examples below are deployed into the namespace <code>default</code>. In case you want to change it, use the optional
parameter <code>--namespace &lt;your-namespace></code> to select the target namespace. The examples require a Kubernetes release ≥ <em>1.8</em>.</li></ul><h2 id=prerequisites>Prerequisites</h2><p>Your deployment was successful (no logical/syntactical errors in the manifest files), but the pod(s) aren&rsquo;t running.</p><h2 id=error-caused-by-wrong-image-name>Error Caused by Wrong Image Name</h2><p>Start by running <code>kubectl describe pod &lt;your-pod> &lt;your-namespace></code> to get detailed information about the pod startup.</p><p>In the <code>Events</code> section, you should get an error message like <code>Failed to pull image ...</code> and <code>Reason: Failed</code>. The pod is
in state <code>ImagePullBackOff</code>.</p><p>The example below is based on a <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/>demo in the Kubernetes documentation</a>. In all examples, the <code>default</code> namespace is used.</p><p>First, perform a cleanup with:</p><p><code>kubectl delete pod termination-demo</code></p><p>Next, create a resource based on the yaml content below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod 
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: termination-demo-container
</span></span><span style=display:flex><span>    image: debiann
</span></span><span style=display:flex><span>    command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>    args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log&#34;</span>]
</span></span></code></pre></div><p><code>kubectl describe pod termination-demo</code> lists in the <code>Event</code> section the content</p><pre tabindex=0><code>Events:
  FirstSeen	LastSeen	Count	From							SubObjectPath					Type		Reason			Message
  ---------	--------	-----	----							-------------					--------	------			-------
  2m		2m		1	default-scheduler											Normal		Scheduled		Successfully assigned termination-demo to ip-10-250-17-112.eu-west-1.compute.internal
  2m		2m		1	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded for volume &#34;default-token-sgccm&#34; 
  2m		1m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Pulling			pulling image &#34;debiann&#34;
  2m		1m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Warning		Failed			Failed to pull image &#34;debiann&#34;: rpc error: code = Unknown desc = Error: image library/debiann:latest not found
  2m		54s		10	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Warning		FailedSync		Error syncing pod
  2m		54s		6	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		BackOff			Back-off pulling image &#34;debiann&#34;
</code></pre><p>The error message with <code>Reason: Failed</code> tells you that there is an error during pulling the image. A closer look at the
image name indicates a misspelling.</p><h2 id=the-app-runs-in-an-error-state-caused-eg-by-missing-environmental-variables-configmaps-or-secrets>The App Runs in an Error State Caused e.g. by Missing Environmental Variables (ConfigMaps) or Secrets</h2><p>This example illustrates the behavior in the case when the app expects environment variables but the corresponding Kubernetes artifacts are missing.</p><p>First, perform a cleanup with:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kubectl delete deployment termination-demo
</span></span><span style=display:flex><span>kubectl delete configmaps app-env
</span></span></code></pre></div><p>Next, deploy the following manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1beta2 
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>     app: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: termination-demo
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: termination-demo
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: termination-demo-container
</span></span><span style=display:flex><span>        image: debian
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>        args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;sed \&#34;s/foo/bar/\&#34; &lt; $MYFILE&#34;</span>]
</span></span></code></pre></div><p>Now, the command <code>kubectl get pods</code> lists the pod <code>termination-demo-xxx</code> in the state <code>Error</code> or <code>CrashLoopBackOff</code>.
The command <code>kubectl describe pod termination-demo-xxx</code> tells you that there is no error during startup but gives no clue about what caused the crash.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  FirstSeen	LastSeen	Count	From							SubObjectPath					Type		Reason		Message
</span></span><span style=display:flex><span>  ---------	--------	-----	----							-------------					--------	------		-------
</span></span><span style=display:flex><span>  19m		19m		1	default-scheduler											Normal		Scheduled	Successfully assigned termination-demo-5fb484867d-xz2x9 to ip-10-250-17-112.eu-west-1.compute.internal
</span></span><span style=display:flex><span>  19m		19m		1	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Normal		SuccessfulMountVolume	MountVolume.SetUp succeeded <span style=color:#00f>for</span> volume <span style=color:#a31515>&#34;default-token-sgccm&#34;</span> 
</span></span><span style=display:flex><span>  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Pulling		pulling image <span style=color:#a31515>&#34;debian&#34;</span>
</span></span><span style=display:flex><span>  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Pulled		Successfully pulled image <span style=color:#a31515>&#34;debian&#34;</span>
</span></span><span style=display:flex><span>  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Created		Created container
</span></span><span style=display:flex><span>  19m		19m		4	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Normal		Started		Started container
</span></span><span style=display:flex><span>  19m		14m		24	kubelet, ip-10-250-17-112.eu-west-1.compute.internal	spec.containers{termination-demo-container}	Warning		BackOff		Back-off restarting failed container
</span></span><span style=display:flex><span>  19m		4m		69	kubelet, ip-10-250-17-112.eu-west-1.compute.internal							Warning		FailedSync	Error syncing pod
</span></span></code></pre></div><p>The command <code>kubectl get logs termination-demo-xxx</code> gives access to the output, the application writes on <code>stderr</code> and
<code>stdout</code>. In this case, you should get an output similar to:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>/bin/sh: 1: cannot open : No such file
</span></span></code></pre></div><p>So you need to have a closer look at the application. In this case, the environmental variable <code>MYFILE</code> is missing. To fix this
issue, you could e.g. add a ConfigMap to your deployment as is shown in the manifest listed below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: app-env
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  MYFILE: <span style=color:#a31515>&#34;/etc/profile&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: apps/v1beta2 
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>     app: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: termination-demo
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: termination-demo
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: termination-demo-container
</span></span><span style=display:flex><span>        image: debian
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>        args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;sed \&#34;s/foo/bar/\&#34; &lt; $MYFILE&#34;</span>]
</span></span><span style=display:flex><span>        envFrom:
</span></span><span style=display:flex><span>        - configMapRef:
</span></span><span style=display:flex><span>            name: app-env 
</span></span></code></pre></div><p>Note that once you fix the error and re-run the scenario, you might still see the pod in a <code>CrashLoopBackOff</code> status.
It is because the container finishes the command <code>sed ...</code> and runs to completion. In order to keep the container in a <code>Running</code> status,
a long running task is required, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: app-env
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  MYFILE: <span style=color:#a31515>&#34;/etc/profile&#34;</span>
</span></span><span style=display:flex><span>  SLEEP: <span style=color:#a31515>&#34;5&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: apps/v1beta2
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>     app: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: termination-demo
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: termination-demo
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: termination-demo-container
</span></span><span style=display:flex><span>        image: debian
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:green># args: [&#34;-c&#34;, &#34;sed \&#34;s/foo/bar/\&#34; &lt; $MYFILE&#34;]</span>
</span></span><span style=display:flex><span>        args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;while true; do sleep $SLEEP; echo sleeping; done;&#34;</span>]
</span></span><span style=display:flex><span>        envFrom:
</span></span><span style=display:flex><span>        - configMapRef:
</span></span><span style=display:flex><span>            name: app-env
</span></span></code></pre></div><h2 id=too-high-resource-consumption-memory-andor-cpu-or-too-strict-quota-settings>Too High Resource Consumption (Memory and/or CPU) or Too Strict Quota Settings</h2><p>You can optionally specify the amount of memory and/or CPU your container gets during runtime. In case these settings are missing,
the default requests settings are taken: CPU: 0m (in Milli CPU) and RAM: 0Gi, which indicate no other limits other than the
ones of the node(s) itself. For more details, e.g. about how to configure limits, see <a href=https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/>Configure Default Memory Requests and Limits for a Namespace</a>.</p><p>In case your application needs more resources, Kubernetes distinguishes between <code>requests</code> and <code>limit</code> settings: <code>requests</code>
specify the guaranteed amount of resource, whereas <code>limit</code> tells Kubernetes the maximum amount of resource the container might
need. Mathematically, both settings could be described by the relation <code>0 &lt;= requests &lt;= limit</code>. For both settings you need to
consider the total amount of resources your nodes provide. For a detailed description of the concept, see <a href=https://github.com/kubernetes/design-proposals-archive/blob/main/node/resource-qos.md>Resource Quality of Service in Kubernetes</a>.</p><p>Use <code>kubectl describe nodes</code> to get a first overview of the resource consumption in your cluster. Of special interest are the
figures indicating the amount of CPU and Memory Requests at the bottom of the output.</p><p>The next example demonstrates what happens in case the CPU request is too high in order to be managed by your cluster.</p><p>First, perform a cleanup with:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kubectl delete deployment termination-demo
</span></span><span style=display:flex><span>kubectl delete configmaps app-env
</span></span></code></pre></div><p>Next, adapt the <code>cpu</code> below in the yaml below to be slightly higher than the remaining CPU resources in your cluster and deploy
this manifest. In this example, <code>600m</code> (milli CPUs) are requested in a Kubernetes system with a single 2 core worker
node which results in an error message.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: apps/v1beta2 
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: termination-demo
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>     app: termination-demo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: termination-demo
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: termination-demo
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: termination-demo-container
</span></span><span style=display:flex><span>        image: debian
</span></span><span style=display:flex><span>        command: [<span style=color:#a31515>&#34;/bin/sh&#34;</span>]
</span></span><span style=display:flex><span>        args: [<span style=color:#a31515>&#34;-c&#34;</span>, <span style=color:#a31515>&#34;sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log&#34;</span>]
</span></span><span style=display:flex><span>        resources:
</span></span><span style=display:flex><span>          requests:
</span></span><span style=display:flex><span>            cpu: <span style=color:#a31515>&#34;600m&#34;</span> 
</span></span></code></pre></div><p>The command <code>kubectl get pods</code> lists the pod <code>termination-demo-xxx</code> in the state <code>Pending</code>. More details on why this happens
could be found by using the command <code>kubectl describe pod termination-demo-xxx</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl describe po termination-demo-fdb7bb7d9-mzvfw
</span></span><span style=display:flex><span>Name:           termination-demo-fdb7bb7d9-mzvfw
</span></span><span style=display:flex><span>Namespace:      default
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Containers:
</span></span><span style=display:flex><span>  termination-demo-container:
</span></span><span style=display:flex><span>    Image:      debian
</span></span><span style=display:flex><span>    Port:       &lt;none&gt;
</span></span><span style=display:flex><span>    Host Port:  &lt;none&gt;
</span></span><span style=display:flex><span>    Command:
</span></span><span style=display:flex><span>      /bin/sh
</span></span><span style=display:flex><span>    Args:
</span></span><span style=display:flex><span>      -c
</span></span><span style=display:flex><span>      sleep 10 &amp;&amp; echo Sleep expired &gt; /dev/termination-log
</span></span><span style=display:flex><span>    Requests:
</span></span><span style=display:flex><span>      cpu:        6
</span></span><span style=display:flex><span>    Environment:  &lt;none&gt;
</span></span><span style=display:flex><span>    Mounts:
</span></span><span style=display:flex><span>      /var/run/secrets/kubernetes.io/serviceaccount from default-token-t549m (ro)
</span></span><span style=display:flex><span>Conditions:
</span></span><span style=display:flex><span>  Type           Status
</span></span><span style=display:flex><span>  PodScheduled   False
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  Type     Reason            Age               From               Message
</span></span><span style=display:flex><span>  ----     ------            ----              ----               -------
</span></span><span style=display:flex><span>  Warning  FailedScheduling  9s (x7 over 40s)  default-scheduler  0/2 nodes are available: 2 Insufficient cpu.
</span></span></code></pre></div><p>You can find more details in:</p><ul><li><a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>Managing Compute Resources for Containters</a></li><li><a href=https://github.com/kubernetes/design-proposals-archive/blob/main/node/resource-qos.md>Resource Quality of Service in Kubernetes</a></li></ul><p>Remarks:</p><ul><li>This example works similarly when specifying a too high request for memory</li><li>In case you configured an autoscaler range when creating your Kubernetes cluster, another worker node will be spinned up automatically if you didn&rsquo;t reach the maximum number of worker nodes</li><li>In case your app is running out of memory (the memory settings are too small), you will typically find an <code>OOMKilled</code> (Out Of Memory) message in the <code>Events</code> section of the <code>kubectl describe pod ...</code> output</li></ul><h2 id=the-container-image-is-not-updated>The Container Image Is Not Updated</h2><p>You applied a fix in your app, created a new container image and pushed it into your container repository. After redeploying your Kubernetes manifests, you expected to get the updated app, but the same bug is still in the new deployment present.</p><p>This behavior is related to how Kubernetes decides whether to pull a new docker image or to use the cached one.</p><p>In case you didn&rsquo;t change the image tag, the default image policy <em>IfNotPresent</em> tells Kubernetes to use the cached image (see <a href=https://kubernetes.io/docs/concepts/containers/images/>Images</a>).</p><p>As a best practice, you should not use the tag <code>latest</code> and change the image tag in case you changed anything in your image (see <a href=https://kubernetes.io/docs/concepts/configuration/overview/#container-images>Configuration Best Practices</a>).</p><p>For more information, see <a href=/docs/guides/applications/image-pull-policy/>Container Image Not Updating</a>.</p><h2 id=related-links>Related Links</h2><ul><li><a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/>Application Introspection and Debugging</a></li><li><a href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/>Debug Pods and Replication Controllers</a></li><li><a href=https://kubernetes.io/docs/concepts/cluster-administration/logging/>Logging Architecture</a></li><li><a href=https://kubernetes.io/docs/tasks/administer-cluster/memory-default-namespace/>Configure Default Memory Requests and Limits for a Namespace</a></li><li><a href=https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/>Managing Compute Resources for Containters</a></li><li><a href=https://github.com/kubernetes/design-proposals-archive/blob/main/node/resource-qos.md>Resource Quality of Service in Kubernetes</a></li><li><a href=https://kubernetes.io/docs/tutorials/kubernetes-basics/explore-intro/>Interactive Tutorial Troubleshooting with Kubectl</a></li><li><a href=https://kubernetes.io/docs/concepts/containers/images/>Images</a></li><li><a href=https://kubernetes.io/docs/concepts/configuration/overview/#container-images>Kubernetes Best Practises</a></li></ul></div></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2023 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.11dbee029dba1a98021fb7be4d7405a7392afb38ff5640a21ff4f4c4c5057b2f.js integrity="sha256-EdvuAp26GpgCH7e+TXQFpzkq+zj/VkCiH/T0xMUFey8=" crossorigin=anonymous></script></body></html>