<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.95.0"><link rel=canonical type=text/html href=https://gardener.cloud/docs/extensions/><link rel=alternate type=application/rss+xml href=https://gardener.cloud/docs/extensions/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>List of Extensions | Gardener</title><meta name=description content="The infrastructure, networking, OS and other extension components for Gardener"><meta property="og:title" content="List of Extensions"><meta property="og:description" content="The infrastructure, networking, OS and other extension components for Gardener"><meta property="og:type" content="website"><meta property="og:url" content="https://gardener.cloud/docs/extensions/"><meta property="og:image" content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta itemprop=name content="List of Extensions"><meta itemprop=description content="The infrastructure, networking, OS and other extension components for Gardener"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gardener.cloud/images/lp/gardener-logo.svg"><meta name=twitter:title content="List of Extensions"><meta name=twitter:description content="The infrastructure, networking, OS and other extension components for Gardener"><link rel=preload href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css as=style><link href=/scss/main.min.b5b806bb2cd9fe9ed809539377398aa9df0eb8ca0c983a6eae0b413d528d8f0e.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=https://unpkg.com/lunr@2.3.9/lunr.min.js integrity=sha384-203J0SNzyqHby3iU6hzvzltrWi/M41wOP5Gu+BiJMz5nwKykbkUx8Kp7iti0Lpli crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7N3XF5XLGV"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7N3XF5XLGV",{anonymize_ip:!1})}</script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class=navbar-logo><svg width="90" height="90" viewBox="0 0 90 90" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><title>logo</title><desc>Created with Sketch.</desc><defs><path d="M41.8864954.994901575c.996545099999999-.479910833 2.6164002-.477918931 3.6088091.0L76.8159138 16.0781121C77.8124589 16.5580229 78.8208647 17.8257185 79.0659694 18.8995926l7.7355517 33.8916663C87.0476474 53.8696088 86.6852538 55.4484075 85.9984855 56.3095876L64.3239514 83.4885938C63.6343208 84.3533632 62.1740175 85.0543973 61.0725268 85.0543973H26.3092731c-1.1060816.0-2.5646564-.704623400000003-3.2514246-1.5658035L1.38331434 56.3095876C.693683723 55.4448182.335174016 53.865133.580278769 52.7912589L8.31583044 18.8995926C8.56195675 17.8212428 9.57347722 16.556031 10.5658861 16.0781121L41.8864954.994901575z" id="path-1"/><linearGradient x1="12.7542673%" y1="-18.6617048%" x2="88.2666158%" y2="84.6075483%" id="linearGradient-3"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="50%" y1="4.93673768%" x2="148.756007%" y2="175.514523%" id="linearGradient-4"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="19.1574381%" y1="-9.04800713%" x2="82.2203149%" y2="77.9084293%" id="linearGradient-5"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient><linearGradient x1="57.4403751%" y1="26.3148481%" x2="137.966711%" y2="158.080556%" id="linearGradient-6"><stop stop-color="#fff" offset="0"/><stop stop-color="#439246" offset="100%"/></linearGradient></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="logo"><g id="Rectangle-2" transform="translate(1.000000, 0.000000)"><mask id="mask-2" fill="#fff"><use xlink:href="#path-1"/></mask><use id="Mask" fill="#009f76" xlink:href="#path-1"/><polygon fill="#000" opacity=".289628623" mask="url(#mask-2)" points="-17.6484375 54.5224609 30.8242188 25.0791016 63.4726562 58.5 24.7324219 92.6689453"/></g><path d="M56.8508631 39.260019C56.4193519 40.443987 55.6088085 41.581593 54.6736295 42.1938694l-8.0738997 5.2861089c-1.3854671.907087099999998-3.6247515.9116711-5.0172201.0L33.50861 42.1938694C32.123143 41.2867823 31 39.206345 31 37.545932V26.4150304c0-.725313.2131118-1.5301454.569268099999999-2.2825772L56.8508631 39.260019z" id="Combined-Shape" fill="url(#linearGradient-3)" transform="translate(43.925432, 36.147233) scale(-1, 1) translate(-43.925432, -36.147233)"/><path d="M56.0774672 25.1412464C56.4306829 25.8903325 56.6425556 26.6907345 56.6425556 27.4119019V38.5428034c0 1.6598979-1.1161415 3.73626640000001-2.50861 4.6479374l-8.0738997 5.286109c-1.3854671.907087000000004-3.6247516.911671000000005-5.0172201.0L32.9689261 43.1907408C32.2918101 42.7474223 31.6773514 42.0238435 31.2260376 41.206007L56.0774672 25.1412464z" id="Combined-Shape" fill="url(#linearGradient-4)" transform="translate(43.821278, 37.246598) scale(-1, 1) translate(-43.821278, -37.246598)"/><path d="M65.0702134 57.1846889C64.5985426 58.2007851 63.8367404 59.1236871 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.1597438 58.7930183 24 56.7816693 24 55.1323495V37.1145303C24 36.3487436 24.249712 35.5060005 24.6599102 34.7400631L65.0702134 57.1846889z" id="Combined-Shape" fill="url(#linearGradient-5)"/><path d="M65.0189476 34.954538C65.3636909 35.6617313 65.5692194 36.42021 65.5692194 37.1145303V55.1323495C65.5692194 56.7842831 64.4072119 58.7943252 62.9788591 59.6189851L47.37497 68.6278947c-1.4306165.825966800000003-3.75236779999999.8246599-5.1807206.0L26.5903603 59.6189851C25.9237304 59.2341061 25.3159155 58.5918431 24.8568495 57.8487596L65.0189476 34.954538z" id="Combined-Shape" fill="url(#linearGradient-6)"/></g></g></svg></span><span class=text-capitalize>Gardener</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/adopter><span>Adopters</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/docs><span>Documentation</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/blog><span>Blogs</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/community><span>Community</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"><input type=search class="form-control td-search-input" placeholder="&#xf002; Search this site…" aria-label="Search this site…" autocomplete=off data-offline-search-index-json-src=/offline-search-index.089e85e50b8e2e90c1366e4f64fbd5d6.json data-offline-search-base-href=/ data-offline-search-max-results=10></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/extensions/>Return to the regular view of this page</a>.</p></div><h1 class=title>List of Extensions</h1><div class=lead>The infrastructure, networking, OS and other extension components for Gardener</div><div class=content></div></div><div class=td-content><h1 id=pg-55e13a93740d6c3c02354485eaf37722>1 - Infrastructure Extensions</h1><div class=lead>Gardener extension controllers for the different infrastructures</div></div><div class=td-content><h1 id=pg-936f45ed7bca2e441d2b1f9f2ad32c57>1.1 - Provider Alicloud</h1><div class=lead>Gardener extension controller for the Alibaba cloud provider</div><h1 id=gardener-extension-for-alicloud-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Alicloud provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener-tests/pipelines/gardener-extension-provider-alicloud-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener-tests/pipelines/gardener-extension-provider-alicloud-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-alicloud><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-alicloud alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the Alicloud provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.28</td><td>1.28.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.27</td><td>1.27.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.27%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.27%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.27 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.26</td><td>1.26.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.26%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.26%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.26 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.25</td><td>1.25.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.24</td><td>1.24.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20Alibaba%20Cloud><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20Alibaba%20Cloud/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-alicloud/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4e551ed938a8801ec37062e140472bde>1.1.1 - Tutorials</h1></div><div class=td-content><h1 id=pg-f56d1ea5f743bb82423b209a4b67be4c>1.1.1.1 - Create a Kubernetes Cluster on Alibaba Cloud with Gardener</h1><h3 id=overview>Overview</h3><p>Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on Alibaba Cloud.</p><h3 id=prerequisites>Prerequisites</h3><ul><li>You have created an <a href=https://www.alibabacloud.com>Alibaba Cloud account</a>.</li><li>You have access to the Gardener dashboard and have permissions to create projects.</li></ul><h3 id=steps>Steps</h3><ol><li><p>Go to the Gardener dashboard and create a project.</p><img src=/__resources/new-gardener-project_a2095b.png><blockquote><p>To be able to add shoot clusters to this project, you must first create a technical user on Alibaba Cloud with sufficient permissions.</p></blockquote></li><li><p>Choose <em>Secrets</em>, then the plus icon <img src=/__resources/plus-icon_99aabe.png> and select <em>AliCloud</em>.</p><img src=/__resources/alicloud-create-secret_fe594e.png></li><li><p>To copy the policy for Alibaba Cloud from the Gardener dashboard, click on the help icon <img src=/__resources/help-icon_57882f.png> for Alibaba Cloud secrets, and choose copy <img src=/__resources/copy-icon_276abc.png>.</p><img src=/__resources/alicloud-copy-policy_270ed2.png></li><li><p>Create a custom policy in Alibaba Cloud:</p><ol><li><p>Log on to your Alibaba account and choose <em>RAM</em> > <em>Permissions</em> > <em>Policies</em>.</p><img src=/__resources/alicloud-create-policy_c01478.png></li><li><p>Enter the name of your policy.</p></li><li><p>Select <code>Script</code>.</p></li><li><p>Paste the policy that you copied from the Gardener dashboard to this custom policy.</p></li><li><p>Choose <em>OK</em>.</p><img src=/__resources/alicloud-paste-policy_2c06ea.png></li></ol></li><li><p>In the Alibaba Cloud console, create a new technical user:</p><ol><li><p>Choose <em>RAM</em> > <em>Users</em>.</p></li><li><p>Choose <em>Create User</em>.</p><img src=/__resources/alicloud-create-user_1bcd01.png></li><li><p>Enter a logon and display name for your user.</p></li><li><p>Select <em>Open API Access</em>.</p></li><li><p>Choose <em>OK</em>.</p><img src=/__resources/alicloud-input-user_a2701c.png></li></ol><blockquote><p>After the user is created, <code>AccessKeyId</code> and <code>AccessKeySecret</code> are generated and displayed. Remember to save them. The <code>AccessKey</code> is used later to create secrets for Gardener.</p></blockquote><img src=/__resources/alicloud-user-created_e5639c.png></li><li><p>Assign the policy you created to the technical user:</p><ol><li><p>Choose <em>RAM</em> > <em>Permissions</em> > <em>Grants</em>.</p></li><li><p>Choose <em>Grant Permission</em>.</p><img src=/__resources/alicloud-grant-permission_e7c523.png></li><li><p>Select <em>Alibaba Cloud Account</em>.</p></li><li><p>Assign the policy you’ve created before to the technical user.</p><img src=/__resources/alicloud-assign-policy_8f5061.png></li></ol></li><li><p>Create your secret.</p><ol><li>Type the name of your secret.</li><li>Copy and paste the <code>Access Key ID</code> and <code>Secret Access Key</code> you saved when you created the technical user on Alibaba Cloud.</li><li>Choose <em>Add secret</em>.
<img src=/__resources/alicloud-create-secret-1_a0435c.png></li></ol><blockquote><p>After completing these steps, you should see your newly created secret in the <em>Infrastructure Secrets</em> section.</p></blockquote><img src=/__resources/alicloud-create-secret-2_5a025a.png></li><li><p>To create a new cluster, choose <em>Clusters</em> and then the plus sign in the upper right corner.</p><img src=/__resources/alicloud-new-cluster_5acfe8.png></li><li><p>In the <em>Create Cluster</em> section:</p><ol><li><p>Select <em>AliCloud</em> in the <em>Infrastructure</em> tab.</p></li><li><p>Type the name of your cluster in the <em>Cluster Details</em> tab.</p></li><li><p>Choose the secret you created before in the <em>Infrastructure Details</em> tab.</p></li><li><p>Choose <em>Create</em>.</p><img src=/__resources/alicloud-create-cluster_1acc19.png></li></ol></li><li><p>Wait for your cluster to get created.</p><img src=/__resources/alicloud-processing-cluster_e882c1.png></li></ol><h2 id=result>Result</h2><p>After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster. With it you can create shoot clusters on Alibaba Cloud.
<img src=/__resources/alicloud-kubeconfig_37fa38.png></p><blockquote><p>The size of persistent volumes in your shoot cluster must at least be 20 GiB large. If you choose smaller sizes in your Kubernetes PV definition, the allocation of cloud disk space on Alibaba Cloud fails.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-72073e0eaa6137b600c6b1d53aaa34b9>1.1.2 - Deployment</h1><h1 id=deployment-of-the-alicloud-provider-extension>Deployment of the AliCloud provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the AliCloud provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the AliCloud provider extension <a href=https://github.com/gardener/gardener-extension-provider-alicloud>repository</a>.</p><h2 id=gardener-extension-admission-alicloud>gardener-extension-admission-alicloud</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-alicloud</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-32bc5a106edddf7757a5b2ef31b5ba5e>1.1.3 - Local Setup</h1><h3 id=admission-alicloud>admission-alicloud</h3><p><code>admission-alicloud</code> is an admission webhook server which is responsible for the validation of the cloud provider (Alicloud in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-admission
</span></span></code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-alicloud.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/dev-setup-admission-alicloud.sh
</span></span></code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-alicloud</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5d077509bcc8f24bf91fd4b88929c2ff>1.1.4 - Operations</h1><h1 id=using-the-alicloud-provider-extension-with-gardener-as-operator>Using the Alicloud provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.
The <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>core.gardener.cloud/v1beta1.Seed</code> resource</a> is structured similarly.
Additionally, it allows configuring settings for the backups of the main etcds&rsquo; data of shoot clusters control planes running in this seed cluster.</p><p>This document explains the necessary configuration for this provider extension. In addition, this document also describes how to enable the use of customized machine images for Alicloud.</p><h2 id=cloudprofile-resource><code>CloudProfile</code> resource</h2><p>This section describes, how the configuration for <code>CloudProfile</code> looks like for Alicloud by providing an example <code>CloudProfile</code> manifest with minimal configuration that can be used to allow the creation of Alicloud shoot clusters.</p><h3 id=cloudprofileconfig><code>CloudProfileConfig</code></h3><p>The cloud profile configuration contains information about the real machine image IDs in the Alicloud environment (AMIs).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the Alicloud extension knows the AMI for every version you want to offer.</p><p>An example <code>CloudProfileConfig</code> for the Alicloud extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: coreos
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 2023.4.0
</span></span><span style=display:flex><span>    regions:
</span></span><span style=display:flex><span>    - name: eu-central-1
</span></span><span style=display:flex><span>      id: coreos_2023_4_0_64_30G_alibase_20190319.vhd
</span></span></code></pre></div><h3 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h3><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: alicloud
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: alicloud
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.27.3
</span></span><span style=display:flex><span>    - version: 1.26.8
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2022-10-31T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2023.4.0
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: ecs.sn2ne.large
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>  volumeTypes:
</span></span><span style=display:flex><span>  - name: cloud_efficiency
</span></span><span style=display:flex><span>    class: standard
</span></span><span style=display:flex><span>  - name: cloud_essd
</span></span><span style=display:flex><span>    class: premium
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - name: eu-central-1
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - name: eu-central-1a
</span></span><span style=display:flex><span>    - name: eu-central-1b
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: coreos
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 2023.4.0
</span></span><span style=display:flex><span>        regions:
</span></span><span style=display:flex><span>        - name: eu-central-1
</span></span><span style=display:flex><span>          id: coreos_2023_4_0_64_30G_alibase_20190319.vhd
</span></span></code></pre></div><h2 id=enable-customized-machine-images-for-the-alicloud-extension>Enable customized machine images for the Alicloud extension</h2><p>Customized machine images can be created for an Alicloud account and shared with other Alicloud accounts.
The same customized machine image has different image ID in different regions on Alicloud.
If you need to enable <code>encrypted system disk</code>, you must provide customized machine images.
Administrators/Operators need to explicitly declare them per imageID per region as below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: customized_coreos
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - imageID: &lt;image_id_in_eu_central_1&gt;
</span></span><span style=display:flex><span>    region: eu-central-1
</span></span><span style=display:flex><span>  - imageID: &lt;image_id_in_cn_shanghai&gt;
</span></span><span style=display:flex><span>    region: cn-shanghai
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  version: 2191.4.1
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>End-users have to have the permission to use the customized image from its creator Alicloud account. To enable end-users to use customized images, the images are shared from Alicloud account of Seed operator with end-users&rsquo; Alicloud accounts. Administrators/Operators need to explicitly provide Seed operator&rsquo;s Alicloud account access credentials (base64 encoded) as below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>machineImageOwnerSecret:
</span></span><span style=display:flex><span>  name: machine-image-owner
</span></span><span style=display:flex><span>  accessKeyID: &lt;base64_encoded_access_key_id&gt;
</span></span><span style=display:flex><span>  accessKeySecret: &lt;base64_encoded_access_key_secret&gt;
</span></span></code></pre></div><p>As a result, a Secret named <code>machine-image-owner</code> by default will be created in namespace of Alicloud provider extension.</p><p>Operators should also maintain custom image IDs which are to be shared with end-users as below:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>toBeSharedImageIDs:
</span></span><span style=display:flex><span>- &lt;image_id_1&gt;
</span></span><span style=display:flex><span>- &lt;image_id_2&gt;
</span></span><span style=display:flex><span>- &lt;image_id_3&gt;
</span></span></code></pre></div><h3 id=example-controllerdeployment-manifest-for-enabling-customized-machine-images>Example <code>ControllerDeployment</code> manifest for enabling customized machine images</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-provider-alicloud
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: helm
</span></span><span style=display:flex><span>   providerConfig:
</span></span><span style=display:flex><span>    chart: |<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>      </span>      H4sIFAAAAAAA/yk...
</span></span><span style=display:flex><span>    values:
</span></span><span style=display:flex><span>      config:
</span></span><span style=display:flex><span>        machineImageOwnerSecret:
</span></span><span style=display:flex><span>          accessKeyID: &lt;base64_encoded_access_key_id&gt;
</span></span><span style=display:flex><span>          accessKeySecret: &lt;base64_encoded_access_key_secret&gt;
</span></span><span style=display:flex><span>        toBeSharedImageIDs:
</span></span><span style=display:flex><span>        - &lt;image_id_1&gt;
</span></span><span style=display:flex><span>        - &lt;image_id_2&gt;
</span></span><span style=display:flex><span>        ...
</span></span><span style=display:flex><span>        machineImages:
</span></span><span style=display:flex><span>        - name: customized_coreos
</span></span><span style=display:flex><span>          regions:
</span></span><span style=display:flex><span>          - imageID: &lt;image_id_in_eu_central_1&gt;
</span></span><span style=display:flex><span>            region: eu-central-1
</span></span><span style=display:flex><span>          - imageID: &lt;image_id_in_cn_shanghai&gt;
</span></span><span style=display:flex><span>            region: cn-shanghai
</span></span><span style=display:flex><span>          ...
</span></span><span style=display:flex><span>          version: 2191.4.1
</span></span><span style=display:flex><span>        ...
</span></span><span style=display:flex><span>        csi:
</span></span><span style=display:flex><span>          enableADController: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      resources:
</span></span><span style=display:flex><span>        limits:
</span></span><span style=display:flex><span>          cpu: 500m
</span></span><span style=display:flex><span>          memory: 1Gi
</span></span><span style=display:flex><span>        requests:
</span></span><span style=display:flex><span>          memory: 128Mi
</span></span></code></pre></div><h2 id=seed-resource><code>Seed</code> resource</h2><p>This provider extension does not support any provider configuration for the <code>Seed</code>&rsquo;s <code>.spec.provider.providerConfig</code> field.
However, it supports to managing of backup infrastructure, i.e., you can specify a configuration for the <code>.spec.backup</code> field.</p><h3 id=backup-configuration>Backup configuration</h3><p>A Seed of type <code>alicloud</code> can be configured to perform backups for the main etcds&rsquo; of the shoot clusters control planes using Alicloud <a href=https://www.alibabacloud.com/help/doc-detail/31817.htm>Object Storage Service</a>.</p><p>The location/region where the backups will be stored defaults to the region of the Seed (<code>spec.provider.region</code>).</p><p>Please find below an example <code>Seed</code> manifest (partly) that configures backups using Alicloud Object Storage Service.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-seed
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: alicloud
</span></span><span style=display:flex><span>    region: cn-shanghai
</span></span><span style=display:flex><span>  backup:
</span></span><span style=display:flex><span>    provider: alicloud
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: backup-credentials
</span></span><span style=display:flex><span>      namespace: garden
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>An example of the referenced secret containing the credentials for the Alicloud Object Storage Service can be found in the <a href=https://github.com/gardener/gardener-extension-provider-alicloud/blob/master/example/30-etcd-backup-secret.yaml>example folder</a>.</p><h4 id=permissions-for-alicloud-object-storage-service>Permissions for Alicloud Object Storage Service</h4><p>Please make sure the RAM user associated with the provided AccessKey pair has the following permission.</p><ul><li>AliyunOSSFullAccess</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2c26ab12b01f167d535f38d6ca62d6a9>1.1.5 - Usage</h1><h1 id=using-the-alicloud-provider-extension-with-gardener-as-end-user>Using the Alicloud provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>This document describes the configurable options for Alicloud and provides an example <code>Shoot</code> manifest with minimal configuration that can be used to create an Alicloud cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=alicloud-provider-credentials>Alicloud Provider Credentials</h2><p>In order for Gardener to create a Kubernetes cluster using Alicloud infrastructure components, a Shoot has to provide credentials with sufficient permissions to the desired Alicloud project.
Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of the Alicloud project.</p><p>This <code>Secret</code> must look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-alicloud
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  accessKeyID: base64(access-key-id)
</span></span><span style=display:flex><span>  accessKeySecret: base64(access-key-secret)
</span></span></code></pre></div><p>The <code>SecretBinding</code> is configurable in the <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>Shoot cluster</a> with the field <code>secretBindingName</code>.</p><p>The required credentials for the Alicloud project are an <a href=https://www.alibabacloud.com/help/doc-detail/29009.htm>AccessKey Pair</a> associated with a <a href=https://www.alibabacloud.com/help/doc-detail/28627.htm>Resource Access Management (RAM) User</a>.
A RAM user is a special account that can be used by services and applications to interact with Alicloud Cloud Platform APIs.
Applications can use AccessKey pair to authorize themselves to a set of APIs and perform actions within the permissions granted to the RAM user.</p><p>Make sure to <a href=https://www.alibabacloud.com/help/doc-detail/93720.htm>create a Resource Access Management User</a>, and <a href=https://partners-intl.aliyun.com/help/doc-detail/116401.htm>create an AccessKey Pair</a> that shall be used for the Shoot cluster.</p><h3 id=permissions>Permissions</h3><p>Please make sure the provided credentials have the correct privileges. You can use the following Alicloud RAM policy document and attach it to the RAM user backed by the credentials you provided.</p><details><summary>Click to expand the Alicloud RAM policy document!</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    &#34;Statement&#34;: [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            &#34;Action&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;vpc:*&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>            &#34;Resource&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            &#34;Action&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;ecs:*&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>            &#34;Resource&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            &#34;Action&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;slb:*&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>            &#34;Resource&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            &#34;Action&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;ram:GetRole&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;ram:CreateRole&#34;</span>,
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;ram:CreateServiceLinkedRole&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>            &#34;Resource&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>            &#34;Action&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;ros:*&#34;</span>
</span></span><span style=display:flex><span>            ],
</span></span><span style=display:flex><span>            &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>            &#34;Resource&#34;: [
</span></span><span style=display:flex><span>                <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>            ]
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    ],
</span></span><span style=display:flex><span>    &#34;Version&#34;: <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></details><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the Alicloud extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span><span style=display:flex><span>networks:
</span></span><span style=display:flex><span>  vpc: <span style=color:green># specify either &#39;id&#39; or &#39;cidr&#39;</span>
</span></span><span style=display:flex><span>  <span style=color:green># id: my-vpc</span>
</span></span><span style=display:flex><span>    cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>  <span style=color:green># gardenerManagedNATGateway: true</span>
</span></span><span style=display:flex><span>  zones:
</span></span><span style=display:flex><span>  - name: eu-central-1a
</span></span><span style=display:flex><span>    workers: 10.250.1.0/24
</span></span><span style=display:flex><span>  <span style=color:green># natGateway:</span>
</span></span><span style=display:flex><span>    <span style=color:green># eipAllocationID: eip-ufxsdg122elmszcg</span>
</span></span></code></pre></div><p>The <code>networks.vpc</code> section describes whether you want to create the shoot cluster in an already existing VPC or whether to create a new one:</p><ul><li>If <code>networks.vpc.id</code> is given then you have to specify the VPC ID of the existing VPC that was created by other means (manually, other tooling, &mldr;).</li><li>If <code>networks.vpc.cidr</code> is given then you have to specify the VPC CIDR of a new VPC that will be created during shoot creation.
You can freely choose a private CIDR range.</li><li>Either <code>networks.vpc.id</code> or <code>networks.vpc.cidr</code> must be present, but not both at the same time.</li><li>When <code>networks.vpc.id</code> is present, in addition, you can also choose to set <code>networks.vpc.gardenerManagedNATGateway</code>. It is by default <code>false</code>. When it is set to <code>true</code>,
Gardener will create an Enhanced NATGateway in the VPC and associate it with a VSwitch created in the first zone in the <code>networks.zones</code>.</li><li>Please note that when <code>networks.vpc.id</code> is present, and <code>networks.vpc.gardenerManagedNATGateway</code> is <code>false</code> or not set, you have to <strong>manually</strong> create an Enhance NATGateway
and associate it with a VSwitch that you <strong>manually</strong> created. In this case, make sure the worker CIDRs in <code>networks.zones</code> do not overlap with the one you created.
If a NATGateway is created manually and a shoot is created in the same VPC with <code>networks.vpc.gardenerManagedNATGateway</code> set <code>true</code>, you need to manually adjust the route rule accordingly.
You may refer to <a href=https://www.alibabacloud.com/help/en/doc-detail/121139.html>here</a>.</li></ul><p>The <code>networks.zones</code> section describes which subnets you want to create in availability zones.
For every zone, the Alicloud extension creates one subnet:</p><ul><li>The <code>workers</code> subnet is used for all shoot worker nodes, i.e., VMs which later run your applications.</li></ul><p>For every subnet, you have to specify a CIDR range contained in the VPC CIDR specified above, or the VPC CIDR of your already existing VPC.
You can freely choose these CIDR and it is your responsibility to properly design the network layout to suit your needs.</p><p>If you want to use multiple availability zones then add a second, third, &mldr; entry to the <code>networks.zones[]</code> list and properly specify the AZ name in <code>networks.zones[].name</code>.</p><p>Apart from the VPC and the subnets the Alicloud extension will also create a NAT gateway (only if a new VPC is created), a key pair, elastic IPs, VSwitches, a SNAT table entry, and security groups.</p><p>By default, the Alicloud extension will create a corresponding Elastic IP that it attaches to this NAT gateway and which is used for egress traffic.
The <code>networks.zones[].natGateway.eipAllocationID</code> field allows you to specify the Elastic IP Allocation ID of an existing Elastic IP allocation in case you want to bring your own.
If provided, no new Elastic IP will be created and, instead, the Elastic IP specified by you will be used.</p><p>⚠️ If you change this field for an already existing infrastructure then it will disrupt egress traffic while Alicloud applies this change, because the NAT gateway must be recreated with the new Elastic IP association.
Also, please note that the existing Elastic IP will be permanently deleted if it was earlier created by the Alicloud extension.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the Alicloud-specific control plane components.
Today, the Alicloud extension deploys the <code>cloud-controller-manager</code> and the CSI controllers.</p><p>An example <code>ControlPlaneConfig</code> for the Alicloud extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span><span style=display:flex><span>csi:
</span></span><span style=display:flex><span>  enableADController: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>cloudControllerManager:
</span></span><span style=display:flex><span>  featureGates:
</span></span><span style=display:flex><span>    RotateKubeletServerCertificate: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The <code>csi.enableADController</code> is used as the value of environment <a href=https://github.com/kubernetes-sigs/alibaba-cloud-csi-driver/blob/cd0788a0a440926d504d8f8fb7f6e738fe96f3ae/pkg/disk/nodeserver.go#L80>DISK_AD_CONTROLLER</a>, which is used for AliCloud csi-disk-plugin. This field is optional. When a new shoot is creatd, this field is automatically set true. For an existing shoot created in previous versions, it remains unchanged. If there are persistent volumes created before year 2021, please be cautious to set this field <em>true</em> because they may fail to mount to nodes.</p><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><h2 id=workerconfig><code>WorkerConfig</code></h2><p>The Alicloud extension does not support a specific <code>WorkerConfig</code>. However, it supports additional data volumes (plus encryption) per machine.
By default (if not stated otherwise), all the disks are unencrypted.
For each data volume, you have to specify a name.
It also supports encrypted system disk.
However, only <a href="https://www.alibabacloud.com/help/doc-detail/172789.htm?spm=a2c63.l28256.b99.244.5da67453bNBrCt">Customized image</a> is currently supported to be used as a basic image for encrypted system disk.
Please be noted that the change of system disk encryption flag will cause reconciliation of a shoot, and it will result in nodes rolling update within the worker group.</p><p>The following YAML is a snippet of a <code>Shoot</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: cpu-worker
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        type: cloud_efficiency
</span></span><span style=display:flex><span>        size: 20Gi
</span></span><span style=display:flex><span>        encrypted: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      dataVolumes:
</span></span><span style=display:flex><span>      - name: kubelet-dir
</span></span><span style=display:flex><span>        type: cloud_efficiency
</span></span><span style=display:flex><span>        size: 25Gi
</span></span><span style=display:flex><span>        encrypted: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=example-shoot-manifest-one-availability-zone>Example <code>Shoot</code> manifest (one availability zone)</h2><p>Please find below an example <code>Shoot</code> manifest for one availability zone:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-alicloud
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: alicloud
</span></span><span style=display:flex><span>  region: eu-central-1
</span></span><span style=display:flex><span>  secretBindingName: core-alicloud
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: alicloud
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        zones:
</span></span><span style=display:flex><span>        - name: eu-central-1a
</span></span><span style=display:flex><span>          workers: 10.250.0.0/19
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: ecs.sn2ne.large
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: cloud_efficiency
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - eu-central-1a
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=example-shoot-manifest-two-availability-zones>Example <code>Shoot</code> manifest (two availability zones)</h2><p>Please find below an example <code>Shoot</code> manifest for two availability zones:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-alicloud
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: alicloud
</span></span><span style=display:flex><span>  region: eu-central-1
</span></span><span style=display:flex><span>  secretBindingName: core-alicloud
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: alicloud
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        zones:
</span></span><span style=display:flex><span>        - name: eu-central-1a
</span></span><span style=display:flex><span>          workers: 10.250.0.0/26
</span></span><span style=display:flex><span>        - name: eu-central-1b
</span></span><span style=display:flex><span>          workers: 10.250.0.64/26
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: alicloud.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: ecs.sn2ne.large
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 4
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: cloud_efficiency
</span></span><span style=display:flex><span>        <span style=color:green># NOTE: Below comment is for the case when encrypted field of an existing shoot is updated from false to true.</span>
</span></span><span style=display:flex><span>        <span style=color:green># It will cause affected nodes to be rolling updated. Users must trigger a MAINTAIN operation of the shoot.</span>
</span></span><span style=display:flex><span>        <span style=color:green># Otherwise, the shoot will fail to reconcile.</span>
</span></span><span style=display:flex><span>        <span style=color:green># You could do it either via Dashboard or annotating the shoot with gardener.cloud/operation=maintain</span>
</span></span><span style=display:flex><span>        encrypted: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - eu-central-1a
</span></span><span style=display:flex><span>      - eu-central-1b
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-alicloud@v1.33</code>.</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> feature gate since <code>gardener-extension-provider-alicloud@v1.36</code> and <code>ShootSARotation</code> feature gate since <code>gardener-extension-provider-alicloud@v1.37</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2fdc592b220f6647eac2d46161fce5c7>1.2 - Provider AWS</h1><div class=lead>Gardener extension controller for the AWS cloud provider</div><h1 id=gardener-extension-for-aws-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for AWS provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener-tests/pipelines/gardener-extension-provider-aws-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener-tests/pipelines/gardener-extension-provider-aws-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-aws><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-aws alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the AWS provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-aws/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.28</td><td>1.28.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.27</td><td>1.27.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.27%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.27%20AWS/tests_status?style=svg" alt="Gardener v1.27 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.26</td><td>1.26.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.26%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.26%20AWS/tests_status?style=svg" alt="Gardener v1.26 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.25</td><td>1.25.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20AWS/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.24</td><td>1.24.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20AWS><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20AWS/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><h2 id=compatibility>Compatibility</h2><p>The following lists known compatibility issues of this extension controller with other Gardener components.</p><table><thead><tr><th>AWS Extension</th><th>Gardener</th><th>Action</th><th>Notes</th></tr></thead><tbody><tr><td><code>&lt;= v1.15.0</code></td><td><code>>v1.10.0</code></td><td>Please update the provider version to <code>> v1.15.0</code> or disable the feature gate <code>MountHostCADirectories</code> in the Gardenlet.</td><td>Applies if feature flag <code>MountHostCADirectories</code> in the Gardenlet is enabled. Shoots with CSI enabled (Kubernetes version >= 1.18) miss a mount to the directory <code>/etc/ssl</code> in the Shoot API Server. This can lead to not trusting external Root CAs when the API Server makes requests via webhooks or OIDC.</td></tr></tbody></table><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-aws/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-979df175ce45dca2d78e2698ba76c8c2>1.2.1 - Tutorials</h1><h3 id=overview>Overview</h3><p>Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on AWS.</p><h3 id=prerequisites>Prerequisites</h3><ul><li>You have created an <a href=https://aws.amazon.com/>AWS account</a>.</li><li>You have access to the Gardener dashboard and have permissions to create projects.</li></ul><h3 id=steps>Steps</h3><ol><li><p>Go to the Gardener dashboard and create a <em>Project</em>.</p><img src=/__resources/new-gardener-project_ad03bc.png></li><li><p>Choose <em>Secrets</em>, then the plus icon <img src=/__resources/plus-icon_3b1f20.png> and select <em>AWS</em>.</p><img src=/__resources/create-secret-aws_79dc1a.png></li><li><p>To copy the policy for AWS from the Gardener dashboard, click on the help icon <img src=/__resources/help-icon_01486c.png> for AWS secrets, and choose copy <img src=/__resources/copy-icon_0f5ab8.png>.</p><img src=/__resources/gardener-copy-policy_a52965.png></li><li><p><a href=https://console.aws.amazon.com/iam/home?#/policies>Create a new policy</a> in AWS:</p><ol><li><p>Choose <em>Create policy</em>.</p><img src=/__resources/amazon-create-policy_5ef114.png></li><li><p>Paste the policy that you copied from the Gardener dashboard to this custom policy.</p><img src=/__resources/amazon-create-policy-json_7d6327.png></li><li><p>Choose <em>Next</em> until you reach the Review section.</p></li><li><p>Fill in the name and description, then choose <em>Create policy</em>.</p><img src=/__resources/amazon-review-policy_6fba71.png></li></ol></li><li><p><a href="https://console.aws.amazon.com/iam/home?#/users$new?step=details">Create a new technical user</a> in AWS:</p><ol><li><p>Type in a username and select the access key credential type.</p><img src=/__resources/add-user_775731.png></li><li><p>Choose <em>Attach an existing policy</em>.</p></li><li><p>Select <em>GardenerAccess</em> from the policy list.</p></li><li><p>Choose <em>Next</em> until you reach the Review section.</p></li></ol><img src=/__resources/attach-policy_a6a81f.png>
<img src=/__resources/finish-user_a9e956.png><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Note: After the user is created, <code>Access key ID</code> and <code>Secret access key</code> are generated and displayed. Remember to save them. The <code>Access key ID</code> is used later to create secrets for Gardener.</div><img src=/__resources/save-keys_f23816.png></li><li><p>On the Gardener dashboard, choose <em>Secrets</em> and then the plus sign <img src=/__resources/plus-icon_3b1f20.png>. Select <em>AWS</em> from the drop down menu to add a new AWS secret.</p></li><li><p>Create your secret.</p><ol><li>Type the name of your secret.</li><li>Copy and paste the <code>Access Key ID</code> and <code>Secret Access Key</code> you saved when you created the technical user on AWS.</li><li>Choose <em>Add secret</em>.
<img src=/__resources/add-aws-secret_ed47ad.png></li></ol><blockquote><p>After completing these steps, you should see your newly created secret in the <em>Infrastructure Secrets</em> section.</p></blockquote><img src=/__resources/secret-stored_a4c7f9.png></li><li><p>To create a new cluster, choose <em>Clusters</em> and then the plus sign in the upper right corner.</p><img src=/__resources/new-cluster_353d7b.png></li><li><p>In the <em>Create Cluster</em> section:</p><ol><li>Select <em>AWS</em> in the <em>Infrastructure</em> tab.</li><li>Type the name of your cluster in the <em>Cluster Details</em> tab.</li><li>Choose the secret you created before in the <em>Infrastructure Details</em> tab.</li><li>Choose <em>Create</em>.</li></ol><img src=/__resources/create-cluster_7a45a2.png></li><li><p>Wait for your cluster to get created.</p><img src=/__resources/processing-cluster_522005.png></li></ol><h3 id=result>Result</h3><p>After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster.</p><img src=/__resources/copy-kubeconfig_752d59.png></div><div class=td-content style=page-break-before:always><h1 id=pg-3df8b1939d1f45daa8220886832c9751>1.2.2 - Deployment</h1><h1 id=deployment-of-the-aws-provider-extension>Deployment of the AWS provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the AWS provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the AWS provider extension <a href=https://github.com/gardener/gardener-extension-provider-aws>repository</a>.</p><h2 id=gardener-extension-admission-aws>gardener-extension-admission-aws</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-aws</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-60ce03f807554a2877fd8358d47eefca>1.2.3 - Dual Stack Ingress</h1><h1 id=using-ipv4ipv6-dual-stack-ingress-in-an-ipv4-single-stack-cluster>Using IPv4/IPv6 (dual-stack) Ingress in an IPv4 single-stack cluster</h1><h2 id=motivation>Motivation</h2><p>IPv6 adoption is continuously growing, already overtaking IPv4 in certain regions, e.g. India, or scenarios, e.g. mobile.
Even though most IPv6 installations deploy means to reach IPv4, it might still be beneficial to expose services
natively via IPv4 and IPv6 instead of just relying on IPv4.</p><h2 id=disadvantages-of-full-ipv4ipv6-dual-stack-deployments>Disadvantages of full IPv4/IPv6 (dual-stack) Deployments</h2><p>Enabling full IPv4/IPv6 (dual-stack) support in a kubernetes cluster is a major endeavor. It requires a lot of changes
and restarts of all pods so that all pods get addresses for both IP families. A side-effect of dual-stack networking
is that failures may be hidden as network traffic may take the other protocol to reach the target. For this reason and
also due to reduced operational complexity, service teams might lean towards staying in a single-stack environment as
much as possible. Luckily, this is possible with Gardener and IPv4/IPv6 (dual-stack) ingress on AWS.</p><h2 id=simplifying-ipv4ipv6-dual-stack-ingress-with-protocol-translation-on-aws>Simplifying IPv4/IPv6 (dual-stack) Ingress with Protocol Translation on AWS</h2><p>Fortunately, the network load balancer on AWS supports automatic protocol translation, i.e. it can expose both IPv4 and
IPv6 endpoints while communicating with just one protocol to the backends. Under the hood, automatic protocol translation
takes place. Client IP address preservation can be achieved by using proxy protocol.</p><p>This approach enables users to expose IPv4 workload to IPv6-only clients without having to change the workload/service.
Without requiring invasive changes, it allows a fairly simple first step into the IPv6 world for services just requiring
ingress (incoming) communication.</p><h2 id=necessary-shoot-cluster-configuration-changes-for-ipv4ipv6-dual-stack-ingress>Necessary Shoot Cluster Configuration Changes for IPv4/IPv6 (dual-stack) Ingress</h2><p>To be able to utilize IPv4/IPv6 (dual-stack) Ingress in an IPv4 shoot cluster, the cluster needs to meet two preconditions:</p><ol><li><code>dualStack.enabled</code> needs to be set to <code>true</code> to configure VPC/subnet for IPv6 and add a routing rule for IPv6.
(This does not add IPv6 addresses to kubernetes nodes.)</li><li><code>loadBalancerController.enabled</code> needs to be set to <code>true</code> as well to use the load balancer controller, which supports
dual-stack ingress.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      dualStack:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>        loadBalancerController:
</span></span><span style=display:flex><span>          enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>After adapting the shoot specification and reconciling the cluster, dual-stack load balancers can be created using
kubernetes services objects.</p><h2 id=creating-an-ipv4ipv6-dual-stack-ingress>Creating an IPv4/IPv6 (dual-stack) Ingress</h2><p>With the preconditions set, creating an IPv4/IPv6 load balancer is as easy as annotating a service with the correct
annotations:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-type: external
</span></span><span style=display:flex><span>  name: ...
</span></span><span style=display:flex><span>  namespace: ...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  type: LoadBalancer
</span></span></code></pre></div><p>In case the client IP address should be preserved, the following annotation can be used to enable proxy protocol.
(The pod receiving the traffic needs to be configured for proxy protocol as well.)</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: <span style=color:#a31515>&#34;*&#34;</span>
</span></span></code></pre></div><p>Please note that changing an existing <code>Service</code> to dual-stack may cause the creation of a new load balancer without
deletion of the old AWS load balancer resource. While this helps in a seamless migration by not cutting existing
connections it may lead to wasted/forgotten resources. Therefore, the (manual) cleanup needs to be taken into account
when migrating an existing <code>Service</code> instance.</p><p>For more details see <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/service/nlb/>AWS Load Balancer Documentation - Network Load Balancer</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f9e02d7a81c7f540c23da08a17dfec76>1.2.4 - Local Setup</h1><h3 id=admission-aws>admission-aws</h3><p><code>admission-aws</code> is an admission webhook server which is responsible for the validation of the cloud provider (AWS in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-admission
</span></span></code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-aws.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/dev-setup-admission-aws.sh
</span></span></code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-aws</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-464bc9c324b16ca3984ff0327efdf6c3>1.2.5 - Operations</h1><h1 id=using-the-aws-provider-extension-with-gardener-as-operator>Using the AWS provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.
Similarly, the <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>core.gardener.cloud/v1beta1.Seed</code> resource</a> is structured.
Additionally, it allows to configure settings for the backups of the main etcds&rsquo; data of shoot clusters control planes running in this seed cluster.</p><p>This document explains what is necessary to configure for this provider extension.</p><h2 id=cloudprofile-resource><code>CloudProfile</code> resource</h2><p>In this section we are describing how the configuration for <code>CloudProfile</code>s looks like for AWS and provide an example <code>CloudProfile</code> manifest with minimal configuration that you can use to allow creating AWS shoot clusters.</p><h3 id=cloudprofileconfig><code>CloudProfileConfig</code></h3><p>The cloud profile configuration contains information about the real machine image IDs in the AWS environment (AMIs).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the AWS extension knows the AMI for every version you want to offer.
For each AMI an <code>architecture</code> field can be specified which specifies the CPU architecture of the machine on which given machine image can be used.</p><p>An example <code>CloudProfileConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: coreos
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 2135.6.0
</span></span><span style=display:flex><span>    regions:
</span></span><span style=display:flex><span>    - name: eu-central-1
</span></span><span style=display:flex><span>      ami: ami-034fd8c3f4026eb39
</span></span><span style=display:flex><span>      <span style=color:green># architecture: amd64 # optional</span>
</span></span></code></pre></div><h3 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h3><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: aws
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: aws
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.27.3
</span></span><span style=display:flex><span>    - version: 1.26.8
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2022-10-31T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2135.6.0
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: m5.large
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  volumeTypes:
</span></span><span style=display:flex><span>  - name: gp2
</span></span><span style=display:flex><span>    class: standard
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  - name: io1
</span></span><span style=display:flex><span>    class: premium
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - name: eu-central-1
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - name: eu-central-1a
</span></span><span style=display:flex><span>    - name: eu-central-1b
</span></span><span style=display:flex><span>    - name: eu-central-1c
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: coreos
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 2135.6.0
</span></span><span style=display:flex><span>        regions:
</span></span><span style=display:flex><span>        - name: eu-central-1
</span></span><span style=display:flex><span>          ami: ami-034fd8c3f4026eb39
</span></span><span style=display:flex><span>          <span style=color:green># architecture: amd64 # optional</span>
</span></span></code></pre></div><h2 id=seed-resource><code>Seed</code> resource</h2><p>This provider extension does not support any provider configuration for the <code>Seed</code>&rsquo;s <code>.spec.provider.providerConfig</code> field.
However, it supports to manage backup infrastructure, i.e., you can specify configuration for the <code>.spec.backup</code> field.</p><h3 id=backup-configuration>Backup configuration</h3><p>Please find below an example <code>Seed</code> manifest (partly) that configures backups.
As you can see, the location/region where the backups will be stored can be different to the region where the seed cluster is running.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: backup-credentials
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  accessKeyID: base64(access-key-id)
</span></span><span style=display:flex><span>  secretAccessKey: base64(secret-access-key)
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-seed
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    region: eu-west-1
</span></span><span style=display:flex><span>  backup:
</span></span><span style=display:flex><span>    provider: aws
</span></span><span style=display:flex><span>    region: eu-central-1
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: backup-credentials
</span></span><span style=display:flex><span>      namespace: garden
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>Please look up <a href=https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys</a> as well.</p><h4 id=permissions-for-aws-iam-user>Permissions for AWS IAM user</h4><p>Please make sure that the provided credentials have the correct privileges. You can use the following AWS IAM policy document and attach it to the IAM user backed by the credentials you provided (please check the <a href=http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html>official AWS documentation</a> as well):</p><details><summary>Click to expand the AWS IAM policy document!</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>  &#34;Statement&#34;: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: <span style=color:#a31515>&#34;s3:*&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></details></div><div class=td-content style=page-break-before:always><h1 id=pg-b56ae0589f68df0d7a157f7430ba7bbd>1.2.6 - Usage</h1><h1 id=using-the-aws-provider-extension-with-gardener-as-end-user>Using the AWS provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for AWS and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create an AWS cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=provider-secret-data>Provider Secret Data</h2><p>Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of your AWS account.
This <code>Secret</code> must look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-aws
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  accessKeyID: base64(access-key-id)
</span></span><span style=display:flex><span>  secretAccessKey: base64(secret-access-key)
</span></span></code></pre></div><p>The <a href=https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>AWS documentation</a> explains the necessary steps to enable programmatic access, i.e. create <strong>access key ID</strong> and <strong>access key</strong>, for the user of your choice.</p><p>⚠️ For security reasons, we recommend creating a <strong>dedicated user with programmatic access only</strong>. Please avoid re-using a IAM user which has access to the AWS console (human user).</p><p>⚠️ Depending on your AWS API usage it can be problematic to reuse the same AWS Account for different Shoot clusters in the same region due to rate limits. Please consider spreading your Shoots over multiple AWS Accounts if you are hitting those limits.</p><h3 id=permissions>Permissions</h3><p>Please make sure that the provided credentials have the correct privileges. You can use the following AWS IAM policy document and attach it to the IAM user backed by the credentials you provided (please check the <a href=http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html>official AWS documentation</a> as well):</p><details><summary>Click to expand the AWS IAM policy document!</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>  &#34;Statement&#34;: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: <span style=color:#a31515>&#34;autoscaling:*&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: <span style=color:#a31515>&#34;ec2:*&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: <span style=color:#a31515>&#34;elasticloadbalancing:*&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Action&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetPolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetPolicyVersion&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:ListPolicyVersions&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:ListRolePolicies&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:ListAttachedRolePolicies&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:ListInstanceProfilesForRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreateInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreatePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreatePolicyVersion&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreateRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreateServiceLinkedRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:AddRoleToInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:AttachRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DetachRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:RemoveRoleFromInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeletePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeletePolicyVersion&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeleteRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeleteRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeleteInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:PutRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:PassRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:UpdateAssumeRolePolicy&#34;</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:green>// The following permission set is only needed, if AWS Load Balancer controller is enabled (see ControlPlaneConfig)
</span></span></span><span style=display:flex><span><span style=color:green></span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;cognito-idp:DescribeUserPoolClient&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;acm:ListCertificates&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;acm:DescribeCertificate&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:ListServerCertificates&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetServerCertificate&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;waf-regional:GetWebACL&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;waf-regional:GetWebACLForResource&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;waf-regional:AssociateWebACL&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;waf-regional:DisassociateWebACL&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;wafv2:GetWebACL&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;wafv2:GetWebACLForResource&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;wafv2:AssociateWebACL&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;wafv2:DisassociateWebACL&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;shield:GetSubscriptionState&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;shield:DescribeProtection&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;shield:CreateProtection&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;shield:DeleteProtection&#34;</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></details><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span><span style=display:flex><span>enableECRAccess: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>dualStack:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>networks:
</span></span><span style=display:flex><span>  vpc: <span style=color:green># specify either &#39;id&#39; or &#39;cidr&#39;</span>
</span></span><span style=display:flex><span>  <span style=color:green># id: vpc-123456</span>
</span></span><span style=display:flex><span>    cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>  <span style=color:green># gatewayEndpoints:</span>
</span></span><span style=display:flex><span>  <span style=color:green># - s3</span>
</span></span><span style=display:flex><span>  zones:
</span></span><span style=display:flex><span>  - name: eu-west-1a
</span></span><span style=display:flex><span>    internal: 10.250.112.0/22
</span></span><span style=display:flex><span>    public: 10.250.96.0/22
</span></span><span style=display:flex><span>    workers: 10.250.0.0/19
</span></span><span style=display:flex><span>  <span style=color:green># elasticIPAllocationID: eipalloc-123456</span>
</span></span><span style=display:flex><span>ignoreTags:
</span></span><span style=display:flex><span>  keys: <span style=color:green># individual ignored tag keys</span>
</span></span><span style=display:flex><span>  - SomeCustomKey
</span></span><span style=display:flex><span>  - AnotherCustomKey
</span></span><span style=display:flex><span>  keyPrefixes: <span style=color:green># ignored tag key prefixes</span>
</span></span><span style=display:flex><span>  - user.specific/prefix/
</span></span></code></pre></div><p>The <code>enableECRAccess</code> flag specifies whether the AWS IAM role policy attached to all worker nodes of the cluster shall contain permissions to access the Elastic Container Registry of the respective AWS account.
If the flag is not provided it is defaulted to <code>true</code>.
Please note that if the <code>iamInstanceProfile</code> is set for a worker pool in the <code>WorkerConfig</code> (see below) then <code>enableECRAccess</code> does not have any effect.
It only applies for those worker pools whose <code>iamInstanceProfile</code> is not set.</p><details><summary>Click to expand the default AWS IAM policy document used for the instance profiles!</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>  &#34;Statement&#34;: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ec2:DescribeInstances&#34;</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      &#34;Resource&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>      ]
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:green>// Only if `.enableECRAccess` is `true`.
</span></span></span><span style=display:flex><span><span style=color:green></span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:GetAuthorizationToken&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:BatchCheckLayerAvailability&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:GetDownloadUrlForLayer&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:GetRepositoryPolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:DescribeRepositories&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:ListImages&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:BatchGetImage&#34;</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      &#34;Resource&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>      ]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></details><p>The <code>dualStack.enabled</code> flag specifies whether dual-stack or IPv4-only should be supported by the infrastructure.
When the flag is set to true an Amazon provided IPv6 CIDR block will be attached to the VPC.
All subnets will receive a <code>/64</code> block from it and a route entry is added to the main route table to route all IPv6 traffic over the IGW.</p><p>The <code>networks.vpc</code> section describes whether you want to create the shoot cluster in an already existing VPC or whether to create a new one:</p><ul><li>If <code>networks.vpc.id</code> is given then you have to specify the VPC ID of the existing VPC that was created by other means (manually, other tooling, &mldr;).
Please make sure that the VPC has attached an internet gateway - the AWS controller won&rsquo;t create one automatically for existing VPCs. To make sure the nodes are able to join and operate in your cluster properly, please make sure that your VPC has enabled <a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html>DNS Support</a>, explicitly the attributes <code>enableDnsHostnames</code> and <code>enableDnsSupport</code> must be set to <code>true</code>.</li><li>If <code>networks.vpc.cidr</code> is given then you have to specify the VPC CIDR of a new VPC that will be created during shoot creation.
You can freely choose a private CIDR range.</li><li>Either <code>networks.vpc.id</code> or <code>networks.vpc.cidr</code> must be present, but not both at the same time.</li><li><code>networks.vpc.gatewayEndpoints</code> is optional. If specified then each item is used as service name in a corresponding Gateway VPC Endpoint.</li></ul><p>The <code>networks.zones</code> section contains configuration for resources you want to create or use in availability zones.
For every zone, the AWS extension creates three subnets:</p><ul><li>The <code>internal</code> subnet is used for <a href=https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internal-load-balancers.html>internal AWS load balancers</a>.</li><li>The <code>public</code> subnet is used for <a href=https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html>public AWS load balancers</a>.</li><li>The <code>workers</code> subnet is used for all shoot worker nodes, i.e., VMs which later run your applications.</li></ul><p>For every subnet, you have to specify a CIDR range contained in the VPC CIDR specified above, or the VPC CIDR of your already existing VPC.
You can freely choose these CIDRs and it is your responsibility to properly design the network layout to suit your needs.</p><p>Also, the AWS extension creates a dedicated NAT gateway for each zone.
By default, it also creates a corresponding Elastic IP that it attaches to this NAT gateway and which is used for egress traffic.
The <code>elasticIPAllocationID</code> field allows you to specify the ID of an existing Elastic IP allocation in case you want to bring your own.
If provided, no new Elastic IP will be created and, instead, the Elastic IP specified by you will be used.</p><p>⚠️ If you change this field for an already existing infrastructure then it will disrupt egress traffic while AWS applies this change.
The reason is that the NAT gateway must be recreated with the new Elastic IP association.
Also, please note that the existing Elastic IP will be permanently deleted if it was earlier created by the AWS extension.</p><p>You can configure <a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html>Gateway VPC Endpoints</a> by adding items in the optional list <code>networks.vpc.gatewayEndpoints</code>. Each item in the list is used as a service name and a corresponding endpoint is created for it. All created endpoints point to the service within the cluster&rsquo;s region. For example, consider this (partial) shoot config:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  region: eu-central-1
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          gatewayEndpoints:
</span></span><span style=display:flex><span>          - s3
</span></span></code></pre></div><p>The service name of the S3 Gateway VPC Endpoint in this example is <code>com.amazonaws.eu-central-1.s3</code>.</p><p>If you want to use multiple availability zones then add a second, third, &mldr; entry to the <code>networks.zones[]</code> list and properly specify the AZ name in <code>networks.zones[].name</code>.</p><p>Apart from the VPC and the subnets the AWS extension will also create DHCP options and an internet gateway (only if a new VPC is created), routing tables, security groups, elastic IPs, NAT gateways, EC2 key pairs, IAM roles, and IAM instance profiles.</p><p>The <code>ignoreTags</code> section allows to configure which resource tags on AWS resources managed by Gardener should be ignored during
infrastructure reconciliation. By default, all tags that are added outside of Gardener&rsquo;s
reconciliation will be removed during the next reconciliation. This field allows users and automation to add
custom tags on AWS resources created and managed by Gardener without loosing them on the next reconciliation.
Tags can ignored either by specifying exact key values (<code>ignoreTags.keys</code>) or key prefixes (<code>ignoreTags.keyPrefixes</code>).
In both cases it is forbidden to ignore the <code>Name</code> tag or any tag starting with <code>kubernetes.io</code> or <code>gardener.cloud</code>.<br>Please note though, that the tags are only ignored on resources created on behalf of the <code>Infrastructure</code> CR (i.e. VPC,
subnets, security groups, keypair, etc.), while tags on machines, volumes, etc. are not in the scope of this controller.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the AWS-specific control plane components.
Today, the only component deployed by the AWS extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span><span style=display:flex><span>cloudControllerManager:
</span></span><span style=display:flex><span>  featureGates:
</span></span><span style=display:flex><span>    RotateKubeletServerCertificate: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  useCustomRouteController: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span><span style=color:green>#loadBalancerController:</span>
</span></span><span style=display:flex><span><span style=color:green>#  enabled: true</span>
</span></span><span style=display:flex><span><span style=color:green>#  ingressClassName: alb</span>
</span></span><span style=display:flex><span>storage:
</span></span><span style=display:flex><span>  managedDefaultClass: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><p>The <code>cloudControllerManager.useCustomRouteController</code> controls if the <a href=https://github.com/gardener/aws-custom-route-controller>custom routes controller</a> should be enabled.
If enabled, it will add routes to the pod CIDRs for all nodes in the route tables for all zones.</p><p>The <code>storage.managedDefaultClass</code> controls if the <code>default</code> storage / volume snapshot classes are marked as default by Gardener. Set it to <code>false</code> to <a href=https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/>mark another storage / volume snapshot class as default</a> without Gardener overwriting this change. If unset, this field defaults to <code>true</code>.</p><p>If the <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/>AWS Load Balancer Controller</a> should be deployed, set <code>loadBalancerController.enabled</code> to <code>true</code>.
In this case, it is assumed that an <code>IngressClass</code> named <code>alb</code> is created <strong>by the user</strong>.
You can overwrite the name by setting <code>loadBalancerController.ingressClassName</code>.</p><p>Please note, that currently only the &ldquo;instance&rdquo; mode is supported.</p><h3 id=examples-for-ingress-and-service-managed-by-the-aws-load-balancer-controller>Examples for <code>Ingress</code> and <code>Service</code> managed by the AWS Load Balancer Controller:</h3><ol start=0><li>Prerequites</li></ol><p>Make sure you have created an <code>IngressClass</code>. For more details about parameters, please see <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.5/guide/ingress/ingress_class/>AWS Load Balancer Controller - IngressClass</a></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: IngressClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: alb <span style=color:green># default name if not specified by `loadBalancerController.ingressClassName` </span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  controller: ingress.k8s.aws/alb
</span></span></code></pre></div><ol><li>Ingress</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  name: echoserver
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    <span style=color:green># complete set of annotations: https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/ingress/annotations/</span>
</span></span><span style=display:flex><span>    alb.ingress.kubernetes.io/scheme: internet-facing
</span></span><span style=display:flex><span>    alb.ingress.kubernetes.io/target-type: instance <span style=color:green># target-type &#34;ip&#34; NOT supported in Gardener</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ingressClassName: alb
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>    - http:
</span></span><span style=display:flex><span>        paths:
</span></span><span style=display:flex><span>        - path: /
</span></span><span style=display:flex><span>          pathType: Prefix
</span></span><span style=display:flex><span>          backend:
</span></span><span style=display:flex><span>            service:
</span></span><span style=display:flex><span>              name: echoserver
</span></span><span style=display:flex><span>              port:
</span></span><span style=display:flex><span>                number: 80
</span></span></code></pre></div><p>For more details see <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/ingress/spec/>AWS Load Balancer Documentation - Ingress Specification</a></p><ol start=2><li>Service of Type <code>LoadBalancer</code></li></ol><p>This can be used to create a Network Load Balancer (NLB).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    <span style=color:green># complete set of annotations: https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/service/annotations/</span>
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance <span style=color:green># target-type &#34;ip&#34; NOT supported in Gardener</span>
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
</span></span><span style=display:flex><span>  name: ingress-nginx-controller
</span></span><span style=display:flex><span>  namespace: ingress-nginx
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  type: LoadBalancer
</span></span><span style=display:flex><span>  loadBalancerClass: service.k8s.aws/nlb <span style=color:green># mandatory to be managed by AWS Load Balancer Controller (otherwise the Cloud Controller Manager will act on it)</span>
</span></span></code></pre></div><p>For more details see <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/service/nlb/>AWS Load Balancer Documentation - Network Load Balancer</a></p><h2 id=workerconfig><code>WorkerConfig</code></h2><p>The AWS extension supports encryption for volumes plus support for additional data volumes per machine.
For each data volume, you have to specify a name.
By default (if not stated otherwise), all the disks (root & data volumes) are encrypted.
Please make sure that your <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html>instance-type supports encryption</a>.
If your instance-type doesn&rsquo;t support encryption, you will have to disable encryption (which is enabled by default) by setting <code>volume.encrpyted</code> to <code>false</code> (refer below shown YAML snippet).</p><p>The following YAML is a snippet of a <code>Shoot</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: cpu-worker
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>        size: 20Gi
</span></span><span style=display:flex><span>        encrypted: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      dataVolumes:
</span></span><span style=display:flex><span>      - name: kubelet-dir
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>        size: 25Gi
</span></span><span style=display:flex><span>        encrypted: <span style=color:#00f>true</span>
</span></span></code></pre></div><blockquote><p>Note: The AWS extension does not support EBS volume (root & data volumes) encryption with <a href=https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk>customer managed CMK</a>. Support for <a href=https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk>customer managed CMK</a> is out of scope for now. Only <a href=https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk>AWS managed CMK</a> is supported.</p></blockquote><p>Additionally, it is possible to provide further AWS-specific values for configuring the worker pools.
It can be provided in <code>.spec.provider.workers[].providerConfig</code> and is evaluated by the AWS worker controller when it reconciles the shoot machines.</p><p>An example <code>WorkerConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: WorkerConfig
</span></span><span style=display:flex><span>volume:
</span></span><span style=display:flex><span>  iops: 10000
</span></span><span style=display:flex><span>  throughput: 200 
</span></span><span style=display:flex><span>dataVolumes:
</span></span><span style=display:flex><span>- name: kubelet-dir
</span></span><span style=display:flex><span>  iops: 12345
</span></span><span style=display:flex><span>  throughput: 150
</span></span><span style=display:flex><span>  snapshotID: snap-1234
</span></span><span style=display:flex><span>iamInstanceProfile: <span style=color:green># (specify either ARN or name)</span>
</span></span><span style=display:flex><span>  name: my-profile
</span></span><span style=display:flex><span>instanceMetadataOptions:
</span></span><span style=display:flex><span>  httpTokens: required
</span></span><span style=display:flex><span>  httpPutResponseHopLimit: 2
</span></span><span style=display:flex><span><span style=color:green># arn: my-instance-profile-arn</span>
</span></span><span style=display:flex><span>nodeTemplate: <span style=color:green># (to be specified only if the node capacity would be different from cloudprofile info during runtime)</span>
</span></span><span style=display:flex><span>  capacity:
</span></span><span style=display:flex><span>    cpu: 2
</span></span><span style=display:flex><span>    gpu: 0
</span></span><span style=display:flex><span>    memory: 50Gi
</span></span></code></pre></div><p>The <code>.volume.iops</code> is the number of I/O operations per second (IOPS) that the volume supports.
For <code>io1</code> and <code>gp3</code> volume type, this represents the number of IOPS that are provisioned for the volume.
For <code>gp2</code> volume type, this represents the baseline performance of the volume and the rate at which the volume accumulates I/O credits for bursting. For more information about General Purpose SSD baseline performance, I/O credits, IOPS range and bursting, see Amazon EBS Volume Types (<a href=http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html>http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html</a>) in the Amazon Elastic Compute Cloud User Guide.<br>Constraint: IOPS should be a positive value. Validation of IOPS (i.e. whether it is allowed and is in the specified range for a particular volume type) is done on aws side.</p><p>The <code>volume.throughput</code> is the throughput that the volume supports, in <code>MiB/s</code>. As of <code>16th Aug 2022</code>, this parameter is valid only for <code>gp3</code> volume types and will return an error from the provider side if specified for other volume types. Its current range of throughput is from <code>125MiB/s</code> to <code>1000 MiB/s</code>. To know more about throughput and its range, see the official AWS documentation <a href=http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html>here</a>.</p><p>The <code>.dataVolumes</code> can optionally contain configurations for the data volumes stated in the <code>Shoot</code> specification in the <code>.spec.provider.workers[].dataVolumes</code> list.
The <code>.name</code> must match to the name of the data volume in the shoot.
It is also possible to provide a snapshot ID. It allows to <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html>restore the data volume from an existing snapshot</a>.</p><p>The <code>iamInstanceProfile</code> section allows to specify the IAM instance profile name xor ARN that should be used for this worker pool.
If not specified, a dedicated IAM instance profile created by the infrastructure controller is used (see above).</p><p>The <code>instanceMetadataOptions</code> controls access to the instance metadata service (IMDS) for members of the worker. You can do the following operations:</p><ul><li>access IMDSv1 (default)</li><li>access IMDSv2 - <code>httpPutResponseHopLimit >= 2</code></li><li>access IMDSv2 only (restrict access to IMDSv1) - <code>httpPutResponseHopLimit >=2</code>, <code>httpTokens = "required"</code></li><li>disable access to IMDS - <code>httpTokens = "required"</code></li></ul><blockquote><p>Note: The accessibility of IMDS discussed in the previous point is referenced from the point of view of containers <strong>NOT</strong> running in the host network.
By default on host network IMDSv2 is already enabled (but not accessible from inside the pods).
It is currently not possible to create a VM with complete restriction to the IMDS service. It is however possible to restrict access from inside the pods by setting <code>httpTokens</code> to <code>required</code> and not setting <code>httpPutResponseHopLimit</code> (or setting it to 1).</p></blockquote><p>You can find more information regarding the options in the <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-IMDS-new-instances.html>AWS documentation</a>.</p><h2 id=example-shoot-manifest-one-availability-zone>Example <code>Shoot</code> manifest (one availability zone)</h2><p>Please find below an example <code>Shoot</code> manifest for one availability zone:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-aws
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: aws
</span></span><span style=display:flex><span>  region: eu-central-1
</span></span><span style=display:flex><span>  secretBindingName: core-aws
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        zones:
</span></span><span style=display:flex><span>        - name: eu-central-1a
</span></span><span style=display:flex><span>          internal: 10.250.112.0/22
</span></span><span style=display:flex><span>          public: 10.250.96.0/22
</span></span><span style=display:flex><span>          workers: 10.250.0.0/19
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: m5.large
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>    <span style=color:green># The following provider config is valid if the volume type is `io1`.</span>
</span></span><span style=display:flex><span>    <span style=color:green># providerConfig:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   kind: WorkerConfig</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   volume:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     iops: 10000</span>
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - eu-central-1a
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=example-shoot-manifest-three-availability-zones>Example <code>Shoot</code> manifest (three availability zones)</h2><p>Please find below an example <code>Shoot</code> manifest for three availability zones:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-aws
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: aws
</span></span><span style=display:flex><span>  region: eu-central-1
</span></span><span style=display:flex><span>  secretBindingName: core-aws
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        zones:
</span></span><span style=display:flex><span>        - name: eu-central-1a
</span></span><span style=display:flex><span>          workers: 10.250.0.0/26
</span></span><span style=display:flex><span>          public: 10.250.96.0/26
</span></span><span style=display:flex><span>          internal: 10.250.112.0/26
</span></span><span style=display:flex><span>        - name: eu-central-1b
</span></span><span style=display:flex><span>          workers: 10.250.0.64/26
</span></span><span style=display:flex><span>          public: 10.250.96.64/26
</span></span><span style=display:flex><span>          internal: 10.250.112.64/26
</span></span><span style=display:flex><span>        - name: eu-central-1c
</span></span><span style=display:flex><span>          workers: 10.250.0.128/26
</span></span><span style=display:flex><span>          public: 10.250.96.128/26
</span></span><span style=display:flex><span>          internal: 10.250.112.128/26
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: m5.large
</span></span><span style=display:flex><span>      minimum: 3
</span></span><span style=display:flex><span>      maximum: 9
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - eu-central-1a
</span></span><span style=display:flex><span>      - eu-central-1b
</span></span><span style=display:flex><span>      - eu-central-1c
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=csi-volume-provisioners>CSI volume provisioners</h2><p>Every AWS shoot cluster will be deployed with the AWS EBS CSI driver.
It is compatible with the legacy in-tree volume provisioner that was deprecated by the Kubernetes community and will be removed in future versions of Kubernetes.
End-users might want to update their custom <code>StorageClass</code>es to the new <code>ebs.csi.aws.com</code> provisioner.</p><h3 id=node-specific-volume-limits>Node-specific Volume Limits</h3><p>The Kubernetes scheduler allows configurable limit for the number of volumes that can be attached to a node. See <a href=https://k8s.io/docs/concepts/storage/storage-limits/#custom-limits>https://k8s.io/docs/concepts/storage/storage-limits/#custom-limits</a>.</p><p>CSI drivers usually have a different procedure for configuring this custom limit. By default, the EBS CSI driver parses the machine type name and then decides the volume limit. However, this is only a rough approximation and not good enough in most cases. Specifying the volume attach limit via command line flag (<code>--volume-attach-limit</code>) is currently the alternative until a more sophisticated solution presents itself (dynamically discovering the maximum number of attachable volume per EC2 machine type, see also <a href=https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/347>https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/347</a>). The AWS extension allows the <code>--volume-attach-limit</code> flag of the EBS CSI driver to be configurable via <code>aws.provider.extensions.gardener.cloud/volume-attach-limit</code> annotation on the <code>Shoot</code> resource. If the annotation is added to an existing <code>Shoot</code>, then reconciliation needs to be triggered manually (see <a href=/docs/gardener/shoot_operations/#immediate-reconciliation>Immediate reconciliation</a>), as in general adding annotation to resource is not a change that leads to <code>.metadata.generation</code> increase in general.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-aws@v1.34</code>.</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> and <code>ShootSARotation</code> feature gates since <code>gardener-extension-provider-aws@v1.36</code>.</p><h2 id=flow-infrastructure-reconciler>Flow Infrastructure Reconciler</h2><p>The extension offers two different reconciler implementations for the infrastructure resource:</p><ul><li>terraform-based</li><li>native Go SDK based (dubbed the &ldquo;flow&rdquo;-based implementation)</li></ul><p>The default implementation currently is the terraform reconciler which uses the <code>https://github.com/gardener/terraformer</code> as the backend for managing the shoot&rsquo;s infrastructure.</p><p>The &ldquo;flow&rdquo; implementation is a newer implementation that is trying to solve issues we faced with managing terraform infrastructure on Kubernetes. The goal is to have more control over the reconciliation process and be able to perform fine-grained tuning over it. The implementation is completely backwards-compatible and offers a migration route from the legacy terraformer implementation.</p><p>For most users there will be no noticable difference. However for certain use-cases, users may notice a slight deviation from the previous behavior. For example, with flow-based infrastructure users may be able to perform certain modifications to infrastructure resources without having them reconciled back by terraform. Operations that would degrade the shoot infrastructure are still expected to be reverted back.</p><p>For the time-being, to take advantage of the flow reconcilier users have to &ldquo;opt-in&rdquo; by annotating the shoot manifest with: <code>aws.provider.extensions.gardener.cloud/use-flow="true"</code>. For existing shoots with this annotation, the migration will take place on the next infrastructure reconciliation (on maintenance window or if other infrastructure changes are requested). The migration is not revertible.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-327ad4a2eb9d221f7e8f6cc268e1fee3>1.3 - Provider Azure</h1><div class=lead>Gardener extension controller for the Azure cloud provider</div><h1 id=gardener-extension-for-azure-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Azure provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener-tests/pipelines/gardener-extension-provider-azure-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener-tests/pipelines/gardener-extension-provider-azure-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-azure><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-azure alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the Azure provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-azure/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.28</td><td>1.28.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.27</td><td>1.27.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.27%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.27%20Azure/tests_status?style=svg" alt="Gardener v1.27 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.26</td><td>1.26.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.26%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.26%20Azure/tests_status?style=svg" alt="Gardener v1.26 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.25</td><td>1.25.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20Azure/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.24</td><td>1.24.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20Azure><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20Azure/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-azure/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-5e517440813fb72278dc8f9748dee579>1.3.1 - Tutorials</h1></div><div class=td-content><h1 id=pg-3c4c1c240f4879d50d22eaff0e08ebbe>1.3.1.1 - Create a Kubernetes Cluster on Azure with Gardener</h1><h3 id=overview>Overview</h3><p>Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on Azure.</p><h3 id=prerequisites>Prerequisites</h3><ul><li>You have created an <a href=https://azure.microsoft.com/en-us/>Azure account</a>.</li><li>You have access to the Gardener dashboard and have permissions to create projects.</li><li>You have an Azure Service Principal assigned to your subscription.</li></ul><h3 id=steps>Steps</h3><ol><li><p>Go to the Gardener dashboard and create a <em>Project</em>.</p><img src=/__resources/new-gardener-project_524827.png></li><li><p>Get the properties of your Azure AD tenant, Subscription and Service Principal.</p><p>Before you can provision and access a Kubernetes cluster on Azure, you need to add the Azure service principal, AD tenant and subscription credentials in Gardener.
Gardener needs the credentials to provision and operate the Azure infrastructure for your Kubernetes cluster.</p><p><strong>Ensure that the Azure service principal has the actions defined within the <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/azure-permissions/>Azure Permissions</a> within your Subscription assigned.
If no fine-grained permission/actions are required, then simply the built-in <code>Contributor</code> role can be assigned.</strong></p><ul><li><p>Tenant ID</p><p>To find your <code>TenantID</code>, follow this <a href=https://learn.microsoft.com/en-us/azure/active-directory/fundamentals/how-to-find-tenant>guide</a>.</p></li><li><p>SubscriptionID</p><p>To find your <code>SubscriptionID</code>, search for and select <em>Subscriptions</em>.
<img src=/__resources/azure-select-subscription_e138b6.png></p><p>After that, copy the <code>SubscriptionID</code> from your subscription of choice.
<img src=/__resources/azure-choose-subscription_d79d28.png></p></li><li><p>Service Principal (SPN)</p><p>A service principal consist of a <code>ClientID</code> (also called <code>ApplicationID</code>) and a Client Secret. For more information, see <a href=https://docs.microsoft.com/en-us/azure/active-directory/develop/app-objects-and-service-principals>Application and service principal objects in Azure Active Directory</a>. You need to obtain the:</p><ul><li><p>Client ID</p><p>Access the <a href=https://portal.azure.com>Azure Portal</a> and navigate to the <em>Active Directory</em> service.
Within the service navigate to <em>App registrations</em> and select your service principal. Copy the <code>ClientID</code> you see there.</p></li><li><p>Client Secret</p><p>Secrets for the Azure Account/Service Principal can be generated/rotated via the Azure Portal.
After copying your <code>ClientID</code>, in the <em>Detail</em> view of your Service Principal navigate to <em>Certificates & secrets</em>. In the section, you can generate a new secret.</p></li></ul></li></ul></li><li><p>Choose <em>Secrets</em>, then the plus icon <img src=/__resources/plus-icon_df44c3.png> and select <em>Azure</em>.</p><img src=/__resources/create-secret-azure_cd3c81.png></li><li><p>Create your secret.</p><ol><li>Type the name of your secret.</li><li>Copy and paste the <code>TenantID</code>, <code>SubscriptionID</code> and the Service Principal credentials (<code>ClientID</code> and <code>ClientSecret</code>).</li><li>Choose <em>Add secret</em>.
<img src=/__resources/add-azure-secret_d9b7cf.png></li></ol><blockquote><p>After completing these steps, you should see your newly created secret in the <em>Infrastructure Secrets</em> section.</p></blockquote><img src=/__resources/secret-stored_6863bb.png></li><li><p>Register resource providers for your subscription.</p><ol><li>Go to your Azure dashboard</li><li>Navigate to <em>Subscriptions</em> -> &lt;your_subscription></li><li>Pick resource providers from the sidebar</li><li>Register microsoft.Network</li><li>Register microsoft.Compute</li></ol></li><li><p>To create a new cluster, choose <em>Clusters</em> and then the plus sign in the upper right corner.</p><img src=/__resources/new-cluster_88ec0e.png></li><li><p>In the <em>Create Cluster</em> section:</p><ol><li>Select <em>Azure</em> in the <em>Infrastructure</em> tab.</li><li>Type the name of your cluster in the <em>Cluster Details</em> tab.</li><li>Choose the secret you created before in the <em>Infrastructure Details</em> tab.</li><li>Choose <em>Create</em>.</li></ol><img src=/__resources/create-cluster_55c4a1.png></li><li><p>Wait for your cluster to get created.</p><img src=/__resources/processing-cluster_19a7da.png></li></ol><h3 id=result>Result</h3><p>After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster.</p><img src=/__resources/copy-kubeconfig_9889da.png></div><div class=td-content style=page-break-before:always><h1 id=pg-202c2224b11eca2eaad739b137e4db73>1.3.2 - Azure Permissions</h1><h1 id=azure-permissions>Azure Permissions</h1><p>The following document describes the required Azure actions manage a Shoot cluster on Azure split by the different Azure provider/services.</p><p>Be aware some actions are just required if particilar deployment sceanrios or features e.g. bring your own vNet, use Azure-file, let the Shoot act as Seed etc. should be used.</p><h2 id=microsoftcompute><code>Microsoft.Compute</code></h2><pre tabindex=0><code># Required if a non zonal cluster based on Availability Set should be used.
Microsoft.Compute/availabilitySets/delete
Microsoft.Compute/availabilitySets/read
Microsoft.Compute/availabilitySets/write

# Required to let Kubernetes manage Azure disks.
Microsoft.Compute/disks/delete
Microsoft.Compute/disks/read
Microsoft.Compute/disks/write

# Required for to fetch meta information about disk and virtual machines sizes.
Microsoft.Compute/locations/diskOperations/read
Microsoft.Compute/locations/operations/read
Microsoft.Compute/locations/vmSizes/read

# Required if csi snapshot capabilities should be used and/or the Shoot should act as a Seed.
Microsoft.Compute/snapshots/delete
Microsoft.Compute/snapshots/read
Microsoft.Compute/snapshots/write

# Required to let Gardener/Machine-Controller-Manager manage the cluster nodes/machines.
Microsoft.Compute/virtualMachines/delete
Microsoft.Compute/virtualMachines/read
Microsoft.Compute/virtualMachines/start/action
Microsoft.Compute/virtualMachines/write

# Required if a non zonal cluster based on VMSS Flex (VMO) should be used.
Microsoft.Compute/virtualMachineScaleSets/delete
Microsoft.Compute/virtualMachineScaleSets/read
Microsoft.Compute/virtualMachineScaleSets/write
</code></pre><h2 id=microsoftmanagedidentity><code>Microsoft.ManagedIdentity</code></h2><pre tabindex=0><code># Required if a user provided Azure managed identity should attached to the cluster nodes.
Microsoft.ManagedIdentity/userAssignedIdentities/assign/action
Microsoft.ManagedIdentity/userAssignedIdentities/read
</code></pre><h2 id=microsoftmarketplaceordering><code>Microsoft.MarketplaceOrdering</code></h2><pre tabindex=0><code># Required if nodes/machines should be created with images hosted on the Azure Marketplace.
Microsoft.MarketplaceOrdering/offertypes/publishers/offers/plans/agreements/read
Microsoft.MarketplaceOrdering/offertypes/publishers/offers/plans/agreements/write
</code></pre><h2 id=microsoftnetwork><code>Microsoft.Network</code></h2><pre tabindex=0><code># Required to let Kubernetes manage services of type &#39;LoadBalancer&#39;.
Microsoft.Network/loadBalancers/backendAddressPools/join/action
Microsoft.Network/loadBalancers/delete
Microsoft.Network/loadBalancers/read
Microsoft.Network/loadBalancers/write

# Required in case the Shoot should use NatGateway(s).
Microsoft.Network/natGateways/delete
Microsoft.Network/natGateways/join/action
Microsoft.Network/natGateways/read
Microsoft.Network/natGateways/write

# Required to let Gardener/Machine-Controller-Manager manage the cluster nodes/machines.
Microsoft.Network/networkInterfaces/delete
Microsoft.Network/networkInterfaces/ipconfigurations/join/action
Microsoft.Network/networkInterfaces/ipconfigurations/read
Microsoft.Network/networkInterfaces/join/action
Microsoft.Network/networkInterfaces/read
Microsoft.Network/networkInterfaces/write

# Required to let Gardener maintain the basic infrastructure of the Shoot cluster and maintaing LoadBalancer services.
Microsoft.Network/networkSecurityGroups/delete
Microsoft.Network/networkSecurityGroups/join/action
Microsoft.Network/networkSecurityGroups/read
Microsoft.Network/networkSecurityGroups/write

# Required for managing LoadBalancers and NatGateways.
Microsoft.Network/publicIPAddresses/delete
Microsoft.Network/publicIPAddresses/join/action
Microsoft.Network/publicIPAddresses/read
Microsoft.Network/publicIPAddresses/write

# Required for managing the basic infrastructure of a cluster and maintaing LoadBalancer services.
Microsoft.Network/routeTables/delete
Microsoft.Network/routeTables/join/action
Microsoft.Network/routeTables/read
Microsoft.Network/routeTables/routes/delete
Microsoft.Network/routeTables/routes/read
Microsoft.Network/routeTables/routes/write
Microsoft.Network/routeTables/write

# Required to let Gardener maintain the basic infrastructure of the Shoot cluster.
# Only a subset is required for the bring your own vNet scenario.
Microsoft.Network/virtualNetworks/delete # not required for bring your own vnet
Microsoft.Network/virtualNetworks/read
Microsoft.Network/virtualNetworks/subnets/delete
Microsoft.Network/virtualNetworks/subnets/join/action
Microsoft.Network/virtualNetworks/subnets/read
Microsoft.Network/virtualNetworks/subnets/write
Microsoft.Network/virtualNetworks/write # not required for bring your own vnet
</code></pre><h2 id=microsoftresources><code>Microsoft.Resources</code></h2><pre tabindex=0><code># Required to let Gardener maintain the basic infrastructure of the Shoot cluster.
Microsoft.Resources/subscriptions/resourceGroups/delete
Microsoft.Resources/subscriptions/resourceGroups/read
Microsoft.Resources/subscriptions/resourceGroups/write
</code></pre><h2 id=microsoftstorage><code>Microsoft.Storage</code></h2><pre tabindex=0><code># Required if Azure File should be used and/or if the Shoot should act as Seed.
Microsoft.Storage/operations/read
Microsoft.Storage/storageAccounts/blobServices/containers/delete
Microsoft.Storage/storageAccounts/blobServices/containers/read
Microsoft.Storage/storageAccounts/blobServices/containers/write
Microsoft.Storage/storageAccounts/blobServices/read
Microsoft.Storage/storageAccounts/delete
Microsoft.Storage/storageAccounts/listkeys/action
Microsoft.Storage/storageAccounts/read
Microsoft.Storage/storageAccounts/write
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-6e41468eb3f4f7e8baad8ac7246ce13a>1.3.3 - Deployment</h1><h1 id=deployment-of-the-azure-provider-extension>Deployment of the Azure provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the Azure provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the Azure provider extension <a href=https://github.com/gardener/gardener-extension-provider-azure>repository</a>.</p><h2 id=gardener-extension-admission-azure>gardener-extension-admission-azure</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-azure</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-c8c8cf7fad861c906c48766068d4d640>1.3.4 - Local Setup</h1><h3 id=admission-azure>admission-azure</h3><p><code>admission-azure</code> is an admission webhook server which is responsible for the validation of the cloud provider (Azure in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-admission
</span></span></code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-azure.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/dev-setup-admission-azure.sh
</span></span></code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-azure</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7034bf1e209bae31626e5792b4e596ff>1.3.5 - Migrate Loadbalancer</h1><h1 id=migrate-azure-shoot-load-balancer-from-basic-to-standard-sku>Migrate Azure Shoot Load Balancer from basic to standard SKU</h1><p>This guide descibes how to migrate the Load Balancer of an Azure Shoot cluster from the basic SKU to the standard SKU.<br><strong>Be aware:</strong> You need to delete and recreate all services of type Load Balancer, which means that the public ip addresses of your service endpoints will change.<br>Please do this only if the Stakeholder really needs to migrate this Shoot to use standard Load Balancers. All new Shoot clusters will automatically use Azure Standard Load Balancers.</p><ol><li>Disable temporarily Gardeners reconciliation.<br>The Gardener Controller Manager need to be configured to allow ignoring Shoot clusters.
This can be configured in its the <code>ControllerManagerConfiguration</code> via the field <code>.controllers.shoot.respectSyncPeriodOverwrite="true"</code>.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># In the Garden cluster.</span>
</span></span><span style=display:flex><span>kubectl annotate shoot &lt;shoot-name&gt; shoot.garden.sapcloud.io/ignore=<span style=color:#a31515>&#34;true&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># In the Seed cluster.</span>
</span></span><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; scale deployment gardener-resource-manager --replicas=0
</span></span></code></pre></div><ol start=2><li>Backup all Kubernetes services of type Load Balancer.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># In the Shoot cluster.</span>
</span></span><span style=display:flex><span><span style=color:green># Determine all Load Balancer services.</span>
</span></span><span style=display:flex><span>kubectl get service --all-namespaces | grep LoadBalancer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Backup each Load Balancer service.</span>
</span></span><span style=display:flex><span>echo <span style=color:#a31515>&#34;---&#34;</span> &gt;&gt; service-backup.yaml &amp;&amp; kubectl -n &lt;namespace&gt; get service &lt;service-name&gt; -o yaml &gt;&gt; service-backup.yaml
</span></span></code></pre></div><ol start=3><li>Delete all Load Balancer services.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># In the Shoot cluster.</span>
</span></span><span style=display:flex><span>kubectl -n &lt;namespace&gt; delete service &lt;service-name&gt;
</span></span></code></pre></div><ol start=4><li>Wait until until Load Balancer is deleted.
Wait until all services of type Load Balancer are deleted and the Azure Load Balancer resource is also deleted.
Check via the Azure Portal if the Load Balancer within the Shoot Resource Group has been deleted.
This should happen automatically after all Kubernetes Load Balancer service are gone within a few minutes.</li></ol><p>Alternatively the Azure cli can be used to check the Load Balancer in the Shoot Resource Group.
The credentials to configure the cli are available on the Seed cluster in the Shoot namespace.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># In the Seed cluster.</span>
</span></span><span style=display:flex><span><span style=color:green># Fetch the credentials from cloudprovider secret.</span>
</span></span><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; get secret cloudprovider -o yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Configure the Azure cli, with the base64 decoded values of the cloudprovider secret.</span>
</span></span><span style=display:flex><span>az login --service-principal --username &lt;clientID&gt; --password &lt;clientSecret&gt; --tenant &lt;tenantID&gt;
</span></span><span style=display:flex><span>az account set -s &lt;subscriptionID&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Fetch the constantly the Shoot Load Balancer in the Shoot Resource Group. Wait until the resource is gone.</span>
</span></span><span style=display:flex><span>watch <span style=color:#a31515>&#39;az network lb show -g shoot--&lt;project-name&gt;--&lt;shoot-name&gt; -n shoot--&lt;project-name&gt;--&lt;shoot-name&gt;&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># Logout.</span>
</span></span><span style=display:flex><span>az logout
</span></span></code></pre></div><ol start=5><li>Modify the <code>cloud-povider-config</code> configmap in the Seed namespace of the Shoot.<br>The key <code>cloudprovider.conf</code> contains the Kubernetes cloud-provider configuration.
The value is a multiline string. Please change the value of the field <code>loadBalancerSku</code> from <code>basic</code> to <code>standard</code>.
Iff the field does not exists then append <code>loadBalancerSku: \"standard\"\n</code> to the value/string.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># In the Seed cluster.</span>
</span></span><span style=display:flex><span>kubectl -n &lt;shoot-namespace&gt; edit cm cloud-provider-config
</span></span></code></pre></div><ol start=6><li>Enable Gardeners reconcilation and trigger a reconciliation.</li></ol><pre tabindex=0><code># In the Garden cluster
# Enable reconcilation
kubectl annotate shoot &lt;shoot-name&gt; shoot.garden.sapcloud.io/ignore-

# Trigger reconcilation
kubectl annotate shoot &lt;shoot-name&gt; shoot.garden.sapcloud.io/operation=&#34;reconcile&#34;
</code></pre><p>Wait until the cluster has been reconciled.</p><ol start=6><li>Recreate the services from the backup file.<br>Probably you need to remove some fields from the service defintions e.g. <code>.spec.clusterIP</code>, <code>.metadata.uid</code> or <code>.status</code> etc.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl apply -f service-backup.yaml
</span></span></code></pre></div><ol start=7><li>If successful remove backup file.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:green># Delete the backup file.</span>
</span></span><span style=display:flex><span>rm -f service-backup.yaml
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-a9331c43e98d69a160078331d2ea1788>1.3.6 - Operations</h1><h1 id=using-the-azure-provider-extension-with-gardener-as-an-operator>Using the Azure provider extension with Gardener as an operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.
The <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>core.gardener.cloud/v1beta1.Seed</code> resource</a> is structured similarly.
Additionally, it allows configuring settings for the backups of the main etcds&rsquo; data of shoot clusters control planes running in this seed cluster.</p><p>This document explains the necessary configuration for the Azure provider extension.</p><h2 id=cloudprofile-resource><code>CloudProfile</code> resource</h2><p>This section describes, how the configuration for <code>CloudProfile</code>s looks like for Azure by providing an example <code>CloudProfile</code> manifest with minimal configuration that can be used to allow the creation of Azure shoot clusters.</p><h3 id=cloudprofileconfig><code>CloudProfileConfig</code></h3><p>The cloud profile configuration contains information about the real machine image IDs in the Azure environment (image <code>urn</code>, <code>id</code>, <code>communityGalleryImageID</code> or <code>sharedGalleryImageID</code>).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> to an available VM image in your subscription.
The VM image can be either from the <a href="https://azuremarketplace.microsoft.com/en-us/marketplace/apps?filters=virtual-machine-images">Azure Marketplace</a> and will then get identified via a <code>urn</code>, it can be a custom VM image from a shared image gallery and is then identified <code>sharedGalleryImageID</code>, or it can be from a community image gallery and is then identified by its <code>communityGalleryImageID</code>. You can use <code>id</code> field also to specifiy the image location in the azure compute gallery (in which case it would have a different kind of path) but it is not recommended as it sometimes faces problems in cross subscription image sharing.
For each machine image version an <code>architecture</code> field can be specified which specifies the CPU architecture of the machine on which given machine image can be used.</p><p>An example <code>CloudProfileConfig</code> for the Azure extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>countUpdateDomains:
</span></span><span style=display:flex><span>- region: westeurope
</span></span><span style=display:flex><span>  count: 5
</span></span><span style=display:flex><span>countFaultDomains:
</span></span><span style=display:flex><span>- region: westeurope
</span></span><span style=display:flex><span>  count: 3
</span></span><span style=display:flex><span>machineTypes:
</span></span><span style=display:flex><span>- name: Standard_D3_v2
</span></span><span style=display:flex><span>  acceleratedNetworking: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>- name: Standard_X
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: coreos
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 2135.6.0
</span></span><span style=display:flex><span>    urn: <span style=color:#a31515>&#34;CoreOS:CoreOS:Stable:2135.6.0&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green># architecture: amd64 # optional</span>
</span></span><span style=display:flex><span>    acceleratedNetworking: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>- name: myimage
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 1.0.0
</span></span><span style=display:flex><span>    id: <span style=color:#a31515>&#34;/subscriptions/&lt;subscription ID where the gallery is located&gt;/resourceGroups/myGalleryRG/providers/Microsoft.Compute/galleries/myGallery/images/myImageDefinition/versions/1.0.0&#34;</span>
</span></span><span style=display:flex><span>- name: GardenLinuxCommunityImage
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 1.0.0
</span></span><span style=display:flex><span>    communityGalleryImageID: <span style=color:#a31515>&#34;/CommunityGalleries/gardenlinux-567905d8-921f-4a85-b423-1fbf4e249d90/Images/gardenlinux/Versions/576.1.1&#34;</span>
</span></span><span style=display:flex><span>- name: SharedGalleryImageName
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>    - version: 1.0.0
</span></span><span style=display:flex><span>      sharedGalleryImageID: <span style=color:#a31515>&#34;/SharedGalleries/sharedGalleryName/Images/sharedGalleryImageName/Versions/sharedGalleryImageVersionName&#34;</span>
</span></span></code></pre></div><p>The cloud profile configuration contains information about the update via <code>.countUpdateDomains[]</code> and failure domain via <code>.countFaultDomains[]</code> counts in the Azure regions you want to offer.</p><p>The <code>.machineTypes[]</code> list contain provider specific information to the machine types e.g. if the machine type support <a href=https://docs.microsoft.com/en-us/azure/virtual-network/create-vm-accelerated-networking-cli>Azure Accelerated Networking</a>, see <code>.machineTypes[].acceleratedNetworking</code>.</p><p>Additionally, it contains the real machine image identifiers in the Azure environment. You can provide either URN for Azure Market Place images or id of <a href=https://docs.microsoft.com/en-us/azure/virtual-machines/linux/shared-image-galleries>Shared Image Gallery</a> images.
When Shared Image Gallery is used, you have to ensure that the image is available in the desired regions and the end-user subscriptions have access to the image or to the whole gallery.
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the Azure extension knows the machine image identifiers for every version you want to offer.
Furthermore, you can specify for each image version via <code>.machineImages[].versions[].acceleratedNetworking</code> if Azure Accelerated Networking is supported.</p><h3 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h3><p>The possible values for <code>.spec.volumeTypes[].name</code> on Azure are <code>Standard_LRS</code>, <code>StandardSSD_LRS</code> and <code>Premium_LRS</code>. There is another volume type called <code>UltraSSD_LRS</code> but this type is not supported to use as os disk. If an end user select a volume type whose name is not equal to one of the valid values then the machine will be created with the default volume type which belong to the selected machine type. Therefore it is recommended to configure only the valid values for the <code>.spec.volumeType[].name</code> in the <code>CloudProfile</code>.</p><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: azure
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: azure
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.24.3
</span></span><span style=display:flex><span>    - version: 1.23.8
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2022-10-31T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2135.6.0
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: Standard_D3_v2
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;4&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 14Gi
</span></span><span style=display:flex><span>  - name: Standard_D4_v3
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;4&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 16Gi
</span></span><span style=display:flex><span>  volumeTypes:
</span></span><span style=display:flex><span>  - name: Standard_LRS
</span></span><span style=display:flex><span>    class: standard
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  - name: StandardSSD_LRS
</span></span><span style=display:flex><span>    class: premium
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  - name: Premium_LRS
</span></span><span style=display:flex><span>    class: premium
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - name: westeurope
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    machineTypes:
</span></span><span style=display:flex><span>    - name: Standard_D3_v2
</span></span><span style=display:flex><span>      acceleratedNetworking: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    - name: Standard_D4_v3
</span></span><span style=display:flex><span>    countUpdateDomains:
</span></span><span style=display:flex><span>    - region: westeurope
</span></span><span style=display:flex><span>      count: 5
</span></span><span style=display:flex><span>    countFaultDomains:
</span></span><span style=display:flex><span>    - region: westeurope
</span></span><span style=display:flex><span>      count: 3
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: coreos
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 2303.3.0
</span></span><span style=display:flex><span>        urn: CoreOS:CoreOS:Stable:2303.3.0
</span></span><span style=display:flex><span>        <span style=color:green># architecture: amd64 # optional</span>
</span></span><span style=display:flex><span>        acceleratedNetworking: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      - version: 2135.6.0
</span></span><span style=display:flex><span>        urn: <span style=color:#a31515>&#34;CoreOS:CoreOS:Stable:2135.6.0&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:green># architecture: amd64 # optional</span>
</span></span></code></pre></div><h2 id=seed-resource><code>Seed</code> resource</h2><p>This provider extension does not support any provider configuration for the <code>Seed</code>&rsquo;s <code>.spec.provider.providerConfig</code> field.
However, it supports managing of backup infrastructure, i.e., you can specify a configuration for the <code>.spec.backup</code> field.</p><h3 id=backup-configuration>Backup configuration</h3><p>A Seed of type <code>azure</code> can be configured to perform backups for the main etcds&rsquo; of the shoot clusters control planes using Azure Blob storage.</p><p>The location/region where the backups will be stored defaults to the region of the Seed (<code>spec.provider.region</code>), but can also be explicitly configured via the field <code>spec.backup.region</code>.
The region of the backup can be different from where the Seed cluster is running.
However, usually it makes sense to pick the same region for the backup bucket as used for the Seed cluster.</p><p>Please find below an example <code>Seed</code> manifest (partly) that configures backups using Azure Blob storage.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-seed
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: azure
</span></span><span style=display:flex><span>    region: westeurope
</span></span><span style=display:flex><span>  backup:
</span></span><span style=display:flex><span>    provider: azure
</span></span><span style=display:flex><span>    region: westeurope <span style=color:green># default region</span>
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: backup-credentials
</span></span><span style=display:flex><span>      namespace: garden
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>The referenced secret has to contain the provider credentials of the Azure subscription.
Please take a look <a href=https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal>here</a> on how to create an Azure Application, Service Principle and how to obtain credentials.
The example below demonstrates how the secret has to look like.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-azure
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  clientID: base64(client-id)
</span></span><span style=display:flex><span>  clientSecret: base64(client-secret)
</span></span><span style=display:flex><span>  subscriptionID: base64(subscription-id)
</span></span><span style=display:flex><span>  tenantID: base64(tenant-id)
</span></span></code></pre></div><h4 id=permissions-for-azure-blob-storage>Permissions for Azure Blob storage</h4><p>Please make sure the Azure application has the following IAM roles.</p><ul><li><a href=https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#contributor>Contributor</a></li></ul><h2 id=miscellaneous>Miscellaneous</h2><h3 id=gardener-managed-service-principals>Gardener managed Service Principals</h3><p>The operators of the Gardener Azure extension can provide a list of managed service principals (technical users) that can be used for Azure Shoots.
This eliminates the need for users to provide own service principals for their clusters.</p><p>The user would need to grant the managed service principal access to their subscription with proper permissions.</p><p>As service principals are managed in an Azure Active Directory for each supported Active Directory, an own service principal needs to be provided.</p><p>In case the user provides an own service principal in the Shoot secret, this one will be used instead of the managed one provided by the operator.</p><p>Each managed service principal will be maintained in a <code>Secret</code> like that:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: service-principal-my-tenant
</span></span><span style=display:flex><span>  namespace: extension-provider-azure
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    azure.provider.extensions.gardener.cloud/purpose: tenant-service-principal-secret
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  tenantID: base64(my-tenant)
</span></span><span style=display:flex><span>  clientID: base64(my-service-princiapl-id)
</span></span><span style=display:flex><span>  clientSecret: base64(my-service-princiapl-secret)
</span></span><span style=display:flex><span>type: Opaque
</span></span></code></pre></div><p>The user needs to provide in its Shoot secret a <code>tenantID</code> and <code>subscriptionID</code>.</p><p>The managed service principal will be assigned based on the <code>tenantID</code>.
In case there is a managed service principal secret with a matching <code>tenantID</code>, this one will be used for the Shoot.
If there is no matching managed service principal secret then the next Shoot operation will fail.</p><p>One of the benefits of having managed service principals is that the operator controls the lifecycle of the service principal and can rotate its secrets.</p><p>After the service principal secret has been rotated and the corresponding secret is updated, all Shoot clusters using it need to be reconciled or the last operation to be retried.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-600730bb439495112133b52f5c68b64d>1.3.7 - Usage</h1><h1 id=using-the-azure-provider-extension-with-gardener-as-end-user>Using the Azure provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>This document describes the configurable options for Azure and provides an example <code>Shoot</code> manifest with minimal configuration that can be used to create an Azure cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=azure-provider-credentials>Azure Provider Credentials</h2><p>In order for Gardener to create a Kubernetes cluster using Azure infrastructure components, a Shoot has to provide credentials with sufficient permissions to the desired Azure subscription.
Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of the Azure subscription.
The <code>SecretBinding</code> is configurable in the <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>Shoot cluster</a> with the field <code>secretBindingName</code>.</p><p>Create an <a href=https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal>Azure Application and Service Principle</a> and obtain its credentials.</p><p>Please ensure that the Azure application (spn) has the IAM actions defined <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-azure/azure-permissions/>here</a> assigned.
If no fine-grained permissions/actions required then simply assign the <a href=https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#contributor>Contributor</a> role.</p><p>The example below demonstrates how the secret containing the client credentials of the Azure Application has to look like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-azure
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  clientID: base64(client-id)
</span></span><span style=display:flex><span>  clientSecret: base64(client-secret)
</span></span><span style=display:flex><span>  subscriptionID: base64(subscription-id)
</span></span><span style=display:flex><span>  tenantID: base64(tenant-id)
</span></span></code></pre></div><p>⚠️ Depending on your API usage it can be problematic to reuse the same Service Principal for different Shoot clusters due to rate limits.
Please consider spreading your Shoots over Service Principals from different Azure subscriptions if you are hitting those limits.</p><h3 id=managed-service-principals>Managed Service Principals</h3><p>The operators of the Gardener Azure extension can provide managed service principals.
This eliminates the need for users to provide an own service principal for a Shoot.</p><p>To make use of a managed service principal, the Azure secret of a Shoot cluster must contain only a <code>subscriptionID</code> and a <code>tenantID</code> field, but no <code>clientID</code> and <code>clientSecret</code>.
Removing those fields from the secret of an existing Shoot will also let it adopt the managed service principal.</p><p>Based on the <code>tenantID</code> field, the Gardener extension will try to assign the managed service principal to the Shoot.
If no managed service principal can be assigned then the next operation on the Shoot will fail.</p><p>⚠️ The managed service principal need to be assigned to the users Azure subscription with proper permissions before using it.</p><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the Azure extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span><span style=display:flex><span>networks:
</span></span><span style=display:flex><span>  vnet: <span style=color:green># specify either &#39;name&#39; and &#39;resourceGroup&#39; or &#39;cidr&#39;</span>
</span></span><span style=display:flex><span>    <span style=color:green># name: my-vnet</span>
</span></span><span style=display:flex><span>    <span style=color:green># resourceGroup: my-vnet-resource-group</span>
</span></span><span style=display:flex><span>    cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>    <span style=color:green># ddosProtectionPlanID: /subscriptions/test/resourceGroups/test/providers/Microsoft.Network/ddosProtectionPlans/test-ddos-protection-plan</span>
</span></span><span style=display:flex><span>  workers: 10.250.0.0/19
</span></span><span style=display:flex><span>  <span style=color:green># natGateway:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   enabled: false</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   idleConnectionTimeoutMinutes: 4</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   zone: 1</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   ipAddresses:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   - name: my-public-ip-name</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     resourceGroup: my-public-ip-resource-group</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     zone: 1</span>
</span></span><span style=display:flex><span>  <span style=color:green># serviceEndpoints:</span>
</span></span><span style=display:flex><span>  <span style=color:green># - Microsoft.Test</span>
</span></span><span style=display:flex><span>  <span style=color:green># zones:</span>
</span></span><span style=display:flex><span>  <span style=color:green># - name: 1</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   cidr: &#34;10.250.0.0/24</span>
</span></span><span style=display:flex><span>  <span style=color:green># - name: 2</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   cidr: &#34;10.250.0.0/24&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   natGateway:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#     enabled: false</span>
</span></span><span style=display:flex><span>zoned: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span><span style=color:green># resourceGroup:</span>
</span></span><span style=display:flex><span><span style=color:green>#   name: mygroup</span>
</span></span><span style=display:flex><span><span style=color:green>#identity:</span>
</span></span><span style=display:flex><span><span style=color:green>#  name: my-identity-name</span>
</span></span><span style=display:flex><span><span style=color:green>#  resourceGroup: my-identity-resource-group</span>
</span></span><span style=display:flex><span><span style=color:green>#  acrAccess: true</span>
</span></span></code></pre></div><p>Currently, it&rsquo;s not yet possible to deploy into existing resource groups, but in the future it will.
The <code>.resourceGroup.name</code> field will allow specifying the name of an already existing resource group that the shoot cluster and all infrastructure resources will be deployed to.</p><p>Via the <code>.zoned</code> boolean you can tell whether you want to use Azure availability zones or not.
If you don&rsquo;t use zones then an availability set will be created and only basic load balancers will be used.
Zoned clusters use standard load balancers.</p><p>The <code>networks.vnet</code> section describes whether you want to create the shoot cluster in an already existing VNet or whether to create a new one:</p><ul><li>If <code>networks.vnet.name</code> and <code>networks.vnet.resourceGroup</code> are given then you have to specify the VNet name and VNet resource group name of the existing VNet that was created by other means (manually, other tooling, &mldr;).</li><li>If <code>networks.vnet.cidr</code> is given then you have to specify the VNet CIDR of a new VNet that will be created during shoot creation.
You can freely choose a private CIDR range.</li><li>Either <code>networks.vnet.name</code> and <code>neworks.vnet.resourceGroup</code> or <code>networks.vnet.cidr</code> must be present, but not both at the same time.</li><li>The <code>networks.vnet.ddosProtectionPlanID</code> field can be used to specify the id of a ddos protection plan which should be assigned to the VNet. This will only work for a VNet managed by Gardener. For externally managed VNets the ddos protection plan must be assigned by other means.</li><li>If a vnet name is given and cilium shoot clusters are created without a network overlay within one vnet make sure that the pod CIDR specified in <code>shoot.spec.networking.pods</code> is not overlapping with any other pod CIDR used in that vnet.
Overlapping pod CIDRs will lead to disfunctional shoot clusters.</li></ul><p>The <code>networks.workers</code> section describes the CIDR for a subnet that is used for all shoot worker nodes, i.e., VMs which later run your applications.
The specified CIDR range must be contained in the VNet CIDR specified above, or the VNet CIDR of your already existing VNet.
You can freely choose this CIDR and it is your responsibility to properly design the network layout to suit your needs.</p><p>In the <code>networks.serviceEndpoints[]</code> list you can specify the list of Azure service endpoints which shall be associated with the worker subnet. All available service endpoints and their technical names can be found in the (Azure Service Endpoint documentation](<a href=https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-service-endpoints-overview>https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-service-endpoints-overview</a>).</p><p>The <code>networks.natGateway</code> section contains configuration for the Azure NatGateway which can be attached to the worker subnet of a Shoot cluster. Here are some key information about the usage of the NatGateway for a Shoot cluster:</p><ul><li>NatGateway usage is optional and can be enabled or disabled via <code>.networks.natGateway.enabled</code>.</li><li>If the NatGateway is not used then the egress connections initiated within the Shoot cluster will be nated via the LoadBalancer of the clusters (default Azure behaviour, see <a href=https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections#scenarios>here</a>).</li><li>NatGateway is only available for zonal clusters <code>.zoned=true</code>.</li><li>The NatGateway is currently <strong>not</strong> zone redundantly deployed. That mean the NatGateway of a Shoot cluster will always be in just one zone. This zone can be optionally selected via <code>.networks.natGateway.zone</code>.</li><li><strong>Caution:</strong> Modifying the <code>.networks.natGateway.zone</code> setting requires a recreation of the NatGateway and the managed public ip (automatically used if no own public ip is specified, see below). That mean you will most likely get a different public ip for egress connections.</li><li>It is possible to bring own zonal public ip(s) via <code>networks.natGateway.ipAddresses</code>. Those public ip(s) need to be in the same zone as the NatGateway (see <code>networks.natGateway.zone</code>) and be of SKU <code>standard</code>. For each public ip the <code>name</code>, the <code>resourceGroup</code> and the <code>zone</code> need to be specified.</li><li>The field <code>networks.natGateway.idleConnectionTimeoutMinutes</code> allows the configuration of NAT Gateway&rsquo;s idle connection timeout property. The idle timeout value can be adjusted from 4 minutes, up to 120 minutes. Omitting this property will set the idle timeout to its default value according to <a href=https://docs.microsoft.com/en-us/azure/virtual-network/nat-gateway-resource#timers>NAT Gateway&rsquo;s documentation</a>.</li></ul><p>In the <code>identity</code> section you can specify an <a href=https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview#how-does-the-managed-identities-for-azure-resources-work>Azure user-assigned managed identity</a> which should be attached to all cluster worker machines. With <code>identity.name</code> you can specify the name of the identity and with <code>identity.resourceGroup</code> you can specify the resource group which contains the identity resource on Azure. The identity need to be created by the user upfront (manually, other tooling, &mldr;). Gardener/Azure Extension will only use the referenced one and won&rsquo;t create an identity. Furthermore the identity have to be in the same subscription as the Shoot cluster. Via the <code>identity.acrAccess</code> you can configure the worker machines to use the passed identity for pulling from an <a href=https://docs.microsoft.com/en-us/azure/container-registry/container-registry-intro>Azure Container Registry (ACR)</a>.
<strong>Caution:</strong> Adding, exchanging or removing the identity will require a rolling update of all worker machines in the Shoot cluster.</p><p>Apart from the VNet and the worker subnet the Azure extension will also create a dedicated resource group, route tables, security groups, and an availability set (if not using zoned clusters).</p><h3 id=infrastructureconfig-with-dedicated-subnets-per-zone>InfrastructureConfig with dedicated subnets per zone</h3><p>Another deployment option <strong>for zonal clusters only</strong>, is to create and configure a separate subnet per availability zone. This network layout is recommended to users that require fine-grained control over their network setup. One prevalent usecase is to create a zone-redundant NAT Gateway deployment by taking advantage of the ability to deploy separate NAT Gateways for each subnet.</p><p>To use this configuration the following requirements must be met:</p><ul><li>the <code>zoned</code> field must be set to <code>true</code>.</li><li>the <code>networks.vnet</code> section must not be empty and must contain a valid configuration. For existing clusters that were not using the <code>networks.vnet</code> section, it is enough if <code>networks.vnet.cidr</code> field is set to the current <code>networks.worker</code> value.</li></ul><p>For each of the target zones a subnet CIDR range must be specified. The specified CIDR range must be contained in the VNet CIDR specified above, or the VNet CIDR of your already existing VNet. In addition, the CIDR ranges must not overlap with the ranges of the other subnets.</p><p><em>ServiceEndpoints</em> and <em>NatGateways</em> can be configured per subnet. Respectively, when <code>networks.zones</code> is specified, the fields <code>networks.workers</code>, <code>networks.serviceEndpoints</code> and <code>networks.natGateway</code> cannot be set. All the configuration for the subnets must be done inside the respective zone&rsquo;s configuration.</p><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span><span style=display:flex><span>networks:
</span></span><span style=display:flex><span>  zoned: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  vnet: <span style=color:green># specify either &#39;name&#39; and &#39;resourceGroup&#39; or &#39;cidr&#39;</span>
</span></span><span style=display:flex><span>    cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>  zones:
</span></span><span style=display:flex><span>  - name: 1
</span></span><span style=display:flex><span>    cidr: <span style=color:#a31515>&#34;10.250.0.0/24&#34;</span>
</span></span><span style=display:flex><span>  - name: 2
</span></span><span style=display:flex><span>    cidr: <span style=color:#a31515>&#34;10.250.0.0/24&#34;</span>
</span></span><span style=display:flex><span>    natGateway:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>false</span>
</span></span></code></pre></div><h3 id=migrating-to-zonal-shoots-with-dedicated-subnets-per-zone>Migrating to zonal shoots with dedicated subnets per zone</h3><p>For existing zonal clusters it is possible to migrate to a network layout with dedicated subnets per zone. The migration works by creating additional network resources as specified in the configuration and progressively roll part of your existing nodes to use the new resources. To achieve the controlled rollout of your nodes, parts of the existing infrastructure must be preserved which is why the following constraint is imposed:</p><p>One of your specified zones must have the exact same CIDR range as the current <code>network.workers</code> field. Here is an example of such migration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>infrastructureConfig:
</span></span><span style=display:flex><span>  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>  kind: InfrastructureConfig
</span></span><span style=display:flex><span>  networks:
</span></span><span style=display:flex><span>    vnet:
</span></span><span style=display:flex><span>      cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>    workers: 10.250.0.0/19
</span></span><span style=display:flex><span>  zoned: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>to</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>infrastructureConfig:
</span></span><span style=display:flex><span>  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>  kind: InfrastructureConfig
</span></span><span style=display:flex><span>  networks:
</span></span><span style=display:flex><span>    vnet:
</span></span><span style=display:flex><span>      cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>      - name: 3
</span></span><span style=display:flex><span>        cidr: 10.250.0.0/19 <span style=color:green># note the preservation of the &#39;workers&#39; CIDR</span>
</span></span><span style=display:flex><span><span style=color:green># optionally add other zones </span>
</span></span><span style=display:flex><span>    <span style=color:green># - name: 2  </span>
</span></span><span style=display:flex><span>    <span style=color:green>#   cidr: 10.250.32.0/19</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   natGateway:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     enabled: true</span>
</span></span><span style=display:flex><span>  zoned: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>Another more advanced example with user-provided public IP addresses for the NAT Gateway and how it can be migrated:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>infrastructureConfig:
</span></span><span style=display:flex><span>  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>  kind: InfrastructureConfig
</span></span><span style=display:flex><span>  networks:
</span></span><span style=display:flex><span>    vnet:
</span></span><span style=display:flex><span>      cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>    workers: 10.250.0.0/19
</span></span><span style=display:flex><span>    natGateway:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      zone: 1
</span></span><span style=display:flex><span>      ipAddresses:
</span></span><span style=display:flex><span>        - name: pip1
</span></span><span style=display:flex><span>          resourceGroup: group
</span></span><span style=display:flex><span>          zone: 1
</span></span><span style=display:flex><span>        - name: pip2
</span></span><span style=display:flex><span>          resourceGroup: group
</span></span><span style=display:flex><span>          zone: 1
</span></span><span style=display:flex><span>  zoned: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>to</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>infrastructureConfig:
</span></span><span style=display:flex><span>  apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>  kind: InfrastructureConfig
</span></span><span style=display:flex><span>  zoned: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  networks:
</span></span><span style=display:flex><span>    vnet:
</span></span><span style=display:flex><span>      cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>      - name: 1
</span></span><span style=display:flex><span>        cidr: 10.250.0.0/19 <span style=color:green># note the preservation of the &#39;workers&#39; CIDR</span>
</span></span><span style=display:flex><span>        natGateway:
</span></span><span style=display:flex><span>          enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>          ipAddresses:
</span></span><span style=display:flex><span>            - name: pip1
</span></span><span style=display:flex><span>              resourceGroup: group
</span></span><span style=display:flex><span>              zone: 1
</span></span><span style=display:flex><span>            - name: pip2
</span></span><span style=display:flex><span>              resourceGroup: group
</span></span><span style=display:flex><span>              zone: 1
</span></span><span style=display:flex><span><span style=color:green># optionally add other zones </span>
</span></span><span style=display:flex><span><span style=color:green>#     - name: 2  </span>
</span></span><span style=display:flex><span><span style=color:green>#       cidr: 10.250.32.0/19</span>
</span></span><span style=display:flex><span><span style=color:green>#       natGateway:</span>
</span></span><span style=display:flex><span><span style=color:green>#         enabled: true</span>
</span></span><span style=display:flex><span><span style=color:green>#         ipAddresses:</span>
</span></span><span style=display:flex><span><span style=color:green>#           - name: pip3</span>
</span></span><span style=display:flex><span><span style=color:green>#             resourceGroup: group</span>
</span></span></code></pre></div><p>You can apply such change to your shoot by issuing a <code>kubectl patch</code> command to replace your current <code>.spec.provider.infrastructureConfig</code> section:</p><pre tabindex=0><code>$ cat new-infra.json

[
  {
    &#34;op&#34;: &#34;replace&#34;,
    &#34;path&#34;: &#34;/spec/provider/infrastructureConfig&#34;,
    &#34;value&#34;: {
      &#34;apiVersion&#34;: &#34;azure.provider.extensions.gardener.cloud/v1alpha1&#34;,
      &#34;kind&#34;: &#34;InfrastructureConfig&#34;,
      &#34;networks&#34;: {
        &#34;vnet&#34;: {
          &#34;cidr&#34;: &#34;&lt;your-vnet-cidr&gt;&#34;
        },
        &#34;zones&#34;: [
          {
            &#34;name&#34;: 1,
            &#34;cidr&#34;: &#34;10.250.0.0/24&#34;,
            &#34;natGateway&#34;: {
              &#34;enabled&#34;: true
            }
          },
          {
            &#34;name&#34;: 1,
            &#34;cidr&#34;: &#34;10.250.1.0/24&#34;,
            &#34;natGateway&#34;: {
              &#34;enabled&#34;: true
            }
          },
        ]
      },
      &#34;zoned&#34;: true
    }
  }
]

kubectl patch --type=&#34;json&#34; --patch-file new-infra.json shoot &lt;my-shoot&gt;
</code></pre><p>⚠️ The migration to shoots with dedicated subnets per zone is a one-way process. Reverting the shoot to the previous configuration is not supported.</p><p>⚠️ During the migration a subset of the nodes will be rolled to the new subnets.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the Azure-specific control plane components.
Today, the only component deployed by the Azure extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the Azure extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span><span style=display:flex><span>cloudControllerManager:
</span></span><span style=display:flex><span>  featureGates:
</span></span><span style=display:flex><span>    RotateKubeletServerCertificate: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><p><code>storage</code> contains options for storage-related control plane component.
<code>storage.managedDefaultStorageClass</code> is enabled by default and will deploy a <code>storageClass</code> and mark it as a default (via the <code>storageclass.kubernetes.io/is-default-class</code> annotation)
<code>storage.managedDefaultVolumeSnapshotClass</code> is enabled by default and will deploy a <code>volumeSnapshotClass</code> and mark it as a default (via the <code>snapshot.storage.kubernetes.io/is-default-classs</code> annotation)
In case you want to manage your own default <code>storageClass</code> or <code>volumeSnapshotClass</code> you need to disable the respective options above, otherwise reconciliation of the controlplane may fail.</p><h2 id=workerconfig><code>WorkerConfig</code></h2><p>The Azure extension supports encryption for volumes plus support for additional data volumes per machine.
Please note that you cannot specify the <code>encrypted</code> flag for Azure disks as they are encrypted by default/out-of-the-box.
For each data volume, you have to specify a name.
The following YAML is a snippet of a <code>Shoot</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: cpu-worker
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        type: Standard_LRS
</span></span><span style=display:flex><span>        size: 20Gi
</span></span><span style=display:flex><span>      dataVolumes:
</span></span><span style=display:flex><span>      - name: kubelet-dir
</span></span><span style=display:flex><span>        type: Standard_LRS
</span></span><span style=display:flex><span>        size: 25Gi
</span></span></code></pre></div><p>Additionally, it supports for other Azure-specific values and could be configured under <code>.spec.provider.workers[].providerConfig</code></p><p>An example <code>WorkerConfig</code> for the Azure extension looks like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: WorkerConfig
</span></span><span style=display:flex><span>nodeTemplate: <span style=color:green># (to be specified only if the node capacity would be different from cloudprofile info during runtime)</span>
</span></span><span style=display:flex><span>  capacity:
</span></span><span style=display:flex><span>    cpu: 2
</span></span><span style=display:flex><span>    gpu: 1
</span></span><span style=display:flex><span>    memory: 50Gi
</span></span></code></pre></div><p>The <code>.nodeTemplate</code> is used to specify resource information of the machine during runtime. This then helps in Scale-from-Zero.
Some points to note for this field:
- Currently only cpu, gpu and memory are configurable.
- a change in the value lead to a rolling update of the machine in the workerpool
- all the resources needs to be specified</p><h2 id=example-shoot-manifest-non-zoned>Example <code>Shoot</code> manifest (non-zoned)</h2><p>Please find below an example <code>Shoot</code> manifest for a non-zoned cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-azure
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: azure
</span></span><span style=display:flex><span>  region: westeurope
</span></span><span style=display:flex><span>  secretBindingName: core-azure
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: azure
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vnet:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        workers: 10.250.0.0/19
</span></span><span style=display:flex><span>      zoned: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: Standard_D4_v3
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: Standard_LRS
</span></span><span style=display:flex><span><span style=color:green>#      providerConfig:</span>
</span></span><span style=display:flex><span><span style=color:green>#        apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span><span style=color:green>#        kind: WorkerConfig</span>
</span></span><span style=display:flex><span><span style=color:green>#        nodeTemplate: # (to be specified only if the node capacity would be different from cloudprofile info during runtime)</span>
</span></span><span style=display:flex><span><span style=color:green>#          capacity:</span>
</span></span><span style=display:flex><span><span style=color:green>#            cpu: 2</span>
</span></span><span style=display:flex><span><span style=color:green>#            gpu: 1</span>
</span></span><span style=display:flex><span><span style=color:green>#            memory: 50Gi</span>
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>    pods: 100.96.0.0/11
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    services: 100.64.0.0/13
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=example-shoot-manifest-zoned>Example <code>Shoot</code> manifest (zoned)</h2><p>Please find below an example <code>Shoot</code> manifest for a zoned cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-azure
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: azure
</span></span><span style=display:flex><span>  region: westeurope
</span></span><span style=display:flex><span>  secretBindingName: core-azure
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: azure
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vnet:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        workers: 10.250.0.0/19
</span></span><span style=display:flex><span>      zoned: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: Standard_D4_v3
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: Standard_LRS
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>    pods: 100.96.0.0/11
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    services: 100.64.0.0/13
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=example-shoot-manifest-zoned-with-nat-gateways-per-zone>Example <code>Shoot</code> manifest (zoned with NAT Gateways per zone)</h2><p>Please find below an example <code>Shoot</code> manifest for a zoned cluster using NAT Gateways per zone:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-azure
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: azure
</span></span><span style=display:flex><span>  region: westeurope
</span></span><span style=display:flex><span>  secretBindingName: core-azure
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: azure
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vnet:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        zones:
</span></span><span style=display:flex><span>        - name: 1
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/24
</span></span><span style=display:flex><span>          serviceEndpoints:
</span></span><span style=display:flex><span>          - Microsoft.Storage
</span></span><span style=display:flex><span>          - Microsoft.Sql
</span></span><span style=display:flex><span>          natGateway:
</span></span><span style=display:flex><span>            enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>            idleConnectionTimeoutMinutes: 4
</span></span><span style=display:flex><span>        - name: 2
</span></span><span style=display:flex><span>          cidr: 10.250.1.0/24
</span></span><span style=display:flex><span>          serviceEndpoints:
</span></span><span style=display:flex><span>          - Microsoft.Storage
</span></span><span style=display:flex><span>          - Microsoft.Sql
</span></span><span style=display:flex><span>          natGateway:
</span></span><span style=display:flex><span>            enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      zoned: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: Standard_D4_v3
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: Standard_LRS
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>    pods: 100.96.0.0/11
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    services: 100.64.0.0/13
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=csi-volume-provisioners>CSI volume provisioners</h2><p>Every Azure shoot cluster will be deployed with the Azure Disk CSI driver and the Azure File CSI driver.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-azure@v1.25</code>.</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> and <code>ShootSARotation</code> feature gates since <code>gardener-extension-provider-azure@v1.28</code>.</p><h2 id=miscellaneous>Miscellaneous</h2><h3 id=azure-accelerated-networking>Azure Accelerated Networking</h3><p>All worker machines of the cluster will be automatically configured to use <a href=https://docs.microsoft.com/en-us/azure/virtual-network/create-vm-accelerated-networking-cli>Azure Accelerated Networking</a> if the prerequisites are fulfilled.
The prerequisites are that the cluster must be zoned, and the used machine type and operating system image version are compatible for Accelerated Networking.
<code>Availability Set</code> based shoot clusters will not be enabled for accelerated networking even if the machine type and operating system support it, this is necessary because all machines from the availability set must be scheduled on special hardware, more daitls can be found <a href=https://github.com/MicrosoftDocs/azure-docs/issues/10536>here</a>.
Supported machine types are listed in the CloudProfile in <code>.spec.providerConfig.machineTypes[].acceleratedNetworking</code> and the supported operating system image versions are defined in <code>.spec.providerConfig.machineImages[].versions[].acceleratedNetworking</code>.</p><h3 id=preview-shoot-clusters-with-vmss-flexible-orchestration-vmss-flexvmo>Preview: Shoot clusters with VMSS Flexible Orchestration (VMSS Flex/VMO)</h3><p>The machines of an Azure cluster can be created while being attached to an <a href=https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-orchestration-modes#scale-sets-with-flexible-orchestration>Azure Virtual Machine ScaleSet with flexible orchestraion</a>.
The Virtual Machine ScaleSet with flexible orchestration feature is currently in preview and not yet general available on Azure.
Subscriptions need to <a href=https://docs.microsoft.com/en-us/azure/virtual-machine-scale-sets/virtual-machine-scale-sets-orchestration-modes#register-for-flexible-orchestration-mode>join the preview</a> to make use of the feature.</p><p>Azure VMSS Flex is intended to replace Azure AvailabilitySet for non-zoned Azure Shoot clusters in the mid-term (once the feature goes GA) as VMSS Flex come with less disadvantages like no blocking machine operations or compability with <code>Standard</code> SKU loadbalancer etc.</p><p>To configure an Azure Shoot cluster which make use of VMSS Flex you need to do the following:</p><ul><li>The <code>InfrastructureConfig</code> of the Shoot configuration need to contain <code>.zoned=false</code></li><li>Shoot resource need to have the following annotation assigned: <code>alpha.azure.provider.extensions.gardener.cloud/vmo=true</code></li></ul><p>Some key facts about VMSS Flex based clusters:</p><ul><li>Unlike regular non-zonal Azure Shoot clusters, which have a primary AvailabilitySet which is shared between all machines in all worker pools of a Shoot cluster, a VMSS Flex based cluster has an own VMSS for each workerpool</li><li>In case the configuration of the VMSS will change (e.g. amount of fault domains in a region change; configured in the CloudProfile) all machines of the worker pool need to be rolled</li><li>It is not possible to migrate an existing primary AvailabilitySet based Shoot cluster to VMSS Flex based Shoot cluster and vice versa</li><li>VMSS Flex based clusters are using <code>Standard</code> SKU LoadBalancers instead of <code>Basic</code> SKU LoadBalancers for AvailabilitySet based Shoot clusters</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b182a904236beecf338898551c7afb28>1.4 - Provider Equinix Metal</h1><div class=lead>Gardener extension controller for the Equinix Metal cloud provider</div><h1 id=gardener-extension-for-equinix-metal-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Equinix Metal provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-provider-equinix-metal-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-provider-equinix-metal-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-equinix-metal><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-equinix-metal alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the Equinix Metal provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-equinix-metal/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.28</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.27</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.26</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.25</td><td>untested</td><td>N/A</td></tr><tr><td>Kubernetes 1.24</td><td>untested</td><td>N/A</td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-equinix-metal/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-5efa3d65b9b0708bfa6b0905e3a03668>1.4.1 - Operations</h1><h1 id=using-the-equinix-metal-provider-extension-with-gardener-as-operator>Using the Equinix Metal provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for Equinix Metal and provide an example <code>CloudProfile</code> manifest with minimal configuration that you can use to allow creating Equinix Metal shoot clusters.</p><h2 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h2><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: equinix-metal
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: equinixmetal
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.27.2
</span></span><span style=display:flex><span>    - version: 1.26.7
</span></span><span style=display:flex><span>    - version: 1.25.10
</span></span><span style=display:flex><span>      <span style=color:green>#expirationDate: &#34;2023-03-15T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: flatcar
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 0.0.0-stable
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: t1.small
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;4&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  regions: <span style=color:green># List of offered metros</span>
</span></span><span style=display:flex><span>  - name: ny
</span></span><span style=display:flex><span>    zones: <span style=color:green># List of offered facilities within the respective metro</span>
</span></span><span style=display:flex><span>    - name: ewr1
</span></span><span style=display:flex><span>    - name: ny5
</span></span><span style=display:flex><span>    - name: ny7
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: flatcar
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 0.0.0-stable
</span></span><span style=display:flex><span>        id: flatcar_stable
</span></span><span style=display:flex><span>      - version: 3510.2.2
</span></span><span style=display:flex><span>        ipxeScriptUrl: https://stable.release.flatcar-linux.net/amd64-usr/3510.2.2/flatcar_production_packet.ipxe
</span></span></code></pre></div><h2 id=cloudprofileconfig><code>CloudProfileConfig</code></h2><p>The cloud profile configuration contains information about the real machine image IDs in the Equinix Metal environment (IDs).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the Equinix Metal extension knows the ID for every version you want to offer.</p><p>Equinix Metal supports two different options to specify the image:</p><ol><li>Supported Operating System: Images that are provided by Equinix Metal. They are referenced by their ID (<code>slug</code>). See (Operating Systems Reference)[https://deploy.equinix.com/developers/docs/metal/operating-systems/supported/#operating-systems-reference] for all supported operating system and their ids.</li><li>Custom iPXE Boot: Equinix Metal supports passing custom iPXE scripts during provisioning, which allows you to install a custom operating system manually. This is useful if you want to have a custom image or want to pin to a specific version. See <a href=https://deploy.equinix.com/developers/docs/metal/operating-systems/custom-ipxe/#provisioning-with-custom-ipxe>Custom iPXE Boot</a> for details.</li></ol><p>An example <code>CloudProfileConfig</code> for the Equinix Metal extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: flatcar
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 0.0.0-stable
</span></span><span style=display:flex><span>    id: flatcar_stable
</span></span><span style=display:flex><span>  - version: 3510.2.2
</span></span><span style=display:flex><span>    ipxeScriptUrl: https://stable.release.flatcar-linux.net/amd64-usr/3510.2.2/flatcar_production_packet.ipxe
</span></span></code></pre></div><blockquote><p>NOTE: <code>CloudProfileConfig</code> is not a Custom Resource, so you cannot create it directly.</p></blockquote></div><div class=td-content style=page-break-before:always><h1 id=pg-5464af5b09e787e0eed69703d21d1a39>1.4.2 - Usage</h1><h1 id=using-the-equinix-metal-provider-extension-with-gardener-as-end-user>Using the Equinix Metal provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for Equinix Metal and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create an Equinix Metal cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=provider-secret-data>Provider secret data</h2><p>Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of your Equinix Metal project.
This <code>Secret</code> must look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-secret
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  apiToken: base64(api-token)
</span></span><span style=display:flex><span>  projectID: base64(project-id)
</span></span></code></pre></div><p>Please look up <a href=https://metal.equinix.com/developers/api/>https://metal.equinix.com/developers/api/</a> as well.</p><p>With <code>Secret</code> created, create a <code>SecretBinding</code> resource referencing it. It may look like this:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: SecretBinding
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-secret
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>secretRef:
</span></span><span style=display:flex><span>  name: my-secret
</span></span><span style=display:flex><span>quotas: []
</span></span></code></pre></div><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>Currently, there is no infrastructure configuration possible for the Equinix Metal environment.</p><p>An example <code>InfrastructureConfig</code> for the Equinix Metal extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span></code></pre></div><p>The Equinix Metal extension will only create a key pair.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the Equinix Metal-specific control plane components.
Today, the Equinix Metal extension deploys the <code>cloud-controller-manager</code> and the CSI controllers, however, it doesn&rsquo;t offer any configuration options at the moment.</p><p>An example <code>ControlPlaneConfig</code> for the Equinix Metal extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span></code></pre></div><h2 id=workerconfig><code>WorkerConfig</code></h2><p>The Equinix Metal extension supports specifying IDs for reserved devices that should be used for the machines of a specific worker pool.</p><p>An example <code>WorkerConfig</code> for the Equinix Metal extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: WorkerConfig
</span></span><span style=display:flex><span>reservationIDs:
</span></span><span style=display:flex><span>- my-reserved-device-1
</span></span><span style=display:flex><span>- my-reserved-device-2
</span></span><span style=display:flex><span>reservedDevicesOnly: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The <code>.reservationIDs[]</code> list contains the list of IDs of the reserved devices.
The <code>.reservedDevicesOnly</code> field indicates whether only reserved devices from the provided list of reservation IDs should be used when new machines are created.
It always will attempt to create a device from one of the reservation IDs.
If none is available, the behaviour depends on the setting:</p><ul><li><code>true</code>: return an error</li><li><code>false</code>: request a regular on-demand device</li></ul><p>The default value is <code>false</code>.</p><h2 id=example-shoot-manifest>Example <code>Shoot</code> manifest</h2><p>Please find below an example <code>Shoot</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-shoot
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: equinix-metal
</span></span><span style=display:flex><span>  region: ny <span style=color:green># Corresponds to a metro</span>
</span></span><span style=display:flex><span>  secretBindingName: my-secret
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: equinixmetal
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-pool1
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: t1.small
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: storage_1
</span></span><span style=display:flex><span>      zones: <span style=color:green># Optional list of facilities, all of which MUST be in the metro; if not provided, then random facilities within the metro will be chosen for each machine.</span>
</span></span><span style=display:flex><span>      - ewr1
</span></span><span style=display:flex><span>      - ny5
</span></span><span style=display:flex><span>    - name: reserved-pool
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: t1.small
</span></span><span style=display:flex><span>      minimum: 1
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      providerConfig:
</span></span><span style=display:flex><span>        apiVersion: equinixmetal.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>        kind: WorkerConfig
</span></span><span style=display:flex><span>        reservationIDs:
</span></span><span style=display:flex><span>        - reserved-device1
</span></span><span style=display:flex><span>        - reserved-device2
</span></span><span style=display:flex><span>        reservedDevicesOnly: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: storage_1
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.27.2
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>⚠️ Note that if you specify multiple facilities in the <code>.spec.provider.workers[].zones[]</code> list then new machines are randomly created in one of the provided facilities.
Particularly, it is not ensured that all facilities are used or that all machines are equally or unequally distributed.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-equinix-metal@v2.2</code>.</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> feature gate since <code>gardener-extension-provider-equinix-metal@v2.3</code> and <code>ShootSARotation</code> feature gate since <code>gardener-extension-provider-equinix-metal@v2.4</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3958d75f5afea52ba6f339a00c82ee93>1.5 - Provider GCP</h1><div class=lead>Gardener extension controller for the GCP cloud provider</div><h1 id=gardener-extension-for-gcp-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for GCP provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener-tests/pipelines/gardener-extension-provider-gcp-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener-tests/pipelines/gardener-extension-provider-gcp-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-gcp><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-gcp alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the GCP provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.28</td><td>1.28.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.27</td><td>1.27.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.27%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.27%20GCE/tests_status?style=svg" alt="Gardener v1.27 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.26</td><td>1.26.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.26%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.26%20GCE/tests_status?style=svg" alt="Gardener v1.26 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.25</td><td>1.25.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20GCE/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.24</td><td>1.24.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20GCE><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20GCE/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-gcp/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-43bc49392a4264d2b4f9a208d1e2eaaa>1.5.1 - Tutorials</h1></div><div class=td-content><h1 id=pg-7d967b4e7be2e449fb9198a8ea60881e>1.5.1.1 - Create a Кubernetes Cluster on GCP with Gardener</h1><h3 id=overview>Overview</h3><p>Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on GCP.</p><h3 id=prerequisites>Prerequisites</h3><ul><li>You have created a <a href=https://console.cloud.google.com/>GCP account</a>.</li><li>You have access to the Gardener dashboard and have permissions to create projects.</li></ul><h3 id=steps>Steps</h3><ol><li><p>Go to the Gardener dashboard and create a <em>Project</em>.</p><img src=/__resources/new-gardener-project_077542.png></li><li><p>Check which roles are required by Gardener.</p><ol><li><p>Choose <em>Secrets</em>, then the plus icon <img src=/__resources/plus-icon_9d16d6.png> and select <em>GCP</em>.</p><img src=/__resources/create-secret-gcp_9b8911.png></li><li><p>Click on the help button <img src=/__resources/help-icon_1fcfd3.png>.</p><img src=/__resources/gardener-gcp-secret-1_3a2741.png>
<img src=/__resources/gardener-gcp-secret-2_fcc008.png></li></ol></li><li><p>Create a service account with the correct roles in GCP:</p><ol><li><p><a href=https://console.cloud.google.com/iam-admin/serviceaccounts>Create a new service account in GCP</a>.</p><img src=/__resources/gcp-create-service-account-0_0a398e.png></li><li><p>Enter the name and description of your service account.</p></li><li><p>Assign the roles required by Gardener.</p></li><li><p>Choose <em>Done</em>.</p><img src=/__resources/gcp-create-service-account-1_c55fe5.png></li></ol></li><li><p>Create a key for your service:</p><ol><li><p>Locate your service account, then choose <em>Actions</em> and <em>Manage keys</em>.</p><img src=/__resources/gcp-create-key-0_b97786.png></li><li><p>Choose <em>Add Key</em>, then <em>Create new key</em>.</p><img src=/__resources/gcp-create-key-1_83ef08.png></li><li><p>Save the private key of the service account in JSON format.</p><img src=/__resources/gcp-create-key-2_a661a1.png>
<img src=/__resources/gcp-create-key-3_ad2cf4.png></li></ol><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Save the key of the user, it’s used later to create secrets for Gardener.</div></li><li><p>Enable the <a href=https://console.developers.google.com/apis/library/compute.googleapis.com>Google Compute API</a> by following <a href=https://cloud.google.com/endpoints/docs/openapi/enable-api>these steps</a>.</p><blockquote><p>When you are finished, you should see the following page:</p></blockquote><img src=/__resources/gcp-compute-engine-api_e4a22b.png></li><li><p>Enable the <a href=https://console.developers.google.com/apis/library/iam.googleapis.com>Google IAM API</a> by following <a href=https://cloud.google.com/endpoints/docs/openapi/enable-api>these steps</a>.</p><blockquote><p>When you are finished, you should see the following page:</p></blockquote><img src=/__resources/gcp-iam-api_8faa7c.png></li><li><p>On the Gardener dashboard, choose <em>Secrets</em> and then the plus sign <img src=/__resources/plus-icon_9d16d6.png>. Select <em>GCP</em> from the drop down menu to add a new GCP secret.</p></li><li><p>Create your secret.</p><ol><li>Type the name of your secret.</li><li>Select your <em>Cloud Profile</em>.</li><li>Copy and paste the contents of the <em>.JSON</em> file you saved when you created the secret key on GCP.</li><li>Choose <em>Add secret</em>.
<img src=/__resources/add-gcp-secret_84ec33.png></li></ol><blockquote><p>After completing these steps, you should see your newly created secret in the <em>Infrastructure Secrets</em> section.</p></blockquote><img src=/__resources/secret-stored_2be4fd.png></li><li><p>To create a new cluster, choose <em>Clusters</em> and then the plus sign in the upper right corner.</p><img src=/__resources/new-cluster_581df0.png></li><li><p>In the <em>Create Cluster</em> section:</p><ol><li>Select <em>GCP</em> in the <em>Infrastructure</em> tab.</li><li>Type the name of your cluster in the <em>Cluster Details</em> tab.</li><li>Choose the secret you created before in the <em>Infrastructure Details</em> tab.</li><li>Choose <em>Create</em>.</li></ol><img src=/__resources/create-cluster_66c946.png></li><li><p>Wait for your cluster to get created.</p><img src=/__resources/processing-cluster_309fda.png></li></ol><h3 id=result>Result</h3><p>After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster.</p><img src=/__resources/copy-kubeconfig_3d13a9.png></div><div class=td-content style=page-break-before:always><h1 id=pg-c3de24169ff0acf606ae8522686249a7>1.5.2 - Deployment</h1><h1 id=deployment-of-the-gcp-provider-extension>Deployment of the GCP provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the GCP provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the GCP provider extension <a href=https://github.com/gardener/gardener-extension-provider-gcp>repository</a>.</p><h2 id=gardener-extension-admission-gcp>gardener-extension-admission-gcp</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-gcp</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-83937afb9b7b37ebb4a6752a7fcdef44>1.5.3 - Local Setup</h1><h3 id=admission-gcp>admission-gcp</h3><p><code>admission-gcp</code> is an admission webhook server which is responsible for the validation of the cloud provider (GCP in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-admission
</span></span></code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-gcp.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/dev-setup-admission-gcp.sh
</span></span></code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-gcp</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-76376d0613ae014a2d188d92ecff4a5d>1.5.4 - Operations</h1><h1 id=using-the-gcp-provider-extension-with-gardener-as-operator>Using the GCP provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.
The <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>core.gardener.cloud/v1beta1.Seed</code> resource</a> is structured similarly.
Additionally, it allows configuring settings for the backups of the main etcds&rsquo; data of shoot clusters control planes running in this seed cluster.</p><p>This document explains the necessary configuration for this provider extension.</p><h2 id=cloudprofile-resource><code>CloudProfile</code> resource</h2><p>This section describes, how the configuration for <code>CloudProfile</code>s looks like for GCP by providing an example <code>CloudProfile</code> manifest with minimal configuration that can be used to allow the creation of GCP shoot clusters.</p><h3 id=cloudprofileconfig><code>CloudProfileConfig</code></h3><p>The cloud profile configuration contains information about the real machine image IDs in the GCP environment (image URLs).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the GCP extension knows the image URL for every version you want to offer.
For each machine image version an <code>architecture</code> field can be specified which specifies the CPU architecture of the machine on which given machine image can be used.</p><p>An example <code>CloudProfileConfig</code> for the GCP extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: coreos
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 2135.6.0
</span></span><span style=display:flex><span>    image: projects/coreos-cloud/global/images/coreos-stable-2135-6-0-v20190801
</span></span><span style=display:flex><span>    <span style=color:green># architecture: amd64 # optional</span>
</span></span></code></pre></div><h3 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h3><p>If you want to allow that shoots can create VMs with local SSDs volumes then you have to specify the type of the disk with <code>SCRATCH</code> in the <code>.spec.volumeTypes[]</code> list.
Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gcp
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: gcp
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.27.3
</span></span><span style=display:flex><span>    - version: 1.26.8
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2022-10-31T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2135.6.0
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: n1-standard-4
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;4&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 15Gi
</span></span><span style=display:flex><span>  volumeTypes:
</span></span><span style=display:flex><span>  - name: pd-standard
</span></span><span style=display:flex><span>    class: standard
</span></span><span style=display:flex><span>  - name: pd-ssd
</span></span><span style=display:flex><span>    class: premium
</span></span><span style=display:flex><span>  - name: SCRATCH
</span></span><span style=display:flex><span>    class: standard
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - region: europe-west1
</span></span><span style=display:flex><span>    names:
</span></span><span style=display:flex><span>    - europe-west1-b
</span></span><span style=display:flex><span>    - europe-west1-c
</span></span><span style=display:flex><span>    - europe-west1-d
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: coreos
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 2135.6.0
</span></span><span style=display:flex><span>        image: projects/coreos-cloud/global/images/coreos-stable-2135-6-0-v20190801
</span></span><span style=display:flex><span>        <span style=color:green># architecture: amd64 # optional</span>
</span></span></code></pre></div><h2 id=seed-resource><code>Seed</code> resource</h2><p>This provider extension does not support any provider configuration for the <code>Seed</code>&rsquo;s <code>.spec.provider.providerConfig</code> field.
However, it supports to managing of backup infrastructure, i.e., you can specify a configuration for the <code>.spec.backup</code> field.</p><h3 id=backup-configuration>Backup configuration</h3><p>A Seed of type <code>gcp</code> can be configured to perform backups for the main etcds&rsquo; of the shoot clusters control planes using Google Cloud Storage buckets.</p><p>The location/region where the backups will be stored defaults to the region of the Seed (<code>spec.provider.region</code>), but can also be explicitly configured via the field <code>spec.backup.region</code>.
The region of the backup can be different from where the seed cluster is running.
However, usually it makes sense to pick the same region for the backup bucket as used for the Seed cluster.</p><p>Please find below an example <code>Seed</code> manifest (partly) that configures backups using Google Cloud Storage buckets.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-seed
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: gcp
</span></span><span style=display:flex><span>    region: europe-west1
</span></span><span style=display:flex><span>  backup:
</span></span><span style=display:flex><span>    provider: gcp
</span></span><span style=display:flex><span>    region: europe-west1 <span style=color:green># default region</span>
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: backup-credentials
</span></span><span style=display:flex><span>      namespace: garden
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>An example of the referenced secret containing the credentials for the GCP Cloud storage can be found in the <a href=https://github.com/gardener/gardener-extension-provider-gcp/blob/master/example/30-etcd-backup-secret.yaml>example folder</a>.</p><h4 id=permissions-for-gcp-cloud-storage>Permissions for GCP Cloud Storage</h4><p>Please make sure the service account associated with the provided credentials has the following IAM roles.</p><ul><li><a href=https://cloud.google.com/storage/docs/access-control/iam-roles>Storage Admin</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a567aa2cf222ff4f19466b657fa42f04>1.5.5 - Usage</h1><h1 id=using-the-gcp-provider-extension-with-gardener-as-end-user>Using the GCP provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>This document describes the configurable options for GCP and provides an example <code>Shoot</code> manifest with minimal configuration that can be used to create a GCP cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=gcp-provider-credentials>GCP Provider Credentials</h2><p>In order for Gardener to create a Kubernetes cluster using GCP infrastructure components, a Shoot has to provide credentials with sufficient permissions to the desired GCP project.
Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of the GCP project.
The <code>SecretBinding</code> is configurable in the <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml>Shoot cluster</a> with the field <code>secretBindingName</code>.</p><p>The required credentials for the GCP project are a <a href=https://cloud.google.com/iam/docs/service-accounts#service_account_keys>Service Account Key</a> to authenticate as a <a href=https://cloud.google.com/compute/docs/access/service-accounts>GCP Service Account</a>.
A service account is a special account that can be used by services and applications to interact with Google Cloud Platform APIs.
Applications can use service account credentials to authorize themselves to a set of APIs and perform actions within the permissions granted to the service account.</p><p>Make sure to <a href=https://cloud.google.com/service-usage/docs/enable-disable>enable the Google Identity and Access Management (IAM) API</a>.
<a href=https://cloud.google.com/iam/docs/creating-managing-service-accounts>Create a Service Account</a> that shall be used for the Shoot cluster.
<a href=https://cloud.google.com/iam/docs/granting-changing-revoking-access>Grant at least the following IAM roles</a> to the Service Account.</p><ul><li>Service Account Admin</li><li>Service Account Token Creator</li><li>Service Account User</li><li>Compute Admin</li></ul><p>Create a <a href=https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating_service_account_keys>JSON Service Account key</a> for the Service Account.
Provide it in the <code>Secret</code> (base64 encoded for field <code>serviceaccount.json</code>), that is being referenced by the <code>SecretBinding</code> in the Shoot cluster configuration.</p><p>This <code>Secret</code> must look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-gcp
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  serviceaccount.json: base64(serviceaccount-json)
</span></span></code></pre></div><p>⚠️ Depending on your API usage it can be problematic to reuse the same Service Account Key for different Shoot clusters due to rate limits.
Please consider spreading your Shoots over multiple Service Accounts on different GCP projects if you are hitting those limits, see <a href=https://cloud.google.com/compute/docs/api-rate-limits>https://cloud.google.com/compute/docs/api-rate-limits</a>.</p><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the GCP extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span><span style=display:flex><span>networks:
</span></span><span style=display:flex><span><span style=color:green># vpc:</span>
</span></span><span style=display:flex><span><span style=color:green>#   name: my-vpc</span>
</span></span><span style=display:flex><span><span style=color:green>#   cloudRouter:</span>
</span></span><span style=display:flex><span><span style=color:green>#     name: my-cloudrouter</span>
</span></span><span style=display:flex><span>  workers: 10.250.0.0/16
</span></span><span style=display:flex><span><span style=color:green># internal: 10.251.0.0/16</span>
</span></span><span style=display:flex><span><span style=color:green># cloudNAT:</span>
</span></span><span style=display:flex><span><span style=color:green>#   minPortsPerVM: 2048</span>
</span></span><span style=display:flex><span><span style=color:green>#   endpointIndependentMapping:</span>
</span></span><span style=display:flex><span><span style=color:green>#     enabled: false</span>
</span></span><span style=display:flex><span><span style=color:green>#   natIPNames:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - name: manualnat1</span>
</span></span><span style=display:flex><span><span style=color:green>#   - name: manualnat2</span>
</span></span><span style=display:flex><span><span style=color:green># flowLogs:</span>
</span></span><span style=display:flex><span><span style=color:green>#   aggregationInterval: INTERVAL_5_SEC</span>
</span></span><span style=display:flex><span><span style=color:green>#   flowSampling: 0.2</span>
</span></span><span style=display:flex><span><span style=color:green>#   metadata: INCLUDE_ALL_METADATA</span>
</span></span></code></pre></div><p>The <code>networks.vpc</code> section describes whether you want to create the shoot cluster in an already existing VPC or whether to create a new one:</p><ul><li><p>If <code>networks.vpc.name</code> is given then you have to specify the VPC name of the existing VPC that was created by other means (manually, other tooling, &mldr;).
If you want to get a fresh VPC for the shoot then just omit the <code>networks.vpc</code> field.</p></li><li><p>If a VPC name is not given then we will create the cloud router + NAT gateway to ensure that worker nodes don&rsquo;t get external IPs.</p></li><li><p>If a VPC name is given then a cloud router name must also be given, failure to do so would result in validation errors
and possibly clusters without egress connectivity.</p></li><li><p>If a VPC name is given and calico shoot clusters are created without a network overlay within one VPC make sure that the pod CIDR specified in <code>shoot.spec.networking.pods</code> is not overlapping with any other pod CIDR used in that VPC.
Overlapping pod CIDRs will lead to disfunctional shoot clusters.</p></li></ul><p>The <code>networks.workers</code> section describes the CIDR for a subnet that is used for all shoot worker nodes, i.e., VMs which later run your applications.</p><p>The <code>networks.internal</code> section is optional and can describe a CIDR for a subnet that is used for <a href=https://cloud.google.com/load-balancing/docs/internal/>internal load balancers</a>,</p><p>The <code>networks.cloudNAT.minPortsPerVM</code> is optional and is used to define the <a href=https://cloud.google.com/nat/docs/overview#number_of_nat_ports_and_connections>minimum number of ports allocated to a VM for the CloudNAT</a></p><p>The <code>networks.cloudNAT.natIPNames</code> is optional and is used to specify the names of the manual ip addresses which should be used by the nat gateway</p><p>The <code>networks.cloudNAT.endpointIndependentMapping</code> is optional and is used to define the <a href=https://cloud.google.com/nat/docs/ports-and-addresses#ports-reuse-endpoints>endpoint mapping behavior</a>. You can enable it or disable it at any point by toggling <code>networks.cloudNAT.endpointIndependentMapping.enabled</code>. By default, it is disabled.</p><p>The specified CIDR ranges must be contained in the VPC CIDR specified above, or the VPC CIDR of your already existing VPC.
You can freely choose these CIDRs and it is your responsibility to properly design the network layout to suit your needs.</p><p>The <code>networks.flowLogs</code> section describes the configuration for the VPC flow logs. In order to enable the VPC flow logs at least one of the following parameters needs to be specified in the flow log section:</p><ul><li><p><code>networks.flowLogs.aggregationInterval</code> an optional parameter describing the aggregation interval for collecting flow logs. For more details, see <a href=https://www.terraform.io/docs/providers/google/r/compute_subnetwork.html#aggregation_interval>aggregation_interval reference</a>.</p></li><li><p><code>networks.flowLogs.flowSampling</code> an optional parameter describing the sampling rate of VPC flow logs within the subnetwork where 1.0 means all collected logs are reported and 0.0 means no logs are reported. For more details, see <a href=https://www.terraform.io/docs/providers/google/r/compute_subnetwork.html#flow_sampling>flow_sampling reference</a>.</p></li><li><p><code>networks.flowLogs.metadata</code> an optional parameter describing whether metadata fields should be added to the reported VPC flow logs. For more details, see <a href=https://www.terraform.io/docs/providers/google/r/compute_subnetwork.html#metadata>metadata reference</a>.</p></li></ul><p>Apart from the VPC and the subnets the GCP extension will also create a dedicated service account for this shoot, and firewall rules.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the GCP-specific control plane components.
Today, the only component deployed by the GCP extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the GCP extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span><span style=display:flex><span>zone: europe-west1-b
</span></span><span style=display:flex><span>cloudControllerManager:
</span></span><span style=display:flex><span>  featureGates:
</span></span><span style=display:flex><span>    RotateKubeletServerCertificate: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>The <code>zone</code> field tells the cloud-controller-manager in which zone it should mainly operate.
You can still create clusters in multiple availability zones, however, the cloud-controller-manager requires one &ldquo;main&rdquo; zone.
⚠️ You always have to specify this field!</p><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><h2 id=workerconfig>WorkerConfig</h2><p>The worker configuration contains:</p><ul><li><p>Local SSD interface for the additional volumes attached to GCP worker machines.</p><p>If you attach the disk with <code>SCRATCH</code> type, either an <code>NVMe</code> interface or a <code>SCSI</code> interface must be specified.
It is only meaningful to provide this volume interface if only <code>SCRATCH</code> data volumes are used.</p></li><li><p>Volume Encryption config that specifies values for <code>kmsKeyName</code> and <code>kmsKeyServiceAccountName</code>.</p><ul><li>The <code>kmsKeyName</code> is the
key name of the cloud kms disk encryption key and must be specified if CMEK disk encryption is needed.</li><li>The <code>kmsKeyServiceAccount</code> is the service account granted the <code>roles/cloudkms.cryptoKeyEncrypterDecrypter</code> on the <code>kmsKeyName</code>
If empty, then the role should be given to the Compute Engine Service Agent Account. This CESA account usually has the name:
<code>service-PROJECT_NUMBER@compute-system.iam.gserviceaccount.com</code>. See: <a href=https://cloud.google.com/iam/docs/service-agents#compute-engine-service-agent>https://cloud.google.com/iam/docs/service-agents#compute-engine-service-agent</a></li><li>Prior to use, the operator should add IAM policy binding using the gcloud CLI:<pre tabindex=0><code>gcloud projects add-iam-policy-binding projectId --member
serviceAccount:name@projectIdgserviceaccount.com --role roles/cloudkms.cryptoKeyEncrypterDecrypter
</code></pre></li></ul></li><li><p>Service Account with their specified scopes, authorized for this worker.</p><p>Service accounts created in advance that generate access tokens that can be accessed through the metadata server and used to authenticate applications on the instance.</p></li><li><p>GPU with its type and count per node. This will attach that GPU to all the machines in the worker grp</p><p><strong>Note</strong>:</p><ul><li><p>A rolling upgrade of the worker group would be triggered in case the <code>acceleratorType</code> or <code>count</code> is updated.</p></li><li><p>Some machineTypes like <a href=https://cloud.google.com/blog/products/compute/announcing-google-cloud-a2-vm-family-based-on-nvidia-a100-gpu>a2 family</a> come with already attached gpu of <code>a100</code> type and pre-defined count. If your workerPool consists of such machineTypes, please specify exact GPU configuration for the machine type as specified in Google cloud documentation. <code>acceleratorType</code> to use for families with attached gpu are stated below:</p><ol><li><em>a2 family</em> -> <code>nvidia-tesla-a100</code></li><li><em>g2 family</em> -> <code>nvidia-l4</code></li></ol></li><li><p>Sufficient quota of gpu is needed in the GCP project. This includes quota to support autoscaling if enabled.</p></li><li><p>GPU-attached machines can&rsquo;t be live migrated during host maintenance events. Find out how to handle that in your application <a href=https://cloud.google.com/compute/docs/gpus/gpu-host-maintenance>here</a></p></li><li><p>GPU count specified here is considered for forming node template during scale-from-zero in Cluster Autoscaler</p></li></ul><p>An example <code>WorkerConfig</code> for the GCP looks as follows:</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: WorkerConfig
</span></span><span style=display:flex><span>volume:
</span></span><span style=display:flex><span>  interface: NVME
</span></span><span style=display:flex><span>serviceAccount:
</span></span><span style=display:flex><span>  email: foo@bar.com
</span></span><span style=display:flex><span>  scopes:
</span></span><span style=display:flex><span>  - https://www.googleapis.com/auth/cloud-platform
</span></span><span style=display:flex><span>gpu:
</span></span><span style=display:flex><span>  acceleratorType: nvidia-tesla-t4
</span></span><span style=display:flex><span>  count: 1
</span></span></code></pre></div><h2 id=example-shoot-manifest>Example <code>Shoot</code> manifest</h2><p>Please find below an example <code>Shoot</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-gcp
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: gcp
</span></span><span style=display:flex><span>  region: europe-west1
</span></span><span style=display:flex><span>  secretBindingName: core-gcp
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: gcp
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        workers: 10.250.0.0/16
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>      zone: europe-west1-b
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: n1-standard-4
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: pd-standard
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - europe-west1-b
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=csi-volume-provisioners>CSI volume provisioners</h2><p>Every GCP shoot cluster will be deployed with the GCP PD CSI driver.
It is compatible with the legacy in-tree volume provisioner that was deprecated by the Kubernetes community and will be removed in future versions of Kubernetes.
End-users might want to update their custom <code>StorageClass</code>es to the new <code>pd.csi.storage.gke.io</code> provisioner.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-gcp@v1.21</code>.</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> and <code>ShootSARotation</code> feature gates since <code>gardener-extension-provider-gcp@v1.23</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-b46674e0cabcf6e848e4d419e8c17465>1.6 - Provider Openstack</h1><div class=lead>Gardener extension controller for the OpenStack cloud provider</div><h1 id=gardener-extension-for-openstack-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for OpenStack provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener-tests/pipelines/gardener-extension-provider-openstack-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener-tests/pipelines/gardener-extension-provider-openstack-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-openstack><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-openstack alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the OpenStack provider.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-openstack/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.28</td><td>1.28.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.27</td><td>1.27.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.27%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.27%20OpenStack/tests_status?style=svg" alt="Gardener v1.27 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.26</td><td>1.26.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.26%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.26%20OpenStack/tests_status?style=svg" alt="Gardener v1.26 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.25</td><td>1.25.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20OpenStack/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.24</td><td>1.24.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20OpenStack><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20OpenStack/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td></tr></tbody></table><p>Please take a look <a href=/docs/gardener/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=compatibility>Compatibility</h2><p>The following lists known compatibility issues of this extension controller with other Gardener components.</p><table><thead><tr><th>OpenStack Extension</th><th>Gardener</th><th>Action</th><th>Notes</th></tr></thead><tbody><tr><td><code>&lt; v1.12.0</code></td><td><code>> v1.10.0</code></td><td>Please update the provider version to <code>>= v1.12.0</code> or disable the feature gate <code>MountHostCADirectories</code> in the Gardenlet.</td><td>Applies if feature flag <code>MountHostCADirectories</code> in the Gardenlet is enabled. This is to prevent duplicate volume mounts to <code>/usr/share/ca-certificates</code> in the Shoot API Server.</td></tr></tbody></table><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-openstack/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0f4e10642c2c30cf21153633c30e4808>1.6.1 - Deployment</h1><h1 id=deployment-of-the-openstack-provider-extension>Deployment of the OpenStack provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the OpenStack provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the OpenStack provider extension <a href=https://github.com/gardener/gardener-extension-provider-openstack>repository</a>.</p><h2 id=gardener-extension-admission-openstack>gardener-extension-admission-openstack</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-openstack</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-ee3a6965a7798320d3839fae1d5ac560>1.6.2 - Local Setup</h1><h3 id=admission-openstack>admission-openstack</h3><p><code>admission-openstack</code> is an admission webhook server which is responsible for the validation of the cloud provider (OpenStack in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-admission
</span></span></code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-openstack.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/dev-setup-admission-openstack.sh
</span></span></code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-openstack</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3df166b7d9a610b7b2c577e05a949958>1.6.3 - Operations</h1><h1 id=using-the-openstack-provider-extension-with-gardener-as-operator>Using the OpenStack provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for OpenStack and provide an example <code>CloudProfile</code> manifest with minimal configuration that you can use to allow creating OpenStack shoot clusters.</p><h2 id=cloudprofileconfig><code>CloudProfileConfig</code></h2><p>The cloud profile configuration contains information about the real machine image IDs in the OpenStack environment (image names).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the OpenStack extension knows the image ID for every version you want to offer.</p><p>It also contains optional default values for DNS servers that shall be used for shoots.
In the <code>dnsServers[]</code> list you can specify IP addresses that are used as DNS configuration for created shoot subnets.</p><p>Also, you have to specify the keystone URL in the <code>keystoneURL</code> field to your environment.</p><p>Additionally, you can influence the HTTP request timeout when talking to the OpenStack API in the <code>requestTimeout</code> field.
This may help when you have for example a long list of load balancers in your environment.</p><p>In case your OpenStack system uses <a href=https://docs.openstack.org/octavia/latest/>Octavia</a> for network load balancing then you have to set the <code>useOctavia</code> field to <code>true</code> such that the cloud-controller-manager for OpenStack gets correctly configured (it defaults to <code>false</code>).</p><p>Some hypervisors (especially those which are VMware-based) don&rsquo;t automatically send a new volume size to a Linux kernel when a volume is resized and in-use.
For those hypervisors you can enable the storage plugin interacting with Cinder to telling the SCSI block device to refresh its information to provide information about it&rsquo;s updated size to the kernel. You might need to enable this behavior depending on the underlying hypervisor of your OpenStack installation. The <code>rescanBlockStorageOnResize</code> field controls this. Please note that it only applies for Kubernetes versions where CSI is used.</p><p>Some openstack configurations do not allow to attach more volumes than a specific amount to a single node.
To tell the k8s scheduler to not over schedule volumes on a node, you can set <code>nodeVolumeAttachLimit</code> which defaults to 256.
Some openstack configurations have different names for volume and compute availability zones, which might cause pods to go into pending state as there are no nodes available in the detected volume AZ. To ignore the volume AZ when scheduling pods, you can set <code>ignoreVolumeAZ</code> to <code>true</code> (it defaults to <code>false</code>).
See <a href=https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md#block-storage>CSI Cinder driver</a>.</p><p>The cloud profile config also contains constraints for floating pools and load balancer providers that can be used in shoots.</p><p>If your OpenStack system supports server groups, the <code>serverGroupPolicies</code> property will enable your end-users to create shoots with workers where the nodes are managed by Nova&rsquo;s server groups.
Specifying <code>serverGroupPolicies</code> is optional and can be omitted. If enabled, the end-user can choose whether or not to use this feature for a shoot&rsquo;s workers. Gardener will handle the creation of the server group and node assignment.</p><p>To enable this feature, an operator should:</p><ul><li>specify the allowed policy values (e.g. <code>affintity</code>, <code>anti-affinity</code>) in this section. Only the policies in the allow-list will be available for end-users.</li><li>make sure that your OpenStack project has enough server group capacity. Otherwise, shoot creation will fail.</li></ul><p>If your OpenStack system has multiple <code>volume-types</code>, the <code>storageClasses</code> property enables the creation of kubernetes <code>storageClasses</code> for shoots.
Set <code>storageClasses[].parameters.type</code> to map it with an openstack <code>volume-type</code>. Specifying <code>storageClasses</code> is optional and can be omitted.</p><p>An example <code>CloudProfileConfig</code> for the OpenStack extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: coreos
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 2135.6.0
</span></span><span style=display:flex><span>    <span style=color:green># Fallback to image name if no region mapping is found</span>
</span></span><span style=display:flex><span>    <span style=color:green># Only works for amd64 and is strongly discouraged. Prefer image IDs!</span>
</span></span><span style=display:flex><span>    image: coreos-2135.6.0
</span></span><span style=display:flex><span>    regions:
</span></span><span style=display:flex><span>    - name: europe
</span></span><span style=display:flex><span>      id: <span style=color:#a31515>&#34;1234-amd64&#34;</span>
</span></span><span style=display:flex><span>      architecture: amd64 <span style=color:green># optional, defaults to amd64</span>
</span></span><span style=display:flex><span>    - name: europe
</span></span><span style=display:flex><span>      id: <span style=color:#a31515>&#34;1234-arm64&#34;</span>
</span></span><span style=display:flex><span>      architecture: arm64
</span></span><span style=display:flex><span>    - name: asia
</span></span><span style=display:flex><span>      id: <span style=color:#a31515>&#34;5678-amd64&#34;</span>
</span></span><span style=display:flex><span>      architecture: amd64
</span></span><span style=display:flex><span><span style=color:green># keystoneURL: https://url-to-keystone/v3/</span>
</span></span><span style=display:flex><span><span style=color:green># keystoneURLs:</span>
</span></span><span style=display:flex><span><span style=color:green># - region: europe</span>
</span></span><span style=display:flex><span><span style=color:green>#   url: https://europe.example.com/v3/</span>
</span></span><span style=display:flex><span><span style=color:green># - region: asia</span>
</span></span><span style=display:flex><span><span style=color:green>#   url: https://asia.example.com/v3/</span>
</span></span><span style=display:flex><span><span style=color:green># dnsServers:</span>
</span></span><span style=display:flex><span><span style=color:green># - 10.10.10.11</span>
</span></span><span style=display:flex><span><span style=color:green># - 10.10.10.12</span>
</span></span><span style=display:flex><span><span style=color:green># requestTimeout: 60s</span>
</span></span><span style=display:flex><span><span style=color:green># useOctavia: true</span>
</span></span><span style=display:flex><span><span style=color:green># useSNAT: true</span>
</span></span><span style=display:flex><span><span style=color:green># rescanBlockStorageOnResize: true</span>
</span></span><span style=display:flex><span><span style=color:green># ignoreVolumeAZ: true</span>
</span></span><span style=display:flex><span><span style=color:green># nodeVolumeAttachLimit: 30</span>
</span></span><span style=display:flex><span><span style=color:green># serverGroupPolicies:</span>
</span></span><span style=display:flex><span><span style=color:green># - soft-anti-affinity</span>
</span></span><span style=display:flex><span><span style=color:green># - anti-affinity</span>
</span></span><span style=display:flex><span><span style=color:green># resolvConfOptions:</span>
</span></span><span style=display:flex><span><span style=color:green># - rotate</span>
</span></span><span style=display:flex><span><span style=color:green># - timeout:1</span>
</span></span><span style=display:flex><span><span style=color:green># storageClasses:</span>
</span></span><span style=display:flex><span><span style=color:green># - name: example-sc</span>
</span></span><span style=display:flex><span><span style=color:green>#   default: false</span>
</span></span><span style=display:flex><span><span style=color:green>#   provisioner: cinder.csi.openstack.org</span>
</span></span><span style=display:flex><span><span style=color:green>#   volumeBindingMode: WaitForFirstConsumer</span>
</span></span><span style=display:flex><span><span style=color:green>#   parameters:</span>
</span></span><span style=display:flex><span><span style=color:green>#     type: storage_premium_perf0</span>
</span></span><span style=display:flex><span>constraints:
</span></span><span style=display:flex><span>  floatingPools:
</span></span><span style=display:flex><span>  - name: fp-pool-1
</span></span><span style=display:flex><span><span style=color:green>#   region: europe</span>
</span></span><span style=display:flex><span><span style=color:green>#   loadBalancerClasses:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - name: lb-class-1</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingSubnetID: &#34;1234&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingNetworkID: &#34;4567&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     subnetID: &#34;7890&#34;</span>
</span></span><span style=display:flex><span><span style=color:green># - name: &#34;fp-pool-*&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#   region: europe</span>
</span></span><span style=display:flex><span><span style=color:green>#   loadBalancerClasses:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - name: lb-class-1</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingSubnetID: &#34;1234&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingNetworkID: &#34;4567&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     subnetID: &#34;7890&#34;</span>
</span></span><span style=display:flex><span><span style=color:green># - name: &#34;fp-pool-eu-demo&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#   region: europe</span>
</span></span><span style=display:flex><span><span style=color:green>#   domain: demo</span>
</span></span><span style=display:flex><span><span style=color:green>#   loadBalancerClasses:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - name: lb-class-1</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingSubnetID: &#34;1234&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingNetworkID: &#34;4567&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     subnetID: &#34;7890&#34;</span>
</span></span><span style=display:flex><span><span style=color:green># - name: &#34;fp-pool-eu-dev&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#   region: europe</span>
</span></span><span style=display:flex><span><span style=color:green>#   domain: dev</span>
</span></span><span style=display:flex><span><span style=color:green>#   nonConstraining: true</span>
</span></span><span style=display:flex><span><span style=color:green>#   loadBalancerClasses:</span>
</span></span><span style=display:flex><span><span style=color:green>#   - name: lb-class-1</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingSubnetID: &#34;1234&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     floatingNetworkID: &#34;4567&#34;</span>
</span></span><span style=display:flex><span><span style=color:green>#     subnetID: &#34;7890&#34;</span>
</span></span><span style=display:flex><span>  loadBalancerProviders:
</span></span><span style=display:flex><span>  - name: haproxy
</span></span><span style=display:flex><span><span style=color:green>#   region: europe</span>
</span></span><span style=display:flex><span><span style=color:green># - name: f5</span>
</span></span><span style=display:flex><span><span style=color:green>#   region: asia</span>
</span></span></code></pre></div><p>Please note that it is possible to configure a region mapping for keystone URLs, floating pools, and load balancer providers.
Additionally, floating pools can be constrainted to a keystone domain by specifying the <code>domain</code> field.
Floating pool names may also contains simple wildcard expressions, like <code>*</code> or <code>fp-pool-*</code> or <code>*-fp-pool</code>. Please note that the <code>*</code> must be either single or at the beginning or at the end. Consequently, <code>fp-*-pool</code> is not possible/allowed.
The default behavior is that, if found, the regional (and/or domain restricted) entry is taken.
If no entry for the given region exists then the fallback value is the most matching entry (w.r.t. wildcard matching) in the list without a <code>region</code> field (or the <code>keystoneURL</code> value for the keystone URLs).
If an additional floating pool should be selectable for a region and/or domain, you can mark it as non constraining
with setting the optional field <code>nonConstraining</code> to <code>true</code>.</p><p>The <code>loadBalancerClasses</code> field is an optional list of load balancer classes which can be when the corresponding floating pool network is choosen. The load balancer classes can be configured in the same way as in the <code>ControlPlaneConfig</code> in the <code>Shoot</code> resource, therefore see <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-openstack/usage/#ControlPlaneConfig>here</a> for more details.</p><p>Some OpenStack environments don&rsquo;t need these regional mappings, hence, the <code>region</code> and <code>keystoneURLs</code> fields are optional.
If your OpenStack environment only has regional values and it doesn&rsquo;t make sense to provide a (non-regional) fallback then simply
omit <code>keystoneURL</code> and always specify <code>region</code>.</p><p>If Gardener creates and manages the router of a shoot cluster, it is additionally possible to specify that the <a href=https://registry.terraform.io/providers/terraform-provider-openstack/openstack/latest/docs/resources/networking_router_v2#enable_snat>enable_snat</a> field is set to <code>true</code> via <code>useSNAT: true</code> in the <code>CloudProfileConfig</code>.</p><p>On some OpenStack enviroments, there may be the need to set options in the file <code>/etc/resolv.conf</code> on worker nodes.
If the field <code>resolvConfOptions</code> is set, a systemd service will be installed which copies <code>/run/systemd/resolve/resolv.conf</code>
on every change to <code>/etc/resolv.conf</code> and appends the given options.</p><h2 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h2><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: openstack
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: openstack
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.27.3
</span></span><span style=display:flex><span>    - version: 1.26.8
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2022-10-31T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2135.6.0
</span></span><span style=display:flex><span>      architectures: <span style=color:green># optional, defaults to [amd64]</span>
</span></span><span style=display:flex><span>      - amd64
</span></span><span style=display:flex><span>      - arm64
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: medium_4_8
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;4&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>    architecture: amd64 <span style=color:green># optional, defaults to amd64</span>
</span></span><span style=display:flex><span>    storage:
</span></span><span style=display:flex><span>      class: standard
</span></span><span style=display:flex><span>      type: default
</span></span><span style=display:flex><span>      size: 40Gi
</span></span><span style=display:flex><span>  - name: medium_4_8_arm
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;4&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>    architecture: arm64
</span></span><span style=display:flex><span>    storage:
</span></span><span style=display:flex><span>      class: standard
</span></span><span style=display:flex><span>      type: default
</span></span><span style=display:flex><span>      size: 40Gi
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - name: europe-1
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - name: europe-1a
</span></span><span style=display:flex><span>    - name: europe-1b
</span></span><span style=display:flex><span>    - name: europe-1c
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: coreos
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 2135.6.0
</span></span><span style=display:flex><span>        <span style=color:green># Fallback to image name if no region mapping is found</span>
</span></span><span style=display:flex><span>        <span style=color:green># Only works for amd64 and is strongly discouraged. Prefer image IDs!</span>
</span></span><span style=display:flex><span>        image: coreos-2135.6.0
</span></span><span style=display:flex><span>        regions:
</span></span><span style=display:flex><span>        - name: europe
</span></span><span style=display:flex><span>          id: <span style=color:#a31515>&#34;1234-amd64&#34;</span>
</span></span><span style=display:flex><span>          architecture: amd64 <span style=color:green># optional, defaults to amd64</span>
</span></span><span style=display:flex><span>        - name: europe
</span></span><span style=display:flex><span>          id: <span style=color:#a31515>&#34;1234-arm64&#34;</span>
</span></span><span style=display:flex><span>          architecture: arm64
</span></span><span style=display:flex><span>        - name: asia
</span></span><span style=display:flex><span>          id: <span style=color:#a31515>&#34;5678-amd64&#34;</span>
</span></span><span style=display:flex><span>          architecture: amd64
</span></span><span style=display:flex><span>    keystoneURL: https://url-to-keystone/v3/
</span></span><span style=display:flex><span>    constraints:
</span></span><span style=display:flex><span>      floatingPools:
</span></span><span style=display:flex><span>      - name: fp-pool-1
</span></span><span style=display:flex><span>      loadBalancerProviders:
</span></span><span style=display:flex><span>      - name: haproxy
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-549286e04ceb30d6fbbbb06051934d8c>1.6.4 - Usage</h1><h1 id=using-the-openstack-provider-extension-with-gardener-as-end-user>Using the OpenStack provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for OpenStack and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create an OpenStack cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=provider-secret-data>Provider Secret Data</h2><p>Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of your OpenStack tenant.
This <code>Secret</code> must look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-openstack
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  domainName: base64(domain-name)
</span></span><span style=display:flex><span>  tenantName: base64(tenant-name)
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>  <span style=color:green># either use username/password</span>
</span></span><span style=display:flex><span>  username: base64(user-name)
</span></span><span style=display:flex><span>  password: base64(password)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># or application credentials</span>
</span></span><span style=display:flex><span>  <span style=color:green>#applicationCredentialID: base64(app-credential-id)</span>
</span></span><span style=display:flex><span>  <span style=color:green>#applicationCredentialName: base64(app-credential-name) # optional</span>
</span></span><span style=display:flex><span>  <span style=color:green>#applicationCredentialSecret: base64(app-credential-secret)</span>
</span></span></code></pre></div><p>Please look up <a href=https://docs.openstack.org/keystone/pike/admin/identity-concepts.html>https://docs.openstack.org/keystone/pike/admin/identity-concepts.html</a> as well.</p><p>For authentication with username/password see <a href=https://docs.openstack.org/keystone/latest/user/supported_clients.html>Keystone username/password</a></p><p>Alternatively, for authentication with application credentials see <a href=https://docs.openstack.org/keystone/latest/user/application_credentials.html>Keystone Application Credentials</a>.</p><p>⚠️ Depending on your API usage it can be problematic to reuse the same provider credentials for different Shoot clusters due to rate limits.
Please consider spreading your Shoots over multiple credentials from different tenants if you are hitting those limits.</p><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the OpenStack extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span><span style=display:flex><span>floatingPoolName: MY-FLOATING-POOL
</span></span><span style=display:flex><span><span style=color:green># floatingPoolSubnetName: my-floating-pool-subnet-name</span>
</span></span><span style=display:flex><span>networks:
</span></span><span style=display:flex><span><span style=color:green># id: 12345678-abcd-efef-08af-0123456789ab</span>
</span></span><span style=display:flex><span><span style=color:green># router:</span>
</span></span><span style=display:flex><span><span style=color:green>#   id: 1234</span>
</span></span><span style=display:flex><span>  workers: 10.250.0.0/19
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green># shareNetwork:</span>
</span></span><span style=display:flex><span><span style=color:green>#   enabled: true</span>
</span></span></code></pre></div><p>The <code>floatingPoolName</code> is the name of the floating pool you want to use for your shoot.
If you don&rsquo;t know which floating pools are available look it up in the respective <code>CloudProfile</code>.</p><p>With <code>floatingPoolSubnetName</code> you can explicitly define to which subnet in the floating pool network (defined via <code>floatingPoolName</code>) the router should be attached to.</p><p><code>networks.id</code> is an optional field. If it is given, you can specify the uuid of an existing private Neutron network (created manually, by other tooling, &mldr;) that should be reused. A new subnet for the Shoot will be created in it.</p><p>If a <code>networks.id</code> is given and calico shoot clusters are created without a network overlay within one network make sure that the pod CIDR specified in <code>shoot.spec.networking.pods</code> is not overlapping with any other pod CIDR used in that network.
Overlapping pod CIDRs will lead to disfunctional shoot clusters.</p><p>The <code>networks.router</code> section describes whether you want to create the shoot cluster in an already existing router or whether to create a new one:</p><ul><li><p>If <code>networks.router.id</code> is given then you have to specify the router id of the existing router that was created by other means (manually, other tooling, &mldr;).
If you want to get a fresh router for the shoot then just omit the <code>networks.router</code> field.</p></li><li><p>In any case, the shoot cluster will be created in a <strong>new</strong> subnet.</p></li></ul><p>The <code>networks.workers</code> section describes the CIDR for a subnet that is used for all shoot worker nodes, i.e., VMs which later run your applications.</p><p>You can freely choose these CIDRs and it is your responsibility to properly design the network layout to suit your needs.</p><p>Apart from the router and the worker subnet the OpenStack extension will also create a network, router interfaces, security groups, and a key pair.</p><p>The optional <code>networks.shareNetwork.enabled</code> field controls the creation of a share network. This is only needed if shared
file system storage (like NFS) should be used. Note, that in this case, the <code>ControlPlaneConfig</code> needs additional configuration, too.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the OpenStack-specific control plane components.
Today, the only component deployed by the OpenStack extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the OpenStack extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span><span style=display:flex><span>loadBalancerProvider: haproxy
</span></span><span style=display:flex><span>loadBalancerClasses:
</span></span><span style=display:flex><span>- name: lbclass-1
</span></span><span style=display:flex><span>  purpose: default
</span></span><span style=display:flex><span>  floatingNetworkID: fips-1-id
</span></span><span style=display:flex><span>  floatingSubnetName: internet-*
</span></span><span style=display:flex><span>- name: lbclass-2
</span></span><span style=display:flex><span>  floatingNetworkID: fips-1-id
</span></span><span style=display:flex><span>  floatingSubnetTags: internal,private
</span></span><span style=display:flex><span>- name: lbclass-3
</span></span><span style=display:flex><span>  purpose: private
</span></span><span style=display:flex><span>  subnetID: internal-id
</span></span><span style=display:flex><span>cloudControllerManager:
</span></span><span style=display:flex><span>  featureGates:
</span></span><span style=display:flex><span>    RotateKubeletServerCertificate: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span><span style=color:green>#storage:</span>
</span></span><span style=display:flex><span><span style=color:green>#  csiManila:</span>
</span></span><span style=display:flex><span><span style=color:green>#    enabled: true</span>
</span></span></code></pre></div><p>The <code>loadBalancerProvider</code> is the provider name you want to use for load balancers in your shoot.
If you don&rsquo;t know which types are available look it up in the respective <code>CloudProfile</code>.</p><p>The <code>loadBalancerClasses</code> field contains an optional list of load balancer classes which will be available in the cluster. Each entry can have the following fields:</p><ul><li><code>name</code> to select the load balancer class via the kubernetes <a href=https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/openstack-cloud-controller-manager/expose-applications-using-loadbalancer-type-service.md#switching-between-floating-subnets-by-using-preconfigured-classes>service annotations</a> <code>loadbalancer.openstack.org/class=name</code></li><li><code>purpose</code> with values <code>default</code> or <code>private</code><ul><li>The configuration of the <code>default</code> load balancer class will be used as default for all other kubernetes loadbalancer services without a class annotation</li><li>The configuration of the <code>private</code> load balancer class will be also set to the global loadbalancer configuration of the cluster, but will be overridden by the <code>default</code> purpose</li></ul></li><li><code>floatingNetworkID</code> can be specified to receive an ip from an floating/external network, additionally the subnet in this network can be selected via<ul><li><code>floatingSubnetName</code> can be either a full subnet name or a regex/glob to match subnet name</li><li><code>floatingSubnetTags</code> a comma seperated list of subnet tags</li><li><code>floatingSubnetID</code> the id of a specific subnet</li></ul></li><li><code>subnetID</code> can be specified by to receive an ip from an internal subnet (will not have an effect in combination with floating/external network configuration)</li></ul><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommended to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><p>The optional <code>storage.csiManila.enabled</code> field is used to enable the deployment of the CSI Manila driver to support NFS persistent volumes.
In this case, please ensure to set <code>networks.shareNetwork.enabled=true</code> in the <code>InfrastructureConfig</code>, too.
Additionally, if CSI Manila driver is enabled, for each availability zone a NFS <code>StorageClass</code> will be created on the shoot
named like <code>csi-manila-nfs-&lt;zone></code>.</p><h2 id=workerconfig><code>WorkerConfig</code></h2><p>Each worker group in a shoot may contain provider-specific configurations and options. These are contained in the <code>providerConfig</code> section of a worker group and can be configured using a <code>WorkerConfig</code> object.
An example of a <code>WorkerConfig</code> looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: WorkerConfig
</span></span><span style=display:flex><span>serverGroup:
</span></span><span style=display:flex><span>  policy: soft-anti-affinity
</span></span><span style=display:flex><span><span style=color:green># nodeTemplate: # (to be specified only if the node capacity would be different from cloudprofile info during runtime)</span>
</span></span><span style=display:flex><span><span style=color:green>#   capacity:</span>
</span></span><span style=display:flex><span><span style=color:green>#     cpu: 2</span>
</span></span><span style=display:flex><span><span style=color:green>#     gpu: 0</span>
</span></span><span style=display:flex><span><span style=color:green>#     memory: 50Gi</span>
</span></span><span style=display:flex><span><span style=color:green># machineLabels:</span>
</span></span><span style=display:flex><span><span style=color:green>#  - name: my-label</span>
</span></span><span style=display:flex><span><span style=color:green>#    value: foo</span>
</span></span><span style=display:flex><span><span style=color:green>#  - name: my-rolling-label</span>
</span></span><span style=display:flex><span><span style=color:green>#    value: bar</span>
</span></span><span style=display:flex><span><span style=color:green>#    triggerRollingOnUpdate: true # means any change of the machine label value will trigger rolling of all machines of the worker pool</span>
</span></span></code></pre></div><h3 id=servergroups>ServerGroups</h3><p>When you specify the <code>serverGroup</code> section in your worker group configuration, a new server group will be created with the configured policy for each worker group that enabled this setting and all machines managed by this worker group will be assigned as members of the created server group.</p><p>For users to have access to the server group feature, it must be enabled on the <code>CloudProfile</code> by your operator.
Existing clusters can take advantage of this feature by updating the server group configuration of their respective worker groups. Worker groups that are already configured with server groups can update their setting to change the policy used, or remove it altogether at any time.</p><p>Users must be aware that <strong>any change to the server group settings will result in a rolling deployment of new nodes for the affected worker group</strong>.</p><p>Please note the following restrictions when deploying workers with server groups:</p><ul><li>The <code>serverGroup</code> section is optional, but if it is included in the worker configuration, it must contain a valid policy value.</li><li>The available <code>policy</code> values that can be used, are defined in the provider specific section of <code>CloudProfile</code> by your operator.</li><li>Certain policy values may induce further constraints. Using the <code>affinity</code> policy is only allowed when the worker group utilizes a single zone.</li></ul><h3 id=machinelabels>MachineLabels</h3><p>The <code>machineLabels</code> section in the worker group configuration allows to specify additional machine labels. These labels are added to the machine
instances only, but not to the node object. Additionally, they have an optional <code>triggerRollingOnUpdate</code> field. If it is set to <code>true</code>, changing the label value
will trigger a rolling of all machines of this worker pool.</p><h3 id=node-templates>Node Templates</h3><p>Node templates allow users to override the capacity of the nodes as defined by the server flavor specified in the <code>CloudProfile</code>&rsquo;s <code>machineTypes</code>. This is useful for certain dynamic scenarios as it allows users to customize cluster-autoscaler&rsquo;s behavior for these workergroup with their provided values.</p><h2 id=example-shoot-manifest-one-availability-zone>Example <code>Shoot</code> manifest (one availability zone)</h2><p>Please find below an example <code>Shoot</code> manifest for one availability zone:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-openstack
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: openstack
</span></span><span style=display:flex><span>  region: europe-1
</span></span><span style=display:flex><span>  secretBindingName: core-openstack
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: openstack
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      floatingPoolName: MY-FLOATING-POOL
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        workers: 10.250.0.0/19
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: openstack.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>      loadBalancerProvider: haproxy
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: medium_4_8
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - europe-1a
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=csi-volume-provisioners>CSI volume provisioners</h2><p>Every OpenStack shoot cluster will be deployed with the OpenStack Cinder CSI driver.
It is compatible with the legacy in-tree volume provisioner that was deprecated by the Kubernetes community and will be removed in future versions of Kubernetes.
End-users might want to update their custom <code>StorageClass</code>es to the new <code>cinder.csi.openstack.org</code> provisioner.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-openstack@v1.23</code>.</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> and <code>ShootSARotation</code> feature gates since <code>gardener-extension-provider-openstack@v1.26</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5b5f3488fb6707d456d4d70245b0f4ab>1.7 - Provider vSphere</h1><div class=lead>Gardener extension controller for the vSphere cloud provider</div><h1 id=gardener-extension-for-vsphere-providerhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for vSphere provider</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener-tests/pipelines/gardener-extension-provider-vsphere-main/jobs/main-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener-tests/pipelines/gardener-extension-provider-vsphere-main/jobs/main-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-provider-vsphere><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-provider-vsphere alt="Go Report Card"></a></p><h2 id=overview>Overview</h2><p>The Gardener Extension for vSphere is a <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> provider implementation that allows Gardener to leverage vSphere clusters for machine provisioning.</p><p>vSphere is an undeniable class leader for commercially supported virtual machine orchestration. The Gardener extension for vSphere provider compliments this leadership by allowing Gardener to create Kubernetes nodes within vSphere.</p><p>Like other Gardener provider extensions, the vSphere provider pairs with a provider-specific Machine Controller Manager providing node services to Kubernetes clusters. This extension provides complimentary APIs to Gardener. A Gardener-hosted Kubernetes
cluster does not know anything about it&rsquo;s environment (such as bare metal vs. public cloud or within a hyperscaler vs. standalone), only that the MCM abstraction can manage requests such as cluster autoscaling.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-provider-vsphere/blob/main/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and the architecture details in the GEP-1 proposal.</p><h2 id=use-cases>Use Cases</h2><p>The primary use case for this extension is organizations who wish to deploy a substantial Gardener landscape and use vSphere for data center fleet management. We intentionally sidestep prescribing any particular extension as this is
an intimately local determination and the benefits of different solutions are more than adequately debated in industry literature.</p><p>While we may inadvertently duplicate some documentation in the mainline Gardener documentation, it is only to reduce tedium as new evaluators and developers come up-to-speed with the concepts relevant to successful deployment.
We refer directly to the mainline Gardener documentation for the most up-to-date information.</p><h2 id=supported-kubernetes-versions>Supported Kubernetes versions</h2><p>This extension controller supports the following Kubernetes versions:</p><table><thead><tr><th>Version</th><th>Support</th><th>Conformance test results</th></tr></thead><tbody><tr><td>Kubernetes 1.28</td><td>1.28.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.27</td><td>1.27.0+</td><td>N/A</td></tr><tr><td>Kubernetes 1.26</td><td>1.26.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.26%20vSphere><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.26%20vSphere/tests_status?style=svg" alt="Gardener v1.26 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.25</td><td>1.25.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.25%20vSphere><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.25%20vSphere/tests_status?style=svg" alt="Gardener v1.25 Conformance Tests"></a></td></tr><tr><td>Kubernetes 1.24</td><td>1.24.0+</td><td><a href=https://testgrid.k8s.io/conformance-gardener#Gardener,%20v1.24%20vSphere><img src="https://testgrid.k8s.io/q/summary/conformance-gardener/Gardener,%20v1.24%20vSphere/tests_status?style=svg" alt="Gardener v1.24 Conformance Tests"></a></td></tr></tbody></table><p>Older versions of the extension <a href=https://github.com/gardener/gardener-extension-provider-vsphere/releases/tag/v0.16.0>(<code>v0.16.0</code> and earlier)</a> are supported prior to current releases.</p><p>Please take a look <a href=/docs/gardener/supported_k8s_versions/>here</a> to see which versions are supported by Gardener in general.</p><hr><h2 id=deployment-patterns>Deployment patterns</h2><p>As with any production software, deployment of Gardener and this extension should be considered in the context of both lifecycle and automation. Orgs should aspire to have apply</p><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-provider-vsphere/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/04-new-core-gardener-cloud-apis.md>GEP-4 (New <code>core.gardener.cloud/v1beta1</code> API)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6625d8fa2e07416ff46b910a7a9beaf8>1.7.1 - Create a Kubernetes Cluster on AWS with Gardener</h1><h3 id=overview>Overview</h3><p>Gardener allows you to create a Kubernetes cluster on different infrastructure providers. This tutorial will guide you through the process of creating a cluster on AWS.</p><h3 id=prerequisites>Prerequisites</h3><ul><li>You have created an <a href=https://aws.amazon.com/>AWS account</a>.</li><li>You have access to the Gardener dashboard and have permissions to create projects.</li></ul><h3 id=steps>Steps</h3><ol><li><p>Go to the Gardener dashboard and create a <em>Project</em>.</p><img src=/__resources/new-gardener-project_ad03bc.png></li><li><p>Choose <em>Secrets</em>, then the plus icon <img src=/__resources/plus-icon_3b1f20.png> and select <em>AWS</em>.</p><img src=/__resources/create-secret-aws_79dc1a.png></li><li><p>To copy the policy for AWS from the Gardener dashboard, click on the help icon <img src=/__resources/help-icon_01486c.png> for AWS secrets, and choose copy <img src=/__resources/copy-icon_0f5ab8.png>.</p><img src=/__resources/gardener-copy-policy_a52965.png></li><li><p><a href=https://console.aws.amazon.com/iam/home?#/policies>Create a new policy</a> in AWS:</p><ol><li><p>Choose <em>Create policy</em>.</p><img src=/__resources/amazon-create-policy_5ef114.png></li><li><p>Paste the policy that you copied from the Gardener dashboard to this custom policy.</p><img src=/__resources/amazon-create-policy-json_7d6327.png></li><li><p>Choose <em>Next</em> until you reach the Review section.</p></li><li><p>Fill in the name and description, then choose <em>Create policy</em>.</p><img src=/__resources/amazon-review-policy_6fba71.png></li></ol></li><li><p><a href="https://console.aws.amazon.com/iam/home?#/users$new?step=details">Create a new technical user</a> in AWS:</p><ol><li><p>Type in a username and select the access key credential type.</p><img src=/__resources/add-user_775731.png></li><li><p>Choose <em>Attach an existing policy</em>.</p></li><li><p>Select <em>GardenerAccess</em> from the policy list.</p></li><li><p>Choose <em>Next</em> until you reach the Review section.</p></li></ol><img src=/__resources/attach-policy_a6a81f.png>
<img src=/__resources/finish-user_a9e956.png><div class="alert alert-info" role=alert><h4 class=alert-heading>Note</h4>Note: After the user is created, <code>Access key ID</code> and <code>Secret access key</code> are generated and displayed. Remember to save them. The <code>Access key ID</code> is used later to create secrets for Gardener.</div><img src=/__resources/save-keys_f23816.png></li><li><p>On the Gardener dashboard, choose <em>Secrets</em> and then the plus sign <img src=/__resources/plus-icon_3b1f20.png>. Select <em>AWS</em> from the drop down menu to add a new AWS secret.</p></li><li><p>Create your secret.</p><ol><li>Type the name of your secret.</li><li>Copy and paste the <code>Access Key ID</code> and <code>Secret Access Key</code> you saved when you created the technical user on AWS.</li><li>Choose <em>Add secret</em>.
<img src=/__resources/add-aws-secret_ed47ad.png></li></ol><blockquote><p>After completing these steps, you should see your newly created secret in the <em>Infrastructure Secrets</em> section.</p></blockquote><img src=/__resources/secret-stored_a4c7f9.png></li><li><p>To create a new cluster, choose <em>Clusters</em> and then the plus sign in the upper right corner.</p><img src=/__resources/new-cluster_353d7b.png></li><li><p>In the <em>Create Cluster</em> section:</p><ol><li>Select <em>AWS</em> in the <em>Infrastructure</em> tab.</li><li>Type the name of your cluster in the <em>Cluster Details</em> tab.</li><li>Choose the secret you created before in the <em>Infrastructure Details</em> tab.</li><li>Choose <em>Create</em>.</li></ol><img src=/__resources/create-cluster_7a45a2.png></li><li><p>Wait for your cluster to get created.</p><img src=/__resources/processing-cluster_522005.png></li></ol><h3 id=result>Result</h3><p>After completing the steps in this tutorial, you will be able to see and download the kubeconfig of your cluster.</p><img src=/__resources/copy-kubeconfig_752d59.png></div><div class=td-content style=page-break-before:always><h1 id=pg-fb380a4678d57961238994fbd6422353>1.7.2 - Deployment</h1><h1 id=deployment-of-the-aws-provider-extension>Deployment of the AWS provider extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step installation guide for the AWS provider extension and only contains some configuration specifics regarding the installation of different components via the helm charts residing in the AWS provider extension <a href=https://github.com/gardener/gardener-extension-provider-aws>repository</a>.</p><h2 id=gardener-extension-admission-aws>gardener-extension-admission-aws</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of <em>Virtual Garden</em></a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster><em>Virtual Garden</em> is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><p><strong>Automounted Service Account Token</strong>
The easiest way to deploy the <code>gardener-extension-admission-aws</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><p><strong>Service Account Token Volume Projection</strong>
Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster><em>Virtual Garden</em> is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><p><strong>Service Account</strong>
The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Client Certificate</strong>
Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><p><strong>Projected Service Account Token</strong>
This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-f7b5d7a9c2adfc452fc679dfaa616375>1.7.3 - Dual Stack Ingress</h1><h1 id=using-ipv4ipv6-dual-stack-ingress-in-an-ipv4-single-stack-cluster>Using IPv4/IPv6 (dual-stack) Ingress in an IPv4 single-stack cluster</h1><h2 id=motivation>Motivation</h2><p>IPv6 adoption is continuously growing, already overtaking IPv4 in certain regions, e.g. India, or scenarios, e.g. mobile.
Even though most IPv6 installations deploy means to reach IPv4, it might still be beneficial to expose services
natively via IPv4 and IPv6 instead of just relying on IPv4.</p><h2 id=disadvantages-of-full-ipv4ipv6-dual-stack-deployments>Disadvantages of full IPv4/IPv6 (dual-stack) Deployments</h2><p>Enabling full IPv4/IPv6 (dual-stack) support in a kubernetes cluster is a major endeavor. It requires a lot of changes
and restarts of all pods so that all pods get addresses for both IP families. A side-effect of dual-stack networking
is that failures may be hidden as network traffic may take the other protocol to reach the target. For this reason and
also due to reduced operational complexity, service teams might lean towards staying in a single-stack environment as
much as possible. Luckily, this is possible with Gardener and IPv4/IPv6 (dual-stack) ingress on AWS.</p><h2 id=simplifying-ipv4ipv6-dual-stack-ingress-with-protocol-translation-on-aws>Simplifying IPv4/IPv6 (dual-stack) Ingress with Protocol Translation on AWS</h2><p>Fortunately, the network load balancer on AWS supports automatic protocol translation, i.e. it can expose both IPv4 and
IPv6 endpoints while communicating with just one protocol to the backends. Under the hood, automatic protocol translation
takes place. Client IP address preservation can be achieved by using proxy protocol.</p><p>This approach enables users to expose IPv4 workload to IPv6-only clients without having to change the workload/service.
Without requiring invasive changes, it allows a fairly simple first step into the IPv6 world for services just requiring
ingress (incoming) communication.</p><h2 id=necessary-shoot-cluster-configuration-changes-for-ipv4ipv6-dual-stack-ingress>Necessary Shoot Cluster Configuration Changes for IPv4/IPv6 (dual-stack) Ingress</h2><p>To be able to utilize IPv4/IPv6 (dual-stack) Ingress in an IPv4 shoot cluster, the cluster needs to meet two preconditions:</p><ol><li><code>dualStack.enabled</code> needs to be set to <code>true</code> to configure VPC/subnet for IPv6 and add a routing rule for IPv6.
(This does not add IPv6 addresses to kubernetes nodes.)</li><li><code>loadBalancerController.enabled</code> needs to be set to <code>true</code> as well to use the load balancer controller, which supports
dual-stack ingress.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      dualStack:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>        loadBalancerController:
</span></span><span style=display:flex><span>          enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>After adapting the shoot specification and reconciling the cluster, dual-stack load balancers can be created using
kubernetes services objects.</p><h2 id=creating-an-ipv4ipv6-dual-stack-ingress>Creating an IPv4/IPv6 (dual-stack) Ingress</h2><p>With the preconditions set, creating an IPv4/IPv6 load balancer is as easy as annotating a service with the correct
annotations:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-ip-address-type: dualstack
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-type: external
</span></span><span style=display:flex><span>  name: ...
</span></span><span style=display:flex><span>  namespace: ...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  type: LoadBalancer
</span></span></code></pre></div><p>In case the client IP address should be preserved, the following annotation can be used to enable proxy protocol.
(The pod receiving the traffic needs to be configured for proxy protocol as well.)</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: <span style=color:#a31515>&#34;*&#34;</span>
</span></span></code></pre></div><p>Please note that changing an existing <code>Service</code> to dual-stack may cause the creation of a new load balancer without
deletion of the old AWS load balancer resource. While this helps in a seamless migration by not cutting existing
connections it may lead to wasted/forgotten resources. Therefore, the (manual) cleanup needs to be taken into account
when migrating an existing <code>Service</code> instance.</p><p>For more details see <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/service/nlb/>AWS Load Balancer Documentation - Network Load Balancer</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-15c565e4de0da3bdaed0abb444451fab>1.7.4 - Local Setup</h1><h3 id=admission-aws>admission-aws</h3><p><code>admission-aws</code> is an admission webhook server which is responsible for the validation of the cloud provider (AWS in this case) specific fields and resources. The Gardener API server is cloud provider agnostic and it wouldn&rsquo;t be able to perform similar validation.</p><p>Follow the steps below to run the admission webhook server locally.</p><ol><li><p>Start the Gardener API server.</p><p>For details, check the Gardener <a href=/docs/gardener/local_setup/>local setup</a>.</p></li><li><p>Start the webhook server</p><p>Make sure that the <code>KUBECONFIG</code> environment variable is pointing to the local garden cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make start-admission
</span></span></code></pre></div></li><li><p>Setup the <code>ValidatingWebhookConfiguration</code>.</p><p><code>hack/dev-setup-admission-aws.sh</code> will configure the webhook Service which will allow the kube-apiserver of your local cluster to reach the webhook server. It will also apply the <code>ValidatingWebhookConfiguration</code> manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>./hack/dev-setup-admission-aws.sh
</span></span></code></pre></div></li></ol><p>You are now ready to experiment with the <code>admission-aws</code> webhook server locally.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4847b5da5877c1607c2563863951d737>1.7.5 - Operations</h1><h1 id=using-the-aws-provider-extension-with-gardener-as-operator>Using the AWS provider extension with Gardener as operator</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/30-cloudprofile.yaml><code>core.gardener.cloud/v1beta1.CloudProfile</code> resource</a> declares a <code>providerConfig</code> field that is meant to contain provider-specific configuration.
Similarly, the <a href=https://github.com/gardener/gardener/blob/master/example/50-seed.yaml><code>core.gardener.cloud/v1beta1.Seed</code> resource</a> is structured.
Additionally, it allows to configure settings for the backups of the main etcds&rsquo; data of shoot clusters control planes running in this seed cluster.</p><p>This document explains what is necessary to configure for this provider extension.</p><h2 id=cloudprofile-resource><code>CloudProfile</code> resource</h2><p>In this section we are describing how the configuration for <code>CloudProfile</code>s looks like for AWS and provide an example <code>CloudProfile</code> manifest with minimal configuration that you can use to allow creating AWS shoot clusters.</p><h3 id=cloudprofileconfig><code>CloudProfileConfig</code></h3><p>The cloud profile configuration contains information about the real machine image IDs in the AWS environment (AMIs).
You have to map every version that you specify in <code>.spec.machineImages[].versions</code> here such that the AWS extension knows the AMI for every version you want to offer.
For each AMI an <code>architecture</code> field can be specified which specifies the CPU architecture of the machine on which given machine image can be used.</p><p>An example <code>CloudProfileConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: CloudProfileConfig
</span></span><span style=display:flex><span>machineImages:
</span></span><span style=display:flex><span>- name: coreos
</span></span><span style=display:flex><span>  versions:
</span></span><span style=display:flex><span>  - version: 2135.6.0
</span></span><span style=display:flex><span>    regions:
</span></span><span style=display:flex><span>    - name: eu-central-1
</span></span><span style=display:flex><span>      ami: ami-034fd8c3f4026eb39
</span></span><span style=display:flex><span>      <span style=color:green># architecture: amd64 # optional</span>
</span></span></code></pre></div><h3 id=example-cloudprofile-manifest>Example <code>CloudProfile</code> manifest</h3><p>Please find below an example <code>CloudProfile</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: CloudProfile
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: aws
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: aws
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 1.27.3
</span></span><span style=display:flex><span>    - version: 1.26.8
</span></span><span style=display:flex><span>      expirationDate: <span style=color:#a31515>&#34;2022-10-31T23:59:59Z&#34;</span>
</span></span><span style=display:flex><span>  machineImages:
</span></span><span style=display:flex><span>  - name: coreos
</span></span><span style=display:flex><span>    versions:
</span></span><span style=display:flex><span>    - version: 2135.6.0
</span></span><span style=display:flex><span>  machineTypes:
</span></span><span style=display:flex><span>  - name: m5.large
</span></span><span style=display:flex><span>    cpu: <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>    gpu: <span style=color:#a31515>&#34;0&#34;</span>
</span></span><span style=display:flex><span>    memory: 8Gi
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  volumeTypes:
</span></span><span style=display:flex><span>  - name: gp2
</span></span><span style=display:flex><span>    class: standard
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  - name: io1
</span></span><span style=display:flex><span>    class: premium
</span></span><span style=display:flex><span>    usable: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  regions:
</span></span><span style=display:flex><span>  - name: eu-central-1
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - name: eu-central-1a
</span></span><span style=display:flex><span>    - name: eu-central-1b
</span></span><span style=display:flex><span>    - name: eu-central-1c
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: CloudProfileConfig
</span></span><span style=display:flex><span>    machineImages:
</span></span><span style=display:flex><span>    - name: coreos
</span></span><span style=display:flex><span>      versions:
</span></span><span style=display:flex><span>      - version: 2135.6.0
</span></span><span style=display:flex><span>        regions:
</span></span><span style=display:flex><span>        - name: eu-central-1
</span></span><span style=display:flex><span>          ami: ami-034fd8c3f4026eb39
</span></span><span style=display:flex><span>          <span style=color:green># architecture: amd64 # optional</span>
</span></span></code></pre></div><h2 id=seed-resource><code>Seed</code> resource</h2><p>This provider extension does not support any provider configuration for the <code>Seed</code>&rsquo;s <code>.spec.provider.providerConfig</code> field.
However, it supports to manage backup infrastructure, i.e., you can specify configuration for the <code>.spec.backup</code> field.</p><h3 id=backup-configuration>Backup configuration</h3><p>Please find below an example <code>Seed</code> manifest (partly) that configures backups.
As you can see, the location/region where the backups will be stored can be different to the region where the seed cluster is running.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: backup-credentials
</span></span><span style=display:flex><span>  namespace: garden
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  accessKeyID: base64(access-key-id)
</span></span><span style=display:flex><span>  secretAccessKey: base64(secret-access-key)
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Seed
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-seed
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    region: eu-west-1
</span></span><span style=display:flex><span>  backup:
</span></span><span style=display:flex><span>    provider: aws
</span></span><span style=display:flex><span>    region: eu-central-1
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: backup-credentials
</span></span><span style=display:flex><span>      namespace: garden
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>Please look up <a href=https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys</a> as well.</p><h4 id=permissions-for-aws-iam-user>Permissions for AWS IAM user</h4><p>Please make sure that the provided credentials have the correct privileges. You can use the following AWS IAM policy document and attach it to the IAM user backed by the credentials you provided (please check the <a href=http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html>official AWS documentation</a> as well):</p><details><summary>Click to expand the AWS IAM policy document!</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>  &#34;Statement&#34;: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: <span style=color:#a31515>&#34;s3:*&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></details></div><div class=td-content style=page-break-before:always><h1 id=pg-0862de020519502986512f43ae3f3599>1.7.6 - Usage</h1><h1 id=using-the-aws-provider-extension-with-gardener-as-end-user>Using the AWS provider extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that are meant to contain provider-specific configuration.</p><p>In this document we are describing how this configuration looks like for AWS and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create an AWS cluster (modulo the landscape-specific information like cloud profile names, secret binding names, etc.).</p><h2 id=provider-secret-data>Provider Secret Data</h2><p>Every shoot cluster references a <code>SecretBinding</code> which itself references a <code>Secret</code>, and this <code>Secret</code> contains the provider credentials of your AWS account.
This <code>Secret</code> must look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: core-aws
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  accessKeyID: base64(access-key-id)
</span></span><span style=display:flex><span>  secretAccessKey: base64(secret-access-key)
</span></span></code></pre></div><p>The <a href=https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>AWS documentation</a> explains the necessary steps to enable programmatic access, i.e. create <strong>access key ID</strong> and <strong>access key</strong>, for the user of your choice.</p><p>⚠️ For security reasons, we recommend creating a <strong>dedicated user with programmatic access only</strong>. Please avoid re-using a IAM user which has access to the AWS console (human user).</p><p>⚠️ Depending on your AWS API usage it can be problematic to reuse the same AWS Account for different Shoot clusters in the same region due to rate limits. Please consider spreading your Shoots over multiple AWS Accounts if you are hitting those limits.</p><h3 id=permissions>Permissions</h3><p>Please make sure that the provided credentials have the correct privileges. You can use the following AWS IAM policy document and attach it to the IAM user backed by the credentials you provided (please check the <a href=http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage.html>official AWS documentation</a> as well):</p><details><summary>Click to expand the AWS IAM policy document!</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>  &#34;Statement&#34;: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: <span style=color:#a31515>&#34;autoscaling:*&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: <span style=color:#a31515>&#34;ec2:*&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: <span style=color:#a31515>&#34;elasticloadbalancing:*&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Action&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetPolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetPolicyVersion&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:ListPolicyVersions&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:ListRolePolicies&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:ListAttachedRolePolicies&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:ListInstanceProfilesForRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreateInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreatePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreatePolicyVersion&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreateRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:CreateServiceLinkedRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:AddRoleToInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:AttachRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DetachRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:RemoveRoleFromInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeletePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeletePolicyVersion&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeleteRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeleteRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:DeleteInstanceProfile&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:PutRolePolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:PassRole&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:UpdateAssumeRolePolicy&#34;</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:green>// The following permission set is only needed, if AWS Load Balancer controller is enabled (see ControlPlaneConfig)
</span></span></span><span style=display:flex><span><span style=color:green></span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;cognito-idp:DescribeUserPoolClient&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;acm:ListCertificates&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;acm:DescribeCertificate&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:ListServerCertificates&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;iam:GetServerCertificate&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;waf-regional:GetWebACL&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;waf-regional:GetWebACLForResource&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;waf-regional:AssociateWebACL&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;waf-regional:DisassociateWebACL&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;wafv2:GetWebACL&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;wafv2:GetWebACLForResource&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;wafv2:AssociateWebACL&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;wafv2:DisassociateWebACL&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;shield:GetSubscriptionState&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;shield:DescribeProtection&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;shield:CreateProtection&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;shield:DeleteProtection&#34;</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      &#34;Resource&#34;: <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></details><h2 id=infrastructureconfig><code>InfrastructureConfig</code></h2><p>The infrastructure configuration mainly describes how the network layout looks like in order to create the shoot worker nodes in a later step, thus, prepares everything relevant to create VMs, load balancers, volumes, etc.</p><p>An example <code>InfrastructureConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: InfrastructureConfig
</span></span><span style=display:flex><span>enableECRAccess: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>dualStack:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>networks:
</span></span><span style=display:flex><span>  vpc: <span style=color:green># specify either &#39;id&#39; or &#39;cidr&#39;</span>
</span></span><span style=display:flex><span>  <span style=color:green># id: vpc-123456</span>
</span></span><span style=display:flex><span>    cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>  <span style=color:green># gatewayEndpoints:</span>
</span></span><span style=display:flex><span>  <span style=color:green># - s3</span>
</span></span><span style=display:flex><span>  zones:
</span></span><span style=display:flex><span>  - name: eu-west-1a
</span></span><span style=display:flex><span>    internal: 10.250.112.0/22
</span></span><span style=display:flex><span>    public: 10.250.96.0/22
</span></span><span style=display:flex><span>    workers: 10.250.0.0/19
</span></span><span style=display:flex><span>  <span style=color:green># elasticIPAllocationID: eipalloc-123456</span>
</span></span><span style=display:flex><span>ignoreTags:
</span></span><span style=display:flex><span>  keys: <span style=color:green># individual ignored tag keys</span>
</span></span><span style=display:flex><span>  - SomeCustomKey
</span></span><span style=display:flex><span>  - AnotherCustomKey
</span></span><span style=display:flex><span>  keyPrefixes: <span style=color:green># ignored tag key prefixes</span>
</span></span><span style=display:flex><span>  - user.specific/prefix/
</span></span></code></pre></div><p>The <code>enableECRAccess</code> flag specifies whether the AWS IAM role policy attached to all worker nodes of the cluster shall contain permissions to access the Elastic Container Registry of the respective AWS account.
If the flag is not provided it is defaulted to <code>true</code>.
Please note that if the <code>iamInstanceProfile</code> is set for a worker pool in the <code>WorkerConfig</code> (see below) then <code>enableECRAccess</code> does not have any effect.
It only applies for those worker pools whose <code>iamInstanceProfile</code> is not set.</p><details><summary>Click to expand the default AWS IAM policy document used for the instance profiles!</summary><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  &#34;Version&#34;: <span style=color:#a31515>&#34;2012-10-17&#34;</span>,
</span></span><span style=display:flex><span>  &#34;Statement&#34;: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ec2:DescribeInstances&#34;</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      &#34;Resource&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>      ]
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:green>// Only if `.enableECRAccess` is `true`.
</span></span></span><span style=display:flex><span><span style=color:green></span>    {
</span></span><span style=display:flex><span>      &#34;Effect&#34;: <span style=color:#a31515>&#34;Allow&#34;</span>,
</span></span><span style=display:flex><span>      &#34;Action&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:GetAuthorizationToken&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:BatchCheckLayerAvailability&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:GetDownloadUrlForLayer&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:GetRepositoryPolicy&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:DescribeRepositories&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:ListImages&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;ecr:BatchGetImage&#34;</span>
</span></span><span style=display:flex><span>      ],
</span></span><span style=display:flex><span>      &#34;Resource&#34;: [
</span></span><span style=display:flex><span>        <span style=color:#a31515>&#34;*&#34;</span>
</span></span><span style=display:flex><span>      ]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></details><p>The <code>dualStack.enabled</code> flag specifies whether dual-stack or IPv4-only should be supported by the infrastructure.
When the flag is set to true an Amazon provided IPv6 CIDR block will be attached to the VPC.
All subnets will receive a <code>/64</code> block from it and a route entry is added to the main route table to route all IPv6 traffic over the IGW.</p><p>The <code>networks.vpc</code> section describes whether you want to create the shoot cluster in an already existing VPC or whether to create a new one:</p><ul><li>If <code>networks.vpc.id</code> is given then you have to specify the VPC ID of the existing VPC that was created by other means (manually, other tooling, &mldr;).
Please make sure that the VPC has attached an internet gateway - the AWS controller won&rsquo;t create one automatically for existing VPCs. To make sure the nodes are able to join and operate in your cluster properly, please make sure that your VPC has enabled <a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html>DNS Support</a>, explicitly the attributes <code>enableDnsHostnames</code> and <code>enableDnsSupport</code> must be set to <code>true</code>.</li><li>If <code>networks.vpc.cidr</code> is given then you have to specify the VPC CIDR of a new VPC that will be created during shoot creation.
You can freely choose a private CIDR range.</li><li>Either <code>networks.vpc.id</code> or <code>networks.vpc.cidr</code> must be present, but not both at the same time.</li><li><code>networks.vpc.gatewayEndpoints</code> is optional. If specified then each item is used as service name in a corresponding Gateway VPC Endpoint.</li></ul><p>The <code>networks.zones</code> section contains configuration for resources you want to create or use in availability zones.
For every zone, the AWS extension creates three subnets:</p><ul><li>The <code>internal</code> subnet is used for <a href=https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internal-load-balancers.html>internal AWS load balancers</a>.</li><li>The <code>public</code> subnet is used for <a href=https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-internet-facing-load-balancers.html>public AWS load balancers</a>.</li><li>The <code>workers</code> subnet is used for all shoot worker nodes, i.e., VMs which later run your applications.</li></ul><p>For every subnet, you have to specify a CIDR range contained in the VPC CIDR specified above, or the VPC CIDR of your already existing VPC.
You can freely choose these CIDRs and it is your responsibility to properly design the network layout to suit your needs.</p><p>Also, the AWS extension creates a dedicated NAT gateway for each zone.
By default, it also creates a corresponding Elastic IP that it attaches to this NAT gateway and which is used for egress traffic.
The <code>elasticIPAllocationID</code> field allows you to specify the ID of an existing Elastic IP allocation in case you want to bring your own.
If provided, no new Elastic IP will be created and, instead, the Elastic IP specified by you will be used.</p><p>⚠️ If you change this field for an already existing infrastructure then it will disrupt egress traffic while AWS applies this change.
The reason is that the NAT gateway must be recreated with the new Elastic IP association.
Also, please note that the existing Elastic IP will be permanently deleted if it was earlier created by the AWS extension.</p><p>You can configure <a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpce-gateway.html>Gateway VPC Endpoints</a> by adding items in the optional list <code>networks.vpc.gatewayEndpoints</code>. Each item in the list is used as a service name and a corresponding endpoint is created for it. All created endpoints point to the service within the cluster&rsquo;s region. For example, consider this (partial) shoot config:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  region: eu-central-1
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          gatewayEndpoints:
</span></span><span style=display:flex><span>          - s3
</span></span></code></pre></div><p>The service name of the S3 Gateway VPC Endpoint in this example is <code>com.amazonaws.eu-central-1.s3</code>.</p><p>If you want to use multiple availability zones then add a second, third, &mldr; entry to the <code>networks.zones[]</code> list and properly specify the AZ name in <code>networks.zones[].name</code>.</p><p>Apart from the VPC and the subnets the AWS extension will also create DHCP options and an internet gateway (only if a new VPC is created), routing tables, security groups, elastic IPs, NAT gateways, EC2 key pairs, IAM roles, and IAM instance profiles.</p><p>The <code>ignoreTags</code> section allows to configure which resource tags on AWS resources managed by Gardener should be ignored during
infrastructure reconciliation. By default, all tags that are added outside of Gardener&rsquo;s
reconciliation will be removed during the next reconciliation. This field allows users and automation to add
custom tags on AWS resources created and managed by Gardener without loosing them on the next reconciliation.
Tags can ignored either by specifying exact key values (<code>ignoreTags.keys</code>) or key prefixes (<code>ignoreTags.keyPrefixes</code>).
In both cases it is forbidden to ignore the <code>Name</code> tag or any tag starting with <code>kubernetes.io</code> or <code>gardener.cloud</code>.<br>Please note though, that the tags are only ignored on resources created on behalf of the <code>Infrastructure</code> CR (i.e. VPC,
subnets, security groups, keypair, etc.), while tags on machines, volumes, etc. are not in the scope of this controller.</p><h2 id=controlplaneconfig><code>ControlPlaneConfig</code></h2><p>The control plane configuration mainly contains values for the AWS-specific control plane components.
Today, the only component deployed by the AWS extension is the <code>cloud-controller-manager</code>.</p><p>An example <code>ControlPlaneConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: ControlPlaneConfig
</span></span><span style=display:flex><span>cloudControllerManager:
</span></span><span style=display:flex><span>  featureGates:
</span></span><span style=display:flex><span>    RotateKubeletServerCertificate: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  useCustomRouteController: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span><span style=color:green>#loadBalancerController:</span>
</span></span><span style=display:flex><span><span style=color:green>#  enabled: true</span>
</span></span><span style=display:flex><span><span style=color:green>#  ingressClassName: alb</span>
</span></span><span style=display:flex><span>storage:
</span></span><span style=display:flex><span>  managedDefaultClass: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The <code>cloudControllerManager.featureGates</code> contains a map of explicitly enabled or disabled feature gates.
For production usage it&rsquo;s not recommend to use this field at all as you can enable alpha features or disable beta/stable features, potentially impacting the cluster stability.
If you don&rsquo;t want to configure anything for the <code>cloudControllerManager</code> simply omit the key in the YAML specification.</p><p>The <code>cloudControllerManager.useCustomRouteController</code> controls if the <a href=https://github.com/gardener/aws-custom-route-controller>custom routes controller</a> should be enabled.
If enabled, it will add routes to the pod CIDRs for all nodes in the route tables for all zones.</p><p>The <code>storage.managedDefaultClass</code> controls if the <code>default</code> storage / volume snapshot classes are marked as default by Gardener. Set it to <code>false</code> to <a href=https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/>mark another storage / volume snapshot class as default</a> without Gardener overwriting this change. If unset, this field defaults to <code>true</code>.</p><p>If the <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/>AWS Load Balancer Controller</a> should be deployed, set <code>loadBalancerController.enabled</code> to <code>true</code>.
In this case, it is assumed that an <code>IngressClass</code> named <code>alb</code> is created <strong>by the user</strong>.
You can overwrite the name by setting <code>loadBalancerController.ingressClassName</code>.</p><p>Please note, that currently only the &ldquo;instance&rdquo; mode is supported.</p><h3 id=examples-for-ingress-and-service-managed-by-the-aws-load-balancer-controller>Examples for <code>Ingress</code> and <code>Service</code> managed by the AWS Load Balancer Controller:</h3><ol start=0><li>Prerequites</li></ol><p>Make sure you have created an <code>IngressClass</code>. For more details about parameters, please see <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.5/guide/ingress/ingress_class/>AWS Load Balancer Controller - IngressClass</a></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: IngressClass
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: alb <span style=color:green># default name if not specified by `loadBalancerController.ingressClassName` </span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  controller: ingress.k8s.aws/alb
</span></span></code></pre></div><ol><li>Ingress</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  name: echoserver
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    <span style=color:green># complete set of annotations: https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/ingress/annotations/</span>
</span></span><span style=display:flex><span>    alb.ingress.kubernetes.io/scheme: internet-facing
</span></span><span style=display:flex><span>    alb.ingress.kubernetes.io/target-type: instance <span style=color:green># target-type &#34;ip&#34; NOT supported in Gardener</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ingressClassName: alb
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>    - http:
</span></span><span style=display:flex><span>        paths:
</span></span><span style=display:flex><span>        - path: /
</span></span><span style=display:flex><span>          pathType: Prefix
</span></span><span style=display:flex><span>          backend:
</span></span><span style=display:flex><span>            service:
</span></span><span style=display:flex><span>              name: echoserver
</span></span><span style=display:flex><span>              port:
</span></span><span style=display:flex><span>                number: 80
</span></span></code></pre></div><p>For more details see <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/ingress/spec/>AWS Load Balancer Documentation - Ingress Specification</a></p><ol start=2><li>Service of Type <code>LoadBalancer</code></li></ol><p>This can be used to create a Network Load Balancer (NLB).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    <span style=color:green># complete set of annotations: https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/service/annotations/</span>
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance <span style=color:green># target-type &#34;ip&#34; NOT supported in Gardener</span>
</span></span><span style=display:flex><span>    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
</span></span><span style=display:flex><span>  name: ingress-nginx-controller
</span></span><span style=display:flex><span>  namespace: ingress-nginx
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  type: LoadBalancer
</span></span><span style=display:flex><span>  loadBalancerClass: service.k8s.aws/nlb <span style=color:green># mandatory to be managed by AWS Load Balancer Controller (otherwise the Cloud Controller Manager will act on it)</span>
</span></span></code></pre></div><p>For more details see <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/service/nlb/>AWS Load Balancer Documentation - Network Load Balancer</a></p><h2 id=workerconfig><code>WorkerConfig</code></h2><p>The AWS extension supports encryption for volumes plus support for additional data volumes per machine.
For each data volume, you have to specify a name.
By default (if not stated otherwise), all the disks (root & data volumes) are encrypted.
Please make sure that your <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html>instance-type supports encryption</a>.
If your instance-type doesn&rsquo;t support encryption, you will have to disable encryption (which is enabled by default) by setting <code>volume.encrpyted</code> to <code>false</code> (refer below shown YAML snippet).</p><p>The following YAML is a snippet of a <code>Shoot</code> resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: cpu-worker
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>        size: 20Gi
</span></span><span style=display:flex><span>        encrypted: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      dataVolumes:
</span></span><span style=display:flex><span>      - name: kubelet-dir
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>        size: 25Gi
</span></span><span style=display:flex><span>        encrypted: <span style=color:#00f>true</span>
</span></span></code></pre></div><blockquote><p>Note: The AWS extension does not support EBS volume (root & data volumes) encryption with <a href=https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk>customer managed CMK</a>. Support for <a href=https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk>customer managed CMK</a> is out of scope for now. Only <a href=https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk>AWS managed CMK</a> is supported.</p></blockquote><p>Additionally, it is possible to provide further AWS-specific values for configuring the worker pools.
It can be provided in <code>.spec.provider.workers[].providerConfig</code> and is evaluated by the AWS worker controller when it reconciles the shoot machines.</p><p>An example <code>WorkerConfig</code> for the AWS extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: WorkerConfig
</span></span><span style=display:flex><span>volume:
</span></span><span style=display:flex><span>  iops: 10000
</span></span><span style=display:flex><span>  throughput: 200 
</span></span><span style=display:flex><span>dataVolumes:
</span></span><span style=display:flex><span>- name: kubelet-dir
</span></span><span style=display:flex><span>  iops: 12345
</span></span><span style=display:flex><span>  throughput: 150
</span></span><span style=display:flex><span>  snapshotID: snap-1234
</span></span><span style=display:flex><span>iamInstanceProfile: <span style=color:green># (specify either ARN or name)</span>
</span></span><span style=display:flex><span>  name: my-profile
</span></span><span style=display:flex><span>instanceMetadataOptions:
</span></span><span style=display:flex><span>  httpTokens: required
</span></span><span style=display:flex><span>  httpPutResponseHopLimit: 2
</span></span><span style=display:flex><span><span style=color:green># arn: my-instance-profile-arn</span>
</span></span><span style=display:flex><span>nodeTemplate: <span style=color:green># (to be specified only if the node capacity would be different from cloudprofile info during runtime)</span>
</span></span><span style=display:flex><span>  capacity:
</span></span><span style=display:flex><span>    cpu: 2
</span></span><span style=display:flex><span>    gpu: 0
</span></span><span style=display:flex><span>    memory: 50Gi
</span></span></code></pre></div><p>The <code>.volume.iops</code> is the number of I/O operations per second (IOPS) that the volume supports.
For <code>io1</code> and <code>gp3</code> volume type, this represents the number of IOPS that are provisioned for the volume.
For <code>gp2</code> volume type, this represents the baseline performance of the volume and the rate at which the volume accumulates I/O credits for bursting. For more information about General Purpose SSD baseline performance, I/O credits, IOPS range and bursting, see Amazon EBS Volume Types (<a href=http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html>http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html</a>) in the Amazon Elastic Compute Cloud User Guide.<br>Constraint: IOPS should be a positive value. Validation of IOPS (i.e. whether it is allowed and is in the specified range for a particular volume type) is done on aws side.</p><p>The <code>volume.throughput</code> is the throughput that the volume supports, in <code>MiB/s</code>. As of <code>16th Aug 2022</code>, this parameter is valid only for <code>gp3</code> volume types and will return an error from the provider side if specified for other volume types. Its current range of throughput is from <code>125MiB/s</code> to <code>1000 MiB/s</code>. To know more about throughput and its range, see the official AWS documentation <a href=http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html>here</a>.</p><p>The <code>.dataVolumes</code> can optionally contain configurations for the data volumes stated in the <code>Shoot</code> specification in the <code>.spec.provider.workers[].dataVolumes</code> list.
The <code>.name</code> must match to the name of the data volume in the shoot.
It is also possible to provide a snapshot ID. It allows to <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html>restore the data volume from an existing snapshot</a>.</p><p>The <code>iamInstanceProfile</code> section allows to specify the IAM instance profile name xor ARN that should be used for this worker pool.
If not specified, a dedicated IAM instance profile created by the infrastructure controller is used (see above).</p><p>The <code>instanceMetadataOptions</code> controls access to the instance metadata service (IMDS) for members of the worker. You can do the following operations:</p><ul><li>access IMDSv1 (default)</li><li>access IMDSv2 - <code>httpPutResponseHopLimit >= 2</code></li><li>access IMDSv2 only (restrict access to IMDSv1) - <code>httpPutResponseHopLimit >=2</code>, <code>httpTokens = "required"</code></li><li>disable access to IMDS - <code>httpTokens = "required"</code></li></ul><blockquote><p>Note: The accessibility of IMDS discussed in the previous point is referenced from the point of view of containers <strong>NOT</strong> running in the host network.
By default on host network IMDSv2 is already enabled (but not accessible from inside the pods).
It is currently not possible to create a VM with complete restriction to the IMDS service. It is however possible to restrict access from inside the pods by setting <code>httpTokens</code> to <code>required</code> and not setting <code>httpPutResponseHopLimit</code> (or setting it to 1).</p></blockquote><p>You can find more information regarding the options in the <a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/configuring-IMDS-new-instances.html>AWS documentation</a>.</p><h2 id=example-shoot-manifest-one-availability-zone>Example <code>Shoot</code> manifest (one availability zone)</h2><p>Please find below an example <code>Shoot</code> manifest for one availability zone:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-aws
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: aws
</span></span><span style=display:flex><span>  region: eu-central-1
</span></span><span style=display:flex><span>  secretBindingName: core-aws
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        zones:
</span></span><span style=display:flex><span>        - name: eu-central-1a
</span></span><span style=display:flex><span>          internal: 10.250.112.0/22
</span></span><span style=display:flex><span>          public: 10.250.96.0/22
</span></span><span style=display:flex><span>          workers: 10.250.0.0/19
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: m5.large
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>    <span style=color:green># The following provider config is valid if the volume type is `io1`.</span>
</span></span><span style=display:flex><span>    <span style=color:green># providerConfig:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   kind: WorkerConfig</span>
</span></span><span style=display:flex><span>    <span style=color:green>#   volume:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#     iops: 10000</span>
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - eu-central-1a
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=example-shoot-manifest-three-availability-zones>Example <code>Shoot</code> manifest (three availability zones)</h2><p>Please find below an example <code>Shoot</code> manifest for three availability zones:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-aws
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: aws
</span></span><span style=display:flex><span>  region: eu-central-1
</span></span><span style=display:flex><span>  secretBindingName: core-aws
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: aws
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vpc:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        zones:
</span></span><span style=display:flex><span>        - name: eu-central-1a
</span></span><span style=display:flex><span>          workers: 10.250.0.0/26
</span></span><span style=display:flex><span>          public: 10.250.96.0/26
</span></span><span style=display:flex><span>          internal: 10.250.112.0/26
</span></span><span style=display:flex><span>        - name: eu-central-1b
</span></span><span style=display:flex><span>          workers: 10.250.0.64/26
</span></span><span style=display:flex><span>          public: 10.250.96.64/26
</span></span><span style=display:flex><span>          internal: 10.250.112.64/26
</span></span><span style=display:flex><span>        - name: eu-central-1c
</span></span><span style=display:flex><span>          workers: 10.250.0.128/26
</span></span><span style=display:flex><span>          public: 10.250.96.128/26
</span></span><span style=display:flex><span>          internal: 10.250.112.128/26
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: m5.large
</span></span><span style=display:flex><span>      minimum: 3
</span></span><span style=display:flex><span>      maximum: 9
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: gp2
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - eu-central-1a
</span></span><span style=display:flex><span>      - eu-central-1b
</span></span><span style=display:flex><span>      - eu-central-1c
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=csi-volume-provisioners>CSI volume provisioners</h2><p>Every AWS shoot cluster will be deployed with the AWS EBS CSI driver.
It is compatible with the legacy in-tree volume provisioner that was deprecated by the Kubernetes community and will be removed in future versions of Kubernetes.
End-users might want to update their custom <code>StorageClass</code>es to the new <code>ebs.csi.aws.com</code> provisioner.</p><h3 id=node-specific-volume-limits>Node-specific Volume Limits</h3><p>The Kubernetes scheduler allows configurable limit for the number of volumes that can be attached to a node. See <a href=https://k8s.io/docs/concepts/storage/storage-limits/#custom-limits>https://k8s.io/docs/concepts/storage/storage-limits/#custom-limits</a>.</p><p>CSI drivers usually have a different procedure for configuring this custom limit. By default, the EBS CSI driver parses the machine type name and then decides the volume limit. However, this is only a rough approximation and not good enough in most cases. Specifying the volume attach limit via command line flag (<code>--volume-attach-limit</code>) is currently the alternative until a more sophisticated solution presents itself (dynamically discovering the maximum number of attachable volume per EC2 machine type, see also <a href=https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/347>https://github.com/kubernetes-sigs/aws-ebs-csi-driver/issues/347</a>). The AWS extension allows the <code>--volume-attach-limit</code> flag of the EBS CSI driver to be configurable via <code>aws.provider.extensions.gardener.cloud/volume-attach-limit</code> annotation on the <code>Shoot</code> resource. If the annotation is added to an existing <code>Shoot</code>, then reconciliation needs to be triggered manually (see <a href=/docs/gardener/shoot_operations/#immediate-reconciliation>Immediate reconciliation</a>), as in general adding annotation to resource is not a change that leads to <code>.metadata.generation</code> increase in general.</p><h2 id=kubernetes-versions-per-worker-pool>Kubernetes Versions per Worker Pool</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>WorkerPoolKubernetesVersion</code> feature gate, i.e., having <a href=https://github.com/gardener/gardener/blob/8a9c88866ec5fce59b5acf57d4227eeeb73669d7/example/90-shoot.yaml#L69-L70>worker pools with overridden Kubernetes versions</a> since <code>gardener-extension-provider-aws@v1.34</code>.</p><h2 id=shoot-ca-certificate-and-serviceaccount-signing-key-rotation>Shoot CA Certificate and <code>ServiceAccount</code> Signing Key Rotation</h2><p>This extension supports <code>gardener/gardener</code>&rsquo;s <code>ShootCARotation</code> and <code>ShootSARotation</code> feature gates since <code>gardener-extension-provider-aws@v1.36</code>.</p><h2 id=flow-infrastructure-reconciler>Flow Infrastructure Reconciler</h2><p>The extension offers two different reconciler implementations for the infrastructure resource:</p><ul><li>terraform-based</li><li>native Go SDK based (dubbed the &ldquo;flow&rdquo;-based implementation)</li></ul><p>The default implementation currently is the terraform reconciler which uses the <code>https://github.com/gardener/terraformer</code> as the backend for managing the shoot&rsquo;s infrastructure.</p><p>The &ldquo;flow&rdquo; implementation is a newer implementation that is trying to solve issues we faced with managing terraform infrastructure on Kubernetes. The goal is to have more control over the reconciliation process and be able to perform fine-grained tuning over it. The implementation is completely backwards-compatible and offers a migration route from the legacy terraformer implementation.</p><p>For most users there will be no noticable difference. However for certain use-cases, users may notice a slight deviation from the previous behavior. For example, with flow-based infrastructure users may be able to perform certain modifications to infrastructure resources without having them reconciled back by terraform. Operations that would degrade the shoot infrastructure are still expected to be reverted back.</p><p>For the time-being, to take advantage of the flow reconcilier users have to &ldquo;opt-in&rdquo; by annotating the shoot manifest with: <code>aws.provider.extensions.gardener.cloud/use-flow="true"</code>. For existing shoots with this annotation, the migration will take place on the next infrastructure reconciliation (on maintenance window or if other infrastructure changes are requested). The migration is not revertible.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cf44505b1a6fee28c6d8a49feb963c88>2 - Operating System Extensions</h1><div class=lead>Gardener extension controllers for the supported operating systems</div></div><div class=td-content><h1 id=pg-cd6184375a7182a45366dc72ad6e13d1>2.1 - CoreOS/FlatCar OS</h1><div class=lead>Gardener extension controller for the CoreOS/FlatCar Container Linux operating system</div><h1 id=gardener-extension-for-coreos-container-linuxhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for CoreOS Container Linux</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-os-coreos-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-os-coreos-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-os-coreos><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-os-coreos alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>. However, the project has grown to a size where it is very hard to extend, maintain, and test. With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.</p><p>This controller operates on the <code>OperatingSystemConfig</code> resource in the <code>extensions.gardener.cloud/v1alpha1</code> API group. It supports <a href=https://coreos.com/os/docs/latest/>CoreOS Container Linux</a> and <a href=https://www.flatcar-linux.org/>Flatcar Container Linux</a> (&ldquo;a friendly fork of CoreOS Container Linux&rdquo;).</p><p>The controller manages those objects that are requesting <a href=https://coreos.com/os/docs/latest/>CoreOS Container Linux</a> configuration (<code>.spec.type=coreos</code>) or <a href=https://www.flatcar-linux.org/>Flatcar Container Linux</a> configuration (<code>.spec.type=flatcar</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OperatingSystemConfig
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: pool-01-original
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: coreos
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  files:
</span></span><span style=display:flex><span>    ...
</span></span></code></pre></div><p>Please find <a href=https://github.com/gardener/gardener-extension-os-coreos/blob/master/example/40-operatingsystemconfig.yaml>a concrete example</a> in the <code>example</code> folder.</p><p>After reconciliation the resulting data will be stored in a secret within the same namespace (as the config itself might contain confidential data). The name of the secret will be written into the resource&rsquo;s <code>.status</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  cloudConfig:
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: osc-result-pool-01-original
</span></span><span style=display:flex><span>      namespace: default
</span></span><span style=display:flex><span>  command: /usr/bin/coreos-cloudinit -from-file=&lt;path&gt;
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>  - docker-monitor.service
</span></span><span style=display:flex><span>  - kubelet-monitor.service
</span></span><span style=display:flex><span>  - kubelet.service
</span></span></code></pre></div><p>The secret has one data key <code>cloud_config</code> that stores the generation.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-os-coreos/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the <code>./dev/kubeconfig</code> file.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-os-coreos/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7490efaa24f7fa053776844f854c404c>2.1.1 - Usage</h1><h1 id=using-the-coreos-extension-with-gardener-as-end-user>Using the CoreOS extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that must be considered when this OS extension is used.</p><p>In this document we describe how this configuration looks like and under which circumstances your attention may be required.</p><h2 id=aws-vpc-settings-for-coreos-workers>AWS VPC settings for CoreOS workers</h2><p>Gardener allows you to create CoreOS based worker nodes by:</p><ol><li>Using a Gardener managed VPC</li><li>Reusing a VPC that already exists (VPC <code>id</code> specified in <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/usage/#infrastructureconfig>InfrastructureConfig</a>]</li></ol><p>If the second option applies to your use-case please make sure that your VPC has enabled <strong>DNS Support</strong>. Otherwise CoreOS based nodes aren&rsquo;t able to join or operate in your cluster properly.</p><p><strong><a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html>DNS</a></strong> settings (required):</p><ul><li><code>enableDnsHostnames</code>: true (necessary for collecting <a href=https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/>node metrics</a>)</li><li><code>enableDnsSupport</code>: true</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0aede9ce025bb447b055d2593567eef6>2.2 - Garden Linux OS</h1><div class=lead>Gardener extension controller for the Garden Linux operating system</div><h1 id=gardener-extension-for-garden-linux-oshttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Garden Linux OS</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-os-gardenlinux-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-os-gardenlinux-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-os-gardenlinux><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-os-gardenlinux alt="Go Report Card"></a></p><p>This controller operates on the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md#cloud-config-user-data-for-bootstrapping-machines><code>OperatingSystemConfig</code></a> resource in the <code>extensions.gardener.cloud/v1alpha1</code> API group.</p><p>It manages those objects that are requesting&mldr;</p><ul><li><p><a href=https://gardenlinux.io/>Garden Linux OS</a> configuration (<code>.spec.type=gardenlinux</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OperatingSystemConfig
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: pool-01-original
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: gardenlinux
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  files:
</span></span><span style=display:flex><span>    ...
</span></span></code></pre></div><p>Please find <a href=https://github.com/gardener/gardener-extension-os-gardenlinux/blob/master/example/40-operatingsystemconfig-gardenlinux.yaml>a concrete example</a> in the <code>example</code> folder.</p></li><li><p>MemoryOne on Garden Linux configuration (<code>spec.type=memoryone-gardenlinux</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OperatingSystemConfig
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: pool-01-original
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: memoryone-gardenlinux
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  files:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: memoryone-gardenlinux.os.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: OperatingSystemConfiguration
</span></span><span style=display:flex><span>    memoryTopology: <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>    systemMemory: <span style=color:#a31515>&#34;6x&#34;</span>
</span></span></code></pre></div><p>Please find <a href=https://github.com/gardener/gardener-extension-os-gardenlinux/blob/master/example/40-operatingsystemconfig-memoryonegardenlinux.yaml>a concrete example</a> in the <code>example</code> folder.</p></li></ul><p>After reconciliation the resulting data will be stored in a secret within the same namespace (as the config itself might contain confidential data). The name of the secret will be written into the resource&rsquo;s <code>.status</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  cloudConfig:
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: osc-result-pool-01-original
</span></span><span style=display:flex><span>      namespace: default
</span></span><span style=display:flex><span>  command: /usr/bin/env bash &lt;path&gt;
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>  - docker-monitor.service
</span></span><span style=display:flex><span>  - kubelet-monitor.service
</span></span><span style=display:flex><span>  - kubelet.service
</span></span></code></pre></div><p>The secret has one data key <code>cloud_config</code> that stores the generation.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-os-gardenlinux/blob/master/example/controller-registration.yaml>here</a>.</p><p>The implementation of this controller is using Gardeners <a href=https://github.com/gardener/gardener/blob/master/extensions/pkg/controller/operatingsystemconfig/oscommon/README.md><code>oscommon</code></a> library for operating system configuration controllers.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the <code>./dev/kubeconfig</code> file.
Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-os-gardenlinux/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e032feff371332908baf100fa0bb6350>2.3 - SUSE CHost OS</h1><div class=lead>Gardener extension controller for the SUSE Container Host operating system (CHost)</div><h1 id=gardener-extension-for-suse-chosthttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for SUSE CHost</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-os-suse-chost-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-os-suse-chost-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-os-suse-chost><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-os-suse-chost alt="Go Report Card"></a></p><p>This controller operates on the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md#cloud-config-user-data-for-bootstrapping-machines><code>OperatingSystemConfig</code></a> resource in the <code>extensions.gardener.cloud/v1alpha1</code> API group. It manages those objects that are requesting SUSE Container Host configuration, i.e. <code>suse-chost</code> type:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OperatingSystemConfig
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: pool-01-original
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: suse-chost
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  files:
</span></span><span style=display:flex><span>    ...
</span></span></code></pre></div><p>Please find <a href=https://github.com/gardener/gardener-extension-os-suse-chost/blob/master/example/40-operatingsystemconfig-chost.yaml>a concrete example</a> in the <code>example</code> folder.</p><p>It is also capable of supporting the <a href="https://marketplace.cloud.vmware.com/services/details/vsmp-memoryone?slug=true">vSMP MemoryOne</a> operating system with the <code>memoryone-chost</code> type. Please find more information <a href=/docs/extensions/os-extensions/gardener-extension-os-suse-chost/usage/#support-for-vsmp-memoryone>here</a>.</p><p>After reconciliation the resulting data will be stored in a secret within the same namespace (as the config itself might contain confidential data). The name of the secret will be written into the resource&rsquo;s <code>.status</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  cloudConfig:
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: osc-result-pool-01-original
</span></span><span style=display:flex><span>      namespace: default
</span></span><span style=display:flex><span>  command: /usr/bin/env bash &lt;path&gt;
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>  - docker-monitor.service
</span></span><span style=display:flex><span>  - kubelet-monitor.service
</span></span><span style=display:flex><span>  - kubelet.service
</span></span></code></pre></div><p>The secret has one data key <code>cloud_config</code> that stores the generation.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-os-suse-chost/blob/master/example/controller-registration.yaml>here</a>.</p><p>This controller is implemented using the <a href=https://github.com/gardener/gardener/blob/master/extensions/pkg/controller/operatingsystemconfig/oscommon/README.md><code>oscommon</code></a> library for operating system configuration controllers.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the <code>./dev/kubeconfig</code> file.
Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-os-suse-chost/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f86412446679d5f86ba051fe37aa4472>2.3.1 - Usage</h1><h1 id=using-the-suse-chost-extension-with-gardener-as-end-user>Using the SuSE CHost extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that must be considered when this OS extension is used.</p><p>In this document we describe how this configuration looks like and under which circumstances your attention may be required.</p><h2 id=aws-vpc-settings-for-suse-chost-workers>AWS VPC settings for SuSE CHost workers</h2><p>Gardener allows you to create SuSE CHost based worker nodes by:</p><ol><li>Using a Gardener managed VPC</li><li>Reusing a VPC that already exists (VPC <code>id</code> specified in <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/usage/#infrastructureconfig>InfrastructureConfig</a>]</li></ol><p>If the second option applies to your use-case please make sure that your VPC has enabled <strong>DNS Support</strong>. Otherwise SuSE CHost based nodes aren&rsquo;t able to join or operate in your cluster properly.</p><p><strong><a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html>DNS</a></strong> settings (required):</p><ul><li><code>enableDnsHostnames</code>: true</li><li><code>enableDnsSupport</code>: true</li></ul><h2 id=support-for-vsmp-memoryone>Support for vSMP MemoryOne</h2><p>This extension controller is also capable of generating user-data for the <a href="https://marketplace.cloud.vmware.com/services/details/vsmp-memoryone?slug=true">vSMP MemoryOne</a> operating system in conjunction with SuSE CHost.
It reacts on the <code>memoryone-chost</code> extension type.
Additionally, it allows certain customizations with the following configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: memoryone-chost.os.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OperatingSystemConfiguration
</span></span><span style=display:flex><span>memoryTopology: <span style=color:#a31515>&#34;3&#34;</span>
</span></span><span style=display:flex><span>systemMemory: <span style=color:#a31515>&#34;7x&#34;</span>
</span></span></code></pre></div><ul><li>The <code>memoryTopology</code> field controls the <code>mem_topology</code> setting. If it&rsquo;s not provided then it will default to <code>2</code>.</li><li>The <code>systemMemory</code> field controls the <code>system_memory</code> setting. If it&rsquo;s not provided then it defaults to <code>6x</code>.</li></ul><p>Please note that it was only e2e-tested on AWS.
Additionally, you need a snapshot ID of a SuSE CHost/CHost volume (see below how to create it).</p><p>An exemplary worker pool configuration inside a <code>Shoot</code> resource using for the vSMP MemoryOne operating system would look as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: vsmp-memoryone
</span></span><span style=display:flex><span>  namespace: garden-foo
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  workers:
</span></span><span style=display:flex><span>  - name: cpu-worker3
</span></span><span style=display:flex><span>    minimum: 1
</span></span><span style=display:flex><span>    maximum: 1
</span></span><span style=display:flex><span>    maxSurge: 1
</span></span><span style=display:flex><span>    maxUnavailable: 0
</span></span><span style=display:flex><span>    machine:
</span></span><span style=display:flex><span>      image:
</span></span><span style=display:flex><span>        name: memoryone-chost
</span></span><span style=display:flex><span>        version: 9.5.195
</span></span><span style=display:flex><span>        providerConfig:
</span></span><span style=display:flex><span>          apiVersion: memoryone-chost.os.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>          kind: OperatingSystemConfiguration
</span></span><span style=display:flex><span>          memoryTopology: <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>          systemMemory: <span style=color:#a31515>&#34;6x&#34;</span>
</span></span><span style=display:flex><span>      type: c5d.metal
</span></span><span style=display:flex><span>    volume:
</span></span><span style=display:flex><span>      size: 20Gi
</span></span><span style=display:flex><span>      type: gp2
</span></span><span style=display:flex><span>    dataVolumes:
</span></span><span style=display:flex><span>    - name: chost
</span></span><span style=display:flex><span>      size: 50Gi
</span></span><span style=display:flex><span>      type: gp2
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: aws.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: WorkerConfig
</span></span><span style=display:flex><span>      dataVolumes:
</span></span><span style=display:flex><span>      - name: chost
</span></span><span style=display:flex><span>        snapshotID: snap-123456
</span></span><span style=display:flex><span>    zones:
</span></span><span style=display:flex><span>    - eu-central-1b
</span></span></code></pre></div><p>Please note that vSMP MemoryOne only works for EC2 bare-metal instance types such as <code>M5d</code>, <code>R5</code>, <code>C5</code>, <code>C5d</code>, etc. - please consult <a href=https://aws.amazon.com/ec2/instance-types/>the EC2 instance types overview page</a> and the documentation of vSMP MemoryOne to find out whether the instance type in question is eligible.</p><h3 id=generating-an-aws-snapshot-id-for-the-chostchost-operating-system>Generating an AWS snapshot ID for the CHost/CHost operating system</h3><p>The following script will help to generate the snapshot ID on AWS.
It runs in the region that is selected in your <code>$HOME/.aws/config</code> file.
Consequently, if you want to generate the snapshot in multiple regions, you have to run in multiple times after configuring the respective region using <code>aws configure</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ami=<span style=color:#a31515>&#34;ami-1234&#34;</span> <span style=color:green>#Replace the ami with the intended one.</span>
</span></span><span style=display:flex><span>name=<span style=color:#a31515>`</span>aws ec2 describe-images --image-ids $ami  --query=<span style=color:#a31515>&#34;Images[].Name&#34;</span> --output=text<span style=color:#a31515>`</span>
</span></span><span style=display:flex><span>cur=<span style=color:#a31515>`</span>aws ec2 describe-snapshots --filter=<span style=color:#a31515>&#34;Name=description,Values=snap-</span>$name<span style=color:#a31515>&#34;</span> --query=<span style=color:#a31515>&#34;Snapshots[].Description&#34;</span> --output=text<span style=color:#a31515>`</span>
</span></span><span style=display:flex><span><span style=color:#00f>if</span> [ -n <span style=color:#a31515>&#34;</span>$cur<span style=color:#a31515>&#34;</span> ]; <span style=color:#00f>then</span>
</span></span><span style=display:flex><span>  echo <span style=color:#a31515>&#34;AMI </span>$name<span style=color:#a31515> exists as snapshot </span>$cur<span style=color:#a31515>&#34;</span>
</span></span><span style=display:flex><span>  <span style=color:#00f>continue</span>
</span></span><span style=display:flex><span><span style=color:#00f>fi</span>
</span></span><span style=display:flex><span>echo <span style=color:#a31515>&#34;AMI </span>$name<span style=color:#a31515> ... creating private snapshot&#34;</span>
</span></span><span style=display:flex><span>inst=<span style=color:#a31515>`</span>aws ec2 run-instances --instance-type t3.nano --image-id $ami --query <span style=color:#a31515>&#39;Instances[0].InstanceId&#39;</span> --output=text --subnet-id subnet-1234 --tag-specifications <span style=color:#a31515>&#39;ResourceType=instance,Tags=[{Key=scalemp-test,Value=scalemp-test}]&#39;</span><span style=color:#a31515>`</span> <span style=color:green>#Replace the subnet-id with the intended one.</span>
</span></span><span style=display:flex><span>aws ec2 wait instance-running --instance-ids $inst
</span></span><span style=display:flex><span>vol=<span style=color:#a31515>`</span>aws ec2 describe-instances --instance-ids $inst --query <span style=color:#a31515>&#34;Reservations[].Instances[].BlockDeviceMappings[0].Ebs.VolumeId&#34;</span> --output=text<span style=color:#a31515>`</span>
</span></span><span style=display:flex><span>snap=<span style=color:#a31515>`</span>aws ec2 create-snapshot --description <span style=color:#a31515>&#34;snap-</span>$name<span style=color:#a31515>&#34;</span> --volume-id $vol --query=<span style=color:#a31515>&#39;SnapshotId&#39;</span> --tag-specifications <span style=color:#a31515>&#34;ResourceType=snapshot,Tags=[{Key=Name,Value=\&#34;</span>$name<span style=color:#a31515>\&#34;}]&#34;</span> --output=text<span style=color:#a31515>`</span>
</span></span><span style=display:flex><span>aws ec2 wait snapshot-completed --snapshot-ids $snap
</span></span><span style=display:flex><span>aws ec2 terminate-instances --instance-id $inst &gt; /dev/null
</span></span><span style=display:flex><span>echo $snap
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-2d1349b5e5eed4ce62ec06792f100731>2.4 - Ubuntu OS</h1><div class=lead>Gardener extension controller for the Ubuntu operating system</div><h1 id=gardener-extension-for-ubuntu-oshttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Ubuntu OS</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-os-ubuntu-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-os-ubuntu-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-os-ubuntu><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-os-ubuntu alt="Go Report Card"></a></p><p>This controller operates on the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md#cloud-config-user-data-for-bootstrapping-machines><code>OperatingSystemConfig</code></a> resource in the <code>extensions.gardener.cloud/v1alpha1</code> API group. It manages those objects that are requesting <a href=https://www.ubuntu.com/>Ubuntu OS</a> configuration (<code>.spec.type=ubuntu</code>). An experimental support for Ubuntu Pro is added (<code>.spec.type=ubuntu-pro</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OperatingSystemConfig
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: pool-01-original
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: ubuntu
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>    ...
</span></span><span style=display:flex><span>  files:
</span></span><span style=display:flex><span>    ...
</span></span></code></pre></div><p>Please find <a href=https://github.com/gardener/gardener-extension-os-ubuntu/blob/master/example/40-operatingsystemconfig.yaml>a concrete example</a> in the <code>example</code> folder.</p><p>After reconciliation the resulting data will be stored in a secret within the same namespace (as the config itself might contain confidential data). The name of the secret will be written into the resource&rsquo;s <code>.status</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  cloudConfig:
</span></span><span style=display:flex><span>    secretRef:
</span></span><span style=display:flex><span>      name: osc-result-pool-01-original
</span></span><span style=display:flex><span>      namespace: default
</span></span><span style=display:flex><span>  command: /usr/bin/env bash &lt;path&gt;
</span></span><span style=display:flex><span>  units:
</span></span><span style=display:flex><span>  - docker-monitor.service
</span></span><span style=display:flex><span>  - kubelet-monitor.service
</span></span><span style=display:flex><span>  - kubelet.service
</span></span></code></pre></div><p>The secret has one data key <code>cloud_config</code> that stores the generation.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-os-ubuntu/blob/master/example/controller-registration.yaml>here</a>.</p><p>This controller is implemented using the <a href=https://github.com/gardener/gardener/blob/master/extensions/pkg/controller/operatingsystemconfig/oscommon/README.md><code>oscommon</code></a> library for operating system configuration controllers.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the <code>./dev/kubeconfig</code> file.
Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-os-ubuntu/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ec8342c0ff3d2e093022f079b19254f8>2.4.1 - Usage</h1><h1 id=using-the-ubuntu-extension-with-gardener-as-end-user>Using the Ubuntu extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a few fields that must be considered when this OS extension is used.</p><p>In this document we describe how this configuration looks like and under which circumstances your attention may be required.</p><h2 id=aws-vpc-settings-for-ubuntu-workers>AWS VPC settings for Ubuntu workers</h2><p>Gardener allows you to create Ubuntu based worker nodes by:</p><ol><li>Using a Gardener managed VPC</li><li>Reusing a VPC that already exists (VPC <code>id</code> specified in <a href=/docs/extensions/infrastructure-extensions/gardener-extension-provider-aws/usage/#infrastructureconfig>InfrastructureConfig</a>]</li></ol><p>If the second option applies to your use-case please make sure that your VPC has enabled <strong>DNS Support</strong>. Otherwise Ubuntu based nodes aren&rsquo;t able to join or operate in your cluster properly.</p><p><strong><a href=https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html>DNS</a></strong> settings (required):</p><ul><li><code>enableDnsHostnames</code>: true</li><li><code>enableDnsSupport</code>: true</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-55b1ec84af5ed73ab7f671bfe8da200e>3 - Network Extensions</h1><div class=lead>Gardener extension controllers for the supported container network interfaces</div></div><div class=td-content><h1 id=pg-306b134fc579fc34549bdcef99158c9b>3.1 - Calico CNI</h1><div class=lead>Gardener extension controller for the Calico CNI network plugin</div><h1 id=gardener-extension-for-calico-networkinghttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Calico Networking</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-networking-calico-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-networking-calico-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-networking-calico><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-networking-calico alt="Go Report Card"></a></p><p>This controller operates on the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/03-networking-extensibility.md#gardener-network-extension><code>Network</code></a> resource in the <code>extensions.gardener.cloud/v1alpha1</code> API group. It manages those objects that are requesting <a href=https://www.projectcalico.org/>Calico Networking</a> configuration (<code>.spec.type=calico</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Network
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: calico-network
</span></span><span style=display:flex><span>  namespace: shoot--core--test-01
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: calico
</span></span><span style=display:flex><span>  clusterCIDR: 192.168.0.0/24
</span></span><span style=display:flex><span>  serviceCIDR:  10.96.0.0/24
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: NetworkConfig
</span></span><span style=display:flex><span>    overlay:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>Please find <a href=https://github.com/gardener/gardener-extension-networking-calico/blob/master/example/20-network.yaml>a concrete example</a> in the <code>example</code> folder. All the <code>Calico</code> specific configuration
should be configured in the <code>providerConfig</code> section. If additional configuration is required, it should be added to
the <code>networking-calico</code> chart in <code>controllers/networking-calico/charts/internal/calico/values.yaml</code> and corresponding code
parts should be adapted (for example in <code>controllers/networking-calico/pkg/charts/utils.go</code>).</p><p>Once the network resource is applied, the <code>networking-calico</code> controller would then create all the necessary <code>managed-resources</code> which should be picked
up by the <a href=https://github.com/gardener/gardener-resource-manager>gardener-resource-manager</a> which will then apply all the
network extensions resources to the shoot cluster.</p><p>Finally after successful reconciliation an output similar to the one below should be expected.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  status:
</span></span><span style=display:flex><span>    lastOperation:
</span></span><span style=display:flex><span>      description: Successfully reconciled network
</span></span><span style=display:flex><span>      lastUpdateTime: <span style=color:#a31515>&#34;...&#34;</span>
</span></span><span style=display:flex><span>      progress: 100
</span></span><span style=display:flex><span>      state: Succeeded
</span></span><span style=display:flex><span>      type: Reconcile
</span></span><span style=display:flex><span>    observedGeneration: 1
</span></span><span style=display:flex><span>    providerStatus:
</span></span><span style=display:flex><span>      apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: NetworkStatus
</span></span></code></pre></div><h2 id=compatibility>Compatibility</h2><p>The following table lists known compatibility issues of this extension controller with other Gardener components.</p><table><thead><tr><th>Calico Extension</th><th>Gardener</th><th>Action</th><th>Notes</th></tr></thead><tbody><tr><td><code>>= v1.30.0</code></td><td><code>&lt; v1.63.0</code></td><td>Please first update Gardener components to <code>>= v1.63.0</code>.</td><td>Without the mentioned minimum Gardener version, Calico <code>Pod</code>s are not only scheduled to dedicated system component nodes in the shoot cluster.</td></tr></tbody></table><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the <code>kubeconfig</code> pointed to the cluster you want to connect to.
Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-networking-calico/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f2920606a4dac52b27a770078c916407>3.1.1 - Deployment</h1><h1 id=deployment-of-the-networking-calico-extension>Deployment of the networking Calico extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step deployment guide for the networking Calico extension and only contains some configuration specifics regarding the deployment of different components via the helm charts residing in the networking Calico extension <a href=https://github.com/gardener/gardener-extension-networking-calico>repository</a>.</p><h2 id=gardener-extension-admission-calico>gardener-extension-admission-calico</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of Virtual Garden</a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster>Virtual Garden is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><h5 id=automounted-service-account-token>Automounted Service Account Token</h5><p>The easiest way to deploy the <code>gardener-extension-admission-calico</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><h5 id=service-account-token-volume-projection>Service Account Token Volume Projection</h5><p>Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster>Virtual Garden is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><h5 id=service-account>Service Account</h5><p>The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><h5 id=client-certificate>Client Certificate</h5><p>Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><h5 id=projected-service-account-token>Projected Service Account Token</h5><p>This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-7b8605820c421d09eab14cf68a7e3d85>3.1.2 - Operations</h1><h1 id=using-the-calico-networking-extension-with-gardener-as-operator>Using the Calico networking extension with Gardener as operator</h1><p>This document explains configuration options supported by the networking-calico extension.</p><h3 id=run-calico-node-in-non-privileged-and-non-root-mode>Run calico-node in non-privileged and non-root mode</h3><p><strong>Feature State</strong>: <code>Alpha</code></p><h5 id=motivation>Motivation</h5><p>Running containers in privileged mode is not recommended as privileged containers run with all <a href=https://man7.org/linux/man-pages/man7/capabilities.7.html>linux capabilities</a> enabled and can access the host&rsquo;s resources. Running containers in privileged mode opens number of security threats such as breakout to underlying host OS.</p><h5 id=support-for-non-privileged-and-non-root-mode>Support for non-privileged and non-root mode</h5><p>The Calico project has a preliminary support for running the calico-node component in non-privileged mode (see <a href=https://projectcalico.docs.tigera.io/security/non-privileged>this guide</a>). Similar to <a href=https://github.com/tigera/operator>Tigera Calico operator</a> the networking-calico extension can also run calico-node in non-privileged and non-root mode. This feature is controller via feature gate named <code>NonPrivilegedCalicoNode</code>. The feature gates are configured in the <a href=https://github.com/gardener/gardener-extension-networking-calico/blob/master/example/00-componentconfig.yaml>ControllerConfiguration</a> of networking-calico. The corresponding ControllerDeployment configuration that enables the <code>NonPrivilegedCalicoNode</code> would look like:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: networking-calico
</span></span><span style=display:flex><span>type: helm
</span></span><span style=display:flex><span>providerConfig:
</span></span><span style=display:flex><span>  values:
</span></span><span style=display:flex><span>    chart: &lt;omitted&gt;
</span></span><span style=display:flex><span>    config:
</span></span><span style=display:flex><span>      featureGates:
</span></span><span style=display:flex><span>        NonPrivilegedCalicoNode: <span style=color:#00f>false</span>
</span></span></code></pre></div><h5 id=limitations>Limitations</h5><ul><li>The support for the non-privileged mode in the Calico project is not ready for productive usage. The <a href=https://projectcalico.docs.tigera.io/security/non-privileged>upstream documentation</a> states that in non-privileged mode the support for features added after Calico v3.21 is not guaranteed.</li><li>Calico in non-privileged mode does not support eBPF dataplane. That&rsquo;s why when eBPF dataplane is enabled, calico-node has to run in privileged mode (even when the <code>NonPrivilegedCalicoNode</code> feature gate is enabled).</li><li>(At the time of writing this guide) there is the following issue <a href=https://github.com/projectcalico/calico/issues/5348>projectcalico/calico#5348</a> that is not addressed.</li><li>(At the time of writing this guide) the upstream adoptions seems to be low. The Calico charts and manifest in <a href=https://github.com/projectcalico/calico>projectcalico/calico</a> run calico-node in privileged mode.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0a971930c7387d0186d6b88a9cdeb689>3.1.3 - Shoot Overlay Network</h1><h1 id=enable--disable-overlay-network-for-shoots-with-calico>Enable / disable overlay network for shoots with Calico</h1><p>Gardener can be used with or without the overlay network.</p><p>Starting versions:</p><ul><li><a href=https://github.com/gardener/gardener-extension-provider-gcp/releases/tag/v1.25.0>provider-gcp@v1.25.0</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-alicloud/releases/tag/v1.43.0>provider-alicloud@v1.43.0</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-aws/releases/tag/v1.38.2>provider-aws@v1.38.2</a></li><li><a href=https://github.com/gardener/gardener-extension-provider-openstack/releases/tag/v1.30.0>provider-openstack@v1.30.0</a></li></ul><p>The default configuration of shoot clusters is without overlay network.</p><h2 id=understanding-overlay-network>Understanding overlay network</h2><p>The Overlay networking permits the routing of packets between multiples pods located on multiple nodes, even if the pod and the node network are not the same.</p><p>This is done through the encapsulation of pod packets in the node network so that the routing can be done as usual. We use <code>ipip</code> encapsulation with calico in case the overlay network is enabled. This (simply put) sends an IP packet as workload in another IP packet.</p><p><img src=/__resources/Overlay-Network.drawio_ebaafc.png alt></p><p>In order to simplify the troubleshooting of problems and reduce the latency of packets traveling between nodes, the overlay network is disabled by default as stated above for all new clusters.</p><p><img src=/__resources/No-Overlay-Network.drawio_db83e3.png alt></p><p>This means that the routing is done directly through the VPC routing table. Basically, when a new node is created, it is assigned a slice (usually a /24) of the pod network. All future pods in that node are going to be in this slice. Then, the cloud-controller-manager updates the cloud provider router to add the new route (all packets within the network slice as destination should go to that node).</p><p>This has the advantage of:</p><ul><li>Doing less work for the node as encapsulation takes some CPU cycles.</li><li>The maximum transmission unit (MTU) is slightly bigger resulting in slightly better performance, i.e. potentially more workload bytes per packet.</li><li>More direct and simpler setup, which makes the problems much easier to troubleshoot.</li></ul><p><strong>In the case where multiple shoots are in the same VPC and the overlay network is disabled, if the pod&rsquo;s network is not configured properly, there is a very strong chance that some pod IP address might overlap, which is going to cause all sorts of funny problems.</strong> So, if someone asks you how to avoid that, they need to make sure that the podCIDRs for each shoot <strong>do not overlap with each other</strong>.</p><h2 id=enabling-the-overlay-network>Enabling the overlay network</h2><p>In certain cases, the overlay network might be preferable if, for example, the customer wants to create multiple clusters in the same VPC without ensuring there&rsquo;s no overlap between the pod networks.</p><p>To enable the overlay network, add the following to the shoot&rsquo;s YAML:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: NetworkConfig
</span></span><span style=display:flex><span>      overlay:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><h2 id=disabling-the-overlay-network>Disabling the overlay network</h2><p>Inversely, here is how to disable the overlay network:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: NetworkConfig
</span></span><span style=display:flex><span>      overlay:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><h2 id=how-to-know-if-a-cluster-is-using-overlay-or-not>How to know if a cluster is using overlay or not?</h2><p>You can look at any of the old nodes. If there are <code>tunl0</code> devices at least at some point in time the overlay network was used.
Another way is to look into the Network object in the shoot&rsquo;s control plane namespace on the seed (see example above).</p><h2 id=do-we-have-some-documentation-somewhere-on-how-to-do-the-migration>Do we have some documentation somewhere on how to do the migration?</h2><p>No, not yet. The migration from no overlay to overlay is fairly simply by just setting the configuration as specified above. The other way is more complicated as the Network configuration needs to be changed AND the local routes need to be cleaned.
Unfortunately, the change will be rolled out slowly (one calico-node at a time). Hence, it implies some network outages during the migration.</p><h2 id=aws-implementation>AWS implementation</h2><p>On AWS, it is not possible to use the cloud-controller-manager for managing the routes as it does not support multiple route tables, which Gardener creates. Therefore, a custom controller is created to manage the routes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2082dbad51907d23dce5fffeb6099f2e>3.1.4 - Usage</h1><h1 id=using-the-networking-calico-extension-with-gardener-as-end-user>Using the Networking Calico extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a <code>networking</code> field that is meant to contain network-specific configuration.</p><p>In this document we are describing how this configuration looks like for Calico and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create a cluster.</p><h2 id=calico-typha>Calico Typha</h2><p>Calico Typha is an optional component of Project Calico designed to offload the Kubernetes API server. The Typha daemon sits between the datastore (such as the Kubernetes API server which is the one used by Gardener managed Kubernetes) and many instances of Felix. Typha’s main purpose is to increase scale by reducing each node’s impact on the datastore. You can opt-out Typha via <code>.spec.networking.providerConfig.typha.enabled=false</code> of your Shoot manifest. By default the Typha is enabled.</p><h2 id=ebpf-dataplane>EBPF Dataplane</h2><p>Calico can be run in ebpf dataplane mode. This has several benefits, calico scales to higher troughput, uses less cpu per GBit and has native support for kubernetes services (without needing kube-proxy).
To switch to a pure ebpf dataplane it is recommended to run without an overlay network. The following configuration can be used to run without an overlay and without kube-proxy.</p><p>An example ebpf dataplane <code>NetworkingConfig</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: NetworkConfig
</span></span><span style=display:flex><span>ebpfDataplane:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>overlay:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>To disable kube-proxy set the enabled field to false in the shoot manifest.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: ebpf-shoot
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeProxy:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>false</span>
</span></span></code></pre></div><h3 id=know-limitations-of-the-ebpf-dataplane>Know limitations of the EBPF Dataplane</h3><p>Please note that the default settings for calico&rsquo;s ebpf dataplane may interfere with
<a href=https://learn.microsoft.com/en-us/azure/virtual-network/accelerated-networking-overview>accelerated networking in azure</a>
rendering nodes with accelerated networking unusable in the network. The reason for this is that calico does not ignore
the accelerated networking interface <code>enP...</code> as it should, but applies its ebpf programs to it. A simple mitigation for
this is to adapt the <code>FelixConfiguration</code> <code>default</code> and ensure that the <code>bpfDataIfacePattern</code> does not include <code>enP...</code>.
Per default <code>bpfDataIfacePattern</code> is not set. The default value for this option can be found
<a href=https://github.com/projectcalico/calico/blob/3f7fe4d290541bbdd73c97bdc89a29a29855a48a/felix/config/config_params.go#L180>here</a>.
For example, you could apply the following change:</p><pre tabindex=0><code>$ kubectl edit felixconfiguration default
...
apiVersion: crd.projectcalico.org/v1
kind: FelixConfiguration
metadata:
  ...
  name: default
  ...
spec:
  bpfDataIfacePattern: ^((en|wl|ww|sl|ib)[opsx].*|(eth|wlan|wwan).*|tunl0$|vxlan.calico$|wireguard.cali$|wg-v6.cali$)
  ...
</code></pre><h2 id=autoscaling>AutoScaling</h2><p>Autoscaling defines how the calico components are automatically scaled. It allows to use either vertical pod or cluster-proportional autoscaler (default: cluster-proportional).</p><p>The cluster-proportional autoscaling mode is preferable when conditions require minimimal disturbances and vpa mode for improved cluster resource utilization.</p><p>Please note VPA must be enabled on the shoot as a pre-requisite to enabling vpa mode.</p><p>An example AutoScaling <code>NetworkingConfig</code> manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: NetworkConfig
</span></span><span style=display:flex><span>autoScaling:
</span></span><span style=display:flex><span>  mode: <span style=color:#a31515>&#34;vpa&#34;</span>
</span></span></code></pre></div><h2 id=example-networkingconfig-manifest>Example <code>NetworkingConfig</code> manifest</h2><p>An example <code>NetworkingConfig</code> for the Calico extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: NetworkConfig
</span></span><span style=display:flex><span>ipam:
</span></span><span style=display:flex><span>  type: host-local
</span></span><span style=display:flex><span>  cidr: usePodCIDR
</span></span><span style=display:flex><span>vethMTU: 1440
</span></span><span style=display:flex><span>typha:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>overlay:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>autoScaling:
</span></span><span style=display:flex><span>  mode: <span style=color:#a31515>&#34;vpa&#34;</span>
</span></span></code></pre></div><h2 id=example-shoot-manifest>Example <code>Shoot</code> manifest</h2><p>Please find below an example <code>Shoot</code> manifest with calico networking configratations:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: johndoe-azure
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  cloudProfileName: azure
</span></span><span style=display:flex><span>  region: westeurope
</span></span><span style=display:flex><span>  secretBindingName: core-azure
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    type: azure
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        vnet:
</span></span><span style=display:flex><span>          cidr: 10.250.0.0/16
</span></span><span style=display:flex><span>        workers: 10.250.0.0/19
</span></span><span style=display:flex><span>      zoned: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: azure.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - name: worker-xoluy
</span></span><span style=display:flex><span>      machine:
</span></span><span style=display:flex><span>        type: Standard_D4_v3
</span></span><span style=display:flex><span>      minimum: 2
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: Standard_LRS
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#34;1&#34;</span>
</span></span><span style=display:flex><span>      - <span style=color:#a31515>&#34;2&#34;</span>
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: calico.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: NetworkConfig
</span></span><span style=display:flex><span>      ipam:
</span></span><span style=display:flex><span>        type: host-local
</span></span><span style=display:flex><span>      vethMTU: 1440
</span></span><span style=display:flex><span>      overlay:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      typha:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    version: 1.24.3
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  addons:
</span></span><span style=display:flex><span>    kubernetesDashboard:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    nginxIngress:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h2 id=known-limitations-in-conjunction-with-nodelocaldns>Known Limitations in conjunction with <code>NodeLocalDNS</code></h2><p>If <a href=/docs/gardener/node-local-dns/><code>NodeLocalDNS</code></a> is active in a shoot cluster, which uses calico as CNI without overlay network, it may be impossible to block DNS traffic to the cluster DNS server via network policy. This is due to <code>FELIX_CHAININSERTMODE</code> being set to <code>APPEND</code> instead of <code>INSERT</code> in case SNAT is being applied to requests to the infrastructure DNS server. In this scenario the <code>iptables</code> rules of <code>NodeLocalDNS</code> already accept the traffic before the network policies are checked.</p><p>This only applies to traffic directed to <code>NodeLocalDNS</code>. If blocking of all DNS traffic is desired via network policy the pod <code>dnsPolicy</code> should be changed to <code>Default</code> so that the cluster DNS is not used. Alternatives are usage of overlay network or disabling of <code>NodeLocalDNS</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4892da3b15a7be1985453914cbefd13a>3.2 - Cilium CNI</h1><div class=lead>Gardener extension controller for the Cilium CNI network plugin</div><h1 id=gardener-extension-for-cilium-networkinghttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for cilium Networking</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-networking-cilium-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-networking-cilium-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-networking-cilium><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-networking-cilium alt="Go Report Card"></a></p><p>This controller operates on the <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/03-networking-extensibility.md#gardener-network-extension><code>Network</code></a> resource in the <code>extensions.gardener.cloud/v1alpha1</code> API group. It manages those objects that are requesting <a href=https://cilium.io/>cilium Networking</a> configuration (<code>.spec.type=cilium</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Network
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cilium-network
</span></span><span style=display:flex><span>  namespace: shoot--foo--bar
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: cilium
</span></span><span style=display:flex><span>  podCIDR: 10.244.0.0/16
</span></span><span style=display:flex><span>  serviceCIDR:  10.96.0.0/24
</span></span><span style=display:flex><span>  providerConfig:
</span></span><span style=display:flex><span>    apiVersion: cilium.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>    kind: NetworkConfig
</span></span><span style=display:flex><span><span style=color:green>#    hubble:</span>
</span></span><span style=display:flex><span><span style=color:green>#      enabled: true</span>
</span></span><span style=display:flex><span><span style=color:green>#    store: kubernetes</span>
</span></span></code></pre></div><p>Please find <a href=https://github.com/gardener/gardener-extension-networking-cilium/blob/master/example/20-network.yaml>a concrete example</a> in the <code>example</code> folder. All the <code>cilium</code> specific configuration
should be configured in the <code>providerConfig</code> section. If additional configuration is required, it should be added to
the <code>networking-cilium</code> chart in <code>controllers/networking-cilium/charts/internal/cilium/values.yaml</code> and corresponding code
parts should be adapted (for example in <code>controllers/networking-cilium/pkg/charts/utils.go</code>).</p><p>Once the network resource is applied, the <code>networking-cilium</code> controller would then create all the necessary <code>managed-resources</code> which should be picked
up by the <a href=https://github.com/gardener/gardener-resource-manager>gardener-resource-manager</a> which will then apply all the
network extensions resources to the shoot cluster.</p><p>Finally after successful reconciliation an output similar to the one below should be expected.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  status:
</span></span><span style=display:flex><span>    lastOperation:
</span></span><span style=display:flex><span>      description: Successfully reconciled network
</span></span><span style=display:flex><span>      lastUpdateTime: <span style=color:#a31515>&#34;...&#34;</span>
</span></span><span style=display:flex><span>      progress: 100
</span></span><span style=display:flex><span>      state: Succeeded
</span></span><span style=display:flex><span>      type: Reconcile
</span></span><span style=display:flex><span>    observedGeneration: 1
</span></span></code></pre></div><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the <code>kubeconfig</code> pointed to the cluster you want to connect to.
Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-networking-cilium/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://docs.cilium.io/>Docs for <code>cilium</code> user</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ff35bd2db0ed090c7cdd3ca86a7dd125>3.2.1 - Usage</h1><h1 id=using-the-networking-cilium-extension-with-gardener-as-end-user>Using the Networking Cilium extension with Gardener as end-user</h1><p>The <a href=https://github.com/gardener/gardener/blob/master/example/90-shoot.yaml><code>core.gardener.cloud/v1beta1.Shoot</code> resource</a> declares a <code>networking</code> field that is meant to contain network-specific configuration.</p><p>In this document we are describing how this configuration looks like for Cilium and provide an example <code>Shoot</code> manifest with minimal configuration that you can use to create a cluster.</p><h2 id=cilium-hubble>Cilium Hubble</h2><p>Hubble is a fully distributed networking and security observability platform build on top of Cilium and BPF. It is optional and is deployed to the cluster when enabled in the <code>NetworkConfig</code>.
If the dashboard is not externally exposed</p><pre tabindex=0><code>kubectl port-forward -n kube-system deployment/hubble-ui 8081
</code></pre><p>can be used to acess it locally.</p><h2 id=example-networkingconfig-manifest>Example <code>NetworkingConfig</code> manifest</h2><p>An example <code>NetworkingConfig</code> for the Cilium extension looks as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cilium.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: NetworkConfig
</span></span><span style=display:flex><span>hubble:
</span></span><span style=display:flex><span>  enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span><span style=color:green>#debug: false</span>
</span></span><span style=display:flex><span><span style=color:green>#psp: true</span>
</span></span><span style=display:flex><span><span style=color:green>#tunnel: vxlan</span>
</span></span><span style=display:flex><span><span style=color:green>#store: kubernetes</span>
</span></span></code></pre></div><h2 id=networkingconfig-options><code>NetworkingConfig</code> options</h2><p>The <code>hubble.enabled</code> field describes whether hubble should be deployed into the cluster or not (default).</p><p>The <code>debug</code> field describes whether you want to run cilium in debug mode or not (default), change this value to <code>true</code> to use debug mode.</p><p>The <code>psp</code> field describes whether <code>cilium-operator</code> and <code>cilium-agent</code> shall be deployed with pod security policies or not (default).</p><p>The <code>tunnel</code> field describes the encapsulation mode for communication between nodes. Possible values are <code>vxlan</code> (default), <code>geneve</code> or <code>disabled</code>.</p><p>The <code>bpfSocketLBHostnsOnly.enabled</code> field describes whether socket LB will be skipped for services when inside a pod namespace (default), in favor of service LB at the pod interface. Socket LB is still used when in the host namespace. This feature is required when using cilium with a service mesh like istio or linkerd.</p><p>Setting the field <code>cni.exclusive</code> to <code>false</code> might be useful when additional plugins, such as Istio or Linkerd, wish to chain after Cilium. This action disables the default behavior of Cilium, which is to overwrite changes to the CNI configuration file.</p><p>The <code>egressGateway.enabled</code> field describes whether egress gateways are enabled or not (default). To use this feature kube-proxy must be disabled. This can be done with the following configuration in the Shoot:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    kubeProxy:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>false</span>
</span></span></code></pre></div><p>The egress gateway feature is only supported in gardener with an overlay network (shoot.spec.networking.providerConfig.overlay.enabled: true) at the moment. This is due to the reason that bpf masquerading is required for the egress gateway feature. Once the overlay network is enabled <code>bpf.masquerade</code> is set to <code>true</code> in the cilium configmap.</p><p>The <code>snatToUpstreamDNS.enabled</code> field describes whether the traffic to the upstream dns server should be masqueraded or not (default). This is needed on some infrastructures where traffic to the dns server with the pod CIDR range is blocked.</p><h2 id=example-shoot-manifest>Example <code>Shoot</code> manifest</h2><p>Please find below an example <code>Shoot</code> manifest with cilium networking configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: aws-cilium
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    type: cilium
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: cilium.networking.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: NetworkConfig
</span></span><span style=display:flex><span>      hubble:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    pods: 100.96.0.0/11
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    services: 100.64.0.0/13
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>If you would like to see a provider specific shoot example, please check out the documentation of the well-known extensions. A list of them can be found <a href=https://github.com/gardener/gardener/tree/master/extensions#infrastructure-provider>here</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-43f4dc9f68e5519a75d0b2b944be89d1>4 - Container Runtime Extensions</h1><div class=lead>Gardener extensions for the supported container runtime interfaces</div></div><div class=td-content><h1 id=pg-60e80e50f871d6bf3e0bd35d36ca22c2>4.1 - GVisor container runtime</h1><div class=lead>Gardener extension controller for the gVisor container runtime sandbox</div><h1 id=gardener-extension-for-the-gvisor-container-runtime-sandboxhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for the gVisor Container Runtime Sandbox</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener-tests/pipelines/gardener-extension-runtime-gvisor-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener-tests/pipelines/gardener-extension-runtime-gvisor-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-runtime-gvisor><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-runtime-gvisor alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>. However, the project has grown to a size where it is very hard to extend, maintain, and test. With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.</p><hr><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the <code>./dev/kubeconfig</code> file.</p><p>Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-runtime-gvisor/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/10-shoot-additional-container-runtimes.md>GEP-10 (Additional Container Runtimes)</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cffa3bdf900ab3321268d1bbbd4743ff>5 - Others</h1><div class=lead>Other Gardener extensions</div></div><div class=td-content><h1 id=pg-ed74ceaa562b93985c3d5ce5042a7a22>5.1 - Certificate services</h1><div class=lead>Gardener extension controller for certificate services for shoot clusters</div><h1 id=gardener-extension-for-certificate-serviceshttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for certificate services</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener/pipelines/gardener-extension-shoot-cert-service-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener/pipelines/gardener-extension-shoot-cert-service-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-shoot-cert-service><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-shoot-cert-service alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>. However, the project has grown to a size where it is very hard to extend, maintain, and test. With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.</p><h2 id=configuration>Configuration</h2><p>Example configuration for this extension controller:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: shoot-cert-service.extensions.config.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Configuration
</span></span><span style=display:flex><span>issuerName: gardener
</span></span><span style=display:flex><span>restrictIssuer: <span style=color:#00f>true</span> <span style=color:green># restrict issuer to any sub-domain of shoot.spec.dns.domain (default)</span>
</span></span><span style=display:flex><span>acme:
</span></span><span style=display:flex><span>  email: john.doe@example.com
</span></span><span style=display:flex><span>  server: https://acme-v02.api.letsencrypt.org/directory
</span></span><span style=display:flex><span><span style=color:green># privateKey: | # Optional key for Let&#39;s Encrypt account.</span>
</span></span><span style=display:flex><span><span style=color:green>#   -----BEGIN BEGIN RSA PRIVATE KEY-----</span>
</span></span><span style=display:flex><span><span style=color:green>#   ...</span>
</span></span><span style=display:flex><span><span style=color:green>#   -----END RSA PRIVATE KEY-----</span>
</span></span></code></pre></div><h2 id=extension-resources>Extension-Resources</h2><p>Example extension resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Extension
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: <span style=color:#a31515>&#34;extension-certificate-service&#34;</span>
</span></span><span style=display:flex><span>  namespace: shoot--project--abc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: shoot-cert-service
</span></span></code></pre></div><p>When an extension resource is reconciled, the extension controller will create an instance of <a href=https://github.com/gardener/cert-management>Cert-Management</a> as well as an <code>Issuer</code> with the ACME information provided in the <a href=#Configuration>configuration</a> above. These resources are placed inside the shoot namespace on the seed. Also, the controller takes care about generating necessary <code>RBAC</code> resources for the seed as well as for the shoot.</p><p>Please note, this extension controller relies on the <a href=https://github.com/gardener/gardener-resource-manager>Gardener-Resource-Manager</a> to deploy k8s resources to seed and shoot clusters, i.e. it never deploys them directly.</p><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the <code>./dev/kubeconfig</code> file.
Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-shoot-cert-service/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a7493d3f6af5f7614666d700176dc331>5.1.1 - Changing alerting settings</h1><div class=lead>How to change the alerting on expiring certificates</div><h1 id=changing-alerting-settings>Changing alerting settings</h1><p>Certificates are normally renewed automatically 30 days before they expire.
As a second line of defense, there is an alerting in Prometheus activated if the certificate is a few days
before expiration. By default, the alert is triggered 15 days before expiration.</p><p>You can configure the days in the <code>providerConfig</code> of the extension.
Setting it to 0 disables the alerting.</p><p>In this example, the days are changed to 3 days before expiration.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: shoot-cert-service
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: CertConfig
</span></span><span style=display:flex><span>      alerting:
</span></span><span style=display:flex><span>        certExpirationAlertDays: 3
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-938a1eb564207b1aa0e30daeca7ffeda>5.1.2 - Manage certificates with Gardener for default domain</h1><div class=lead>Use the Gardener cert-management to get fully managed, publicly trusted TLS certificates</div><h1 id=manage-certificates-with-gardener-for-default-domain>Manage certificates with Gardener for default domain</h1><h2 id=introduction>Introduction</h2><p>Dealing with applications on Kubernetes which offer a secure service endpoints (e.g. HTTPS) also require you to enable a
secured communication via SSL/TLS. With the <a href=https://github.com/gardener/gardener-extension-shoot-cert-service>certificate extension</a> enabled, Gardener can manage commonly trusted X.509 certificate for your application
endpoint. From initially requesting certificate, it also handeles their renewal in time using the free Let&rsquo;s Encrypt API.</p><p><strong>There are two senarios with which you can use the certificate extension</strong></p><ul><li>You want to use a certificate for a subdomain the shoot&rsquo;s default DNS (see <code>.spec.dns.domain</code> of your shoot resource, e.g. <code>short.ingress.shoot.project.default-domain.gardener.cloud</code>). If this is your case, please keep reading this article.</li><li>You want to use a certificate for a custom domain. If this is your case, please see <a href=/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/>Manage certificates with Gardener for public domain</a></li></ul><h2 id=prerequisites>Prerequisites</h2><p>Before you start this guide there are a few requirements you need to fulfill:</p><ul><li>You have an existing shoot cluster</li></ul><p>Since you are using the default DNS name, all DNS configuration should already be done and ready.</p><h2 id=issue-a-certificate>Issue a certificate</h2><p>Every X.509 certificate is represented by a Kubernetes custom resource <code>certificate.cert.gardener.cloud</code> in your cluster. A <code>Certificate</code> resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener&rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.</p><blockquote><p>Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.</p></blockquote><p>Certificates can be requested via 3 resources type</p><ul><li>Ingress</li><li>Service (type LoadBalancer)</li><li>certificate (Gardener CRD)</li></ul><p>If either of the first 2 are used, a corresponding <code>Certificate</code> resource will automatically be created.</p><h3 id=using-an-ingress-resource>Using an ingress Resource</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/issuer: custom-issuer                    # optional to specify custom issuer (use namespace/name for shoot issuers)</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/follow-cname: &#34;true&#34;                     # optional, same as spec.followCNAME in certificates</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/secret-labels: &#34;key1=value1,key2=value2&#34; # optional labels for the certificate secret</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/preferred-chain: &#34;chain name&#34;            # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tls:
</span></span><span style=display:flex><span>  - hosts:
</span></span><span style=display:flex><span>    <span style=color:green># Must not exceed 64 characters.</span>
</span></span><span style=display:flex><span>    - short.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>    <span style=color:green># Certificate and private key reside in this secret.</span>
</span></span><span style=display:flex><span>    secretName: tls-secret
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: short.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span></code></pre></div><h3 id=using-a-service-type-loadbalancer>Using a service type LoadBalancer</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    <span style=color:green># Certificate and private key reside in this secret.</span>
</span></span><span style=display:flex><span>    cert.gardener.cloud/secretname: tls-secret
</span></span><span style=display:flex><span>    <span style=color:green># You may add more domains separated by commas (e.g. &#34;service.shoot.project.default-domain.gardener.cloud, amazing.shoot.project.default-domain.gardener.cloud&#34;)</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: <span style=color:#a31515>&#34;service.shoot.project.default-domain.gardener.cloud&#34;</span> 
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/issuer: custom-issuer                    # optional to specify custom issuer (use namespace/name for shoot issuers)</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/follow-cname: &#34;true&#34;                     # optional, same as spec.followCNAME in certificates</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/secret-labels: &#34;key1=value1,key2=value2&#34; # optional labels for the certificate secret</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/preferred-chain: &#34;chain name&#34;            # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)</span>
</span></span><span style=display:flex><span>  name: test-service
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>    - name: http
</span></span><span style=display:flex><span>      port: 80
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>      targetPort: 8080
</span></span><span style=display:flex><span>  type: LoadBalancer
</span></span></code></pre></div><h3 id=using-the-custom-certificate-resource>Using the custom Certificate resource</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Certificate
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cert-example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  commonName: short.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: tls-secret
</span></span><span style=display:flex><span>    namespace: default
</span></span><span style=display:flex><span>  <span style=color:green># Optionnal if using the default issuer</span>
</span></span><span style=display:flex><span>  issuerRef:
</span></span><span style=display:flex><span>    name: garden
</span></span></code></pre></div><p>If you&rsquo;re interested in the current progress of your request, you&rsquo;re advised to consult the description, more specifically the <code>status</code> attribute in case the issuance failed.</p><h2 id=request-a-wildcard-certificate>Request a wildcard certificate</h2><p>In order to avoid the creation of multiples certificates for every single endpoints, you may want to create a wildcard certificate for your shoot&rsquo;s default cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    cert.gardener.cloud/commonName: <span style=color:#a31515>&#34;*.ingress.shoot.project.default-domain.gardener.cloud&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tls:
</span></span><span style=display:flex><span>  - hosts:
</span></span><span style=display:flex><span>    - amazing.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>    secretName: tls-secret
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: amazing.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span></code></pre></div><p>Please note that this can also be achived by directly adding an annotation to a Service type LoadBalancer. You could also create a Certificate object with a wildcard domain.</p><h2 id=more-information>More information</h2><p>For more information and more examples about using the certificate extension, please see <a href=/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/>Manage certificates with Gardener for public domain</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-37cf7e12cfd0a0c8e335a954d4e324a1>5.1.3 - Manage certificates with Gardener for public domain</h1><div class=lead>Use the Gardener cert-management to get fully managed, publicly trusted TLS certificates</div><h1 id=manage-certificates-with-gardener-for-public-domain>Manage certificates with Gardener for public domain</h1><h2 id=introduction>Introduction</h2><p>Dealing with applications on Kubernetes which offer a secure service endpoints (e.g. HTTPS) also require you to enable a
secured communication via SSL/TLS. With the <a href=https://github.com/gardener/gardener-extension-shoot-cert-service>certificate extension</a> enabled, Gardener can manage commonly trusted X.509 certificate for your application
endpoint. From initially requesting certificate, it also handeles their renewal in time using the free Let&rsquo;s Encrypt API.</p><p><strong>There are two senarios with which you can use the certificate extension</strong></p><ul><li>You want to use a certificate for a subdomain the shoot&rsquo;s default DNS (see <code>.spec.dns.domain</code> of your shoot resource, e.g. <code>short.ingress.shoot.project.default-domain.gardener.cloud</code>). If this is your case, please see <a href=/docs/extensions/others/gardener-extension-shoot-cert-service/request_default_domain_cert/>Manage certificates with Gardener for default domain</a></li><li>You want to use a certificate for a custom domain. If this is your case, please keep reading this article.</li></ul><h2 id=prerequisites>Prerequisites</h2><p>Before you start this guide there are a few requirements you need to fulfill:</p><ul><li>You have an existing shoot cluster</li><li>Your custom domain is under a <a href=https://www.iana.org/domains/root/db>public top level domain</a> (e.g. <code>.com</code>)</li><li>Your custom zone is resolvable with a public resolver via the internet (e.g. <code>8.8.8.8</code>)</li><li>You have a custom DNS provider configured and working (see <a href=/docs/extensions/others/gardener-extension-shoot-dns-service/dns_providers/>&ldquo;DNS Providers&rdquo;</a>)</li></ul><p>As part of the <a href=https://letsencrypt.org/>Let&rsquo;s Encrypt</a> <a href=https://tools.ietf.org/html/rfc8555>ACME</a> challenge validation process, Gardener sets a DNS TXT entry and Let&rsquo;s Encrypt checks if it can both resolve and authenticate it. Therefore, it&rsquo;s important that your DNS-entries are publicly resolvable. You can check this by querying e.g. Googles public DNS server and if it returns an entry your DNS is publicly visible:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># returns the A record for cert-example.example.com using Googles DNS server (8.8.8.8)</span>
</span></span><span style=display:flex><span>dig cert-example.example.com @8.8.8.8 A
</span></span></code></pre></div><h3 id=dns-provider>DNS provider</h3><p>In order to issue certificates for a custom domain you need to specify a DNS provider which is permitted to create DNS records for subdomains of your requested domain in the certificate. For example, if you request a certificate for <code>host.example.com</code> your DNS provider must be capable of managing subdomains of <code>host.example.com</code>.</p><p>DNS providers are normally specified in the shoot manifest. To learn more on how to configure one, please see the <a href=/docs/extensions/others/gardener-extension-shoot-dns-service/dns_providers/>DNS provider</a> documentation.</p><h2 id=issue-a-certificate>Issue a certificate</h2><p>Every X.509 certificate is represented by a Kubernetes custom resource <code>certificate.cert.gardener.cloud</code> in your cluster. A <code>Certificate</code> resource may be used to initiate a new certificate request as well as to manage its lifecycle. Gardener&rsquo;s certificate service regularly checks the expiration timestamp of Certificates, triggers a renewal process if necessary and replaces the existing X.509 certificate with a new one.</p><blockquote><p>Your application should be able to reload replaced certificates in a timely manner to avoid service disruptions.</p></blockquote><p>Certificates can be requested via 3 resources type</p><ul><li>Ingress</li><li>Service (type LoadBalancer)</li><li>Certificate (Gardener CRD)</li></ul><p>If either of the first 2 are used, a corresponding <code>Certificate</code> resource will be created automatically.</p><h3 id=using-an-ingress-resource>Using an ingress Resource</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    <span style=color:green># Optional but recommended, this is going to create the DNS entry at the same time</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/commonname: &#34;*.example.com&#34;              # optional, if not specified the first name from spec.tls[].hosts is used as common name</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/dnsnames: &#34;&#34;                             # optional, if not specified the names from spec.tls[].hosts are used</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/follow-cname: &#34;true&#34;                     # optional, same as spec.followCNAME in certificates</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/secret-labels: &#34;key1=value1,key2=value2&#34; # optional labels for the certificate secret</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/issuer: custom-issuer                    # optional to specify custom issuer (use namespace/name for shoot issuers)</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/preferred-chain: &#34;chain name&#34;            # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tls:
</span></span><span style=display:flex><span>  - hosts:
</span></span><span style=display:flex><span>    <span style=color:green># Must not exceed 64 characters.</span>
</span></span><span style=display:flex><span>    - amazing.example.com
</span></span><span style=display:flex><span>    <span style=color:green># Certificate and private key reside in this secret.</span>
</span></span><span style=display:flex><span>    secretName: tls-secret
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: amazing.example.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span></code></pre></div><p>Replace the <code>hosts</code> and <code>rules[].host</code> value again with your own domain and adjust the remaining Ingress attributes in accordance with your deployment (e.g. the above is for an <code>istio</code> Ingress controller and forwards traffic to a <code>service1</code> on port 80).</p><h3 id=using-a-service-type-loadbalancer>Using a service type LoadBalancer</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/secretname: tls-secret
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: example.example.com
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>    <span style=color:green># Optional</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    cert.gardener.cloud/commonname: <span style=color:#a31515>&#34;*.example.example.com&#34;</span>
</span></span><span style=display:flex><span>    cert.gardener.cloud/dnsnames: <span style=color:#a31515>&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/follow-cname: &#34;true&#34;                     # optional, same as spec.followCNAME in certificates</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/secret-labels: &#34;key1=value1,key2=value2&#34; # optional labels for the certificate secret</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/issuer: custom-issuer                    # optional to specify custom issuer (use namespace/name for shoot issuers)</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/preferred-chain: &#34;chain name&#34;            # optional to specify preferred-chain (value is the Subject Common Name of the root issuer)</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>  name: test-service
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>    - name: http
</span></span><span style=display:flex><span>      port: 80
</span></span><span style=display:flex><span>      protocol: TCP
</span></span><span style=display:flex><span>      targetPort: 8080
</span></span><span style=display:flex><span>  type: LoadBalancer
</span></span></code></pre></div><h3 id=using-the-custom-certificate-resource>Using the custom Certificate resource</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Certificate
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cert-example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  commonName: amazing.example.com
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: tls-secret
</span></span><span style=display:flex><span>    namespace: default
</span></span><span style=display:flex><span>  <span style=color:green># Optionnal if using the default issuer</span>
</span></span><span style=display:flex><span>  issuerRef:
</span></span><span style=display:flex><span>    name: garden
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># If delegated domain for DNS01 challenge should be used. This has only an effect if a CNAME record is set for</span>
</span></span><span style=display:flex><span>  <span style=color:green># &#39;_acme-challenge.amazing.example.com&#39;.</span>
</span></span><span style=display:flex><span>  <span style=color:green># For example: If a CNAME record exists &#39;_acme-challenge.amazing.example.com&#39; =&gt; &#39;_acme-challenge.writable.domain.com&#39;,</span>
</span></span><span style=display:flex><span>  <span style=color:green># the DNS challenge will be written to &#39;_acme-challenge.writable.domain.com&#39;.</span>
</span></span><span style=display:flex><span>  <span style=color:green>#followCNAME: true</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># optionally set labels for the secret</span>
</span></span><span style=display:flex><span>  <span style=color:green>#secretLabels:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#  key1: value1</span>
</span></span><span style=display:flex><span>  <span style=color:green>#  key2: value2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># Optionally specify the preferred certificate chain: if the CA offers multiple certificate chains, prefer the chain with an issuer matching this Subject Common Name. If no match, the default offered chain will be used.</span>
</span></span><span style=display:flex><span>  <span style=color:green>#preferredChain: &#34;ISRG Root X1&#34;</span>
</span></span></code></pre></div><h2 id=supported-attributes>Supported attributes</h2><p>Here is a list of all supported annotations regarding the certificate extension:</p><table><thead><tr><th>Path</th><th>Annotation</th><th>Value</th><th>Required</th><th>Description</th></tr></thead><tbody><tr><td>N/A</td><td><code>cert.gardener.cloud/purpose:</code></td><td><code>managed</code></td><td>Yes when using annotations</td><td>Flag for Gardener that this specific Ingress or Service requires a certificate</td></tr><tr><td><code>spec.commonName</code></td><td><code>cert.gardener.cloud/commonname:</code></td><td>E.g. &ldquo;*.demo.example.com&rdquo; or<br>&ldquo;special.example.com&rdquo;</td><td>Certificate and Ingress : No<br>Service: yes</td><td>Specifies for which domain the certificate request will be created. If not specified, the names from spec.tls[].hosts are used. This entry must comply with the <a href=#Character-Restrictions>64 character</a> limit.</td></tr><tr><td><code>spec.dnsName</code></td><td><code>cert.gardener.cloud/dnsnames:</code></td><td>E.g. &ldquo;special.example.com&rdquo;</td><td>Certificate and Ingress : No<br>Service: yes</td><td>Additional domains the certificate should be valid for (Subject Alternative Name). If not specified, the names from spec.tls[].hosts are used. Entries in this list can be longer than 64 characters.</td></tr><tr><td><code>spec.secretRef.name</code></td><td><code>cert.gardener.cloud/secretname:</code></td><td><code>any-name</code></td><td>Yes for certificate and Service</td><td>Specifies the secret which contains the certificate/key pair. If the secret is not available yet, it&rsquo;ll be created automatically as soon as the certificate has been issued.</td></tr><tr><td><code>spec.issuerRef.name</code></td><td><code>cert.gardener.cloud/issuer:</code></td><td>E.g. <code>gardener</code></td><td>No</td><td>Specifies the issuer you want to use. Only necessary if you request certificates for <a href=#Custom-Domains>custom domains</a>.</td></tr><tr><td>N/A</td><td><code>cert.gardener.cloud/revoked:</code></td><td><code>true</code> otherwise always false</td><td>No</td><td>Use only to revoke a certificate, see <a href=#references>reference</a> for more details</td></tr><tr><td><code>spec.followCNAME</code></td><td><code>cert.gardener.cloud/follow-cname</code></td><td>E.g. <code>true</code></td><td>No</td><td>Specifies that the usage of a delegated domain for DNS challenges is allowed. Details see <a href=https://github.com/gardener/cert-management#follow-cname>Follow CNAME</a>.</td></tr><tr><td><code>spec.preferredChain</code></td><td><code>cert.gardener.cloud/preferred-chain</code></td><td>E.g. <code>ISRG Root X1</code></td><td>No</td><td>Specifies the Common Name of the issuer for selecting the certificate chain. Details see <a href=https://github.com/gardener/cert-management#preferred-chain>Preferred Chain</a>.</td></tr><tr><td><code>spec.secretLabels</code></td><td><code>cert.gardener.cloud/secret-labels</code></td><td>for annotation use e.g. <code>key1=value1,key2=value2</code></td><td>No</td><td>Specifies labels for the certificate secret.</td></tr></tbody></table><h2 id=request-a-wildcard-certificate>Request a wildcard certificate</h2><p>In order to avoid the creation of multiples certificates for every single endpoints, you may want to create a wildcard certificate for your shoot&rsquo;s default cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    cert.gardener.cloud/commonName: <span style=color:#a31515>&#34;*.example.com&#34;</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  tls:
</span></span><span style=display:flex><span>  - hosts:
</span></span><span style=display:flex><span>    - amazing.example.com
</span></span><span style=display:flex><span>    secretName: tls-secret
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: amazing.example.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span></code></pre></div><p>Please note that this can also be achived by directly adding an annotation to a Service type LoadBalancer. You could also create a Certificate object with a wildcard domain.</p><h2 id=using-a-custom-issuer>Using a custom Issuer</h2><p>Most Gardener deployment with the certification extension enabled have a preconfigured <code>garden</code> issuer. It is also usually configured to use Let&rsquo;s Encrypt as the certificate provider.</p><p>If you need a custom issuer for a specific cluster, please see <a href=/docs/extensions/others/gardener-extension-shoot-cert-service/custom_shoot_issuer/>Using a custom Issuer</a></p><h2 id=quotas>Quotas</h2><p>For security reasons there may be a default quota on the certificate requests per day set globally in the controller
registration of the shoot-cert-service.</p><p>The default quota only applies if there is no explicit quota defined for the issuer itself with the field
<code>requestsPerDayQuota</code>, e.g.:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: shoot-cert-service
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: CertConfig
</span></span><span style=display:flex><span>      issuers:
</span></span><span style=display:flex><span>        - email: your-email@example.com
</span></span><span style=display:flex><span>          name: custom-issuer <span style=color:green># issuer name must be specified in every custom issuer request, must not be &#34;garden&#34;</span>
</span></span><span style=display:flex><span>          server: <span style=color:#a31515>&#39;https://acme-v02.api.letsencrypt.org/directory&#39;</span>
</span></span><span style=display:flex><span>          requestsPerDayQuota: 10
</span></span></code></pre></div><h2 id=dns-propagation>DNS Propagation</h2><p>As stated before, cert-manager uses the ACME challenge protocol to authenticate that you are the DNS owner for the domain&rsquo;s certificate you are requesting. This works by creating a DNS TXT record in your DNS provider under <code>_acme-challenge.example.example.com</code> containing a token to compare with. The TXT record is only visible during the domain validation. Typically, the record is propagated within a few minutes. But if the record is not visible to the ACME server for any reasons, the certificate request is retried again after several minutes. This means you may have to wait up to one hour after the propagation problem has been resolved before the certificate request is retried. Take a look in the events with <code>kubectl describe ingress example</code> for troubleshooting.</p><h2 id=character-restrictions>Character Restrictions</h2><p>Due to the ACME protocol specification, at least one domain of the domains you request a certificate for must not exceed a character limit of 64 (CN - Common Name).</p><p>For example, the following request is invalid:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Certificate
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cert-invalid
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  commonName: morethan64characters.ingress.shoot.project.default-domain.gardener.cloud
</span></span></code></pre></div><p>But it is valid to request a certificate for this domain if you have at least one domain which does not exceed the mentioned limit:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Certificate
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cert-example
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  commonName: short.ingress.shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>  dnsNames:
</span></span><span style=display:flex><span>  - morethan64characters.ingress.shoot.project.default-domain.gardener.cloud
</span></span></code></pre></div><h2 id=references>References</h2><ul><li><a href=https://github.com/gardener/cert-management>Gardener cert-management</a></li><li><a href=https://github.com/gardener/gardener-extension-shoot-dns-service>Managing DNS with Gardener</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3402884b4c1c0b3f1b0c817d7375a826>5.1.4 - Using a custom Issuer</h1><div class=lead>How to define a custom issuer forma shoot cluster</div><h1 id=using-a-custom-issuer>Using a custom Issuer</h1><p>Another possibility to request certificates for custom domains is a dedicated issuer.</p><blockquote><p>Note: This is only needed if the default issuer provided by Gardener is restricted to shoot related domains or you are using domain names not visible to public DNS servers. <strong>Which means that your senario most likely doesn&rsquo;t require your to add an issuer</strong>.</p></blockquote><p>The custom issuers are specified normally in the shoot manifest. If the <code>shootIssuers</code> feature is enabled, it can alternatively be defined in the shoot cluster.</p><h2 id=custom-issuer-in-the-shoot-manifest>Custom issuer in the shoot manifest</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: shoot-cert-service
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: CertConfig
</span></span><span style=display:flex><span>      issuers:
</span></span><span style=display:flex><span>        - email: your-email@example.com
</span></span><span style=display:flex><span>          name: custom-issuer <span style=color:green># issuer name must be specified in every custom issuer request, must not be &#34;garden&#34;</span>
</span></span><span style=display:flex><span>          server: <span style=color:#a31515>&#39;https://acme-v02.api.letsencrypt.org/directory&#39;</span>
</span></span><span style=display:flex><span>          privateKeySecretName: my-privatekey <span style=color:green># referenced resource, the private key must be stored in the secret at `data.privateKey` (optionally, only needed as alternative to auto registration) </span>
</span></span><span style=display:flex><span>          <span style=color:green>#precheckNameservers: # to provide special set of nameservers to be used for prechecking DNSChallenges for an issuer</span>
</span></span><span style=display:flex><span>          <span style=color:green>#- dns1.private.company-net:53</span>
</span></span><span style=display:flex><span>          <span style=color:green>#- dns2.private.company-net:53&#34; </span>
</span></span><span style=display:flex><span>      <span style=color:green>#shootIssuers:</span>
</span></span><span style=display:flex><span>        <span style=color:green># if true, allows to specify issuers in the shoot cluster</span>
</span></span><span style=display:flex><span>        <span style=color:green>#enabled: true </span>
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>  - name: my-privatekey
</span></span><span style=display:flex><span>    resourceRef:
</span></span><span style=display:flex><span>      apiVersion: v1
</span></span><span style=display:flex><span>      kind: Secret
</span></span><span style=display:flex><span>      name: custom-issuer-privatekey <span style=color:green># name of secret in Gardener project</span>
</span></span></code></pre></div><p>If you are using an ACME provider for private domains, you may need to change the nameservers used for
checking the availability of the DNS challenge&rsquo;s TXT record before the certificate is requested from the ACME provider.
By default, only public DNS servers may be used for this purpose.
At least one of the <code>precheckNameservers</code> must be able to resolve the private domain names.</p><h3 id=using-the-custom-issuer>Using the custom issuer</h3><p>To use the custom issuer in a certificate, just specify its name in the spec.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Certificate
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  issuerRef:
</span></span><span style=display:flex><span>    name: custom-issuer
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>For source resources like <code>Ingress</code> or <code>Service</code> use the <code>cert.gardener.cloud/issuer</code> annotation.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    cert.gardener.cloud/issuer: custom-issuer
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=custom-issuer-in-the-shoot-cluster>Custom issuer in the shoot cluster</h2><p><em>Prerequiste</em>: The <code>shootIssuers</code> feature has to be enabled.
It is either enabled globally in the <code>ControllerDeployment</code> or in the shoot manifest
with:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: shoot-cert-service
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: CertConfig
</span></span><span style=display:flex><span>      shootIssuers:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>true</span> <span style=color:green># if true, allows to specify issuers in the shoot cluster</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>Example for specifying an <code>Issuer</code> resource and its <code>Secret</code> directly in any
namespace of the shoot cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Issuer
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-own-issuer
</span></span><span style=display:flex><span>  namespace: my-namespace
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  acme:
</span></span><span style=display:flex><span>    domains:
</span></span><span style=display:flex><span>      include:
</span></span><span style=display:flex><span>      - my.own.domain.com
</span></span><span style=display:flex><span>    email: some.user@my.own.domain.com
</span></span><span style=display:flex><span>    privateKeySecretRef:
</span></span><span style=display:flex><span>      name: my-own-issuer-secret
</span></span><span style=display:flex><span>      namespace: my-namespace
</span></span><span style=display:flex><span>    server: https://acme-v02.api.letsencrypt.org/directory
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-own-issuer-secret
</span></span><span style=display:flex><span>  namespace: my-namespace
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  privateKey: ... <span style=color:green># replace &#39;...&#39; with valus encoded as base64</span>
</span></span></code></pre></div><h3 id=using-the-custom-shoot-issuer>Using the custom shoot issuer</h3><p>To use the custom issuer in a certificate, just specify its name and namespace in the spec.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: cert.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Certificate
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  issuerRef:
</span></span><span style=display:flex><span>    name: my-own-issuer
</span></span><span style=display:flex><span>    namespace: my-namespace
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><p>For source resources like <code>Ingress</code> or <code>Service</code> use the <code>cert.gardener.cloud/issuer</code> annotation.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/purpose: managed
</span></span><span style=display:flex><span>    cert.gardener.cloud/issuer: my-namespace/my-own-issuer
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-2b9b78037572abda5ff1054023909c78>5.1.5 - Deployment</h1><h1 id=gardener-certificate-management>Gardener Certificate Management</h1><h2 id=introduction>Introduction</h2><p>Gardener comes with an extension that enables shoot owners to request X.509 compliant certificates for shoot domains.</p><h2 id=extension-installation>Extension Installation</h2><p>The <code>Shoot-Cert-Service</code> extension can be deployed and configured via Gardener&rsquo;s native resource <a href=/docs/gardener/extensions/controllerregistration/>ControllerRegistration</a>.</p><h3 id=prerequisites>Prerequisites</h3><p>To let the <code>Shoot-Cert-Service</code> operate properly, you need to have:</p><ul><li>a <a href=https://github.com/gardener/external-dns-management>DNS service</a> in your seed</li><li>contact details and optionally a private key for a pre-existing <a href=https://letsencrypt.org/>Let&rsquo;s Encrypt</a> account</li></ul><h3 id=controllerregistration>ControllerRegistration</h3><p>An example of a <code>ControllerRegistration</code> for the <code>Shoot-Cert-Service</code> can be found at <a href=https://github.com/gardener/gardener-extension-shoot-cert-service/blob/master/example/controller-registration.yaml>controller-registration.yaml</a>.</p><p>The <code>ControllerRegistration</code> contains a Helm chart which eventually deploy the <code>Shoot-Cert-Service</code> to seed clusters. It offers some configuration options, mainly to set up a default issuer for shoot clusters. With a default issuer, pre-existing Let&rsquo;s Encrypt accounts can be used and shared with shoot clusters (See &ldquo;One Account or Many?&rdquo; of the <a href=https://letsencrypt.org/docs/integration-guide/>Integration Guide</a>).</p><blockquote><p>Please keep the Let&rsquo;s Encrypt <a href=https://letsencrypt.org/docs/rate-limits/>Rate Limits</a> in mind when using this shared account model. Depending on the amount of shoots and domains it is recommended to use an account with increased rate limits.</p></blockquote><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerRegistration
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>  values:
</span></span><span style=display:flex><span>    certificateConfig:
</span></span><span style=display:flex><span>      defaultIssuer:
</span></span><span style=display:flex><span>        acme:
</span></span><span style=display:flex><span>            email: foo@example.com
</span></span><span style=display:flex><span>            privateKey: |-<span style=color:#a31515>
</span></span></span><span style=display:flex><span><span style=color:#a31515>            -----BEGIN RSA PRIVATE KEY-----
</span></span></span><span style=display:flex><span><span style=color:#a31515>            ...
</span></span></span><span style=display:flex><span><span style=color:#a31515>            -----END RSA PRIVATE KEY-----
</span></span></span><span style=display:flex><span><span style=color:#a31515>            server: https://acme-v02.api.letsencrypt.org/directory</span>            
</span></span><span style=display:flex><span>        name: default-issuer
</span></span><span style=display:flex><span><span style=color:green>#       restricted: true # restrict default issuer to any sub-domain of shoot.spec.dns.domain</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>#     defaultRequestsPerDayQuota: 50</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>#     precheckNameservers: 8.8.8.8,8.8.4.4</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>#     caCertificates: | # optional custom CA certificates when using private ACME provider</span>
</span></span><span style=display:flex><span><span style=color:green>#     -----BEGIN CERTIFICATE-----</span>
</span></span><span style=display:flex><span><span style=color:green>#     ...</span>
</span></span><span style=display:flex><span><span style=color:green>#     -----END CERTIFICATE-----</span>
</span></span><span style=display:flex><span><span style=color:green>#</span>
</span></span><span style=display:flex><span><span style=color:green>#     -----BEGIN CERTIFICATE-----</span>
</span></span><span style=display:flex><span><span style=color:green>#     ...</span>
</span></span><span style=display:flex><span><span style=color:green>#     -----END CERTIFICATE-----</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      shootIssuers:
</span></span><span style=display:flex><span>        enabled: <span style=color:#00f>false</span> <span style=color:green># if true, allows to specify issuers in the shoot clusters</span>
</span></span></code></pre></div><h4 id=enablement>Enablement</h4><p>If the <code>Shoot-Cert-Service</code> should be enabled for every shoot cluster in your Gardener managed environment, you need to globally enable it in the <code>ControllerRegistration</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerRegistration
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>  - globallyEnabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    kind: Extension
</span></span><span style=display:flex><span>    type: shoot-cert-service
</span></span></code></pre></div><p>Alternatively, you&rsquo;re given the option to only enable the service for certain shoots:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: shoot-cert-service
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-6d9e639c54740cffc4077bf1de9de8a1>5.1.6 - Gardener yourself a Shoot with Istio, custom Domains, and Certificates</h1><p>As we ramp up more and more friends of Gardener, I thought it worthwile to explore and write a tutorial about how to simply</p><ul><li>create a Gardener managed Kubernetes Cluster (Shoot) via kubectl,</li><li>install Istio as a preferred, production ready Ingress/Service Mesh (instead of the Nginx Ingress addon),</li><li>attach your own custom domain to be managed by Gardener,</li><li>combine everything with certificates from Let&rsquo;s Encrypt.</li></ul><p>Here are some pre-pointers that you will need to go deeper:</p><ul><li><a href=/docs/guides/administer-shoots/create-delete-shoot/>CRUD Gardener Shoot</a></li><li><a href=https://github.com/gardener/external-dns-management/blob/master/README.md>DNS Management</a></li><li><a href=https://github.com/gardener/cert-management/blob/master/README.md>Certificate Management</a></li><li><a href=/docs/extensions/others/gardener-extension-shoot-dns-service/dns_names/>Tutorial Domain Names</a></li><li><a href=/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/>Tutorial Certificates</a></li></ul><div class="alert alert-primary" role=alert><h4 class=alert-heading>Tip</h4>If you try my instructions and fail, then read the alternative title of this tutorial as "Shoot yourself in the foot with Gardener, custom Domains, Istio and Certificates".</div><h2 id=first-things-first>First Things First</h2><p>Login to your Gardener landscape, setup a project with adequate infrastructure credentials and then navigate to your account. Note down the name of your secret. I chose the GCP infrastructure from the vast possible options that my Gardener provides me with, so i had named the secret as <code>shoot-operator-gcp</code>.</p><p>From the Access widget (leave the default settings) download your personalized <code>kubeconfig</code> into <code>~/.kube/kubeconfig-garden-myproject</code>. Follow the instructions to setup <code>kubelogin</code>:</p><p><img src=/__resources/access_0ffd66.png alt=access></p><p>For convinience, let us set an alias command with</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>alias kgarden=<span style=color:#a31515>&#34;kubectl --kubeconfig ~/.kube/kubeconfig-garden-myproject.yaml&#34;</span>
</span></span></code></pre></div><p><code>kgarden</code> now gives you all botanical powers and connects you directly with your Gardener.</p><p>You should now be able to run <code>kgarden get shoots</code>, automatically get an oidc token, and list already running clusters/shoots.</p><h2 id=prepare-your-custom-domain>Prepare your Custom Domain</h2><p>I am going to use <a href=https://www.cloudflare.com/>Cloud Flare</a> as programmatic DNS of my custom domain <code>mydomain.io</code>. Please follow detailed instructions from Cloud Flare on how to delegate your domain (the free account does not support delegating subdomains). Alternatively, AWS Route53 (and most others) support <a href=https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/CreatingNewSubdomain.html>delegating subdomains</a>.</p><p>I needed to follow these <a href=https://github.com/gardener/external-dns-management/blob/master/docs/cloudflare/README.md>instructions</a> and created the following secret:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: cloudflare-mydomain-io
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  CLOUDFLARE_API_TOKEN: useYOURownDAMITzNDU2Nzg5MDEyMzQ1Njc4OQ==
</span></span></code></pre></div><p>Apply this secret into your project with <code>kgarden create -f cloudflare-mydomain-io.yaml</code>.</p><p>Our <a href=https://github.com/gardener/external-dns-management/>External DNS Manager</a> also supports Amazon Route53, Google CloudDNS, AliCloud DNS, Azure DNS, or OpenStack Designate. Check it out.</p><h2 id=prepare-gardener-extensions>Prepare Gardener Extensions</h2><p>I now need to prepare the Gardener extensions <code>shoot-dns-service</code> and <code>shoot-cert-service</code> and set the parameters accordingly.</p><div class="alert alert-info" role=alert>Please note, that the availability of Gardener Extensions depends on how your administrator has configured the Gardener landscape. Please contact your Gardener administrator in case you experience any issues during activation.</div><p>The following snipplet allows Gardener to manage my entire custom domain, whereas with the <code>include:</code> attribute I restrict all dynamic entries under the subdomain <code>gsicdc.mydomain.io</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  dns:
</span></span><span style=display:flex><span>    providers:
</span></span><span style=display:flex><span>      - domains:
</span></span><span style=display:flex><span>          include:
</span></span><span style=display:flex><span>            - gsicdc.mydomain.io
</span></span><span style=display:flex><span>        primary: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>        secretName: cloudflare-mydomain-io
</span></span><span style=display:flex><span>        type: cloudflare-dns
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-dns-service
</span></span></code></pre></div><p>The next snipplet allows Gardener to manage certificates automatically from <em><a href=https://letsencrypt.org/>Let&rsquo;s Encrypt</a></em> on <code>mydomain.io</code> for me:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-cert-service
</span></span><span style=display:flex><span>      providerConfig:
</span></span><span style=display:flex><span>        apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>        issuers:
</span></span><span style=display:flex><span>          - email: me@mail.com
</span></span><span style=display:flex><span>            name: mydomain
</span></span><span style=display:flex><span>            server: <span style=color:#a31515>&#39;https://acme-v02.api.letsencrypt.org/directory&#39;</span>
</span></span><span style=display:flex><span>          - email: me@mail.com
</span></span><span style=display:flex><span>            name: mydomain-staging
</span></span><span style=display:flex><span>            server: <span style=color:#a31515>&#39;https://acme-staging-v02.api.letsencrypt.org/directory&#39;</span>
</span></span></code></pre></div><div class="alert alert-info" role=alert>Adjust the snipplets with your parameters (don't forget your email). And please use the mydomain-staging issuer while you are testing and learning. Otherwise, Let's Encrypt will rate limit your frequent requests and you can wait a week until you can continue.</div><p>References for <a href=https://letsencrypt.org>Let&rsquo;s Encrypt</a>:</p><ul><li><a href=https://letsencrypt.org/docs/rate-limits/>Rate limit</a></li><li><a href=https://letsencrypt.org/docs/staging-environment/>Staging environment</a></li><li><a href=https://letsencrypt.org/docs/challenge-types/>Challenge Types</a></li><li><a href=https://community.letsencrypt.org/t/acme-v2-production-environment-wildcards/55578>Wildcard Certificates</a></li></ul><h2 id=create-the-gardener-shoot-cluster>Create the Gardener Shoot Cluster</h2><p>Remember I chose to create the Shoot on GCP, so below is the simplest declarative shoot or cluster order document. Notice that I am referring to the infrastructure credentials with <code>shoot-operator-gcp</code> and I combined the above snipplets into the yaml file:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: gsicdc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  dns:
</span></span><span style=display:flex><span>    providers:
</span></span><span style=display:flex><span>    - domains:
</span></span><span style=display:flex><span>        include:
</span></span><span style=display:flex><span>          - gsicdc.mydomain.io
</span></span><span style=display:flex><span>      primary: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      secretName: cloudflare-mydomain-io
</span></span><span style=display:flex><span>      type: cloudflare-dns
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: shoot-dns-service
</span></span><span style=display:flex><span>  - type: shoot-cert-service
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: service.cert.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      issuers:
</span></span><span style=display:flex><span>        - email: me@mail.com
</span></span><span style=display:flex><span>          name: mydomain
</span></span><span style=display:flex><span>          server: <span style=color:#a31515>&#39;https://acme-v02.api.letsencrypt.org/directory&#39;</span>
</span></span><span style=display:flex><span>        - email: me@mail.com
</span></span><span style=display:flex><span>          name: mydomain-staging
</span></span><span style=display:flex><span>          server: <span style=color:#a31515>&#39;https://acme-staging-v02.api.letsencrypt.org/directory&#39;</span>
</span></span><span style=display:flex><span>  cloudProfileName: gcp
</span></span><span style=display:flex><span>  kubernetes:
</span></span><span style=display:flex><span>    allowPrivilegedContainers: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    version: 1.24.8
</span></span><span style=display:flex><span>  maintenance:
</span></span><span style=display:flex><span>    autoUpdate:
</span></span><span style=display:flex><span>      kubernetesVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      machineImageVersion: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  networking:
</span></span><span style=display:flex><span>    nodes: 10.250.0.0/16
</span></span><span style=display:flex><span>    pods: 100.96.0.0/11
</span></span><span style=display:flex><span>    services: 100.64.0.0/13
</span></span><span style=display:flex><span>    type: calico
</span></span><span style=display:flex><span>  provider:
</span></span><span style=display:flex><span>    controlPlaneConfig:
</span></span><span style=display:flex><span>      apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: ControlPlaneConfig
</span></span><span style=display:flex><span>      zone: europe-west1-d
</span></span><span style=display:flex><span>    infrastructureConfig:
</span></span><span style=display:flex><span>      apiVersion: gcp.provider.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>      kind: InfrastructureConfig
</span></span><span style=display:flex><span>      networks:
</span></span><span style=display:flex><span>        workers: 10.250.0.0/16
</span></span><span style=display:flex><span>    type: gcp
</span></span><span style=display:flex><span>    workers:
</span></span><span style=display:flex><span>    - machine:
</span></span><span style=display:flex><span>        image:
</span></span><span style=display:flex><span>          name: gardenlinux
</span></span><span style=display:flex><span>          version: 576.9.0
</span></span><span style=display:flex><span>        type: n1-standard-2
</span></span><span style=display:flex><span>      maxSurge: 1
</span></span><span style=display:flex><span>      maxUnavailable: 0
</span></span><span style=display:flex><span>      maximum: 2
</span></span><span style=display:flex><span>      minimum: 1
</span></span><span style=display:flex><span>      name: my-workerpool
</span></span><span style=display:flex><span>      volume:
</span></span><span style=display:flex><span>        size: 50Gi
</span></span><span style=display:flex><span>        type: pd-standard
</span></span><span style=display:flex><span>      zones:
</span></span><span style=display:flex><span>      - europe-west1-d
</span></span><span style=display:flex><span>  purpose: testing
</span></span><span style=display:flex><span>  region: europe-west1
</span></span><span style=display:flex><span>  secretBindingName: shoot-operator-gcp
</span></span></code></pre></div><p>Create your cluster and wait for it to be ready (about 5 to 7min).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kgarden create -f gsicdc.yaml
</span></span><span style=display:flex><span>shoot.core.gardener.cloud/gsicdc created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kgarden get shoot gsicdc --watch
</span></span><span style=display:flex><span>NAME     CLOUDPROFILE   VERSION   SEED   DOMAIN                                        HIBERNATION   OPERATION    PROGRESS   APISERVER     CONTROL       NODES     SYSTEM    AGE
</span></span><span style=display:flex><span>gsicdc   gcp            1.24.8    gcp    gsicdc.myproject.shoot.devgarden.cloud   Awake         Processing   38         Progressing   Progressing   Unknown   Unknown   83s
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>gsicdc   gcp            1.24.8    gcp    gsicdc.myproject.shoot.devgarden.cloud   Awake         Succeeded    100        True          True          True          False         6m7s
</span></span></code></pre></div><p>Get access to your freshly baked cluster and set your <code>KUBECONFIG</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kgarden get secrets gsicdc.kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d &gt;kubeconfig-gsicdc.yaml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ export KUBECONFIG=<span style=color:#00f>$(</span>pwd<span style=color:#00f>)</span>/kubeconfig-gsicdc.yaml
</span></span><span style=display:flex><span>$ kubectl get all
</span></span><span style=display:flex><span>NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
</span></span><span style=display:flex><span>service/kubernetes   ClusterIP   100.64.0.1   &lt;none&gt;        443/TCP   89m
</span></span></code></pre></div><h2 id=install-istio>Install Istio</h2><p>Please follow the Istio installation <a href=https://istio.io/docs/setup/getting-started/>instructions</a> and download <code>istioctl</code>. If you are on a Mac, I recommend</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ brew install istioctl
</span></span></code></pre></div><p>I want to install Istio with a default profile and SDS enabled. Furthermore I pass the following annotations to the service object <code>istio-ingressgateway</code> in the <code>istio-system</code> namespace.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    cert.gardener.cloud/issuer: mydomain-staging
</span></span><span style=display:flex><span>    cert.gardener.cloud/secretname: wildcard-tls
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: <span style=color:#a31515>&#34;*.gsicdc.mydomain.io&#34;</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;120&#34;</span>
</span></span></code></pre></div><p>With these annotations three things now happen automagically:</p><ol><li>The <a href=https://github.com/gardener/external-dns-management/blob/master/README.md>External DNS Manager</a>, provided to you as a service (<code>dns.gardener.cloud/class: garden</code>), picks up the request and creates the wildcard DNS entry <code>*.gsicdc.mydomain.io</code> with a time to live of 120sec at your DNS provider. My provider Cloud Flare is very very quick (as opposed to some other services). You should be able to verify the entry with <code>dig lovemygardener.gsicdc.mydomain.io</code> within seconds.</li><li>The <a href=https://github.com/gardener/cert-management/blob/master/README.md>Certificate Management</a> picks up the request as well and initates a DNS01 protocol exchange with Let&rsquo;s Encrypt; using the staging environment referred to with the issuer behind <code>mydomain-staging</code>.</li><li>After aproximately 70sec (give and take) you will receive the wildcard certificate in the <code>wildcard-tls</code> secret in the namespace <code>istio-system</code>.</li></ol><div class="alert alert-info" role=alert>Notice, that the namespace for the certificate secret is often the cause of many troubeshooting sessions: the secret must reside in the same namespace of the gateway.</div><p>Here is the istio-install script:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ export domainname=<span style=color:#a31515>&#34;*.gsicdc.mydomain.io&#34;</span>
</span></span><span style=display:flex><span>$ export issuer=<span style=color:#a31515>&#34;mydomain-staging&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ cat <span style=color:#a31515>&lt;&lt;EOF | istioctl install -y -f -
</span></span></span><span style=display:flex><span><span style=color:#a31515>apiVersion: install.istio.io/v1alpha1
</span></span></span><span style=display:flex><span><span style=color:#a31515>kind: IstioOperator
</span></span></span><span style=display:flex><span><span style=color:#a31515>spec:
</span></span></span><span style=display:flex><span><span style=color:#a31515>  profile: default
</span></span></span><span style=display:flex><span><span style=color:#a31515>  components:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    ingressGateways:
</span></span></span><span style=display:flex><span><span style=color:#a31515>    - name: istio-ingressgateway
</span></span></span><span style=display:flex><span><span style=color:#a31515>      enabled: true
</span></span></span><span style=display:flex><span><span style=color:#a31515>      k8s:
</span></span></span><span style=display:flex><span><span style=color:#a31515>        serviceAnnotations:
</span></span></span><span style=display:flex><span><span style=color:#a31515>          cert.gardener.cloud/issuer: &#34;${issuer}&#34;
</span></span></span><span style=display:flex><span><span style=color:#a31515>          cert.gardener.cloud/secretname: wildcard-tls
</span></span></span><span style=display:flex><span><span style=color:#a31515>          dns.gardener.cloud/class: garden
</span></span></span><span style=display:flex><span><span style=color:#a31515>          dns.gardener.cloud/dnsnames: &#34;${domainname}&#34;
</span></span></span><span style=display:flex><span><span style=color:#a31515>          dns.gardener.cloud/ttl: &#34;120&#34;
</span></span></span><span style=display:flex><span><span style=color:#a31515>EOF</span>
</span></span></code></pre></div><p>Verify that setup is working and that DNS and certificates have been created/delivered:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl -n istio-system describe service istio-ingressgateway
</span></span><span style=display:flex><span>&lt;snip&gt;
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  Type    Reason                Age                From                     Message
</span></span><span style=display:flex><span>  ----    ------                ----               ----                     -------
</span></span><span style=display:flex><span>  Normal  EnsuringLoadBalancer  58s                service-controller       Ensuring load balancer
</span></span><span style=display:flex><span>  Normal  reconcile             58s                cert-controller-manager  created certificate object istio-system/istio-ingressgateway-service-pwqdm
</span></span><span style=display:flex><span>  Normal  cert-annotation       58s                cert-controller-manager  wildcard-tls: cert request is pending
</span></span><span style=display:flex><span>  Normal  cert-annotation       54s                cert-controller-manager  wildcard-tls: certificate pending: certificate requested, preparing/waiting <span style=color:#00f>for</span> successful DNS01 challenge
</span></span><span style=display:flex><span>  Normal  cert-annotation       28s                cert-controller-manager  wildcard-tls: certificate ready
</span></span><span style=display:flex><span>  Normal  EnsuredLoadBalancer   26s                service-controller       Ensured load balancer
</span></span><span style=display:flex><span>  Normal  reconcile             26s                dns-controller-manager   created dns entry object shoot--core--gsicdc/istio-ingressgateway-service-p9qqb
</span></span><span style=display:flex><span>  Normal  dns-annotation        26s                dns-controller-manager   *.gsicdc.mydomain.io: dns entry is pending
</span></span><span style=display:flex><span>  Normal  dns-annotation        21s (x3 over 21s)  dns-controller-manager   *.gsicdc.mydomain.io: dns entry active
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ dig lovemygardener.gsicdc.mydomain.io
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; lovemygardener.gsicdc.mydomain.io
</span></span><span style=display:flex><span>&lt;snip&gt;
</span></span><span style=display:flex><span>;; ANSWER SECTION:
</span></span><span style=display:flex><span>lovemygardener.gsicdc.mydomain.io. 120 IN A	35.195.120.62
</span></span><span style=display:flex><span>&lt;snip&gt;
</span></span></code></pre></div><p>There you have it, the wildcard-tls certificate is ready and the *.gsicdc.mydomain.io dns entry is active. Traffic will be going your way.</p><h2 id=handy-tools-to-install>Handy tools to install</h2><p>Another set of fine tools to use are <a href=https://get-kapp.io/>kapp</a> (formerly known as k14s), <a href=https://k9scli.io/>k9s</a> and <a href=https://httpie.org/>HTTPie</a>. While we are at it, let&rsquo;s install them all. If you are on a Mac, I recommend:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>brew tap vmware-tanzu/carvel
</span></span><span style=display:flex><span>brew install ytt kbld kapp kwt imgpkg vendir
</span></span><span style=display:flex><span>brew install derailed/k9s/k9s
</span></span><span style=display:flex><span>brew install httpie
</span></span></code></pre></div><h2 id=ingress-to-your-service>Ingress to your service</h2><div class="alert alert-info" role=alert>Networking is a central part of Kubernetes, but it can be challenging to understand exactly how it is expected to work. You should learn about Kubernetes networking, and first try to debug problems yourself. With a solid managed cluster from Gardener, it is always PEBCAK!</div><p>Kubernetes Ingress is a subject that is evolving to much broader standard. Please watch <a href="https://www.youtube.com/watch?v=cduG0FrjdJA">Evolving the Kubernetes Ingress APIs to GA and Beyond</a> for a good introduction. In this example, I did not want to use the Kubernetes <code>Ingress</code> compatibility option of Istio. Instead, I used <code>VirtualService</code> and <code>Gateway</code> from the Istio&rsquo;s API group <code>networking.istio.io/v1beta1</code> directly, and enabled istio-injection generically for the namespace.</p><p>I use <a href=https://httpbin.org/>httpbin</a> as service that I want to expose to the internet, or where my ingress should be routed to (depends on your point of view, I guess).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Namespace
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: production
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    istio-injection: enabled
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: httpbin
</span></span><span style=display:flex><span>  namespace: production
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: httpbin
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>  - name: http
</span></span><span style=display:flex><span>    port: 8000
</span></span><span style=display:flex><span>    targetPort: 80
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    app: httpbin
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: httpbin
</span></span><span style=display:flex><span>  namespace: production
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: 1
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: httpbin
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: httpbin
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - image: docker.io/kennethreitz/httpbin
</span></span><span style=display:flex><span>        imagePullPolicy: IfNotPresent
</span></span><span style=display:flex><span>        name: httpbin
</span></span><span style=display:flex><span>        ports:
</span></span><span style=display:flex><span>        - containerPort: 80
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: networking.istio.io/v1beta1
</span></span><span style=display:flex><span>kind: Gateway
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: httpbin-gw
</span></span><span style=display:flex><span>  namespace: production
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    istio: ingressgateway <span style=color:green>#! use istio default ingress gateway</span>
</span></span><span style=display:flex><span>  servers:
</span></span><span style=display:flex><span>  - port:
</span></span><span style=display:flex><span>      number: 80
</span></span><span style=display:flex><span>      name: http
</span></span><span style=display:flex><span>      protocol: HTTP
</span></span><span style=display:flex><span>    tls:
</span></span><span style=display:flex><span>      httpsRedirect: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>    hosts:
</span></span><span style=display:flex><span>    - <span style=color:#a31515>&#34;httpbin.gsicdc.mydomain.io&#34;</span>
</span></span><span style=display:flex><span>  - port:
</span></span><span style=display:flex><span>      number: 443
</span></span><span style=display:flex><span>      name: https
</span></span><span style=display:flex><span>      protocol: HTTPS
</span></span><span style=display:flex><span>    tls:
</span></span><span style=display:flex><span>      mode: SIMPLE
</span></span><span style=display:flex><span>      credentialName: wildcard-tls
</span></span><span style=display:flex><span>    hosts:
</span></span><span style=display:flex><span>    - <span style=color:#a31515>&#34;httpbin.gsicdc.mydomain.io&#34;</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: networking.istio.io/v1beta1
</span></span><span style=display:flex><span>kind: VirtualService
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: httpbin-vs
</span></span><span style=display:flex><span>  namespace: production
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  hosts:
</span></span><span style=display:flex><span>  - <span style=color:#a31515>&#34;httpbin.gsicdc.mydomain.io&#34;</span>
</span></span><span style=display:flex><span>  gateways:
</span></span><span style=display:flex><span>  - httpbin-gw
</span></span><span style=display:flex><span>  http:
</span></span><span style=display:flex><span>  - match:
</span></span><span style=display:flex><span>    - uri:
</span></span><span style=display:flex><span>        regex: /.*
</span></span><span style=display:flex><span>    route:
</span></span><span style=display:flex><span>    - destination:
</span></span><span style=display:flex><span>        port:
</span></span><span style=display:flex><span>          number: 8000
</span></span><span style=display:flex><span>        host: httpbin
</span></span><span style=display:flex><span>---
</span></span></code></pre></div><p>Let us now deploy the whole package of Kubernetes primitives using <code>kapp</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kapp deploy -a httpbin -f httpbin-kapp.yaml
</span></span><span style=display:flex><span>Target cluster <span style=color:#a31515>&#39;https://api.gsicdc.myproject.shoot.devgarden.cloud&#39;</span> (nodes: shoot--myproject--gsicdc-my-workerpool-z1-6586c8f6cb-x24kh)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Changes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Namespace   Name        Kind            Conds.  Age  Op      Wait to    Rs  Ri
</span></span><span style=display:flex><span>(cluster)   production  Namespace       -       -    create  reconcile  -   -
</span></span><span style=display:flex><span>production  httpbin     Deployment      -       -    create  reconcile  -   -
</span></span><span style=display:flex><span>^           httpbin     Service         -       -    create  reconcile  -   -
</span></span><span style=display:flex><span>^           httpbin-gw  Gateway         -       -    create  reconcile  -   -
</span></span><span style=display:flex><span>^           httpbin-vs  VirtualService  -       -    create  reconcile  -   -
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Op:      5 create, 0 delete, 0 update, 0 noop
</span></span><span style=display:flex><span>Wait to: 5 reconcile, 0 delete, 0 noop
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Continue? [yN]: y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>5:36:31PM: ---- applying 1 changes [0/5 <span style=color:#00f>done</span>] ----
</span></span><span style=display:flex><span>&lt;snip&gt;
</span></span><span style=display:flex><span>5:37:00PM: ok: reconcile deployment/httpbin (apps/v1) namespace: production
</span></span><span style=display:flex><span>5:37:00PM: ---- applying complete [5/5 <span style=color:#00f>done</span>] ----
</span></span><span style=display:flex><span>5:37:00PM: ---- waiting complete [5/5 <span style=color:#00f>done</span>] ----
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Succeeded
</span></span></code></pre></div><p>Let&rsquo;s finaly test the service (Of course you can use the browser as well):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ http httpbin.gsicdc.mydomain.io
</span></span><span style=display:flex><span>HTTP/1.1 301 Moved Permanently
</span></span><span style=display:flex><span>content-length: 0
</span></span><span style=display:flex><span>date: Wed, 13 May 2020 21:29:13 GMT
</span></span><span style=display:flex><span>location: https://httpbin.gsicdc.mydomain.io/
</span></span><span style=display:flex><span>server: istio-envoy
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ curl -k https://httpbin.gsicdc.mydomain.io/ip
</span></span><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:#a31515>&#34;origin&#34;</span>: <span style=color:#a31515>&#34;10.250.0.2&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Quod erat demonstrandum.
The proof of exchanging the issuer is now left to the reader.</p><div class="alert alert-primary" role=alert><h4 class=alert-heading>Tip</h4>Remember that the certificate is actually not valid because it is issued from the Let's encrypt staging environment. Thus, we needed "curl -k" or "http --verify no".</div><p>Hint: use the interactive k9s tool.
<img src=/__resources/k9s_97660c.png alt=k9s></p><h2 id=cleanup>Cleanup</h2><p>Remove the cloud native application:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kapp ls
</span></span><span style=display:flex><span>Apps in namespace <span style=color:#a31515>&#39;default&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Name     Namespaces            Lcs   Lca
</span></span><span style=display:flex><span>httpbin  (cluster),production  true  17m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kapp delete -a httpbin
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Continue? [yN]: y
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>11:47:47PM: ---- waiting complete [8/8 <span style=color:#00f>done</span>] ----
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Succeeded
</span></span></code></pre></div><p>Remove Istio:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ istioctl x uninstall --purge
</span></span><span style=display:flex><span>clusterrole.rbac.authorization.k8s.io <span style=color:#a31515>&#34;prometheus-istio-system&#34;</span> deleted
</span></span><span style=display:flex><span>clusterrolebinding.rbac.authorization.k8s.io <span style=color:#a31515>&#34;prometheus-istio-system&#34;</span> deleted
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>Delete your Shoot:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kgarden annotate shoot gsicdc confirmation.gardener.cloud/deletion=true --overwrite
</span></span><span style=display:flex><span>kgarden delete shoot gsicdc --wait=false
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-4773e80fa0a0952ed75e41786df93e8c>5.2 - DNS services</h1><div class=lead>Gardener extension controller for DNS services for shoot clusters</div><h1 id=gardener-extension-for-dns-serviceshttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for DNS services</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener-tests/pipelines/gardener-extension-shoot-dns-service-master/jobs/master-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener-tests/pipelines/gardener-extension-shoot-dns-service-master/jobs/master-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-shoot-dns-service><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-shoot-dns-service alt="Go Report Card"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service. Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>. However, the project has grown to a size where it is very hard to extend, maintain, and test. With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics. This way, we can keep Gardener core clean and independent.</p><h2 id=extension-resources>Extension-Resources</h2><p>Example extension resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Extension
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: <span style=color:#a31515>&#34;extension-dns-service&#34;</span>
</span></span><span style=display:flex><span>  namespace: shoot--project--abc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: shoot-dns-service
</span></span></code></pre></div><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>. Please make sure to have the kubeconfig to the cluster you want to connect to ready in the <code>./dev/kubeconfig</code> file.
Static code checks and tests can be executed by running <code>make verify</code>. We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-shoot-dns-service/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-28d1868bb81c37360bc1080f4bd76d56>5.2.1 - Configuration</h1><h1 id=deployment-of-the-shoot-dns-service-extension>Deployment of the shoot DNS service extension</h1><p><strong>Disclaimer:</strong> This document is NOT a step by step deployment guide for the shoot DNS service extension and only contains some configuration specifics regarding the deployment of different components via the helm charts residing in the shoot DNS service extension <a href=https://github.com/gardener/gardener-extension-shoot-dns-service>repository</a>.</p><h2 id=gardener-extension-admission-shoot-dns-service>gardener-extension-admission-shoot-dns-service</h2><h3 id=authentication-against-the-garden-cluster>Authentication against the Garden cluster</h3><p>There are several authentication possibilities depending on whether or not <a href=https://github.com/gardener/garden-setup#concept-the-virtual-cluster>the concept of Virtual Garden</a> is used.</p><h4 id=virtual-garden-is-not-used-ie-the-runtime-garden-cluster-is-also-the-target-garden-cluster>Virtual Garden is not used, i.e., the <code>runtime</code> Garden cluster is also the <code>target</code> Garden cluster.</h4><h5 id=automounted-service-account-token>Automounted Service Account Token</h5><p>The easiest way to deploy the <code>gardener-extension-admission-shoot-dns-service</code> component will be to not provide <code>kubeconfig</code> at all. This way in-cluster configuration and an automounted service account token will be used. The drawback of this approach is that the automounted token will not be automatically rotated.</p><h5 id=service-account-token-volume-projection>Service Account Token Volume Projection</h5><p>Another solution will be to use <a href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection>Service Account Token Volume Projection</a> combined with a <code>kubeconfig</code> referencing a token file (see example below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://default.kubernetes.svc.cluster.local
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: garden
</span></span><span style=display:flex><span>    user: garden
</span></span><span style=display:flex><span>  name: garden
</span></span><span style=display:flex><span>current-context: garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div><p>This will allow for automatic rotation of the service account token by the <code>kubelet</code>. The configuration can be achieved by setting both <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.kubeconfig</code> in the respective chart&rsquo;s <code>values.yaml</code> file.</p><h4 id=virtual-garden-is-used-ie-the-runtime-garden-cluster-is-different-from-the-target-garden-cluster>Virtual Garden is used, i.e., the <code>runtime</code> Garden cluster is different from the <code>target</code> Garden cluster.</h4><h5 id=service-account>Service Account</h5><p>The easiest way to setup the authentication will be to create a service account and the respective roles will be bound to this service account in the <code>target</code> cluster. Then use the generated service account token and craft a <code>kubeconfig</code> which will be used by the workload in the <code>runtime</code> cluster. This approach does not provide a solution for the rotation of the service account token. However, this setup can be achieved by setting <code>.Values.global.virtualGarden.enabled: true</code> and following these steps:</p><ol><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Get the service account token and craft the <code>kubeconfig</code>.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><h5 id=client-certificate>Client Certificate</h5><p>Another solution will be to bind the roles in the <code>target</code> cluster to a <code>User</code> subject instead of a service account and use a client certificate for authentication. This approach does not provide a solution for the client certificate rotation. However, this setup can be achieved by setting both <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>, then following these steps:</p><ol><li>Generate a client certificate for the <code>target</code> cluster for the respective user.</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Craft a <code>kubeconfig</code> using the already generated client certificate.</li><li>Set the crafted <code>kubeconfig</code> and deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><h5 id=projected-service-account-token>Projected Service Account Token</h5><p>This approach requires an already deployed and configured <a href=https://github.com/gardener/oidc-webhook-authenticator>oidc-webhook-authenticator</a> for the <code>target</code> cluster. Also the <code>runtime</code> cluster should be registered as a trusted identity provider in the <code>target</code> cluster. Then projected service accounts tokens from the <code>runtime</code> cluster can be used to authenticate against the <code>target</code> cluster. The needed steps are as follows:</p><ol><li>Deploy <a href=https://github.com/gardener/oidc-webhook-authenticator>OWA</a> and establish the needed trust.</li><li>Set <code>.Values.global.virtualGarden.enabled: true</code> and <code>.Values.global.virtualGarden.user.name</code>. <strong>Note:</strong> username value will depend on the trust configuration, e.g., <code>&lt;prefix>:system:serviceaccount:&lt;namespace>:&lt;serviceaccount></code></li><li>Set <code>.Values.global.serviceAccountTokenVolumeProjection.enabled: true</code> and <code>.Values.global.serviceAccountTokenVolumeProjection.audience</code>. <strong>Note:</strong> audience value will depend on the trust configuration, e.g., <code>&lt;cliend-id-from-trust-config></code>.</li><li>Craft a kubeconfig (see example below).</li><li>Deploy the <code>application</code> part of the charts in the <code>target</code> cluster.</li><li>Deploy the <code>runtime</code> part of the charts in the <code>runtime</code> cluster.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Config
</span></span><span style=display:flex><span>clusters:
</span></span><span style=display:flex><span>- cluster:
</span></span><span style=display:flex><span>    certificate-authority-data: &lt;CA-DATA&gt;
</span></span><span style=display:flex><span>    server: https://virtual-garden.api
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>contexts:
</span></span><span style=display:flex><span>- context:
</span></span><span style=display:flex><span>    cluster: virtual-garden
</span></span><span style=display:flex><span>    user: virtual-garden
</span></span><span style=display:flex><span>  name: virtual-garden
</span></span><span style=display:flex><span>current-context: virtual-garden
</span></span><span style=display:flex><span>users:
</span></span><span style=display:flex><span>- name: virtual-garden
</span></span><span style=display:flex><span>  user:
</span></span><span style=display:flex><span>    tokenFile: /var/run/secrets/projected/serviceaccount/token
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-12a6b686a7df4d2160d67befee1ebdfa>5.2.2 - Deployment</h1><h1 id=gardener-dns-management-for-shoots>Gardener DNS Management for Shoots</h1><h2 id=introduction>Introduction</h2><p>Gardener allows Shoot clusters to request DNS names for Ingresses and Services out of the box.
To support this the gardener must be installed with the <code>shoot-dns-service</code>
extension.
This extension uses the seed&rsquo;s dns management infrastructure to maintain DNS
names for shoot clusters. So, far only the external DNS domain of a shoot
(already used for the kubernetes api server and ingress DNS names) can be used
for managed DNS names.</p><h2 id=configuration>Configuration</h2><p>To generally enable the DNS management for shoot objects the
<code>shoot-dns-service</code> extension must be registered by providing an
appropriate <a href=https://github.com/gardener/gardener-extension-shoot-dns-service/blob/master/example/controller-registration.yaml>extension registration</a> in the garden cluster.</p><p>Here it is possible to decide whether the extension should be always available
for all shoots or whether the extension must be separately enabled per shoot.</p><p>If the extension should be used for all shoots, the registration must set the <em>globallyEnabled</em> flag to <code>true</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>    - kind: Extension
</span></span><span style=display:flex><span>      type: shoot-dns-service
</span></span><span style=display:flex><span>      globallyEnabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h3 id=deployment-of-dns-controller-manager>Deployment of DNS controller manager</h3><p>If you are using Gardener version >= <code>1.54</code>, please make sure to deploy the DNS controller manager by
adding the <code>dnsControllerManager</code> section to the <code>providerConfig.values</code> section.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-shoot-dns-service
</span></span><span style=display:flex><span>type: helm
</span></span><span style=display:flex><span>providerConfig:
</span></span><span style=display:flex><span>  chart: ...
</span></span><span style=display:flex><span>  values:
</span></span><span style=display:flex><span>    image:
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>    dnsControllerManager:
</span></span><span style=display:flex><span>      image:
</span></span><span style=display:flex><span>        repository: eu.gcr.io/gardener-project/dns-controller-manager
</span></span><span style=display:flex><span>        tag: v0.13.3
</span></span><span style=display:flex><span>      configuration:
</span></span><span style=display:flex><span>        cacheTtl: 300
</span></span><span style=display:flex><span>        controllers: dnscontrollers,dnssources
</span></span><span style=display:flex><span>        dnsPoolResyncPeriod: 30m
</span></span><span style=display:flex><span>        <span style=color:green>#poolSize: 20</span>
</span></span><span style=display:flex><span>        <span style=color:green>#providersPoolResyncPeriod: 24h</span>
</span></span><span style=display:flex><span>        serverPortHttp: 8080
</span></span><span style=display:flex><span>      createCRDs: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>      deploy: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>      replicaCount: 1
</span></span><span style=display:flex><span>      <span style=color:green>#resources:</span>
</span></span><span style=display:flex><span>      <span style=color:green>#  limits:</span>
</span></span><span style=display:flex><span>      <span style=color:green>#    memory: 1Gi</span>
</span></span><span style=display:flex><span>      <span style=color:green>#  requests:</span>
</span></span><span style=display:flex><span>      <span style=color:green>#    cpu: 50m</span>
</span></span><span style=display:flex><span>      <span style=color:green>#    memory: 500Mi</span>
</span></span><span style=display:flex><span>    dnsProviderManagement:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h3 id=providing-base-domains-usable-for-a-shoot>Providing Base Domains usable for a Shoot</h3><p>So, far only the external DNS domain of a shoot already used
for the kubernetes api server and ingress DNS names can be used for managed
DNS names. This is either the shoot domain as subdomain of the default domain
configured for the gardener installation, or a dedicated domain with dedicated
access credentials configured for a dedicated shoot via the shoot manifest.</p><p>Alternatively, you can specify <code>DNSProviders</code> and its credentials
<code>Secret</code> directly in the shoot, if this feature is enabled.
By default, <code>DNSProvider</code> replication is disabled, but it can be enabled globally in the <code>ControllerDeployment</code>
or for a shoot cluster in the shoot manifest (details see further below).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerDeployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-shoot-dns-service
</span></span><span style=display:flex><span>type: helm
</span></span><span style=display:flex><span>providerConfig:
</span></span><span style=display:flex><span>  chart: ...
</span></span><span style=display:flex><span>  values:
</span></span><span style=display:flex><span>    image:
</span></span><span style=display:flex><span>      ...
</span></span><span style=display:flex><span>    dnsProviderReplication:
</span></span><span style=display:flex><span>      enabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><p>See <a href=https://github.com/gardener/external-dns-management/tree/master/examples>example files (20-* and 30-*)</a>
for details for the various provider types.</p><h3 id=shoot-feature-gate>Shoot Feature Gate</h3><p>If the shoot DNS feature is not globally enabled by default (depends on the
extension registration on the garden cluster), it must be enabled per shoot.</p><p>To enable the feature for a shoot, the shoot manifest must explicitly add the
<code>shoot-dns-service</code> extension.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-dns-service
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h4 id=enabledisable-dns-provider-replication-for-a-shoot>Enable/disable DNS provider replication for a shoot</h4><p>The DNSProvider` replication feature enablement can be overwritten in the
shoot manifest, e.g.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>Kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-dns-service
</span></span><span style=display:flex><span>      providerConfig:
</span></span><span style=display:flex><span>        apiVersion: service.dns.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>        kind: DNSConfig
</span></span><span style=display:flex><span>        dnsProviderReplication:
</span></span><span style=display:flex><span>          enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-25999a79f656efd317f31f09d85cb4cf>5.2.3 - DNS Names</h1><h1 id=request-dns-names-in-shoot-clusters>Request DNS Names in Shoot Clusters</h1><h2 id=introduction>Introduction</h2><p>Within a shoot cluster, it is possible to request DNS records via the following resource types:</p><ul><li><a href=https://kubernetes.io/docs/concepts/services-networking/ingress/>Ingress</a></li><li><a href=https://kubernetes.io/docs/concepts/services-networking/service/>Service</a></li><li><a href=https://github.com/gardener/external-dns-management/blob/master/README.md#the-model>DNSEntry</a></li></ul><p>It is necessary that the Gardener installation your shoot cluster runs in is equipped with a <code>shoot-dns-service</code> extension. This extension uses the seed&rsquo;s dns management infrastructure to maintain DNS names for shoot clusters. Please ask your Gardener operator if the extension is available in your environment.</p><h2 id=shoot-feature-gate>Shoot Feature Gate</h2><p>In some Gardener setups the <code>shoot-dns-service</code> extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-dns-service
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=before-you-start>Before you start</h2><p>You should :</p><ul><li>Have created a shoot cluster</li><li>Have created and correctly configured a DNS Provider (Please consult <a href=/docs/extensions/others/gardener-extension-shoot-dns-service/dns_providers/>this page</a> for more information)</li><li>Have a basic understanding of DNS (see link under <a href=#references>References</a>)</li></ul><p>There are 2 types of DNS that you can use within Kubernetes :</p><ul><li>internal (usually managed by coreDNS)</li><li>external (managed by a public DNS provider).</li></ul><p>This page, and the extension, exclusively works for external DNS handling.</p><p>Gardener allows 2 way of managing your external DNS:</p><ul><li>Manually, which means you are in charge of creating / maintaining your Kubernetes related DNS entries</li><li>Via the Gardener DNS extension</li></ul><h2 id=gardener-dns-extension>Gardener DNS extension</h2><p>The managed external DNS records feature of the Gardener clusters makes all this easier. You do not need DNS service provider specific knowledge, and in fact you do not need to leave your cluster at all to achieve that. You simply annotate the Ingress / Service that needs its DNS records managed and it will be automatically created / managed by Gardener.</p><p>Managed external DNS records are supported with the following DNS provider types:</p><ul><li>aws-route53</li><li>azure-dns</li><li>azure-private-dns</li><li>google-clouddns</li><li>openstack-designate</li><li>alicloud-dns</li><li>cloudflare-dns</li></ul><h3 id=request-dns-records-for-ingress-resources>Request DNS records for Ingress resources</h3><p>To request a DNS name for an Ingress or Service object in the shoot cluster it must be annotated with the DNS class <code>garden</code> and an annotation denoting the desired DNS names.</p><p>Example for an annotated Ingress resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>kind: Ingress
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-ingress
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    <span style=color:green># Let Gardener manage external DNS records for this Ingress.</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: special.example.com <span style=color:green># Use &#34;*&#34; to collects domains names from .spec.rules[].host</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>    <span style=color:green># If you are delegating the certificate management to Gardener, uncomment the following line</span>
</span></span><span style=display:flex><span>    <span style=color:green>#cert.gardener.cloud/purpose: managed</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  rules:
</span></span><span style=display:flex><span>  - host: special.example.com
</span></span><span style=display:flex><span>    http:
</span></span><span style=display:flex><span>      paths:
</span></span><span style=display:flex><span>      - pathType: Prefix
</span></span><span style=display:flex><span>        path: <span style=color:#a31515>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        backend:
</span></span><span style=display:flex><span>          service:
</span></span><span style=display:flex><span>            name: amazing-svc
</span></span><span style=display:flex><span>            port:
</span></span><span style=display:flex><span>              number: 8080
</span></span><span style=display:flex><span>  <span style=color:green># Uncomment the following part if you are delegating the certificate management to Gardener</span>
</span></span><span style=display:flex><span>  <span style=color:green>#tls:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#  - hosts:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#      - special.example.com</span>
</span></span><span style=display:flex><span>  <span style=color:green>#    secretName: my-cert-secret-name</span>
</span></span></code></pre></div><p>For an Ingress, the DNS names are already declared in the specification. Nevertheless the <em>dnsnames</em> annotation must be present. Here a subset of the DNS names of the ingress can be specified. If DNS names for all names are desired, the value <code>all</code> can be used.</p><p>Keep in mind that ingress resources are ignored unless an ingress controller is set up. Gardener does not provide an ingress controller by default. For more details, see <a href=https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/>Ingress Controllers</a> and <a href=https://kubernetes.io/docs/concepts/services-networking/service/>Service</a> in the Kubernetes documentation.</p><h3 id=request-dns-records-for-service-type-loadbalancer>Request DNS records for service type LoadBalancer</h3><p>Example for an annotated Service (it must have the type <code>LoadBalancer</code>) resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: amazing-svc
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    <span style=color:green># Let Gardener manage external DNS records for this Service.</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: special.example.com
</span></span><span style=display:flex><span>    dns.gardener.cloud/ttl: <span style=color:#a31515>&#34;600&#34;</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    app: amazing-app
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>    - protocol: TCP
</span></span><span style=display:flex><span>      port: 80
</span></span><span style=display:flex><span>      targetPort: 8080
</span></span><span style=display:flex><span>  type: LoadBalancer
</span></span></code></pre></div><h4 id=creating-a-dnsentry-resource-explicitly>Creating a DNSEntry resource explicitly</h4><p>It is also possible to create a DNS entry via the Kubernetes resource called <code>DNSEntry</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: dns.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: DNSEntry
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    <span style=color:green># Let Gardener manage this DNS entry.</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>  name: special-dnsentry
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  dnsName: special.example.com
</span></span><span style=display:flex><span>  ttl: 600
</span></span><span style=display:flex><span>  targets:
</span></span><span style=display:flex><span>  - 1.2.3.4
</span></span></code></pre></div><p>If one of the accepted DNS names is a direct subname of the shoot&rsquo;s ingress domain, this is already handled by the standard wildcard entry for the ingress domain. Therefore this name should be excluded from the <em>dnsnames</em> list in the annotation. If only this DNS name is configured in the ingress, no explicit DNS entry is required, and the DNS annotations should be omitted at all.</p><p>You can check the status of the <code>DNSEntry</code> with</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl get dnsentry
</span></span><span style=display:flex><span>NAME          DNS                                                            TYPE          PROVIDER      STATUS    AGE
</span></span><span style=display:flex><span>mydnsentry    special.example.com     aws-route53   default/aws   Ready     24s
</span></span></code></pre></div><p>As soon as the status of the entry is <code>Ready</code>, the provider has accepted the new DNS record. Depending on the provider and your DNS settings and cache, <strong>it may take up to 24 hours for the new entry to be propagated over all internet</strong>.</p><p>More examples can be found <a href=https://github.com/gardener/external-dns-management/blob/master/examples/>here</a></p><h3 id=request-dns-records-for-serviceingress-resources-using-a-dnsannotation-resource>Request DNS records for Service/Ingress resources using a DNSAnnotation resource</h3><p>In rare cases it may not be possible to add annotations to a <code>Service</code> or <code>Ingress</code> resource object.</p><p>E.g.: the helm chart used to deploy the resource may not be adaptable for some reasons or some automation is used, which always restores the original content of the resource object by dropping any additional annotations.</p><p>In these cases, it is recommended to use an additional <code>DNSAnnotation</code> resource in order to have more flexibility that <code>DNSentry resources</code>. The <code>DNSAnnotation</code> resource makes the DNS shoot service behave as if annotations have been added to the referenced resource.</p><p>For the Ingress example shown above, you can create a <code>DNSAnnotation</code> resource alternatively to provide the annotations.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: dns.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: DNSAnnotation
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>  name: test-ingress-annotation
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  resourceRef:
</span></span><span style=display:flex><span>    kind: Ingress
</span></span><span style=display:flex><span>    apiVersion: networking.k8s.io/v1
</span></span><span style=display:flex><span>    name: test-ingress
</span></span><span style=display:flex><span>    namespace: default
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    dns.gardener.cloud/dnsnames: <span style=color:#a31515>&#39;*&#39;</span>
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden    
</span></span></code></pre></div><p>Note that the DNSAnnotation resource itself needs the <code>dns.gardener.cloud/class=garden</code> annotation. This also only works for annotations known to the DNS shoot service (see <a href=#accepted-external-dns-records-annotations>Accepted External DNS Records Annotations</a>).</p><p>For more details, see also <a href=https://github.com/gardener/external-dns-management#dnsannotation-objects>DNSAnnotation objects</a></p><h3 id=accepted-external-dns-records-annotations>Accepted External DNS Records Annotations</h3><p>Here are all of the accepted annotation related to the DNS extension:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>- dns.gardener.cloud/dnsnames <span style=color:green># Mandatory, accepts a comma-separated list of DNS names if multiple names are required</span>
</span></span><span style=display:flex><span>- dns.gardener.cloud/class <span style=color:green># Mandatory, DNS extension class name (usually &#34;garden&#34;)</span>
</span></span><span style=display:flex><span>- dns.gardener.cloud/ttl <span style=color:green># Recommended, Time-To-Live of the DNS record</span>
</span></span><span style=display:flex><span>- dns.gardener.cloud/cname-lookup-interval <span style=color:green># Optional, lookup interval for CNAMEs that must be resolved to IP (in seconds)</span>
</span></span><span style=display:flex><span>- dns.gardener.cloud/realms <span style=color:green># Optional, for restricting provider access for shoot DNS entries</span>
</span></span></code></pre></div><p>If one of the accepted DNS names is a direct subdomain of the shoot&rsquo;s ingress domain, this is already handled by the standard wildcard entry for the ingress domain. Therefore, this name should be excluded from the <em>dnsnames</em> list in the annotation. If only this DNS name is configured in the ingress, no explicit DNS entry is required, and the DNS annotations should be omitted at all.</p><h2 id=troubleshooting>Troubleshooting</h2><h3 id=general-dns-tools>General DNS tools</h3><p>To check the DNS resolution, use the <code>nslookup</code> or <code>dig</code> command.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ nslookup special.your-domain.com
</span></span></code></pre></div><p>or with dig</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ dig +short special.example.com
</span></span><span style=display:flex><span>Depending on your network settings, you may get a successful response faster using a public DNS server (e.g. 8.8.8.8, 8.8.4.4, or 1.1.1.1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>dig @8.8.8.8 +short special.example.com
</span></span></code></pre></div><h3 id=dns-record-events>DNS record events</h3><p>The DNS controller publishes Kubernetes events for the resource which requested the DNS record (Ingress, Service, DNSEntry). These events reveal more information about the DNS requests being processed and are especially useful to check any kind of misconfiguration, e.g. requests for a domain you don&rsquo;t own.</p><p>Events for a successfully created DNS record:</p><pre tabindex=0><code>$ kubectl describe service my-service

Events:
  Type    Reason          Age                From                    Message
  ----    ------          ----               ----                    -------
  Normal  dns-annotation  19s                dns-controller-manager  special.example.com: dns entry is pending
  Normal  dns-annotation  19s (x3 over 19s)  dns-controller-manager  special.example.com: dns entry pending: waiting for dns reconciliation
  Normal  dns-annotation  9s (x3 over 10s)   dns-controller-manager  special.example.com: dns entry active
</code></pre><p>Please note, events vanish after their retention period (usually <code>1h</code>).</p><h3 id=dnsentry-status>DNSEntry status</h3><p><code>DNSEntry</code> resources offer a <code>.status</code> sub-resource which can be used to check the current state of the object.</p><p>Status of a erroneous <code>DNSEntry</code>.</p><pre tabindex=0><code>  status:
    message: No responsible provider found
    observedGeneration: 3
    provider: remote
    state: Error
</code></pre><h2 id=references>References</h2><ul><li><a href=https://www.cloudflare.com/en-ca/learning/dns/what-is-dns>Understanding DNS</a></li><li><a href=https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/>Kubernetes Internal DNS</a></li><li><a href=https://github.com/gardener/external-dns-management/blob/master/pkg/apis/dns/v1alpha1/dnsentry.go>DNSEntry API (Golang)</a></li><li><a href=/docs/extensions/others/gardener-extension-shoot-cert-service/request_cert/>Managing Certificates with Gardener</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ecf32a50f50533e421c6d743abdd9ee2>5.2.4 - DNS Providers</h1><h1 id=dns-providers>DNS Providers</h1><h2 id=introduction>Introduction</h2><p>Gardener can manage DNS records on your behalf, so that you can request them via different resource types (see <a href=/docs/extensions/others/gardener-extension-shoot-dns-service/dns_names/>here</a>) within the shoot cluster. The domains for which you are permitted to request records, are however restricted and depend on the DNS provider configuration.</p><h2 id=shoot-provider>Shoot provider</h2><p>By default, every shoot cluster is equipped with a default provider. It is the very same provider that manages the shoot cluster&rsquo;s <code>kube-apiserver</code> public DNS record (DNS address in your Kubeconfig).</p><pre tabindex=0><code>kind: Shoot
...
dns:
  domain: shoot.project.default-domain.gardener.cloud
</code></pre><p>You are permitted to request any sub-domain of <code>.dns.domain</code> that is not already taken (e.g. <code>api.shoot.project.default-domain.gardener.cloud</code>, <code>*.ingress.shoot.project.default-domain.gardener.cloud</code>) with this provider.</p><h2 id=additional-providers>Additional providers</h2><p>If you need to request DNS records for domains not managed by the <a href=#Shoot-provider>default provider</a>, additional providers can
be configured in the shoot specification.
Alternatively, if it is enabled, it can be added as <code>DNSProvider</code> resources to the shoot cluster.</p><h3 id=additional-providers-in-the-shoot-specification>Additional providers in the shoot specification</h3><p>To add a providers in the shoot spec, you need set them in the <code>spec.dns.providers</code> list.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  dns:
</span></span><span style=display:flex><span>    domain: shoot.project.default-domain.gardener.cloud
</span></span><span style=display:flex><span>    providers:
</span></span><span style=display:flex><span>    - secretName: my-aws-account
</span></span><span style=display:flex><span>      type: aws-route53
</span></span><span style=display:flex><span>    - secretName: my-gcp-account
</span></span><span style=display:flex><span>      type: google-clouddns
</span></span></code></pre></div><blockquote><p>Please consult the <a href=https://gardener.cloud/docs/gardener/api-reference/core/#core.gardener.cloud/v1beta1.DNSProvider>API-Reference</a> to get a complete list of supported fields and configuration options.</p></blockquote><p>Referenced secrets should exist in the project namespace in the Garden cluster and must comply with the provider specific credentials format. The <strong>External-DNS-Management</strong> project provides corresponding examples (<a href=https://github.com/gardener/external-dns-management/tree/master/examples>20-secret-&lt;provider-name>-credentials.yaml</a>) for known providers.</p><h3 id=additional-providers-as-resources-in-the-shoot-cluster>Additional providers as resources in the shoot cluster</h3><p>If it is not enabled globally, you have to enable the feature in the shoot manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>Kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-dns-service
</span></span><span style=display:flex><span>      providerConfig:
</span></span><span style=display:flex><span>        apiVersion: service.dns.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>        kind: DNSConfig
</span></span><span style=display:flex><span>        dnsProviderReplication:
</span></span><span style=display:flex><span>          enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>To add a provider directly in the shoot cluster, provide a <code>DNSProvider</code> in any namespace together
with <code>Secret</code> containing the credentials.</p><p>For example if the domain is hosted with AWS Route 53 (provider type <code>aws-route53</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: dns.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: DNSProvider
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    dns.gardener.cloud/class: garden
</span></span><span style=display:flex><span>  name: my-own-domain
</span></span><span style=display:flex><span>  namespace: my-namespace
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: aws-route53
</span></span><span style=display:flex><span>  secretRef:
</span></span><span style=display:flex><span>    name: my-own-domain-credentials
</span></span><span style=display:flex><span>  domains:
</span></span><span style=display:flex><span>    include:
</span></span><span style=display:flex><span>    - my.own.domain.com
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Secret
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: my-own-domain-credentials
</span></span><span style=display:flex><span>  namespace: my-namespace
</span></span><span style=display:flex><span>type: Opaque
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  <span style=color:green># replace &#39;...&#39; with values encoded as base64</span>
</span></span><span style=display:flex><span>  AWS_ACCESS_KEY_ID: ...
</span></span><span style=display:flex><span>  AWS_SECRET_ACCESS_KEY: ...
</span></span></code></pre></div><p>The <strong>External-DNS-Management</strong> project provides examples with more details for <code>DNSProviders</code> (30-provider-&lt;provider-name>.yaml)
and credential <code>Secrets</code> (20-secret-&lt;provider-name>.yaml) at <a href=https://github.com/gardener/external-dns-management/tree/master/examples>https://github.com/gardener/external-dns-management//examples</a>
for all supported provider types.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-26c3d61f82c9dd9b0643835430f275f6>5.3 - Egress filtering</h1><div class=lead>Gardener extension controller for egress filtering for shoot clusters</div><h1 id=gardener-extension-for-networking-filterhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Networking Filter</a></h1><p><a href=https://reuse.software/><img src=https://reuse.software/badge/reuse-compliant.svg alt="reuse compliant"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the <code>shoot-networking-filter</code> extension.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-shoot-networking-filter/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=extension-resources>Extension Resources</h2><p>Currently there is nothing to specify in the extension spec.</p><p>Example extension resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Extension
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-shoot-networking-filter
</span></span><span style=display:flex><span>  namespace: shoot--project--abc
</span></span><span style=display:flex><span>spec:
</span></span></code></pre></div><p>When an extension resource is reconciled, the extension controller will create a daemonset <code>egress-filter-applier</code> on the shoot containing a <a href=https://github.com/gardener/egress-filter-refresher/blob/master/Dockerfile>Dockerfile</a> container.</p><p>Please note, this extension controller relies on the <a href=/docs/gardener/concepts/resource-manager/>Gardener-Resource-Manager</a> to deploy k8s resources to seed and shoot clusters.</p><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-shoot-networking-filter/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0d6e87b50dd76f1d75b01b3bb7cc698f>5.3.1 - Deployment</h1><h1 id=gardener-networking-policy-filter-for-shoots>Gardener Networking Policy Filter for Shoots</h1><h2 id=introduction>Introduction</h2><p>Gardener allows shoot clusters to filter egress traffic on node level. To support this the Gardener must be installed with the <code>shoot-networking-filter</code> extension.</p><h2 id=configuration>Configuration</h2><p>To generally enable the networking filter for shoot objects the <code>shoot-networking-filter</code> extension must be registered by providing an appropriate <a href=https://github.com/gardener/gardener-extension-shoot-networking-filter/blob/master/example/controller-registration.yaml>extension registration</a> in the garden cluster.</p><p>Here it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.</p><p>If the extension should be used for all shoots the <code>globallyEnabled</code> flag should be set to <code>true</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerRegistration
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>    - kind: Extension
</span></span><span style=display:flex><span>      type: shoot-networking-filter
</span></span><span style=display:flex><span>      globallyEnabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h3 id=controllerregistration>ControllerRegistration</h3><p>An example of a <code>ControllerRegistration</code> for the <code>shoot-networking-filter</code> can be found at <a href=https://github.com/gardener/gardener-extension-shoot-networking-filter/blob/master/example/controller-registration.yaml>controller-registration.yaml</a>.</p><p>The <code>ControllerRegistration</code> contains a Helm chart which eventually deploys the <code>shoot-networking-filter</code> to seed clusters. It offers some configuration options, mainly to set up a static filter list or provide the configuration for downloading the filter list from a service endpoint.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerDeployment
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>  values:
</span></span><span style=display:flex><span>    egressFilter:
</span></span><span style=display:flex><span>      blackholingEnabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      filterListProviderType: static
</span></span><span style=display:flex><span>      staticFilterList:
</span></span><span style=display:flex><span>        - network: 1.2.3.4/31
</span></span><span style=display:flex><span>          policy: BLOCK_ACCESS
</span></span><span style=display:flex><span>        - network: 5.6.7.8/32
</span></span><span style=display:flex><span>          policy: BLOCK_ACCESS
</span></span><span style=display:flex><span>        - network: ::2/128
</span></span><span style=display:flex><span>          policy: BLOCK_ACCESS
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:green>#filterListProviderType: download</span>
</span></span><span style=display:flex><span>      <span style=color:green>#downloaderConfig:</span>
</span></span><span style=display:flex><span>      <span style=color:green>#  endpoint: https://my.filter.list.server/lists/policy</span>
</span></span><span style=display:flex><span>      <span style=color:green>#  oauth2Endpoint: https://my.auth.server/oauth2/token</span>
</span></span><span style=display:flex><span>      <span style=color:green>#  refreshPeriod: 1h</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:green>## if the downloader needs an OAuth2 access token, client credentials can be provided with oauth2Secret</span>
</span></span><span style=display:flex><span>      <span style=color:green>#oauth2Secret:</span>
</span></span><span style=display:flex><span>      <span style=color:green># clientID: 1-2-3-4</span>
</span></span><span style=display:flex><span>      <span style=color:green># clientSecret: secret!!</span>
</span></span><span style=display:flex><span>      <span style=color:green>## either clientSecret of client certificate is required</span>
</span></span><span style=display:flex><span>      <span style=color:green># client.crt.pem: |</span>
</span></span><span style=display:flex><span>      <span style=color:green>#   -----BEGIN CERTIFICATE-----</span>
</span></span><span style=display:flex><span>      <span style=color:green>#   ...</span>
</span></span><span style=display:flex><span>      <span style=color:green>#   -----END CERTIFICATE-----</span>
</span></span><span style=display:flex><span>      <span style=color:green># client.key.pem: |</span>
</span></span><span style=display:flex><span>      <span style=color:green>#   -----BEGIN PRIVATE KEY-----</span>
</span></span><span style=display:flex><span>      <span style=color:green>#   ...</span>
</span></span><span style=display:flex><span>      <span style=color:green>#   -----END PRIVATE KEY-----</span>
</span></span></code></pre></div><h3 id=enablement-for-a-shoot>Enablement for a Shoot</h3><p>If the shoot networking filter is not globally enabled by default (depends on the extension registration on the garden cluster), it can be enabled per shoot. To enable the service for a shoot, the shoot manifest must explicitly add the <code>shoot-networking-filter</code> extension.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-networking-filter
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>If the shoot networking filter is globally enabled by default, it can be disabled per shoot. To disable the service for a shoot, the shoot manifest must explicitly state it.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-networking-filter
</span></span><span style=display:flex><span>      disabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-51bc5860c666351f02b783c6f9331d81>5.3.2 - Shoot Networking Filter</h1><h1 id=register-shoot-networking-filter-extension-in-shoot-clusters>Register Shoot Networking Filter Extension in Shoot Clusters</h1><h2 id=introduction>Introduction</h2><p>Within a shoot cluster, it is possible to enable the networking filter. It is necessary that the Gardener installation your shoot cluster runs in is equipped with a <code>shoot-networking-filter</code> extension. Please ask your Gardener operator if the extension is available in your environment.</p><h2 id=shoot-feature-gate>Shoot Feature Gate</h2><p>In most of the Gardener setups the <code>shoot-networking-filter</code> extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-networking-filter
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=opt-out>Opt-out</h2><p>If the shoot networking filter is globally enabled by default, it can be disabled per shoot. To disable the service for a shoot, the shoot manifest must explicitly state it.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-networking-filter
</span></span><span style=display:flex><span>      disabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=ingress-filtering>Ingress Filtering</h2><p>By default, the networking filter only filters egress traffic. However, if you enable blackholing, incoming traffic will also be blocked.
You can enable blackholing on a per-shoot basis.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-networking-filter
</span></span><span style=display:flex><span>      providerConfig:
</span></span><span style=display:flex><span>        egressFilter:
</span></span><span style=display:flex><span>          blackholingEnabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>Ingress traffic can only be blocked by blackhole routing, if the source IP address is preserved. On Azure, GCP and AliCloud this works by default.
The default on AWS is a classic load balancer that replaces the source IP by it&rsquo;s own IP address. Here, a network load balancer has to be
configured adding the annotation <code>service.beta.kubernetes.io/aws-load-balancer-type: "nlb"</code> to the service.
On OpenStack, load balancers don&rsquo;t preserve the source address.</p><p>Please note that if you disable <code>blackholing</code> in an existing shoot, the associated blackhole routes will not be removed automatically.
To remove these routes, you can either replace the affected nodes or delete the routes manually.</p><h2 id=custom-ip>Custom IP</h2><p>It is possible to add custom IP addresses to the network filter. This can be useful for testing purposes.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-networking-filter
</span></span><span style=display:flex><span>      providerConfig:
</span></span><span style=display:flex><span>        egressFilter:
</span></span><span style=display:flex><span>          staticFilterList:
</span></span><span style=display:flex><span>          - network: 1.2.3.4/31
</span></span><span style=display:flex><span>            policy: BLOCK_ACCESS
</span></span><span style=display:flex><span>          - network: 5.6.7.8/32
</span></span><span style=display:flex><span>            policy: BLOCK_ACCESS
</span></span><span style=display:flex><span>          - network: ::2/128
</span></span><span style=display:flex><span>            policy: BLOCK_ACCESS
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-ccc7f4f683b2c7803e99873d2bc4a42a>5.4 - Lakom service</h1><div class=lead>A k8s admission controller verifying pods are using signed images (cosign signatures) and a gardener extension to install it for shoots and seeds.</div><h1 id=gardener-extension-for-lakom-serviceshttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for lakom services</a></h1><p><a href=https://reuse.software/><img src=https://reuse.software/badge/reuse-compliant.svg alt="reuse compliant"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the <code>shoot-lakom-service</code> extension.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-shoot-lakom-service/blob/main/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=lakom-admission-controller>Lakom Admission Controller</h2><p>Lakom is kubernetes admission controller which purpose is to implement <a href=https://github.com/sigstore/cosign>cosign</a> image signature verification against public cosign key. It also takes care to resolve image tags to sha256 digests. It also caches all OCI artifacts to reduce the load toward the OCI registry.</p><h2 id=extension-resources>Extension Resources</h2><p>Example extension resource:</p><blockquote></blockquote><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Extension
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-shoot-lakom-service
</span></span><span style=display:flex><span>  namespace: shoot--project--abc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: shoot-lakom-service
</span></span></code></pre></div><p>When an extension resource is reconciled, the extension controller will create an instance of <code>lakom</code> admission controller. These resources are placed inside the shoot namespace on the seed. Also, the controller takes care about generating necessary <code>RBAC</code> resources for the seed as well as for the shoot.</p><p>Please note, this extension controller relies on the <a href=/docs/gardener/concepts/resource-manager/>Gardener-Resource-Manager</a> to deploy k8s resources to seed and shoot clusters.</p><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>The <code>Lakom</code> admission controller can be configured with <code>make dev-setup</code> and started with <code>make start-lakom</code>.
You can run the lakom extension controller locally on your machine by executing <code>make start</code>.</p><p>We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-shoot-lakom-service/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c0e9837a0c6f3e6821df938a89b541e1>5.4.1 - Deployment</h1><h1 id=gardener-lakom-service-for-shoots>Gardener Lakom Service for Shoots</h1><h2 id=introduction>Introduction</h2><p>Gardener allows Shoot clusters to use <code>Lakom</code> admission controller for cosign image signing verification. To support this the Gardener must be installed with the <code>shoot-lakom-service</code> extension.</p><h2 id=configuration>Configuration</h2><p>To generally enable the Lakom service for shoot objects the <code>shoot-lakom-service</code> extension must be registered by providing an appropriate <a href=https://github.com/gardener/gardener-extension-shoot-lakom-service/blob/main/example/controller-registration.yaml>extension registration</a> in the garden cluster.</p><p>Here it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.</p><p>If the extension should be used for all shoots the <code>globallyEnabled</code> flag should be set to <code>true</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>    - kind: Extension
</span></span><span style=display:flex><span>      type: shoot-lakom-service
</span></span><span style=display:flex><span>      globallyEnabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h3 id=shoot-feature-gate>Shoot Feature Gate</h3><p>If the shoot Lakom service is not globally enabled by default (depends on the extension registration on the garden cluster), it can be enabled per shoot. To enable the service for a shoot, the shoot manifest must explicitly add the <code>shoot-lakom-service</code> extension.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-lakom-service
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>If the shoot Lakom service is globally enabled by default, it can be disabled per shoot. To disable the service for a shoot, the shoot manifest must explicitly state it.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-lakom-service
</span></span><span style=display:flex><span>      disabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-f2dfab902de56f28952cebcd9108225f>5.4.2 - Shoot Extension</h1><h2 id=introduction>Introduction</h2><p>This extension implements <a href=https://github.com/sigstore/cosign>cosign</a> image verification. It is strictly limited only to the kubernetes system components deployed by Gardener and other Gardener Extensions in the <code>kube-system</code> namespace of a shoot cluster.</p><h2 id=shoot-feature-gate>Shoot Feature Gate</h2><p>In most of the Gardener setups the <code>shoot-lakom-service</code> extension is enabled globally and thus can be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to disable the extension individually.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: shoot-lakom-service
</span></span><span style=display:flex><span>    disabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-d21abe30553ea86e1f5973d8484fc9d1>5.5 - Networking problemdetector</h1><div class=lead>Gardener extension for deploying network problem detector</div><h1 id=gardener-extension-for-network-problem-detectorhttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Network Problem Detector</a></h1><p><a href=https://reuse.software/><img src=https://reuse.software/badge/reuse-compliant.svg alt="reuse compliant"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the <code>shoot-networking-problemdetector</code> extension.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-shoot-networking-problemdetector/blob/main/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=extension-resources>Extension Resources</h2><p>Currently there is nothing to specify in the extension spec.</p><p>Example extension resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Extension
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-shoot-networking-problemdetector
</span></span><span style=display:flex><span>  namespace: shoot--project--abc
</span></span><span style=display:flex><span>spec:
</span></span></code></pre></div><p>When an extension resource is reconciled, the extension controller will create two daemonsets <code>nwpd-agent-pod-net</code> and <code>nwpd-agent-node-net</code> deploying
the &ldquo;network problem detector agent&rdquo;.
These daemon sets perform and collect various checks between all nodes of the Kubernetes cluster, to its Kube API server and/or external endpoints.
Checks are performed using TCP connections, PING (ICMP) or mDNS (UDP).
More details about the network problem detector agent can be found in its repository <a href=https://github.com/gardener/network-problem-detector>gardener/network-problem-detector</a>.</p><p>Please note, this extension controller relies on the <a href=/docs/gardener/concepts/resource-manager/>Gardener-Resource-Manager</a> to deploy k8s resources to seed and shoot clusters.</p><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-shoot-networking-problemdetector/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-bebb19906236a4aead1990c60039a112>5.5.1 - Deployment</h1><h1 id=gardener-networking-policy-filter-for-shoots>Gardener Networking Policy Filter for Shoots</h1><h2 id=introduction>Introduction</h2><p>Gardener allows shoot clusters to add network problem observability using the network problem detector.
To support this the Gardener must be installed with the <code>shoot-networking-problemdetector</code> extension.</p><h2 id=configuration>Configuration</h2><p>To generally enable the networking problem detector for shoot objects the <code>shoot-networking-problemdetector</code> extension must be registered by providing an appropriate <a href=https://github.com/gardener/gardener-extension-shoot-networking-problemdetector/blob/main/example/controller-registration.yaml>extension registration</a> in the garden cluster.</p><p>Here it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.</p><p>If the extension should be used for all shoots the <code>globallyEnabled</code> flag should be set to <code>true</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerRegistration
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>    - kind: Extension
</span></span><span style=display:flex><span>      type: shoot-networking-problemdetector
</span></span><span style=display:flex><span>      globallyEnabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h3 id=controllerregistration>ControllerRegistration</h3><p>An example of a <code>ControllerRegistration</code> for the <code>shoot-networking-problemdetector</code> can be found at <a href=https://github.com/gardener/gardener-extension-shoot-networking-problemdetector/blob/master/example/controller-registration.yaml>controller-registration.yaml</a>.</p><p>The <code>ControllerRegistration</code> contains a Helm chart which eventually deploys the <code>shoot-networking-problemdetector</code> to seed clusters. It offers some configuration options, mainly to set up a static filter list or provide the configuration for downloading the filter list from a service endpoint.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: ControllerDeployment
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>  values:
</span></span><span style=display:flex><span>    <span style=color:green>#networkProblemDetector:</span>
</span></span><span style=display:flex><span>    <span style=color:green>#  defaultPeriod: 30s</span>
</span></span></code></pre></div><h3 id=enablement-for-a-shoot>Enablement for a Shoot</h3><p>If the shoot network problem detector is not globally enabled by default (depends on the extension registration on the garden cluster), it can be enabled per shoot. To enable the service for a shoot, the shoot manifest must explicitly add the <code>shoot-networking-problemdetector</code> extension.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-networking-problemdetector
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>If the shoot network problem detector is globally enabled by default, it can be disabled per shoot. To disable the service for a shoot, the shoot manifest must explicitly state it.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-networking-problemdetector
</span></span><span style=display:flex><span>      disabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-c9d138e16c74e4d2eff97bab16571286>5.5.2 - Shoot Networking Problemdetector</h1><h1 id=register-shoot-networking-filter-extension-in-shoot-clusters>Register Shoot Networking Filter Extension in Shoot Clusters</h1><h2 id=introduction>Introduction</h2><p>Within a shoot cluster, it is possible to enable the network problem detector. It is necessary that the Gardener installation your shoot cluster runs in is equipped with a <code>shoot-networking-problemdetector</code> extension. Please ask your Gardener operator if the extension is available in your environment.</p><h2 id=shoot-feature-gate>Shoot Feature Gate</h2><p>In most of the Gardener setups the <code>shoot-networking-problemdetector</code> extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-networking-problemdetector
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=opt-out>Opt-out</h2><p>If the shoot network problem detector is globally enabled by default, it can be disabled per shoot. To disable the service for a shoot, the shoot manifest must explicitly state it.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-networking-problemdetector
</span></span><span style=display:flex><span>      disabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-4872a011abb88ad2166ddf01e06b0631>5.6 - OpenID Connect services</h1><div class=lead>Gardener extension controller for OpenID Connect services for shoot clusters</div><h1 id=gardener-extension-for-openid-connect-serviceshttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for openid connect services</a></h1><p><a href=https://reuse.software/><img src=https://reuse.software/badge/reuse-compliant.svg alt="reuse compliant"></a></p><p>Project Gardener implements the automated management and operation of <a href=https://kubernetes.io/>Kubernetes</a> clusters as a service.
Its main principle is to leverage Kubernetes concepts for all of its tasks.</p><p>Recently, most of the vendor specific logic has been developed <a href=https://github.com/gardener/gardener>in-tree</a>.
However, the project has grown to a size where it is very hard to extend, maintain, and test.
With <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1</a> we have proposed how the architecture can be changed in a way to support external controllers that contain their very own vendor specifics.
This way, we can keep Gardener core clean and independent.</p><p>This controller implements Gardener&rsquo;s extension contract for the <code>shoot-oidc-service</code> extension.</p><p>An example for a <code>ControllerRegistration</code> resource that can be used to register this controller to Gardener can be found <a href=https://github.com/gardener/gardener-extension-shoot-oidc-service/blob/master/example/controller-registration.yaml>here</a>.</p><p>Please find more information regarding the extensibility concepts and a detailed proposal <a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>here</a>.</p><h2 id=compatibility>Compatibility</h2><p>The following lists compatibility requirements of this extension controller with regards to other Gardener components.</p><table><thead><tr><th>OIDC Extension</th><th>Gardener</th><th>Notes</th></tr></thead><tbody><tr><td><code>== v0.15.0</code></td><td><code>>= 1.60.0 &lt;= v1.64.0</code></td><td>A typical side-effect when running Gardener &lt; v1.63.0 is an unexpected scale-down of the OIDC webhook from <code>2 -> 1</code>.</td></tr><tr><td><code>== v0.16.0</code></td><td><code>>= 1.65.0</code></td><td></td></tr></tbody></table><hr><h2 id=extension-resources>Extension Resources</h2><p>Example extension resource:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: Extension
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: extension-shoot-oidc-service
</span></span><span style=display:flex><span>  namespace: shoot--project--abc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  type: shoot-oidc-service
</span></span></code></pre></div><p>When an extension resource is reconciled, the extension controller will create an instance of <a href=https://github.com/gardener/oidc-webhook-authenticator>OIDC Webhook Authenticator</a>. These resources are placed inside the shoot namespace on the seed. Also, the controller takes care about generating necessary <code>RBAC</code> resources for the seed as well as for the shoot.</p><p>Please note, this extension controller relies on the <a href=/docs/gardener/concepts/resource-manager/>Gardener-Resource-Manager</a> to deploy k8s resources to seed and shoot clusters.</p><h2 id=how-to-start-using-or-developing-this-extension-controller-locally>How to start using or developing this extension controller locally</h2><p>You can run the controller locally on your machine by executing <code>make start</code>.</p><p>We are using Go modules for Golang package dependency management and <a href=https://github.com/onsi/ginkgo>Ginkgo</a>/<a href=https://github.com/onsi/gomega>Gomega</a> for testing.</p><h2 id=feedback-and-support>Feedback and Support</h2><p>Feedback and contributions are always welcome. Please report bugs or suggestions as <a href=https://github.com/gardener/gardener-extension-shoot-oidc-service/issues>GitHub issues</a> or join our <a href=https://kubernetes.slack.com/messages/gardener>Slack channel #gardener</a> (please invite yourself to the Kubernetes workspace <a href=http://slack.k8s.io>here</a>).</p><h2 id=learn-more>Learn more!</h2><p>Please find further resources about out project here:</p><ul><li><a href=https://gardener.cloud/>Our landing page gardener.cloud</a></li><li><a href=https://kubernetes.io/blog/2018/05/17/gardener/>&ldquo;Gardener, the Kubernetes Botanist&rdquo; blog on kubernetes.io</a></li><li><a href=https://kubernetes.io/blog/2019/12/02/gardener-project-update/>&ldquo;Gardener Project Update&rdquo; blog on kubernetes.io</a></li><li><a href=https://github.com/gardener/gardener/blob/master/docs/proposals/01-extensibility.md>GEP-1 (Gardener Enhancement Proposal) on extensibility</a></li><li><a href=https://github.com/gardener/gardener/tree/master/docs/extensions>Extensibility API documentation</a></li><li><a href=https://godoc.org/github.com/gardener/gardener/extensions/pkg>Gardener Extensions Golang library</a></li><li><a href=https://gardener.cloud/api-reference/>Gardener API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-486a89ff012d57a06dd34631cf82838a>5.6.1 - Deployment</h1><h1 id=gardener-oidc-service-for-shoots>Gardener OIDC Service for Shoots</h1><h2 id=introduction>Introduction</h2><p>Gardener allows Shoot clusters to dynamically register OpenID Connect providers. To support this the Gardener must be installed with the <code>shoot-oidc-service</code> extension.</p><h2 id=configuration>Configuration</h2><p>To generally enable the OIDC service for shoot objects the <code>shoot-oidc-service</code> extension must be registered by providing an appropriate <a href=https://github.com/gardener/gardener-extension-shoot-oidc-service/blob/master/example/controller-registration.yaml>extension registration</a> in the garden cluster.</p><p>Here it is possible to decide whether the extension should be always available for all shoots or whether the extension must be separately enabled per shoot.</p><p>If the extension should be used for all shoots the <code>globallyEnabled</code> flag should be set to <code>true</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>    - kind: Extension
</span></span><span style=display:flex><span>      type: shoot-oidc-service
</span></span><span style=display:flex><span>      globallyEnabled: <span style=color:#00f>true</span>
</span></span></code></pre></div><h3 id=shoot-feature-gate>Shoot Feature Gate</h3><p>If the shoot OIDC service is not globally enabled by default (depends on the extension registration on the garden cluster), it can be enabled per shoot. To enable the service for a shoot, the shoot manifest must explicitly add the <code>shoot-oidc-service</code> extension.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-oidc-service
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>If the shoot OIDC service is globally enabled by default, it can be disabled per shoot. To disable the service for a shoot, the shoot manifest must explicitly state it.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-oidc-service
</span></span><span style=display:flex><span>      disabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-22a2de35d1bea1230de182b1da4a55fc>5.6.2 - Openidconnects</h1><h1 id=register-openid-connect-provider-in-shoot-clusters>Register OpenID Connect provider in Shoot Clusters</h1><h2 id=introduction>Introduction</h2><p>Within a shoot cluster, it is possible to dynamically register OpenID Connect providers. It is necessary that the Gardener installation your shoot cluster runs in is equipped with a <code>shoot-oidc-service</code> extension. Please ask your Gardener operator if the extension is available in your environment.</p><h2 id=shoot-feature-gate>Shoot Feature Gate</h2><p>In most of the Gardener setups the <code>shoot-oidc-service</code> extension is not enabled globally and thus must be configured per shoot cluster. Please adapt the shoot specification by the configuration shown below to activate the extension individually.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>    - type: shoot-oidc-service
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=openid-connect-provider>OpenID Connect provider</h2><p>In order to register an OpenID Connect provider an <code>openidconnect</code> resource should be deployed in the shoot cluster.</p><p>It is <strong>strongly</strong> recommended to <strong>NOT</strong> disable prefixing since it may result in unwanted impersonations. The rule of thumb is to always use meaningful and unique prefixes for both <code>username</code> and <code>groups</code>. A good way to ensure this is to use the name of the <code>openidconnect</code> resource as shown in the example below.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: authentication.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: OpenIDConnect
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: abc
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  <span style=color:green># issuerURL is the URL the provider signs ID Tokens as.</span>
</span></span><span style=display:flex><span>  <span style=color:green># This will be the &#34;iss&#34; field of all tokens produced by the provider and is used for configuration discovery.</span>
</span></span><span style=display:flex><span>  issuerURL: https://abc-oidc-provider.example
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># clientID is the audience for which the JWT must be issued for, the &#34;aud&#34; field.</span>
</span></span><span style=display:flex><span>  clientID: my-shoot-cluster
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># usernameClaim is the JWT field to use as the user&#39;s username.</span>
</span></span><span style=display:flex><span>  usernameClaim: sub
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># usernamePrefix, if specified, causes claims mapping to username to be prefix with the provided value.</span>
</span></span><span style=display:flex><span>  <span style=color:green># A value &#34;oidc:&#34; would result in usernames like &#34;oidc:john&#34;.</span>
</span></span><span style=display:flex><span>  <span style=color:green># If not provided, the prefix defaults to &#34;( .metadata.name )/&#34;. The value &#34;-&#34; can be used to disable all prefixing.</span>
</span></span><span style=display:flex><span>  usernamePrefix: <span style=color:#a31515>&#34;abc:&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># groupsClaim, if specified, causes the OIDCAuthenticator to try to populate the user&#39;s groups with an ID Token field.</span>
</span></span><span style=display:flex><span>  <span style=color:green># If the groupsClaim field is present in an ID Token the value must be a string or list of strings.</span>
</span></span><span style=display:flex><span>  <span style=color:green># groupsClaim: groups</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># groupsPrefix, if specified, causes claims mapping to group names to be prefixed with the value.</span>
</span></span><span style=display:flex><span>  <span style=color:green># A value &#34;oidc:&#34; would result in groups like &#34;oidc:engineering&#34; and &#34;oidc:marketing&#34;.</span>
</span></span><span style=display:flex><span>  <span style=color:green># If not provided, the prefix defaults to &#34;( .metadata.name )/&#34;.</span>
</span></span><span style=display:flex><span>  <span style=color:green># The value &#34;-&#34; can be used to disable all prefixing.</span>
</span></span><span style=display:flex><span>  <span style=color:green># groupsPrefix: &#34;abc:&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># caBundle is a PEM encoded CA bundle which will be used to validate the OpenID server&#39;s certificate. If unspecified, system&#39;s trusted certificates are used.</span>
</span></span><span style=display:flex><span>  <span style=color:green># caBundle: &lt;base64 encoded bundle&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># supportedSigningAlgs sets the accepted set of JOSE signing algorithms that can be used by the provider to sign tokens.</span>
</span></span><span style=display:flex><span>  <span style=color:green># The default value is RS256.</span>
</span></span><span style=display:flex><span>  <span style=color:green># supportedSigningAlgs:</span>
</span></span><span style=display:flex><span>  <span style=color:green># - RS256</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># requiredClaims, if specified, causes the OIDCAuthenticator to verify that all the</span>
</span></span><span style=display:flex><span>  <span style=color:green># required claims key value pairs are present in the ID Token.</span>
</span></span><span style=display:flex><span>  <span style=color:green># requiredClaims:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   customclaim: requiredvalue</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># maxTokenExpirationSeconds if specified, sets a limit in seconds to the maximum validity duration of a token.</span>
</span></span><span style=display:flex><span>  <span style=color:green># Tokens issued with validity greater that this value will not be verified.</span>
</span></span><span style=display:flex><span>  <span style=color:green># Setting this will require that the tokens have the &#34;iat&#34; and &#34;exp&#34; claims.</span>
</span></span><span style=display:flex><span>  <span style=color:green># maxTokenExpirationSeconds: 3600</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green># jwks if specified, provides an option to specify JWKS keys offline.</span>
</span></span><span style=display:flex><span>  <span style=color:green># jwks:</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   keys is a base64 encoded JSON webkey Set. If specified, the OIDCAuthenticator skips the request to the issuer&#39;s jwks_uri endpoint to retrieve the keys.</span>
</span></span><span style=display:flex><span>  <span style=color:green>#   keys: &lt;base64 encoded jwks&gt;</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-a725bc0c0d89baaccba25cbb56b86675>5.7 - Registry cache</h1><div class=lead>Gardener extension controller which deploys pull-through caches for container registries.</div><h1 id=gardener-extension-for-registry-cachehttpsgardenercloud><a href=https://gardener.cloud>Gardener Extension for Registry Cache</a></h1><p><a href=https://concourse.ci.gardener.cloud/teams/gardener-tests/pipelines/gardener-extension-registry-cache-main/jobs/main-head-update-job><img src=https://concourse.ci.gardener.cloud/api/v1/teams/gardener-tests/pipelines/gardener-extension-registry-cache-main/jobs/main-head-update-job/badge alt="CI Build status"></a>
<a href=https://goreportcard.com/report/github.com/gardener/gardener-extension-registry-cache><img src=https://goreportcard.com/badge/github.com/gardener/gardener-extension-registry-cache alt="Go Report Card"></a></p><p>Gardener extension controller which deploys pull-through caches for container registries.</p><h2 id=usage>Usage</h2><ul><li><a href=/docs/extensions/others/gardener-extension-registry-cache/configuration/>Configuring the Registry Cache Extension</a> - learn what is the use-case for a pull-through cache, how to enable it and configure it</li><li><a href=/docs/extensions/others/gardener-extension-registry-cache/upstream-credentials/>How to provide credentials for upstream repository?</a></li><li><a href=/docs/extensions/others/gardener-extension-registry-cache/migration-from-v1alpha1-to-v1alpha2/>Migration from <code>v1alpha1</code> to <code>v1alpha2</code></a> - learn how to migrate from the <code>v1alpha1</code> API version of the <code>RegistryConfig</code> to <code>v1alpha2</code></li></ul><h2 id=local-setup-and-development>Local setup and development</h2><ul><li><a href=/docs/extensions/others/gardener-extension-registry-cache/getting-started-locally/>Deploying Registry Cache Extension Locally</a> - learn how to set up a local development environment</li><li><a href=/docs/extensions/others/gardener-extension-registry-cache/extension-registry-cache/>Developer Docs for Gardener Extension Registry Cache</a> - learn about the inner workings</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-87d8490f33137ca866b523574411cfbb>5.7.1 - Configuration</h1><h1 id=configuring-the-registry-cache-extension>Configuring the Registry Cache Extension</h1><h2 id=introduction>Introduction</h2><h3 id=use-case>Use-case</h3><p>For a Shoot cluster, the containerd daemon of every Node goes to the internet and fetches an image that it doesn&rsquo;t have locally in the Node&rsquo;s image cache. New Nodes are often created due to events such as auto-scaling (scale up), rolling update, or replacement of unhealthy Node. Such a new Node would need to pull all of the images of the Pods running on it from the internet because the Node&rsquo;s cache is initially empty. Pulling an image from a registry produces network traffic and registry costs. To avoid these network traffic and registry costs, you can use the registry-cache extension to run a registry as pull-through cache.</p><p>The following diagram shows a rough outline of how image pull looks like for a Shoot cluster <strong>without registry cache</strong>:
<img src=/__resources/shoot-cluster-without-registry-cache_a0770d.png alt=shoot-cluster-without-registry-cache></p><h3 id=solution>Solution</h3><p>The registry-cache extension deploys and manages a registry in the Shoot cluster that runs as pull-through cache. The used registry implementation is <a href=https://github.com/distribution/distribution>distribution/distribution</a>.</p><h3 id=how-does-it-work>How does it work?</h3><p>When the extension is enabled, a registry cache for each configured upstream is deployed to the Shoot cluster. Along with this, the containerd daemon on the Shoot cluster Nodes gets configured to use as a mirror the Service IP address of the deployed registry cache. For example, if a registry cache for upstream <code>docker.io</code> is requested via the Shoot spec, then containerd gets configured to first pull the image from the deployed cache in the Shoot cluster. If this image pull operation fails, containerd falls back to the upstream itself (<code>docker.io</code> in that case).</p><p>The first time an image is requested from the pull-through cache, it pulls the image from the configured upstream registry and stores it locally before handing it back to the client. On subsequent requests, the pull-through cache is able to serve the image from its own storage.</p><blockquote><p>Note: The used registry implementation (<a href=https://github.com/distribution/distribution>distribution/distribution</a>) supports mirroring of only one upstream registry.</p></blockquote><p>The following diagram shows a rough outline of how image pull looks like for a Shoot cluster <strong>with registry cache</strong>:
<img src=/__resources/shoot-cluster-with-registry-cache_9bbc9e.png alt=shoot-cluster-with-registry-cache></p><h2 id=shoot-configuration>Shoot Configuration</h2><p>The extension is not globally enabled and must be configured per Shoot cluster. The Shoot specification has to be adapted to include the <code>registry-cache</code> extension configuration.</p><p>Below is an example of <code>registry-cache</code> extension configuration as part of the Shoot spec:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: crazy-botany
</span></span><span style=display:flex><span>  namespace: garden-dev
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: registry-cache
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: registry.extensions.gardener.cloud/v1alpha2
</span></span><span style=display:flex><span>      kind: RegistryConfig
</span></span><span style=display:flex><span>      caches:
</span></span><span style=display:flex><span>      - upstream: docker.io
</span></span><span style=display:flex><span>        volume:
</span></span><span style=display:flex><span>          size: 100Gi
</span></span><span style=display:flex><span>          storageClassName: premium
</span></span><span style=display:flex><span>      - upstream: ghcr.io
</span></span><span style=display:flex><span>      - upstream: quay.io
</span></span><span style=display:flex><span>        garbageCollection:
</span></span><span style=display:flex><span>          enabled: <span style=color:#00f>false</span>
</span></span><span style=display:flex><span>        secretReferenceName: quay-credentials
</span></span><span style=display:flex><span>  <span style=color:green># ...</span>
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>  - name: quay-credentials
</span></span><span style=display:flex><span>    resourceRef:
</span></span><span style=display:flex><span>      apiVersion: v1
</span></span><span style=display:flex><span>      kind: Secret
</span></span><span style=display:flex><span>      name: quay-credentials-v1
</span></span></code></pre></div><p>The <code>providerConfig</code> field is required.</p><p>The <code>providerConfig.caches</code> field contains information about the registry caches to deploy. It is a required field. At least one cache has to be specified.</p><p>The <code>providerConfig.caches[].upstream</code> field is the remote registry host (and optionally port) to cache. It is a required field.
The desired format is <code>host[:port]</code>. The value must not include a scheme. The configured upstream registry must be accessible by <code>https</code> (<code>https://</code> is the assumed scheme).</p><p>The <code>providerConfig.caches[].volume</code> field contains settings for the registry cache volume.
The registry-cache extension deploys a StatefulSet with a volume claim template. A PersistentVolumeClaim is created with the configured size and StorageClass name.</p><p>The <code>providerConfig.caches[].volume.size</code> field is the size of the registry cache. Defaults to <code>10Gi</code>. The size must be a positive quantity (greater than 0).
This field is immutable. See <a href=#increase-the-cache-disk-size>Increase the cache disk size</a> on how to resize the disk.</p><p>The <code>providerConfig.caches[].volume.storageClassName</code> field is the name of the StorageClass used by the registry cache volume.
This field is immutable. If the field is not specified, then the <a href=https://kubernetes.io/docs/concepts/storage/storage-classes/#default-storageclass>default StorageClass</a> will be used.</p><p>The <code>providerConfig.caches[].garbageCollection.enabled</code> field enables/disables the cache&rsquo;s garbage collection. Defaults to <code>true</code>. The time to live (ttl) for an image is <code>7d</code>. See the <a href=#garbage-collection>garbage collection section</a> for more details.</p><p>The <code>providerConfig.caches[].secretReferenceName</code> is the name of the reference for the Secret containing the upstream registry credentials. To cache images from a private registry, credentials to the upstream registry should be supplied. For details see <a href=/docs/extensions/others/gardener-extension-registry-cache/upstream-credentials/#how-to-provide-credentials-for-upstream-registry>How to provide credentials for upstream registry</a>.</p><blockquote><p><strong>Note</strong>: It&rsquo;s only possible to provide one set of credentials for one private upstream registry.</p></blockquote><h2 id=garbage-collection>Garbage Collection</h2><p>When the registry cache receives a request for an image that is not present in its local store, it fetches the image from the upstream, returns it to the client and stores the image in the local store. The registry cache runs a scheduler that deletes images when their time to live (ttl) expires. When adding an image to the local store, the registry cache also adds a time to live for the image. The ttl value is <code>7d</code>. Requesting an image from the registry cache does not extend the time to live of the image. Hence, an image is always garbage collected from the registry cache store after <code>7d</code>.
At the time of writing this document, there is no functionality for garbage collection based on disk size - e.g. garbage collecting images when a certain disk usage threshold is passed.</p><h2 id=increase-the-cache-disk-size>Increase the cache disk size</h2><p>When there is no available disk space, the registry cache continues to respond to requests. However, it cannot store the remotely fetched images locally because it has no free disk space. In such case, it is simply acting as a proxy without being able to cache the images in its local store. The disk has to be resized to ensure that the registry cache continues to cache images.</p><p>There are two alternatives to enlarge the cache&rsquo;s disk size.</p><h4 id=alternative-1-resize-the-pvc>[Alternative 1] Resize the PVC</h4><p>To enlarge the PVC&rsquo;s size follow the following steps:</p><ol><li><p>Make sure that the <code>KUBECONFIG</code> environment variable is targeting the correct Shoot cluster.</p></li><li><p>Find the PVC name to resize for the desired upstream. The below example fetches the PVC for the <code>docker.io</code> upstream:</p><pre tabindex=0><code>% kubectl -n kube-system get pvc -l upstream-host=docker.io
</code></pre></li><li><p>Patch the PVC&rsquo;s size to the desired size. The below example patches the size of a PVC to <code>10Gi</code>:</p><pre tabindex=0><code>% kubectl -n kube-system patch pvc $PVC_NAME --type merge -p &#39;{&#34;spec&#34;:{&#34;resources&#34;:{&#34;requests&#34;: {&#34;storage&#34;: &#34;10Gi&#34;}}}}&#39;
</code></pre></li><li><p>Make sure that the PVC gets resized. Describe the PVC to check the resize operation result:</p><pre tabindex=0><code>% kubectl -n kube-system describe pvc -l upstream-host=docker.io
</code></pre></li></ol><blockquote><p>Drawback of this approach: The cache&rsquo;s size in the Shoot spec (<code>providerConfig.caches[].size</code>) diverges from the PVC&rsquo;s size.</p></blockquote><h4 id=alternative-2-remove-and-re-add-the-cache>[Alternative 2] Remove and re-add the cache</h4><p>There is always the option to remove the cache from the Shoot spec and to re-add it again with the updated size.</p><blockquote><p>Drawback of this approach: The already cached images get lost and the cache starts with an empty disk.</p></blockquote><h2 id=high-availability>High-availability</h2><p>The registry cache runs with a single replica. This fact may lead to concerns for the high-availability such as &ldquo;What happens when the registry cache is down? Does containerd fail to pull the image?&rdquo;. As outlined in the <a href=#how-does-it-work>How does it work? section</a>, containerd is configured to fall back to the upstream registry if it fails to pull the image from the registry cache. Hence, when the registry cache is unavailable, the containerd&rsquo;s image pull operations are not affected because containerd falls back to image pull from the upstream registry.</p><h2 id=gotchas>Gotchas</h2><ul><li>The used registry implementation (<a href=https://github.com/distribution/distribution>distribution/distribution</a>) supports mirroring of only one upstream registry. The extension deploys a pull-through cache for each configured upstream.</li><li><code>gcr.io</code>, <code>us.gcr.io</code>, <code>eu.gcr.io</code>, and <code>asia.gcr.io</code> are different upstreams. Hence, configuring <code>gcr.io</code> as upstream won&rsquo;t cache images from <code>us.gcr.io</code>, <code>eu.gcr.io</code>, or <code>asia.gcr.io</code>.</li></ul><h2 id=limitations>Limitations</h2><ul><li>A registry cache cannot cache content from the Shoot system components if such upstream is requested:<ul><li>On Shoot creation with the registry cache extension enabled, a registry cache is unable to cache all of images from the Shoot system components because a registry cache Pod requires its PVC to be provisioned, attached and mounted (the corresponding CSI node plugin needs to be running). Usually, until the registry cache Pod is running containerd falls back to the upstream for pulling the images from the Shoot system components.</li><li>On new Node creation for existing Shoot with the registry cache extension enabled, a registry cache is unable to cache most of the images from Shoot system components because the containerd registry configuration on that Node is applied after the registry cache Service is reachable from the Node (the <code>configure-containerd-registries.service</code> unit is the machinery that does this). The reachability of the registry cache Service requires the Service network to be set up, i.e. the kube-proxy for that new Node to be running and to have set up iptables/IPVS configuration for the registry cache Service.</li></ul></li><li>Services cannot be resolved by DNS from the Node. That&rsquo;s why the registry cache&rsquo;s Service cluster IP is configured in containerd (instead of the Service DNS). A Service&rsquo;s cluster IP is assigned on its creation by the kube-apiserver. Deletion of the registry cache&rsquo;s Service by Shoot owner would lead the Service to be recreated with a new cluster IP. In such case, until the next Shoot reconciliation, containerd will be configured with the old cluster IP. Hence, containerd will fail to pull images from the cache.</li><li>containerd is configured to fall back to the upstream itself if a request against the cache fails. However, if the cluster IP of the registry cache Service does not exist or if kube-proxy hasn&rsquo;t configured iptables/IPVS rules for the registry cache Service, then containerd requests against the registry cache time out in 30 seconds. This increases significantly the image pull times because containerd does multiple requests as part of the image pull (HEAD request to resolve the manifest by tag, GET request for the manifest by SHA, GET requests for blobs).<ul><li>Example: If the Service of a registry cache is deleted, then a new Service will be created. containerd registry config will still contain the old Service&rsquo;s cluster IP.<ul><li>Image pull of <code>docker.io/library/alpine:3.13.2</code> from the upstream takes ~2s while image pull of the same image with invalid registry cache cluster IP takes ~2m.2s.</li><li>Image pull of <code>eu.gcr.io/gardener-project/gardener/ops-toolbelt:0.18.0</code> from the upstream takes ~10s while image pull of the same image with invalid registry cache cluster IP takes ~3m.10s.</li></ul></li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c7f55909c55d9671db41105c674d8223>5.7.2 - Extension Registry Cache</h1><h1 id=developer-docs-for-gardener-extension-registry-cache>Developer Docs for Gardener Extension Registry Cache</h1><p>This document outlines how the Shoot reconciliation and deletion work for a Shoot with the registry-cache extension enabled.</p><h2 id=shoot-reconciliation>Shoot reconciliation</h2><p>This section outlines how the reconciliation works for a Shoot with the registry-cache extension enabled.</p><h4 id=extension-enablementreconciliation>Extension enablement/reconciliation</h4><p>This section outlines how the extension enablement/reconciliation works, e.g the extension has beeen added to the Shoot spec.</p><ol><li>As part of the Shoot reconciliation flow, gardenlet deploys the <a href=https://github.com/gardener/gardener/blob/v1.82.0/docs/extensions/extension.md>Extension</a> resource.</li><li>The registry-cache extension reconciles the Extension resource. <a href=https://github.com/gardener/gardener-extension-registry-cache/blob/main/pkg/controller/extension/actuator.go>pkg/controller/extension/actuator.go</a> contains the implementation of the <a href=https://github.com/gardener/gardener/blob/v1.82.0/extensions/pkg/controller/extension/actuator.go>extension.Actuator</a> interface. The reconciliation of an Extension of type <code>registry-cache</code> consists of the following steps:<ol><li>The extension checks if a registry has been removed (by comparing the status and the spec of the Extension). If an upstream is being removed, then it deploys the <a href=https://github.com/gardener/gardener-extension-registry-cache/blob/main/pkg/component/registryconfigurationcleaner/registry_configuration_cleaner.go><code>registry-cleaner</code> DaemonSet</a> to the Shoot cluster to clean up the existing configuration for the upstream that has to be removed.</li><li>The registry-cache extension deploys resources to the Shoot cluster via ManagedResource. For every configured upstream it creates a StatefulSet (with PVC), Service and other resources.</li><li>It lists all Services from the <code>kube-system</code> namespace having the <code>upstream-host</code> label. It will return an error (and retry in exponential backoff) until the Services count matches the configured registries count.</li><li>When there is a Service created for each configured upstream registry, the registry-cache extension populates the Extension resource status. In the Extension status, for each upstream, it maintains an endpoint (in format <code>http://&lt;cluster-ip>:5000</code>) which can be used to access the registry cache from within the Shoot cluster. <code>&lt;cluster-ip></code> is the cluster IP of the registry cache Service. The cluster IP of a Service is assigned by the Kubernetes API server on Service creation.</li></ol></li><li>As part of the Shoot reconciliation flow, gardenlet deploys the <a href=https://github.com/gardener/gardener/blob/v1.82.0/docs/extensions/operatingsystemconfig.md>OperatingSystemConfig</a> resource.</li><li>The registry-cache extension serves a webhook that mutates the OperatingSystemConfig resource for Shoots having the registry-cache extension enabled (the corresponding namespace gets labeled by gardenlet with <code>extensions.gardener.cloud/registry-cache=true</code>). <a href=https://github.com/gardener/gardener-extension-registry-cache/blob/main/pkg/webhook/operatingsystemconfig/ensurer.go>pkg/webhook/operatingsystemconfig/ensurer.go</a> contains implementation of the <a href=https://github.com/gardener/gardener/blob/v1.82.0/extensions/pkg/webhook/controlplane/genericmutator/mutator.go>genericmutator.Ensurer</a> interface.<ol><li>The webhook appends the <a href=https://github.com/gardener/gardener-extension-registry-cache/blob/main/pkg/webhook/operatingsystemconfig/scripts/configure-containerd-registries.sh>configure-containerd-registries.sh</a> script to the OperatingSystemConfig files. The script accepts registries in the format <code>&lt;upstream_host>,&lt;registry_cache_endpoint>,&lt;upstream_url></code> separated by a space. For each given registry the script waits until the given registry is available (a request to the <code>&lt;registry_cache_endpoint></code> succeeds). Then it creates a <code>hosts.toml</code> file for the given <code>&lt;upstream_host></code>. In short, the <code>hosts.toml</code> file instructs containerd to first try to pull images for the given <code>&lt;upstream_host></code> from the configured <code>&lt;registry_cache_endpoint></code>. For more information about containerd registry configuration, see the <a href=https://github.com/containerd/containerd/blob/main/docs/hosts.md>containerd documentation</a>. The motivation to introduce the <code>configure-containerd-registries.sh</code> script is that we need to create the <code>hosts.toml</code> file when the corresponding registry is available. For more details, see <a href=https://github.com/gardener/gardener-extension-registry-cache/pull/68>https://github.com/gardener/gardener-extension-registry-cache/pull/68</a>.</li><li>The webhook appends the <code>configure-containerd-registries.service</code> unit to the OperatingSystemConfig units. The webhook fetches the Extension resource and then it configures the unit to invoke the <code>configure-containerd-registries.sh</code> script with the registries from the Extension status.</li></ol></li></ol><h4 id=extension-disablement>Extension disablement</h4><p>This section outlines how the extension disablement works, i.e the extension has be removed from the Shoot spec.</p><ol><li>As part of the Shoot reconciliation flow, gardenlet destroys the <a href=https://github.com/gardener/gardener/blob/v1.82.0/docs/extensions/extension.md>Extension</a> resource because it is no longer needed.<ol><li>If the Extension resource contains registries in its status, the registry-cache extension deploys the <a href=https://github.com/gardener/gardener-extension-registry-cache/blob/main/pkg/component/registryconfigurationcleaner/registry_configuration_cleaner.go><code>registry-cleaner</code> DaemonSet</a> to the Shoot cluster to clean up the existing registry configuration.</li><li>The extension deletes the ManagedResource containing the registry cache resources.</li></ol></li></ol><h2 id=shoot-deletion>Shoot deletion</h2><p>This section outlines how the deletion works for a Shoot with the registry-cache extension enabled.</p><ol><li>As part of the Shoot deletion flow, gardenlet destroys the <a href=https://github.com/gardener/gardener/blob/v1.82.0/docs/extensions/extension.md>Extension</a> resource.<ol><li>In the Shoot deletion flow the Extension resource is deleted after the Worker resource. Hence, there is no need to deploy the <a href=https://github.com/gardener/gardener-extension-registry-cache/blob/main/pkg/component/registryconfigurationcleaner/registry_configuration_cleaner.go><code>registry-cleaner</code> DaemonSet</a> to the Shoot cluster to clean up the existing registry configuration.</li><li>The extension deletes the ManagedResource containing the registry cache resources.</li></ol></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-6292d6e3adb445c003eb0156582838fc>5.7.3 - Getting Started Locally</h1><h1 id=deploying-registry-cache-extension-locally>Deploying Registry Cache Extension Locally</h1><h2 id=prerequisites>Prerequisites</h2><ul><li>Make sure that you have a running local Gardener setup. The steps to complete this can be found in the <a href=/docs/gardener/deployment/getting_started_locally/>Deploying Gardener Locally guide</a>.</li></ul><h2 id=setting-up-the-registry-cache-extension>Setting up the Registry Cache Extension</h2><p>Make sure that your <code>KUBECONFIG</code> environment variable is targeting the local Gardener cluster. When this is ensured, run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make extension-up
</span></span></code></pre></div><p>The corresponding make target will build the extension image, load it into the kind cluster Nodes, and deploy the registry-cache ControllerDeployment and ControllerRegistration resources. The container image in the ControllerDeployment will be the image that was build and loaded into the kind cluster Nodes.</p><p>The make target will then deploy then registry-cache admission component. It will build the admission image, load it into the kind cluster Nodes, and finally install the admission component charts to the kind cluster.</p><h2 id=creating-a-shoot-cluster>Creating a <code>Shoot</code> Cluster</h2><p>Once the above step is completed you can create a Shoot cluster. Review the Shoot specification in <a href=https://github.com/gardener/gardener-extension-registry-cache/blob/main/example/shoot.yaml><code>example/shoot.yaml</code></a>. Create the Shoot:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl create -f example/shoot.yaml
</span></span></code></pre></div><h2 id=tearing-down-the-dev-environment>Tearing Down the Dev Environment</h2><p>To tear down the development environment delete the Shoot cluster or disable the <code>registry-cache</code> extension in the Shoot&rsquo;s specification. When the extension is not used by the Shoot anymore, you can run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>make extension-down
</span></span></code></pre></div><p>The make target will delete the ControllerDeployment and ControllerRegistration of the extension, and the registry-cache admission helm deployment.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bcc26d03c6847a4b525645d597bb7ce6>5.7.4 - Migration From V1alpha1 To V1alpha2</h1><h1 id=migration-from-v1alpha1-to-v1alpha2>Migration from <code>v1alpha1</code> to <code>v1alpha2</code></h1><p>This document descibres how to migrate from API version <code>registry.extensions.gardener.cloud/v1alpha1</code> of the <code>RegistryConfig</code> to <code>registry.extensions.gardener.cloud/v1alpha2</code>.</p><p>The <code>registry.extensions.gardener.cloud/v1alpha1</code> is deprecated and will be removed in a future version. Use <code>registry.extensions.gardener.cloud/v1alpha2</code> instead.</p><p>Let&rsquo;s first inspect how the <code>RegistryConfig</code> looks like in API version <code>registry.extensions.gardener.cloud/v1alpha1</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: registry.extensions.gardener.cloud/v1alpha1
</span></span><span style=display:flex><span>kind: RegistryConfig
</span></span><span style=display:flex><span>caches:
</span></span><span style=display:flex><span>- upstream: docker.io
</span></span><span style=display:flex><span>  size: 10Gi
</span></span><span style=display:flex><span>  garbageCollection:
</span></span><span style=display:flex><span>    enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  secretReferenceName: docker-credentials
</span></span></code></pre></div><p>The translation of the above <code>RegistryConfig</code> in API version <code>registry.extensions.gardener.cloud/v1alpha2</code> is:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: registry.extensions.gardener.cloud/v1alpha2
</span></span><span style=display:flex><span>kind: RegistryConfig
</span></span><span style=display:flex><span>caches:
</span></span><span style=display:flex><span>- upstream: docker.io
</span></span><span style=display:flex><span>  volume:
</span></span><span style=display:flex><span>    size: 10Gi
</span></span><span style=display:flex><span>    storageClassName: default
</span></span><span style=display:flex><span>  garbageCollection:
</span></span><span style=display:flex><span>    enabled: <span style=color:#00f>true</span>
</span></span><span style=display:flex><span>  secretReferenceName: docker-credentials
</span></span></code></pre></div><p>As you can notice, there is one breaking change in API version <code>registry.extensions.gardener.cloud/v1alpha2</code> - the <code>caches[].size</code> field is moved to <code>caches[].volume.size</code>.</p><p><code>registry.extensions.gardener.cloud/v1alpha2</code> also adds a new field <code>caches[].volume.storageClassName</code>. In <code>v1alpha1</code> the StorageClass name was not configurable and the registry-cache extension assumed the StorageClass name to be <code>default</code>. When migrating from <code>v1alpha1</code> to <code>v1alpha2</code>, the <code>caches[].volume.storageClassName</code> field has to be set to <code>default</code>. This is required due to backwards-compatibility reasons for registry caches created according to the <code>v1alpha1</code> API version.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c91d6cfd4f3d49b959122675e2c865e7>5.7.5 - Upstream Credentials</h1><h1 id=how-to-provide-credentials-for-upstream-registry>How to provide credentials for upstream registry?</h1><p>In order to pull private images through registry cache, it is required to supply credentials for the private upstream registry.</p><h2 id=how-to-configure-the-registry-cache-to-use-upstream-registry-credentials>How to configure the registry cache to use upstream registry credentials?</h2><ol><li><p>Create an immutable Secret with the upstream registry credentials</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>% kubectl create -f - <span style=color:#a31515>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#a31515>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#a31515>kind: Secret
</span></span></span><span style=display:flex><span><span style=color:#a31515>metadata:
</span></span></span><span style=display:flex><span><span style=color:#a31515>  name: ro-docker-secret-v1
</span></span></span><span style=display:flex><span><span style=color:#a31515>  namespace: garden-dev
</span></span></span><span style=display:flex><span><span style=color:#a31515>type: Opaque
</span></span></span><span style=display:flex><span><span style=color:#a31515>immutable: true
</span></span></span><span style=display:flex><span><span style=color:#a31515>data:
</span></span></span><span style=display:flex><span><span style=color:#a31515>  username: $(echo -n $USERNAME | base64 -w0)
</span></span></span><span style=display:flex><span><span style=color:#a31515>  password: $(echo -n $PASSWORD | base64 -w0)
</span></span></span><span style=display:flex><span><span style=color:#a31515>EOF</span>
</span></span></code></pre></div><p>For GCR, the username is <code>_json_key</code> and the password is the service account key in JSON format. To base64 encode the service account key, copy it and run:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>% echo -n $SERVICE_ACCOUNT_KEY_JSON | base64 -w0
</span></span></code></pre></div></li><li><p>Add the newly created Secret as a reference to the Shoot spec, and then to the registry-cache extension configuration</p><p>In the registry-cache configuration set the <code>secretReferenceName</code> field. It should point to a resource reference under <code>spec.resources</code>. The resource reference itself points to the Secret in project namespace.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>apiVersion: core.gardener.cloud/v1beta1
</span></span><span style=display:flex><span>kind: Shoot
</span></span><span style=display:flex><span><span style=color:green># ...</span>
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  extensions:
</span></span><span style=display:flex><span>  - type: registry-cache
</span></span><span style=display:flex><span>    providerConfig:
</span></span><span style=display:flex><span>      apiVersion: registry.extensions.gardener.cloud/v1alpha2
</span></span><span style=display:flex><span>      kind: RegistryConfig
</span></span><span style=display:flex><span>      caches:
</span></span><span style=display:flex><span>      - upstream: docker.io
</span></span><span style=display:flex><span>        secretReferenceName: docker-secret
</span></span><span style=display:flex><span>  <span style=color:green># ...</span>
</span></span><span style=display:flex><span>  resources:
</span></span><span style=display:flex><span>  - name: docker-secret
</span></span><span style=display:flex><span>    resourceRef:
</span></span><span style=display:flex><span>      apiVersion: v1
</span></span><span style=display:flex><span>      kind: Secret
</span></span><span style=display:flex><span>      name: ro-docker-secret-v1
</span></span><span style=display:flex><span><span style=color:green># ...</span>
</span></span></code></pre></div></li></ol><h2 id=how-to-rotate-the-registry-credentials>How to rotate the registry credentials?</h2><p>To rotate registry credentials perform the following steps:</p><ol><li>Generate new pair of credentials in the cloud provider account. Do not invalidate the old ones.</li><li>Create a new Secret (e.g. <code>ro-docker-secret-v2</code>) with the newly generated credentials as described step 1. in <a href=#how-to-configure-the-registry-cache-to-use-upstream-registry-credentials>How to configure the registry cache to use upstream registry credentials?</a>.</li><li>Update the Shoot spec with newly created Secret as described step 2. in <a href=#how-to-configure-the-registry-cache-to-use-upstream-registry-credentials>How to configure the registry cache to use upstream registry credentials?</a>.
1 The above step will trigger a Shoot reconciliation. Wait for the Shoot reconciliation to complete.</li><li>Make sure that the old Secret is no longer referenced by any Shoot cluster. Finally, delete the Secret containing the old credentials (e.g. <code>ro-docker-secret-v1</code>).</li><li>Delete the corresponding old credentials from the cloud provider account.</li></ol><h2 id=gotchas>Gotchas</h2><ul><li>The registry cache provides the credentials for every request against the corresponding upstream. In some cases, misconfigured credentials can prevent the registry cache to pull even public images from the upstream (for example: invalid service account key for GCR). However, this behaviour is controlled by the server-side logic of the upstream registry.</li></ul></div></main></div></div><footer class="footer row d-print-none"><div class="container-fluid footer-wrapper"><ul class=nav><li><a href=https://gardener.cloud/blog/>Blogs</a></li><li><a href=https://gardener.cloud/community/>Community</a></li><li><a href=https://gardener.cloud/adopter/>Adopters</a></li><li><a href=/docs/>Documentation</a></li></ul><img src=/images/lp/gardener-logo.svg alt="Logo Gardener" class=logo><ul class=media-wr><li><a target=_blank href=https://kubernetes.slack.com/archives/CB57N0BFG><img src=/images/branding/slack-logo-white.svg class=media-icon><div class=media-text>Slack</div></a></li><li><a target=_blank href=https://github.com/gardener><img src=/images/branding/github-mark-logo.png class=media-icon><div class=media-text>GitHub</div></a></li><li><a target=_blank href=https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw><img src=/images/branding/youtube-logo-dark.svg class=media-icon><div class=media-text>YouTube</div></a></li><li><a target=_blank href=https://twitter.com/GardenerProject><img src=/images/branding/twitter-logo-white.svg class=media-icon><div class=media-text>Twitter</div></a></li></ul><span class=copyright>Copyright 2019-2023 Gardener project authors. <a href=https://www.sap.com/corporate/en/legal/privacy.html>Privacy policy
<i class="fa fa-external-link" aria-hidden=true></i></a></span></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js integrity="sha512-JERecFUBbsm75UpkVheAuDOE8NdHjQBrPACfEQYPwvPG+fjgCpHAz1Jw2ci9EXmd3DdfiWth3O3CQvcfEg8gsA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.7b24c0fb082ffb2de6cb14d6c95e9f8053053709ffcf8c761ef8e9ad2f8021e4.js integrity="sha256-eyTA+wgv+y3myxTWyV6fgFMFNwn/z4x2HvjprS+AIeQ=" crossorigin=anonymous></script></body></html>