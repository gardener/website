<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gardener – 2021</title><link>https://gardener.cloud/blog/2021/</link><description>Recent content in 2021 on Gardener</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Mon, 01 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://gardener.cloud/blog/2021/index.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Happy Anniversary, Gardener! Three Years of Open Source Kubernetes Management</title><link>https://gardener.cloud/blog/2021/02.01-happy-anniversary-gardener/</link><pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2021/02.01-happy-anniversary-gardener/</guid><description>
&lt;p>Happy New Year Gardeners!
As we greet 2021, we also celebrate Gardener’s third anniversary. Gardener was born with its first open source
&lt;a href="https://github.com/gardener/gardener/commit/d9619d01845db8c7105d27596fdb7563158effe1">commit&lt;/a>
on 10.1.2018 (its inception within SAP was of course some 9 months earlier):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-text" data-lang="text">&lt;span style="display:flex;">&lt;span>commit d9619d01845db8c7105d27596fdb7563158effe1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Author: Gardener Development Community &amp;lt;gardener.opensource@sap.com&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Date: Wed Jan 10 13:07:09 2018 +0100
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Initial version of gardener
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> This is the initial contribution to the Open Source Gardener project.
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Looking back, three years down the line, the project initiators were working towards a special goal: Publishing Gardener as an open source project on Github.com.
Join us as we look back at how it all began, the challenges Gardener aims to solve, and why open source and the community was and is the project’s key enabler.&lt;/p>
&lt;h2 id="gardener-kick-off-we-opted-to-build-ourselves">Gardener Kick-Off: “We opted to BUILD ourselves”&lt;/h2>
&lt;p>Early 2017, SAP put together a small, jelled team of experts with a clear mission: work out how SAP could serve Kubernetes based
environments (as a service) for all teams within the company. Later that same year, SAP also joined the &lt;a href="https://www.cncf.io/">CNCF&lt;/a> as a platinum member.&lt;/p>
&lt;p>We first deliberated intensively on the BUY options (including acquisitions, due to the size and estimated volume needed at SAP). There were some early products from commercial vendors and startups available that did not bind exclusively to one of the hyperscalers, but these products did not cover many of our crucial and immediate requirements for a multi-cloud environment.&lt;/p>
&lt;p>Ultimately, we opted to BUILD ourselves. This decision was not made lightly, because right from the start, we knew that we would have to cover thousands of clusters, across the globe, on all kinds of infrastructures. We would have to be able to create them at scale as well as manage them 24x7. And thus, we predicted the need to invest into automation of all aspects, to keep the service TCO at a minimum, and to offer an enterprise worthy SLA early on. This particular endeavor grew into launching the project Gardener, first internally, and ultimately fulfilling all checks, externally based on open source.
Its mission statement, in a nutshell, is “Universal Kubernetes at scale”.
Now, that’s quite bold. But we also had a nifty innovation that helped us tremendously along the way. And we can openly reveal the secret here: Gardener was built, not only for creating Kubernetes at scale, but it was built (recursively) in Kubernetes itself.&lt;/p>
&lt;h2 id="what-do-you-get-with-gardener">What Do You Get with Gardener?&lt;/h2>
&lt;p>Gardener offers managed and homogenous Kubernetes clusters on IaaS providers like &lt;em>AWS, Azure, GCP, AliCloud, Open Telekom Cloud, SCS, OVH&lt;/em> and more, but also covers versatile infrastructures like &lt;em>OpenStack, VMware&lt;/em> or &lt;em>bare metal&lt;/em>. Day-1 and Day-2 operations are an integral part of a cluster’s feature set. This means that Gardener is not only capable of provisioning or de-provisioning thousands of clusters, but also of monitoring your cluster’s health state, upgrading components in a rolling fashion, or scaling the control plane as well as worker nodes up and down depending on the current resource demand.&lt;/p>
&lt;p>Some features mentioned above might sound familiar to you, simply because they’re squarely derived from Kubernetes. Concretely, if you explore a Gardener managed end-user cluster, you’ll never see the so-called “control plane components” (&lt;em>Kube-Apiserver, Kube-Controller-Manager, Kube-Scheduler, etc.&lt;/em>) The reason is that they run as Pods inside another, hosting/seeding Kubernetes cluster. Speaking in Gardener terms, the latter is called a &lt;em>Seed&lt;/em> cluster, and the end-user cluster is called a &lt;em>Shoot&lt;/em> cluster; and thus the botanical naming scheme for Gardener was born. Further assets like infrastructure components or worker machines are modelled as managed Kubernetes objects too. This allows Gardener to leverage all the great and production proven features of Kubernetes for managing Kubernetes clusters. Our &lt;a href="https://kubernetes.io/blog/2019/12/02/gardener-project-update/">blog post&lt;/a> on Kubernetes.io reveals more details about the architectural refinements.&lt;/p>
&lt;img title="Figure 1: Gardener architecture overview" src="https://gardener.cloud/__resources/gardener-01_dedaf9.png" style="width:90%; height:auto"/>
&lt;figcaption style="text-align:center;margin-top: 0px;margin-bottom: 30px;font-size: 90%;">Figure 1: Gardener architecture overview&lt;/figcaption>
&lt;p>End-users directly benefit from Gardener’s recursive architecture. Many of the requirements that we identified for the Gardener service turned out to be highly convenient for shoot owners. For instance, Seed clusters are usually equipped with &lt;em>DNS&lt;/em> and &lt;em>x509&lt;/em> services. At the same time, these service offerings can be extended to requests coming from the Shoot clusters i.e., end-users get domain names and certificates for their applications out of the box.&lt;/p>
&lt;h2 id="recognizing-the-power-of-open-source">Recognizing the Power of Open Source&lt;/h2>
&lt;p>The Gardener team immediately profited from open source: from Kubernetes obviously, and all its ecosystem projects. That all facilitated our project’s very fast and robust development. But it does not answer:&lt;/p>
&lt;p>“Why would SAP open source a tool that clearly solves a monetizable enterprise requirement?&amp;quot;_&lt;/p>
&lt;p>Short spoiler alert: it initially involved a leap of faith. If we just look at our own decision path, it is undeniable that developers, and with them entire industries, gravitate towards open source. We chose Linux, Containers, and Kubernetes exactly because they are open, and we could bet on network effects, especially around skills. The same decision process is currently replicated in thousands of companies, with the same results. Why? Because all companies are digitally transforming. They are becoming software companies as well to a certain extent. Many of them are also our customers and in many discussions, we recognized that they have the same challenges that we are solving with Gardener. This, in essence, was a key eye opener. We were confident that if we developed Gardener as open source, we’d not only seize the opportunity to shape a Kubernetes management tool that finds broad interest and adoption outside of our use case at SAP, but we could solve common challenges faster with the help of a community, and that in consequence would sustain continuous feature development.&lt;/p>
&lt;p>Coincidently, that was also when the &lt;em>SAP Open Source Program Office (OSPO)&lt;/em> was launched. It supported us making a case to develop Gardener completely as open source.
Today, we can witness that this strategy has unfolded. It opened the gates not only for adoption, but for co-innovation, investment security, and user feedback directly in code. Below you can see an example of how the Gardener project benefits from this external community power as contributions are submitted right away.&lt;/p>
&lt;img title="Figure 2: Example immediate community contribution" src="https://gardener.cloud/__resources/gardener-02_f71c73.png" style="width:90%; height:auto"/>
&lt;figcaption style="text-align:center;margin-top: 0px;margin-bottom: 30px;font-size: 90%;">Figure 2: Example immediate community contribution&lt;/figcaption>
&lt;h2 id="differentiating-gardener-from-other-kubernetes-management-solutions">Differentiating Gardener from Other Kubernetes Management Solutions&lt;/h2>
&lt;p>Imagine that you have created a modern solid cloud native app or service, fully scalable, in containers. And the business case requires you to run the service on multiple clouds, like &lt;em>AWS, AliCloud, Azure&lt;/em>, &amp;hellip; maybe even on-premises like &lt;em>OpenStack&lt;/em> or &lt;em>VMware&lt;/em>. Your development team has done everything to ensure that the workload is highly portable. But they would need to qualify each providers’ managed Kubernetes offering and their custom &lt;em>Bill-of-Material (BoM)&lt;/em>, their versions, their deprecation plan, roadmap etc. Your TCD would explode and this is exactly what teams at SAP experienced. Now, with Gardener you can, instead, roll out homogeneous clusters and stay in control of your versions and a single roadmap. Across all supported providers!&lt;/p>
&lt;p>Also, teams that have serious, or say, more demanding workloads running on Kubernetes will come to the same conclusion: They require the full management control of the Kubernetes underlay. Not only that, they need access, visibility, and all the tuning options for the control plane to safeguard their service. This is a conclusion not only from teams at SAP, but also from our community members, like &lt;em>PingCap&lt;/em>, who use Gardener to serve &lt;em>TiDB Cloud service&lt;/em>. Whenever you need to get serious and need more than one or two clusters, Gardener is your friend.&lt;/p>
&lt;h2 id="who-is-using-gardener">Who Is Using Gardener?&lt;/h2>
&lt;p>Well, there is SAP itself of course, but also the number of Gardener adopters and companies interested in Gardener is growing (~1700 GitHub stars), as more are challenged by multi-cluster and multi-cloud requirements.&lt;/p>
&lt;p>&lt;em>Flant, PingCap, STACKIT, T-Systems, Sky&lt;/em>, or &lt;em>b’nerd&lt;/em> are among these companies, to name a few. They use Gardener to either run products they sell on top or offer managed Kubernetes clusters directly to their clients, or even only components that are re-usable from Gardener.&lt;/p>
&lt;p>An interesting journey in the open source space started with &lt;em>Finanz Informatik Technologie Service (FI-TS)&lt;/em>, an European Central Bank regulated and certified hoster for banks. They operate in very restricted environments, as you can imagine, and as such, they re-designed their datacenter for cloud native workloads from scratch, that is from cabling, racking and stacking to an API that serves bare metal servers.
For Kubernetes-as-a-Service, they evaluated and chose Gardener because it was open and a perfect candidate. With Gardener’s extension capabilities, it was possible to bring managed Kubernetes clusters to their very own bare metal stack, &lt;a href="https://metal-stack.io/">metal-stack.io&lt;/a>.
Of course, this meant implementation effort. But by reusing the Gardener project, &lt;em>FI-TS&lt;/em> was able to leverage our standard with minimal adjustments for their special use-case. Subsequently, with their contributions, SAP was able to make Gardener more open for the community.&lt;/p>
&lt;h2 id="full-speed-ahead-with-the-community-in-2021">Full Speed Ahead with the Community in 2021&lt;/h2>
&lt;p>Some of the current and most active topics are about the installer (&lt;a href="https://github.com/gardener/landscaper">Landscaper&lt;/a>),
&lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/07-shoot-control-plane-migration.md">control plane migration&lt;/a>,
&lt;a href="https://github.com/gardener/gardener/blob/master/docs/proposals/13-automated-seed-management.md">automated seed management&lt;/a> and
documentation. Even though once you are into Kubernetes and then Gardener, all complexity falls into place, you can make all the semantic connections yourself. But beginners that join the community without much prior knowledge should experience a ramp-up with slighter slope. And that is currently a pain point. Experts directly ask questions about documentation not being up-to-date or clear enough. We prioritized the functionality of what you get with Gardener at the outset and need to catch up.
But here is the good part: Now that we are starting the installation subject, later we will have a much broader picture of what we need to install and maintain Gardener, and how we will build it.&lt;/p>
&lt;span style="display: block; text-align: center;">
&lt;img title="Figure 3: Gardener Landscaper" src="https://gardener.cloud/__resources/gardener-03_74be5a.png" style="width:10%; height:auto;"/>
&lt;/span>
&lt;p>In a &lt;a href="https://www.youtube.com/watch?v=pGlHpJle7Zk">community call&lt;/a> last summer, we gave an overview of what we are building:
The Landscaper. With this tool, we will be able to not only install a full Gardener landscape, but we will also streamline patches, updates and upgrades with the Landscaper. Gardener adopters can then attach to a release train from the project and deploy Gardener into a dev, canary and multiple production environments sequentially. Like we do at SAP.&lt;/p>
&lt;h2 id="key-takeaways-in-three-years-of-gardener">Key Takeaways in Three Years of Gardener&lt;/h2>
&lt;h3 id="1-open-source-is-strategic">#1 Open Source is Strategic&lt;/h3>
&lt;p>Open Source is not just about using freely available libraries, components, or tools to optimize your own software production anymore. It is strategic, unfolds for projects like Gardener, and that in the meantime has also reached the Board Room.&lt;/p>
&lt;h3 id="2-solving-concrete-challenges-by-co-innovation">#2 Solving Concrete Challenges by Co-Innovation&lt;/h3>
&lt;p>Users of a particular product or service increasingly vote/decide for open source variants, such as project Gardener, because that allows them to freely innovate and solve concrete challenges by developing exactly what they require (see FI-TS example). This user-centric process has tremendous advantages. It clears out the middleman and other vested interests. You have access to the full code. And lastly, if others start using and contributing to your innovation, it allows enterprises to secure their investments for the long term. And that re-enforces point #1 for enterprises that have yet to create a strategic Open Source Program Office.&lt;/p>
&lt;h3 id="3-cloud-native-skills">#3 Cloud Native Skills&lt;/h3>
&lt;p>Gardener solves problems by applying Kubernetes and Kubernetes principles itself. Developers and operators who obtain familiarity with Kubernetes will immediately notice and appreciate our concept and can contribute intuitively. The Gardener maintainers feel responsible to facilitate community members and contributors. Barriers will further be reduced by our ongoing landscaper and documentation efforts. This is why we are so confident on &lt;a href="https://gardener.cloud/adopter/">Gardener adoption&lt;/a>.&lt;/p>
&lt;p>The Gardener team is gladly welcoming new community members, especially regarding adoption and contribution. Feel invited to try out your very own Gardener installation, join our &lt;a href="https://kubernetes.slack.com/archives/CB57N0BFG">Slack channel&lt;/a> or &lt;a href="https://gardener.cloud/docs/contribute/#bi-weekly-meetings">community calls&lt;/a>. We’re looking forward to seeing you there!&lt;/p></description></item><item><title>Blog: Machine Controller Manager</title><link>https://gardener.cloud/blog/2021/01.25-machine-controller-manager/</link><pubDate>Mon, 25 Jan 2021 00:00:00 +0000</pubDate><guid>https://gardener.cloud/blog/2021/01.25-machine-controller-manager/</guid><description>
&lt;p>Kubernetes is a cloud-native enabler built around the principles for a resilient, manageable, observable, highly automated, loosely coupled system. We know that Kubernetes is infrastructure agnostic with the help of a provider specific &lt;a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/">Cloud Controller Manager&lt;/a>. But Kubernetes has explicitly externalized the management of the nodes. Once they appear - correctly configured - in the cluster, Kubernetes can use them. If nodes fail, Kubernetes can&amp;rsquo;t do anything about it, external tooling is required. But every tool, every provider is different. So, why not elevate node management to a first class Kubernetes citizen? Why not create a Kubernetes native resource that manages machines just like pods? Such an approach is brought to you by the &lt;a href="https://github.com/gardener/machine-controller-manager">Machine Controller Manager&lt;/a> (aka MCM), which, of course, is an open sourced project. MCM gives you the following benefits:&lt;/p>
&lt;ul>
&lt;li>seamlessly manage machines/nodes with a declarative API (of course, across different cloud providers)&lt;/li>
&lt;li>integrate generically with the &lt;a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">cluster autoscaler&lt;/a>&lt;/li>
&lt;li>plugin with tools such as the &lt;a href="https://github.com/kubernetes/node-problem-detector">node-problem-detector&lt;/a>&lt;/li>
&lt;li>transport the immutability design principle to machine/nodes&lt;/li>
&lt;li>implement e.g. rolling upgrades of machines/nodes&lt;/li>
&lt;/ul>
&lt;h2 id="machine-controller-manager-aka-mcm">Machine Controller Manager aka MCM&lt;/h2>
&lt;p>&lt;a href="https://github.com/gardener/machine-controller-manager">Machine Controller Manager&lt;/a> is a group of cooperative controllers that manage the lifecycle of the worker machines. It is inspired by the design of Kube Controller Manager in which various sub controllers manage their respective Kubernetes Clients.&lt;/p>
&lt;p>Machine Controller Manager reconciles a set of Custom Resources namely &lt;code>MachineDeployment&lt;/code>, &lt;code>MachineSet&lt;/code> and &lt;code>Machines&lt;/code> which are managed &amp;amp; monitored by their controllers &lt;code>MachineDeployment Controller&lt;/code>, &lt;code>MachineSet Controller&lt;/code>, &lt;code>Machine Controller&lt;/code> respectively along with another cooperative controller called the &lt;code>Safety Controller&lt;/code>.&lt;/p>
&lt;h2 id="understanding-the-sub-controllers-and-custom-resources-of-mcm">Understanding the sub-controllers and Custom Resources of MCM&lt;/h2>
&lt;p>The Custom Resources &lt;code>MachineDeployment&lt;/code>, &lt;code>MachineSet&lt;/code> and &lt;code>Machines&lt;/code> are very much analogous to the native K8s resources of &lt;code>Deployment&lt;/code>, &lt;code>ReplicaSet&lt;/code> and &lt;code>Pods&lt;/code> respectively. So, in the context of MCM:&lt;/p>
&lt;ul>
&lt;li>&lt;code>MachineDeployment&lt;/code> provides a declarative update for &lt;code>MachineSet&lt;/code> and &lt;code>Machines&lt;/code>. &lt;code>MachineDeployment Controller&lt;/code> reconciles the &lt;code>MachineDeployment&lt;/code> objects and manages the lifecycle of &lt;code>MachineSet&lt;/code> objects. &lt;code>MachineDeployment&lt;/code> consumes a provider specific &lt;code>MachineClass&lt;/code> in its &lt;code>spec.template.spec&lt;/code>, which is the template of the VM spec that would be spawned on the cloud by MCM.&lt;/li>
&lt;li>&lt;code>MachineSet&lt;/code> ensures that the specified number of &lt;code>Machine&lt;/code> replicas are running at a given point of time. &lt;code>MachineSet Controller&lt;/code> reconciles the &lt;code>MachineSet&lt;/code> objects and manages the lifecycle of &lt;code>Machine&lt;/code> objects.&lt;/li>
&lt;li>&lt;code>Machines&lt;/code> are the actual VMs running on the cloud platform provided by one of the supported cloud providers. &lt;code>Machine Controller&lt;/code> is the controller that actually communicates with the cloud provider to create/update/delete machines on the cloud.&lt;/li>
&lt;li>There is a &lt;code>Safety Controller&lt;/code> responsible for handling the unidentified or unknown behaviours from the cloud providers.&lt;/li>
&lt;li>Along with the above Custom Controllers and Resources, MCM requires the &lt;code>MachineClass&lt;/code> to use K8s &lt;code>Secret&lt;/code> that stores cloudconfig (initialization scripts used to create VMs) and cloud specific credentials.&lt;/li>
&lt;/ul>
&lt;h2 id="workings-of-mcm">Workings of MCM&lt;/h2>
&lt;img title="Figure 1: In-Tree Machine Controller Manager" src="https://gardener.cloud/__resources/mcm-00_61bee2.png" style="width:90%; height:auto"/>
&lt;figcaption style="text-align:center;margin-top: 0px;margin-bottom: 30px;font-size: 90%;">Figure 1: In-Tree Machine Controller Manager&lt;/figcaption>
&lt;p>In MCM, there are two K8s clusters in the scope — a &lt;em>Control Cluster&lt;/em> and a &lt;em>Target Cluster&lt;/em>. The Control Cluster is the K8s cluster where the MCM is installed to manage the machine lifecycle of the Target Cluster. In other words, the Control Cluster is the one where the machine-* objects are stored. The Target Cluster is where all the node objects are registered. These clusters can be two distinct clusters or the same cluster, whichever fits.&lt;/p>
&lt;p>When a &lt;code>MachineDeployment&lt;/code> object is created, the &lt;code>MachineDeployment Controller&lt;/code> creates the corresponding &lt;code>MachineSet&lt;/code> object. The &lt;code>MachineSet Controller&lt;/code> in-turn creates the &lt;code>Machine&lt;/code> objects. The &lt;code>Machine Controller&lt;/code> then talks to the cloud provider API and actually creates the VMs on the cloud.&lt;/p>
&lt;p>The cloud initialization script that is introduced into the VMs via the K8s &lt;code>Secret&lt;/code> consumed by the &lt;code>MachineClasses&lt;/code> talks to the KCM (K8s Controller Manager) and creates the node objects. After registering themselves to the Target Cluster, nodes start sending health signals to the machine objects. That is when MCM updates the status of the machine object from &lt;code>Pending&lt;/code> to &lt;code>Running&lt;/code>.&lt;/p>
&lt;h2 id="more-on-safety-controller">More on Safety Controller&lt;/h2>
&lt;p>Safety Controller contains the following functions:&lt;/p>
&lt;h3 id="orphan-vm-handling">Orphan VM Handling&lt;/h3>
&lt;ul>
&lt;li>It lists all the VMs in the cloud; matching the tag of given cluster name and maps the VMs with the &lt;code>Machine&lt;/code> objects using the &lt;code>ProviderID&lt;/code> field. VMs without any backing &lt;code>Machine&lt;/code> objects are logged and deleted after confirmation.&lt;/li>
&lt;li>This handler runs every 30 minutes and is configurable via &lt;code>--machine-safety-orphan-vms-period&lt;/code> flag.&lt;/li>
&lt;/ul>
&lt;h3 id="freeze-mechanism">Freeze Mechanism&lt;/h3>
&lt;ul>
&lt;li>&lt;code>Safety Controller&lt;/code> freezes the &lt;code>MachineDeployment&lt;/code> and &lt;code>MachineSet controller&lt;/code> if the number of &lt;code>Machine&lt;/code> objects goes beyond a certain threshold on top of the &lt;code>Spec.Replicas&lt;/code>. It can be configured by the flag &lt;code>--safety-up&lt;/code> or &lt;code>--safety-down&lt;/code> and also &lt;code>--machine-safety-overshooting-period&lt;/code>.&lt;/li>
&lt;li>&lt;code>Safety Controller&lt;/code> freezes the functionality of the MCM if either of the &lt;code>target-apiserver&lt;/code> or the &lt;code>control-apiserver&lt;/code> is not reachable.&lt;/li>
&lt;li>&lt;code>Safety Controller&lt;/code> unfreezes the MCM automatically once situation is resolved to normal. A &lt;code>freeze&lt;/code> label is applied on &lt;code>MachineDeployment&lt;/code>/&lt;code>MachineSet&lt;/code> to enforce the freeze condition.&lt;/li>
&lt;/ul>
&lt;h2 id="evolution-of-mcm-from-in-tree-to-out-of-tree-oot">Evolution of MCM from In-Tree to Out-of-Tree (OOT)&lt;/h2>
&lt;p>MCM supports declarative management of machines in a K8s Cluster on various cloud providers like AWS, Azure, GCP, AliCloud, OpenStack, Metal-stack, Packet, KubeVirt, VMWare, Yandex. It can, of course, be easily extended to support other cloud providers.&lt;/p>
&lt;p>Going ahead, having the implementation of the Machine Controller Manager supporting too many cloud providers would be too much upkeep from both a development and a maintenance point of view. Which is why the &lt;code>Machine Controller&lt;/code> component of MCM has been moved to Out-of-Tree design, where the &lt;code>Machine Controller&lt;/code> for each respective cloud provider runs as an independent executable, even though typically packaged under the same deployment.&lt;/p>
&lt;img title="Figure 2: Out-Of-Tree Machine Controller Manager" src="https://gardener.cloud/__resources/mcm-01_18218f.png" style="width:90%; height:auto"/>
&lt;figcaption style="text-align:center;margin-top: 0px;margin-bottom: 30px;font-size: 90%;">Figure 2: Out-Of-Tree (OOT) Machine Controller Manager&lt;/figcaption>
&lt;p>This OOT Machine Controller will implement a common interface to manage the VMs on the respective cloud provider. Now, while the &lt;code>Machine Controller&lt;/code> deals with the &lt;code>Machine&lt;/code> objects, the Machine Controller Manager (MCM) deals with higher level objects such as the &lt;code>MachineSet&lt;/code> and &lt;code>MachineDeployment&lt;/code> objects.&lt;/p>
&lt;p>A lot of contributions are already being made towards an OOT Machine Controller Manager for various cloud providers. Below are the links to the repositories:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/gardener/machine-controller-manager-provider-alicloud">Out of Tree Machine Controller Manager for AliCloud&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/machine-controller-manager-provider-aws">Out of Tree Machine Controller Manager for AWS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/machine-controller-manager-provider-azure">Out of Tree Machine Controller Manager for Azure&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/machine-controller-manager-provider-gcp">Out of Tree Machine Controller Manager for GCP&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/machine-controller-manager-provider-kubevirt">Out of Tree Machine Controller Manager for KubeVirt&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/metal-stack/machine-controller-manager-provider-metal">Out of Tree Machine Controller Manager for Metal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/machine-controller-manager-provider-vsphere">Out of Tree Machine Controller Manager for vSphere&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/gardener/machine-controller-manager-provider-yandex">Out of Tree Machine Controller Manager for Yandex&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Watch the &lt;a href="https://youtu.be/p9BJRpdkxjU">Out of Tree Machine Controller Manager&lt;/a> video on our &lt;a href="https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw">Gardener Project&lt;/a> YouTube channel to understand more about OOT MCM.&lt;/p>
&lt;h2 id="who-uses-mcm">Who Uses MCM?&lt;/h2>
&lt;p>&lt;strong>&lt;a href="http://gardener.cloud">Gardener&lt;/a>&lt;/strong>&lt;/p>
&lt;p>MCM is originally developed and employed by a K8s Control Plane as a Service called Gardener. However, the MCM’s design is elegant enough to be employed when managing the machines of any independent K8s clusters, without having to necessarily associate it with Gardener.&lt;/p>
&lt;p>&lt;strong>&lt;a href="https://metal-stack.io">Metal Stack&lt;/a>&lt;/strong>&lt;/p>
&lt;p>Metal-stack is a set of microservices that implements Metal as a Service (MaaS). It enables you to turn your hardware into elastic cloud infrastructure. Metal-stack employs the &lt;a href="https://github.com/metal-stack/machine-controller-manager-provider-metal">adopted&lt;/a> Machine Controller Manager to their Metal API. Check out an introduction to it in &lt;a href="https://www.youtube.com/watch?v=XE-Kpyn8x2k">metal-stack - kubernetes on bare metal&lt;/a>.&lt;/p>
&lt;p>&lt;strong>&lt;a href="http://sky.com">Sky UK Limited&lt;/a>&lt;/strong>&lt;/p>
&lt;p>Sky UK Limited (a broadcaster) migrated their Kubernetes node management from Ansible to Machine Controller Manager. Check out the &lt;a href="https://youtu.be/yF4wq7GAeEM">How Sky is using Machine Controller Manager (MCM) and autoscaler&lt;/a> video on our &lt;a href="https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw">Gardener Project&lt;/a> YouTube channel.&lt;/p>
&lt;p>Also, other interesting use cases with MCM are implemented by Kubernetes enthusiasts, who for example adjusted the Machine Controller Manager to provision machines in the cloud to extend a local Raspberry-Pi K3s cluster. This topic is covered in detail in the &lt;a href="https://youtu.be/UuveyEOn4_o?t=60">2020-07-03 Gardener Community Meeting&lt;/a> on our &lt;a href="https://www.youtube.com/channel/UCwUhwKFREV8Su0gwAJQX7tw">Gardener Project&lt;/a> YouTube channel.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>Machine Controller Manager is the leading automation tool for machine management for, and in, Kubernetes. And the best part is that it is open sourced. It is freely (and easily) usable and extensible, and the community more than welcomes contributions.&lt;/p>
&lt;p>If you want to know more about Machine Controller Manager or find out about a similar scope for your solutions, feel free to visit the GitHub page &lt;a href="https://github.com/gardener/machine-controller-manager">machine-controller-manager&lt;/a>. We are so excited to see what you achieve with Machine Controller Manager.&lt;/p></description></item></channel></rss>